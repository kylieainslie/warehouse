[
  {
    "id": 628,
    "package_name": "ggplot2",
    "title": "Create Elegant Data Visualisations Using the Grammar of Graphics",
    "description": "A system for 'declaratively' creating graphics, based on\n\"The Grammar of Graphics\". You provide the data, tell 'ggplot2'\nhow to map variables to aesthetics, what graphical primitives\nto use, and it takes care of the details.",
    "version": "4.0.1.9000",
    "maintainer": "Thomas Lin Pedersen <thomas.pedersen@posit.co>",
    "author": "Hadley Wickham [aut] (ORCID: <https://orcid.org/0000-0003-4757-117X>),\nWinston Chang [aut] (ORCID: <https://orcid.org/0000-0002-1576-2126>),\nLionel Henry [aut],\nThomas Lin Pedersen [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-5147-4711>),\nKohske Takahashi [aut],\nClaus Wilke [aut] (ORCID: <https://orcid.org/0000-0002-7470-9261>),\nKara Woo [aut] (ORCID: <https://orcid.org/0000-0002-5125-4188>),\nHiroaki Yutani [aut] (ORCID: <https://orcid.org/0000-0002-3385-7233>),\nDewey Dunnington [aut] (ORCID: <https://orcid.org/0000-0002-9415-4582>),\nTeun van den Brand [aut] (ORCID:\n<https://orcid.org/0000-0002-9335-7468>),\nPosit, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://ggplot2.tidyverse.org,\nhttps://github.com/tidyverse/ggplot2",
    "bug_reports": "https://github.com/tidyverse/ggplot2/issues",
    "repository": "",
    "exports": [
      [
        ".data"
      ],
      [
        ".expose_data"
      ],
      [
        ".ignore_data"
      ],
      [
        ".pt"
      ],
      [
        ".stroke"
      ],
      [
        "%+%"
      ],
      [
        "%+replace%"
      ],
      [
        "add_gg"
      ],
      [
        "aes"
      ],
      [
        "aes_"
      ],
      [
        "aes_all"
      ],
      [
        "aes_auto"
      ],
      [
        "aes_q"
      ],
      [
        "aes_string"
      ],
      [
        "after_scale"
      ],
      [
        "after_stat"
      ],
      [
        "alpha"
      ],
      [
        "annotate"
      ],
      [
        "annotation_borders"
      ],
      [
        "annotation_custom"
      ],
      [
        "annotation_logticks"
      ],
      [
        "annotation_map"
      ],
      [
        "annotation_raster"
      ],
      [
        "arrow"
      ],
      [
        "as_label"
      ],
      [
        "as_labeller"
      ],
      [
        "autolayer"
      ],
      [
        "autoplot"
      ],
      [
        "AxisSecondary"
      ],
      [
        "benchplot"
      ],
      [
        "binned_scale"
      ],
      [
        "borders"
      ],
      [
        "calc_element"
      ],
      [
        "check_device"
      ],
      [
        "class_coord"
      ],
      [
        "class_derive"
      ],
      [
        "class_facet"
      ],
      [
        "class_gg"
      ],
      [
        "class_ggplot"
      ],
      [
        "class_ggplot_built"
      ],
      [
        "class_ggproto"
      ],
      [
        "class_guide"
      ],
      [
        "class_guides"
      ],
      [
        "class_labels"
      ],
      [
        "class_layer"
      ],
      [
        "class_layout"
      ],
      [
        "class_mapping"
      ],
      [
        "class_rel"
      ],
      [
        "class_S3_gg"
      ],
      [
        "class_scale"
      ],
      [
        "class_scales_list"
      ],
      [
        "class_theme"
      ],
      [
        "class_waiver"
      ],
      [
        "class_zero_grob"
      ],
      [
        "combine_vars"
      ],
      [
        "complete_theme"
      ],
      [
        "continuous_scale"
      ],
      [
        "Coord"
      ],
      [
        "coord_cartesian"
      ],
      [
        "coord_equal"
      ],
      [
        "coord_fixed"
      ],
      [
        "coord_flip"
      ],
      [
        "coord_map"
      ],
      [
        "coord_munch"
      ],
      [
        "coord_polar"
      ],
      [
        "coord_quickmap"
      ],
      [
        "coord_radial"
      ],
      [
        "coord_sf"
      ],
      [
        "coord_trans"
      ],
      [
        "coord_transform"
      ],
      [
        "CoordCartesian"
      ],
      [
        "CoordFixed"
      ],
      [
        "CoordFlip"
      ],
      [
        "CoordMap"
      ],
      [
        "CoordPolar"
      ],
      [
        "CoordQuickmap"
      ],
      [
        "CoordRadial"
      ],
      [
        "CoordSf"
      ],
      [
        "CoordTrans"
      ],
      [
        "CoordTransform"
      ],
      [
        "cut_interval"
      ],
      [
        "cut_number"
      ],
      [
        "cut_width"
      ],
      [
        "datetime_scale"
      ],
      [
        "derive"
      ],
      [
        "discrete_scale"
      ],
      [
        "draw_key_abline"
      ],
      [
        "draw_key_blank"
      ],
      [
        "draw_key_boxplot"
      ],
      [
        "draw_key_crossbar"
      ],
      [
        "draw_key_dotplot"
      ],
      [
        "draw_key_label"
      ],
      [
        "draw_key_linerange"
      ],
      [
        "draw_key_path"
      ],
      [
        "draw_key_point"
      ],
      [
        "draw_key_pointrange"
      ],
      [
        "draw_key_polygon"
      ],
      [
        "draw_key_rect"
      ],
      [
        "draw_key_smooth"
      ],
      [
        "draw_key_text"
      ],
      [
        "draw_key_timeseries"
      ],
      [
        "draw_key_vline"
      ],
      [
        "draw_key_vpath"
      ],
      [
        "dup_axis"
      ],
      [
        "el_def"
      ],
      [
        "element"
      ],
      [
        "element_blank"
      ],
      [
        "element_geom"
      ],
      [
        "element_grob"
      ],
      [
        "element_line"
      ],
      [
        "element_point"
      ],
      [
        "element_polygon"
      ],
      [
        "element_rect"
      ],
      [
        "element_render"
      ],
      [
        "element_text"
      ],
      [
        "enexpr"
      ],
      [
        "enexprs"
      ],
      [
        "enquo"
      ],
      [
        "enquos"
      ],
      [
        "ensym"
      ],
      [
        "ensyms"
      ],
      [
        "expand_limits"
      ],
      [
        "expand_scale"
      ],
      [
        "expansion"
      ],
      [
        "expr"
      ],
      [
        "Facet"
      ],
      [
        "facet_grid"
      ],
      [
        "facet_null"
      ],
      [
        "facet_wrap"
      ],
      [
        "FacetGrid"
      ],
      [
        "FacetNull"
      ],
      [
        "FacetWrap"
      ],
      [
        "fill_alpha"
      ],
      [
        "find_panel"
      ],
      [
        "flip_data"
      ],
      [
        "flipped_names"
      ],
      [
        "fortify"
      ],
      [
        "from_theme"
      ],
      [
        "Geom"
      ],
      [
        "geom_abline"
      ],
      [
        "geom_area"
      ],
      [
        "geom_bar"
      ],
      [
        "geom_bin_2d"
      ],
      [
        "geom_bin2d"
      ],
      [
        "geom_blank"
      ],
      [
        "geom_boxplot"
      ],
      [
        "geom_col"
      ],
      [
        "geom_contour"
      ],
      [
        "geom_contour_filled"
      ],
      [
        "geom_count"
      ],
      [
        "geom_crossbar"
      ],
      [
        "geom_curve"
      ],
      [
        "geom_density"
      ],
      [
        "geom_density_2d"
      ],
      [
        "geom_density_2d_filled"
      ],
      [
        "geom_density2d"
      ],
      [
        "geom_density2d_filled"
      ],
      [
        "geom_dotplot"
      ],
      [
        "geom_errorbar"
      ],
      [
        "geom_errorbarh"
      ],
      [
        "geom_freqpoly"
      ],
      [
        "geom_function"
      ],
      [
        "geom_hex"
      ],
      [
        "geom_histogram"
      ],
      [
        "geom_hline"
      ],
      [
        "geom_jitter"
      ],
      [
        "geom_label"
      ],
      [
        "geom_line"
      ],
      [
        "geom_linerange"
      ],
      [
        "geom_map"
      ],
      [
        "geom_path"
      ],
      [
        "geom_point"
      ],
      [
        "geom_pointrange"
      ],
      [
        "geom_polygon"
      ],
      [
        "geom_qq"
      ],
      [
        "geom_qq_line"
      ],
      [
        "geom_quantile"
      ],
      [
        "geom_raster"
      ],
      [
        "geom_rect"
      ],
      [
        "geom_ribbon"
      ],
      [
        "geom_rug"
      ],
      [
        "geom_segment"
      ],
      [
        "geom_sf"
      ],
      [
        "geom_sf_label"
      ],
      [
        "geom_sf_text"
      ],
      [
        "geom_smooth"
      ],
      [
        "geom_spoke"
      ],
      [
        "geom_step"
      ],
      [
        "geom_text"
      ],
      [
        "geom_tile"
      ],
      [
        "geom_violin"
      ],
      [
        "geom_vline"
      ],
      [
        "GeomAbline"
      ],
      [
        "GeomAnnotationMap"
      ],
      [
        "GeomArea"
      ],
      [
        "GeomBar"
      ],
      [
        "GeomBin2d"
      ],
      [
        "GeomBlank"
      ],
      [
        "GeomBoxplot"
      ],
      [
        "GeomCol"
      ],
      [
        "GeomContour"
      ],
      [
        "GeomContourFilled"
      ],
      [
        "GeomCrossbar"
      ],
      [
        "GeomCurve"
      ],
      [
        "GeomCustomAnn"
      ],
      [
        "GeomDensity"
      ],
      [
        "GeomDensity2d"
      ],
      [
        "GeomDensity2dFilled"
      ],
      [
        "GeomDotplot"
      ],
      [
        "GeomErrorbar"
      ],
      [
        "GeomErrorbarh"
      ],
      [
        "GeomFunction"
      ],
      [
        "GeomHex"
      ],
      [
        "GeomHline"
      ],
      [
        "GeomLabel"
      ],
      [
        "GeomLine"
      ],
      [
        "GeomLinerange"
      ],
      [
        "GeomLogticks"
      ],
      [
        "GeomMap"
      ],
      [
        "GeomPath"
      ],
      [
        "GeomPoint"
      ],
      [
        "GeomPointrange"
      ],
      [
        "GeomPolygon"
      ],
      [
        "GeomQuantile"
      ],
      [
        "GeomRaster"
      ],
      [
        "GeomRasterAnn"
      ],
      [
        "GeomRect"
      ],
      [
        "GeomRibbon"
      ],
      [
        "GeomRug"
      ],
      [
        "GeomSegment"
      ],
      [
        "GeomSf"
      ],
      [
        "GeomSmooth"
      ],
      [
        "GeomSpoke"
      ],
      [
        "GeomStep"
      ],
      [
        "GeomText"
      ],
      [
        "GeomTile"
      ],
      [
        "GeomViolin"
      ],
      [
        "GeomVline"
      ],
      [
        "get_alt_text"
      ],
      [
        "get_element_tree"
      ],
      [
        "get_geom_defaults"
      ],
      [
        "get_ggplot2_edition"
      ],
      [
        "get_guide_data"
      ],
      [
        "get_labs"
      ],
      [
        "get_last_plot"
      ],
      [
        "get_layer_data"
      ],
      [
        "get_layer_grob"
      ],
      [
        "get_panel_scales"
      ],
      [
        "get_strip_labels"
      ],
      [
        "get_theme"
      ],
      [
        "gg_dep"
      ],
      [
        "gg_par"
      ],
      [
        "ggplot"
      ],
      [
        "ggplot_add"
      ],
      [
        "ggplot_build"
      ],
      [
        "ggplot_gtable"
      ],
      [
        "ggplotGrob"
      ],
      [
        "ggproto"
      ],
      [
        "ggproto_parent"
      ],
      [
        "ggsave"
      ],
      [
        "ggtitle"
      ],
      [
        "Guide"
      ],
      [
        "guide_axis"
      ],
      [
        "guide_axis_logticks"
      ],
      [
        "guide_axis_stack"
      ],
      [
        "guide_axis_theta"
      ],
      [
        "guide_bins"
      ],
      [
        "guide_colorbar"
      ],
      [
        "guide_colorsteps"
      ],
      [
        "guide_colourbar"
      ],
      [
        "guide_coloursteps"
      ],
      [
        "guide_custom"
      ],
      [
        "guide_gengrob"
      ],
      [
        "guide_geom"
      ],
      [
        "guide_legend"
      ],
      [
        "guide_merge"
      ],
      [
        "guide_none"
      ],
      [
        "guide_train"
      ],
      [
        "guide_transform"
      ],
      [
        "GuideAxis"
      ],
      [
        "GuideAxisLogticks"
      ],
      [
        "GuideAxisStack"
      ],
      [
        "GuideAxisTheta"
      ],
      [
        "GuideBins"
      ],
      [
        "GuideColourbar"
      ],
      [
        "GuideColoursteps"
      ],
      [
        "GuideCustom"
      ],
      [
        "GuideLegend"
      ],
      [
        "GuideNone"
      ],
      [
        "GuideOld"
      ],
      [
        "guides"
      ],
      [
        "has_flipped_aes"
      ],
      [
        "is_coord"
      ],
      [
        "is_facet"
      ],
      [
        "is_geom"
      ],
      [
        "is_ggplot"
      ],
      [
        "is_ggproto"
      ],
      [
        "is_guide"
      ],
      [
        "is_guides"
      ],
      [
        "is_layer"
      ],
      [
        "is_mapping"
      ],
      [
        "is_margin"
      ],
      [
        "is_position"
      ],
      [
        "is_scale"
      ],
      [
        "is_stat"
      ],
      [
        "is_theme"
      ],
      [
        "is_theme_element"
      ],
      [
        "is_waiver"
      ],
      [
        "is.Coord"
      ],
      [
        "is.facet"
      ],
      [
        "is.ggplot"
      ],
      [
        "is.ggproto"
      ],
      [
        "is.theme"
      ],
      [
        "label_both"
      ],
      [
        "label_bquote"
      ],
      [
        "label_context"
      ],
      [
        "label_parsed"
      ],
      [
        "label_value"
      ],
      [
        "label_wrap_gen"
      ],
      [
        "labeller"
      ],
      [
        "labs"
      ],
      [
        "last_plot"
      ],
      [
        "layer"
      ],
      [
        "layer_data"
      ],
      [
        "layer_grob"
      ],
      [
        "layer_scales"
      ],
      [
        "layer_sf"
      ],
      [
        "Layout"
      ],
      [
        "lims"
      ],
      [
        "local_ggplot2_edition"
      ],
      [
        "make_constructor"
      ],
      [
        "map_data"
      ],
      [
        "margin"
      ],
      [
        "margin_auto"
      ],
      [
        "margin_part"
      ],
      [
        "max_height"
      ],
      [
        "max_width"
      ],
      [
        "mean_cl_boot"
      ],
      [
        "mean_cl_normal"
      ],
      [
        "mean_sdl"
      ],
      [
        "mean_se"
      ],
      [
        "median_hilow"
      ],
      [
        "merge_element"
      ],
      [
        "new_guide"
      ],
      [
        "old_guide"
      ],
      [
        "panel_cols"
      ],
      [
        "panel_rows"
      ],
      [
        "pattern_alpha"
      ],
      [
        "Position"
      ],
      [
        "position_dodge"
      ],
      [
        "position_dodge2"
      ],
      [
        "position_fill"
      ],
      [
        "position_identity"
      ],
      [
        "position_jitter"
      ],
      [
        "position_jitterdodge"
      ],
      [
        "position_nudge"
      ],
      [
        "position_stack"
      ],
      [
        "PositionDodge"
      ],
      [
        "PositionDodge2"
      ],
      [
        "PositionFill"
      ],
      [
        "PositionIdentity"
      ],
      [
        "PositionJitter"
      ],
      [
        "PositionJitterdodge"
      ],
      [
        "PositionNudge"
      ],
      [
        "PositionStack"
      ],
      [
        "qplot"
      ],
      [
        "quickplot"
      ],
      [
        "quo"
      ],
      [
        "quo_name"
      ],
      [
        "quos"
      ],
      [
        "register_theme_elements"
      ],
      [
        "rel"
      ],
      [
        "remove_missing"
      ],
      [
        "render_axes"
      ],
      [
        "render_strips"
      ],
      [
        "replace_theme"
      ],
      [
        "reset_geom_defaults"
      ],
      [
        "reset_stat_defaults"
      ],
      [
        "reset_theme_settings"
      ],
      [
        "resolution"
      ],
      [
        "Scale"
      ],
      [
        "scale_alpha"
      ],
      [
        "scale_alpha_binned"
      ],
      [
        "scale_alpha_continuous"
      ],
      [
        "scale_alpha_date"
      ],
      [
        "scale_alpha_datetime"
      ],
      [
        "scale_alpha_discrete"
      ],
      [
        "scale_alpha_identity"
      ],
      [
        "scale_alpha_manual"
      ],
      [
        "scale_alpha_ordinal"
      ],
      [
        "scale_color_binned"
      ],
      [
        "scale_color_brewer"
      ],
      [
        "scale_color_continuous"
      ],
      [
        "scale_color_date"
      ],
      [
        "scale_color_datetime"
      ],
      [
        "scale_color_discrete"
      ],
      [
        "scale_color_distiller"
      ],
      [
        "scale_color_fermenter"
      ],
      [
        "scale_color_gradient"
      ],
      [
        "scale_color_gradient2"
      ],
      [
        "scale_color_gradientn"
      ],
      [
        "scale_color_grey"
      ],
      [
        "scale_color_hue"
      ],
      [
        "scale_color_identity"
      ],
      [
        "scale_color_manual"
      ],
      [
        "scale_color_ordinal"
      ],
      [
        "scale_color_steps"
      ],
      [
        "scale_color_steps2"
      ],
      [
        "scale_color_stepsn"
      ],
      [
        "scale_color_viridis_b"
      ],
      [
        "scale_color_viridis_c"
      ],
      [
        "scale_color_viridis_d"
      ],
      [
        "scale_colour_binned"
      ],
      [
        "scale_colour_brewer"
      ],
      [
        "scale_colour_continuous"
      ],
      [
        "scale_colour_date"
      ],
      [
        "scale_colour_datetime"
      ],
      [
        "scale_colour_discrete"
      ],
      [
        "scale_colour_distiller"
      ],
      [
        "scale_colour_fermenter"
      ],
      [
        "scale_colour_gradient"
      ],
      [
        "scale_colour_gradient2"
      ],
      [
        "scale_colour_gradientn"
      ],
      [
        "scale_colour_grey"
      ],
      [
        "scale_colour_hue"
      ],
      [
        "scale_colour_identity"
      ],
      [
        "scale_colour_manual"
      ],
      [
        "scale_colour_ordinal"
      ],
      [
        "scale_colour_steps"
      ],
      [
        "scale_colour_steps2"
      ],
      [
        "scale_colour_stepsn"
      ],
      [
        "scale_colour_viridis_b"
      ],
      [
        "scale_colour_viridis_c"
      ],
      [
        "scale_colour_viridis_d"
      ],
      [
        "scale_continuous_identity"
      ],
      [
        "scale_discrete_identity"
      ],
      [
        "scale_discrete_manual"
      ],
      [
        "scale_fill_binned"
      ],
      [
        "scale_fill_brewer"
      ],
      [
        "scale_fill_continuous"
      ],
      [
        "scale_fill_date"
      ],
      [
        "scale_fill_datetime"
      ],
      [
        "scale_fill_discrete"
      ],
      [
        "scale_fill_distiller"
      ],
      [
        "scale_fill_fermenter"
      ],
      [
        "scale_fill_gradient"
      ],
      [
        "scale_fill_gradient2"
      ],
      [
        "scale_fill_gradientn"
      ],
      [
        "scale_fill_grey"
      ],
      [
        "scale_fill_hue"
      ],
      [
        "scale_fill_identity"
      ],
      [
        "scale_fill_manual"
      ],
      [
        "scale_fill_ordinal"
      ],
      [
        "scale_fill_steps"
      ],
      [
        "scale_fill_steps2"
      ],
      [
        "scale_fill_stepsn"
      ],
      [
        "scale_fill_viridis_b"
      ],
      [
        "scale_fill_viridis_c"
      ],
      [
        "scale_fill_viridis_d"
      ],
      [
        "scale_linetype"
      ],
      [
        "scale_linetype_binned"
      ],
      [
        "scale_linetype_continuous"
      ],
      [
        "scale_linetype_discrete"
      ],
      [
        "scale_linetype_identity"
      ],
      [
        "scale_linetype_manual"
      ],
      [
        "scale_linewidth"
      ],
      [
        "scale_linewidth_binned"
      ],
      [
        "scale_linewidth_continuous"
      ],
      [
        "scale_linewidth_date"
      ],
      [
        "scale_linewidth_datetime"
      ],
      [
        "scale_linewidth_discrete"
      ],
      [
        "scale_linewidth_identity"
      ],
      [
        "scale_linewidth_manual"
      ],
      [
        "scale_linewidth_ordinal"
      ],
      [
        "scale_radius"
      ],
      [
        "scale_shape"
      ],
      [
        "scale_shape_binned"
      ],
      [
        "scale_shape_continuous"
      ],
      [
        "scale_shape_discrete"
      ],
      [
        "scale_shape_identity"
      ],
      [
        "scale_shape_manual"
      ],
      [
        "scale_shape_ordinal"
      ],
      [
        "scale_size"
      ],
      [
        "scale_size_area"
      ],
      [
        "scale_size_binned"
      ],
      [
        "scale_size_binned_area"
      ],
      [
        "scale_size_continuous"
      ],
      [
        "scale_size_date"
      ],
      [
        "scale_size_datetime"
      ],
      [
        "scale_size_discrete"
      ],
      [
        "scale_size_identity"
      ],
      [
        "scale_size_manual"
      ],
      [
        "scale_size_ordinal"
      ],
      [
        "scale_type"
      ],
      [
        "scale_x_binned"
      ],
      [
        "scale_x_continuous"
      ],
      [
        "scale_x_date"
      ],
      [
        "scale_x_datetime"
      ],
      [
        "scale_x_discrete"
      ],
      [
        "scale_x_log10"
      ],
      [
        "scale_x_reverse"
      ],
      [
        "scale_x_sqrt"
      ],
      [
        "scale_x_time"
      ],
      [
        "scale_y_binned"
      ],
      [
        "scale_y_continuous"
      ],
      [
        "scale_y_date"
      ],
      [
        "scale_y_datetime"
      ],
      [
        "scale_y_discrete"
      ],
      [
        "scale_y_log10"
      ],
      [
        "scale_y_reverse"
      ],
      [
        "scale_y_sqrt"
      ],
      [
        "scale_y_time"
      ],
      [
        "ScaleBinned"
      ],
      [
        "ScaleBinnedPosition"
      ],
      [
        "ScaleContinuous"
      ],
      [
        "ScaleContinuousDate"
      ],
      [
        "ScaleContinuousDatetime"
      ],
      [
        "ScaleContinuousIdentity"
      ],
      [
        "ScaleContinuousPosition"
      ],
      [
        "ScaleDiscrete"
      ],
      [
        "ScaleDiscreteIdentity"
      ],
      [
        "ScaleDiscretePosition"
      ],
      [
        "sec_axis"
      ],
      [
        "set_ggplot2_edition"
      ],
      [
        "set_last_plot"
      ],
      [
        "set_theme"
      ],
      [
        "sf_transform_xy"
      ],
      [
        "should_stop"
      ],
      [
        "stage"
      ],
      [
        "standardise_aes_names"
      ],
      [
        "stat"
      ],
      [
        "Stat"
      ],
      [
        "stat_align"
      ],
      [
        "stat_bin"
      ],
      [
        "stat_bin_2d"
      ],
      [
        "stat_bin_hex"
      ],
      [
        "stat_bin2d"
      ],
      [
        "stat_binhex"
      ],
      [
        "stat_boxplot"
      ],
      [
        "stat_connect"
      ],
      [
        "stat_contour"
      ],
      [
        "stat_contour_filled"
      ],
      [
        "stat_count"
      ],
      [
        "stat_density"
      ],
      [
        "stat_density_2d"
      ],
      [
        "stat_density_2d_filled"
      ],
      [
        "stat_density2d"
      ],
      [
        "stat_density2d_filled"
      ],
      [
        "stat_ecdf"
      ],
      [
        "stat_ellipse"
      ],
      [
        "stat_function"
      ],
      [
        "stat_identity"
      ],
      [
        "stat_manual"
      ],
      [
        "stat_qq"
      ],
      [
        "stat_qq_line"
      ],
      [
        "stat_quantile"
      ],
      [
        "stat_sf"
      ],
      [
        "stat_sf_coordinates"
      ],
      [
        "stat_smooth"
      ],
      [
        "stat_spoke"
      ],
      [
        "stat_sum"
      ],
      [
        "stat_summary"
      ],
      [
        "stat_summary_2d"
      ],
      [
        "stat_summary_bin"
      ],
      [
        "stat_summary_hex"
      ],
      [
        "stat_summary2d"
      ],
      [
        "stat_unique"
      ],
      [
        "stat_ydensity"
      ],
      [
        "StatAlign"
      ],
      [
        "StatBin"
      ],
      [
        "StatBin2d"
      ],
      [
        "StatBindot"
      ],
      [
        "StatBinhex"
      ],
      [
        "StatBoxplot"
      ],
      [
        "StatConnect"
      ],
      [
        "StatContour"
      ],
      [
        "StatContourFilled"
      ],
      [
        "StatCount"
      ],
      [
        "StatDensity"
      ],
      [
        "StatDensity2d"
      ],
      [
        "StatDensity2dFilled"
      ],
      [
        "StatEcdf"
      ],
      [
        "StatEllipse"
      ],
      [
        "StatFunction"
      ],
      [
        "StatIdentity"
      ],
      [
        "StatManual"
      ],
      [
        "StatQq"
      ],
      [
        "StatQqLine"
      ],
      [
        "StatQuantile"
      ],
      [
        "StatSf"
      ],
      [
        "StatSfCoordinates"
      ],
      [
        "StatSmooth"
      ],
      [
        "StatSum"
      ],
      [
        "StatSummary"
      ],
      [
        "StatSummary2d"
      ],
      [
        "StatSummaryBin"
      ],
      [
        "StatSummaryHex"
      ],
      [
        "StatUnique"
      ],
      [
        "StatYdensity"
      ],
      [
        "summarise_coord"
      ],
      [
        "summarise_layers"
      ],
      [
        "summarise_layout"
      ],
      [
        "sym"
      ],
      [
        "syms"
      ],
      [
        "theme"
      ],
      [
        "theme_bw"
      ],
      [
        "theme_classic"
      ],
      [
        "theme_dark"
      ],
      [
        "theme_get"
      ],
      [
        "theme_gray"
      ],
      [
        "theme_grey"
      ],
      [
        "theme_light"
      ],
      [
        "theme_linedraw"
      ],
      [
        "theme_minimal"
      ],
      [
        "theme_replace"
      ],
      [
        "theme_set"
      ],
      [
        "theme_sub_axis"
      ],
      [
        "theme_sub_axis_bottom"
      ],
      [
        "theme_sub_axis_left"
      ],
      [
        "theme_sub_axis_right"
      ],
      [
        "theme_sub_axis_top"
      ],
      [
        "theme_sub_axis_x"
      ],
      [
        "theme_sub_axis_y"
      ],
      [
        "theme_sub_legend"
      ],
      [
        "theme_sub_panel"
      ],
      [
        "theme_sub_plot"
      ],
      [
        "theme_sub_strip"
      ],
      [
        "theme_test"
      ],
      [
        "theme_update"
      ],
      [
        "theme_void"
      ],
      [
        "transform_position"
      ],
      [
        "translate_shape_string"
      ],
      [
        "unit"
      ],
      [
        "update_geom_defaults"
      ],
      [
        "update_ggplot"
      ],
      [
        "update_labels"
      ],
      [
        "update_stat_defaults"
      ],
      [
        "update_theme"
      ],
      [
        "vars"
      ],
      [
        "waiver"
      ],
      [
        "with_ggplot2_edition"
      ],
      [
        "wrap_dims"
      ],
      [
        "xlab"
      ],
      [
        "xlim"
      ],
      [
        "ylab"
      ],
      [
        "ylim"
      ],
      [
        "zeroGrob"
      ]
    ],
    "topics": [
      [
        "data-visualisation"
      ],
      [
        "visualisation"
      ],
      [
        "quarto"
      ]
    ],
    "score": 25.4706,
    "stars": 6835,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "ggplot2 Create Elegant Data Visualisations Using the Grammar of Graphics A system for 'declaratively' creating graphics, based on\n\"The Grammar of Graphics\". You provide the data, tell 'ggplot2'\nhow to map variables to aesthetics, what graphical primitives\nto use, and it takes care of the details. .data .expose_data .ignore_data .pt .stroke %+% %+replace% add_gg aes aes_ aes_all aes_auto aes_q aes_string after_scale after_stat alpha annotate annotation_borders annotation_custom annotation_logticks annotation_map annotation_raster arrow as_label as_labeller autolayer autoplot AxisSecondary benchplot binned_scale borders calc_element check_device class_coord class_derive class_facet class_gg class_ggplot class_ggplot_built class_ggproto class_guide class_guides class_labels class_layer class_layout class_mapping class_rel class_S3_gg class_scale class_scales_list class_theme class_waiver class_zero_grob combine_vars complete_theme continuous_scale Coord coord_cartesian coord_equal coord_fixed coord_flip coord_map coord_munch coord_polar coord_quickmap coord_radial coord_sf coord_trans coord_transform CoordCartesian CoordFixed CoordFlip CoordMap CoordPolar CoordQuickmap CoordRadial CoordSf CoordTrans CoordTransform cut_interval cut_number cut_width datetime_scale derive discrete_scale draw_key_abline draw_key_blank draw_key_boxplot draw_key_crossbar draw_key_dotplot draw_key_label draw_key_linerange draw_key_path draw_key_point draw_key_pointrange draw_key_polygon draw_key_rect draw_key_smooth draw_key_text draw_key_timeseries draw_key_vline draw_key_vpath dup_axis el_def element element_blank element_geom element_grob element_line element_point element_polygon element_rect element_render element_text enexpr enexprs enquo enquos ensym ensyms expand_limits expand_scale expansion expr Facet facet_grid facet_null facet_wrap FacetGrid FacetNull FacetWrap fill_alpha find_panel flip_data flipped_names fortify from_theme Geom geom_abline geom_area geom_bar geom_bin_2d geom_bin2d geom_blank geom_boxplot geom_col geom_contour geom_contour_filled geom_count geom_crossbar geom_curve geom_density geom_density_2d geom_density_2d_filled geom_density2d geom_density2d_filled geom_dotplot geom_errorbar geom_errorbarh geom_freqpoly geom_function geom_hex geom_histogram geom_hline geom_jitter geom_label geom_line geom_linerange geom_map geom_path geom_point geom_pointrange geom_polygon geom_qq geom_qq_line geom_quantile geom_raster geom_rect geom_ribbon geom_rug geom_segment geom_sf geom_sf_label geom_sf_text geom_smooth geom_spoke geom_step geom_text geom_tile geom_violin geom_vline GeomAbline GeomAnnotationMap GeomArea GeomBar GeomBin2d GeomBlank GeomBoxplot GeomCol GeomContour GeomContourFilled GeomCrossbar GeomCurve GeomCustomAnn GeomDensity GeomDensity2d GeomDensity2dFilled GeomDotplot GeomErrorbar GeomErrorbarh GeomFunction GeomHex GeomHline GeomLabel GeomLine GeomLinerange GeomLogticks GeomMap GeomPath GeomPoint GeomPointrange GeomPolygon GeomQuantile GeomRaster GeomRasterAnn GeomRect GeomRibbon GeomRug GeomSegment GeomSf GeomSmooth GeomSpoke GeomStep GeomText GeomTile GeomViolin GeomVline get_alt_text get_element_tree get_geom_defaults get_ggplot2_edition get_guide_data get_labs get_last_plot get_layer_data get_layer_grob get_panel_scales get_strip_labels get_theme gg_dep gg_par ggplot ggplot_add ggplot_build ggplot_gtable ggplotGrob ggproto ggproto_parent ggsave ggtitle Guide guide_axis guide_axis_logticks guide_axis_stack guide_axis_theta guide_bins guide_colorbar guide_colorsteps guide_colourbar guide_coloursteps guide_custom guide_gengrob guide_geom guide_legend guide_merge guide_none guide_train guide_transform GuideAxis GuideAxisLogticks GuideAxisStack GuideAxisTheta GuideBins GuideColourbar GuideColoursteps GuideCustom GuideLegend GuideNone GuideOld guides has_flipped_aes is_coord is_facet is_geom is_ggplot is_ggproto is_guide is_guides is_layer is_mapping is_margin is_position is_scale is_stat is_theme is_theme_element is_waiver is.Coord is.facet is.ggplot is.ggproto is.theme label_both label_bquote label_context label_parsed label_value label_wrap_gen labeller labs last_plot layer layer_data layer_grob layer_scales layer_sf Layout lims local_ggplot2_edition make_constructor map_data margin margin_auto margin_part max_height max_width mean_cl_boot mean_cl_normal mean_sdl mean_se median_hilow merge_element new_guide old_guide panel_cols panel_rows pattern_alpha Position position_dodge position_dodge2 position_fill position_identity position_jitter position_jitterdodge position_nudge position_stack PositionDodge PositionDodge2 PositionFill PositionIdentity PositionJitter PositionJitterdodge PositionNudge PositionStack qplot quickplot quo quo_name quos register_theme_elements rel remove_missing render_axes render_strips replace_theme reset_geom_defaults reset_stat_defaults reset_theme_settings resolution Scale scale_alpha scale_alpha_binned scale_alpha_continuous scale_alpha_date scale_alpha_datetime scale_alpha_discrete scale_alpha_identity scale_alpha_manual scale_alpha_ordinal scale_color_binned scale_color_brewer scale_color_continuous scale_color_date scale_color_datetime scale_color_discrete scale_color_distiller scale_color_fermenter scale_color_gradient scale_color_gradient2 scale_color_gradientn scale_color_grey scale_color_hue scale_color_identity scale_color_manual scale_color_ordinal scale_color_steps scale_color_steps2 scale_color_stepsn scale_color_viridis_b scale_color_viridis_c scale_color_viridis_d scale_colour_binned scale_colour_brewer scale_colour_continuous scale_colour_date scale_colour_datetime scale_colour_discrete scale_colour_distiller scale_colour_fermenter scale_colour_gradient scale_colour_gradient2 scale_colour_gradientn scale_colour_grey scale_colour_hue scale_colour_identity scale_colour_manual scale_colour_ordinal scale_colour_steps scale_colour_steps2 scale_colour_stepsn scale_colour_viridis_b scale_colour_viridis_c scale_colour_viridis_d scale_continuous_identity scale_discrete_identity scale_discrete_manual scale_fill_binned scale_fill_brewer scale_fill_continuous scale_fill_date scale_fill_datetime scale_fill_discrete scale_fill_distiller scale_fill_fermenter scale_fill_gradient scale_fill_gradient2 scale_fill_gradientn scale_fill_grey scale_fill_hue scale_fill_identity scale_fill_manual scale_fill_ordinal scale_fill_steps scale_fill_steps2 scale_fill_stepsn scale_fill_viridis_b scale_fill_viridis_c scale_fill_viridis_d scale_linetype scale_linetype_binned scale_linetype_continuous scale_linetype_discrete scale_linetype_identity scale_linetype_manual scale_linewidth scale_linewidth_binned scale_linewidth_continuous scale_linewidth_date scale_linewidth_datetime scale_linewidth_discrete scale_linewidth_identity scale_linewidth_manual scale_linewidth_ordinal scale_radius scale_shape scale_shape_binned scale_shape_continuous scale_shape_discrete scale_shape_identity scale_shape_manual scale_shape_ordinal scale_size scale_size_area scale_size_binned scale_size_binned_area scale_size_continuous scale_size_date scale_size_datetime scale_size_discrete scale_size_identity scale_size_manual scale_size_ordinal scale_type scale_x_binned scale_x_continuous scale_x_date scale_x_datetime scale_x_discrete scale_x_log10 scale_x_reverse scale_x_sqrt scale_x_time scale_y_binned scale_y_continuous scale_y_date scale_y_datetime scale_y_discrete scale_y_log10 scale_y_reverse scale_y_sqrt scale_y_time ScaleBinned ScaleBinnedPosition ScaleContinuous ScaleContinuousDate ScaleContinuousDatetime ScaleContinuousIdentity ScaleContinuousPosition ScaleDiscrete ScaleDiscreteIdentity ScaleDiscretePosition sec_axis set_ggplot2_edition set_last_plot set_theme sf_transform_xy should_stop stage standardise_aes_names stat Stat stat_align stat_bin stat_bin_2d stat_bin_hex stat_bin2d stat_binhex stat_boxplot stat_connect stat_contour stat_contour_filled stat_count stat_density stat_density_2d stat_density_2d_filled stat_density2d stat_density2d_filled stat_ecdf stat_ellipse stat_function stat_identity stat_manual stat_qq stat_qq_line stat_quantile stat_sf stat_sf_coordinates stat_smooth stat_spoke stat_sum stat_summary stat_summary_2d stat_summary_bin stat_summary_hex stat_summary2d stat_unique stat_ydensity StatAlign StatBin StatBin2d StatBindot StatBinhex StatBoxplot StatConnect StatContour StatContourFilled StatCount StatDensity StatDensity2d StatDensity2dFilled StatEcdf StatEllipse StatFunction StatIdentity StatManual StatQq StatQqLine StatQuantile StatSf StatSfCoordinates StatSmooth StatSum StatSummary StatSummary2d StatSummaryBin StatSummaryHex StatUnique StatYdensity summarise_coord summarise_layers summarise_layout sym syms theme theme_bw theme_classic theme_dark theme_get theme_gray theme_grey theme_light theme_linedraw theme_minimal theme_replace theme_set theme_sub_axis theme_sub_axis_bottom theme_sub_axis_left theme_sub_axis_right theme_sub_axis_top theme_sub_axis_x theme_sub_axis_y theme_sub_legend theme_sub_panel theme_sub_plot theme_sub_strip theme_test theme_update theme_void transform_position translate_shape_string unit update_geom_defaults update_ggplot update_labels update_stat_defaults update_theme vars waiver with_ggplot2_edition wrap_dims xlab xlim ylab ylim zeroGrob data-visualisation visualisation quarto"
  },
  {
    "id": 321,
    "package_name": "broom",
    "title": "Convert Statistical Objects into Tidy Tibbles",
    "description": "Summarizes key information about statistical objects in\ntidy tibbles. This makes it easy to report results, create\nplots and consistently work with large numbers of models at\nonce.  Broom provides three verbs that each provide different\ntypes of information about a model. tidy() summarizes\ninformation about model components such as coefficients of a\nregression. glance() reports information about an entire model,\nsuch as goodness of fit measures like AIC and BIC. augment()\nadds information about individual observations to a dataset,\nsuch as fitted values or influence measures.",
    "version": "1.0.11.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "David Robinson [aut],\nAlex Hayes [aut] (ORCID: <https://orcid.org/0000-0002-4985-5160>),\nSimon Couch [aut] (ORCID: <https://orcid.org/0000-0001-5676-5107>),\nEmil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>),\nIndrajeet Patil [ctb] (ORCID: <https://orcid.org/0000-0003-1995-6531>),\nDerek Chiu [ctb],\nMatthieu Gomez [ctb],\nBoris Demeshev [ctb],\nDieter Menne [ctb],\nBenjamin Nutter [ctb],\nLuke Johnston [ctb],\nBen Bolker [ctb],\nFrancois Briatte [ctb],\nJeffrey Arnold [ctb],\nJonah Gabry [ctb],\nLuciano Selzer [ctb],\nGavin Simpson [ctb],\nJens Preussner [ctb],\nJay Hesselberth [ctb],\nHadley Wickham [ctb],\nMatthew Lincoln [ctb],\nAlessandro Gasparini [ctb],\nLukasz Komsta [ctb],\nFrederick Novometsky [ctb],\nWilson Freitas [ctb],\nMichelle Evans [ctb],\nJason Cory Brunson [ctb],\nSimon Jackson [ctb],\nBen Whalley [ctb],\nKarissa Whiting [ctb],\nYves Rosseel [ctb],\nMichael Kuehn [ctb],\nJorge Cimentada [ctb],\nErle Holgersen [ctb],\nKarl Dunkle Werner [ctb] (ORCID:\n<https://orcid.org/0000-0003-0523-7309>),\nEthan Christensen [ctb],\nSteven Pav [ctb],\nPaul PJ [ctb],\nBen Schneider [ctb],\nPatrick Kennedy [ctb],\nLily Medina [ctb],\nBrian Fannin [ctb],\nJason Muhlenkamp [ctb],\nMatt Lehman [ctb],\nBill Denney [ctb] (ORCID: <https://orcid.org/0000-0002-5759-428X>),\nNic Crane [ctb],\nAndrew Bates [ctb],\nVincent Arel-Bundock [ctb] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>),\nHideaki Hayashi [ctb],\nLuis Tobalina [ctb],\nAnnie Wang [ctb],\nWei Yang Tham [ctb],\nClara Wang [ctb],\nAbby Smith [ctb] (ORCID: <https://orcid.org/0000-0002-3207-0375>),\nJasper Cooper [ctb] (ORCID: <https://orcid.org/0000-0002-8639-3188>),\nE Auden Krauska [ctb] (ORCID: <https://orcid.org/0000-0002-1466-5850>),\nAlex Wang [ctb],\nMalcolm Barrett [ctb] (ORCID: <https://orcid.org/0000-0003-0299-5825>),\nCharles Gray [ctb] (ORCID: <https://orcid.org/0000-0002-9978-011X>),\nJared Wilber [ctb],\nVilmantas Gegzna [ctb] (ORCID: <https://orcid.org/0000-0002-9500-5167>),\nEduard Szoecs [ctb],\nFrederik Aust [ctb] (ORCID: <https://orcid.org/0000-0003-4900-788X>),\nAngus Moore [ctb],\nNick Williams [ctb],\nMarius Barth [ctb] (ORCID: <https://orcid.org/0000-0002-3421-6665>),\nBruna Wundervald [ctb] (ORCID: <https://orcid.org/0000-0001-8163-220X>),\nJoyce Cahoon [ctb] (ORCID: <https://orcid.org/0000-0001-7217-4702>),\nGrant McDermott [ctb] (ORCID: <https://orcid.org/0000-0001-7883-8573>),\nKevin Zarca [ctb],\nShiro Kuriwaki [ctb] (ORCID: <https://orcid.org/0000-0002-5687-2647>),\nLukas Wallrich [ctb] (ORCID: <https://orcid.org/0000-0003-2121-5177>),\nJames Martherus [ctb] (ORCID: <https://orcid.org/0000-0002-8285-3300>),\nChuliang Xiao [ctb] (ORCID: <https://orcid.org/0000-0002-8466-9398>),\nJoseph Larmarange [ctb],\nMax Kuhn [ctb],\nMichal Bojanowski [ctb],\nHakon Malmedal [ctb],\nClara Wang [ctb],\nSergio Oller [ctb],\nLuke Sonnet [ctb],\nJim Hester [ctb],\nBen Schneider [ctb],\nBernie Gray [ctb] (ORCID: <https://orcid.org/0000-0001-9190-6032>),\nMara Averick [ctb],\nAaron Jacobs [ctb],\nAndreas Bender [ctb],\nSven Templer [ctb],\nPaul-Christian Buerkner [ctb],\nMatthew Kay [ctb],\nErwan Le Pennec [ctb],\nJohan Junkka [ctb],\nHao Zhu [ctb],\nBenjamin Soltoff [ctb],\nZoe Wilkinson Saldana [ctb],\nTyler Littlefield [ctb],\nCharles T. Gray [ctb],\nShabbh E. Banks [ctb],\nSerina Robinson [ctb],\nRoger Bivand [ctb],\nRiinu Ots [ctb],\nNicholas Williams [ctb],\nNina Jakobsen [ctb],\nMichael Weylandt [ctb],\nLisa Lendway [ctb],\nKarl Hailperin [ctb],\nJosue Rodriguez [ctb],\nJenny Bryan [ctb],\nChris Jarvis [ctb],\nGreg Macfarlane [ctb],\nBrian Mannakee [ctb],\nDrew Tyre [ctb],\nShreyas Singh [ctb],\nLaurens Geffert [ctb],\nHong Ooi [ctb],\nHenrik Bengtsson [ctb],\nEduard Szocs [ctb],\nDavid Hugh-Jones [ctb],\nMatthieu Stigler [ctb],\nHugo Tavares [ctb] (ORCID: <https://orcid.org/0000-0001-9373-2726>),\nR. Willem Vervoort [ctb],\nBrenton M. Wiernik [ctb],\nJosh Yamamoto [ctb],\nJasme Lee [ctb],\nTaren Sanders [ctb] (ORCID: <https://orcid.org/0000-0002-4504-6008>),\nIlaria Prosdocimi [ctb] (ORCID:\n<https://orcid.org/0000-0001-8565-094X>),\nDaniel D. Sjoberg [ctb] (ORCID:\n<https://orcid.org/0000-0003-0862-2018>),\nAlex Reinhart [ctb] (ORCID: <https://orcid.org/0000-0002-6658-514X>)",
    "url": "https://broom.tidymodels.org/, https://github.com/tidymodels/broom",
    "bug_reports": "https://github.com/tidymodels/broom/issues",
    "repository": "",
    "exports": [
      [
        "augment"
      ],
      [
        "augment_columns"
      ],
      [
        "bootstrap"
      ],
      [
        "confint_tidy"
      ],
      [
        "finish_glance"
      ],
      [
        "fix_data_frame"
      ],
      [
        "glance"
      ],
      [
        "tidy"
      ],
      [
        "tidy_irlba"
      ]
    ],
    "topics": [
      [
        "modeling"
      ],
      [
        "tidy-data"
      ]
    ],
    "score": 21.9333,
    "stars": 1503,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "broom Convert Statistical Objects into Tidy Tibbles Summarizes key information about statistical objects in\ntidy tibbles. This makes it easy to report results, create\nplots and consistently work with large numbers of models at\nonce.  Broom provides three verbs that each provide different\ntypes of information about a model. tidy() summarizes\ninformation about model components such as coefficients of a\nregression. glance() reports information about an entire model,\nsuch as goodness of fit measures like AIC and BIC. augment()\nadds information about individual observations to a dataset,\nsuch as fitted values or influence measures. augment augment_columns bootstrap confint_tidy finish_glance fix_data_frame glance tidy tidy_irlba modeling tidy-data"
  },
  {
    "id": 1121,
    "package_name": "rmarkdown",
    "title": "Dynamic Documents for R",
    "description": "Convert R Markdown documents into a variety of formats.",
    "version": "2.30.3",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "JJ Allaire [aut],\nYihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nChristophe Dervieux [aut] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nJonathan McPherson [aut],\nJavier Luraschi [aut],\nKevin Ushey [aut],\nAron Atkins [aut],\nHadley Wickham [aut],\nJoe Cheng [aut],\nWinston Chang [aut],\nRichard Iannone [aut] (ORCID: <https://orcid.org/0000-0003-3925-190X>),\nAndrew Dunning [ctb] (ORCID: <https://orcid.org/0000-0003-0464-5036>),\nAtsushi Yasumoto [ctb, cph] (ORCID:\n<https://orcid.org/0000-0002-8335-495X>, cph: Number sections Lua\nfilter),\nBarret Schloerke [ctb],\nCarson Sievert [ctb] (ORCID: <https://orcid.org/0000-0002-4958-2844>),\nDevon Ryan [ctb] (ORCID: <https://orcid.org/0000-0002-8549-0971>),\nFrederik Aust [ctb] (ORCID: <https://orcid.org/0000-0003-4900-788X>),\nJeff Allen [ctb],\nJooYoung Seo [ctb] (ORCID: <https://orcid.org/0000-0002-4064-6012>),\nMalcolm Barrett [ctb],\nRob Hyndman [ctb],\nRomain Lesur [ctb],\nRoy Storey [ctb],\nRuben Arslan [ctb],\nSergio Oller [ctb],\nPosit Software, PBC [cph, fnd],\njQuery UI contributors [ctb, cph] (jQuery UI library; authors listed in\ninst/rmd/h/jqueryui/AUTHORS.txt),\nMark Otto [ctb] (Bootstrap library),\nJacob Thornton [ctb] (Bootstrap library),\nBootstrap contributors [ctb] (Bootstrap library),\nTwitter, Inc [cph] (Bootstrap library),\nAlexander Farkas [ctb, cph] (html5shiv library),\nScott Jehl [ctb, cph] (Respond.js library),\nIvan Sagalaev [ctb, cph] (highlight.js library),\nGreg Franko [ctb, cph] (tocify library),\nJohn MacFarlane [ctb, cph] (Pandoc templates),\nGoogle, Inc. [ctb, cph] (ioslides library),\nDave Raggett [ctb] (slidy library),\nW3C [cph] (slidy library),\nDave Gandy [ctb, cph] (Font-Awesome),\nBen Sperry [ctb] (Ionicons),\nDrifty [cph] (Ionicons),\nAidan Lister [ctb, cph] (jQuery StickyTabs),\nBenct Philip Jonsson [ctb, cph] (pagebreak Lua filter),\nAlbert Krewinkel [ctb, cph] (pagebreak Lua filter)",
    "url": "https://github.com/rstudio/rmarkdown,\nhttps://pkgs.rstudio.com/rmarkdown/",
    "bug_reports": "https://github.com/rstudio/rmarkdown/issues",
    "repository": "",
    "exports": [
      [
        "all_output_formats"
      ],
      [
        "available_templates"
      ],
      [
        "beamer_presentation"
      ],
      [
        "clean_site"
      ],
      [
        "context_document"
      ],
      [
        "convert_ipynb"
      ],
      [
        "default_output_format"
      ],
      [
        "default_site_generator"
      ],
      [
        "draft"
      ],
      [
        "find_external_resources"
      ],
      [
        "find_pandoc"
      ],
      [
        "from_rmarkdown"
      ],
      [
        "github_document"
      ],
      [
        "html_dependency_bootstrap"
      ],
      [
        "html_dependency_codefolding_lua"
      ],
      [
        "html_dependency_font_awesome"
      ],
      [
        "html_dependency_highlightjs"
      ],
      [
        "html_dependency_ionicons"
      ],
      [
        "html_dependency_jquery"
      ],
      [
        "html_dependency_jqueryui"
      ],
      [
        "html_dependency_pagedtable"
      ],
      [
        "html_dependency_tabset"
      ],
      [
        "html_dependency_tocify"
      ],
      [
        "html_document"
      ],
      [
        "html_document_base"
      ],
      [
        "html_fragment"
      ],
      [
        "html_notebook"
      ],
      [
        "html_notebook_metadata"
      ],
      [
        "html_notebook_output_code"
      ],
      [
        "html_notebook_output_html"
      ],
      [
        "html_notebook_output_img"
      ],
      [
        "html_notebook_output_png"
      ],
      [
        "html_vignette"
      ],
      [
        "includes"
      ],
      [
        "includes_to_pandoc_args"
      ],
      [
        "ioslides_presentation"
      ],
      [
        "knit_params_ask"
      ],
      [
        "knitr_options"
      ],
      [
        "knitr_options_html"
      ],
      [
        "knitr_options_pdf"
      ],
      [
        "latex_dependency"
      ],
      [
        "latex_dependency_tikz"
      ],
      [
        "latex_document"
      ],
      [
        "latex_fragment"
      ],
      [
        "md_document"
      ],
      [
        "metadata"
      ],
      [
        "navbar_html"
      ],
      [
        "navbar_links_html"
      ],
      [
        "odt_document"
      ],
      [
        "output_format"
      ],
      [
        "output_format_dependency"
      ],
      [
        "output_metadata"
      ],
      [
        "paged_table"
      ],
      [
        "pandoc_available"
      ],
      [
        "pandoc_citeproc_args"
      ],
      [
        "pandoc_citeproc_convert"
      ],
      [
        "pandoc_convert"
      ],
      [
        "pandoc_exec"
      ],
      [
        "pandoc_highlight_args"
      ],
      [
        "pandoc_include_args"
      ],
      [
        "pandoc_latex_engine_args"
      ],
      [
        "pandoc_lua_filter_args"
      ],
      [
        "pandoc_metadata_arg"
      ],
      [
        "pandoc_metadata_file_arg"
      ],
      [
        "pandoc_options"
      ],
      [
        "pandoc_path_arg"
      ],
      [
        "pandoc_self_contained_html"
      ],
      [
        "pandoc_template"
      ],
      [
        "pandoc_toc_args"
      ],
      [
        "pandoc_variable_arg"
      ],
      [
        "pandoc_version"
      ],
      [
        "parse_html_notebook"
      ],
      [
        "pdf_document"
      ],
      [
        "pkg_file_lua"
      ],
      [
        "powerpoint_presentation"
      ],
      [
        "publish_site"
      ],
      [
        "relative_to"
      ],
      [
        "render"
      ],
      [
        "render_delayed"
      ],
      [
        "render_site"
      ],
      [
        "render_supporting_files"
      ],
      [
        "resolve_output_format"
      ],
      [
        "rmarkdown_format"
      ],
      [
        "rtf_document"
      ],
      [
        "run"
      ],
      [
        "shiny_prerendered_chunk"
      ],
      [
        "shiny_prerendered_clean"
      ],
      [
        "shiny_prerendered_server_start_code"
      ],
      [
        "site_config"
      ],
      [
        "site_generator"
      ],
      [
        "site_resources"
      ],
      [
        "slidy_presentation"
      ],
      [
        "word_document"
      ],
      [
        "yaml_front_matter"
      ]
    ],
    "topics": [
      [
        "literate-programming"
      ],
      [
        "markdown"
      ],
      [
        "pandoc"
      ],
      [
        "rmarkdown"
      ]
    ],
    "score": 21.8779,
    "stars": 3000,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "rmarkdown Dynamic Documents for R Convert R Markdown documents into a variety of formats. all_output_formats available_templates beamer_presentation clean_site context_document convert_ipynb default_output_format default_site_generator draft find_external_resources find_pandoc from_rmarkdown github_document html_dependency_bootstrap html_dependency_codefolding_lua html_dependency_font_awesome html_dependency_highlightjs html_dependency_ionicons html_dependency_jquery html_dependency_jqueryui html_dependency_pagedtable html_dependency_tabset html_dependency_tocify html_document html_document_base html_fragment html_notebook html_notebook_metadata html_notebook_output_code html_notebook_output_html html_notebook_output_img html_notebook_output_png html_vignette includes includes_to_pandoc_args ioslides_presentation knit_params_ask knitr_options knitr_options_html knitr_options_pdf latex_dependency latex_dependency_tikz latex_document latex_fragment md_document metadata navbar_html navbar_links_html odt_document output_format output_format_dependency output_metadata paged_table pandoc_available pandoc_citeproc_args pandoc_citeproc_convert pandoc_convert pandoc_exec pandoc_highlight_args pandoc_include_args pandoc_latex_engine_args pandoc_lua_filter_args pandoc_metadata_arg pandoc_metadata_file_arg pandoc_options pandoc_path_arg pandoc_self_contained_html pandoc_template pandoc_toc_args pandoc_variable_arg pandoc_version parse_html_notebook pdf_document pkg_file_lua powerpoint_presentation publish_site relative_to render render_delayed render_site render_supporting_files resolve_output_format rmarkdown_format rtf_document run shiny_prerendered_chunk shiny_prerendered_clean shiny_prerendered_server_start_code site_config site_generator site_resources slidy_presentation word_document yaml_front_matter literate-programming markdown pandoc rmarkdown"
  },
  {
    "id": 35,
    "package_name": "DT",
    "title": "A Wrapper of the JavaScript Library 'DataTables'",
    "description": "Data objects in R can be rendered as HTML tables using the\nJavaScript library 'DataTables' (typically via R Markdown or\nShiny). The 'DataTables' library has been included in this R\npackage. The package name 'DT' is an abbreviation of\n'DataTables'.",
    "version": "0.34.0.9000",
    "maintainer": "Garrick Aden-Buie <garrick@posit.co>",
    "author": "Yihui Xie [aut],\nJoe Cheng [aut],\nXianying Tan [aut],\nGarrick Aden-Buie [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7111-0077>),\nJJ Allaire [ctb],\nMaximilian Girlich [ctb],\nGreg Freedman Ellis [ctb],\nJohannes Rauh [ctb],\nSpryMedia Limited [ctb, cph] (DataTables in htmlwidgets/lib),\nBrian Reavis [ctb, cph] (selectize.js in htmlwidgets/lib),\nLeon Gersen [ctb, cph] (noUiSlider in htmlwidgets/lib),\nBartek Szopka [ctb, cph] (jquery.highlight.js in htmlwidgets/lib),\nAlex Pickering [ctb],\nWilliam Holmes [ctb],\nMikko Marttila [ctb],\nAndres Quintero [ctb],\nSt\u00e9phane Laurent [ctb],\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/DT",
    "bug_reports": "https://github.com/rstudio/DT/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "addRow"
      ],
      [
        "clearSearch"
      ],
      [
        "coerceValue"
      ],
      [
        "colReorder"
      ],
      [
        "datatable"
      ],
      [
        "dataTableAjax"
      ],
      [
        "dataTableOutput"
      ],
      [
        "dataTableProxy"
      ],
      [
        "doColumnSearch"
      ],
      [
        "doGlobalSearch"
      ],
      [
        "DTOutput"
      ],
      [
        "editData"
      ],
      [
        "formatCurrency"
      ],
      [
        "formatDate"
      ],
      [
        "formatPercentage"
      ],
      [
        "formatRound"
      ],
      [
        "formatSignif"
      ],
      [
        "formatString"
      ],
      [
        "formatStyle"
      ],
      [
        "hideCols"
      ],
      [
        "JS"
      ],
      [
        "reloadData"
      ],
      [
        "renderDataTable"
      ],
      [
        "renderDT"
      ],
      [
        "replaceData"
      ],
      [
        "saveWidget"
      ],
      [
        "selectCells"
      ],
      [
        "selectColumns"
      ],
      [
        "selectPage"
      ],
      [
        "selectRows"
      ],
      [
        "showCols"
      ],
      [
        "styleColorBar"
      ],
      [
        "styleEqual"
      ],
      [
        "styleInterval"
      ],
      [
        "styleRow"
      ],
      [
        "styleValue"
      ],
      [
        "tableFooter"
      ],
      [
        "tableHeader"
      ],
      [
        "updateCaption"
      ],
      [
        "updateFilters"
      ],
      [
        "updateSearch"
      ]
    ],
    "topics": [
      [
        "datatables"
      ],
      [
        "htmlwidgets"
      ],
      [
        "javascript"
      ],
      [
        "shiny"
      ]
    ],
    "score": 18.9369,
    "stars": 614,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "DT A Wrapper of the JavaScript Library 'DataTables' Data objects in R can be rendered as HTML tables using the\nJavaScript library 'DataTables' (typically via R Markdown or\nShiny). The 'DataTables' library has been included in this R\npackage. The package name 'DT' is an abbreviation of\n'DataTables'. %>% addRow clearSearch coerceValue colReorder datatable dataTableAjax dataTableOutput dataTableProxy doColumnSearch doGlobalSearch DTOutput editData formatCurrency formatDate formatPercentage formatRound formatSignif formatString formatStyle hideCols JS reloadData renderDataTable renderDT replaceData saveWidget selectCells selectColumns selectPage selectRows showCols styleColorBar styleEqual styleInterval styleRow styleValue tableFooter tableHeader updateCaption updateFilters updateSearch datatables htmlwidgets javascript shiny"
  },
  {
    "id": 978,
    "package_name": "pkgdown",
    "title": "Make Static HTML Documentation for a Package",
    "description": "Generate an attractive and useful website from a source\npackage.  'pkgdown' converts your documentation, vignettes,\n'README', and more to 'HTML' making it easy to share\ninformation about your package online.",
    "version": "2.2.0.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4757-117X>),\nJay Hesselberth [aut] (ORCID: <https://orcid.org/0000-0002-6299-179X>),\nMa\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nOlivier Roy [aut],\nSalim Br\u00fcggemann [aut] (ORCID: <https://orcid.org/0000-0002-5329-5987>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://pkgdown.r-lib.org/, https://github.com/r-lib/pkgdown",
    "bug_reports": "https://github.com/r-lib/pkgdown/issues",
    "repository": "",
    "exports": [
      [
        "as_pkgdown"
      ],
      [
        "build_article"
      ],
      [
        "build_articles"
      ],
      [
        "build_articles_index"
      ],
      [
        "build_favicons"
      ],
      [
        "build_home"
      ],
      [
        "build_home_index"
      ],
      [
        "build_llm_docs"
      ],
      [
        "build_news"
      ],
      [
        "build_redirects"
      ],
      [
        "build_reference"
      ],
      [
        "build_reference_index"
      ],
      [
        "build_search"
      ],
      [
        "build_site"
      ],
      [
        "build_site_github_pages"
      ],
      [
        "build_tutorials"
      ],
      [
        "check_pkgdown"
      ],
      [
        "clean_cache"
      ],
      [
        "clean_site"
      ],
      [
        "data_template"
      ],
      [
        "deploy_site_github"
      ],
      [
        "deploy_to_branch"
      ],
      [
        "fig_settings"
      ],
      [
        "in_pkgdown"
      ],
      [
        "init_site"
      ],
      [
        "pkgdown_print"
      ],
      [
        "pkgdown_sitrep"
      ],
      [
        "preview_site"
      ],
      [
        "rd2html"
      ],
      [
        "render_page"
      ],
      [
        "template_articles"
      ],
      [
        "template_navbar"
      ],
      [
        "template_reference"
      ]
    ],
    "topics": [
      [
        "documentation-tool"
      ],
      [
        "quarto"
      ]
    ],
    "score": 18.4311,
    "stars": 754,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "pkgdown Make Static HTML Documentation for a Package Generate an attractive and useful website from a source\npackage.  'pkgdown' converts your documentation, vignettes,\n'README', and more to 'HTML' making it easy to share\ninformation about your package online. as_pkgdown build_article build_articles build_articles_index build_favicons build_home build_home_index build_llm_docs build_news build_redirects build_reference build_reference_index build_search build_site build_site_github_pages build_tutorials check_pkgdown clean_cache clean_site data_template deploy_site_github deploy_to_branch fig_settings in_pkgdown init_site pkgdown_print pkgdown_sitrep preview_site rd2html render_page template_articles template_navbar template_reference documentation-tool quarto"
  },
  {
    "id": 1135,
    "package_name": "roxygen2",
    "title": "In-Line Documentation for R",
    "description": "Generate your Rd documentation, 'NAMESPACE' file, and\ncollation field using specially formatted comments. Writing\ndocumentation in-line with code makes it easier to keep your\ndocumentation up-to-date as your requirements change.\n'roxygen2' is inspired by the 'Doxygen' system for C++.",
    "version": "7.3.3.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-4757-117X>),\nPeter Danenberg [aut, cph],\nG\u00e1bor Cs\u00e1rdi [aut],\nManuel Eugster [aut, cph],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://roxygen2.r-lib.org/, https://github.com/r-lib/roxygen2",
    "bug_reports": "https://github.com/r-lib/roxygen2/issues",
    "repository": "",
    "exports": [
      [
        "block_get_tag"
      ],
      [
        "block_get_tag_value"
      ],
      [
        "block_get_tags"
      ],
      [
        "block_has_tags"
      ],
      [
        "env_file"
      ],
      [
        "env_package"
      ],
      [
        "escape_examples"
      ],
      [
        "is_s3_generic"
      ],
      [
        "is_s3_method"
      ],
      [
        "load_installed"
      ],
      [
        "load_options"
      ],
      [
        "load_pkgload"
      ],
      [
        "load_source"
      ],
      [
        "namespace_roclet"
      ],
      [
        "object"
      ],
      [
        "object_format"
      ],
      [
        "parse_file"
      ],
      [
        "parse_package"
      ],
      [
        "parse_text"
      ],
      [
        "rd_roclet"
      ],
      [
        "rd_section"
      ],
      [
        "roc_proc_text"
      ],
      [
        "roclet"
      ],
      [
        "roclet_clean"
      ],
      [
        "roclet_find"
      ],
      [
        "roclet_output"
      ],
      [
        "roclet_preprocess"
      ],
      [
        "roclet_process"
      ],
      [
        "roclet_tags"
      ],
      [
        "roxy_block"
      ],
      [
        "roxy_meta_get"
      ],
      [
        "roxy_tag"
      ],
      [
        "roxy_tag_parse"
      ],
      [
        "roxy_tag_rd"
      ],
      [
        "roxy_tag_warning"
      ],
      [
        "roxygenise"
      ],
      [
        "roxygenize"
      ],
      [
        "tag_code"
      ],
      [
        "tag_examples"
      ],
      [
        "tag_inherit"
      ],
      [
        "tag_markdown"
      ],
      [
        "tag_markdown_with_sections"
      ],
      [
        "tag_name"
      ],
      [
        "tag_name_description"
      ],
      [
        "tag_toggle"
      ],
      [
        "tag_two_part"
      ],
      [
        "tag_value"
      ],
      [
        "tag_words"
      ],
      [
        "tag_words_line"
      ],
      [
        "tags_list"
      ],
      [
        "tags_metadata"
      ],
      [
        "update_collate"
      ],
      [
        "vignette_roclet"
      ],
      [
        "warn_roxy_tag"
      ]
    ],
    "topics": [
      [
        "devtools"
      ],
      [
        "documentation"
      ],
      [
        "cpp"
      ]
    ],
    "score": 18.3671,
    "stars": 622,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "roxygen2 In-Line Documentation for R Generate your Rd documentation, 'NAMESPACE' file, and\ncollation field using specially formatted comments. Writing\ndocumentation in-line with code makes it easier to keep your\ndocumentation up-to-date as your requirements change.\n'roxygen2' is inspired by the 'Doxygen' system for C++. block_get_tag block_get_tag_value block_get_tags block_has_tags env_file env_package escape_examples is_s3_generic is_s3_method load_installed load_options load_pkgload load_source namespace_roclet object object_format parse_file parse_package parse_text rd_roclet rd_section roc_proc_text roclet roclet_clean roclet_find roclet_output roclet_preprocess roclet_process roclet_tags roxy_block roxy_meta_get roxy_tag roxy_tag_parse roxy_tag_rd roxy_tag_warning roxygenise roxygenize tag_code tag_examples tag_inherit tag_markdown tag_markdown_with_sections tag_name tag_name_description tag_toggle tag_two_part tag_value tag_words tag_words_line tags_list tags_metadata update_collate vignette_roclet warn_roxy_tag devtools documentation cpp"
  },
  {
    "id": 325,
    "package_name": "bslib",
    "title": "Custom 'Bootstrap' 'Sass' Themes for 'shiny' and 'rmarkdown'",
    "description": "Simplifies custom 'CSS' styling of both 'shiny' and\n'rmarkdown' via 'Bootstrap' 'Sass'. Supports 'Bootstrap' 3, 4\nand 5 as well as their various 'Bootswatch' themes. An\ninteractive widget is also provided for previewing themes in\nreal time.",
    "version": "0.9.0.9002",
    "maintainer": "Carson Sievert <carson@posit.co>",
    "author": "Carson Sievert [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-4958-2844>),\nJoe Cheng [aut],\nGarrick Aden-Buie [aut] (ORCID:\n<https://orcid.org/0000-0002-7111-0077>),\nPosit Software, PBC [cph, fnd],\nBootstrap contributors [ctb] (Bootstrap library),\nTwitter, Inc [cph] (Bootstrap library),\nJavi Aguilar [ctb, cph] (Bootstrap colorpicker library),\nThomas Park [ctb, cph] (Bootswatch library),\nPayPal [ctb, cph] (Bootstrap accessibility plugin)",
    "url": "https://rstudio.github.io/bslib/, https://github.com/rstudio/bslib",
    "bug_reports": "https://github.com/rstudio/bslib/issues",
    "repository": "",
    "exports": [
      [
        "accordion"
      ],
      [
        "accordion_panel"
      ],
      [
        "accordion_panel_close"
      ],
      [
        "accordion_panel_insert"
      ],
      [
        "accordion_panel_open"
      ],
      [
        "accordion_panel_remove"
      ],
      [
        "accordion_panel_set"
      ],
      [
        "accordion_panel_update"
      ],
      [
        "as_fill_carrier"
      ],
      [
        "as_fill_item"
      ],
      [
        "as_fillable_container"
      ],
      [
        "as.card_item"
      ],
      [
        "bind_task_button"
      ],
      [
        "bootstrap"
      ],
      [
        "bootstrap_sass"
      ],
      [
        "bootswatch_themes"
      ],
      [
        "breakpoints"
      ],
      [
        "bs_add_declarations"
      ],
      [
        "bs_add_functions"
      ],
      [
        "bs_add_mixins"
      ],
      [
        "bs_add_rules"
      ],
      [
        "bs_add_variables"
      ],
      [
        "bs_bundle"
      ],
      [
        "bs_current_theme"
      ],
      [
        "bs_dependency"
      ],
      [
        "bs_dependency_defer"
      ],
      [
        "bs_get_contrast"
      ],
      [
        "bs_get_variables"
      ],
      [
        "bs_global_add_rules"
      ],
      [
        "bs_global_add_variables"
      ],
      [
        "bs_global_bundle"
      ],
      [
        "bs_global_clear"
      ],
      [
        "bs_global_get"
      ],
      [
        "bs_global_set"
      ],
      [
        "bs_global_theme"
      ],
      [
        "bs_global_theme_update"
      ],
      [
        "bs_remove"
      ],
      [
        "bs_retrieve"
      ],
      [
        "bs_theme"
      ],
      [
        "bs_theme_accent_colors"
      ],
      [
        "bs_theme_add"
      ],
      [
        "bs_theme_add_variables"
      ],
      [
        "bs_theme_base_colors"
      ],
      [
        "bs_theme_clear"
      ],
      [
        "bs_theme_dependencies"
      ],
      [
        "bs_theme_fonts"
      ],
      [
        "bs_theme_get"
      ],
      [
        "bs_theme_get_variables"
      ],
      [
        "bs_theme_new"
      ],
      [
        "bs_theme_preview"
      ],
      [
        "bs_theme_set"
      ],
      [
        "bs_theme_update"
      ],
      [
        "bs_themer"
      ],
      [
        "builtin_themes"
      ],
      [
        "card"
      ],
      [
        "card_body"
      ],
      [
        "card_body_fill"
      ],
      [
        "card_footer"
      ],
      [
        "card_header"
      ],
      [
        "card_image"
      ],
      [
        "card_title"
      ],
      [
        "css"
      ],
      [
        "font_collection"
      ],
      [
        "font_face"
      ],
      [
        "font_google"
      ],
      [
        "font_link"
      ],
      [
        "hide_toast"
      ],
      [
        "input_dark_mode"
      ],
      [
        "input_submit_textarea"
      ],
      [
        "input_switch"
      ],
      [
        "input_task_button"
      ],
      [
        "is_bs_theme"
      ],
      [
        "is_fill_carrier"
      ],
      [
        "is_fill_item"
      ],
      [
        "is_fillable_container"
      ],
      [
        "is.card_item"
      ],
      [
        "layout_column_wrap"
      ],
      [
        "layout_columns"
      ],
      [
        "layout_sidebar"
      ],
      [
        "nav"
      ],
      [
        "nav_append"
      ],
      [
        "nav_content"
      ],
      [
        "nav_hide"
      ],
      [
        "nav_insert"
      ],
      [
        "nav_item"
      ],
      [
        "nav_menu"
      ],
      [
        "nav_panel"
      ],
      [
        "nav_panel_hidden"
      ],
      [
        "nav_prepend"
      ],
      [
        "nav_remove"
      ],
      [
        "nav_select"
      ],
      [
        "nav_show"
      ],
      [
        "nav_spacer"
      ],
      [
        "navbar_options"
      ],
      [
        "navs_bar"
      ],
      [
        "navs_hidden"
      ],
      [
        "navs_pill"
      ],
      [
        "navs_pill_card"
      ],
      [
        "navs_pill_list"
      ],
      [
        "navs_tab"
      ],
      [
        "navs_tab_card"
      ],
      [
        "navset_bar"
      ],
      [
        "navset_card_pill"
      ],
      [
        "navset_card_tab"
      ],
      [
        "navset_card_underline"
      ],
      [
        "navset_hidden"
      ],
      [
        "navset_pill"
      ],
      [
        "navset_pill_list"
      ],
      [
        "navset_tab"
      ],
      [
        "navset_underline"
      ],
      [
        "page"
      ],
      [
        "page_fill"
      ],
      [
        "page_fillable"
      ],
      [
        "page_fixed"
      ],
      [
        "page_fluid"
      ],
      [
        "page_navbar"
      ],
      [
        "page_sidebar"
      ],
      [
        "popover"
      ],
      [
        "precompiled_css_path"
      ],
      [
        "remove_all_fill"
      ],
      [
        "run_with_themer"
      ],
      [
        "show_toast"
      ],
      [
        "showcase_bottom"
      ],
      [
        "showcase_left_center"
      ],
      [
        "showcase_top_right"
      ],
      [
        "sidebar"
      ],
      [
        "sidebar_toggle"
      ],
      [
        "theme_bootswatch"
      ],
      [
        "theme_version"
      ],
      [
        "toast"
      ],
      [
        "toast_header"
      ],
      [
        "toggle_dark_mode"
      ],
      [
        "toggle_popover"
      ],
      [
        "toggle_sidebar"
      ],
      [
        "toggle_switch"
      ],
      [
        "toggle_tooltip"
      ],
      [
        "tooltip"
      ],
      [
        "update_popover"
      ],
      [
        "update_submit_textarea"
      ],
      [
        "update_switch"
      ],
      [
        "update_task_button"
      ],
      [
        "update_tooltip"
      ],
      [
        "value_box"
      ],
      [
        "value_box_theme"
      ],
      [
        "version_default"
      ],
      [
        "versions"
      ]
    ],
    "topics": [
      [
        "bootstrap"
      ],
      [
        "htmltools"
      ],
      [
        "rmarkdown"
      ],
      [
        "sass"
      ],
      [
        "shiny"
      ]
    ],
    "score": 18.2424,
    "stars": 542,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "bslib Custom 'Bootstrap' 'Sass' Themes for 'shiny' and 'rmarkdown' Simplifies custom 'CSS' styling of both 'shiny' and\n'rmarkdown' via 'Bootstrap' 'Sass'. Supports 'Bootstrap' 3, 4\nand 5 as well as their various 'Bootswatch' themes. An\ninteractive widget is also provided for previewing themes in\nreal time. accordion accordion_panel accordion_panel_close accordion_panel_insert accordion_panel_open accordion_panel_remove accordion_panel_set accordion_panel_update as_fill_carrier as_fill_item as_fillable_container as.card_item bind_task_button bootstrap bootstrap_sass bootswatch_themes breakpoints bs_add_declarations bs_add_functions bs_add_mixins bs_add_rules bs_add_variables bs_bundle bs_current_theme bs_dependency bs_dependency_defer bs_get_contrast bs_get_variables bs_global_add_rules bs_global_add_variables bs_global_bundle bs_global_clear bs_global_get bs_global_set bs_global_theme bs_global_theme_update bs_remove bs_retrieve bs_theme bs_theme_accent_colors bs_theme_add bs_theme_add_variables bs_theme_base_colors bs_theme_clear bs_theme_dependencies bs_theme_fonts bs_theme_get bs_theme_get_variables bs_theme_new bs_theme_preview bs_theme_set bs_theme_update bs_themer builtin_themes card card_body card_body_fill card_footer card_header card_image card_title css font_collection font_face font_google font_link hide_toast input_dark_mode input_submit_textarea input_switch input_task_button is_bs_theme is_fill_carrier is_fill_item is_fillable_container is.card_item layout_column_wrap layout_columns layout_sidebar nav nav_append nav_content nav_hide nav_insert nav_item nav_menu nav_panel nav_panel_hidden nav_prepend nav_remove nav_select nav_show nav_spacer navbar_options navs_bar navs_hidden navs_pill navs_pill_card navs_pill_list navs_tab navs_tab_card navset_bar navset_card_pill navset_card_tab navset_card_underline navset_hidden navset_pill navset_pill_list navset_tab navset_underline page page_fill page_fillable page_fixed page_fluid page_navbar page_sidebar popover precompiled_css_path remove_all_fill run_with_themer show_toast showcase_bottom showcase_left_center showcase_top_right sidebar sidebar_toggle theme_bootswatch theme_version toast toast_header toggle_dark_mode toggle_popover toggle_sidebar toggle_switch toggle_tooltip tooltip update_popover update_submit_textarea update_switch update_task_button update_tooltip value_box value_box_theme version_default versions bootstrap htmltools rmarkdown sass shiny"
  },
  {
    "id": 1361,
    "package_name": "tinytex",
    "title": "Helper Functions to Install and Maintain TeX Live, and Compile\nLaTeX Documents",
    "description": "Helper functions to install and maintain the 'LaTeX'\ndistribution named 'TinyTeX' (<https://yihui.org/tinytex/>), a\nlightweight, cross-platform, portable, and easy-to-maintain\nversion of 'TeX Live'. This package also contains helper\nfunctions to compile 'LaTeX' documents, and install missing\n'LaTeX' packages automatically.",
    "version": "0.58.1",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-0645-5666>),\nPosit Software, PBC [cph, fnd],\nChristophe Dervieux [ctb] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nDevon Ryan [ctb] (ORCID: <https://orcid.org/0000-0002-8549-0971>),\nEthan Heinzen [ctb],\nFernando Cagua [ctb]",
    "url": "https://github.com/rstudio/tinytex",
    "bug_reports": "https://github.com/rstudio/tinytex/issues",
    "repository": "",
    "exports": [
      [
        "check_installed"
      ],
      [
        "copy_tinytex"
      ],
      [
        "install_tinytex"
      ],
      [
        "is_tinytex"
      ],
      [
        "latexmk"
      ],
      [
        "lualatex"
      ],
      [
        "parse_install"
      ],
      [
        "parse_packages"
      ],
      [
        "pdflatex"
      ],
      [
        "r_texmf"
      ],
      [
        "reinstall_tinytex"
      ],
      [
        "tinytex_root"
      ],
      [
        "tl_pkgs"
      ],
      [
        "tl_sizes"
      ],
      [
        "tlmgr"
      ],
      [
        "tlmgr_conf"
      ],
      [
        "tlmgr_install"
      ],
      [
        "tlmgr_path"
      ],
      [
        "tlmgr_remove"
      ],
      [
        "tlmgr_repo"
      ],
      [
        "tlmgr_search"
      ],
      [
        "tlmgr_update"
      ],
      [
        "tlmgr_version"
      ],
      [
        "uninstall_tinytex"
      ],
      [
        "use_tinytex"
      ],
      [
        "xelatex"
      ]
    ],
    "topics": [
      [
        "latex"
      ],
      [
        "texlive"
      ]
    ],
    "score": 17.8154,
    "stars": 1062,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "tinytex Helper Functions to Install and Maintain TeX Live, and Compile\nLaTeX Documents Helper functions to install and maintain the 'LaTeX'\ndistribution named 'TinyTeX' (<https://yihui.org/tinytex/>), a\nlightweight, cross-platform, portable, and easy-to-maintain\nversion of 'TeX Live'. This package also contains helper\nfunctions to compile 'LaTeX' documents, and install missing\n'LaTeX' packages automatically. check_installed copy_tinytex install_tinytex is_tinytex latexmk lualatex parse_install parse_packages pdflatex r_texmf reinstall_tinytex tinytex_root tl_pkgs tl_sizes tlmgr tlmgr_conf tlmgr_install tlmgr_path tlmgr_remove tlmgr_repo tlmgr_search tlmgr_update tlmgr_version uninstall_tinytex use_tinytex xelatex latex texlive"
  },
  {
    "id": 309,
    "package_name": "bookdown",
    "title": "Authoring Books and Technical Documents with R Markdown",
    "description": "Output formats and utilities for authoring books and\ntechnical documents with R Markdown.",
    "version": "0.46.2",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nChristophe Dervieux [ctb] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nJJ Allaire [ctb],\nAlbert Kim [ctb],\nAlessandro Samuel-Rosa [ctb],\nAndrzej Oles [ctb],\nAtsushi Yasumoto [ctb] (ORCID: <https://orcid.org/0000-0002-8335-495X>),\nAust Frederik [ctb] (ORCID: <https://orcid.org/0000-0003-4900-788X>),\nBastiaan Quast [ctb],\nBen Marwick [ctb],\nChester Ismay [ctb],\nClifton Franklund [ctb],\nDaniel Emaasit [ctb],\nDavid Shuman [ctb],\nDean Attali [ctb],\nDrew Tyre [ctb],\nEllis Valentiner [ctb],\nFrans van Dunne [ctb],\nHadley Wickham [ctb],\nJeff Allen [ctb],\nJennifer Bryan [ctb],\nJonathan McPhers [ctb],\nJooYoung Seo [ctb] (ORCID: <https://orcid.org/0000-0002-4064-6012>),\nJoyce Robbins [ctb],\nJunwen Huang [ctb],\nKevin Cheung [ctb],\nKevin Ushey [ctb],\nKim Seonghyun [ctb],\nKirill Muller [ctb],\nLuciano Selzer [ctb],\nMatthew Lincoln [ctb],\nMaximilian Held [ctb],\nMichael Sachs [ctb],\nMichal Bojanowski [ctb],\nNathan Werth [ctb],\nNoam Ross [ctb],\nPeter Hickey [ctb],\nPedro Rafael D. Marinho [ctb] (ORCID:\n<https://orcid.org/0000-0003-1591-8300>),\nRomain Lesur [ctb] (ORCID: <https://orcid.org/0000-0002-0721-5595>),\nSahir Bhatnagar [ctb],\nShir Dekel [ctb] (ORCID: <https://orcid.org/0000-0003-1773-2446>),\nSteve Simpson [ctb],\nThierry Onkelinx [ctb] (ORCID: <https://orcid.org/0000-0001-8804-4216>),\nVincent Fulco [ctb],\nYixuan Qiu [ctb],\nZhuoer Dong [ctb],\nPosit Software, PBC [cph, fnd],\nBartek Szopka [ctb] (The jQuery Highlight plugin),\nZeno Rocha [cph] (clipboard.js library),\nMathQuill contributors [ctb] (The MathQuill library; authors listed in\ninst/resources/AUTHORS),\nFriendCode Inc [cph, ctb] (The gitbook style, with modifications)",
    "url": "https://github.com/rstudio/bookdown,\nhttps://pkgs.rstudio.com/bookdown/",
    "bug_reports": "https://github.com/rstudio/bookdown/issues",
    "repository": "",
    "exports": [
      [
        "beamer_presentation2"
      ],
      [
        "bookdown_site"
      ],
      [
        "bs4_book"
      ],
      [
        "bs4_book_theme"
      ],
      [
        "build_chapter"
      ],
      [
        "calibre"
      ],
      [
        "clean_book"
      ],
      [
        "context_document2"
      ],
      [
        "create_bs4_book"
      ],
      [
        "create_gitbook"
      ],
      [
        "epub_book"
      ],
      [
        "fence_theorems"
      ],
      [
        "gitbook"
      ],
      [
        "github_document2"
      ],
      [
        "html_book"
      ],
      [
        "html_chapters"
      ],
      [
        "html_document2"
      ],
      [
        "html_fragment2"
      ],
      [
        "html_notebook2"
      ],
      [
        "html_vignette2"
      ],
      [
        "ioslides_presentation2"
      ],
      [
        "markdown_document2"
      ],
      [
        "odt_document2"
      ],
      [
        "pdf_book"
      ],
      [
        "pdf_document2"
      ],
      [
        "powerpoint_presentation2"
      ],
      [
        "preview_chapter"
      ],
      [
        "publish_book"
      ],
      [
        "render_book"
      ],
      [
        "resolve_refs_html"
      ],
      [
        "rtf_document2"
      ],
      [
        "serve_book"
      ],
      [
        "slidy_presentation2"
      ],
      [
        "tufte_book2"
      ],
      [
        "tufte_handout2"
      ],
      [
        "tufte_html_book"
      ],
      [
        "tufte_html2"
      ],
      [
        "word_document2"
      ]
    ],
    "topics": [
      [
        "book"
      ],
      [
        "bookdown"
      ],
      [
        "epub"
      ],
      [
        "gitbook"
      ],
      [
        "html"
      ],
      [
        "latex"
      ],
      [
        "rmarkdown"
      ]
    ],
    "score": 17.808,
    "stars": 3984,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "bookdown Authoring Books and Technical Documents with R Markdown Output formats and utilities for authoring books and\ntechnical documents with R Markdown. beamer_presentation2 bookdown_site bs4_book bs4_book_theme build_chapter calibre clean_book context_document2 create_bs4_book create_gitbook epub_book fence_theorems gitbook github_document2 html_book html_chapters html_document2 html_fragment2 html_notebook2 html_vignette2 ioslides_presentation2 markdown_document2 odt_document2 pdf_book pdf_document2 powerpoint_presentation2 preview_chapter publish_book render_book resolve_refs_html rtf_document2 serve_book slidy_presentation2 tufte_book2 tufte_handout2 tufte_html_book tufte_html2 word_document2 book bookdown epub gitbook html latex rmarkdown"
  },
  {
    "id": 747,
    "package_name": "leaflet",
    "title": "Create Interactive Web Maps with the JavaScript 'Leaflet'\nLibrary",
    "description": "Create and customize interactive maps using the 'Leaflet'\nJavaScript library and the 'htmlwidgets' package. These maps\ncan be used directly from the R console, from 'RStudio', in\nShiny applications and R Markdown documents.",
    "version": "2.2.3.9000",
    "maintainer": "Garrick Aden-Buie <garrick@posit.co>",
    "author": "Joe Cheng [aut],\nBarret Schloerke [aut] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\nBhaskar Karambelkar [aut],\nYihui Xie [aut],\nGarrick Aden-Buie [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7111-0077>),\nHadley Wickham [ctb],\nKenton Russell [ctb],\nKent Johnson [ctb],\nVladimir Agafonkin [ctb, cph] (Leaflet library),\nCloudMade [cph] (Leaflet library),\nLeaflet contributors [ctb] (Leaflet library),\nBrandon Copeland [ctb, cph] (leaflet-measure plugin),\nJoerg Dietrich [ctb, cph] (Leaflet.Terminator plugin),\nBenjamin Becquet [ctb, cph] (Leaflet.MagnifyingGlass plugin),\nNorkart AS [ctb, cph] (Leaflet.MiniMap plugin),\nL. Voogdt [ctb, cph] (Leaflet.awesome-markers plugin),\nDaniel Montague [ctb, cph] (Leaflet.EasyButton plugin),\nKartena AB [ctb, cph] (Proj4Leaflet plugin),\nRobert Kajic [ctb, cph] (leaflet-locationfilter plugin),\nMapbox [ctb, cph] (leaflet-omnivore plugin),\nMichael Bostock [ctb, cph] (topojson),\nPosit Software, PBC [cph, fnd]",
    "url": "https://rstudio.github.io/leaflet/,\nhttps://github.com/rstudio/leaflet",
    "bug_reports": "https://github.com/rstudio/leaflet/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "addAwesomeMarkers"
      ],
      [
        "addCircleMarkers"
      ],
      [
        "addCircles"
      ],
      [
        "addControl"
      ],
      [
        "addEasyButton"
      ],
      [
        "addEasyButtonBar"
      ],
      [
        "addGeoJSON"
      ],
      [
        "addGraticule"
      ],
      [
        "addLabelOnlyMarkers"
      ],
      [
        "addLayersControl"
      ],
      [
        "addLegend"
      ],
      [
        "addMapPane"
      ],
      [
        "addMarkers"
      ],
      [
        "addMeasure"
      ],
      [
        "addMiniMap"
      ],
      [
        "addPolygons"
      ],
      [
        "addPolylines"
      ],
      [
        "addPopups"
      ],
      [
        "addProviderTiles"
      ],
      [
        "addRasterImage"
      ],
      [
        "addRasterLegend"
      ],
      [
        "addRectangles"
      ],
      [
        "addScaleBar"
      ],
      [
        "addSimpleGraticule"
      ],
      [
        "addTerminator"
      ],
      [
        "addTiles"
      ],
      [
        "addTopoJSON"
      ],
      [
        "addWMSTiles"
      ],
      [
        "awesomeIconList"
      ],
      [
        "awesomeIcons"
      ],
      [
        "clearBounds"
      ],
      [
        "clearControls"
      ],
      [
        "clearGeoJSON"
      ],
      [
        "clearGroup"
      ],
      [
        "clearImages"
      ],
      [
        "clearMarkerClusters"
      ],
      [
        "clearMarkers"
      ],
      [
        "clearPopups"
      ],
      [
        "clearShapes"
      ],
      [
        "clearTiles"
      ],
      [
        "clearTopoJSON"
      ],
      [
        "colorBin"
      ],
      [
        "colorFactor"
      ],
      [
        "colorNumeric"
      ],
      [
        "colorQuantile"
      ],
      [
        "createLeafletMap"
      ],
      [
        "derivePoints"
      ],
      [
        "derivePolygons"
      ],
      [
        "dispatch"
      ],
      [
        "easyButton"
      ],
      [
        "easyButtonState"
      ],
      [
        "evalFormula"
      ],
      [
        "expandLimits"
      ],
      [
        "expandLimitsBbox"
      ],
      [
        "filterNULL"
      ],
      [
        "fitBounds"
      ],
      [
        "flyTo"
      ],
      [
        "flyToBounds"
      ],
      [
        "getMapData"
      ],
      [
        "gridOptions"
      ],
      [
        "groupOptions"
      ],
      [
        "hideGroup"
      ],
      [
        "highlightOptions"
      ],
      [
        "iconList"
      ],
      [
        "icons"
      ],
      [
        "invokeMethod"
      ],
      [
        "JS"
      ],
      [
        "labelFormat"
      ],
      [
        "labelOptions"
      ],
      [
        "layersControlOptions"
      ],
      [
        "leaflet"
      ],
      [
        "leafletCRS"
      ],
      [
        "leafletDependencies"
      ],
      [
        "leafletMap"
      ],
      [
        "leafletOptions"
      ],
      [
        "leafletOutput"
      ],
      [
        "leafletProxy"
      ],
      [
        "leafletSizingPolicy"
      ],
      [
        "makeAwesomeIcon"
      ],
      [
        "makeIcon"
      ],
      [
        "mapOptions"
      ],
      [
        "markerClusterOptions"
      ],
      [
        "markerOptions"
      ],
      [
        "pathOptions"
      ],
      [
        "popupOptions"
      ],
      [
        "previewColors"
      ],
      [
        "projectRasterForLeaflet"
      ],
      [
        "providers"
      ],
      [
        "providers.details"
      ],
      [
        "providerTileOptions"
      ],
      [
        "removeControl"
      ],
      [
        "removeGeoJSON"
      ],
      [
        "removeImage"
      ],
      [
        "removeLayersControl"
      ],
      [
        "removeMarker"
      ],
      [
        "removeMarkerCluster"
      ],
      [
        "removeMarkerFromCluster"
      ],
      [
        "removeMeasure"
      ],
      [
        "removePopup"
      ],
      [
        "removeScaleBar"
      ],
      [
        "removeShape"
      ],
      [
        "removeTiles"
      ],
      [
        "removeTopoJSON"
      ],
      [
        "renderLeaflet"
      ],
      [
        "safeLabel"
      ],
      [
        "scaleBarOptions"
      ],
      [
        "setMaxBounds"
      ],
      [
        "setView"
      ],
      [
        "showGroup"
      ],
      [
        "tileOptions"
      ],
      [
        "validateCoords"
      ],
      [
        "WMSTileOptions"
      ]
    ],
    "topics": [
      [
        "gis"
      ],
      [
        "leaflet-map"
      ],
      [
        "spatial"
      ]
    ],
    "score": 17.1575,
    "stars": 837,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "leaflet Create Interactive Web Maps with the JavaScript 'Leaflet'\nLibrary Create and customize interactive maps using the 'Leaflet'\nJavaScript library and the 'htmlwidgets' package. These maps\ncan be used directly from the R console, from 'RStudio', in\nShiny applications and R Markdown documents. %>% addAwesomeMarkers addCircleMarkers addCircles addControl addEasyButton addEasyButtonBar addGeoJSON addGraticule addLabelOnlyMarkers addLayersControl addLegend addMapPane addMarkers addMeasure addMiniMap addPolygons addPolylines addPopups addProviderTiles addRasterImage addRasterLegend addRectangles addScaleBar addSimpleGraticule addTerminator addTiles addTopoJSON addWMSTiles awesomeIconList awesomeIcons clearBounds clearControls clearGeoJSON clearGroup clearImages clearMarkerClusters clearMarkers clearPopups clearShapes clearTiles clearTopoJSON colorBin colorFactor colorNumeric colorQuantile createLeafletMap derivePoints derivePolygons dispatch easyButton easyButtonState evalFormula expandLimits expandLimitsBbox filterNULL fitBounds flyTo flyToBounds getMapData gridOptions groupOptions hideGroup highlightOptions iconList icons invokeMethod JS labelFormat labelOptions layersControlOptions leaflet leafletCRS leafletDependencies leafletMap leafletOptions leafletOutput leafletProxy leafletSizingPolicy makeAwesomeIcon makeIcon mapOptions markerClusterOptions markerOptions pathOptions popupOptions previewColors projectRasterForLeaflet providers providers.details providerTileOptions removeControl removeGeoJSON removeImage removeLayersControl removeMarker removeMarkerCluster removeMarkerFromCluster removeMeasure removePopup removeScaleBar removeShape removeTiles removeTopoJSON renderLeaflet safeLabel scaleBarOptions setMaxBounds setView showGroup tileOptions validateCoords WMSTileOptions gis leaflet-map spatial"
  },
  {
    "id": 200,
    "package_name": "StanHeaders",
    "title": "C++ Header Files for Stan",
    "description": "The C++ header files of the Stan project are provided by\nthis package, but it contains little R code or documentation.\nThe main reference is the vignette. There is a shared object\ncontaining part of the 'CVODES' library, but its functionality\nis not accessible from R. 'StanHeaders' is primarily useful for\ndevelopers who want to utilize the 'LinkingTo' directive of\ntheir package's DESCRIPTION file to build on the Stan library\nwithout incurring unnecessary dependencies. The Stan project\ndevelops a probabilistic programming language that implements\nfull or approximate Bayesian statistical inference via Markov\nChain Monte Carlo or 'variational' methods and implements\n(optionally penalized) maximum likelihood estimation via\noptimization. The Stan library includes an advanced automatic\ndifferentiation scheme, 'templated' statistical and linear\nalgebra functions that can handle the automatically\n'differentiable' scalar types (and doubles, 'ints', etc.), and\na parser for the Stan language. The 'rstan' package provides\nuser-facing R functions to parse, compile, test, estimate, and\nanalyze Stan models.",
    "version": "2.36.0.9000",
    "maintainer": "Ben Goodrich <benjamin.goodrich@columbia.edu>",
    "author": "Ben Goodrich [cre, aut],\nJoshua Pritikin [ctb],\nAndrew Gelman [aut],\nBob Carpenter [aut],\nMatt Hoffman [aut],\nDaniel Lee [aut],\nMichael Betancourt [aut],\nMarcus Brubaker [aut],\nJiqiang Guo [aut],\nPeter Li [aut],\nAllen Riddell [aut],\nMarco Inacio [aut],\nMitzi Morris [aut],\nJeffrey Arnold [aut],\nRob Goedman [aut],\nBrian Lau [aut],\nRob Trangucci [aut],\nJonah Gabry [aut],\nAlp Kucukelbir [aut],\nRobert Grant [aut],\nDustin Tran [aut],\nMichael Malecki [aut],\nYuanjun Gao [aut],\nHamada S. Badr [aut] (ORCID: <https://orcid.org/0000-0002-9808-2344>),\nTrustees of Columbia University [cph],\nLawrence Livermore National Security [cph] (CVODES),\nThe Regents of the University of California [cph] (CVODES),\nSouthern Methodist University [cph] (CVODES)",
    "url": "https://mc-stan.org/",
    "bug_reports": "https://github.com/stan-dev/rstan/issues",
    "repository": "",
    "exports": [
      [
        "stanFunction"
      ]
    ],
    "topics": [
      [
        "bayesian-data-analysis"
      ],
      [
        "bayesian-inference"
      ],
      [
        "bayesian-statistics"
      ],
      [
        "mcmc"
      ],
      [
        "stan"
      ]
    ],
    "score": 17.123,
    "stars": 1068,
    "primary_category": "statistics",
    "source_universe": "stan-dev",
    "search_text": "StanHeaders C++ Header Files for Stan The C++ header files of the Stan project are provided by\nthis package, but it contains little R code or documentation.\nThe main reference is the vignette. There is a shared object\ncontaining part of the 'CVODES' library, but its functionality\nis not accessible from R. 'StanHeaders' is primarily useful for\ndevelopers who want to utilize the 'LinkingTo' directive of\ntheir package's DESCRIPTION file to build on the Stan library\nwithout incurring unnecessary dependencies. The Stan project\ndevelops a probabilistic programming language that implements\nfull or approximate Bayesian statistical inference via Markov\nChain Monte Carlo or 'variational' methods and implements\n(optionally penalized) maximum likelihood estimation via\noptimization. The Stan library includes an advanced automatic\ndifferentiation scheme, 'templated' statistical and linear\nalgebra functions that can handle the automatically\n'differentiable' scalar types (and doubles, 'ints', etc.), and\na parser for the Stan language. The 'rstan' package provides\nuser-facing R functions to parse, compile, test, estimate, and\nanalyze Stan models. stanFunction bayesian-data-analysis bayesian-inference bayesian-statistics mcmc stan"
  },
  {
    "id": 757,
    "package_name": "lifecycle",
    "title": "Manage the Life Cycle of your Package Functions",
    "description": "Manage the life cycle of your exported functions with\nshared conventions, documentation badges, and user-friendly\ndeprecation warnings.",
    "version": "1.0.4.9000",
    "maintainer": "Lionel Henry <lionel@posit.co>",
    "author": "Lionel Henry [aut, cre],\nHadley Wickham [aut] (ORCID: <https://orcid.org/0000-0003-4757-117X>),\nPosit Software, PBC [cph, fnd]",
    "url": "https://lifecycle.r-lib.org/, https://github.com/r-lib/lifecycle",
    "bug_reports": "https://github.com/r-lib/lifecycle/issues",
    "repository": "",
    "exports": [
      [
        "badge"
      ],
      [
        "deprecate_soft"
      ],
      [
        "deprecate_stop"
      ],
      [
        "deprecate_warn"
      ],
      [
        "deprecated"
      ],
      [
        "expect_defunct"
      ],
      [
        "expect_deprecated"
      ],
      [
        "is_present"
      ],
      [
        "last_lifecycle_warnings"
      ],
      [
        "lifecycle_linter"
      ],
      [
        "lint_lifecycle"
      ],
      [
        "lint_tidyverse_lifecycle"
      ],
      [
        "pkg_lifecycle_statuses"
      ],
      [
        "signal_experimental"
      ],
      [
        "signal_stage"
      ],
      [
        "signal_superseded"
      ]
    ],
    "topics": [],
    "score": 17.0017,
    "stars": 92,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "lifecycle Manage the Life Cycle of your Package Functions Manage the life cycle of your exported functions with\nshared conventions, documentation badges, and user-friendly\ndeprecation warnings. badge deprecate_soft deprecate_stop deprecate_warn deprecated expect_defunct expect_deprecated is_present last_lifecycle_warnings lifecycle_linter lint_lifecycle lint_tidyverse_lifecycle pkg_lifecycle_statuses signal_experimental signal_stage signal_superseded "
  },
  {
    "id": 419,
    "package_name": "covr",
    "title": "Test Coverage for Packages",
    "description": "Track and report code coverage for your package and\n(optionally) upload the results to a coverage service like\n'Codecov' <https://about.codecov.io> or 'Coveralls'\n<https://coveralls.io>. Code coverage is a measure of the\namount of code being exercised by a set of tests. It is an\nindirect measure of test quality and completeness. This package\nis compatible with any testing methodology or framework and\ntracks coverage of both R code and compiled C/C++/FORTRAN code.",
    "version": "3.6.5.9000",
    "maintainer": "Jim Hester <james.f.hester@gmail.com>",
    "author": "Jim Hester [aut, cre],\nWillem Ligtenberg [ctb],\nKirill M\u00fcller [ctb],\nHenrik Bengtsson [ctb],\nSteve Peak [ctb],\nKirill Sevastyanenko [ctb],\nJon Clayden [ctb],\nRobert Flight [ctb],\nEric Brown [ctb],\nBrodie Gaslam [ctb],\nWill Beasley [ctb],\nRobert Krzyzanowski [ctb],\nMarkus Wamser [ctb],\nKarl Forner [ctb],\nGergely Dar\u00f3czi [ctb],\nJouni Helske [ctb],\nKun Ren [ctb],\nJeroen Ooms [ctb],\nKen Williams [ctb],\nChris Campbell [ctb],\nDavid Hugh-Jones [ctb],\nQin Wang [ctb],\nDoug Kelkhoff [ctb],\nIvan Sagalaev [ctb, cph] (highlight.js library),\nMark Otto [ctb] (Bootstrap library),\nJacob Thornton [ctb] (Bootstrap library),\nBootstrap contributors [ctb] (Bootstrap library),\nTwitter, Inc [cph] (Bootstrap library)",
    "url": "https://covr.r-lib.org, https://github.com/r-lib/covr",
    "bug_reports": "https://github.com/r-lib/covr/issues",
    "repository": "",
    "exports": [
      [
        "azure"
      ],
      [
        "code_coverage"
      ],
      [
        "codecov"
      ],
      [
        "coverage_to_list"
      ],
      [
        "coveralls"
      ],
      [
        "display_name"
      ],
      [
        "environment_coverage"
      ],
      [
        "file_coverage"
      ],
      [
        "file_report"
      ],
      [
        "function_coverage"
      ],
      [
        "gitlab"
      ],
      [
        "in_covr"
      ],
      [
        "package_coverage"
      ],
      [
        "percent_coverage"
      ],
      [
        "report"
      ],
      [
        "tally_coverage"
      ],
      [
        "to_cobertura"
      ],
      [
        "to_sonarqube"
      ],
      [
        "value"
      ],
      [
        "zero_coverage"
      ]
    ],
    "topics": [
      [
        "codecov"
      ],
      [
        "coverage"
      ],
      [
        "coverage-report"
      ],
      [
        "travis-ci"
      ]
    ],
    "score": 15.8274,
    "stars": 345,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "covr Test Coverage for Packages Track and report code coverage for your package and\n(optionally) upload the results to a coverage service like\n'Codecov' <https://about.codecov.io> or 'Coveralls'\n<https://coveralls.io>. Code coverage is a measure of the\namount of code being exercised by a set of tests. It is an\nindirect measure of test quality and completeness. This package\nis compatible with any testing methodology or framework and\ntracks coverage of both R code and compiled C/C++/FORTRAN code. azure code_coverage codecov coverage_to_list coveralls display_name environment_coverage file_coverage file_report function_coverage gitlab in_covr package_coverage percent_coverage report tally_coverage to_cobertura to_sonarqube value zero_coverage codecov coverage coverage-report travis-ci"
  },
  {
    "id": 583,
    "package_name": "fontawesome",
    "title": "Easily Work with 'Font Awesome' Icons",
    "description": "Easily and flexibly insert 'Font Awesome' icons into 'R\nMarkdown' documents and 'Shiny' apps. These icons can be\ninserted into HTML content through inline 'SVG' tags or 'i'\ntags. There is also a utility function for exporting 'Font\nAwesome' icons as 'PNG' images for those situations where\nraster graphics are needed.",
    "version": "0.5.3.9000",
    "maintainer": "Richard Iannone <rich@posit.co>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3925-190X>),\nChristophe Dervieux [ctb] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nWinston Chang [ctb],\nDave Gandy [ctb, cph] (Font-Awesome font),\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/fontawesome,\nhttps://rstudio.github.io/fontawesome/",
    "bug_reports": "https://github.com/rstudio/fontawesome/issues",
    "repository": "",
    "exports": [
      [
        "fa"
      ],
      [
        "fa_html_dependency"
      ],
      [
        "fa_i"
      ],
      [
        "fa_metadata"
      ],
      [
        "fa_png"
      ]
    ],
    "topics": [
      [
        "font-awesome"
      ],
      [
        "svg-icons"
      ]
    ],
    "score": 15.7921,
    "stars": 300,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "fontawesome Easily Work with 'Font Awesome' Icons Easily and flexibly insert 'Font Awesome' icons into 'R\nMarkdown' documents and 'Shiny' apps. These icons can be\ninserted into HTML content through inline 'SVG' tags or 'i'\ntags. There is also a utility function for exporting 'Font\nAwesome' icons as 'PNG' images for those situations where\nraster graphics are needed. fa fa_html_dependency fa_i fa_metadata fa_png font-awesome svg-icons"
  },
  {
    "id": 1090,
    "package_name": "reprex",
    "title": "Prepare Reproducible Example Code via the Clipboard",
    "description": "Convenience wrapper that uses the 'rmarkdown' package to\nrender small snippets of code to target formats that include\nboth code and output.  The goal is to encourage the sharing of\nsmall, reproducible, and runnable examples on code-oriented\nwebsites, such as <https://stackoverflow.com> and\n<https://github.com>, or in email. The user's clipboard is the\ndefault source of input code and the default target for\nrendered output. 'reprex' also extracts clean, runnable R code\nfrom various common formats, such as copy/paste from an R\nsession.",
    "version": "2.1.1.9000",
    "maintainer": "Jennifer Bryan <jenny@posit.co>",
    "author": "Jennifer Bryan [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6983-2759>),\nJim Hester [aut] (ORCID: <https://orcid.org/0000-0002-2739-7082>),\nDavid Robinson [aut],\nHadley Wickham [aut] (ORCID: <https://orcid.org/0000-0003-4757-117X>),\nChristophe Dervieux [aut] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nPosit Software, PBC [cph, fnd]",
    "url": "https://reprex.tidyverse.org, https://github.com/tidyverse/reprex",
    "bug_reports": "https://github.com/tidyverse/reprex/issues",
    "repository": "",
    "exports": [
      [
        "reprex"
      ],
      [
        "reprex_addin"
      ],
      [
        "reprex_clean"
      ],
      [
        "reprex_document"
      ],
      [
        "reprex_html"
      ],
      [
        "reprex_invert"
      ],
      [
        "reprex_locale"
      ],
      [
        "reprex_r"
      ],
      [
        "reprex_render"
      ],
      [
        "reprex_rescue"
      ],
      [
        "reprex_rtf"
      ],
      [
        "reprex_selection"
      ],
      [
        "reprex_slack"
      ]
    ],
    "topics": [
      [
        "github"
      ],
      [
        "reproducibility"
      ],
      [
        "rmarkdown"
      ],
      [
        "stackoverflow"
      ]
    ],
    "score": 15.4592,
    "stars": 747,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "reprex Prepare Reproducible Example Code via the Clipboard Convenience wrapper that uses the 'rmarkdown' package to\nrender small snippets of code to target formats that include\nboth code and output.  The goal is to encourage the sharing of\nsmall, reproducible, and runnable examples on code-oriented\nwebsites, such as <https://stackoverflow.com> and\n<https://github.com>, or in email. The user's clipboard is the\ndefault source of input code and the default target for\nrendered output. 'reprex' also extracts clean, runnable R code\nfrom various common formats, such as copy/paste from an R\nsession. reprex reprex_addin reprex_clean reprex_document reprex_html reprex_invert reprex_locale reprex_r reprex_render reprex_rescue reprex_rtf reprex_selection reprex_slack github reproducibility rmarkdown stackoverflow"
  },
  {
    "id": 1150,
    "package_name": "rsconnect",
    "title": "Deploy Docs, Apps, and APIs to 'Posit Connect', 'shinyapps.io',\nand 'RPubs'",
    "description": "Programmatic deployment interface for 'RPubs',\n'shinyapps.io', and 'Posit Connect'. Supported content types\ninclude R Markdown documents, Shiny applications, Plumber APIs,\nplots, and static web content.",
    "version": "1.7.0.9000",
    "maintainer": "Aron Atkins <aron@posit.co>",
    "author": "Aron Atkins [aut, cre],\nToph Allen [aut],\nHadley Wickham [aut],\nJonathan McPherson [aut],\nJJ Allaire [aut],\nPosit Software, PBC [cph, fnd]",
    "url": "https://rstudio.github.io/rsconnect/,\nhttps://github.com/rstudio/rsconnect",
    "bug_reports": "https://github.com/rstudio/rsconnect/issues",
    "repository": "",
    "exports": [
      [
        "accountInfo"
      ],
      [
        "accounts"
      ],
      [
        "accountUsage"
      ],
      [
        "addAuthorizedUser"
      ],
      [
        "addLinter"
      ],
      [
        "addServer"
      ],
      [
        "addServerCertificate"
      ],
      [
        "appDependencies"
      ],
      [
        "applications"
      ],
      [
        "configureApp"
      ],
      [
        "connectApiUser"
      ],
      [
        "connectCloudUser"
      ],
      [
        "connectSPCSUser"
      ],
      [
        "connectUser"
      ],
      [
        "deployAPI"
      ],
      [
        "deployApp"
      ],
      [
        "deployDoc"
      ],
      [
        "deployments"
      ],
      [
        "deploySite"
      ],
      [
        "deployTFModel"
      ],
      [
        "forgetDeployment"
      ],
      [
        "generateAppName"
      ],
      [
        "getLogs"
      ],
      [
        "lint"
      ],
      [
        "linter"
      ],
      [
        "listAccountEnvVars"
      ],
      [
        "listBundleFiles"
      ],
      [
        "listDeploymentFiles"
      ],
      [
        "purgeApp"
      ],
      [
        "removeAccount"
      ],
      [
        "removeAuthorizedUser"
      ],
      [
        "removeServer"
      ],
      [
        "resendInvitation"
      ],
      [
        "restartApp"
      ],
      [
        "rpubsUpload"
      ],
      [
        "serverInfo"
      ],
      [
        "servers"
      ],
      [
        "setAccountInfo"
      ],
      [
        "setProperty"
      ],
      [
        "showInvited"
      ],
      [
        "showLogs"
      ],
      [
        "showMetrics"
      ],
      [
        "showProperties"
      ],
      [
        "showUsage"
      ],
      [
        "showUsers"
      ],
      [
        "syncAppMetadata"
      ],
      [
        "taskLog"
      ],
      [
        "tasks"
      ],
      [
        "terminateApp"
      ],
      [
        "unsetProperty"
      ],
      [
        "updateAccountEnvVars"
      ],
      [
        "writeManifest"
      ]
    ],
    "topics": [
      [
        "posit-connect"
      ]
    ],
    "score": 15.2035,
    "stars": 144,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "rsconnect Deploy Docs, Apps, and APIs to 'Posit Connect', 'shinyapps.io',\nand 'RPubs' Programmatic deployment interface for 'RPubs',\n'shinyapps.io', and 'Posit Connect'. Supported content types\ninclude R Markdown documents, Shiny applications, Plumber APIs,\nplots, and static web content. accountInfo accounts accountUsage addAuthorizedUser addLinter addServer addServerCertificate appDependencies applications configureApp connectApiUser connectCloudUser connectSPCSUser connectUser deployAPI deployApp deployDoc deployments deploySite deployTFModel forgetDeployment generateAppName getLogs lint linter listAccountEnvVars listBundleFiles listDeploymentFiles purgeApp removeAccount removeAuthorizedUser removeServer resendInvitation restartApp rpubsUpload serverInfo servers setAccountInfo setProperty showInvited showLogs showMetrics showProperties showUsage showUsers syncAppMetadata taskLog tasks terminateApp unsetProperty updateAccountEnvVars writeManifest posit-connect"
  },
  {
    "id": 991,
    "package_name": "plumber",
    "title": "An API Generator for R",
    "description": "Gives the ability to automatically generate and serve an\nHTTP API from R functions using the annotations in the R\ndocumentation around your functions.",
    "version": "1.3.0.9000",
    "maintainer": "Barret Schloerke <barret@posit.co>",
    "author": "Barret Schloerke [cre, aut] (ORCID:\n<https://orcid.org/0000-0001-9986-114X>),\nJeff Allen [aut, ccp],\nBruno Tremblay [ctb],\nFrans van Dunn\u00e9 [ctb],\nSebastiaan Vandewoude [ctb],\nPosit Software, PBC [cph, fnd]",
    "url": "https://www.rplumber.io, https://github.com/rstudio/plumber",
    "bug_reports": "https://github.com/rstudio/plumber/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "addSerializer"
      ],
      [
        "as_attachment"
      ],
      [
        "available_apis"
      ],
      [
        "do_configure_https"
      ],
      [
        "do_deploy_api"
      ],
      [
        "do_forward"
      ],
      [
        "do_provision"
      ],
      [
        "do_remove_api"
      ],
      [
        "do_remove_forward"
      ],
      [
        "endpoint_serializer"
      ],
      [
        "forward"
      ],
      [
        "get_character_set"
      ],
      [
        "get_option_or_env"
      ],
      [
        "getCharacterSet"
      ],
      [
        "include_file"
      ],
      [
        "include_html"
      ],
      [
        "include_md"
      ],
      [
        "include_rmd"
      ],
      [
        "is_plumber"
      ],
      [
        "options_plumber"
      ],
      [
        "parser_arrow_ipc_stream"
      ],
      [
        "parser_csv"
      ],
      [
        "parser_excel"
      ],
      [
        "parser_feather"
      ],
      [
        "parser_form"
      ],
      [
        "parser_geojson"
      ],
      [
        "parser_json"
      ],
      [
        "parser_multi"
      ],
      [
        "parser_none"
      ],
      [
        "parser_octet"
      ],
      [
        "parser_parquet"
      ],
      [
        "parser_rds"
      ],
      [
        "parser_read_file"
      ],
      [
        "parser_text"
      ],
      [
        "parser_tsv"
      ],
      [
        "parser_yaml"
      ],
      [
        "plumb"
      ],
      [
        "plumb_api"
      ],
      [
        "plumber"
      ],
      [
        "Plumber"
      ],
      [
        "PlumberEndpoint"
      ],
      [
        "PlumberStatic"
      ],
      [
        "pr"
      ],
      [
        "pr_cookie"
      ],
      [
        "pr_delete"
      ],
      [
        "pr_filter"
      ],
      [
        "pr_get"
      ],
      [
        "pr_handle"
      ],
      [
        "pr_head"
      ],
      [
        "pr_hook"
      ],
      [
        "pr_hooks"
      ],
      [
        "pr_mount"
      ],
      [
        "pr_post"
      ],
      [
        "pr_put"
      ],
      [
        "pr_run"
      ],
      [
        "pr_set_404"
      ],
      [
        "pr_set_api_spec"
      ],
      [
        "pr_set_debug"
      ],
      [
        "pr_set_docs"
      ],
      [
        "pr_set_docs_callback"
      ],
      [
        "pr_set_error"
      ],
      [
        "pr_set_parsers"
      ],
      [
        "pr_set_serializer"
      ],
      [
        "pr_static"
      ],
      [
        "random_cookie_key"
      ],
      [
        "register_docs"
      ],
      [
        "register_parser"
      ],
      [
        "register_serializer"
      ],
      [
        "registered_docs"
      ],
      [
        "registered_parsers"
      ],
      [
        "registered_serializers"
      ],
      [
        "serializer_agg_jpeg"
      ],
      [
        "serializer_agg_png"
      ],
      [
        "serializer_agg_tiff"
      ],
      [
        "serializer_arrow_ipc_stream"
      ],
      [
        "serializer_bmp"
      ],
      [
        "serializer_cat"
      ],
      [
        "serializer_content_type"
      ],
      [
        "serializer_csv"
      ],
      [
        "serializer_device"
      ],
      [
        "serializer_excel"
      ],
      [
        "serializer_feather"
      ],
      [
        "serializer_format"
      ],
      [
        "serializer_geojson"
      ],
      [
        "serializer_headers"
      ],
      [
        "serializer_html"
      ],
      [
        "serializer_htmlwidget"
      ],
      [
        "serializer_jpeg"
      ],
      [
        "serializer_json"
      ],
      [
        "serializer_octet"
      ],
      [
        "serializer_parquet"
      ],
      [
        "serializer_pdf"
      ],
      [
        "serializer_png"
      ],
      [
        "serializer_print"
      ],
      [
        "serializer_rds"
      ],
      [
        "serializer_svg"
      ],
      [
        "serializer_svglite"
      ],
      [
        "serializer_text"
      ],
      [
        "serializer_tiff"
      ],
      [
        "serializer_tsv"
      ],
      [
        "serializer_unboxed_json"
      ],
      [
        "serializer_write_file"
      ],
      [
        "serializer_yaml"
      ],
      [
        "session_cookie"
      ],
      [
        "sessionCookie"
      ],
      [
        "validate_api_spec"
      ]
    ],
    "topics": [
      [
        "api"
      ],
      [
        "api-server"
      ],
      [
        "plumber"
      ]
    ],
    "score": 15.1808,
    "stars": 1436,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "plumber An API Generator for R Gives the ability to automatically generate and serve an\nHTTP API from R functions using the annotations in the R\ndocumentation around your functions. %>% addSerializer as_attachment available_apis do_configure_https do_deploy_api do_forward do_provision do_remove_api do_remove_forward endpoint_serializer forward get_character_set get_option_or_env getCharacterSet include_file include_html include_md include_rmd is_plumber options_plumber parser_arrow_ipc_stream parser_csv parser_excel parser_feather parser_form parser_geojson parser_json parser_multi parser_none parser_octet parser_parquet parser_rds parser_read_file parser_text parser_tsv parser_yaml plumb plumb_api plumber Plumber PlumberEndpoint PlumberStatic pr pr_cookie pr_delete pr_filter pr_get pr_handle pr_head pr_hook pr_hooks pr_mount pr_post pr_put pr_run pr_set_404 pr_set_api_spec pr_set_debug pr_set_docs pr_set_docs_callback pr_set_error pr_set_parsers pr_set_serializer pr_static random_cookie_key register_docs register_parser register_serializer registered_docs registered_parsers registered_serializers serializer_agg_jpeg serializer_agg_png serializer_agg_tiff serializer_arrow_ipc_stream serializer_bmp serializer_cat serializer_content_type serializer_csv serializer_device serializer_excel serializer_feather serializer_format serializer_geojson serializer_headers serializer_html serializer_htmlwidget serializer_jpeg serializer_json serializer_octet serializer_parquet serializer_pdf serializer_png serializer_print serializer_rds serializer_svg serializer_svglite serializer_text serializer_tiff serializer_tsv serializer_unboxed_json serializer_write_file serializer_yaml session_cookie sessionCookie validate_api_spec api api-server plumber"
  },
  {
    "id": 1069,
    "package_name": "reactable",
    "title": "Interactive Data Tables for R",
    "description": "Interactive data tables for R, based on the 'React Table'\nJavaScript library. Provides an HTML widget that can be used in\n'R Markdown' or 'Quarto' documents, 'Shiny' applications, or\nviewed from an R console.",
    "version": "0.4.5.9000",
    "maintainer": "Greg Lin <glin@glin.io>",
    "author": "Greg Lin [aut, cre],\nTanner Linsley [ctb, cph] (React Table library),\nEmotion team and other contributors [ctb, cph] (Emotion library),\nKent Russell [ctb, cph] (reactR package),\nRamnath Vaidyanathan [ctb, cph] (htmlwidgets package),\nJoe Cheng [ctb, cph] (htmlwidgets package),\nJJ Allaire [ctb, cph] (htmlwidgets package),\nYihui Xie [ctb, cph] (htmlwidgets package),\nKenton Russell [ctb, cph] (htmlwidgets package),\nFacebook, Inc. and its affiliates [ctb, cph] (React library),\nFormatJS [ctb, cph] (FormatJS libraries),\nFeross Aboukhadijeh, and other contributors [ctb, cph] (buffer library),\nRoman Shtylman [ctb, cph] (process library),\nJames Halliday [ctb, cph] (stream-browserify library),\nPosit Software, PBC [fnd, cph]",
    "url": "https://glin.github.io/reactable/,\nhttps://github.com/glin/reactable",
    "bug_reports": "https://github.com/glin/reactable/issues",
    "repository": "",
    "exports": [
      [
        "colDef"
      ],
      [
        "colFormat"
      ],
      [
        "colGroup"
      ],
      [
        "getReactableState"
      ],
      [
        "JS"
      ],
      [
        "knit_print.reactable"
      ],
      [
        "reactable"
      ],
      [
        "reactableLang"
      ],
      [
        "reactableOutput"
      ],
      [
        "reactableServerData"
      ],
      [
        "reactableServerInit"
      ],
      [
        "reactableTheme"
      ],
      [
        "renderReactable"
      ],
      [
        "resolvedData"
      ],
      [
        "updateReactable"
      ]
    ],
    "topics": [
      [
        "htmlwidgets"
      ],
      [
        "react"
      ],
      [
        "shiny"
      ],
      [
        "table"
      ]
    ],
    "score": 15.1338,
    "stars": 669,
    "primary_category": "visualization",
    "source_universe": "glin",
    "search_text": "reactable Interactive Data Tables for R Interactive data tables for R, based on the 'React Table'\nJavaScript library. Provides an HTML widget that can be used in\n'R Markdown' or 'Quarto' documents, 'Shiny' applications, or\nviewed from an R console. colDef colFormat colGroup getReactableState JS knit_print.reactable reactable reactableLang reactableOutput reactableServerData reactableServerInit reactableTheme renderReactable resolvedData updateReactable htmlwidgets react shiny table"
  },
  {
    "id": 753,
    "package_name": "learnr",
    "title": "Interactive Tutorials for R",
    "description": "Create interactive tutorials using R Markdown. Use a\ncombination of narrative, figures, videos, exercises, and\nquizzes to create self-paced tutorials for learning about R and\nR packages.",
    "version": "0.11.6.9000",
    "maintainer": "Garrick Aden-Buie <garrick@posit.co>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7111-0077>),\nBarret Schloerke [aut] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\nJJ Allaire [aut, ccp],\nAlexander Rossell Hayes [aut] (ORCID:\n<https://orcid.org/0000-0001-9412-0457>),\nNischal Shrestha [ctb] (ORCID: <https://orcid.org/0000-0003-3321-1712>),\nAngela Li [ctb] (vignette),\nPosit, PBC [cph, fnd],\nAjax.org B.V. [ctb, cph] (Ace library),\nZeno Rocha [ctb, cph] (clipboard.js library),\nNick Payne [ctb, cph] (Bootbox library),\nJake Archibald [ctb, cph] (idb-keyval library),\ni18next authors [ctb, cph] (i18next library)",
    "url": "https://rstudio.github.io/learnr/,\nhttps://github.com/rstudio/learnr",
    "bug_reports": "https://github.com/rstudio/learnr/issues",
    "repository": "",
    "exports": [
      [
        "answer"
      ],
      [
        "answer_fn"
      ],
      [
        "available_tutorials"
      ],
      [
        "correct"
      ],
      [
        "disable_all_tags"
      ],
      [
        "duplicate_env"
      ],
      [
        "event_register_handler"
      ],
      [
        "external_evaluator"
      ],
      [
        "filesystem_storage"
      ],
      [
        "finalize_question"
      ],
      [
        "get_tutorial_info"
      ],
      [
        "get_tutorial_state"
      ],
      [
        "incorrect"
      ],
      [
        "initialize_tutorial"
      ],
      [
        "mark_as"
      ],
      [
        "mock_chunk"
      ],
      [
        "mock_exercise"
      ],
      [
        "one_time"
      ],
      [
        "question"
      ],
      [
        "question_checkbox"
      ],
      [
        "question_is_correct"
      ],
      [
        "question_is_valid"
      ],
      [
        "question_numeric"
      ],
      [
        "question_radio"
      ],
      [
        "question_text"
      ],
      [
        "question_ui_completed"
      ],
      [
        "question_ui_initialize"
      ],
      [
        "question_ui_try_again"
      ],
      [
        "quiz"
      ],
      [
        "random_encouragement"
      ],
      [
        "random_phrases_add"
      ],
      [
        "random_praise"
      ],
      [
        "run_tutorial"
      ],
      [
        "safe"
      ],
      [
        "safe_env"
      ],
      [
        "tutorial"
      ],
      [
        "tutorial_html_dependency"
      ],
      [
        "tutorial_options"
      ],
      [
        "tutorial_package_dependencies"
      ]
    ],
    "topics": [
      [
        "interactive"
      ],
      [
        "python"
      ],
      [
        "rmarkdown"
      ],
      [
        "shiny"
      ],
      [
        "sql"
      ],
      [
        "teaching"
      ],
      [
        "tutorial"
      ]
    ],
    "score": 15.0078,
    "stars": 726,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "learnr Interactive Tutorials for R Create interactive tutorials using R Markdown. Use a\ncombination of narrative, figures, videos, exercises, and\nquizzes to create self-paced tutorials for learning about R and\nR packages. answer answer_fn available_tutorials correct disable_all_tags duplicate_env event_register_handler external_evaluator filesystem_storage finalize_question get_tutorial_info get_tutorial_state incorrect initialize_tutorial mark_as mock_chunk mock_exercise one_time question question_checkbox question_is_correct question_is_valid question_numeric question_radio question_text question_ui_completed question_ui_initialize question_ui_try_again quiz random_encouragement random_phrases_add random_praise run_tutorial safe safe_env tutorial tutorial_html_dependency tutorial_options tutorial_package_dependencies interactive python rmarkdown shiny sql teaching tutorial"
  },
  {
    "id": 1088,
    "package_name": "report",
    "title": "Automated Reporting of Results and Statistical Models",
    "description": "The aim of the 'report' package is to bridge the gap\nbetween R\u2019s output and the formatted results contained in your\nmanuscript. This package converts statistical models and data\nframes into textual reports suited for publication, ensuring\nstandardization and quality in results reporting.",
    "version": "0.6.2",
    "maintainer": "R\u00e9mi Th\u00e9riault <remi.theriault@mail.mcgill.ca>",
    "author": "Dominique Makowski [aut] (ORCID:\n<https://orcid.org/0000-0001-5375-9967>),\nDaniel L\u00fcdecke [aut] (ORCID: <https://orcid.org/0000-0002-8895-3206>),\nIndrajeet Patil [aut] (ORCID: <https://orcid.org/0000-0003-1995-6531>),\nR\u00e9mi Th\u00e9riault [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4315-6788>),\nMattan S. Ben-Shachar [aut] (ORCID:\n<https://orcid.org/0000-0002-4287-4801>),\nBrenton M. Wiernik [aut] (ORCID:\n<https://orcid.org/0000-0001-9560-6336>),\nRudolf Siegel [ctb] (ORCID: <https://orcid.org/0000-0002-6021-804X>),\nCamden Bock [ctb] (ORCID: <https://orcid.org/0000-0002-3907-7748>)",
    "url": "https://easystats.github.io/report/",
    "bug_reports": "https://github.com/easystats/report/issues",
    "repository": "",
    "exports": [
      [
        "as.report"
      ],
      [
        "as.report_effectsize"
      ],
      [
        "as.report_info"
      ],
      [
        "as.report_intercept"
      ],
      [
        "as.report_model"
      ],
      [
        "as.report_parameters"
      ],
      [
        "as.report_performance"
      ],
      [
        "as.report_priors"
      ],
      [
        "as.report_random"
      ],
      [
        "as.report_statistics"
      ],
      [
        "as.report_table"
      ],
      [
        "as.report_text"
      ],
      [
        "cite_citation"
      ],
      [
        "cite_easystats"
      ],
      [
        "cite_packages"
      ],
      [
        "clean_citation"
      ],
      [
        "display"
      ],
      [
        "format_algorithm"
      ],
      [
        "format_citation"
      ],
      [
        "format_formula"
      ],
      [
        "format_model"
      ],
      [
        "is.report"
      ],
      [
        "print_html"
      ],
      [
        "print_md"
      ],
      [
        "report"
      ],
      [
        "report_date"
      ],
      [
        "report_effectsize"
      ],
      [
        "report_info"
      ],
      [
        "report_intercept"
      ],
      [
        "report_model"
      ],
      [
        "report_packages"
      ],
      [
        "report_parameters"
      ],
      [
        "report_participants"
      ],
      [
        "report_performance"
      ],
      [
        "report_priors"
      ],
      [
        "report_random"
      ],
      [
        "report_s"
      ],
      [
        "report_sample"
      ],
      [
        "report_statistics"
      ],
      [
        "report_story"
      ],
      [
        "report_system"
      ],
      [
        "report_table"
      ],
      [
        "report_text"
      ]
    ],
    "topics": [
      [
        "anovas"
      ],
      [
        "apa"
      ],
      [
        "automated-report-generation"
      ],
      [
        "automatic"
      ],
      [
        "bayesian"
      ],
      [
        "describe"
      ],
      [
        "easystats"
      ],
      [
        "hacktoberfest"
      ],
      [
        "manuscript"
      ],
      [
        "models"
      ],
      [
        "report"
      ],
      [
        "reporting"
      ],
      [
        "reports"
      ],
      [
        "scientific"
      ],
      [
        "statsmodels"
      ]
    ],
    "score": 14.7453,
    "stars": 710,
    "primary_category": "statistics",
    "source_universe": "easystats",
    "search_text": "report Automated Reporting of Results and Statistical Models The aim of the 'report' package is to bridge the gap\nbetween R\u2019s output and the formatted results contained in your\nmanuscript. This package converts statistical models and data\nframes into textual reports suited for publication, ensuring\nstandardization and quality in results reporting. as.report as.report_effectsize as.report_info as.report_intercept as.report_model as.report_parameters as.report_performance as.report_priors as.report_random as.report_statistics as.report_table as.report_text cite_citation cite_easystats cite_packages clean_citation display format_algorithm format_citation format_formula format_model is.report print_html print_md report report_date report_effectsize report_info report_intercept report_model report_packages report_parameters report_participants report_performance report_priors report_random report_s report_sample report_statistics report_story report_system report_table report_text anovas apa automated-report-generation automatic bayesian describe easystats hacktoberfest manuscript models report reporting reports scientific statsmodels"
  },
  {
    "id": 795,
    "package_name": "markdown",
    "title": "A thin wrapper of 'litedown' to render Markdown documents",
    "description": "Render Markdown to HTML/LaTeX. This package has been\nsuperseded by 'litedown'. Please call 'litedown::mark()'\ndirectly.",
    "version": "2.0.1",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nJJ Allaire [aut],\nJeffrey Horner [aut],\nHenrik Bengtsson [ctb],\nJim Hester [ctb],\nYixuan Qiu [ctb],\nKohske Takahashi [ctb],\nAdam November [ctb],\nNacho Caballero [ctb],\nJeroen Ooms [ctb],\nThomas Leeper [ctb],\nJoe Cheng [ctb],\nAndrzej Oles [ctb],\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/markdown",
    "bug_reports": "https://github.com/rstudio/markdown/issues",
    "repository": "",
    "exports": [
      [
        "html_format"
      ],
      [
        "latex_format"
      ],
      [
        "mark"
      ],
      [
        "mark_html"
      ],
      [
        "mark_latex"
      ],
      [
        "markdown_options"
      ],
      [
        "markdownToHTML"
      ],
      [
        "renderMarkdown"
      ],
      [
        "rpubsUpload"
      ],
      [
        "smartypants"
      ]
    ],
    "topics": [
      [
        "commonmark"
      ],
      [
        "markdown"
      ]
    ],
    "score": 14.707,
    "stars": 88,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "markdown A thin wrapper of 'litedown' to render Markdown documents Render Markdown to HTML/LaTeX. This package has been\nsuperseded by 'litedown'. Please call 'litedown::mark()'\ndirectly. html_format latex_format mark mark_html mark_latex markdown_options markdownToHTML renderMarkdown rpubsUpload smartypants commonmark markdown"
  },
  {
    "id": 954,
    "package_name": "pdftools",
    "title": "Text Extraction, Rendering and Converting of PDF Documents",
    "description": "Utilities based on 'libpoppler'\n<https://poppler.freedesktop.org> for extracting text, fonts,\nattachments and metadata from a PDF file. Also supports high\nquality rendering of PDF documents into PNG, JPEG, TIFF format,\nor into raw bitmap vectors for further processing in R.",
    "version": "3.7.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://ropensci.r-universe.dev/pdftools,\nhttps://docs.ropensci.org/pdftools/",
    "bug_reports": "https://github.com/ropensci/pdftools/issues",
    "repository": "",
    "exports": [
      [
        "pdf_attachments"
      ],
      [
        "pdf_combine"
      ],
      [
        "pdf_compress"
      ],
      [
        "pdf_convert"
      ],
      [
        "pdf_data"
      ],
      [
        "pdf_fonts"
      ],
      [
        "pdf_info"
      ],
      [
        "pdf_length"
      ],
      [
        "pdf_ocr_data"
      ],
      [
        "pdf_ocr_text"
      ],
      [
        "pdf_pagesize"
      ],
      [
        "pdf_render_page"
      ],
      [
        "pdf_split"
      ],
      [
        "pdf_subset"
      ],
      [
        "pdf_text"
      ],
      [
        "pdf_toc"
      ],
      [
        "poppler_config"
      ]
    ],
    "topics": [
      [
        "pdf-files"
      ],
      [
        "pdf-format"
      ],
      [
        "pdftools"
      ],
      [
        "poppler"
      ],
      [
        "poppler-library"
      ],
      [
        "text-extraction"
      ],
      [
        "cpp"
      ]
    ],
    "score": 14.1436,
    "stars": 544,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "pdftools Text Extraction, Rendering and Converting of PDF Documents Utilities based on 'libpoppler'\n<https://poppler.freedesktop.org> for extracting text, fonts,\nattachments and metadata from a PDF file. Also supports high\nquality rendering of PDF documents into PNG, JPEG, TIFF format,\nor into raw bitmap vectors for further processing in R. pdf_attachments pdf_combine pdf_compress pdf_convert pdf_data pdf_fonts pdf_info pdf_length pdf_ocr_data pdf_ocr_text pdf_pagesize pdf_render_page pdf_split pdf_subset pdf_text pdf_toc poppler_config pdf-files pdf-format pdftools poppler poppler-library text-extraction cpp"
  },
  {
    "id": 389,
    "package_name": "commonmark",
    "title": "High Performance CommonMark and Github Markdown Rendering in R",
    "description": "The CommonMark specification\n<https://github.github.com/gfm/> defines a rationalized version\nof markdown syntax. This package uses the 'cmark' reference\nimplementation for converting markdown text into various\nformats including html, latex and groff man. In addition it\nexposes the markdown parse tree in xml format. Also includes\nopt-in support for GFM extensions including tables, autolinks,\nand strikethrough text.",
    "version": "2.0.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nJohn MacFarlane [cph] (Author of cmark)",
    "url": "https://docs.ropensci.org/commonmark/\nhttps://ropensci.r-universe.dev/commonmark",
    "bug_reports": "https://github.com/r-lib/commonmark/issues",
    "repository": "",
    "exports": [
      [
        "list_extensions"
      ],
      [
        "markdown_commonmark"
      ],
      [
        "markdown_html"
      ],
      [
        "markdown_latex"
      ],
      [
        "markdown_man"
      ],
      [
        "markdown_text"
      ],
      [
        "markdown_xml"
      ]
    ],
    "topics": [
      [
        "cmark"
      ],
      [
        "cmark-gfm"
      ],
      [
        "gfm"
      ],
      [
        "markdown"
      ]
    ],
    "score": 13.73,
    "stars": 99,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "commonmark High Performance CommonMark and Github Markdown Rendering in R The CommonMark specification\n<https://github.github.com/gfm/> defines a rationalized version\nof markdown syntax. This package uses the 'cmark' reference\nimplementation for converting markdown text into various\nformats including html, latex and groff man. In addition it\nexposes the markdown parse tree in xml format. Also includes\nopt-in support for GFM extensions including tables, autolinks,\nand strikethrough text. list_extensions markdown_commonmark markdown_html markdown_latex markdown_man markdown_text markdown_xml cmark cmark-gfm gfm markdown"
  },
  {
    "id": 834,
    "package_name": "modelsummary",
    "title": "Summary Tables and Plots for Statistical Models and Data:\nBeautiful, Customizable, and Publication-Ready",
    "description": "Create beautiful and customizable tables to summarize\nseveral statistical models side-by-side. Draw coefficient\nplots, multi-level cross-tabs, dataset summaries, balance\ntables (a.k.a. \"Table 1s\"), and correlation matrices. This\npackage supports dozens of statistical models, and it can\nproduce tables in HTML, LaTeX, Word, Markdown, PDF, PowerPoint,\nExcel, RTF, JPG, or PNG. Tables can easily be embedded in\n'Rmarkdown' or 'knitr' dynamic documents. Details can be found\nin Arel-Bundock (2022) <doi:10.18637/jss.v103.i01>.",
    "version": "2.5.0.3",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "author": "Vincent Arel-Bundock [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>),\nJoachim Gassen [ctb] (ORCID: <https://orcid.org/0000-0003-4364-2911>),\nNathan Eastwood [ctb],\nNick Huntington-Klein [ctb] (ORCID:\n<https://orcid.org/0000-0002-7352-3991>),\nMoritz Schwarz [ctb] (ORCID: <https://orcid.org/0000-0003-0340-3780>),\nBenjamin Elbers [ctb] (ORCID: <https://orcid.org/0000-0001-5392-3448>),\nGrant McDermott [ctb] (ORCID: <https://orcid.org/0000-0001-7883-8573>),\nLukas Wallrich [ctb] (ORCID: <https://orcid.org/0000-0003-2121-5177>)",
    "url": "https://modelsummary.com",
    "bug_reports": "https://github.com/vincentarelbundock/modelsummary/issues/",
    "repository": "",
    "exports": [
      [
        "All"
      ],
      [
        "AllObs"
      ],
      [
        "Arguments"
      ],
      [
        "coef_rename"
      ],
      [
        "colLabels"
      ],
      [
        "config_modelsummary"
      ],
      [
        "datasummary"
      ],
      [
        "datasummary_balance"
      ],
      [
        "datasummary_correlation"
      ],
      [
        "datasummary_correlation_format"
      ],
      [
        "datasummary_crosstab"
      ],
      [
        "datasummary_df"
      ],
      [
        "datasummary_skim"
      ],
      [
        "DropEmpty"
      ],
      [
        "dsummary"
      ],
      [
        "dvnames"
      ],
      [
        "Factor"
      ],
      [
        "fmt_decimal"
      ],
      [
        "fmt_equivalence"
      ],
      [
        "fmt_sci"
      ],
      [
        "fmt_significant"
      ],
      [
        "fmt_sprintf"
      ],
      [
        "fmt_statistic"
      ],
      [
        "fmt_term"
      ],
      [
        "Format"
      ],
      [
        "get_estimates"
      ],
      [
        "get_gof"
      ],
      [
        "glance"
      ],
      [
        "glance_custom"
      ],
      [
        "gof_map"
      ],
      [
        "Heading"
      ],
      [
        "Histogram"
      ],
      [
        "labelSubset"
      ],
      [
        "Max"
      ],
      [
        "Mean"
      ],
      [
        "Median"
      ],
      [
        "Min"
      ],
      [
        "modelplot"
      ],
      [
        "modelsummary"
      ],
      [
        "msummary"
      ],
      [
        "Multicolumn"
      ],
      [
        "N"
      ],
      [
        "Ncol"
      ],
      [
        "NPercent"
      ],
      [
        "NUnique"
      ],
      [
        "P0"
      ],
      [
        "P100"
      ],
      [
        "P25"
      ],
      [
        "P50"
      ],
      [
        "P75"
      ],
      [
        "Paste"
      ],
      [
        "Percent"
      ],
      [
        "PercentMissing"
      ],
      [
        "PlusMinus"
      ],
      [
        "RowFactor"
      ],
      [
        "rowLabels"
      ],
      [
        "RowNum"
      ],
      [
        "SD"
      ],
      [
        "supported_models"
      ],
      [
        "tidy"
      ],
      [
        "tidy_custom"
      ],
      [
        "update_modelsummary"
      ],
      [
        "Var"
      ]
    ],
    "topics": [],
    "score": 13.7268,
    "stars": 943,
    "primary_category": "statistics",
    "source_universe": "vincentarelbundock",
    "search_text": "modelsummary Summary Tables and Plots for Statistical Models and Data:\nBeautiful, Customizable, and Publication-Ready Create beautiful and customizable tables to summarize\nseveral statistical models side-by-side. Draw coefficient\nplots, multi-level cross-tabs, dataset summaries, balance\ntables (a.k.a. \"Table 1s\"), and correlation matrices. This\npackage supports dozens of statistical models, and it can\nproduce tables in HTML, LaTeX, Word, Markdown, PDF, PowerPoint,\nExcel, RTF, JPG, or PNG. Tables can easily be embedded in\n'Rmarkdown' or 'knitr' dynamic documents. Details can be found\nin Arel-Bundock (2022) <doi:10.18637/jss.v103.i01>. All AllObs Arguments coef_rename colLabels config_modelsummary datasummary datasummary_balance datasummary_correlation datasummary_correlation_format datasummary_crosstab datasummary_df datasummary_skim DropEmpty dsummary dvnames Factor fmt_decimal fmt_equivalence fmt_sci fmt_significant fmt_sprintf fmt_statistic fmt_term Format get_estimates get_gof glance glance_custom gof_map Heading Histogram labelSubset Max Mean Median Min modelplot modelsummary msummary Multicolumn N Ncol NPercent NUnique P0 P100 P25 P50 P75 Paste Percent PercentMissing PlusMinus RowFactor rowLabels RowNum SD supported_models tidy tidy_custom update_modelsummary Var "
  },
  {
    "id": 1306,
    "package_name": "taxize",
    "title": "Taxonomic Information from Around the Web",
    "description": "Interacts with a suite of web application programming\ninterfaces (API) for taxonomic tasks, such as getting database\nspecific taxonomic identifiers, verifying species names,\ngetting taxonomic hierarchies, fetching downstream and upstream\ntaxonomic names, getting taxonomic synonyms, converting\nscientific to common names and vice versa, and more. Some of\nthe services supported include 'NCBI E-utilities'\n(<https://www.ncbi.nlm.nih.gov/books/NBK25501/>), 'Encyclopedia\nof Life' (<https://eol.org/docs/what-is-eol/data-services>),\n'Global Biodiversity Information Facility'\n(<https://techdocs.gbif.org/en/openapi/>), and many more. Links\nto the API documentation for other supported services are\navailable in the documentation for their respective functions\nin this package.",
    "version": "0.10.0",
    "maintainer": "Zachary Foster <zacharyfoster1989@gmail.com>",
    "author": "Scott Chamberlain [aut],\nEduard Szoecs [aut],\nZachary Foster [aut, cre],\nZebulun Arendsee [aut],\nCarl Boettiger [ctb],\nKarthik Ram [ctb],\nIgnasi Bartomeus [ctb],\nJohn Baumgartner [ctb],\nJames O'Donnell [ctb],\nJari Oksanen [ctb],\nBastian Greshake Tzovaras [ctb],\nPhilippe Marchand [ctb],\nVinh Tran [ctb],\nMa\u00eblle Salmon [ctb],\nGaopeng Li [ctb],\nMatthias Greni\u00e9 [ctb],\nrOpenSci [fnd]",
    "url": "https://docs.ropensci.org/taxize/ (website),\nhttps://github.com/ropensci/taxize (devel)",
    "bug_reports": "https://github.com/ropensci/taxize/issues",
    "repository": "",
    "exports": [
      [
        "apg_lookup"
      ],
      [
        "apgFamilies"
      ],
      [
        "apgOrders"
      ],
      [
        "as.boldid"
      ],
      [
        "as.colid"
      ],
      [
        "as.eolid"
      ],
      [
        "as.gbifid"
      ],
      [
        "as.iucn"
      ],
      [
        "as.natservid"
      ],
      [
        "as.nbnid"
      ],
      [
        "as.pow"
      ],
      [
        "as.tolid"
      ],
      [
        "as.tpsid"
      ],
      [
        "as.tsn"
      ],
      [
        "as.ubioid"
      ],
      [
        "as.uid"
      ],
      [
        "as.wiki"
      ],
      [
        "as.wormsid"
      ],
      [
        "bold_children"
      ],
      [
        "bold_downstream"
      ],
      [
        "bold_ping"
      ],
      [
        "bold_search"
      ],
      [
        "children"
      ],
      [
        "class2tree"
      ],
      [
        "classification"
      ],
      [
        "col_children"
      ],
      [
        "col_classification"
      ],
      [
        "col_downstream"
      ],
      [
        "col_ping"
      ],
      [
        "col_search"
      ],
      [
        "comm2sci"
      ],
      [
        "downstream"
      ],
      [
        "eol_dataobjects"
      ],
      [
        "eol_hierarchy"
      ],
      [
        "eol_invasive"
      ],
      [
        "eol_pages"
      ],
      [
        "eol_ping"
      ],
      [
        "eol_search"
      ],
      [
        "eubon"
      ],
      [
        "eubon_capabilities"
      ],
      [
        "eubon_children"
      ],
      [
        "eubon_hierarchy"
      ],
      [
        "eubon_search"
      ],
      [
        "fg_all_updated_names"
      ],
      [
        "fg_author_search"
      ],
      [
        "fg_deprecated_names"
      ],
      [
        "fg_epithet_search"
      ],
      [
        "fg_name_by_key"
      ],
      [
        "fg_name_full_by_lsid"
      ],
      [
        "fg_name_search"
      ],
      [
        "fg_ping"
      ],
      [
        "gbif_downstream"
      ],
      [
        "gbif_name_usage"
      ],
      [
        "gbif_parse"
      ],
      [
        "gbif_ping"
      ],
      [
        "genbank2uid"
      ],
      [
        "get_boldid"
      ],
      [
        "get_boldid_"
      ],
      [
        "get_colid"
      ],
      [
        "get_colid_"
      ],
      [
        "get_eolid"
      ],
      [
        "get_eolid_"
      ],
      [
        "get_gbifid"
      ],
      [
        "get_gbifid_"
      ],
      [
        "get_genes"
      ],
      [
        "get_genes_avail"
      ],
      [
        "get_ids"
      ],
      [
        "get_ids_"
      ],
      [
        "get_iucn"
      ],
      [
        "get_natservid"
      ],
      [
        "get_natservid_"
      ],
      [
        "get_nbnid"
      ],
      [
        "get_nbnid_"
      ],
      [
        "get_pow"
      ],
      [
        "get_pow_"
      ],
      [
        "get_seqs"
      ],
      [
        "get_tolid"
      ],
      [
        "get_tolid_"
      ],
      [
        "get_tpsid"
      ],
      [
        "get_tpsid_"
      ],
      [
        "get_tsn"
      ],
      [
        "get_tsn_"
      ],
      [
        "get_ubioid"
      ],
      [
        "get_ubioid_"
      ],
      [
        "get_uid"
      ],
      [
        "get_uid_"
      ],
      [
        "get_wiki"
      ],
      [
        "get_wiki_"
      ],
      [
        "get_wormsid"
      ],
      [
        "get_wormsid_"
      ],
      [
        "getkey"
      ],
      [
        "gisd_isinvasive"
      ],
      [
        "gna_data_sources"
      ],
      [
        "gna_parse"
      ],
      [
        "gna_search"
      ],
      [
        "gna_verifier"
      ],
      [
        "gni_details"
      ],
      [
        "gni_parse"
      ],
      [
        "gni_seach"
      ],
      [
        "gnr_datasources"
      ],
      [
        "gnr_resolve"
      ],
      [
        "id2name"
      ],
      [
        "ion"
      ],
      [
        "iplant_resolve"
      ],
      [
        "ipni_ping"
      ],
      [
        "ipni_search"
      ],
      [
        "itis_acceptname"
      ],
      [
        "itis_downstream"
      ],
      [
        "itis_getrecord"
      ],
      [
        "itis_hierarchy"
      ],
      [
        "itis_kingdomnames"
      ],
      [
        "itis_lsid"
      ],
      [
        "itis_name"
      ],
      [
        "itis_native"
      ],
      [
        "itis_ping"
      ],
      [
        "itis_refs"
      ],
      [
        "itis_taxrank"
      ],
      [
        "itis_terms"
      ],
      [
        "iucn_getname"
      ],
      [
        "iucn_id"
      ],
      [
        "iucn_status"
      ],
      [
        "iucn_summary"
      ],
      [
        "lowest_common"
      ],
      [
        "names_list"
      ],
      [
        "nbn_classification"
      ],
      [
        "nbn_ping"
      ],
      [
        "nbn_search"
      ],
      [
        "nbn_synonyms"
      ],
      [
        "ncbi_children"
      ],
      [
        "ncbi_downstream"
      ],
      [
        "ncbi_get_taxon_summary"
      ],
      [
        "ncbi_getbyid"
      ],
      [
        "ncbi_getbyname"
      ],
      [
        "ncbi_ping"
      ],
      [
        "ncbi_search"
      ],
      [
        "phylomatic_format"
      ],
      [
        "phylomatic_tree"
      ],
      [
        "plantminer"
      ],
      [
        "pow_lookup"
      ],
      [
        "pow_search"
      ],
      [
        "pow_synonyms"
      ],
      [
        "rankagg"
      ],
      [
        "resolve"
      ],
      [
        "sci2comm"
      ],
      [
        "scrapenames"
      ],
      [
        "status_codes"
      ],
      [
        "synonyms"
      ],
      [
        "synonyms_df"
      ],
      [
        "tax_agg"
      ],
      [
        "tax_name"
      ],
      [
        "tax_rank"
      ],
      [
        "taxize_capwords"
      ],
      [
        "taxize_cite"
      ],
      [
        "taxize_ldfast"
      ],
      [
        "taxize_options"
      ],
      [
        "taxon_clear"
      ],
      [
        "taxon_last"
      ],
      [
        "tnrs"
      ],
      [
        "tnrs_sources"
      ],
      [
        "tol_resolve"
      ],
      [
        "tp_acceptednames"
      ],
      [
        "tp_accnames"
      ],
      [
        "tp_classification"
      ],
      [
        "tp_dist"
      ],
      [
        "tp_namedistributions"
      ],
      [
        "tp_namereferences"
      ],
      [
        "tp_refs"
      ],
      [
        "tp_search"
      ],
      [
        "tp_summary"
      ],
      [
        "tp_synonyms"
      ],
      [
        "tpl_families"
      ],
      [
        "tpl_get"
      ],
      [
        "tpl_search"
      ],
      [
        "tropicos_ping"
      ],
      [
        "ubio_classification"
      ],
      [
        "ubio_classification_search"
      ],
      [
        "ubio_id"
      ],
      [
        "ubio_ping"
      ],
      [
        "ubio_search"
      ],
      [
        "ubio_synonyms"
      ],
      [
        "upstream"
      ],
      [
        "use_entrez"
      ],
      [
        "use_eol"
      ],
      [
        "use_iucn"
      ],
      [
        "use_tropicos"
      ],
      [
        "vascan_ping"
      ],
      [
        "vascan_search"
      ],
      [
        "worms_downstream"
      ]
    ],
    "topics": [
      [
        "taxonomy"
      ],
      [
        "biology"
      ],
      [
        "nomenclature"
      ],
      [
        "json"
      ],
      [
        "api"
      ],
      [
        "web"
      ],
      [
        "api-client"
      ],
      [
        "identifiers"
      ],
      [
        "species"
      ],
      [
        "names"
      ],
      [
        "api-wrapper"
      ],
      [
        "biodiversity"
      ],
      [
        "darwincore"
      ],
      [
        "data"
      ],
      [
        "taxize"
      ]
    ],
    "score": 13.5332,
    "stars": 293,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxize Taxonomic Information from Around the Web Interacts with a suite of web application programming\ninterfaces (API) for taxonomic tasks, such as getting database\nspecific taxonomic identifiers, verifying species names,\ngetting taxonomic hierarchies, fetching downstream and upstream\ntaxonomic names, getting taxonomic synonyms, converting\nscientific to common names and vice versa, and more. Some of\nthe services supported include 'NCBI E-utilities'\n(<https://www.ncbi.nlm.nih.gov/books/NBK25501/>), 'Encyclopedia\nof Life' (<https://eol.org/docs/what-is-eol/data-services>),\n'Global Biodiversity Information Facility'\n(<https://techdocs.gbif.org/en/openapi/>), and many more. Links\nto the API documentation for other supported services are\navailable in the documentation for their respective functions\nin this package. apg_lookup apgFamilies apgOrders as.boldid as.colid as.eolid as.gbifid as.iucn as.natservid as.nbnid as.pow as.tolid as.tpsid as.tsn as.ubioid as.uid as.wiki as.wormsid bold_children bold_downstream bold_ping bold_search children class2tree classification col_children col_classification col_downstream col_ping col_search comm2sci downstream eol_dataobjects eol_hierarchy eol_invasive eol_pages eol_ping eol_search eubon eubon_capabilities eubon_children eubon_hierarchy eubon_search fg_all_updated_names fg_author_search fg_deprecated_names fg_epithet_search fg_name_by_key fg_name_full_by_lsid fg_name_search fg_ping gbif_downstream gbif_name_usage gbif_parse gbif_ping genbank2uid get_boldid get_boldid_ get_colid get_colid_ get_eolid get_eolid_ get_gbifid get_gbifid_ get_genes get_genes_avail get_ids get_ids_ get_iucn get_natservid get_natservid_ get_nbnid get_nbnid_ get_pow get_pow_ get_seqs get_tolid get_tolid_ get_tpsid get_tpsid_ get_tsn get_tsn_ get_ubioid get_ubioid_ get_uid get_uid_ get_wiki get_wiki_ get_wormsid get_wormsid_ getkey gisd_isinvasive gna_data_sources gna_parse gna_search gna_verifier gni_details gni_parse gni_seach gnr_datasources gnr_resolve id2name ion iplant_resolve ipni_ping ipni_search itis_acceptname itis_downstream itis_getrecord itis_hierarchy itis_kingdomnames itis_lsid itis_name itis_native itis_ping itis_refs itis_taxrank itis_terms iucn_getname iucn_id iucn_status iucn_summary lowest_common names_list nbn_classification nbn_ping nbn_search nbn_synonyms ncbi_children ncbi_downstream ncbi_get_taxon_summary ncbi_getbyid ncbi_getbyname ncbi_ping ncbi_search phylomatic_format phylomatic_tree plantminer pow_lookup pow_search pow_synonyms rankagg resolve sci2comm scrapenames status_codes synonyms synonyms_df tax_agg tax_name tax_rank taxize_capwords taxize_cite taxize_ldfast taxize_options taxon_clear taxon_last tnrs tnrs_sources tol_resolve tp_acceptednames tp_accnames tp_classification tp_dist tp_namedistributions tp_namereferences tp_refs tp_search tp_summary tp_synonyms tpl_families tpl_get tpl_search tropicos_ping ubio_classification ubio_classification_search ubio_id ubio_ping ubio_search ubio_synonyms upstream use_entrez use_eol use_iucn use_tropicos vascan_ping vascan_search worms_downstream taxonomy biology nomenclature json api web api-client identifiers species names api-wrapper biodiversity darwincore data taxize"
  },
  {
    "id": 1365,
    "package_name": "tokenizers",
    "title": "Fast, Consistent Tokenization of Natural Language Text",
    "description": "Convert natural language text into tokens. Includes\ntokenizers for shingled n-grams, skip n-grams, words, word\nstems, sentences, paragraphs, characters, shingled characters,\nlines, Penn Treebank, regular expressions, as well as functions\nfor counting characters, words, and sentences, and a function\nfor splitting longer texts into separate documents, each with\nthe same number of words.  The tokenizers have a consistent\ninterface, and the package is built on the 'stringi' and 'Rcpp'\npackages for fast yet correct tokenization in 'UTF-8'.",
    "version": "0.3.1",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "author": "Thomas Charlon [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-7497-0470>),\nLincoln Mullen [aut] (ORCID: <https://orcid.org/0000-0001-5103-6917>),\nOs Keyes [ctb] (ORCID: <https://orcid.org/0000-0001-5196-609X>),\nDmitriy Selivanov [ctb],\nJeffrey Arnold [ctb] (ORCID: <https://orcid.org/0000-0001-9953-3904>),\nKenneth Benoit [ctb] (ORCID: <https://orcid.org/0000-0002-0797-564X>)",
    "url": "https://docs.ropensci.org/tokenizers/,\nhttps://github.com/ropensci/tokenizers",
    "bug_reports": "https://github.com/ropensci/tokenizers/issues",
    "repository": "",
    "exports": [
      [
        "chunk_text"
      ],
      [
        "count_characters"
      ],
      [
        "count_sentences"
      ],
      [
        "count_words"
      ],
      [
        "tokenize_character_shingles"
      ],
      [
        "tokenize_characters"
      ],
      [
        "tokenize_lines"
      ],
      [
        "tokenize_ngrams"
      ],
      [
        "tokenize_paragraphs"
      ],
      [
        "tokenize_ptb"
      ],
      [
        "tokenize_regex"
      ],
      [
        "tokenize_sentences"
      ],
      [
        "tokenize_skip_ngrams"
      ],
      [
        "tokenize_word_stems"
      ],
      [
        "tokenize_words"
      ]
    ],
    "topics": [
      [
        "nlp"
      ],
      [
        "peer-reviewed"
      ],
      [
        "text-mining"
      ],
      [
        "tokenizer"
      ],
      [
        "cpp"
      ]
    ],
    "score": 13.4746,
    "stars": 186,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tokenizers Fast, Consistent Tokenization of Natural Language Text Convert natural language text into tokens. Includes\ntokenizers for shingled n-grams, skip n-grams, words, word\nstems, sentences, paragraphs, characters, shingled characters,\nlines, Penn Treebank, regular expressions, as well as functions\nfor counting characters, words, and sentences, and a function\nfor splitting longer texts into separate documents, each with\nthe same number of words.  The tokenizers have a consistent\ninterface, and the package is built on the 'stringi' and 'Rcpp'\npackages for fast yet correct tokenization in 'UTF-8'. chunk_text count_characters count_sentences count_words tokenize_character_shingles tokenize_characters tokenize_lines tokenize_ngrams tokenize_paragraphs tokenize_ptb tokenize_regex tokenize_sentences tokenize_skip_ngrams tokenize_word_stems tokenize_words nlp peer-reviewed text-mining tokenizer cpp"
  },
  {
    "id": 695,
    "package_name": "hunspell",
    "title": "High-Performance Stemmer, Tokenizer, and Spell Checker",
    "description": "Low level spell checker and morphological analyzer based\non the famous 'hunspell' library <https://hunspell.github.io>.\nThe package can analyze or check individual words as well as\nparse text, latex, html or xml documents. For a more\nuser-friendly interface use the 'spelling' package which builds\non this package to automate checking of files, documentation\nand vignettes in all common formats.",
    "version": "3.0.7",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre],\nAuthors of libhunspell [cph] (see AUTHORS file)",
    "url": "https://docs.ropensci.org/hunspell/\nhttps://ropensci.r-universe.dev/hunspell",
    "bug_reports": "https://github.com/ropensci/hunspell/issues",
    "repository": "",
    "exports": [
      [
        "dicpath"
      ],
      [
        "dictionary"
      ],
      [
        "en_stats"
      ],
      [
        "hunspell"
      ],
      [
        "hunspell_analyze"
      ],
      [
        "hunspell_check"
      ],
      [
        "hunspell_find"
      ],
      [
        "hunspell_info"
      ],
      [
        "hunspell_parse"
      ],
      [
        "hunspell_stem"
      ],
      [
        "hunspell_suggest"
      ],
      [
        "list_dictionaries"
      ]
    ],
    "topics": [
      [
        "hunspell"
      ],
      [
        "spell-check"
      ],
      [
        "spellchecker"
      ],
      [
        "stemmer"
      ],
      [
        "tokenizer"
      ],
      [
        "cpp"
      ]
    ],
    "score": 13.3629,
    "stars": 113,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "hunspell High-Performance Stemmer, Tokenizer, and Spell Checker Low level spell checker and morphological analyzer based\non the famous 'hunspell' library <https://hunspell.github.io>.\nThe package can analyze or check individual words as well as\nparse text, latex, html or xml documents. For a more\nuser-friendly interface use the 'spelling' package which builds\non this package to automate checking of files, documentation\nand vignettes in all common formats. dicpath dictionary en_stats hunspell hunspell_analyze hunspell_check hunspell_find hunspell_info hunspell_parse hunspell_stem hunspell_suggest list_dictionaries hunspell spell-check spellchecker stemmer tokenizer cpp"
  },
  {
    "id": 154,
    "package_name": "RSelenium",
    "title": "R Bindings for 'Selenium WebDriver'",
    "description": "Provides a set of R bindings for the 'Selenium 2.0\nWebDriver' (see <https://www.selenium.dev/documentation/> for\nmore information) using the 'JsonWireProtocol' (see\n<https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol>\nfor more information). 'Selenium 2.0 WebDriver' allows driving\na web browser natively as a user would either locally or on a\nremote machine using the Selenium server it marks a leap\nforward in terms of web browser automation. Selenium automates\nweb browsers (commonly referred to as browsers). Using\nRSelenium you can automate browsers locally or remotely.",
    "version": "1.7.9",
    "maintainer": "Jonathan V\u00f6lkle <jonathan.voelkle@web.de>",
    "author": "John Harrison [aut] (original author),\nJu Yeong Kim [aut] (rOpenSci maintainer),\nJonathan V\u00f6lkle [aut, cre],\nIndranil Gayen [ctb]",
    "url": "https://docs.ropensci.org/RSelenium/",
    "bug_reports": "https://github.com/ropensci/RSelenium/issues",
    "repository": "",
    "exports": [
      [
        "errorHandler"
      ],
      [
        "getChromeProfile"
      ],
      [
        "getFirefoxProfile"
      ],
      [
        "makeFirefoxProfile"
      ],
      [
        "remoteDriver"
      ],
      [
        "rsDriver"
      ],
      [
        "selKeys"
      ],
      [
        "webElement"
      ]
    ],
    "topics": [
      [
        "rselenium"
      ],
      [
        "selenium"
      ],
      [
        "webdriver"
      ]
    ],
    "score": 13.3534,
    "stars": 346,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "RSelenium R Bindings for 'Selenium WebDriver' Provides a set of R bindings for the 'Selenium 2.0\nWebDriver' (see <https://www.selenium.dev/documentation/> for\nmore information) using the 'JsonWireProtocol' (see\n<https://github.com/SeleniumHQ/selenium/wiki/JsonWireProtocol>\nfor more information). 'Selenium 2.0 WebDriver' allows driving\na web browser natively as a user would either locally or on a\nremote machine using the Selenium server it marks a leap\nforward in terms of web browser automation. Selenium automates\nweb browsers (commonly referred to as browsers). Using\nRSelenium you can automate browsers locally or remotely. errorHandler getChromeProfile getFirefoxProfile makeFirefoxProfile remoteDriver rsDriver selKeys webElement rselenium selenium webdriver"
  },
  {
    "id": 1348,
    "package_name": "tidyquant",
    "title": "Tidy Quantitative Financial Analysis",
    "description": "Bringing business and financial analysis to the\n'tidyverse'. The 'tidyquant' package provides a convenient\nwrapper to various 'xts', 'zoo', 'quantmod', 'TTR' and\n'PerformanceAnalytics' package functions and returns the\nobjects in the tidy 'tibble' format. The main advantage is\nbeing able to use quantitative functions with the 'tidyverse'\nfunctions including 'purrr', 'dplyr', 'tidyr', 'ggplot2',\n'lubridate', etc. See the 'tidyquant' website for more\ninformation, documentation and examples.",
    "version": "1.0.11.9000",
    "maintainer": "Matt Dancho <mdancho@business-science.io>",
    "author": "Matt Dancho [aut, cre],\nDavis Vaughan [aut]",
    "url": "https://business-science.github.io/tidyquant/,\nhttps://github.com/business-science/tidyquant",
    "bug_reports": "https://github.com/business-science/tidyquant/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "ABS"
      ],
      [
        "AS_DATE"
      ],
      [
        "AS_DATETIME"
      ],
      [
        "av_api_key"
      ],
      [
        "AVERAGE"
      ],
      [
        "AVERAGE_IFS"
      ],
      [
        "CEILING_DATE"
      ],
      [
        "CEILING_DAY"
      ],
      [
        "CEILING_MONTH"
      ],
      [
        "CEILING_QUARTER"
      ],
      [
        "CEILING_WEEK"
      ],
      [
        "CEILING_YEAR"
      ],
      [
        "CHANGE"
      ],
      [
        "CHANGE_FIRSTLAST"
      ],
      [
        "coord_x_date"
      ],
      [
        "coord_x_datetime"
      ],
      [
        "COR"
      ],
      [
        "COUNT"
      ],
      [
        "COUNT_DAYS"
      ],
      [
        "COUNT_IFS"
      ],
      [
        "COUNT_UNIQUE"
      ],
      [
        "COV"
      ],
      [
        "CREATE_IFS"
      ],
      [
        "CUMULATIVE_MAX"
      ],
      [
        "CUMULATIVE_MEAN"
      ],
      [
        "CUMULATIVE_MEDIAN"
      ],
      [
        "CUMULATIVE_MIN"
      ],
      [
        "CUMULATIVE_PRODUCT"
      ],
      [
        "CUMULATIVE_SUM"
      ],
      [
        "DATE"
      ],
      [
        "DATE_SEQUENCE"
      ],
      [
        "DATE_TO_DECIMAL"
      ],
      [
        "DATE_TO_NUMERIC"
      ],
      [
        "DATEVALUE"
      ],
      [
        "DAY"
      ],
      [
        "DMY"
      ],
      [
        "DMY_H"
      ],
      [
        "DMY_HM"
      ],
      [
        "DMY_HMS"
      ],
      [
        "DOM"
      ],
      [
        "DOW"
      ],
      [
        "EDATE"
      ],
      [
        "EOMONTH"
      ],
      [
        "EXP"
      ],
      [
        "FIRST"
      ],
      [
        "FLOOR_DATE"
      ],
      [
        "FLOOR_DAY"
      ],
      [
        "FLOOR_MONTH"
      ],
      [
        "FLOOR_QUARTER"
      ],
      [
        "FLOOR_WEEK"
      ],
      [
        "FLOOR_YEAR"
      ],
      [
        "FV"
      ],
      [
        "geom_barchart"
      ],
      [
        "geom_bbands"
      ],
      [
        "geom_bbands_"
      ],
      [
        "geom_candlestick"
      ],
      [
        "geom_ma"
      ],
      [
        "geom_ma_"
      ],
      [
        "HOLIDAY_SEQUENCE"
      ],
      [
        "HOLIDAY_TABLE"
      ],
      [
        "HOUR"
      ],
      [
        "IRR"
      ],
      [
        "LAG"
      ],
      [
        "LAST"
      ],
      [
        "LEAD"
      ],
      [
        "LOG"
      ],
      [
        "MAX"
      ],
      [
        "MAX_IFS"
      ],
      [
        "MDAY"
      ],
      [
        "MDY"
      ],
      [
        "MDY_H"
      ],
      [
        "MDY_HM"
      ],
      [
        "MDY_HMS"
      ],
      [
        "MEDIAN"
      ],
      [
        "MEDIAN_IFS"
      ],
      [
        "MIN"
      ],
      [
        "MIN_IFS"
      ],
      [
        "MINUTE"
      ],
      [
        "MONTH"
      ],
      [
        "MONTHDAY"
      ],
      [
        "NET_WORKDAYS"
      ],
      [
        "NOW"
      ],
      [
        "NPV"
      ],
      [
        "NTH"
      ],
      [
        "palette_dark"
      ],
      [
        "palette_green"
      ],
      [
        "palette_light"
      ],
      [
        "PCT_CHANGE"
      ],
      [
        "PCT_CHANGE_FIRSTLAST"
      ],
      [
        "pivot_table"
      ],
      [
        "PMT"
      ],
      [
        "PV"
      ],
      [
        "QDAY"
      ],
      [
        "quandl_api_key"
      ],
      [
        "quandl_search"
      ],
      [
        "QUARTER"
      ],
      [
        "QUARTERDAY"
      ],
      [
        "RATE"
      ],
      [
        "RETURN"
      ],
      [
        "ROUND_DATE"
      ],
      [
        "ROUND_DAY"
      ],
      [
        "ROUND_MONTH"
      ],
      [
        "ROUND_QUARTER"
      ],
      [
        "ROUND_WEEK"
      ],
      [
        "ROUND_YEAR"
      ],
      [
        "scale_color_tq"
      ],
      [
        "scale_colour_tq"
      ],
      [
        "scale_fill_tq"
      ],
      [
        "SECOND"
      ],
      [
        "SQRT"
      ],
      [
        "STDEV"
      ],
      [
        "SUM"
      ],
      [
        "SUM_IFS"
      ],
      [
        "theme_tq"
      ],
      [
        "theme_tq_dark"
      ],
      [
        "theme_tq_green"
      ],
      [
        "tidyquant_conflicts"
      ],
      [
        "tiingo_api_key"
      ],
      [
        "TODAY"
      ],
      [
        "tq_exchange"
      ],
      [
        "tq_exchange_options"
      ],
      [
        "tq_fund_holdings"
      ],
      [
        "tq_fund_source_options"
      ],
      [
        "tq_get"
      ],
      [
        "tq_get_options"
      ],
      [
        "tq_index"
      ],
      [
        "tq_index_options"
      ],
      [
        "tq_mutate"
      ],
      [
        "tq_mutate_"
      ],
      [
        "tq_mutate_fun_options"
      ],
      [
        "tq_mutate_xy"
      ],
      [
        "tq_mutate_xy_"
      ],
      [
        "tq_performance"
      ],
      [
        "tq_performance_"
      ],
      [
        "tq_performance_fun_options"
      ],
      [
        "tq_portfolio"
      ],
      [
        "tq_portfolio_"
      ],
      [
        "tq_repeat_df"
      ],
      [
        "tq_transform"
      ],
      [
        "tq_transform_xy"
      ],
      [
        "tq_transmute"
      ],
      [
        "tq_transmute_"
      ],
      [
        "tq_transmute_fun_options"
      ],
      [
        "tq_transmute_xy"
      ],
      [
        "tq_transmute_xy_"
      ],
      [
        "VAR"
      ],
      [
        "VLOOKUP"
      ],
      [
        "WDAY"
      ],
      [
        "WEEK"
      ],
      [
        "WEEKDAY"
      ],
      [
        "WEEKNUM"
      ],
      [
        "WEEKNUM_ISO"
      ],
      [
        "WORKDAY_SEQUENCE"
      ],
      [
        "YEAR"
      ],
      [
        "YEAR_ISO"
      ],
      [
        "YEARFRAC"
      ],
      [
        "YMD"
      ],
      [
        "YMD_H"
      ],
      [
        "YMD_HM"
      ],
      [
        "YMD_HMS"
      ]
    ],
    "topics": [
      [
        "dplyr"
      ],
      [
        "financial-analysis"
      ],
      [
        "financial-data"
      ],
      [
        "financial-statements"
      ],
      [
        "multiple-stocks"
      ],
      [
        "performance-analysis"
      ],
      [
        "performanceanalytics"
      ],
      [
        "quantmod"
      ],
      [
        "stock"
      ],
      [
        "stock-exchanges"
      ],
      [
        "stock-indexes"
      ],
      [
        "stock-lists"
      ],
      [
        "stock-performance"
      ],
      [
        "stock-prices"
      ],
      [
        "stock-symbol"
      ],
      [
        "tidyverse"
      ],
      [
        "time-series"
      ],
      [
        "timeseries"
      ],
      [
        "xts"
      ]
    ],
    "score": 13.2668,
    "stars": 897,
    "primary_category": "tidyverse",
    "source_universe": "business-science",
    "search_text": "tidyquant Tidy Quantitative Financial Analysis Bringing business and financial analysis to the\n'tidyverse'. The 'tidyquant' package provides a convenient\nwrapper to various 'xts', 'zoo', 'quantmod', 'TTR' and\n'PerformanceAnalytics' package functions and returns the\nobjects in the tidy 'tibble' format. The main advantage is\nbeing able to use quantitative functions with the 'tidyverse'\nfunctions including 'purrr', 'dplyr', 'tidyr', 'ggplot2',\n'lubridate', etc. See the 'tidyquant' website for more\ninformation, documentation and examples. %>% ABS AS_DATE AS_DATETIME av_api_key AVERAGE AVERAGE_IFS CEILING_DATE CEILING_DAY CEILING_MONTH CEILING_QUARTER CEILING_WEEK CEILING_YEAR CHANGE CHANGE_FIRSTLAST coord_x_date coord_x_datetime COR COUNT COUNT_DAYS COUNT_IFS COUNT_UNIQUE COV CREATE_IFS CUMULATIVE_MAX CUMULATIVE_MEAN CUMULATIVE_MEDIAN CUMULATIVE_MIN CUMULATIVE_PRODUCT CUMULATIVE_SUM DATE DATE_SEQUENCE DATE_TO_DECIMAL DATE_TO_NUMERIC DATEVALUE DAY DMY DMY_H DMY_HM DMY_HMS DOM DOW EDATE EOMONTH EXP FIRST FLOOR_DATE FLOOR_DAY FLOOR_MONTH FLOOR_QUARTER FLOOR_WEEK FLOOR_YEAR FV geom_barchart geom_bbands geom_bbands_ geom_candlestick geom_ma geom_ma_ HOLIDAY_SEQUENCE HOLIDAY_TABLE HOUR IRR LAG LAST LEAD LOG MAX MAX_IFS MDAY MDY MDY_H MDY_HM MDY_HMS MEDIAN MEDIAN_IFS MIN MIN_IFS MINUTE MONTH MONTHDAY NET_WORKDAYS NOW NPV NTH palette_dark palette_green palette_light PCT_CHANGE PCT_CHANGE_FIRSTLAST pivot_table PMT PV QDAY quandl_api_key quandl_search QUARTER QUARTERDAY RATE RETURN ROUND_DATE ROUND_DAY ROUND_MONTH ROUND_QUARTER ROUND_WEEK ROUND_YEAR scale_color_tq scale_colour_tq scale_fill_tq SECOND SQRT STDEV SUM SUM_IFS theme_tq theme_tq_dark theme_tq_green tidyquant_conflicts tiingo_api_key TODAY tq_exchange tq_exchange_options tq_fund_holdings tq_fund_source_options tq_get tq_get_options tq_index tq_index_options tq_mutate tq_mutate_ tq_mutate_fun_options tq_mutate_xy tq_mutate_xy_ tq_performance tq_performance_ tq_performance_fun_options tq_portfolio tq_portfolio_ tq_repeat_df tq_transform tq_transform_xy tq_transmute tq_transmute_ tq_transmute_fun_options tq_transmute_xy tq_transmute_xy_ VAR VLOOKUP WDAY WEEK WEEKDAY WEEKNUM WEEKNUM_ISO WORKDAY_SEQUENCE YEAR YEAR_ISO YEARFRAC YMD YMD_H YMD_HM YMD_HMS dplyr financial-analysis financial-data financial-statements multiple-stocks performance-analysis performanceanalytics quantmod stock stock-exchanges stock-indexes stock-lists stock-performance stock-prices stock-symbol tidyverse time-series timeseries xts"
  },
  {
    "id": 508,
    "package_name": "easystats",
    "title": "Framework for Easy Statistical Modeling, Visualization, and\nReporting",
    "description": "A meta-package that installs and loads a set of packages\nfrom 'easystats' ecosystem in a single step. This collection of\npackages provide a unifying and consistent framework for\nstatistical modeling, visualization, and reporting.\nAdditionally, it provides articles targeted at instructors for\nteaching 'easystats', and a dashboard targeted at new R users\nfor easily conducting statistical analysis by accessing summary\nresults, model fit indices, and visualizations with minimal\nprogramming.",
    "version": "0.7.5.2",
    "maintainer": "Daniel L\u00fcdecke <d.luedecke@uke.de>",
    "author": "Daniel L\u00fcdecke [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-8895-3206>),\nDominique Makowski [aut] (ORCID:\n<https://orcid.org/0000-0001-5375-9967>),\nMattan S. Ben-Shachar [aut] (ORCID:\n<https://orcid.org/0000-0002-4287-4801>),\nIndrajeet Patil [aut] (ORCID: <https://orcid.org/0000-0003-1995-6531>),\nBrenton M. Wiernik [aut] (ORCID:\n<https://orcid.org/0000-0001-9560-6336>),\nEtienne Bacher [aut] (ORCID: <https://orcid.org/0000-0002-9271-5075>),\nR\u00e9mi Th\u00e9riault [aut] (ORCID: <https://orcid.org/0000-0003-4315-6788>)",
    "url": "https://easystats.github.io/easystats/",
    "bug_reports": "https://github.com/easystats/easystats/issues",
    "repository": "",
    "exports": [
      [
        "display"
      ],
      [
        "easystats_citations"
      ],
      [
        "easystats_downloads"
      ],
      [
        "easystats_packages"
      ],
      [
        "easystats_update"
      ],
      [
        "easystats_zen"
      ],
      [
        "install_latest"
      ],
      [
        "install_suggested"
      ],
      [
        "model_dashboard"
      ],
      [
        "print_html"
      ],
      [
        "print_md"
      ],
      [
        "show_reverse_dependencies"
      ],
      [
        "show_suggested"
      ]
    ],
    "topics": [
      [
        "dataanalytics"
      ],
      [
        "datascience"
      ],
      [
        "easystats"
      ],
      [
        "hacktoberfest"
      ],
      [
        "models"
      ],
      [
        "performance-metrics"
      ],
      [
        "regression-models"
      ],
      [
        "statistics"
      ]
    ],
    "score": 13.1228,
    "stars": 1146,
    "primary_category": "statistics",
    "source_universe": "easystats",
    "search_text": "easystats Framework for Easy Statistical Modeling, Visualization, and\nReporting A meta-package that installs and loads a set of packages\nfrom 'easystats' ecosystem in a single step. This collection of\npackages provide a unifying and consistent framework for\nstatistical modeling, visualization, and reporting.\nAdditionally, it provides articles targeted at instructors for\nteaching 'easystats', and a dashboard targeted at new R users\nfor easily conducting statistical analysis by accessing summary\nresults, model fit indices, and visualizations with minimal\nprogramming. display easystats_citations easystats_downloads easystats_packages easystats_update easystats_zen install_latest install_suggested model_dashboard print_html print_md show_reverse_dependencies show_suggested dataanalytics datascience easystats hacktoberfest models performance-metrics regression-models statistics"
  },
  {
    "id": 579,
    "package_name": "flexdashboard",
    "title": "R Markdown Format for Flexible Dashboards",
    "description": "Format for converting an R Markdown document to a grid\noriented dashboard. The dashboard flexibly adapts the size of\nit's components to the containing web page.",
    "version": "0.6.2.9000",
    "maintainer": "Garrick Aden-Buie <garrick@posit.co>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7111-0077>),\nCarson Sievert [aut] (ORCID: <https://orcid.org/0000-0002-4958-2844>),\nRichard Iannone [aut] (ORCID: <https://orcid.org/0000-0003-3925-190X>),\nJJ Allaire [aut],\nBarbara Borges [aut],\nPosit Software, PBC [cph, fnd],\nKeen IO [ctb, cph] (Dashboard CSS),\nAbdullah Almsaeed [ctb, cph] (Dashboard CSS),\nJonas Mosbech [ctb, cph] (StickyTableHeaders),\nNoel Bossart [ctb, cph] (Featherlight),\nLea Verou [ctb, cph] (Prism),\nDmitry Baranovskiy [ctb, cph] (Raphael.js),\nSencha Labs [ctb, cph] (Raphael.js),\nBojan Djuricic [ctb, cph] (JustGage),\nTomas Sardyha [ctb, cph] (Sly),\nBryan Lewis [ctb, cph] (Examples),\nJoshua Kunst [ctb, cph] (Examples),\nRyan Hafen [ctb, cph] (Examples),\nBob Rudis [ctb, cph] (Examples),\nJoe Cheng [ctb] (Examples)",
    "url": "https://pkgs.rstudio.com/flexdashboard/,\nhttps://github.com/rstudio/flexdashboard/",
    "bug_reports": "https://github.com/rstudio/flexdashboard/issues",
    "repository": "",
    "exports": [
      [
        "flex_dashboard"
      ],
      [
        "gauge"
      ],
      [
        "gaugeOutput"
      ],
      [
        "gaugeSectors"
      ],
      [
        "renderGauge"
      ],
      [
        "renderValueBox"
      ],
      [
        "valueBox"
      ],
      [
        "valueBoxOutput"
      ]
    ],
    "topics": [],
    "score": 13.1,
    "stars": 842,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "flexdashboard R Markdown Format for Flexible Dashboards Format for converting an R Markdown document to a grid\noriented dashboard. The dashboard flexibly adapts the size of\nit's components to the containing web page. flex_dashboard gauge gaugeOutput gaugeSectors renderGauge renderValueBox valueBox valueBoxOutput "
  },
  {
    "id": 1360,
    "package_name": "tinytable",
    "title": "Simple and Configurable Tables in 'HTML', 'LaTeX', 'Markdown',\n'Word', 'PNG', 'PDF', and 'Typst' Formats",
    "description": "Create highly customized tables with this simple and\ndependency-free package. Data frames can be converted to\n'HTML', 'LaTeX', 'Markdown', 'Word', 'PNG', 'PDF', or 'Typst'\ntables. The user interface is minimalist and easy to learn. The\nsyntax is concise. 'HTML' tables can be customized using the\nflexible 'Bootstrap' framework, and 'LaTeX' code with the\n'tabularray' package.",
    "version": "0.15.1.7",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "author": "Vincent Arel-Bundock [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>)",
    "url": "https://vincentarelbundock.github.io/tinytable/",
    "bug_reports": "https://github.com/vincentarelbundock/tinytable/issues",
    "repository": "",
    "exports": [
      [
        "colnames"
      ],
      [
        "colnames<-"
      ],
      [
        "format_tt"
      ],
      [
        "format_vector"
      ],
      [
        "group_tt"
      ],
      [
        "knit_print.tinytable"
      ],
      [
        "plot_tt"
      ],
      [
        "plot_vector"
      ],
      [
        "rbind2"
      ],
      [
        "save_tt"
      ],
      [
        "style_tt"
      ],
      [
        "style_vector"
      ],
      [
        "theme_default"
      ],
      [
        "theme_empty"
      ],
      [
        "theme_grid"
      ],
      [
        "theme_html"
      ],
      [
        "theme_latex"
      ],
      [
        "theme_markdown"
      ],
      [
        "theme_revealjs"
      ],
      [
        "theme_rotate"
      ],
      [
        "theme_striped"
      ],
      [
        "theme_tt"
      ],
      [
        "theme_typst"
      ],
      [
        "tt"
      ],
      [
        "tt_format"
      ],
      [
        "tt_group"
      ],
      [
        "tt_plot"
      ],
      [
        "tt_save"
      ],
      [
        "tt_style"
      ]
    ],
    "topics": [],
    "score": 13.0129,
    "stars": 318,
    "primary_category": "statistics",
    "source_universe": "vincentarelbundock",
    "search_text": "tinytable Simple and Configurable Tables in 'HTML', 'LaTeX', 'Markdown',\n'Word', 'PNG', 'PDF', and 'Typst' Formats Create highly customized tables with this simple and\ndependency-free package. Data frames can be converted to\n'HTML', 'LaTeX', 'Markdown', 'Word', 'PNG', 'PDF', or 'Typst'\ntables. The user interface is minimalist and easy to learn. The\nsyntax is concise. 'HTML' tables can be customized using the\nflexible 'Bootstrap' framework, and 'LaTeX' code with the\n'tabularray' package. colnames colnames<- format_tt format_vector group_tt knit_print.tinytable plot_tt plot_vector rbind2 save_tt style_tt style_vector theme_default theme_empty theme_grid theme_html theme_latex theme_markdown theme_revealjs theme_rotate theme_striped theme_tt theme_typst tt tt_format tt_group tt_plot tt_save tt_style "
  },
  {
    "id": 1071,
    "package_name": "readODS",
    "title": "Read and Write ODS Files",
    "description": "Read ODS (OpenDocument Spreadsheet) into R as data frame.\nAlso support writing data frame into ODS file.",
    "version": "2.3.4",
    "maintainer": "Chung-hong Chan <chainsawtiney@gmail.com>",
    "author": "Gerrit-Jan Schutten [aut],\nChung-hong Chan [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6232-7530>),\nPeter Brohan [aut],\nDetlef Steuer [aut] (ORCID: <https://orcid.org/0000-0003-2676-5290>),\nThomas J. Leeper [aut] (ORCID: <https://orcid.org/0000-0003-4097-6326>),\nJohn Foster [ctb],\nSergio Oller [ctb],\nJim Hester [ctb] (ORCID: <https://orcid.org/0000-0002-2739-7082>),\nStephen Watts [ctb],\nArthur Katossky [ctb],\nStas Malavin [ctb],\nDuncan Garmonsway [ctb],\nMehrad Mahmoudian [ctb],\nMatt Kerlogue [ctb],\nMichal Lauer [ctb],\nTill Straube [ctb],\nMauricio Vargas Sepulveda [ctb],\nMarcin Kalicinski [ctb, cph] (Author of included RapidXML code)",
    "url": "https://docs.ropensci.org/readODS/,\nhttps://github.com/ropensci/readODS",
    "bug_reports": "https://github.com/ropensci/readODS/issues",
    "repository": "",
    "exports": [
      [
        "list_fods_sheets"
      ],
      [
        "list_ods_sheets"
      ],
      [
        "ods_sheets"
      ],
      [
        "read_fods"
      ],
      [
        "read_ods"
      ],
      [
        "readODS_progress"
      ],
      [
        "write_fods"
      ],
      [
        "write_ods"
      ]
    ],
    "topics": [
      [
        "cpp"
      ]
    ],
    "score": 12.8084,
    "stars": 58,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "readODS Read and Write ODS Files Read ODS (OpenDocument Spreadsheet) into R as data frame.\nAlso support writing data frame into ODS file. list_fods_sheets list_ods_sheets ods_sheets read_fods read_ods readODS_progress write_fods write_ods cpp"
  },
  {
    "id": 490,
    "package_name": "downlit",
    "title": "Syntax Highlighting and Automatic Linking",
    "description": "Syntax highlighting of R code, specifically designed for\nthe needs of 'RMarkdown' packages like 'pkgdown', 'hugodown',\nand 'bookdown'. It includes linking of function calls to their\ndocumentation on the web, and automatic translation of ANSI\nescapes in output to the equivalent HTML.",
    "version": "0.4.5.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre],\nPosit Software, PBC [cph, fnd]",
    "url": "https://downlit.r-lib.org/, https://github.com/r-lib/downlit",
    "bug_reports": "https://github.com/r-lib/downlit/issues",
    "repository": "",
    "exports": [
      [
        "autolink"
      ],
      [
        "autolink_url"
      ],
      [
        "classes_chroma"
      ],
      [
        "classes_pandoc"
      ],
      [
        "downlit_html_node"
      ],
      [
        "downlit_html_path"
      ],
      [
        "downlit_md_path"
      ],
      [
        "downlit_md_string"
      ],
      [
        "evaluate_and_highlight"
      ],
      [
        "highlight"
      ],
      [
        "href_article"
      ],
      [
        "href_package"
      ],
      [
        "href_topic"
      ],
      [
        "is_low_change"
      ]
    ],
    "topics": [
      [
        "syntax-highlighting"
      ]
    ],
    "score": 12.552,
    "stars": 90,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "downlit Syntax Highlighting and Automatic Linking Syntax highlighting of R code, specifically designed for\nthe needs of 'RMarkdown' packages like 'pkgdown', 'hugodown',\nand 'bookdown'. It includes linking of function calls to their\ndocumentation on the web, and automatic translation of ANSI\nescapes in output to the equivalent HTML. autolink autolink_url classes_chroma classes_pandoc downlit_html_node downlit_html_path downlit_md_path downlit_md_string evaluate_and_highlight highlight href_article href_package href_topic is_low_change syntax-highlighting"
  },
  {
    "id": 165,
    "package_name": "RefManageR",
    "title": "Straightforward 'BibTeX' and 'BibLaTeX' Bibliography Management",
    "description": "Provides tools for importing and working with\nbibliographic references. It greatly enhances the 'bibentry'\nclass by providing a class 'BibEntry' which stores 'BibTeX' and\n'BibLaTeX' references, supports 'UTF-8' encoding, and can be\neasily searched by any field, by date ranges, and by various\nformats for name lists (author by last names, translator by\nfull names, etc.). Entries can be updated, combined, sorted,\nprinted in a number of styles, and exported. 'BibTeX' and\n'BibLaTeX' '.bib' files can be read into 'R' and converted to\n'BibEntry' objects. Interfaces to 'NCBI Entrez', 'CrossRef',\nand 'Zotero' are provided for importing references and\nreferences can be created from locally stored 'PDF' files using\n'Poppler'. Includes functions for citing and generating a\nbibliography with hyperlinks for documents prepared with\n'RMarkdown' or 'RHTML'.",
    "version": "1.4.3",
    "maintainer": "Mathew W. McLean <mathew.w.mclean@gmail.com>",
    "author": "Mathew W. McLean [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7891-9645>),\nAndy Bunn [ctb] (function latexify used by toBiblatex)",
    "url": "https://github.com/ropensci/RefManageR/,\nhttps://docs.ropensci.org/RefManageR/",
    "bug_reports": "https://github.com/ropensci/RefManageR/issues",
    "repository": "",
    "exports": [
      [
        "as.BibEntry"
      ],
      [
        "AutoCite"
      ],
      [
        "BibEntry"
      ],
      [
        "BibOptions"
      ],
      [
        "Cite"
      ],
      [
        "Citep"
      ],
      [
        "Citet"
      ],
      [
        "fields"
      ],
      [
        "GetBibEntryWithDOI"
      ],
      [
        "GetPubMedByID"
      ],
      [
        "GetPubMedRelated"
      ],
      [
        "is.BibEntry"
      ],
      [
        "LookupPubMedID"
      ],
      [
        "NoCite"
      ],
      [
        "PrintBibliography"
      ],
      [
        "ReadBib"
      ],
      [
        "ReadCrossRef"
      ],
      [
        "ReadGS"
      ],
      [
        "ReadPDFs"
      ],
      [
        "ReadPubMed"
      ],
      [
        "ReadZotero"
      ],
      [
        "RelistBibEntry"
      ],
      [
        "SearchBib"
      ],
      [
        "TextCite"
      ],
      [
        "toBiblatex"
      ],
      [
        "UpdateFieldName"
      ],
      [
        "WriteBib"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ]
    ],
    "score": 12.4288,
    "stars": 120,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "RefManageR Straightforward 'BibTeX' and 'BibLaTeX' Bibliography Management Provides tools for importing and working with\nbibliographic references. It greatly enhances the 'bibentry'\nclass by providing a class 'BibEntry' which stores 'BibTeX' and\n'BibLaTeX' references, supports 'UTF-8' encoding, and can be\neasily searched by any field, by date ranges, and by various\nformats for name lists (author by last names, translator by\nfull names, etc.). Entries can be updated, combined, sorted,\nprinted in a number of styles, and exported. 'BibTeX' and\n'BibLaTeX' '.bib' files can be read into 'R' and converted to\n'BibEntry' objects. Interfaces to 'NCBI Entrez', 'CrossRef',\nand 'Zotero' are provided for importing references and\nreferences can be created from locally stored 'PDF' files using\n'Poppler'. Includes functions for citing and generating a\nbibliography with hyperlinks for documents prepared with\n'RMarkdown' or 'RHTML'. as.BibEntry AutoCite BibEntry BibOptions Cite Citep Citet fields GetBibEntryWithDOI GetPubMedByID GetPubMedRelated is.BibEntry LookupPubMedID NoCite PrintBibliography ReadBib ReadCrossRef ReadGS ReadPDFs ReadPubMed ReadZotero RelistBibEntry SearchBib TextCite toBiblatex UpdateFieldName WriteBib peer-reviewed"
  },
  {
    "id": 1046,
    "package_name": "r2d3",
    "title": "Interface to 'D3' Visualizations",
    "description": "Suite of tools for using 'D3', a library for producing\ndynamic, interactive data visualizations. Supports translating\nobjects into 'D3' friendly data structures, rendering 'D3'\nscripts, publishing 'D3' visualizations, incorporating 'D3' in\nR Markdown, creating interactive 'D3' applications with Shiny,\nand distributing 'D3' based 'htmlwidgets' in R packages.",
    "version": "0.2.5",
    "maintainer": "Nick Strayer <nick.strayer@rstudio.com>",
    "author": "Nick Strayer [aut, cre],\nJavier Luraschi [aut],\nJJ Allaire [aut],\nMike Bostock [ctb, cph] (d3.js library, http://d3js.org),\nRStudio [cph]",
    "url": "https://rstudio.github.io/r2d3/, https://github.com/rstudio/r2d3",
    "bug_reports": "https://github.com/rstudio/r2d3/issues",
    "repository": "",
    "exports": [
      [
        "as_d3_data"
      ],
      [
        "d3Output"
      ],
      [
        "default_sizing"
      ],
      [
        "html_dependencies_d3"
      ],
      [
        "r2d3"
      ],
      [
        "read_json"
      ],
      [
        "renderD3"
      ],
      [
        "save_d3_html"
      ],
      [
        "save_d3_png"
      ],
      [
        "sizingPolicy"
      ]
    ],
    "topics": [
      [
        "d3"
      ],
      [
        "r2d3"
      ],
      [
        "visualization"
      ]
    ],
    "score": 12.3205,
    "stars": 525,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "r2d3 Interface to 'D3' Visualizations Suite of tools for using 'D3', a library for producing\ndynamic, interactive data visualizations. Supports translating\nobjects into 'D3' friendly data structures, rendering 'D3'\nscripts, publishing 'D3' visualizations, incorporating 'D3' in\nR Markdown, creating interactive 'D3' applications with Shiny,\nand distributing 'D3' based 'htmlwidgets' in R packages. as_d3_data d3Output default_sizing html_dependencies_d3 r2d3 read_json renderD3 save_d3_html save_d3_png sizingPolicy d3 r2d3 visualization"
  },
  {
    "id": 388,
    "package_name": "colourpicker",
    "title": "A Colour Picker Tool for Shiny and for Selecting Colours in\nPlots",
    "description": "A colour picker that can be used as an input in 'Shiny'\napps or Rmarkdown documents. The colour picker supports alpha\nopacity, custom colour palettes, and many more options. A Plot\nColour Helper tool is available as an 'RStudio' Addin, which\nhelps you pick colours to use in your plots. A more generic\nColour Picker 'RStudio' Addin is also provided to let you\nselect colours to use in your R code.",
    "version": "1.3.0",
    "maintainer": "Dean Attali <daattali@gmail.com>",
    "author": "Dean Attali [aut, cre] (ORCID: <https://orcid.org/0000-0002-5645-3493>),\nDavid Griswold [ctb]",
    "url": "https://github.com/daattali/colourpicker,\nhttps://daattali.com/shiny/colourInput/",
    "bug_reports": "https://github.com/daattali/colourpicker/issues",
    "repository": "",
    "exports": [
      [
        "colourInput"
      ],
      [
        "colourPicker"
      ],
      [
        "colourWidget"
      ],
      [
        "plotHelper"
      ],
      [
        "runExample"
      ],
      [
        "updateColourInput"
      ]
    ],
    "topics": [
      [
        "rstudio-addin"
      ],
      [
        "shiny"
      ],
      [
        "shiny-r"
      ]
    ],
    "score": 12.2983,
    "stars": 226,
    "primary_category": "visualization",
    "source_universe": "daattali",
    "search_text": "colourpicker A Colour Picker Tool for Shiny and for Selecting Colours in\nPlots A colour picker that can be used as an input in 'Shiny'\napps or Rmarkdown documents. The colour picker supports alpha\nopacity, custom colour palettes, and many more options. A Plot\nColour Helper tool is available as an 'RStudio' Addin, which\nhelps you pick colours to use in your plots. A more generic\nColour Picker 'RStudio' Addin is also provided to let you\nselect colours to use in your R code. colourInput colourPicker colourWidget plotHelper runExample updateColourInput rstudio-addin shiny shiny-r"
  },
  {
    "id": 1248,
    "package_name": "sortable",
    "title": "Drag-and-Drop in 'shiny' Apps with 'SortableJS'",
    "description": "Enables drag-and-drop behaviour in Shiny apps, by exposing\nthe functionality of the 'SortableJS'\n<https://sortablejs.github.io/Sortable/> JavaScript library as\nan 'htmlwidget'.  You can use this in Shiny apps and widgets,\n'learnr' tutorials as well as R Markdown. In addition, provides\na custom 'learnr' question type - 'question_rank()' - that\nallows ranking questions with drag-and-drop.",
    "version": "0.6.0",
    "maintainer": "Andrie de Vries <apdevries@gmail.com>",
    "author": "Andrie de Vries [cre, aut],\nBarret Schloerke [aut],\nKenton Russell [aut, ccp] (Original author),\nPosit [cph, fnd],\nLebedev Konstantin [cph] ('SortableJS',\nhttps://sortablejs.github.io/Sortable/)",
    "url": "https://rstudio.github.io/sortable/",
    "bug_reports": "https://github.com/rstudio/sortable/issues",
    "repository": "",
    "exports": [
      [
        "add_rank_list"
      ],
      [
        "bucket_list"
      ],
      [
        "chain_js_events"
      ],
      [
        "enable_modules"
      ],
      [
        "is_modules_enabled"
      ],
      [
        "is_sortable_options"
      ],
      [
        "question_rank"
      ],
      [
        "rank_list"
      ],
      [
        "render_sortable"
      ],
      [
        "sortable_js"
      ],
      [
        "sortable_js_capture_bucket_input"
      ],
      [
        "sortable_js_capture_input"
      ],
      [
        "sortable_options"
      ],
      [
        "sortable_output"
      ],
      [
        "update_bucket_list"
      ],
      [
        "update_rank_list"
      ]
    ],
    "topics": [
      [
        "htmlwidget"
      ]
    ],
    "score": 12.295,
    "stars": 135,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "sortable Drag-and-Drop in 'shiny' Apps with 'SortableJS' Enables drag-and-drop behaviour in Shiny apps, by exposing\nthe functionality of the 'SortableJS'\n<https://sortablejs.github.io/Sortable/> JavaScript library as\nan 'htmlwidget'.  You can use this in Shiny apps and widgets,\n'learnr' tutorials as well as R Markdown. In addition, provides\na custom 'learnr' question type - 'question_rank()' - that\nallows ranking questions with drag-and-drop. add_rank_list bucket_list chain_js_events enable_modules is_modules_enabled is_sortable_options question_rank rank_list render_sortable sortable_js sortable_js_capture_bucket_input sortable_js_capture_input sortable_options sortable_output update_bucket_list update_rank_list htmlwidget"
  },
  {
    "id": 1163,
    "package_name": "rticles",
    "title": "Article Formats for R Markdown",
    "description": "A suite of custom R Markdown formats and templates for\nauthoring journal articles and conference submissions.",
    "version": "0.27.12",
    "maintainer": "Christophe Dervieux <cderv@posit.co>",
    "author": "JJ Allaire [aut],\nYihui Xie [aut] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nChristophe Dervieux [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nPosit Software, PBC [cph, fnd],\nR Foundation [aut, cph],\nHadley Wickham [aut],\nJournal of Statistical Software [aut, cph],\nRamnath Vaidyanathan [aut, cph],\nAssociation for Computing Machinery [aut, cph],\nCarl Boettiger [aut, cph],\nElsevier [aut, cph],\nKarl Broman [aut, cph],\nKirill Mueller [aut, cph],\nBastiaan Quast [aut, cph],\nRandall Pruim [aut, cph],\nBen Marwick [aut, cph],\nCharlotte Wickham [aut, cph],\nOliver Keyes [aut, cph],\nMiao Yu [aut, cph],\nDaniel Emaasit [aut, cph],\nThierry Onkelinx [aut, cph],\nAlessandro Gasparini [aut, cph] (ORCID:\n<https://orcid.org/0000-0002-8319-7624>),\nMarc-Andre Desautels [aut, cph],\nDominik Leutnant [aut, cph] (ORCID:\n<https://orcid.org/0000-0003-3293-2315>),\nMDPI [aut, cph],\nTaylor and Francis [aut, cph],\nO\u011fuzhan \u00d6\u011freden [aut] (ORCID: <https://orcid.org/0000-0002-9949-3348>),\nDalton Hance [aut],\nDaniel N\u00fcst [aut, cph] (ORCID: <https://orcid.org/0000-0002-0024-5046>),\nPetter Uvesten [aut, cph],\nElio Campitelli [aut, cph] (ORCID:\n<https://orcid.org/0000-0002-7742-9230>),\nJohn Muschelli [aut, cph] (ORCID:\n<https://orcid.org/0000-0001-6469-1750>),\nAlex Hayes [aut] (ORCID: <https://orcid.org/0000-0002-4985-5160>),\nZhian N. Kamvar [aut, cph] (ORCID:\n<https://orcid.org/0000-0003-1458-7108>),\nNoam Ross [aut, cph] (ORCID: <https://orcid.org/0000-0002-2136-0000>),\nRobrecht Cannoodt [aut, cph] (ORCID:\n<https://orcid.org/0000-0003-3641-729X>, github: rcannood),\nDuncan Luguern [aut],\nDavid M. Kaplan [aut, ctb] (ORCID:\n<https://orcid.org/0000-0001-6087-359X>, github: dmkaplan2000),\nSebastian Kreutzer [aut] (ORCID:\n<https://orcid.org/0000-0002-0734-2199>),\nShixiang Wang [aut, ctb] (ORCID:\n<https://orcid.org/0000-0001-9855-7357>),\nJay Hesselberth [aut, ctb] (ORCID:\n<https://orcid.org/0000-0002-6299-179X>),\nAlfredo Hern\u00e1ndez [ctb] (ORCID:\n<https://orcid.org/0000-0002-2660-4545>),\nStefano Coretta [ctb] (ORCID: <https://orcid.org/0000-0001-9627-5532>,\ngithub: stefanocoretta),\nGreg Macfarlane [ctb] (github: gregmacfarlane),\nMatthias Templ [ctb] (ORCID: <https://orcid.org/0000-0002-8638-5276>,\ngithub: matthias-da),\nAlvaro Uzaheta [ctb] (github: auzaheta),\nJooYoung Seo [ctb] (ORCID: <https://orcid.org/0000-0002-4064-6012>),\nCallum Arnold [ctb] (github: arnold-c),\nRob Hyndman [aut] (ORCID: <https://orcid.org/0000-0002-2140-5352>),\nDmytro Perepolkin [ctb] (ORCID:\n<https://orcid.org/0000-0001-8558-6183>, github: dmi3kno),\nTom Palmer [ctb] (ORCID: <https://orcid.org/0000-0003-4655-4511>,\ngithub: remlapmot),\nRafael Laboissi\u00e8re [ctb] (ORCID:\n<https://orcid.org/0000-0002-2180-9250>, github: rlaboiss)",
    "url": "https://github.com/rstudio/rticles,\nhttps://pkgs.rstudio.com/rticles/",
    "bug_reports": "https://github.com/rstudio/rticles/issues",
    "repository": "",
    "exports": [
      [
        "acm_article"
      ],
      [
        "acs_article"
      ],
      [
        "aea_article"
      ],
      [
        "agu_article"
      ],
      [
        "ajs_article"
      ],
      [
        "amq_article"
      ],
      [
        "ams_article"
      ],
      [
        "arxiv_article"
      ],
      [
        "asa_article"
      ],
      [
        "bioinformatics_article"
      ],
      [
        "biometrics_article"
      ],
      [
        "copernicus_article"
      ],
      [
        "copernicus_journal_abbreviations"
      ],
      [
        "ctex"
      ],
      [
        "ctex_article"
      ],
      [
        "elsevier_article"
      ],
      [
        "frontiers_article"
      ],
      [
        "glossa_article"
      ],
      [
        "ieee_article"
      ],
      [
        "ims_article"
      ],
      [
        "informs_article"
      ],
      [
        "iop_article"
      ],
      [
        "isba_article"
      ],
      [
        "jasa_article"
      ],
      [
        "jedm_article"
      ],
      [
        "joss_article"
      ],
      [
        "journals"
      ],
      [
        "jss_article"
      ],
      [
        "lipics_article"
      ],
      [
        "lncs_article"
      ],
      [
        "mdpi_article"
      ],
      [
        "mnras_article"
      ],
      [
        "oup_article"
      ],
      [
        "peerj_article"
      ],
      [
        "pihph_article"
      ],
      [
        "plos_article"
      ],
      [
        "pnas_article"
      ],
      [
        "rjournal_article"
      ],
      [
        "rsos_article"
      ],
      [
        "rss_article"
      ],
      [
        "sage_article"
      ],
      [
        "sim_article"
      ],
      [
        "springer_article"
      ],
      [
        "string_to_table"
      ],
      [
        "tf_article"
      ],
      [
        "trb_article"
      ],
      [
        "wellcomeor_article"
      ]
    ],
    "topics": [
      [
        "article"
      ],
      [
        "journal"
      ],
      [
        "paper"
      ],
      [
        "rmarkdown"
      ]
    ],
    "score": 12.2005,
    "stars": 1533,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "rticles Article Formats for R Markdown A suite of custom R Markdown formats and templates for\nauthoring journal articles and conference submissions. acm_article acs_article aea_article agu_article ajs_article amq_article ams_article arxiv_article asa_article bioinformatics_article biometrics_article copernicus_article copernicus_journal_abbreviations ctex ctex_article elsevier_article frontiers_article glossa_article ieee_article ims_article informs_article iop_article isba_article jasa_article jedm_article joss_article journals jss_article lipics_article lncs_article mdpi_article mnras_article oup_article peerj_article pihph_article plos_article pnas_article rjournal_article rsos_article rss_article sage_article sim_article springer_article string_to_table tf_article trb_article wellcomeor_article article journal paper rmarkdown"
  },
  {
    "id": 589,
    "package_name": "fresh",
    "title": "Create Custom 'Bootstrap' Themes to Use in 'Shiny'",
    "description": "Customize 'Bootstrap' and 'Bootswatch' themes, like\ncolors, fonts, grid layout, to use in 'Shiny' applications,\n'rmarkdown' documents and 'flexdashboard'.",
    "version": "0.2.2",
    "maintainer": "Victor Perrier <victor.perrier@dreamrs.fr>",
    "author": "Victor Perrier [aut, cre, cph],\nFanny Meyer [aut],\nThomas Park [ctb, cph] (Bootswatch themes),\nMark Otto [ctb] (Bootstrap library),\nJacob Thornton [ctb] (Bootstrap library),\nBootstrap contributors [ctb] (Bootstrap library),\nTwitter, Inc [cph] (Bootstrap library),\nonkbear [ctb, cph] (admin-lte-2-sass),\nColorlib [ctb, cph] (AdminLTE)",
    "url": "https://github.com/dreamRs/fresh",
    "bug_reports": "https://github.com/dreamRs/fresh/issues",
    "repository": "",
    "exports": [
      [
        "adminlte_color"
      ],
      [
        "adminlte_global"
      ],
      [
        "adminlte_sidebar"
      ],
      [
        "adminlte_vars"
      ],
      [
        "bs_theme_nord_dark"
      ],
      [
        "bs_theme_nord_light"
      ],
      [
        "bs_vars"
      ],
      [
        "bs_vars_alert"
      ],
      [
        "bs_vars_badge"
      ],
      [
        "bs_vars_button"
      ],
      [
        "bs_vars_color"
      ],
      [
        "bs_vars_component"
      ],
      [
        "bs_vars_dropdown"
      ],
      [
        "bs_vars_file"
      ],
      [
        "bs_vars_font"
      ],
      [
        "bs_vars_global"
      ],
      [
        "bs_vars_input"
      ],
      [
        "bs_vars_modal"
      ],
      [
        "bs_vars_nav"
      ],
      [
        "bs_vars_navbar"
      ],
      [
        "bs_vars_panel"
      ],
      [
        "bs_vars_pills"
      ],
      [
        "bs_vars_progress"
      ],
      [
        "bs_vars_state"
      ],
      [
        "bs_vars_table"
      ],
      [
        "bs_vars_tabs"
      ],
      [
        "bs_vars_wells"
      ],
      [
        "bs4dash_button"
      ],
      [
        "bs4dash_color"
      ],
      [
        "bs4dash_font"
      ],
      [
        "bs4dash_layout"
      ],
      [
        "bs4dash_sidebar_dark"
      ],
      [
        "bs4dash_sidebar_light"
      ],
      [
        "bs4dash_status"
      ],
      [
        "bs4Dash_theme"
      ],
      [
        "bs4dash_vars"
      ],
      [
        "bs4dash_yiq"
      ],
      [
        "create_pretty"
      ],
      [
        "create_theme"
      ],
      [
        "search_vars"
      ],
      [
        "search_vars_adminlte2"
      ],
      [
        "search_vars_bs"
      ],
      [
        "search_vars_bs4dash"
      ],
      [
        "use_googlefont"
      ],
      [
        "use_pretty"
      ],
      [
        "use_theme"
      ],
      [
        "use_vars_template"
      ]
    ],
    "topics": [
      [
        "bootstrap"
      ],
      [
        "shiny"
      ],
      [
        "shiny-applications"
      ],
      [
        "shiny-themes"
      ]
    ],
    "score": 12.1303,
    "stars": 233,
    "primary_category": "visualization",
    "source_universe": "dreamrs",
    "search_text": "fresh Create Custom 'Bootstrap' Themes to Use in 'Shiny' Customize 'Bootstrap' and 'Bootswatch' themes, like\ncolors, fonts, grid layout, to use in 'Shiny' applications,\n'rmarkdown' documents and 'flexdashboard'. adminlte_color adminlte_global adminlte_sidebar adminlte_vars bs_theme_nord_dark bs_theme_nord_light bs_vars bs_vars_alert bs_vars_badge bs_vars_button bs_vars_color bs_vars_component bs_vars_dropdown bs_vars_file bs_vars_font bs_vars_global bs_vars_input bs_vars_modal bs_vars_nav bs_vars_navbar bs_vars_panel bs_vars_pills bs_vars_progress bs_vars_state bs_vars_table bs_vars_tabs bs_vars_wells bs4dash_button bs4dash_color bs4dash_font bs4dash_layout bs4dash_sidebar_dark bs4dash_sidebar_light bs4dash_status bs4Dash_theme bs4dash_vars bs4dash_yiq create_pretty create_theme search_vars search_vars_adminlte2 search_vars_bs search_vars_bs4dash use_googlefont use_pretty use_theme use_vars_template bootstrap shiny shiny-applications shiny-themes"
  },
  {
    "id": 1437,
    "package_name": "webshot2",
    "title": "Take Screenshots of Web Pages",
    "description": "Takes screenshots of web pages, including Shiny\napplications and R Markdown documents. 'webshot2' uses headless\nChrome or Chromium as the browser back-end.",
    "version": "0.1.2.9000",
    "maintainer": "Winston Chang <winston@posit.co>",
    "author": "Winston Chang [aut, cre],\nBarret Schloerke [ctb] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://rstudio.github.io/webshot2/,\nhttps://github.com/rstudio/webshot2",
    "bug_reports": "https://github.com/rstudio/webshot2/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "appshot"
      ],
      [
        "resize"
      ],
      [
        "rmdshot"
      ],
      [
        "shrink"
      ],
      [
        "webshot"
      ]
    ],
    "topics": [],
    "score": 11.8889,
    "stars": 119,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "webshot2 Take Screenshots of Web Pages Takes screenshots of web pages, including Shiny\napplications and R Markdown documents. 'webshot2' uses headless\nChrome or Chromium as the browser back-end. %>% appshot resize rmdshot shrink webshot "
  },
  {
    "id": 936,
    "package_name": "pagedown",
    "title": "Paginate the HTML Output of R Markdown with CSS for Print",
    "description": "Use the paged media properties in CSS and the JavaScript\nlibrary 'paged.js' to split the content of an HTML document\ninto discrete pages. Each page can have its page size, page\nnumbers, margin boxes, and running headers, etc. Applications\nof this package include books, letters, reports, papers,\nbusiness cards, resumes, and posters.",
    "version": "0.23.1",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nRomain Lesur [aut, cph] (ORCID:\n<https://orcid.org/0000-0002-0721-5595>),\nChristophe Dervieux [ctb] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nBrent Thorne [aut] (ORCID: <https://orcid.org/0000-0002-1099-3857>),\nXianying Tan [aut] (ORCID: <https://orcid.org/0000-0002-6072-3521>),\nAtsushi Yasumoto [ctb] (ORCID: <https://orcid.org/0000-0002-8335-495X>),\nPosit Software, PBC [cph, fnd],\nAdam Hyde [ctb] (paged.js in resources/js/),\nMin-Zhong Lu [ctb] (resume.css in resources/css/),\nZulko [ctb] (poster-relaxed.css in resources/css/)",
    "url": "https://github.com/rstudio/pagedown",
    "bug_reports": "https://github.com/rstudio/pagedown/issues",
    "repository": "",
    "exports": [
      [
        "book_crc"
      ],
      [
        "business_card"
      ],
      [
        "chrome_print"
      ],
      [
        "find_chrome"
      ],
      [
        "html_letter"
      ],
      [
        "html_paged"
      ],
      [
        "html_resume"
      ],
      [
        "jss_paged"
      ],
      [
        "poster_jacobs"
      ],
      [
        "poster_relaxed"
      ],
      [
        "thesis_paged"
      ]
    ],
    "topics": [
      [
        "css"
      ],
      [
        "html"
      ],
      [
        "paged-media"
      ],
      [
        "pdf"
      ],
      [
        "printing"
      ],
      [
        "typesetting"
      ]
    ],
    "score": 11.8253,
    "stars": 914,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "pagedown Paginate the HTML Output of R Markdown with CSS for Print Use the paged media properties in CSS and the JavaScript\nlibrary 'paged.js' to split the content of an HTML document\ninto discrete pages. Each page can have its page size, page\nnumbers, margin boxes, and running headers, etc. Applications\nof this package include books, letters, reports, papers,\nbusiness cards, resumes, and posters. book_crc business_card chrome_print find_chrome html_letter html_paged html_resume jss_paged poster_jacobs poster_relaxed thesis_paged css html paged-media pdf printing typesetting"
  },
  {
    "id": 998,
    "package_name": "pointblank",
    "title": "Data Validation and Organization of Metadata for Local and\nRemote Tables",
    "description": "Validate data in data frames, 'tibble' objects, 'Spark'\n'DataFrames', and database tables. Validation pipelines can be\nmade using easily-readable, consecutive validation steps. Upon\nexecution of the validation plan, several reporting options are\navailable. User-defined thresholds for failure rates allow for\nthe determination of appropriate reporting actions. Many other\nworkflows are available including an information management\nworkflow, where the aim is to record, collect, and generate\nuseful information on data tables.",
    "version": "0.12.3.9000",
    "maintainer": "Richard Iannone <rich@posit.co>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3925-190X>),\nMauricio Vargas [aut] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\nJune Choe [aut] (ORCID: <https://orcid.org/0000-0002-0701-921X>),\nOlivier Roy [ctb]",
    "url": "https://rstudio.github.io/pointblank/,\nhttps://github.com/rstudio/pointblank",
    "bug_reports": "https://github.com/rstudio/pointblank/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "action_levels"
      ],
      [
        "activate_steps"
      ],
      [
        "affix_date"
      ],
      [
        "affix_datetime"
      ],
      [
        "all_passed"
      ],
      [
        "between"
      ],
      [
        "case_when"
      ],
      [
        "col_count_match"
      ],
      [
        "col_exists"
      ],
      [
        "col_is_character"
      ],
      [
        "col_is_date"
      ],
      [
        "col_is_factor"
      ],
      [
        "col_is_integer"
      ],
      [
        "col_is_logical"
      ],
      [
        "col_is_numeric"
      ],
      [
        "col_is_posix"
      ],
      [
        "col_schema"
      ],
      [
        "col_schema_match"
      ],
      [
        "col_vals_between"
      ],
      [
        "col_vals_decreasing"
      ],
      [
        "col_vals_equal"
      ],
      [
        "col_vals_expr"
      ],
      [
        "col_vals_gt"
      ],
      [
        "col_vals_gte"
      ],
      [
        "col_vals_in_set"
      ],
      [
        "col_vals_increasing"
      ],
      [
        "col_vals_lt"
      ],
      [
        "col_vals_lte"
      ],
      [
        "col_vals_make_set"
      ],
      [
        "col_vals_make_subset"
      ],
      [
        "col_vals_not_between"
      ],
      [
        "col_vals_not_equal"
      ],
      [
        "col_vals_not_in_set"
      ],
      [
        "col_vals_not_null"
      ],
      [
        "col_vals_null"
      ],
      [
        "col_vals_regex"
      ],
      [
        "col_vals_within_spec"
      ],
      [
        "conjointly"
      ],
      [
        "create_agent"
      ],
      [
        "create_informant"
      ],
      [
        "create_multiagent"
      ],
      [
        "creds"
      ],
      [
        "creds_anonymous"
      ],
      [
        "creds_file"
      ],
      [
        "creds_key"
      ],
      [
        "db_tbl"
      ],
      [
        "deactivate_steps"
      ],
      [
        "draft_validation"
      ],
      [
        "email_blast"
      ],
      [
        "email_create"
      ],
      [
        "expect_col_count_match"
      ],
      [
        "expect_col_exists"
      ],
      [
        "expect_col_is_character"
      ],
      [
        "expect_col_is_date"
      ],
      [
        "expect_col_is_factor"
      ],
      [
        "expect_col_is_integer"
      ],
      [
        "expect_col_is_logical"
      ],
      [
        "expect_col_is_numeric"
      ],
      [
        "expect_col_is_posix"
      ],
      [
        "expect_col_schema_match"
      ],
      [
        "expect_col_vals_between"
      ],
      [
        "expect_col_vals_decreasing"
      ],
      [
        "expect_col_vals_equal"
      ],
      [
        "expect_col_vals_expr"
      ],
      [
        "expect_col_vals_gt"
      ],
      [
        "expect_col_vals_gte"
      ],
      [
        "expect_col_vals_in_set"
      ],
      [
        "expect_col_vals_increasing"
      ],
      [
        "expect_col_vals_lt"
      ],
      [
        "expect_col_vals_lte"
      ],
      [
        "expect_col_vals_make_set"
      ],
      [
        "expect_col_vals_make_subset"
      ],
      [
        "expect_col_vals_not_between"
      ],
      [
        "expect_col_vals_not_equal"
      ],
      [
        "expect_col_vals_not_in_set"
      ],
      [
        "expect_col_vals_not_null"
      ],
      [
        "expect_col_vals_null"
      ],
      [
        "expect_col_vals_regex"
      ],
      [
        "expect_col_vals_within_spec"
      ],
      [
        "expect_conjointly"
      ],
      [
        "expect_row_count_match"
      ],
      [
        "expect_rows_complete"
      ],
      [
        "expect_rows_distinct"
      ],
      [
        "expect_serially"
      ],
      [
        "expect_specially"
      ],
      [
        "expect_tbl_match"
      ],
      [
        "export_report"
      ],
      [
        "expr"
      ],
      [
        "file_tbl"
      ],
      [
        "from_github"
      ],
      [
        "get_agent_report"
      ],
      [
        "get_agent_x_list"
      ],
      [
        "get_data_extracts"
      ],
      [
        "get_informant_report"
      ],
      [
        "get_multiagent_report"
      ],
      [
        "get_sundered_data"
      ],
      [
        "get_tt_param"
      ],
      [
        "has_columns"
      ],
      [
        "incorporate"
      ],
      [
        "info_columns"
      ],
      [
        "info_columns_from_tbl"
      ],
      [
        "info_section"
      ],
      [
        "info_snippet"
      ],
      [
        "info_tabular"
      ],
      [
        "interrogate"
      ],
      [
        "log4r_step"
      ],
      [
        "read_disk_multiagent"
      ],
      [
        "remove_steps"
      ],
      [
        "row_count_match"
      ],
      [
        "rows_complete"
      ],
      [
        "rows_distinct"
      ],
      [
        "scan_data"
      ],
      [
        "serially"
      ],
      [
        "set_tbl"
      ],
      [
        "small_table_sqlite"
      ],
      [
        "snip_highest"
      ],
      [
        "snip_list"
      ],
      [
        "snip_lowest"
      ],
      [
        "snip_stats"
      ],
      [
        "specially"
      ],
      [
        "stock_msg_body"
      ],
      [
        "stock_msg_footer"
      ],
      [
        "stop_if_not"
      ],
      [
        "stop_on_fail"
      ],
      [
        "tbl_get"
      ],
      [
        "tbl_match"
      ],
      [
        "tbl_source"
      ],
      [
        "tbl_store"
      ],
      [
        "test_col_count_match"
      ],
      [
        "test_col_exists"
      ],
      [
        "test_col_is_character"
      ],
      [
        "test_col_is_date"
      ],
      [
        "test_col_is_factor"
      ],
      [
        "test_col_is_integer"
      ],
      [
        "test_col_is_logical"
      ],
      [
        "test_col_is_numeric"
      ],
      [
        "test_col_is_posix"
      ],
      [
        "test_col_schema_match"
      ],
      [
        "test_col_vals_between"
      ],
      [
        "test_col_vals_decreasing"
      ],
      [
        "test_col_vals_equal"
      ],
      [
        "test_col_vals_expr"
      ],
      [
        "test_col_vals_gt"
      ],
      [
        "test_col_vals_gte"
      ],
      [
        "test_col_vals_in_set"
      ],
      [
        "test_col_vals_increasing"
      ],
      [
        "test_col_vals_lt"
      ],
      [
        "test_col_vals_lte"
      ],
      [
        "test_col_vals_make_set"
      ],
      [
        "test_col_vals_make_subset"
      ],
      [
        "test_col_vals_not_between"
      ],
      [
        "test_col_vals_not_equal"
      ],
      [
        "test_col_vals_not_in_set"
      ],
      [
        "test_col_vals_not_null"
      ],
      [
        "test_col_vals_null"
      ],
      [
        "test_col_vals_regex"
      ],
      [
        "test_col_vals_within_spec"
      ],
      [
        "test_conjointly"
      ],
      [
        "test_row_count_match"
      ],
      [
        "test_rows_complete"
      ],
      [
        "test_rows_distinct"
      ],
      [
        "test_serially"
      ],
      [
        "test_specially"
      ],
      [
        "test_tbl_match"
      ],
      [
        "tt_string_info"
      ],
      [
        "tt_summary_stats"
      ],
      [
        "tt_tbl_colnames"
      ],
      [
        "tt_tbl_dims"
      ],
      [
        "tt_time_shift"
      ],
      [
        "tt_time_slice"
      ],
      [
        "validate_rmd"
      ],
      [
        "vars"
      ],
      [
        "warn_on_fail"
      ],
      [
        "write_testthat_file"
      ],
      [
        "x_read_disk"
      ],
      [
        "x_write_disk"
      ],
      [
        "yaml_agent_interrogate"
      ],
      [
        "yaml_agent_show_exprs"
      ],
      [
        "yaml_agent_string"
      ],
      [
        "yaml_exec"
      ],
      [
        "yaml_informant_incorporate"
      ],
      [
        "yaml_read_agent"
      ],
      [
        "yaml_read_informant"
      ],
      [
        "yaml_write"
      ]
    ],
    "topics": [
      [
        "data-assertions"
      ],
      [
        "data-checker"
      ],
      [
        "data-dictionaries"
      ],
      [
        "data-frames"
      ],
      [
        "data-inference"
      ],
      [
        "data-management"
      ],
      [
        "data-profiler"
      ],
      [
        "data-quality"
      ],
      [
        "data-validation"
      ],
      [
        "data-verification"
      ],
      [
        "database-tables"
      ],
      [
        "easy-to-understand"
      ],
      [
        "reporting-tool"
      ],
      [
        "schema-validation"
      ],
      [
        "testing-tools"
      ],
      [
        "yaml-configuration"
      ]
    ],
    "score": 11.5175,
    "stars": 1016,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "pointblank Data Validation and Organization of Metadata for Local and\nRemote Tables Validate data in data frames, 'tibble' objects, 'Spark'\n'DataFrames', and database tables. Validation pipelines can be\nmade using easily-readable, consecutive validation steps. Upon\nexecution of the validation plan, several reporting options are\navailable. User-defined thresholds for failure rates allow for\nthe determination of appropriate reporting actions. Many other\nworkflows are available including an information management\nworkflow, where the aim is to record, collect, and generate\nuseful information on data tables. %>% action_levels activate_steps affix_date affix_datetime all_passed between case_when col_count_match col_exists col_is_character col_is_date col_is_factor col_is_integer col_is_logical col_is_numeric col_is_posix col_schema col_schema_match col_vals_between col_vals_decreasing col_vals_equal col_vals_expr col_vals_gt col_vals_gte col_vals_in_set col_vals_increasing col_vals_lt col_vals_lte col_vals_make_set col_vals_make_subset col_vals_not_between col_vals_not_equal col_vals_not_in_set col_vals_not_null col_vals_null col_vals_regex col_vals_within_spec conjointly create_agent create_informant create_multiagent creds creds_anonymous creds_file creds_key db_tbl deactivate_steps draft_validation email_blast email_create expect_col_count_match expect_col_exists expect_col_is_character expect_col_is_date expect_col_is_factor expect_col_is_integer expect_col_is_logical expect_col_is_numeric expect_col_is_posix expect_col_schema_match expect_col_vals_between expect_col_vals_decreasing expect_col_vals_equal expect_col_vals_expr expect_col_vals_gt expect_col_vals_gte expect_col_vals_in_set expect_col_vals_increasing expect_col_vals_lt expect_col_vals_lte expect_col_vals_make_set expect_col_vals_make_subset expect_col_vals_not_between expect_col_vals_not_equal expect_col_vals_not_in_set expect_col_vals_not_null expect_col_vals_null expect_col_vals_regex expect_col_vals_within_spec expect_conjointly expect_row_count_match expect_rows_complete expect_rows_distinct expect_serially expect_specially expect_tbl_match export_report expr file_tbl from_github get_agent_report get_agent_x_list get_data_extracts get_informant_report get_multiagent_report get_sundered_data get_tt_param has_columns incorporate info_columns info_columns_from_tbl info_section info_snippet info_tabular interrogate log4r_step read_disk_multiagent remove_steps row_count_match rows_complete rows_distinct scan_data serially set_tbl small_table_sqlite snip_highest snip_list snip_lowest snip_stats specially stock_msg_body stock_msg_footer stop_if_not stop_on_fail tbl_get tbl_match tbl_source tbl_store test_col_count_match test_col_exists test_col_is_character test_col_is_date test_col_is_factor test_col_is_integer test_col_is_logical test_col_is_numeric test_col_is_posix test_col_schema_match test_col_vals_between test_col_vals_decreasing test_col_vals_equal test_col_vals_expr test_col_vals_gt test_col_vals_gte test_col_vals_in_set test_col_vals_increasing test_col_vals_lt test_col_vals_lte test_col_vals_make_set test_col_vals_make_subset test_col_vals_not_between test_col_vals_not_equal test_col_vals_not_in_set test_col_vals_not_null test_col_vals_null test_col_vals_regex test_col_vals_within_spec test_conjointly test_row_count_match test_rows_complete test_rows_distinct test_serially test_specially test_tbl_match tt_string_info tt_summary_stats tt_tbl_colnames tt_tbl_dims tt_time_shift tt_time_slice validate_rmd vars warn_on_fail write_testthat_file x_read_disk x_write_disk yaml_agent_interrogate yaml_agent_show_exprs yaml_agent_string yaml_exec yaml_informant_incorporate yaml_read_agent yaml_read_informant yaml_write data-assertions data-checker data-dictionaries data-frames data-inference data-management data-profiler data-quality data-validation data-verification database-tables easy-to-understand reporting-tool schema-validation testing-tools yaml-configuration"
  },
  {
    "id": 1205,
    "package_name": "shinybusy",
    "title": "Busy Indicators and Notifications for 'Shiny' Applications",
    "description": "Add indicators (spinner, progress bar, gif) in your\n'shiny' applications to show the user that the server is busy.\nAnd other tools to let your users know something is happening\n(send notifications, reports, ...).",
    "version": "0.3.3",
    "maintainer": "Victor Perrier <victor.perrier@dreamrs.fr>",
    "author": "Fanny Meyer [aut],\nVictor Perrier [aut, cre],\nSilex Technologies [fnd] (https://www.silex-ip.com)",
    "url": "https://github.com/dreamRs/shinybusy,\nhttps://dreamrs.github.io/shinybusy/",
    "bug_reports": "https://github.com/dreamRs/shinybusy/issues",
    "repository": "",
    "exports": [
      [
        "add_busy_bar"
      ],
      [
        "add_busy_gif"
      ],
      [
        "add_busy_spinner"
      ],
      [
        "add_loading_state"
      ],
      [
        "block"
      ],
      [
        "block_output"
      ],
      [
        "busy_start_up"
      ],
      [
        "config_notify"
      ],
      [
        "config_report"
      ],
      [
        "hide_spinner"
      ],
      [
        "html_dependency_block"
      ],
      [
        "html_dependency_busy"
      ],
      [
        "html_dependency_epic"
      ],
      [
        "html_dependency_freezeframe"
      ],
      [
        "html_dependency_loading"
      ],
      [
        "html_dependency_nanobar"
      ],
      [
        "html_dependency_notiflix"
      ],
      [
        "html_dependency_notify"
      ],
      [
        "html_dependency_report"
      ],
      [
        "html_dependency_shinybusy"
      ],
      [
        "html_dependency_spinkit"
      ],
      [
        "html_dependency_startup"
      ],
      [
        "logo_silex"
      ],
      [
        "notify"
      ],
      [
        "notify_failure"
      ],
      [
        "notify_info"
      ],
      [
        "notify_success"
      ],
      [
        "notify_warning"
      ],
      [
        "play_gif"
      ],
      [
        "progress_circle"
      ],
      [
        "progress_line"
      ],
      [
        "progress_semicircle"
      ],
      [
        "remove_modal_gif"
      ],
      [
        "remove_modal_progress"
      ],
      [
        "remove_modal_spinner"
      ],
      [
        "remove_start_up"
      ],
      [
        "report"
      ],
      [
        "report_failure"
      ],
      [
        "report_info"
      ],
      [
        "report_success"
      ],
      [
        "report_warning"
      ],
      [
        "show_modal_gif"
      ],
      [
        "show_modal_progress_circle"
      ],
      [
        "show_modal_progress_line"
      ],
      [
        "show_modal_spinner"
      ],
      [
        "show_spinner"
      ],
      [
        "spin_epic"
      ],
      [
        "spin_kit"
      ],
      [
        "stop_gif"
      ],
      [
        "unblock"
      ],
      [
        "update_busy_bar"
      ],
      [
        "update_modal_progress"
      ],
      [
        "update_modal_spinner"
      ],
      [
        "update_progress"
      ],
      [
        "use_busy_bar"
      ],
      [
        "use_busy_gif"
      ],
      [
        "use_busy_spinner"
      ]
    ],
    "topics": [
      [
        "shiny"
      ]
    ],
    "score": 11.4977,
    "stars": 146,
    "primary_category": "visualization",
    "source_universe": "dreamrs",
    "search_text": "shinybusy Busy Indicators and Notifications for 'Shiny' Applications Add indicators (spinner, progress bar, gif) in your\n'shiny' applications to show the user that the server is busy.\nAnd other tools to let your users know something is happening\n(send notifications, reports, ...). add_busy_bar add_busy_gif add_busy_spinner add_loading_state block block_output busy_start_up config_notify config_report hide_spinner html_dependency_block html_dependency_busy html_dependency_epic html_dependency_freezeframe html_dependency_loading html_dependency_nanobar html_dependency_notiflix html_dependency_notify html_dependency_report html_dependency_shinybusy html_dependency_spinkit html_dependency_startup logo_silex notify notify_failure notify_info notify_success notify_warning play_gif progress_circle progress_line progress_semicircle remove_modal_gif remove_modal_progress remove_modal_spinner remove_start_up report report_failure report_info report_success report_warning show_modal_gif show_modal_progress_circle show_modal_progress_line show_modal_spinner show_spinner spin_epic spin_kit stop_gif unblock update_busy_bar update_modal_progress update_modal_spinner update_progress use_busy_bar use_busy_gif use_busy_spinner shiny"
  },
  {
    "id": 304,
    "package_name": "blogdown",
    "title": "Create Blogs and Websites with R Markdown",
    "description": "Write blog posts and web pages in R Markdown. This package\nsupports the static site generator 'Hugo' (<https://gohugo.io>)\nbest, and it also supports 'Jekyll' (<https://jekyllrb.com>)\nand 'Hexo' (<https://hexo.io>).",
    "version": "1.22.1",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nChristophe Dervieux [aut] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nAlison Presmanes Hill [aut] (ORCID:\n<https://orcid.org/0000-0002-8082-1890>),\nAmber Thomas [ctb],\nBeilei Bian [ctb],\nBrandon Greenwell [ctb],\nBrian Barkley [ctb],\nDeependra Dhakal [ctb],\nEric Nantz [ctb],\nForest Fang [ctb],\nGarrick Aden-Buie [ctb],\nHiroaki Yutani [ctb],\nIan Lyttle [ctb],\nJake Barlow [ctb],\nJames Balamuta [ctb],\nJJ Allaire [ctb],\nJon Calder [ctb],\nJozef Hajnala [ctb],\nJuan Manuel Vazquez [ctb],\nKevin Ushey [ctb],\nLeonardo Collado-Torres [ctb],\nMa\u00eblle Salmon [ctb],\nMaria Paula Caldas [ctb],\nNicolas Roelandt [ctb],\nOliver Madsen [ctb],\nRaniere Silva [ctb],\nTC Zhang [ctb],\nXianying Tan [ctb],\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/blogdown,\nhttps://pkgs.rstudio.com/blogdown/",
    "bug_reports": "https://github.com/rstudio/blogdown/issues",
    "repository": "",
    "exports": [
      [
        "build_dir"
      ],
      [
        "build_site"
      ],
      [
        "bundle_site"
      ],
      [
        "check_config"
      ],
      [
        "check_content"
      ],
      [
        "check_gitignore"
      ],
      [
        "check_hugo"
      ],
      [
        "check_netlify"
      ],
      [
        "check_site"
      ],
      [
        "check_vercel"
      ],
      [
        "clean_duplicates"
      ],
      [
        "config_netlify"
      ],
      [
        "config_Rprofile"
      ],
      [
        "config_vercel"
      ],
      [
        "count_yaml"
      ],
      [
        "dep_path"
      ],
      [
        "edit_draft"
      ],
      [
        "filter_md5sum"
      ],
      [
        "filter_newfile"
      ],
      [
        "filter_timestamp"
      ],
      [
        "find_categories"
      ],
      [
        "find_hugo"
      ],
      [
        "find_tags"
      ],
      [
        "find_yaml"
      ],
      [
        "html_page"
      ],
      [
        "hugo_available"
      ],
      [
        "hugo_build"
      ],
      [
        "hugo_cmd"
      ],
      [
        "hugo_convert"
      ],
      [
        "hugo_installers"
      ],
      [
        "hugo_server"
      ],
      [
        "hugo_version"
      ],
      [
        "install_hugo"
      ],
      [
        "install_theme"
      ],
      [
        "new_content"
      ],
      [
        "new_post"
      ],
      [
        "new_site"
      ],
      [
        "read_toml"
      ],
      [
        "remove_hugo"
      ],
      [
        "serve_site"
      ],
      [
        "shortcode"
      ],
      [
        "shortcode_close"
      ],
      [
        "shortcode_html"
      ],
      [
        "shortcode_open"
      ],
      [
        "shortcodes"
      ],
      [
        "stop_server"
      ],
      [
        "toml2yaml"
      ],
      [
        "update_hugo"
      ],
      [
        "write_toml"
      ],
      [
        "yaml2toml"
      ]
    ],
    "topics": [
      [
        "blog-engine"
      ],
      [
        "blogdown"
      ],
      [
        "hugo"
      ],
      [
        "rmarkdown"
      ],
      [
        "rstudio"
      ],
      [
        "website-generation"
      ]
    ],
    "score": 11.4507,
    "stars": 1777,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "blogdown Create Blogs and Websites with R Markdown Write blog posts and web pages in R Markdown. This package\nsupports the static site generator 'Hugo' (<https://gohugo.io>)\nbest, and it also supports 'Jekyll' (<https://jekyllrb.com>)\nand 'Hexo' (<https://hexo.io>). build_dir build_site bundle_site check_config check_content check_gitignore check_hugo check_netlify check_site check_vercel clean_duplicates config_netlify config_Rprofile config_vercel count_yaml dep_path edit_draft filter_md5sum filter_newfile filter_timestamp find_categories find_hugo find_tags find_yaml html_page hugo_available hugo_build hugo_cmd hugo_convert hugo_installers hugo_server hugo_version install_hugo install_theme new_content new_post new_site read_toml remove_hugo serve_site shortcode shortcode_close shortcode_html shortcode_open shortcodes stop_server toml2yaml update_hugo write_toml yaml2toml blog-engine blogdown hugo rmarkdown rstudio website-generation"
  },
  {
    "id": 1020,
    "package_name": "projpred",
    "title": "Projection Predictive Feature Selection",
    "description": "Performs projection predictive feature selection for\ngeneralized linear models (Piironen, Paasiniemi, and Vehtari,\n2020, <doi:10.1214/20-EJS1711>) with or without multilevel or\nadditive terms (Catalina, B\u00fcrkner, and Vehtari, 2022,\n<https://proceedings.mlr.press/v151/catalina22a.html>), for\nsome ordinal and nominal regression models (Weber, Glass, and\nVehtari, 2025, <doi:10.1007/s00180-024-01506-0>), and for many\nother regression models (using the latent projection by\nCatalina, B\u00fcrkner, and Vehtari, 2021,\n<doi:10.48550/arXiv.2109.04702>, which can also be applied to\nmost of the former models). The package is compatible with the\n'rstanarm' and 'brms' packages, but other reference models can\nalso be used. See the vignettes and the documentation for more\ninformation and examples.",
    "version": "2.10.0.9000",
    "maintainer": "Osvaldo Martin <aloctavodia@gmail.com>",
    "author": "Juho Piironen [aut],\nMarkus Paasiniemi [aut],\nAlejandro Catalina [aut],\nFrank Weber [aut],\nOsvaldo Martin [cre, aut],\nAki Vehtari [aut],\nJonah Gabry [ctb],\nMarco Colombo [ctb],\nPaul-Christian B\u00fcrkner [ctb],\nHamada S. Badr [ctb],\nBrian Sullivan [ctb],\nS\u00f6lvi R\u00f6gnvaldsson [ctb],\nThe LME4 Authors [cph] (see file 'LICENSE' for details),\nYann McLatchie [ctb],\nJuho Timonen [ctb]",
    "url": "https://mc-stan.org/projpred/, https://discourse.mc-stan.org",
    "bug_reports": "https://github.com/stan-dev/projpred/issues/",
    "repository": "",
    "exports": [
      [
        "augdat_ilink_binom"
      ],
      [
        "augdat_link_binom"
      ],
      [
        "break_up_matrix_term"
      ],
      [
        "cl_agg"
      ],
      [
        "cv_folds"
      ],
      [
        "cv_ids"
      ],
      [
        "cv_proportions"
      ],
      [
        "cv_varsel"
      ],
      [
        "cvfolds"
      ],
      [
        "do_call"
      ],
      [
        "extend_family"
      ],
      [
        "force_search_terms"
      ],
      [
        "get_refmodel"
      ],
      [
        "init_refmodel"
      ],
      [
        "performances"
      ],
      [
        "predictor_terms"
      ],
      [
        "proj_linpred"
      ],
      [
        "proj_predict"
      ],
      [
        "project"
      ],
      [
        "ranking"
      ],
      [
        "run_cvfun"
      ],
      [
        "solution_terms"
      ],
      [
        "Student_t"
      ],
      [
        "suggest_size"
      ],
      [
        "varsel"
      ],
      [
        "y_wobs_offs"
      ]
    ],
    "topics": [
      [
        "bayes"
      ],
      [
        "bayesian"
      ],
      [
        "bayesian-inference"
      ],
      [
        "rstanarm"
      ],
      [
        "stan"
      ],
      [
        "statistics"
      ],
      [
        "variable-selection"
      ],
      [
        "openblas"
      ],
      [
        "cpp"
      ]
    ],
    "score": 11.3464,
    "stars": 112,
    "primary_category": "statistics",
    "source_universe": "stan-dev",
    "search_text": "projpred Projection Predictive Feature Selection Performs projection predictive feature selection for\ngeneralized linear models (Piironen, Paasiniemi, and Vehtari,\n2020, <doi:10.1214/20-EJS1711>) with or without multilevel or\nadditive terms (Catalina, B\u00fcrkner, and Vehtari, 2022,\n<https://proceedings.mlr.press/v151/catalina22a.html>), for\nsome ordinal and nominal regression models (Weber, Glass, and\nVehtari, 2025, <doi:10.1007/s00180-024-01506-0>), and for many\nother regression models (using the latent projection by\nCatalina, B\u00fcrkner, and Vehtari, 2021,\n<doi:10.48550/arXiv.2109.04702>, which can also be applied to\nmost of the former models). The package is compatible with the\n'rstanarm' and 'brms' packages, but other reference models can\nalso be used. See the vignettes and the documentation for more\ninformation and examples. augdat_ilink_binom augdat_link_binom break_up_matrix_term cl_agg cv_folds cv_ids cv_proportions cv_varsel cvfolds do_call extend_family force_search_terms get_refmodel init_refmodel performances predictor_terms proj_linpred proj_predict project ranking run_cvfun solution_terms Student_t suggest_size varsel y_wobs_offs bayes bayesian bayesian-inference rstanarm stan statistics variable-selection openblas cpp"
  },
  {
    "id": 494,
    "package_name": "drake",
    "title": "A Pipeline Toolkit for Reproducible Computation at Scale",
    "description": "A general-purpose computational engine for data analysis,\ndrake rebuilds intermediate data objects when their\ndependencies change, and it skips work when the results are\nalready up to date.  Not every execution starts from scratch,\nthere is native support for parallel and distributed computing,\nand completed projects have tangible evidence that they are\nreproducible.  Extensive documentation, from beginner-friendly\ntutorials to practical examples and more, is available at the\nreference website <https://docs.ropensci.org/drake/> and the\nonline manual <https://books.ropensci.org/drake/>.",
    "version": "7.13.11",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1878-3253>),\nAlex Axthelm [ctb],\nJasper Clarkberg [ctb],\nKirill M\u00fcller [ctb],\nBen Bond-Lamberty [ctb] (ORCID:\n<https://orcid.org/0000-0001-9525-4633>),\nTristan Mahr [ctb] (ORCID: <https://orcid.org/0000-0002-8890-5116>),\nMiles McBain [ctb] (ORCID: <https://orcid.org/0000-0003-2865-2548>),\nNoam Ross [ctb] (ORCID: <https://orcid.org/0000-0002-2136-0000>),\nEllis Hughes [ctb],\nMatthew Mark Strasiotto [ctb],\nBen Marwick [rev],\nPeter Slaughter [rev],\nEli Lilly and Company [cph]",
    "url": "https://github.com/ropensci/drake,\nhttps://docs.ropensci.org/drake/,\nhttps://books.ropensci.org/drake/",
    "bug_reports": "https://github.com/ropensci/drake/issues",
    "repository": "",
    "exports": [
      [
        "all_of"
      ],
      [
        "analyses"
      ],
      [
        "analysis_wildcard"
      ],
      [
        "any_of"
      ],
      [
        "as_drake_filename"
      ],
      [
        "as_file"
      ],
      [
        "available_hash_algos"
      ],
      [
        "backend"
      ],
      [
        "bind_plans"
      ],
      [
        "build_drake_graph"
      ],
      [
        "build_graph"
      ],
      [
        "build_times"
      ],
      [
        "built"
      ],
      [
        "cache_namespaces"
      ],
      [
        "cache_path"
      ],
      [
        "cached"
      ],
      [
        "cached_planned"
      ],
      [
        "cached_unplanned"
      ],
      [
        "cancel"
      ],
      [
        "cancel_if"
      ],
      [
        "check"
      ],
      [
        "check_plan"
      ],
      [
        "clean"
      ],
      [
        "clean_main_example"
      ],
      [
        "clean_mtcars_example"
      ],
      [
        "cleaned_namespaces"
      ],
      [
        "cmq_build"
      ],
      [
        "code_to_function"
      ],
      [
        "code_to_plan"
      ],
      [
        "config"
      ],
      [
        "configure_cache"
      ],
      [
        "contains"
      ],
      [
        "dataframes_graph"
      ],
      [
        "dataset_wildcard"
      ],
      [
        "debug_and_run"
      ],
      [
        "default_graph_title"
      ],
      [
        "default_long_hash_algo"
      ],
      [
        "default_Makefile_args"
      ],
      [
        "default_Makefile_command"
      ],
      [
        "default_parallelism"
      ],
      [
        "default_recipe_command"
      ],
      [
        "default_short_hash_algo"
      ],
      [
        "default_system2_args"
      ],
      [
        "default_verbose"
      ],
      [
        "dependency_profile"
      ],
      [
        "deprecate_wildcard"
      ],
      [
        "deps"
      ],
      [
        "deps_code"
      ],
      [
        "deps_knitr"
      ],
      [
        "deps_profile"
      ],
      [
        "deps_profile_impl"
      ],
      [
        "deps_target"
      ],
      [
        "deps_target_impl"
      ],
      [
        "deps_targets"
      ],
      [
        "diagnose"
      ],
      [
        "do_prework"
      ],
      [
        "doc_of_function_call"
      ],
      [
        "drake_batchtools_tmpl_file"
      ],
      [
        "drake_build"
      ],
      [
        "drake_build_impl"
      ],
      [
        "drake_cache"
      ],
      [
        "drake_cache_log"
      ],
      [
        "drake_cache_log_file"
      ],
      [
        "drake_cancelled"
      ],
      [
        "drake_config"
      ],
      [
        "drake_debug"
      ],
      [
        "drake_done"
      ],
      [
        "drake_envir"
      ],
      [
        "drake_example"
      ],
      [
        "drake_examples"
      ],
      [
        "drake_failed"
      ],
      [
        "drake_gc"
      ],
      [
        "drake_get_session_info"
      ],
      [
        "drake_ggraph"
      ],
      [
        "drake_ggraph_impl"
      ],
      [
        "drake_graph_info"
      ],
      [
        "drake_graph_info_impl"
      ],
      [
        "drake_history"
      ],
      [
        "drake_hpc_template_file"
      ],
      [
        "drake_hpc_template_files"
      ],
      [
        "drake_meta"
      ],
      [
        "drake_palette"
      ],
      [
        "drake_plan"
      ],
      [
        "drake_plan_source"
      ],
      [
        "drake_progress"
      ],
      [
        "drake_quotes"
      ],
      [
        "drake_running"
      ],
      [
        "drake_script"
      ],
      [
        "drake_session"
      ],
      [
        "drake_slice"
      ],
      [
        "drake_strings"
      ],
      [
        "drake_tempfile"
      ],
      [
        "drake_tip"
      ],
      [
        "drake_unquote"
      ],
      [
        "ends_with"
      ],
      [
        "evaluate"
      ],
      [
        "evaluate_plan"
      ],
      [
        "everything"
      ],
      [
        "example_drake"
      ],
      [
        "examples_drake"
      ],
      [
        "expand"
      ],
      [
        "expand_plan"
      ],
      [
        "expose_imports"
      ],
      [
        "failed"
      ],
      [
        "file_in"
      ],
      [
        "file_out"
      ],
      [
        "file_store"
      ],
      [
        "find_cache"
      ],
      [
        "find_knitr_doc"
      ],
      [
        "find_project"
      ],
      [
        "from_plan"
      ],
      [
        "future_build"
      ],
      [
        "gather"
      ],
      [
        "gather_by"
      ],
      [
        "gather_plan"
      ],
      [
        "get_cache"
      ],
      [
        "get_trace"
      ],
      [
        "id_chr"
      ],
      [
        "ignore"
      ],
      [
        "imported"
      ],
      [
        "in_progress"
      ],
      [
        "is_function_call"
      ],
      [
        "isolate_example"
      ],
      [
        "knitr_deps"
      ],
      [
        "knitr_in"
      ],
      [
        "last_col"
      ],
      [
        "legend_nodes"
      ],
      [
        "load_basic_example"
      ],
      [
        "load_main_example"
      ],
      [
        "load_mtcars_example"
      ],
      [
        "loadd"
      ],
      [
        "long_hash"
      ],
      [
        "make"
      ],
      [
        "make_impl"
      ],
      [
        "make_imports"
      ],
      [
        "make_targets"
      ],
      [
        "make_with_config"
      ],
      [
        "Makefile_recipe"
      ],
      [
        "manage_memory"
      ],
      [
        "map_plan"
      ],
      [
        "matches"
      ],
      [
        "max_useful_jobs"
      ],
      [
        "migrate_drake_project"
      ],
      [
        "missed"
      ],
      [
        "missed_impl"
      ],
      [
        "new_cache"
      ],
      [
        "no_deps"
      ],
      [
        "num_range"
      ],
      [
        "one_of"
      ],
      [
        "outdated"
      ],
      [
        "outdated_impl"
      ],
      [
        "parallel_stages"
      ],
      [
        "parallelism_choices"
      ],
      [
        "plan"
      ],
      [
        "plan_analyses"
      ],
      [
        "plan_drake"
      ],
      [
        "plan_summaries"
      ],
      [
        "plan_to_code"
      ],
      [
        "plan_to_notebook"
      ],
      [
        "plot_graph"
      ],
      [
        "predict_load_balancing"
      ],
      [
        "predict_runtime"
      ],
      [
        "predict_runtime_impl"
      ],
      [
        "predict_workers"
      ],
      [
        "predict_workers_impl"
      ],
      [
        "process_import"
      ],
      [
        "progress"
      ],
      [
        "prune_drake_graph"
      ],
      [
        "r_deps_target"
      ],
      [
        "r_drake_build"
      ],
      [
        "r_drake_ggraph"
      ],
      [
        "r_drake_graph_info"
      ],
      [
        "r_make"
      ],
      [
        "r_missed"
      ],
      [
        "r_outdated"
      ],
      [
        "r_predict_runtime"
      ],
      [
        "r_predict_workers"
      ],
      [
        "r_recipe_wildcard"
      ],
      [
        "r_recoverable"
      ],
      [
        "r_sankey_drake_graph"
      ],
      [
        "r_text_drake_graph"
      ],
      [
        "r_vis_drake_graph"
      ],
      [
        "rate_limiting_times"
      ],
      [
        "read_config"
      ],
      [
        "read_drake_config"
      ],
      [
        "read_drake_graph"
      ],
      [
        "read_drake_meta"
      ],
      [
        "read_drake_plan"
      ],
      [
        "read_drake_seed"
      ],
      [
        "read_graph"
      ],
      [
        "read_plan"
      ],
      [
        "read_trace"
      ],
      [
        "readd"
      ],
      [
        "recover_cache"
      ],
      [
        "recoverable"
      ],
      [
        "recoverable_impl"
      ],
      [
        "reduce_by"
      ],
      [
        "reduce_plan"
      ],
      [
        "render_drake_ggraph"
      ],
      [
        "render_drake_graph"
      ],
      [
        "render_graph"
      ],
      [
        "render_sankey_drake_graph"
      ],
      [
        "render_static_drake_graph"
      ],
      [
        "render_text_drake_graph"
      ],
      [
        "rescue_cache"
      ],
      [
        "rs_addin_loadd"
      ],
      [
        "rs_addin_r_make"
      ],
      [
        "rs_addin_r_outdated"
      ],
      [
        "rs_addin_r_vis_drake_graph"
      ],
      [
        "running"
      ],
      [
        "sankey_drake_graph"
      ],
      [
        "sankey_drake_graph_impl"
      ],
      [
        "session"
      ],
      [
        "shell_file"
      ],
      [
        "short_hash"
      ],
      [
        "show_source"
      ],
      [
        "starts_with"
      ],
      [
        "static_drake_graph"
      ],
      [
        "subtargets"
      ],
      [
        "summaries"
      ],
      [
        "target"
      ],
      [
        "target_namespaces"
      ],
      [
        "text_drake_graph"
      ],
      [
        "text_drake_graph_impl"
      ],
      [
        "this_cache"
      ],
      [
        "tracked"
      ],
      [
        "transform_plan"
      ],
      [
        "trigger"
      ],
      [
        "triggers"
      ],
      [
        "type_sum.expr_list"
      ],
      [
        "use_drake"
      ],
      [
        "vis_drake_graph"
      ],
      [
        "vis_drake_graph_impl"
      ],
      [
        "which_clean"
      ],
      [
        "workflow"
      ],
      [
        "workplan"
      ]
    ],
    "topics": [
      [
        "data-science"
      ],
      [
        "drake"
      ],
      [
        "high-performance-computing"
      ],
      [
        "makefile"
      ],
      [
        "peer-reviewed"
      ],
      [
        "pipeline"
      ],
      [
        "reproducibility"
      ],
      [
        "reproducible-research"
      ],
      [
        "ropensci"
      ],
      [
        "workflow"
      ]
    ],
    "score": 11.2444,
    "stars": 1341,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "drake A Pipeline Toolkit for Reproducible Computation at Scale A general-purpose computational engine for data analysis,\ndrake rebuilds intermediate data objects when their\ndependencies change, and it skips work when the results are\nalready up to date.  Not every execution starts from scratch,\nthere is native support for parallel and distributed computing,\nand completed projects have tangible evidence that they are\nreproducible.  Extensive documentation, from beginner-friendly\ntutorials to practical examples and more, is available at the\nreference website <https://docs.ropensci.org/drake/> and the\nonline manual <https://books.ropensci.org/drake/>. all_of analyses analysis_wildcard any_of as_drake_filename as_file available_hash_algos backend bind_plans build_drake_graph build_graph build_times built cache_namespaces cache_path cached cached_planned cached_unplanned cancel cancel_if check check_plan clean clean_main_example clean_mtcars_example cleaned_namespaces cmq_build code_to_function code_to_plan config configure_cache contains dataframes_graph dataset_wildcard debug_and_run default_graph_title default_long_hash_algo default_Makefile_args default_Makefile_command default_parallelism default_recipe_command default_short_hash_algo default_system2_args default_verbose dependency_profile deprecate_wildcard deps deps_code deps_knitr deps_profile deps_profile_impl deps_target deps_target_impl deps_targets diagnose do_prework doc_of_function_call drake_batchtools_tmpl_file drake_build drake_build_impl drake_cache drake_cache_log drake_cache_log_file drake_cancelled drake_config drake_debug drake_done drake_envir drake_example drake_examples drake_failed drake_gc drake_get_session_info drake_ggraph drake_ggraph_impl drake_graph_info drake_graph_info_impl drake_history drake_hpc_template_file drake_hpc_template_files drake_meta drake_palette drake_plan drake_plan_source drake_progress drake_quotes drake_running drake_script drake_session drake_slice drake_strings drake_tempfile drake_tip drake_unquote ends_with evaluate evaluate_plan everything example_drake examples_drake expand expand_plan expose_imports failed file_in file_out file_store find_cache find_knitr_doc find_project from_plan future_build gather gather_by gather_plan get_cache get_trace id_chr ignore imported in_progress is_function_call isolate_example knitr_deps knitr_in last_col legend_nodes load_basic_example load_main_example load_mtcars_example loadd long_hash make make_impl make_imports make_targets make_with_config Makefile_recipe manage_memory map_plan matches max_useful_jobs migrate_drake_project missed missed_impl new_cache no_deps num_range one_of outdated outdated_impl parallel_stages parallelism_choices plan plan_analyses plan_drake plan_summaries plan_to_code plan_to_notebook plot_graph predict_load_balancing predict_runtime predict_runtime_impl predict_workers predict_workers_impl process_import progress prune_drake_graph r_deps_target r_drake_build r_drake_ggraph r_drake_graph_info r_make r_missed r_outdated r_predict_runtime r_predict_workers r_recipe_wildcard r_recoverable r_sankey_drake_graph r_text_drake_graph r_vis_drake_graph rate_limiting_times read_config read_drake_config read_drake_graph read_drake_meta read_drake_plan read_drake_seed read_graph read_plan read_trace readd recover_cache recoverable recoverable_impl reduce_by reduce_plan render_drake_ggraph render_drake_graph render_graph render_sankey_drake_graph render_static_drake_graph render_text_drake_graph rescue_cache rs_addin_loadd rs_addin_r_make rs_addin_r_outdated rs_addin_r_vis_drake_graph running sankey_drake_graph sankey_drake_graph_impl session shell_file short_hash show_source starts_with static_drake_graph subtargets summaries target target_namespaces text_drake_graph text_drake_graph_impl this_cache tracked transform_plan trigger triggers type_sum.expr_list use_drake vis_drake_graph vis_drake_graph_impl which_clean workflow workplan data-science drake high-performance-computing makefile peer-reviewed pipeline reproducibility reproducible-research ropensci workflow"
  },
  {
    "id": 726,
    "package_name": "jose",
    "title": "JavaScript Object Signing and Encryption",
    "description": "Read and write JSON Web Keys (JWK, rfc7517), generate and\nverify JSON Web Signatures (JWS, rfc7515) and encode/decode\nJSON Web Tokens (JWT, rfc7519)\n<https://datatracker.ietf.org/wg/jose/documents/>. These\nstandards provide modern signing and encryption formats that\nare natively supported by browsers via the JavaScript\nWebCryptoAPI <https://www.w3.org/TR/WebCryptoAPI/#jose>, and\nused by services like OAuth 2.0, LetsEncrypt, and Github Apps.",
    "version": "1.2.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://r-lib.r-universe.dev/jose",
    "bug_reports": "https://github.com/r-lib/jose/issues",
    "repository": "",
    "exports": [
      [
        "base64url_decode"
      ],
      [
        "base64url_encode"
      ],
      [
        "jwk_read"
      ],
      [
        "jwk_write"
      ],
      [
        "jwt_claim"
      ],
      [
        "jwt_decode_hmac"
      ],
      [
        "jwt_decode_sig"
      ],
      [
        "jwt_encode_hmac"
      ],
      [
        "jwt_encode_sig"
      ],
      [
        "jwt_split"
      ],
      [
        "read_jwk"
      ],
      [
        "write_jwk"
      ]
    ],
    "topics": [],
    "score": 11.2256,
    "stars": 54,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "jose JavaScript Object Signing and Encryption Read and write JSON Web Keys (JWK, rfc7517), generate and\nverify JSON Web Signatures (JWS, rfc7515) and encode/decode\nJSON Web Tokens (JWT, rfc7519)\n<https://datatracker.ietf.org/wg/jose/documents/>. These\nstandards provide modern signing and encryption formats that\nare natively supported by browsers via the JavaScript\nWebCryptoAPI <https://www.w3.org/TR/WebCryptoAPI/#jose>, and\nused by services like OAuth 2.0, LetsEncrypt, and Github Apps. base64url_decode base64url_encode jwk_read jwk_write jwt_claim jwt_decode_hmac jwt_decode_sig jwt_encode_hmac jwt_encode_sig jwt_split read_jwk write_jwk "
  },
  {
    "id": 1260,
    "package_name": "spelling",
    "title": "Tools for Spell Checking in R",
    "description": "Spell checking common document formats including latex,\nmarkdown, manual pages, and description files. Includes\nutilities to automate checking of documentation and vignettes\nas a unit test during 'R CMD check'. Both British and American\nEnglish are supported out of the box and other languages can be\nadded. In addition, packages may define a 'wordlist' to allow\ncustom terminology without having to abuse punctuation.",
    "version": "2.3.2",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [cre, aut] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nJim Hester [aut]",
    "url": "https://ropensci.r-universe.dev/spelling\nhttps://docs.ropensci.org/spelling/",
    "bug_reports": "https://github.com/ropensci/spelling/issues",
    "repository": "",
    "exports": [
      [
        "get_wordlist"
      ],
      [
        "spell_check_files"
      ],
      [
        "spell_check_package"
      ],
      [
        "spell_check_setup"
      ],
      [
        "spell_check_test"
      ],
      [
        "spell_check_text"
      ],
      [
        "update_wordlist"
      ]
    ],
    "topics": [
      [
        "spell-check"
      ],
      [
        "spell-checker"
      ],
      [
        "spellcheck"
      ],
      [
        "spellchecker"
      ],
      [
        "spelling"
      ]
    ],
    "score": 11.0486,
    "stars": 108,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "spelling Tools for Spell Checking in R Spell checking common document formats including latex,\nmarkdown, manual pages, and description files. Includes\nutilities to automate checking of documentation and vignettes\nas a unit test during 'R CMD check'. Both British and American\nEnglish are supported out of the box and other languages can be\nadded. In addition, packages may define a 'wordlist' to allow\ncustom terminology without having to abuse punctuation. get_wordlist spell_check_files spell_check_package spell_check_setup spell_check_test spell_check_text update_wordlist spell-check spell-checker spellcheck spellchecker spelling"
  },
  {
    "id": 1394,
    "package_name": "tufte",
    "title": "Tufte's Styles for R Markdown Documents",
    "description": "Provides R Markdown output formats to use Tufte styles for\nPDF and HTML output.",
    "version": "0.14.0.9000",
    "maintainer": "Christophe Dervieux <cderv@posit.co>",
    "author": "Yihui Xie [aut] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nChristophe Dervieux [ctb, cre] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nJJ Allaire [aut],\nAndrzej Oles [ctb],\nDave Liepmann [ctb] (Tufte CSS in\ninst/rmarkdown/templates/tufte_html/resources),\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/tufte",
    "bug_reports": "https://github.com/rstudio/tufte/issues",
    "repository": "",
    "exports": [
      [
        "margin_note"
      ],
      [
        "newthought"
      ],
      [
        "quote_footer"
      ],
      [
        "sans_serif"
      ],
      [
        "tufte_book"
      ],
      [
        "tufte_handout"
      ],
      [
        "tufte_html"
      ]
    ],
    "topics": [
      [
        "r-markdown"
      ],
      [
        "tufte"
      ],
      [
        "tufte-style"
      ]
    ],
    "score": 10.7122,
    "stars": 422,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "tufte Tufte's Styles for R Markdown Documents Provides R Markdown output formats to use Tufte styles for\nPDF and HTML output. margin_note newthought quote_footer sans_serif tufte_book tufte_handout tufte_html r-markdown tufte tufte-style"
  },
  {
    "id": 1332,
    "package_name": "themis",
    "title": "Extra Recipes Steps for Dealing with Unbalanced Data",
    "description": "A dataset with an uneven number of cases in each class is\nsaid to be unbalanced. Many models produce a subpar performance\non unbalanced datasets. A dataset can be balanced by increasing\nthe number of minority cases using SMOTE 2011\n<doi:10.48550/arXiv.1106.1813>, BorderlineSMOTE 2005\n<doi:10.1007/11538059_91> and ADASYN 2008\n<https://ieeexplore.ieee.org/document/4633969>. Or by\ndecreasing the number of majority cases using NearMiss 2003\n<https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf> or\nTomek link removal 1976\n<https://ieeexplore.ieee.org/document/4309452>.",
    "version": "1.0.3.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/tidymodels/themis,\nhttps://themis.tidymodels.org",
    "bug_reports": "https://github.com/tidymodels/themis/issues",
    "repository": "",
    "exports": [
      [
        "adasyn"
      ],
      [
        "bsmote"
      ],
      [
        "nearmiss"
      ],
      [
        "required_pkgs"
      ],
      [
        "smote"
      ],
      [
        "smotenc"
      ],
      [
        "step_adasyn"
      ],
      [
        "step_bsmote"
      ],
      [
        "step_downsample"
      ],
      [
        "step_nearmiss"
      ],
      [
        "step_rose"
      ],
      [
        "step_smote"
      ],
      [
        "step_smotenc"
      ],
      [
        "step_tomek"
      ],
      [
        "step_upsample"
      ],
      [
        "tidy"
      ],
      [
        "tomek"
      ],
      [
        "tunable"
      ]
    ],
    "topics": [],
    "score": 10.5555,
    "stars": 142,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "themis Extra Recipes Steps for Dealing with Unbalanced Data A dataset with an uneven number of cases in each class is\nsaid to be unbalanced. Many models produce a subpar performance\non unbalanced datasets. A dataset can be balanced by increasing\nthe number of minority cases using SMOTE 2011\n<doi:10.48550/arXiv.1106.1813>, BorderlineSMOTE 2005\n<doi:10.1007/11538059_91> and ADASYN 2008\n<https://ieeexplore.ieee.org/document/4633969>. Or by\ndecreasing the number of majority cases using NearMiss 2003\n<https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf> or\nTomek link removal 1976\n<https://ieeexplore.ieee.org/document/4309452>. adasyn bsmote nearmiss required_pkgs smote smotenc step_adasyn step_bsmote step_downsample step_nearmiss step_rose step_smote step_smotenc step_tomek step_upsample tidy tomek tunable "
  },
  {
    "id": 301,
    "package_name": "blastula",
    "title": "Easily Send HTML Email Messages",
    "description": "Compose and send out responsive HTML email messages that\nrender perfectly across a range of email clients and device\nsizes. Helper functions let the user insert embedded images,\nweb link buttons, and 'ggplot2' plot objects into the message\nbody. Messages can be sent through an 'SMTP' server, through\nthe 'Posit Connect' service, or through the 'Mailgun' API\nservice <https://www.mailgun.com/>.",
    "version": "0.3.6.9000",
    "maintainer": "Richard Iannone <rich@posit.co>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3925-190X>),\nJoe Cheng [aut],\nJeroen Ooms [ctb] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nTed Goas [cph] (cerberus-meta.html),\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/blastula,\nhttps://pkgs.rstudio.com/blastula/",
    "bug_reports": "https://github.com/rstudio/blastula/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "add_attachment"
      ],
      [
        "add_cta_button"
      ],
      [
        "add_ggplot"
      ],
      [
        "add_image"
      ],
      [
        "add_imgur_image"
      ],
      [
        "add_readable_time"
      ],
      [
        "article"
      ],
      [
        "attach_connect_email"
      ],
      [
        "blastula_email"
      ],
      [
        "blastula_template"
      ],
      [
        "block_articles"
      ],
      [
        "block_social_links"
      ],
      [
        "block_spacer"
      ],
      [
        "block_text"
      ],
      [
        "block_title"
      ],
      [
        "blocks"
      ],
      [
        "compose_email"
      ],
      [
        "create_smtp_creds_file"
      ],
      [
        "create_smtp_creds_key"
      ],
      [
        "creds"
      ],
      [
        "creds_anonymous"
      ],
      [
        "creds_envvar"
      ],
      [
        "creds_file"
      ],
      [
        "creds_key"
      ],
      [
        "delete_all_credential_keys"
      ],
      [
        "delete_credential_key"
      ],
      [
        "get_html_str"
      ],
      [
        "md"
      ],
      [
        "prepare_rsc_example_files"
      ],
      [
        "prepare_test_message"
      ],
      [
        "render_connect_email"
      ],
      [
        "render_email"
      ],
      [
        "send_by_mailgun"
      ],
      [
        "smtp_send"
      ],
      [
        "social_link"
      ],
      [
        "suppress_scheduled_email"
      ],
      [
        "view_credential_keys"
      ]
    ],
    "topics": [
      [
        "easy-to-use"
      ],
      [
        "email"
      ],
      [
        "html"
      ],
      [
        "markdown"
      ],
      [
        "responsive-email"
      ],
      [
        "smtp"
      ]
    ],
    "score": 10.4744,
    "stars": 563,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "blastula Easily Send HTML Email Messages Compose and send out responsive HTML email messages that\nrender perfectly across a range of email clients and device\nsizes. Helper functions let the user insert embedded images,\nweb link buttons, and 'ggplot2' plot objects into the message\nbody. Messages can be sent through an 'SMTP' server, through\nthe 'Posit Connect' service, or through the 'Mailgun' API\nservice <https://www.mailgun.com/>. %>% add_attachment add_cta_button add_ggplot add_image add_imgur_image add_readable_time article attach_connect_email blastula_email blastula_template block_articles block_social_links block_spacer block_text block_title blocks compose_email create_smtp_creds_file create_smtp_creds_key creds creds_anonymous creds_envvar creds_file creds_key delete_all_credential_keys delete_credential_key get_html_str md prepare_rsc_example_files prepare_test_message render_connect_email render_email send_by_mailgun smtp_send social_link suppress_scheduled_email view_credential_keys easy-to-use email html markdown responsive-email smtp"
  },
  {
    "id": 1470,
    "package_name": "ymlthis",
    "title": "Write 'YAML' for 'R Markdown', 'bookdown', 'blogdown', and More",
    "description": "Write 'YAML' front matter for R Markdown and related\ndocuments. Work with 'YAML' objects more naturally and write\nthe resulting 'YAML' to your clipboard or to 'YAML' files\nrelated to your project.",
    "version": "0.1.7",
    "maintainer": "Malcolm Barrett <malcolmbarrett@gmail.com>",
    "author": "Malcolm Barrett [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-0299-5825>),\nRichard Iannone [aut] (ORCID: <https://orcid.org/0000-0003-3925-190X>),\nRStudio [cph, fnd]",
    "url": "https://ymlthis.r-lib.org, https://github.com/r-lib/ymlthis",
    "bug_reports": "https://github.com/r-lib/ymlthis/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "as_yml"
      ],
      [
        "asis_yaml_output"
      ],
      [
        "bib2yml"
      ],
      [
        "blogdown_archetypes"
      ],
      [
        "blogdown_template"
      ],
      [
        "code_chunk"
      ],
      [
        "distill_collection"
      ],
      [
        "distill_listing"
      ],
      [
        "distill_resources"
      ],
      [
        "draw_yml_tree"
      ],
      [
        "get_rmd_defaults"
      ],
      [
        "get_yml_defaults"
      ],
      [
        "gitbook_config"
      ],
      [
        "has_field"
      ],
      [
        "includes2"
      ],
      [
        "is_yml"
      ],
      [
        "is_yml_blank"
      ],
      [
        "last_yml"
      ],
      [
        "navbar_page"
      ],
      [
        "navbar_separator"
      ],
      [
        "pagedown_business_card_template"
      ],
      [
        "pagedown_person"
      ],
      [
        "pandoc_highlight_styles"
      ],
      [
        "pandoc_template_types"
      ],
      [
        "pkgdown_article"
      ],
      [
        "pkgdown_ref"
      ],
      [
        "pkgdown_template"
      ],
      [
        "pkgdown_tutorial"
      ],
      [
        "read_json"
      ],
      [
        "read_toml"
      ],
      [
        "reference"
      ],
      [
        "rticles_address"
      ],
      [
        "rticles_author"
      ],
      [
        "rticles_corr_author"
      ],
      [
        "setup_chunk"
      ],
      [
        "shiny_checkbox"
      ],
      [
        "shiny_date"
      ],
      [
        "shiny_file"
      ],
      [
        "shiny_numeric"
      ],
      [
        "shiny_params"
      ],
      [
        "shiny_password"
      ],
      [
        "shiny_radio"
      ],
      [
        "shiny_select"
      ],
      [
        "shiny_slider"
      ],
      [
        "shiny_text"
      ],
      [
        "use_bookdown_yml"
      ],
      [
        "use_index_rmd"
      ],
      [
        "use_navbar_yml"
      ],
      [
        "use_output_yml"
      ],
      [
        "use_pandoc_highlight_style"
      ],
      [
        "use_pandoc_template"
      ],
      [
        "use_pkgdown_yml"
      ],
      [
        "use_rmarkdown"
      ],
      [
        "use_rmd_defaults"
      ],
      [
        "use_site_yml"
      ],
      [
        "use_yml"
      ],
      [
        "use_yml_defaults"
      ],
      [
        "use_yml_file"
      ],
      [
        "write_as_json"
      ],
      [
        "write_as_toml"
      ],
      [
        "yml"
      ],
      [
        "yml_abstract"
      ],
      [
        "yml_author"
      ],
      [
        "yml_blank"
      ],
      [
        "yml_blogdown_opts"
      ],
      [
        "yml_bookdown_opts"
      ],
      [
        "yml_bookdown_site"
      ],
      [
        "yml_category"
      ],
      [
        "yml_chuck"
      ],
      [
        "yml_citations"
      ],
      [
        "yml_clean"
      ],
      [
        "yml_code"
      ],
      [
        "yml_date"
      ],
      [
        "yml_description"
      ],
      [
        "yml_discard"
      ],
      [
        "yml_distill_author"
      ],
      [
        "yml_distill_opts"
      ],
      [
        "yml_empty"
      ],
      [
        "yml_handlers"
      ],
      [
        "yml_keywords"
      ],
      [
        "yml_lang"
      ],
      [
        "yml_latex_opts"
      ],
      [
        "yml_load"
      ],
      [
        "yml_navbar"
      ],
      [
        "yml_output"
      ],
      [
        "yml_output_metadata"
      ],
      [
        "yml_pagedown_opts"
      ],
      [
        "yml_params"
      ],
      [
        "yml_params_code"
      ],
      [
        "yml_pkgdown"
      ],
      [
        "yml_pkgdown_articles"
      ],
      [
        "yml_pkgdown_development"
      ],
      [
        "yml_pkgdown_docsearch"
      ],
      [
        "yml_pkgdown_figures"
      ],
      [
        "yml_pkgdown_news"
      ],
      [
        "yml_pkgdown_opts"
      ],
      [
        "yml_pkgdown_reference"
      ],
      [
        "yml_pkgdown_template"
      ],
      [
        "yml_pkgdown_tutorial"
      ],
      [
        "yml_pluck"
      ],
      [
        "yml_reference"
      ],
      [
        "yml_replace"
      ],
      [
        "yml_resource_files"
      ],
      [
        "yml_rsconnect_email"
      ],
      [
        "yml_rticles_opts"
      ],
      [
        "yml_runtime"
      ],
      [
        "yml_site_opts"
      ],
      [
        "yml_subject"
      ],
      [
        "yml_subtitle"
      ],
      [
        "yml_title"
      ],
      [
        "yml_toc"
      ],
      [
        "yml_toplevel"
      ],
      [
        "yml_verbatim"
      ],
      [
        "yml_vignette"
      ]
    ],
    "topics": [],
    "score": 10.2478,
    "stars": 167,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "ymlthis Write 'YAML' for 'R Markdown', 'bookdown', 'blogdown', and More Write 'YAML' front matter for R Markdown and related\ndocuments. Work with 'YAML' objects more naturally and write\nthe resulting 'YAML' to your clipboard or to 'YAML' files\nrelated to your project. %>% as_yml asis_yaml_output bib2yml blogdown_archetypes blogdown_template code_chunk distill_collection distill_listing distill_resources draw_yml_tree get_rmd_defaults get_yml_defaults gitbook_config has_field includes2 is_yml is_yml_blank last_yml navbar_page navbar_separator pagedown_business_card_template pagedown_person pandoc_highlight_styles pandoc_template_types pkgdown_article pkgdown_ref pkgdown_template pkgdown_tutorial read_json read_toml reference rticles_address rticles_author rticles_corr_author setup_chunk shiny_checkbox shiny_date shiny_file shiny_numeric shiny_params shiny_password shiny_radio shiny_select shiny_slider shiny_text use_bookdown_yml use_index_rmd use_navbar_yml use_output_yml use_pandoc_highlight_style use_pandoc_template use_pkgdown_yml use_rmarkdown use_rmd_defaults use_site_yml use_yml use_yml_defaults use_yml_file write_as_json write_as_toml yml yml_abstract yml_author yml_blank yml_blogdown_opts yml_bookdown_opts yml_bookdown_site yml_category yml_chuck yml_citations yml_clean yml_code yml_date yml_description yml_discard yml_distill_author yml_distill_opts yml_empty yml_handlers yml_keywords yml_lang yml_latex_opts yml_load yml_navbar yml_output yml_output_metadata yml_pagedown_opts yml_params yml_params_code yml_pkgdown yml_pkgdown_articles yml_pkgdown_development yml_pkgdown_docsearch yml_pkgdown_figures yml_pkgdown_news yml_pkgdown_opts yml_pkgdown_reference yml_pkgdown_template yml_pkgdown_tutorial yml_pluck yml_reference yml_replace yml_resource_files yml_rsconnect_email yml_rticles_opts yml_runtime yml_site_opts yml_subject yml_subtitle yml_title yml_toc yml_toplevel yml_verbatim yml_vignette "
  },
  {
    "id": 476,
    "package_name": "distill",
    "title": "'R Markdown' Format for Scientific and Technical Writing",
    "description": "Scientific and technical article format for the web.\n'Distill' articles feature attractive, reader-friendly\ntypography, flexible layout options for visualizations, and\nfull support for footnotes and citations.",
    "version": "1.6.1",
    "maintainer": "Christophe Dervieux <cderv@posit.co>",
    "author": "Christophe Dervieux [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nJJ Allaire [aut] (ORCID: <https://orcid.org/0000-0003-0174-9868>),\nRich Iannone [aut],\nAlison Presmanes Hill [aut] (ORCID:\n<https://orcid.org/0000-0002-8082-1890>),\nYihui Xie [aut] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\nPosit Software, PBC [cph, fnd],\nGoogle LLC [ctb, cph] (https://distill.pub/guide/),\nNick Williams [ctb, cph] (https://wicky.nillia.ms/headroom.js/),\nDenis Demchenko [ctb, cph] (https://github.com/lancedikson/bowser),\nThe Polymer Authors [ctb, cph]\n(https://www.webcomponents.org/polyfills/),\nG\u00e1bor Cs\u00e1rdi [ctb, cph] (whoami),\nJooYoung Seo [ctb] (ORCID: <https://orcid.org/0000-0002-4064-6012>)",
    "url": "https://github.com/rstudio/distill,\nhttps://pkgs.rstudio.com/distill",
    "bug_reports": "https://github.com/rstudio/distill/issues",
    "repository": "",
    "exports": [
      [
        "create_article"
      ],
      [
        "create_blog"
      ],
      [
        "create_post"
      ],
      [
        "create_theme"
      ],
      [
        "create_website"
      ],
      [
        "distill_article"
      ],
      [
        "distill_website"
      ],
      [
        "import_post"
      ],
      [
        "publish_website"
      ],
      [
        "rename_post_dir"
      ],
      [
        "update_post"
      ]
    ],
    "topics": [],
    "score": 10.2009,
    "stars": 424,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "distill 'R Markdown' Format for Scientific and Technical Writing Scientific and technical article format for the web.\n'Distill' articles feature attractive, reader-friendly\ntypography, flexible layout options for visualizations, and\nfull support for footnotes and citations. create_article create_blog create_post create_theme create_website distill_article distill_website import_post publish_website rename_post_dir update_post "
  },
  {
    "id": 798,
    "package_name": "marquee",
    "title": "Markdown Parser and Renderer for R Graphics",
    "description": "Provides the mean to parse and render markdown text with\ngrid along with facilities to define the styling of the text.",
    "version": "1.2.1.9000",
    "maintainer": "Thomas Lin Pedersen <thomas.pedersen@posit.co>",
    "author": "Thomas Lin Pedersen [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-5147-4711>),\nMartin Mit\u00e1\u0161 [aut] (Author of MD4C),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://marquee.r-lib.org, https://github.com/r-lib/marquee",
    "bug_reports": "https://github.com/r-lib/marquee/issues",
    "repository": "",
    "exports": [
      [
        "base_style"
      ],
      [
        "classic_style"
      ],
      [
        "element_marquee"
      ],
      [
        "em"
      ],
      [
        "geom_marquee"
      ],
      [
        "GeomMarquee"
      ],
      [
        "guide_marquee"
      ],
      [
        "GuideMarquee"
      ],
      [
        "ink"
      ],
      [
        "marquee_bullets"
      ],
      [
        "marquee_glue"
      ],
      [
        "marquee_glue_data"
      ],
      [
        "marquee_grob"
      ],
      [
        "marquee_parse"
      ],
      [
        "marquefy_theme"
      ],
      [
        "modify_style"
      ],
      [
        "relative"
      ],
      [
        "rem"
      ],
      [
        "remove_style"
      ],
      [
        "skip_inherit"
      ],
      [
        "style"
      ],
      [
        "style_set"
      ],
      [
        "trbl"
      ]
    ],
    "topics": [
      [
        "cpp"
      ]
    ],
    "score": 10.0536,
    "stars": 97,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "marquee Markdown Parser and Renderer for R Graphics Provides the mean to parse and render markdown text with\ngrid along with facilities to define the styling of the text. base_style classic_style element_marquee em geom_marquee GeomMarquee guide_marquee GuideMarquee ink marquee_bullets marquee_glue marquee_glue_data marquee_grob marquee_parse marquefy_theme modify_style relative rem remove_style skip_inherit style style_set trbl cpp"
  },
  {
    "id": 915,
    "package_name": "orderly",
    "title": "Lightweight Reproducible Reporting",
    "description": "Distributed reproducible computing framework, adopting\nideas from git, docker and other software.  By defining a\nlightweight interface around the inputs and outputs of an\nanalysis, a lot of the repetitive work for reproducible\nresearch can be automated.  We define a simple format for\norganising and describing work that facilitates collaborative\nreproducible research and acknowledges that all analyses are\nrun multiple times over their lifespans.",
    "version": "2.0.1",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\nRobert Ashton [aut],\nMartin Eden [aut],\nAlex Hill [aut],\nWes Hinsley [aut],\nMantra Kusumgar [aut],\nPaul Li\u00e9tar [aut],\nJames Thompson [aut],\nKaty Gaythorpe [aut],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "https://github.com/mrc-ide/orderly",
    "bug_reports": "https://github.com/mrc-ide/orderly/issues",
    "repository": "",
    "exports": [
      [
        "orderly_artefact"
      ],
      [
        "orderly_cleanup"
      ],
      [
        "orderly_cleanup_status"
      ],
      [
        "orderly_compare_packets"
      ],
      [
        "orderly_comparison_explain"
      ],
      [
        "orderly_config"
      ],
      [
        "orderly_config_set"
      ],
      [
        "orderly_copy_files"
      ],
      [
        "orderly_dependency"
      ],
      [
        "orderly_description"
      ],
      [
        "orderly_example"
      ],
      [
        "orderly_example_show"
      ],
      [
        "orderly_gitignore_update"
      ],
      [
        "orderly_hash_data"
      ],
      [
        "orderly_hash_file"
      ],
      [
        "orderly_init"
      ],
      [
        "orderly_interactive_set_search_options"
      ],
      [
        "orderly_list_src"
      ],
      [
        "orderly_location_add"
      ],
      [
        "orderly_location_add_http"
      ],
      [
        "orderly_location_add_packit"
      ],
      [
        "orderly_location_add_path"
      ],
      [
        "orderly_location_fetch_metadata"
      ],
      [
        "orderly_location_list"
      ],
      [
        "orderly_location_pull"
      ],
      [
        "orderly_location_pull_metadata"
      ],
      [
        "orderly_location_pull_packet"
      ],
      [
        "orderly_location_push"
      ],
      [
        "orderly_location_remove"
      ],
      [
        "orderly_location_rename"
      ],
      [
        "orderly_metadata"
      ],
      [
        "orderly_metadata_extract"
      ],
      [
        "orderly_metadata_read"
      ],
      [
        "orderly_migrate_source"
      ],
      [
        "orderly_new"
      ],
      [
        "orderly_parameters"
      ],
      [
        "orderly_parse_expr"
      ],
      [
        "orderly_parse_file"
      ],
      [
        "orderly_plugin_add_metadata"
      ],
      [
        "orderly_plugin_context"
      ],
      [
        "orderly_plugin_register"
      ],
      [
        "orderly_prune_orphans"
      ],
      [
        "orderly_query"
      ],
      [
        "orderly_query_explain"
      ],
      [
        "orderly_resource"
      ],
      [
        "orderly_run"
      ],
      [
        "orderly_run_info"
      ],
      [
        "orderly_search"
      ],
      [
        "orderly_search_options"
      ],
      [
        "orderly_shared_resource"
      ],
      [
        "orderly_strict_mode"
      ],
      [
        "orderly_validate_archive"
      ]
    ],
    "topics": [],
    "score": 9.8877,
    "stars": 10,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "orderly Lightweight Reproducible Reporting Distributed reproducible computing framework, adopting\nideas from git, docker and other software.  By defining a\nlightweight interface around the inputs and outputs of an\nanalysis, a lot of the repetitive work for reproducible\nresearch can be automated.  We define a simple format for\norganising and describing work that facilitates collaborative\nreproducible research and acknowledges that all analyses are\nrun multiple times over their lifespans. orderly_artefact orderly_cleanup orderly_cleanup_status orderly_compare_packets orderly_comparison_explain orderly_config orderly_config_set orderly_copy_files orderly_dependency orderly_description orderly_example orderly_example_show orderly_gitignore_update orderly_hash_data orderly_hash_file orderly_init orderly_interactive_set_search_options orderly_list_src orderly_location_add orderly_location_add_http orderly_location_add_packit orderly_location_add_path orderly_location_fetch_metadata orderly_location_list orderly_location_pull orderly_location_pull_metadata orderly_location_pull_packet orderly_location_push orderly_location_remove orderly_location_rename orderly_metadata orderly_metadata_extract orderly_metadata_read orderly_migrate_source orderly_new orderly_parameters orderly_parse_expr orderly_parse_file orderly_plugin_add_metadata orderly_plugin_context orderly_plugin_register orderly_prune_orphans orderly_query orderly_query_explain orderly_resource orderly_run orderly_run_info orderly_search orderly_search_options orderly_shared_resource orderly_strict_mode orderly_validate_archive "
  },
  {
    "id": 1051,
    "package_name": "ragnar",
    "title": "Retrieval-Augmented Generation (RAG) Workflows",
    "description": "Provides tools for implementing Retrieval-Augmented\nGeneration (RAG) workflows with Large Language Models (LLM).\nIncludes functions for document processing, text chunking,\nembedding generation, storage management, and content\nretrieval. Supports various document types and embedding\nproviders ('Ollama', 'OpenAI'), with 'DuckDB' as the default\nstorage backend. Integrates with the 'ellmer' package to equip\nchat objects with retrieval capabilities. Designed to offer\nboth sensible defaults and customization options with\ntransparent access to intermediate outputs.  For a review of\nretrieval-augmented generation methods, see Gao et al. (2023)\n\"Retrieval-Augmented Generation for Large Language Models: A\nSurvey\" <doi:10.48550/arXiv.2312.10997>.",
    "version": "0.2.1.9000",
    "maintainer": "Tomasz Kalinowski <tomasz@posit.co>",
    "author": "Tomasz Kalinowski [aut, cre],\nDaniel Falbel [aut],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://ragnar.tidyverse.org/, https://github.com/tidyverse/ragnar",
    "bug_reports": "https://github.com/tidyverse/ragnar/issues",
    "repository": "",
    "exports": [
      [
        "chunks_deoverlap"
      ],
      [
        "embed_azure_openai"
      ],
      [
        "embed_bedrock"
      ],
      [
        "embed_databricks"
      ],
      [
        "embed_google_gemini"
      ],
      [
        "embed_google_vertex"
      ],
      [
        "embed_lm_studio"
      ],
      [
        "embed_ollama"
      ],
      [
        "embed_openai"
      ],
      [
        "embed_snowflake"
      ],
      [
        "markdown_chunk"
      ],
      [
        "markdown_frame"
      ],
      [
        "markdown_segment"
      ],
      [
        "MarkdownDocument"
      ],
      [
        "MarkdownDocumentChunks"
      ],
      [
        "mcp_serve_store"
      ],
      [
        "ragnar_chunk"
      ],
      [
        "ragnar_chunk_segments"
      ],
      [
        "ragnar_chunks_view"
      ],
      [
        "ragnar_find_links"
      ],
      [
        "ragnar_read"
      ],
      [
        "ragnar_read_document"
      ],
      [
        "ragnar_register_tool_retrieve"
      ],
      [
        "ragnar_retrieve"
      ],
      [
        "ragnar_retrieve_bm25"
      ],
      [
        "ragnar_retrieve_vss"
      ],
      [
        "ragnar_segment"
      ],
      [
        "ragnar_store_atlas"
      ],
      [
        "ragnar_store_build_index"
      ],
      [
        "ragnar_store_connect"
      ],
      [
        "ragnar_store_create"
      ],
      [
        "ragnar_store_ingest"
      ],
      [
        "ragnar_store_insert"
      ],
      [
        "ragnar_store_inspect"
      ],
      [
        "ragnar_store_update"
      ],
      [
        "read_as_markdown"
      ]
    ],
    "topics": [],
    "score": 9.7775,
    "stars": 151,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "ragnar Retrieval-Augmented Generation (RAG) Workflows Provides tools for implementing Retrieval-Augmented\nGeneration (RAG) workflows with Large Language Models (LLM).\nIncludes functions for document processing, text chunking,\nembedding generation, storage management, and content\nretrieval. Supports various document types and embedding\nproviders ('Ollama', 'OpenAI'), with 'DuckDB' as the default\nstorage backend. Integrates with the 'ellmer' package to equip\nchat objects with retrieval capabilities. Designed to offer\nboth sensible defaults and customization options with\ntransparent access to intermediate outputs.  For a review of\nretrieval-augmented generation methods, see Gao et al. (2023)\n\"Retrieval-Augmented Generation for Large Language Models: A\nSurvey\" <doi:10.48550/arXiv.2312.10997>. chunks_deoverlap embed_azure_openai embed_bedrock embed_databricks embed_google_gemini embed_google_vertex embed_lm_studio embed_ollama embed_openai embed_snowflake markdown_chunk markdown_frame markdown_segment MarkdownDocument MarkdownDocumentChunks mcp_serve_store ragnar_chunk ragnar_chunk_segments ragnar_chunks_view ragnar_find_links ragnar_read ragnar_read_document ragnar_register_tool_retrieve ragnar_retrieve ragnar_retrieve_bm25 ragnar_retrieve_vss ragnar_segment ragnar_store_atlas ragnar_store_build_index ragnar_store_connect ragnar_store_create ragnar_store_ingest ragnar_store_insert ragnar_store_inspect ragnar_store_update read_as_markdown "
  },
  {
    "id": 1363,
    "package_name": "toastui",
    "title": "Interactive Tables, Calendars, Charts and Editor for the Web",
    "description": "Create interactive tables, calendars, charts and markdown\nWYSIWYG editor with 'TOAST UI' <https://ui.toast.com/>\nlibraries to integrate in 'shiny' applications or 'rmarkdown'\n'HTML' documents.",
    "version": "0.4.0.9000",
    "maintainer": "Victor Perrier <victor.perrier@dreamrs.fr>",
    "author": "Victor Perrier [aut, cre, cph],\nFanny Meyer [aut],\nNHN FE Development Lab [cph] (tui-grid, tui-calendar, tui-chart\nlibraries)",
    "url": "https://dreamrs.github.io/toastui/",
    "bug_reports": "https://github.com/dreamRs/toastui/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "caes"
      ],
      [
        "cal_demo_data"
      ],
      [
        "cal_demo_props"
      ],
      [
        "cal_events"
      ],
      [
        "cal_month_options"
      ],
      [
        "cal_props"
      ],
      [
        "cal_proxy_add"
      ],
      [
        "cal_proxy_clear"
      ],
      [
        "cal_proxy_clear_selection"
      ],
      [
        "cal_proxy_date"
      ],
      [
        "cal_proxy_delete"
      ],
      [
        "cal_proxy_next"
      ],
      [
        "cal_proxy_options"
      ],
      [
        "cal_proxy_prev"
      ],
      [
        "cal_proxy_today"
      ],
      [
        "cal_proxy_toggle"
      ],
      [
        "cal_proxy_update"
      ],
      [
        "cal_proxy_view"
      ],
      [
        "cal_schedules"
      ],
      [
        "cal_template"
      ],
      [
        "cal_theme"
      ],
      [
        "cal_timezone"
      ],
      [
        "cal_week_options"
      ],
      [
        "calendar"
      ],
      [
        "calendar_proxy"
      ],
      [
        "calendarOutput"
      ],
      [
        "chart"
      ],
      [
        "chart_labs"
      ],
      [
        "chart_options"
      ],
      [
        "chartOutput"
      ],
      [
        "datagrid"
      ],
      [
        "datagrid_proxy"
      ],
      [
        "datagridOutput"
      ],
      [
        "datagridOutput2"
      ],
      [
        "editor"
      ],
      [
        "editor_proxy"
      ],
      [
        "editor_proxy_change_preview"
      ],
      [
        "editor_proxy_hide"
      ],
      [
        "editor_proxy_insert"
      ],
      [
        "editor_proxy_reset"
      ],
      [
        "editor_proxy_show"
      ],
      [
        "editorOutput"
      ],
      [
        "grid_click"
      ],
      [
        "grid_col_button"
      ],
      [
        "grid_col_checkbox"
      ],
      [
        "grid_colorbar"
      ],
      [
        "grid_columns"
      ],
      [
        "grid_columns_opts"
      ],
      [
        "grid_complex_header"
      ],
      [
        "grid_editor"
      ],
      [
        "grid_editor_date"
      ],
      [
        "grid_editor_opts"
      ],
      [
        "grid_filters"
      ],
      [
        "grid_format"
      ],
      [
        "grid_header"
      ],
      [
        "grid_proxy_add_row"
      ],
      [
        "grid_proxy_delete_row"
      ],
      [
        "grid_row_merge"
      ],
      [
        "grid_selection_cell"
      ],
      [
        "grid_selection_row"
      ],
      [
        "grid_sparkline"
      ],
      [
        "grid_style_cell"
      ],
      [
        "grid_style_cells"
      ],
      [
        "grid_style_column"
      ],
      [
        "grid_style_row"
      ],
      [
        "grid_summary"
      ],
      [
        "guess_colwidths_options"
      ],
      [
        "JS"
      ],
      [
        "navigation_options"
      ],
      [
        "renderCalendar"
      ],
      [
        "renderChart"
      ],
      [
        "renderDatagrid"
      ],
      [
        "renderDatagrid2"
      ],
      [
        "renderEditor"
      ],
      [
        "reset_grid_theme"
      ],
      [
        "set_grid_lang"
      ],
      [
        "set_grid_theme"
      ],
      [
        "validateOpts"
      ]
    ],
    "topics": [
      [
        "calendar"
      ],
      [
        "charts"
      ],
      [
        "datagrid"
      ],
      [
        "htmlwidgets"
      ],
      [
        "tables"
      ],
      [
        "wysiwyg"
      ]
    ],
    "score": 9.6572,
    "stars": 93,
    "primary_category": "visualization",
    "source_universe": "dreamrs",
    "search_text": "toastui Interactive Tables, Calendars, Charts and Editor for the Web Create interactive tables, calendars, charts and markdown\nWYSIWYG editor with 'TOAST UI' <https://ui.toast.com/>\nlibraries to integrate in 'shiny' applications or 'rmarkdown'\n'HTML' documents. %>% caes cal_demo_data cal_demo_props cal_events cal_month_options cal_props cal_proxy_add cal_proxy_clear cal_proxy_clear_selection cal_proxy_date cal_proxy_delete cal_proxy_next cal_proxy_options cal_proxy_prev cal_proxy_today cal_proxy_toggle cal_proxy_update cal_proxy_view cal_schedules cal_template cal_theme cal_timezone cal_week_options calendar calendar_proxy calendarOutput chart chart_labs chart_options chartOutput datagrid datagrid_proxy datagridOutput datagridOutput2 editor editor_proxy editor_proxy_change_preview editor_proxy_hide editor_proxy_insert editor_proxy_reset editor_proxy_show editorOutput grid_click grid_col_button grid_col_checkbox grid_colorbar grid_columns grid_columns_opts grid_complex_header grid_editor grid_editor_date grid_editor_opts grid_filters grid_format grid_header grid_proxy_add_row grid_proxy_delete_row grid_row_merge grid_selection_cell grid_selection_row grid_sparkline grid_style_cell grid_style_cells grid_style_column grid_style_row grid_summary guess_colwidths_options JS navigation_options renderCalendar renderChart renderDatagrid renderDatagrid2 renderEditor reset_grid_theme set_grid_lang set_grid_theme validateOpts calendar charts datagrid htmlwidgets tables wysiwyg"
  },
  {
    "id": 950,
    "package_name": "parzer",
    "title": "Parse Messy Geographic Coordinates",
    "description": "Parse messy geographic coordinates from various character\nformats to decimal degree numeric values. Parse coordinates\ninto their parts (degree, minutes, seconds); calculate\nhemisphere from coordinates; pull out individually degrees,\nminutes, or seconds; add and subtract degrees, minutes, and\nseconds. C++ code herein originally inspired from code written\nby Jeffrey D. Bogan, but then completely re-written.",
    "version": "0.4.4",
    "maintainer": "Alban Sagouis <sagouis@pm.me>",
    "author": "Scott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nAlban Sagouis [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-3827-1063>),\nAlec L. Robitaille [ctb] (ORCID:\n<https://orcid.org/0000-0002-4706-1762>),\nMa\u00eblle Salmon [ctb] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nHiroaki Yutani [ctb],\nJeffrey Bogan [ctb] (C++ code originally from Jeffrey Bogan, but\ncompletely re-written),\nJulien Brun [rev] (ORCID: <https://orcid.org/0000-0002-7751-6238>,\nJulien Brun reviewed the package, see\nhttps://github.com/ropensci/onboarding/issues/341),\nMaria Munaf\u00f3 [rev] (Maria Munaf\u00f3 reviewed the package, see\nhttps://github.com/ropensci/onboarding/issues/341),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>),\nGerman Centre for Integrative Biodiversity Research (iDiv)\nHalle-Jena-Leipzig [fnd] (ROR: <https://ror.org/01jty7g66>)",
    "url": "https://github.com/ropensci/parzer,\nhttps://docs.ropensci.org/parzer/",
    "bug_reports": "https://github.com/ropensci/parzer/issues",
    "repository": "",
    "exports": [
      [
        "parse_hemisphere"
      ],
      [
        "parse_lat"
      ],
      [
        "parse_llstr"
      ],
      [
        "parse_lon"
      ],
      [
        "parse_lon_lat"
      ],
      [
        "parse_parts_lat"
      ],
      [
        "parse_parts_lon"
      ],
      [
        "pz_d"
      ],
      [
        "pz_degree"
      ],
      [
        "pz_m"
      ],
      [
        "pz_minute"
      ],
      [
        "pz_s"
      ],
      [
        "pz_second"
      ]
    ],
    "topics": [
      [
        "geospatial"
      ],
      [
        "data"
      ],
      [
        "latitude"
      ],
      [
        "longitude"
      ],
      [
        "parser"
      ],
      [
        "coordinates"
      ],
      [
        "geo"
      ],
      [
        "quarto"
      ],
      [
        "cpp"
      ]
    ],
    "score": 9.5117,
    "stars": 64,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "parzer Parse Messy Geographic Coordinates Parse messy geographic coordinates from various character\nformats to decimal degree numeric values. Parse coordinates\ninto their parts (degree, minutes, seconds); calculate\nhemisphere from coordinates; pull out individually degrees,\nminutes, or seconds; add and subtract degrees, minutes, and\nseconds. C++ code herein originally inspired from code written\nby Jeffrey D. Bogan, but then completely re-written. parse_hemisphere parse_lat parse_llstr parse_lon parse_lon_lat parse_parts_lat parse_parts_lon pz_d pz_degree pz_m pz_minute pz_s pz_second geospatial data latitude longitude parser coordinates geo quarto cpp"
  },
  {
    "id": 1357,
    "package_name": "timevis",
    "title": "Create Interactive Timeline Visualizations in R",
    "description": "Create rich and fully interactive timeline visualizations.\nTimelines can be included in Shiny apps or R markdown\ndocuments. 'timevis' includes an extensive API to manipulate a\ntimeline after creation, and supports getting data out of the\nvisualization into R. Based on the 'vis.js' Timeline JavaScript\nlibrary.",
    "version": "2.1.0.9000",
    "maintainer": "Dean Attali <daattali@gmail.com>",
    "author": "Dean Attali [aut, cre] (R interface, ORCID:\n<https://orcid.org/0000-0002-5645-3493>),\nAlmende B.V. [aut, cph] (vis.js Timeline library,\nhttps://visjs.github.io/vis-timeline/docs/timeline/)",
    "url": "https://github.com/daattali/timevis,\nhttps://daattali.com/shiny/timevis-demo/",
    "bug_reports": "https://github.com/daattali/timevis/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "addCustomTime"
      ],
      [
        "addItem"
      ],
      [
        "addItems"
      ],
      [
        "centerItem"
      ],
      [
        "centerTime"
      ],
      [
        "fitWindow"
      ],
      [
        "removeCustomTime"
      ],
      [
        "removeItem"
      ],
      [
        "renderTimevis"
      ],
      [
        "runExample"
      ],
      [
        "setCurrentTime"
      ],
      [
        "setCustomTime"
      ],
      [
        "setGroups"
      ],
      [
        "setItems"
      ],
      [
        "setOptions"
      ],
      [
        "setSelection"
      ],
      [
        "setWindow"
      ],
      [
        "timevis"
      ],
      [
        "timevisOutput"
      ],
      [
        "zoomIn"
      ],
      [
        "zoomOut"
      ]
    ],
    "topics": [
      [
        "shiny"
      ],
      [
        "shiny-r"
      ]
    ],
    "score": 9.3833,
    "stars": 675,
    "primary_category": "visualization",
    "source_universe": "daattali",
    "search_text": "timevis Create Interactive Timeline Visualizations in R Create rich and fully interactive timeline visualizations.\nTimelines can be included in Shiny apps or R markdown\ndocuments. 'timevis' includes an extensive API to manipulate a\ntimeline after creation, and supports getting data out of the\nvisualization into R. Based on the 'vis.js' Timeline JavaScript\nlibrary. %>% addCustomTime addItem addItems centerItem centerTime fitWindow removeCustomTime removeItem renderTimevis runExample setCurrentTime setCustomTime setGroups setItems setOptions setSelection setWindow timevis timevisOutput zoomIn zoomOut shiny shiny-r"
  },
  {
    "id": 1321,
    "package_name": "textreuse",
    "title": "Detect Text Reuse and Document Similarity",
    "description": "Tools for measuring similarity among documents and\ndetecting passages which have been reused. Implements shingled\nn-gram, skip n-gram, and other tokenizers;\nsimilarity/dissimilarity functions; pairwise comparisons;\nminhash and locality sensitive hashing algorithms; and a\nversion of the Smith-Waterman local alignment algorithm\nsuitable for natural language.",
    "version": "0.1.5",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-9200-1016>),\nLincoln Mullen [aut] (ORCID: <https://orcid.org/0000-0001-5103-6917>)",
    "url": "https://docs.ropensci.org/textreuse (website)\nhttps://github.com/ropensci/textreuse",
    "bug_reports": "https://github.com/ropensci/textreuse/issues",
    "repository": "",
    "exports": [
      [
        "align_local"
      ],
      [
        "content"
      ],
      [
        "content<-"
      ],
      [
        "filenames"
      ],
      [
        "has_content"
      ],
      [
        "has_hashes"
      ],
      [
        "has_minhashes"
      ],
      [
        "has_tokens"
      ],
      [
        "hash_string"
      ],
      [
        "hashes"
      ],
      [
        "hashes<-"
      ],
      [
        "is.TextReuseCorpus"
      ],
      [
        "is.TextReuseTextDocument"
      ],
      [
        "jaccard_bag_similarity"
      ],
      [
        "jaccard_dissimilarity"
      ],
      [
        "jaccard_similarity"
      ],
      [
        "lsh"
      ],
      [
        "lsh_candidates"
      ],
      [
        "lsh_compare"
      ],
      [
        "lsh_probability"
      ],
      [
        "lsh_query"
      ],
      [
        "lsh_subset"
      ],
      [
        "lsh_threshold"
      ],
      [
        "meta"
      ],
      [
        "meta<-"
      ],
      [
        "minhash_generator"
      ],
      [
        "minhashes"
      ],
      [
        "minhashes<-"
      ],
      [
        "pairwise_candidates"
      ],
      [
        "pairwise_compare"
      ],
      [
        "ratio_of_matches"
      ],
      [
        "rehash"
      ],
      [
        "skipped"
      ],
      [
        "TextReuseCorpus"
      ],
      [
        "TextReuseTextDocument"
      ],
      [
        "tokenize"
      ],
      [
        "tokenize_ngrams"
      ],
      [
        "tokenize_sentences"
      ],
      [
        "tokenize_skip_ngrams"
      ],
      [
        "tokenize_words"
      ],
      [
        "tokens"
      ],
      [
        "tokens<-"
      ],
      [
        "wordcount"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ],
      [
        "cpp"
      ]
    ],
    "score": 9.3355,
    "stars": 202,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "textreuse Detect Text Reuse and Document Similarity Tools for measuring similarity among documents and\ndetecting passages which have been reused. Implements shingled\nn-gram, skip n-gram, and other tokenizers;\nsimilarity/dissimilarity functions; pairwise comparisons;\nminhash and locality sensitive hashing algorithms; and a\nversion of the Smith-Waterman local alignment algorithm\nsuitable for natural language. align_local content content<- filenames has_content has_hashes has_minhashes has_tokens hash_string hashes hashes<- is.TextReuseCorpus is.TextReuseTextDocument jaccard_bag_similarity jaccard_dissimilarity jaccard_similarity lsh lsh_candidates lsh_compare lsh_probability lsh_query lsh_subset lsh_threshold meta meta<- minhash_generator minhashes minhashes<- pairwise_candidates pairwise_compare ratio_of_matches rehash skipped TextReuseCorpus TextReuseTextDocument tokenize tokenize_ngrams tokenize_sentences tokenize_skip_ngrams tokenize_words tokens tokens<- wordcount peer-reviewed cpp"
  },
  {
    "id": 1297,
    "package_name": "tabulapdf",
    "title": "Extract Tables from PDF Documents",
    "description": "Bindings for the 'Tabula' <https://tabula.technology/>\n'Java' library, which can extract tables from PDF files. This\ntool can reduce time and effort in data extraction processes in\nfields like investigative journalism. It allows for automatic\nand manual table extraction, the latter facilitated through a\n'Shiny' interface, enabling manual areas selection\\ with a\ncomputer mouse for data retrieval.",
    "version": "1.0.5-5",
    "maintainer": "Mauricio Vargas Sepulveda <m.sepulveda@mail.utoronto.ca>",
    "author": "Thomas J. Leeper [aut] (ORCID: <https://orcid.org/0000-0003-4097-6326>),\nMauricio Vargas Sepulveda [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1017-7574>),\nTom Paskhalis [aut] (ORCID: <https://orcid.org/0000-0001-9298-8850>),\nManuel Aristaran [ctb],\nDavid Gohel [ctb] (rOpenSci reviewer),\nLincoln Mullen [ctb] (rOpenSci reviewer),\nMunk School of Global Affairs and Public Policy [fnd]",
    "url": "https://docs.ropensci.org/tabulapdf/ (website)\nhttps://github.com/ropensci/tabulapdf/",
    "bug_reports": "https://github.com/ropensci/tabulapdf/issues/",
    "repository": "",
    "exports": [
      [
        "extract_areas"
      ],
      [
        "extract_metadata"
      ],
      [
        "extract_tables"
      ],
      [
        "extract_text"
      ],
      [
        "get_n_pages"
      ],
      [
        "get_page_dims"
      ],
      [
        "locate_areas"
      ],
      [
        "make_thumbnails"
      ],
      [
        "merge_pdfs"
      ],
      [
        "split_pdf"
      ],
      [
        "stop_logging"
      ]
    ],
    "topics": [
      [
        "java"
      ],
      [
        "pdf"
      ],
      [
        "pdf-document"
      ],
      [
        "peer-reviewed"
      ],
      [
        "ropensci"
      ],
      [
        "tabula"
      ],
      [
        "tabular-data"
      ],
      [
        "openjdk"
      ]
    ],
    "score": 9.2821,
    "stars": 563,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tabulapdf Extract Tables from PDF Documents Bindings for the 'Tabula' <https://tabula.technology/>\n'Java' library, which can extract tables from PDF files. This\ntool can reduce time and effort in data extraction processes in\nfields like investigative journalism. It allows for automatic\nand manual table extraction, the latter facilitated through a\n'Shiny' interface, enabling manual areas selection\\ with a\ncomputer mouse for data retrieval. extract_areas extract_metadata extract_tables extract_text get_n_pages get_page_dims locate_areas make_thumbnails merge_pdfs split_pdf stop_logging java pdf pdf-document peer-reviewed ropensci tabula tabular-data openjdk"
  },
  {
    "id": 1033,
    "package_name": "qgisprocess",
    "title": "Use 'QGIS' Processing Algorithms",
    "description": "Provides seamless access to the 'QGIS'\n(<https://qgis.org>) processing toolbox using the standalone\n'qgis_process' command-line utility.  Both native and\nthird-party (plugin) processing providers are supported.\nBeside referring data sources from file, also common objects\nfrom 'sf', 'terra' and 'stars' are supported. The native\nprocessing algorithms are documented by QGIS.org (2024)\n<https://docs.qgis.org/latest/en/docs/user_manual/processing_algs/>.",
    "version": "0.4.1.9000",
    "maintainer": "Floris Vanderhaeghe <floris.vanderhaeghe@inbo.be>",
    "author": "Dewey Dunnington [aut] (ORCID: <https://orcid.org/0000-0002-9415-4582>,\naffiliation: Voltron Data),\nFloris Vanderhaeghe [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6378-6229>, affiliation: Research\nInstitute for Nature and Forest (INBO)),\nJan Caha [aut] (ORCID: <https://orcid.org/0000-0003-0165-0606>),\nJannes Muenchow [aut] (ORCID: <https://orcid.org/0000-0001-7834-4717>),\nAntony Barja [ctb] (ORCID: <https://orcid.org/0000-0001-5921-2858>),\nRobin Lovelace [ctb] (ORCID: <https://orcid.org/0000-0001-5679-6536>),\nJakub Nowosad [ctb] (ORCID: <https://orcid.org/0000-0002-1057-3721>),\nResearch Institute for Nature and Forest (INBO) [cph, fnd]\n(https://www.inbo.be/en/)",
    "url": "https://r-spatial.github.io/qgisprocess/,\nhttps://github.com/r-spatial/qgisprocess",
    "bug_reports": "https://github.com/r-spatial/qgisprocess/issues",
    "repository": "",
    "exports": [
      [
        "as_qgis_argument"
      ],
      [
        "has_qgis"
      ],
      [
        "qgis_algorithms"
      ],
      [
        "qgis_arguments"
      ],
      [
        "qgis_as_brick"
      ],
      [
        "qgis_as_raster"
      ],
      [
        "qgis_as_terra"
      ],
      [
        "qgis_clean_argument"
      ],
      [
        "qgis_clean_result"
      ],
      [
        "qgis_clean_tmp"
      ],
      [
        "qgis_configure"
      ],
      [
        "qgis_description"
      ],
      [
        "qgis_detect_macos"
      ],
      [
        "qgis_detect_macos_paths"
      ],
      [
        "qgis_detect_paths"
      ],
      [
        "qgis_detect_windows"
      ],
      [
        "qgis_detect_windows_paths"
      ],
      [
        "qgis_dict_input"
      ],
      [
        "qgis_disable_plugins"
      ],
      [
        "qgis_enable_plugins"
      ],
      [
        "qgis_extract_output"
      ],
      [
        "qgis_extract_output_by_class"
      ],
      [
        "qgis_extract_output_by_name"
      ],
      [
        "qgis_extract_output_by_position"
      ],
      [
        "qgis_function"
      ],
      [
        "qgis_get_argument_specs"
      ],
      [
        "qgis_get_description"
      ],
      [
        "qgis_get_output_specs"
      ],
      [
        "qgis_has_algorithm"
      ],
      [
        "qgis_has_plugin"
      ],
      [
        "qgis_has_provider"
      ],
      [
        "qgis_list_input"
      ],
      [
        "qgis_output"
      ],
      [
        "qgis_outputs"
      ],
      [
        "qgis_path"
      ],
      [
        "qgis_pipe"
      ],
      [
        "qgis_plugins"
      ],
      [
        "qgis_providers"
      ],
      [
        "qgis_result_args"
      ],
      [
        "qgis_result_clean"
      ],
      [
        "qgis_result_single"
      ],
      [
        "qgis_result_status"
      ],
      [
        "qgis_result_stderr"
      ],
      [
        "qgis_result_stdout"
      ],
      [
        "qgis_run"
      ],
      [
        "qgis_run_algorithm"
      ],
      [
        "qgis_run_algorithm_p"
      ],
      [
        "qgis_search_algorithms"
      ],
      [
        "qgis_show_help"
      ],
      [
        "qgis_tmp_base"
      ],
      [
        "qgis_tmp_clean"
      ],
      [
        "qgis_tmp_file"
      ],
      [
        "qgis_tmp_folder"
      ],
      [
        "qgis_tmp_raster"
      ],
      [
        "qgis_tmp_vector"
      ],
      [
        "qgis_unconfigure"
      ],
      [
        "qgis_use_json_input"
      ],
      [
        "qgis_use_json_output"
      ],
      [
        "qgis_using_json_input"
      ],
      [
        "qgis_using_json_output"
      ],
      [
        "qgis_version"
      ]
    ],
    "topics": [],
    "score": 9.2203,
    "stars": 214,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "qgisprocess Use 'QGIS' Processing Algorithms Provides seamless access to the 'QGIS'\n(<https://qgis.org>) processing toolbox using the standalone\n'qgis_process' command-line utility.  Both native and\nthird-party (plugin) processing providers are supported.\nBeside referring data sources from file, also common objects\nfrom 'sf', 'terra' and 'stars' are supported. The native\nprocessing algorithms are documented by QGIS.org (2024)\n<https://docs.qgis.org/latest/en/docs/user_manual/processing_algs/>. as_qgis_argument has_qgis qgis_algorithms qgis_arguments qgis_as_brick qgis_as_raster qgis_as_terra qgis_clean_argument qgis_clean_result qgis_clean_tmp qgis_configure qgis_description qgis_detect_macos qgis_detect_macos_paths qgis_detect_paths qgis_detect_windows qgis_detect_windows_paths qgis_dict_input qgis_disable_plugins qgis_enable_plugins qgis_extract_output qgis_extract_output_by_class qgis_extract_output_by_name qgis_extract_output_by_position qgis_function qgis_get_argument_specs qgis_get_description qgis_get_output_specs qgis_has_algorithm qgis_has_plugin qgis_has_provider qgis_list_input qgis_output qgis_outputs qgis_path qgis_pipe qgis_plugins qgis_providers qgis_result_args qgis_result_clean qgis_result_single qgis_result_status qgis_result_stderr qgis_result_stdout qgis_run qgis_run_algorithm qgis_run_algorithm_p qgis_search_algorithms qgis_show_help qgis_tmp_base qgis_tmp_clean qgis_tmp_file qgis_tmp_folder qgis_tmp_raster qgis_tmp_vector qgis_unconfigure qgis_use_json_input qgis_use_json_output qgis_using_json_input qgis_using_json_output qgis_version "
  },
  {
    "id": 324,
    "package_name": "bsicons",
    "title": "Easily Work with 'Bootstrap' Icons",
    "description": "Easily use 'Bootstrap' icons inside 'Shiny' apps and 'R\nMarkdown' documents. More generally, icons can be inserted in\nany 'htmltools' document through inline 'SVG'.",
    "version": "0.1.2.9000",
    "maintainer": "Carson Sievert <carson@posit.co>",
    "author": "Carson Sievert [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-4958-2844>),\nPosit Software, PBC [cph, fnd],\nMark Otto [cph] (Bootstrap icons maintainer)",
    "url": "https://github.com/rstudio/bsicons",
    "bug_reports": "https://github.com/rstudio/bsicons/issues",
    "repository": "",
    "exports": [
      [
        "bs_icon"
      ]
    ],
    "topics": [],
    "score": 8.9068,
    "stars": 16,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "bsicons Easily Work with 'Bootstrap' Icons Easily use 'Bootstrap' icons inside 'Shiny' apps and 'R\nMarkdown' documents. More generally, icons can be inserted in\nany 'htmltools' document through inline 'SVG'. bs_icon "
  },
  {
    "id": 68,
    "package_name": "GSODR",
    "title": "Global Surface Summary of the Day ('GSOD') Weather Data Client",
    "description": "Provides automated downloading, parsing, cleaning, unit\nconversion and formatting of Global Surface Summary of the Day\n('GSOD') weather data from the from the USA National Centers\nfor Environmental Information ('NCEI').  Units are converted\nfrom from United States Customary System ('USCS') units to\nInternational System of Units ('SI').  Stations may be\nindividually checked for number of missing days defined by the\nuser, where stations with too many missing observations are\nomitted.  Only stations with valid reported latitude and\nlongitude values are permitted in the final data.  Additional\nuseful elements, saturation vapour pressure ('es'), actual\nvapour pressure ('ea') and relative humidity ('RH') are\ncalculated from the original data using the improved\nAugust-Roche-Magnus approximation (Alduchov & Eskridge 1996)\nand included in the final data set.  The resulting metadata\ninclude station identification information, country, state,\nlatitude, longitude, elevation, weather observations and\nassociated flags.  For information on the 'GSOD' data from\n'NCEI', please see the 'GSOD' 'readme.txt' file available from,\n<https://www1.ncdc.noaa.gov/pub/data/gsod/readme.txt>.",
    "version": "5.0.0",
    "maintainer": "Adam H. Sparks <adamhsparks@gmail.com>",
    "author": "Adam H. Sparks [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0061-8359>),\nTomislav Hengl [aut] (ORCID: <https://orcid.org/0000-0002-9921-5129>),\nAndrew Nelson [aut] (ORCID: <https://orcid.org/0000-0002-7249-3778>),\nHugh Parsonage [cph, ctb] (ORCID:\n<https://orcid.org/0000-0003-4055-0835>),\nTaras Kaduk [ctb] (Suggestion for handling bulk station downloads more\nefficiently),\nGwenael Giboire [ctb] (Several bug reports in early versions and\ntesting feedback),\n\u0141ukasz Pawlik [ctb] (Reported bug in windspeed conversion calculation),\nRoss Darnell [ctb] (Reported bug in 'Windows OS' versions causing\n'GSOD' data untarring to fail, ORCID:\n<https://orcid.org/0000-0002-7973-6322>),\nTyler Widdison [ctb] (Reported bug where `nearest_stations()` did not\nreturn stations in order of nearest to farthest),\nWenbo Lv [ctb] (Provided suggestions for improving documentation),\nCurtin University [fnd, cph] (url: http://www.curtin.edu.au/, ROR:\n<https://ror.org/02n415q13>),\nGrains Research and Development Corporation [fnd, cph] (Project: GRDC\nProject CUR2210-005OPX (AAGI-CU), ROR: <https://ror.org/02xwr1996>)",
    "url": "https://docs.ropensci.org/GSODR/,\nhttps://github.com/ropensci/GSODR",
    "bug_reports": "https://github.com/ropensci/GSODR/issues",
    "repository": "",
    "exports": [
      [
        "get_GSOD"
      ],
      [
        "get_updates"
      ],
      [
        "nearest_stations"
      ],
      [
        "reformat_GSOD"
      ]
    ],
    "topics": [
      [
        "us-ncei"
      ],
      [
        "meteorological-data"
      ],
      [
        "global-weather"
      ],
      [
        "weather"
      ],
      [
        "weather-data"
      ],
      [
        "meteorology"
      ],
      [
        "station-data"
      ],
      [
        "surface-weather"
      ],
      [
        "data-access"
      ],
      [
        "us-ncdc"
      ],
      [
        "daily-data"
      ],
      [
        "daily-weather"
      ],
      [
        "global-data"
      ],
      [
        "gsod"
      ],
      [
        "historical-data"
      ],
      [
        "historical-weather"
      ],
      [
        "ncdc"
      ],
      [
        "ncei"
      ],
      [
        "weather-information"
      ],
      [
        "weather-stations"
      ]
    ],
    "score": 8.835,
    "stars": 97,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "GSODR Global Surface Summary of the Day ('GSOD') Weather Data Client Provides automated downloading, parsing, cleaning, unit\nconversion and formatting of Global Surface Summary of the Day\n('GSOD') weather data from the from the USA National Centers\nfor Environmental Information ('NCEI').  Units are converted\nfrom from United States Customary System ('USCS') units to\nInternational System of Units ('SI').  Stations may be\nindividually checked for number of missing days defined by the\nuser, where stations with too many missing observations are\nomitted.  Only stations with valid reported latitude and\nlongitude values are permitted in the final data.  Additional\nuseful elements, saturation vapour pressure ('es'), actual\nvapour pressure ('ea') and relative humidity ('RH') are\ncalculated from the original data using the improved\nAugust-Roche-Magnus approximation (Alduchov & Eskridge 1996)\nand included in the final data set.  The resulting metadata\ninclude station identification information, country, state,\nlatitude, longitude, elevation, weather observations and\nassociated flags.  For information on the 'GSOD' data from\n'NCEI', please see the 'GSOD' 'readme.txt' file available from,\n<https://www1.ncdc.noaa.gov/pub/data/gsod/readme.txt>. get_GSOD get_updates nearest_stations reformat_GSOD us-ncei meteorological-data global-weather weather weather-data meteorology station-data surface-weather data-access us-ncdc daily-data daily-weather global-data gsod historical-data historical-weather ncdc ncei weather-information weather-stations"
  },
  {
    "id": 36,
    "package_name": "DataPackageR",
    "title": "Construct Reproducible Analytic Data Sets as R Packages",
    "description": "A framework to help construct R data packages in a\nreproducible manner. Potentially time consuming processing of\nraw data sets into analysis ready data sets is done in a\nreproducible manner and decoupled from the usual 'R CMD build'\nprocess so that data sets can be processed into R objects in\nthe data package and the data package can then be shared,\nbuilt, and installed by others without the need to repeat\ncomputationally costly data processing.  The package maintains\ndata provenance by turning the data processing scripts into\npackage vignettes, as well as enforcing documentation and\nversion checking of included data objects. Data packages can be\nversion controlled on 'GitHub', and used to share data for\nmanuscripts, collaboration and reproducible research.",
    "version": "0.16.2",
    "maintainer": "Dave Slager <dslager@fredhutch.org>",
    "author": "Greg Finak [aut, cph] (Original author and creator of DataPackageR),\nPaul Obrecht [ctb],\nEllis Hughes [ctb] (ORCID: <https://orcid.org/0000-0003-0637-4436>),\nJimmy Fulp [ctb],\nMarie Vendettuoli [ctb] (ORCID:\n<https://orcid.org/0000-0001-9321-1410>),\nDave Slager [ctb, cre] (ORCID: <https://orcid.org/0000-0003-2525-2039>),\nJason Taylor [ctb],\nKara Woo [rev] (Kara reviewed the package for rOpenSci, see\n<https://github.com/ropensci/onboarding/issues/230>),\nWilliam Landau [rev] (William reviewed the package for rOpenSci, see\n<https://github.com/ropensci/onboarding/issues/230>)",
    "url": "https://github.com/ropensci/DataPackageR,\nhttps://docs.ropensci.org/DataPackageR/",
    "bug_reports": "https://github.com/ropensci/DataPackageR/issues",
    "repository": "",
    "exports": [
      [
        "assert_data_version"
      ],
      [
        "construct_yml_config"
      ],
      [
        "data_version"
      ],
      [
        "datapackage_skeleton"
      ],
      [
        "datapackage.skeleton"
      ],
      [
        "datapackager_object_read"
      ],
      [
        "dataVersion"
      ],
      [
        "document"
      ],
      [
        "keepDataObjects"
      ],
      [
        "package_build"
      ],
      [
        "project_data_path"
      ],
      [
        "project_extdata_path"
      ],
      [
        "project_path"
      ],
      [
        "use_data_object"
      ],
      [
        "use_ignore"
      ],
      [
        "use_processing_script"
      ],
      [
        "use_raw_dataset"
      ],
      [
        "yml_add_files"
      ],
      [
        "yml_add_objects"
      ],
      [
        "yml_disable_compile"
      ],
      [
        "yml_enable_compile"
      ],
      [
        "yml_find"
      ],
      [
        "yml_list_files"
      ],
      [
        "yml_list_objects"
      ],
      [
        "yml_remove_files"
      ],
      [
        "yml_remove_objects"
      ],
      [
        "yml_write"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ],
      [
        "reproducibility"
      ]
    ],
    "score": 8.792,
    "stars": 155,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "DataPackageR Construct Reproducible Analytic Data Sets as R Packages A framework to help construct R data packages in a\nreproducible manner. Potentially time consuming processing of\nraw data sets into analysis ready data sets is done in a\nreproducible manner and decoupled from the usual 'R CMD build'\nprocess so that data sets can be processed into R objects in\nthe data package and the data package can then be shared,\nbuilt, and installed by others without the need to repeat\ncomputationally costly data processing.  The package maintains\ndata provenance by turning the data processing scripts into\npackage vignettes, as well as enforcing documentation and\nversion checking of included data objects. Data packages can be\nversion controlled on 'GitHub', and used to share data for\nmanuscripts, collaboration and reproducible research. assert_data_version construct_yml_config data_version datapackage_skeleton datapackage.skeleton datapackager_object_read dataVersion document keepDataObjects package_build project_data_path project_extdata_path project_path use_data_object use_ignore use_processing_script use_raw_dataset yml_add_files yml_add_objects yml_disable_compile yml_enable_compile yml_find yml_list_files yml_list_objects yml_remove_files yml_remove_objects yml_write peer-reviewed reproducibility"
  },
  {
    "id": 735,
    "package_name": "katex",
    "title": "Rendering Math to HTML, 'MathML', or R-Documentation Format",
    "description": "Convert latex math expressions to HTML and 'MathML' for\nuse in markdown documents or package manual pages. The\nrendering is done in R using the V8 engine (i.e. server-side),\nwhich eliminates the need for embedding the 'MathJax' library\ninto your web pages. In addition a 'math-to-rd' wrapper is\nprovided to automatically render beautiful math in R\ndocumentation files.",
    "version": "1.5.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nKhan Academy and other contributors [cph] (KaTeX JavaScript library)",
    "url": "https://docs.ropensci.org/katex/,\nhttps://github.com/ropensci/katex\nhttps://katex.org/docs/options.html (upstream)",
    "bug_reports": "https://github.com/ropensci/katex/issues",
    "repository": "",
    "exports": [
      [
        "example_math"
      ],
      [
        "katex_html"
      ],
      [
        "katex_mathml"
      ],
      [
        "math_to_rd"
      ],
      [
        "render_math_in_html"
      ]
    ],
    "topics": [],
    "score": 8.7784,
    "stars": 37,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "katex Rendering Math to HTML, 'MathML', or R-Documentation Format Convert latex math expressions to HTML and 'MathML' for\nuse in markdown documents or package manual pages. The\nrendering is done in R using the V8 engine (i.e. server-side),\nwhich eliminates the need for embedding the 'MathJax' library\ninto your web pages. In addition a 'math-to-rd' wrapper is\nprovided to automatically render beautiful math in R\ndocumentation files. example_math katex_html katex_mathml math_to_rd render_math_in_html "
  },
  {
    "id": 621,
    "package_name": "gfonts",
    "title": "Offline 'Google' Fonts for 'Markdown' and 'Shiny'",
    "description": "Download 'Google' fonts and generate 'CSS' to use in\n'rmarkdown' documents and 'shiny' applications. Some popular\nfonts are included and ready to use.",
    "version": "0.2.0",
    "maintainer": "Victor Perrier <victor.perrier@dreamrs.fr>",
    "author": "Victor Perrier [aut, cre],\nFanny Meyer [aut],\nMario Ranftl [ctb, cph] (google-webfonts-helper)",
    "url": "https://dreamrs.github.io/gfonts/,\nhttps://github.com/dreamRs/gfonts",
    "bug_reports": "https://github.com/dreamRs/gfonts/issues",
    "repository": "",
    "exports": [
      [
        "download_font"
      ],
      [
        "generate_css"
      ],
      [
        "get_all_fonts"
      ],
      [
        "get_font_info"
      ],
      [
        "setup_font"
      ],
      [
        "tag_example"
      ],
      [
        "use_font"
      ],
      [
        "use_pkg_gfont"
      ]
    ],
    "topics": [
      [
        "fonts"
      ],
      [
        "rmarkdown"
      ],
      [
        "shiny"
      ]
    ],
    "score": 8.613,
    "stars": 117,
    "primary_category": "visualization",
    "source_universe": "dreamrs",
    "search_text": "gfonts Offline 'Google' Fonts for 'Markdown' and 'Shiny' Download 'Google' fonts and generate 'CSS' to use in\n'rmarkdown' documents and 'shiny' applications. Some popular\nfonts are included and ready to use. download_font generate_css get_all_fonts get_font_info setup_font tag_example use_font use_pkg_gfont fonts rmarkdown shiny"
  },
  {
    "id": 1096,
    "package_name": "revealjs",
    "title": "R Markdown Format for 'reveal.js' Presentations",
    "description": "R Markdown format for 'reveal.js' presentations, a\nframework for easily creating beautiful presentations using\nHTML.",
    "version": "0.9.1.9007",
    "maintainer": "Christophe Dervieux <cderv@posit.co>",
    "author": "Christophe Dervieux [cre],\nJJ Allaire [aut],\nHakim El Hattab [aut, cph] (reveal.js,\nhttps://github.com/hakimel/reveal.js),\nAsvin Goel [ctb, cph] (chalkboard plugin),\nGreg Denehy [ctb, cph] (menu plugin),\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/revealjs",
    "bug_reports": "https://github.com/rstudio/revealjs/issues",
    "repository": "",
    "exports": [
      [
        "revealjs_presentation"
      ]
    ],
    "topics": [],
    "score": 8.6117,
    "stars": 329,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "revealjs R Markdown Format for 'reveal.js' Presentations R Markdown format for 'reveal.js' presentations, a\nframework for easily creating beautiful presentations using\nHTML. revealjs_presentation "
  },
  {
    "id": 1463,
    "package_name": "xslt",
    "title": "Extensible Style-Sheet Language Transformations",
    "description": "An extension for the 'xml2' package to transform XML\ndocuments by applying an 'xslt' style-sheet.",
    "version": "1.5.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://ropensci.r-universe.dev/xslt\nhttps://docs.ropensci.org/xslt/",
    "bug_reports": "https://github.com/ropensci/xslt/issues",
    "repository": "",
    "exports": [
      [
        "xml_xslt"
      ],
      [
        "xslt_version"
      ]
    ],
    "topics": [
      [
        "xml"
      ],
      [
        "xslt"
      ],
      [
        "libxslt"
      ],
      [
        "libxml2"
      ],
      [
        "cpp"
      ]
    ],
    "score": 8.5831,
    "stars": 29,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "xslt Extensible Style-Sheet Language Transformations An extension for the 'xml2' package to transform XML\ndocuments by applying an 'xslt' style-sheet. xml_xslt xslt_version xml xslt libxslt libxml2 cpp"
  },
  {
    "id": 487,
    "package_name": "dittodb",
    "title": "A Test Environment for Database Requests",
    "description": "Testing and documenting code that communicates with remote\ndatabases can be painful. Although the interaction with R is\nusually relatively simple (e.g. data(frames) passed to and from\na database), because they rely on a separate service and the\ndata there, testing them can be difficult to set up,\nunsustainable in a continuous integration environment, or\nimpossible without replicating an entire production cluster.\nThis package addresses that by allowing you to make recordings\nfrom your database interactions and then play them back while\ntesting (or in other contexts) all without needing to spin up\nor have access to the database your code would typically\nconnect to.",
    "version": "0.1.9.9000",
    "maintainer": "Jonathan Keane <jkeane@gmail.com>",
    "author": "Jonathan Keane [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-7087-9776>),\nMauricio Vargas [aut] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\nHelen Miller [rev] (reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/366),\nEtienne Racine [rev] (reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/366)",
    "url": "https://dittodb.jonkeane.com/, https://github.com/ropensci/dittodb",
    "bug_reports": "https://github.com/ropensci/dittodb/issues",
    "repository": "",
    "exports": [
      [
        ".db_mock_paths"
      ],
      [
        ".dittodb_env"
      ],
      [
        "capture_db_requests"
      ],
      [
        "check_db_path"
      ],
      [
        "check_for_pkg"
      ],
      [
        "clean_statement"
      ],
      [
        "db_mock_paths"
      ],
      [
        "db_path_sanitize"
      ],
      [
        "dbBegin"
      ],
      [
        "dbClearResult"
      ],
      [
        "dbColumnInfo"
      ],
      [
        "dbCommit"
      ],
      [
        "dbDisconnect"
      ],
      [
        "dbExistsTable"
      ],
      [
        "dbFetch"
      ],
      [
        "dbGetInfo"
      ],
      [
        "dbGetQuery"
      ],
      [
        "dbGetRowsAffected"
      ],
      [
        "dbHasCompleted"
      ],
      [
        "dbListFields"
      ],
      [
        "dbListTables"
      ],
      [
        "dbMockConnect"
      ],
      [
        "dbQuoteIdentifier"
      ],
      [
        "dbQuoteString"
      ],
      [
        "dbRemoveTable"
      ],
      [
        "dbRollback"
      ],
      [
        "dbSendQuery"
      ],
      [
        "dbSendStatement"
      ],
      [
        "dbWriteTable"
      ],
      [
        "dittodb_debug_level"
      ],
      [
        "expect_sql"
      ],
      [
        "fetch"
      ],
      [
        "get_dbname"
      ],
      [
        "get_redactor"
      ],
      [
        "get_type"
      ],
      [
        "hash"
      ],
      [
        "hash_db_object"
      ],
      [
        "make_path"
      ],
      [
        "nycflights_sqlite"
      ],
      [
        "nycflights13_create_sql"
      ],
      [
        "nycflights13_create_sqlite"
      ],
      [
        "redact_columns"
      ],
      [
        "sanitize_table_id"
      ],
      [
        "serialize_bit64"
      ],
      [
        "set_dittodb_debug_level"
      ],
      [
        "start_db_capturing"
      ],
      [
        "start_mock_db"
      ],
      [
        "stop_db_capturing"
      ],
      [
        "stop_mock_db"
      ],
      [
        "use_dittodb"
      ],
      [
        "with_mock_db"
      ],
      [
        "with_mock_path"
      ]
    ],
    "topics": [],
    "score": 8.5756,
    "stars": 81,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "dittodb A Test Environment for Database Requests Testing and documenting code that communicates with remote\ndatabases can be painful. Although the interaction with R is\nusually relatively simple (e.g. data(frames) passed to and from\na database), because they rely on a separate service and the\ndata there, testing them can be difficult to set up,\nunsustainable in a continuous integration environment, or\nimpossible without replicating an entire production cluster.\nThis package addresses that by allowing you to make recordings\nfrom your database interactions and then play them back while\ntesting (or in other contexts) all without needing to spin up\nor have access to the database your code would typically\nconnect to. .db_mock_paths .dittodb_env capture_db_requests check_db_path check_for_pkg clean_statement db_mock_paths db_path_sanitize dbBegin dbClearResult dbColumnInfo dbCommit dbDisconnect dbExistsTable dbFetch dbGetInfo dbGetQuery dbGetRowsAffected dbHasCompleted dbListFields dbListTables dbMockConnect dbQuoteIdentifier dbQuoteString dbRemoveTable dbRollback dbSendQuery dbSendStatement dbWriteTable dittodb_debug_level expect_sql fetch get_dbname get_redactor get_type hash hash_db_object make_path nycflights_sqlite nycflights13_create_sql nycflights13_create_sqlite redact_columns sanitize_table_id serialize_bit64 set_dittodb_debug_level start_db_capturing start_mock_db stop_db_capturing stop_mock_db use_dittodb with_mock_db with_mock_path "
  },
  {
    "id": 425,
    "package_name": "crch",
    "title": "Censored Regression with Conditional Heteroscedasticity",
    "description": "Different approaches to censored or truncated regression\nwith conditional heteroscedasticity are provided. First,\ncontinuous distributions can be used for the (right and/or left\ncensored or truncated) response with separate linear predictors\nfor the mean and variance. Second, cumulative link models for\nordinal data (obtained by interval-censoring continuous data)\ncan be employed for heteroscedastic extended logistic\nregression (HXLR). In the latter type of models, the intercepts\ndepend on the thresholds that define the intervals.\nInfrastructure for working with censored or truncated normal,\nlogistic, and Student-t distributions, i.e., d/p/q/r functions\nand distributions3 objects.",
    "version": "1.2-2",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-0918-3766>),\nJakob W. Messner [aut] (ORCID: <https://orcid.org/0000-0002-1027-3673>),\nReto Stauffer [aut] (ORCID: <https://orcid.org/0000-0002-3798-5507>),\nIoannis Kosmidis [ctb] (ORCID: <https://orcid.org/0000-0003-1556-0302>),\nGeorg J. Mayr [ctb] (ORCID: <https://orcid.org/0000-0001-6661-9453>)",
    "url": "https://topmodels.R-Forge.R-project.org/crch/",
    "bug_reports": "https://topmodels.R-Forge.R-project.org/crch/contact.html",
    "repository": "",
    "exports": [
      [
        "CensoredLogistic"
      ],
      [
        "CensoredNormal"
      ],
      [
        "CensoredStudentsT"
      ],
      [
        "crch"
      ],
      [
        "crch.boost"
      ],
      [
        "crch.boost.fit"
      ],
      [
        "crch.control"
      ],
      [
        "crch.fit"
      ],
      [
        "crch.stabsel"
      ],
      [
        "dclogis"
      ],
      [
        "dcnorm"
      ],
      [
        "dct"
      ],
      [
        "dtlogis"
      ],
      [
        "dtnorm"
      ],
      [
        "dtt"
      ],
      [
        "getSummary.crch"
      ],
      [
        "hxlr"
      ],
      [
        "hxlr.control"
      ],
      [
        "pclogis"
      ],
      [
        "pcnorm"
      ],
      [
        "pct"
      ],
      [
        "ptlogis"
      ],
      [
        "ptnorm"
      ],
      [
        "ptt"
      ],
      [
        "qclogis"
      ],
      [
        "qcnorm"
      ],
      [
        "qct"
      ],
      [
        "qtlogis"
      ],
      [
        "qtnorm"
      ],
      [
        "qtt"
      ],
      [
        "rclogis"
      ],
      [
        "rcnorm"
      ],
      [
        "rct"
      ],
      [
        "rtlogis"
      ],
      [
        "rtnorm"
      ],
      [
        "rtt"
      ],
      [
        "trch"
      ],
      [
        "TruncatedLogistic"
      ],
      [
        "TruncatedNormal"
      ],
      [
        "TruncatedStudentsT"
      ]
    ],
    "topics": [
      [
        "quarto"
      ]
    ],
    "score": 8.5487,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "crch Censored Regression with Conditional Heteroscedasticity Different approaches to censored or truncated regression\nwith conditional heteroscedasticity are provided. First,\ncontinuous distributions can be used for the (right and/or left\ncensored or truncated) response with separate linear predictors\nfor the mean and variance. Second, cumulative link models for\nordinal data (obtained by interval-censoring continuous data)\ncan be employed for heteroscedastic extended logistic\nregression (HXLR). In the latter type of models, the intercepts\ndepend on the thresholds that define the intervals.\nInfrastructure for working with censored or truncated normal,\nlogistic, and Student-t distributions, i.e., d/p/q/r functions\nand distributions3 objects. CensoredLogistic CensoredNormal CensoredStudentsT crch crch.boost crch.boost.fit crch.control crch.fit crch.stabsel dclogis dcnorm dct dtlogis dtnorm dtt getSummary.crch hxlr hxlr.control pclogis pcnorm pct ptlogis ptnorm ptt qclogis qcnorm qct qtlogis qtnorm qtt rclogis rcnorm rct rtlogis rtnorm rtt trch TruncatedLogistic TruncatedNormal TruncatedStudentsT quarto"
  },
  {
    "id": 1290,
    "package_name": "swagger",
    "title": "Dynamically Generates Documentation from a 'Swagger' Compliant\nAPI",
    "description": "A collection of 'HTML', 'JavaScript', and 'CSS' assets\nthat dynamically generate beautiful documentation from a\n'Swagger' compliant API: <https://swagger.io/specification/>.",
    "version": "5.30.0",
    "maintainer": "Bruno Tremblay <cran@neoxone.com>",
    "author": "Barret Schloerke [aut] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\nJavier Luraschi [aut],\nBruno Tremblay [cre, ctb],\nRStudio [cph],\nSmartBear Software [aut, cph]",
    "url": "https://rstudio.github.io/swagger/,\nhttps://github.com/rstudio/swagger",
    "bug_reports": "https://github.com/rstudio/swagger/issues",
    "repository": "",
    "exports": [
      [
        "plumber_docs"
      ],
      [
        "swagger_index"
      ],
      [
        "swagger_path"
      ],
      [
        "swagger_spec"
      ]
    ],
    "topics": [],
    "score": 8.4674,
    "stars": 54,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "swagger Dynamically Generates Documentation from a 'Swagger' Compliant\nAPI A collection of 'HTML', 'JavaScript', and 'CSS' assets\nthat dynamically generate beautiful documentation from a\n'Swagger' compliant API: <https://swagger.io/specification/>. plumber_docs swagger_index swagger_path swagger_spec "
  },
  {
    "id": 531,
    "package_name": "epinowcast",
    "title": "A Bayesian Framework for Real-time Infectious Disease\nSurveillance",
    "description": "A modular Bayesian framework for real-time infectious\ndisease surveillance. Provides tools for nowcasting,\nreproduction number estimation, delay estimation, and\nforecasting from data subject to reporting delays,\nright-truncation, missing data, and incomplete ascertainment.\nUsers can build models suited to their setting using a flexible\nformula interface supporting fixed effects, random effects,\nrandom walks, and time-varying parameters, with options\nincluding parametric and non-parametric delay distributions\nwith optional modifiers (via discrete-time hazard models),\nrenewal processes, observation models, missing data imputation,\nand stratified analyses with partial pooling. By jointly\nestimating disease dynamics and reporting patterns, our\nframework enables earlier and more reliable detection of\ntrends. While designed with epidemiological applications in\nmind, the framework can be applied to any right-truncated time\nseries count data.",
    "version": "0.4.0",
    "maintainer": "Sam Abbott <contact@samabbott.co.uk>",
    "author": "Sam Abbott [aut, cre] (ORCID: <https://orcid.org/0000-0001-8057-8037>),\nAdrian Lison [aut] (ORCID: <https://orcid.org/0000-0002-6822-8437>),\nSebastian Funk [aut],\nCarl Pearson [aut] (ORCID: <https://orcid.org/0000-0003-0701-7860>),\nHugo Gruson [aut] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nFelix Guenther [aut] (ORCID: <https://orcid.org/0000-0001-6582-1174>),\nMichael DeWitt [aut] (ORCID: <https://orcid.org/0000-0001-8940-1967>),\nJames Mba Azam [aut] (ORCID: <https://orcid.org/0000-0001-5782-7330>),\nJessalyn Sebastian [aut] (ORCID:\n<https://orcid.org/0000-0002-1768-3229>),\nHannah Choi [ctb],\nPratik Gupte [ctb] (ORCID: <https://orcid.org/0000-0001-5294-7819>),\nJoel Hellewell [ctb] (ORCID: <https://orcid.org/0000-0003-2683-0849>),\nLuis Rivas [ctb],\nSang Woo Park [ctb] (ORCID: <https://orcid.org/0000-0003-2202-3361>),\nNathan McIntosh [ctb],\nKath Sherratt [ctb] (ORCID: <https://orcid.org/0000-0003-2049-3423>),\nNikos Bosse [ctb] (ORCID: <https://orcid.org/0000-0002-7750-5280>),\nAdam Howes [ctb] (ORCID: <https://orcid.org/0000-0003-2386-4031>),\nKaitlyn Johnson [ctb] (ORCID: <https://orcid.org/0000-0001-8011-0012>),\nBarbora Nemcova [ctb] (ORCID: <https://orcid.org/0009-0004-7565-4145>)",
    "url": "https://package.epinowcast.org,\nhttps://github.com/epinowcast/epinowcast/",
    "bug_reports": "https://github.com/epinowcast/epinowcast/issues/",
    "repository": "",
    "exports": [
      [
        "add_pmfs"
      ],
      [
        "as_forecast_sample"
      ],
      [
        "check_max_delay"
      ],
      [
        "coerce_date"
      ],
      [
        "convolution_matrix"
      ],
      [
        "enw_add_cumulative"
      ],
      [
        "enw_add_cumulative_membership"
      ],
      [
        "enw_add_delay"
      ],
      [
        "enw_add_incidence"
      ],
      [
        "enw_add_latest_obs_to_nowcast"
      ],
      [
        "enw_add_max_reported"
      ],
      [
        "enw_add_metaobs_features"
      ],
      [
        "enw_add_pooling_effect"
      ],
      [
        "enw_aggregate_cumulative"
      ],
      [
        "enw_assign_group"
      ],
      [
        "enw_complete_dates"
      ],
      [
        "enw_construct_data"
      ],
      [
        "enw_design"
      ],
      [
        "enw_effects_metadata"
      ],
      [
        "enw_example"
      ],
      [
        "enw_expectation"
      ],
      [
        "enw_extend_date"
      ],
      [
        "enw_filter_delay"
      ],
      [
        "enw_filter_reference_dates"
      ],
      [
        "enw_filter_report_dates"
      ],
      [
        "enw_fit_opts"
      ],
      [
        "enw_flag_observed_observations"
      ],
      [
        "enw_formula"
      ],
      [
        "enw_formula_as_data_list"
      ],
      [
        "enw_get_cache"
      ],
      [
        "enw_impute_na_observations"
      ],
      [
        "enw_incidence_to_linelist"
      ],
      [
        "enw_latest_data"
      ],
      [
        "enw_linelist_to_incidence"
      ],
      [
        "enw_manual_formula"
      ],
      [
        "enw_metadata"
      ],
      [
        "enw_metadata_delay"
      ],
      [
        "enw_missing"
      ],
      [
        "enw_missing_reference"
      ],
      [
        "enw_model"
      ],
      [
        "enw_nowcast_samples"
      ],
      [
        "enw_nowcast_summary"
      ],
      [
        "enw_obs"
      ],
      [
        "enw_one_hot_encode_feature"
      ],
      [
        "enw_pathfinder"
      ],
      [
        "enw_plot_nowcast_quantiles"
      ],
      [
        "enw_plot_obs"
      ],
      [
        "enw_plot_pp_quantiles"
      ],
      [
        "enw_plot_quantiles"
      ],
      [
        "enw_plot_theme"
      ],
      [
        "enw_posterior"
      ],
      [
        "enw_pp_summary"
      ],
      [
        "enw_preprocess_data"
      ],
      [
        "enw_priors_as_data_list"
      ],
      [
        "enw_quantiles_to_long"
      ],
      [
        "enw_reference"
      ],
      [
        "enw_replace_priors"
      ],
      [
        "enw_report"
      ],
      [
        "enw_reporting_triangle"
      ],
      [
        "enw_reporting_triangle_to_long"
      ],
      [
        "enw_sample"
      ],
      [
        "enw_set_cache"
      ],
      [
        "enw_simulate_missing_reference"
      ],
      [
        "enw_stan_to_r"
      ],
      [
        "enw_summarise_samples"
      ],
      [
        "enw_unset_cache"
      ],
      [
        "epinowcast"
      ],
      [
        "extract_sparse_matrix"
      ],
      [
        "re"
      ],
      [
        "rw"
      ]
    ],
    "topics": [
      [
        "cmdstanr"
      ],
      [
        "effective-reproduction-number-estimation"
      ],
      [
        "epidemiology"
      ],
      [
        "infectious-disease-surveillance"
      ],
      [
        "nowcasting"
      ],
      [
        "outbreak-analysis"
      ],
      [
        "pandemic-preparedness"
      ],
      [
        "real-time-infectious-disease-modelling"
      ],
      [
        "stan"
      ]
    ],
    "score": 8.4145,
    "stars": 62,
    "primary_category": "epidemiology",
    "source_universe": "epiforecasts",
    "search_text": "epinowcast A Bayesian Framework for Real-time Infectious Disease\nSurveillance A modular Bayesian framework for real-time infectious\ndisease surveillance. Provides tools for nowcasting,\nreproduction number estimation, delay estimation, and\nforecasting from data subject to reporting delays,\nright-truncation, missing data, and incomplete ascertainment.\nUsers can build models suited to their setting using a flexible\nformula interface supporting fixed effects, random effects,\nrandom walks, and time-varying parameters, with options\nincluding parametric and non-parametric delay distributions\nwith optional modifiers (via discrete-time hazard models),\nrenewal processes, observation models, missing data imputation,\nand stratified analyses with partial pooling. By jointly\nestimating disease dynamics and reporting patterns, our\nframework enables earlier and more reliable detection of\ntrends. While designed with epidemiological applications in\nmind, the framework can be applied to any right-truncated time\nseries count data. add_pmfs as_forecast_sample check_max_delay coerce_date convolution_matrix enw_add_cumulative enw_add_cumulative_membership enw_add_delay enw_add_incidence enw_add_latest_obs_to_nowcast enw_add_max_reported enw_add_metaobs_features enw_add_pooling_effect enw_aggregate_cumulative enw_assign_group enw_complete_dates enw_construct_data enw_design enw_effects_metadata enw_example enw_expectation enw_extend_date enw_filter_delay enw_filter_reference_dates enw_filter_report_dates enw_fit_opts enw_flag_observed_observations enw_formula enw_formula_as_data_list enw_get_cache enw_impute_na_observations enw_incidence_to_linelist enw_latest_data enw_linelist_to_incidence enw_manual_formula enw_metadata enw_metadata_delay enw_missing enw_missing_reference enw_model enw_nowcast_samples enw_nowcast_summary enw_obs enw_one_hot_encode_feature enw_pathfinder enw_plot_nowcast_quantiles enw_plot_obs enw_plot_pp_quantiles enw_plot_quantiles enw_plot_theme enw_posterior enw_pp_summary enw_preprocess_data enw_priors_as_data_list enw_quantiles_to_long enw_reference enw_replace_priors enw_report enw_reporting_triangle enw_reporting_triangle_to_long enw_sample enw_set_cache enw_simulate_missing_reference enw_stan_to_r enw_summarise_samples enw_unset_cache epinowcast extract_sparse_matrix re rw cmdstanr effective-reproduction-number-estimation epidemiology infectious-disease-surveillance nowcasting outbreak-analysis pandemic-preparedness real-time-infectious-disease-modelling stan"
  },
  {
    "id": 523,
    "package_name": "eph",
    "title": "Argentina's Permanent Household Survey Data and Manipulation\nUtilities",
    "description": "Tools to download and manipulate the Permanent Household\nSurvey from Argentina (EPH is the Spanish acronym for Permanent\nHousehold Survey). e.g: get_microdata() for downloading the\ndatasets, get_poverty_lines() for downloading the official\npoverty baskets, calculate_poverty() for the calculation of\nstating if a household is in poverty or not, following the\nofficial methodology. organize_panels() is used to concatenate\nobservations from different periods, and organize_labels() adds\nthe official labels to the data. The implemented methods are\nbased on INDEC (2016)\n<http://www.estadistica.ec.gba.gov.ar/dpe/images/SOCIEDAD/EPH_metodologia_22_pobreza.pdf>.\nAs this package works with the argentinian Permanent Household\nSurvey and its main audience is from this country, the\ndocumentation was written in Spanish.",
    "version": "1.0.2",
    "maintainer": "Carolina Pradier <carolinapradier@gmail.com>",
    "author": "Carolina Pradier [aut, cre] (ORCID:\n<https://orcid.org/0009-0007-5058-6352>),\nDiego Kozlowski [aut] (ORCID: <https://orcid.org/0000-0002-5396-3471>),\nPablo Tiscornia [aut],\nGuido Weksler [aut],\nNatsumi Shokida [aut],\nGerman Rosati [aut] (ORCID: <https://orcid.org/0000-0002-9775-0435>),\nJuan Gabriel Juara [ctb]",
    "url": "https://github.com/ropensci/eph",
    "bug_reports": "https://github.com/ropensci/eph/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "calculate_errors"
      ],
      [
        "calculate_poverty"
      ],
      [
        "calculate_tabulates"
      ],
      [
        "get_eahu"
      ],
      [
        "get_microdata"
      ],
      [
        "get_poverty_lines"
      ],
      [
        "get_total_urbano"
      ],
      [
        "map_agglomerates"
      ],
      [
        "organize_caes"
      ],
      [
        "organize_cno"
      ],
      [
        "organize_labels"
      ],
      [
        "organize_panels"
      ]
    ],
    "topics": [
      [
        "eph"
      ],
      [
        "indec"
      ],
      [
        "mercado-de-trabajo"
      ],
      [
        "rstatses"
      ]
    ],
    "score": 8.2494,
    "stars": 62,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "eph Argentina's Permanent Household Survey Data and Manipulation\nUtilities Tools to download and manipulate the Permanent Household\nSurvey from Argentina (EPH is the Spanish acronym for Permanent\nHousehold Survey). e.g: get_microdata() for downloading the\ndatasets, get_poverty_lines() for downloading the official\npoverty baskets, calculate_poverty() for the calculation of\nstating if a household is in poverty or not, following the\nofficial methodology. organize_panels() is used to concatenate\nobservations from different periods, and organize_labels() adds\nthe official labels to the data. The implemented methods are\nbased on INDEC (2016)\n<http://www.estadistica.ec.gba.gov.ar/dpe/images/SOCIEDAD/EPH_metodologia_22_pobreza.pdf>.\nAs this package works with the argentinian Permanent Household\nSurvey and its main audience is from this country, the\ndocumentation was written in Spanish. %>% calculate_errors calculate_poverty calculate_tabulates get_eahu get_microdata get_poverty_lines get_total_urbano map_agglomerates organize_caes organize_cno organize_labels organize_panels eph indec mercado-de-trabajo rstatses"
  },
  {
    "id": 247,
    "package_name": "apexcharter",
    "title": "Create Interactive Chart with the JavaScript 'ApexCharts'\nLibrary",
    "description": "Provides an 'htmlwidgets' interface to 'apexcharts.js'.\n'Apexcharts' is a modern JavaScript charting library to build\ninteractive charts and visualizations with simple API.\n'Apexcharts' examples and documentation are available here:\n<https://apexcharts.com/>.",
    "version": "0.4.4.9800",
    "maintainer": "Victor Perrier <victor.perrier@dreamrs.fr>",
    "author": "Victor Perrier [aut, cre],\nFanny Meyer [aut],\nJuned Chhipa [cph] (apexcharts.js library),\nMike Bostock [cph] (d3.format library)",
    "url": "https://github.com/dreamRs/apexcharter,\nhttps://dreamrs.github.io/apexcharter/",
    "bug_reports": "https://github.com/dreamRs/apexcharter/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "add_event"
      ],
      [
        "add_event_marker"
      ],
      [
        "add_hline"
      ],
      [
        "add_line"
      ],
      [
        "add_point"
      ],
      [
        "add_shade"
      ],
      [
        "add_shade_weekend"
      ],
      [
        "add_smooth_line"
      ],
      [
        "add_vline"
      ],
      [
        "aes"
      ],
      [
        "apex"
      ],
      [
        "apex_grid"
      ],
      [
        "apexchart"
      ],
      [
        "apexchartOutput"
      ],
      [
        "apexchartProxy"
      ],
      [
        "apexfacetOutput"
      ],
      [
        "apexgridOutput"
      ],
      [
        "ax_annotations"
      ],
      [
        "ax_chart"
      ],
      [
        "ax_colors"
      ],
      [
        "ax_colors_manual"
      ],
      [
        "ax_dataLabels"
      ],
      [
        "ax_facet_grid"
      ],
      [
        "ax_facet_wrap"
      ],
      [
        "ax_fill"
      ],
      [
        "ax_forecast_data_points"
      ],
      [
        "ax_grid"
      ],
      [
        "ax_labels"
      ],
      [
        "ax_labels2"
      ],
      [
        "ax_labs"
      ],
      [
        "ax_legend"
      ],
      [
        "ax_markers"
      ],
      [
        "ax_nodata"
      ],
      [
        "ax_plotOptions"
      ],
      [
        "ax_proxy_options"
      ],
      [
        "ax_proxy_series"
      ],
      [
        "ax_responsive"
      ],
      [
        "ax_series"
      ],
      [
        "ax_series2"
      ],
      [
        "ax_states"
      ],
      [
        "ax_stroke"
      ],
      [
        "ax_subtitle"
      ],
      [
        "ax_theme"
      ],
      [
        "ax_title"
      ],
      [
        "ax_tooltip"
      ],
      [
        "ax_xaxis"
      ],
      [
        "ax_yaxis"
      ],
      [
        "ax_yaxis2"
      ],
      [
        "bar_opts"
      ],
      [
        "boxplot_opts"
      ],
      [
        "bubble_opts"
      ],
      [
        "config_update"
      ],
      [
        "events_opts"
      ],
      [
        "format_date"
      ],
      [
        "format_num"
      ],
      [
        "heatmap_opts"
      ],
      [
        "JS"
      ],
      [
        "label"
      ],
      [
        "label_value"
      ],
      [
        "parse_df"
      ],
      [
        "pie_opts"
      ],
      [
        "radialBar_opts"
      ],
      [
        "renderApexchart"
      ],
      [
        "renderApexfacet"
      ],
      [
        "renderApexgrid"
      ],
      [
        "renderSparkBox"
      ],
      [
        "run_demo_input"
      ],
      [
        "run_demo_sparkbox"
      ],
      [
        "run_demo_sync"
      ],
      [
        "set_input_click"
      ],
      [
        "set_input_export"
      ],
      [
        "set_input_selection"
      ],
      [
        "set_input_zoom"
      ],
      [
        "set_tooltip_fixed"
      ],
      [
        "spark_box"
      ],
      [
        "sparkBoxOutput"
      ],
      [
        "vars"
      ]
    ],
    "topics": [
      [
        "data-visualization"
      ],
      [
        "htmlwidgets"
      ]
    ],
    "score": 8.2487,
    "stars": 147,
    "primary_category": "visualization",
    "source_universe": "dreamrs",
    "search_text": "apexcharter Create Interactive Chart with the JavaScript 'ApexCharts'\nLibrary Provides an 'htmlwidgets' interface to 'apexcharts.js'.\n'Apexcharts' is a modern JavaScript charting library to build\ninteractive charts and visualizations with simple API.\n'Apexcharts' examples and documentation are available here:\n<https://apexcharts.com/>. %>% add_event add_event_marker add_hline add_line add_point add_shade add_shade_weekend add_smooth_line add_vline aes apex apex_grid apexchart apexchartOutput apexchartProxy apexfacetOutput apexgridOutput ax_annotations ax_chart ax_colors ax_colors_manual ax_dataLabels ax_facet_grid ax_facet_wrap ax_fill ax_forecast_data_points ax_grid ax_labels ax_labels2 ax_labs ax_legend ax_markers ax_nodata ax_plotOptions ax_proxy_options ax_proxy_series ax_responsive ax_series ax_series2 ax_states ax_stroke ax_subtitle ax_theme ax_title ax_tooltip ax_xaxis ax_yaxis ax_yaxis2 bar_opts boxplot_opts bubble_opts config_update events_opts format_date format_num heatmap_opts JS label label_value parse_df pie_opts radialBar_opts renderApexchart renderApexfacet renderApexgrid renderSparkBox run_demo_input run_demo_sparkbox run_demo_sync set_input_click set_input_export set_input_selection set_input_zoom set_tooltip_fixed spark_box sparkBoxOutput vars data-visualization htmlwidgets"
  },
  {
    "id": 884,
    "package_name": "nodbi",
    "title": "'NoSQL' Database Connector",
    "description": "Simplified JSON document database access and manipulation,\nproviding a common API across supported 'NoSQL' databases\n'Elasticsearch', 'CouchDB', 'MongoDB' as well as\n'SQLite/JSON1', 'PostgreSQL', and 'DuckDB'.",
    "version": "0.14.0",
    "maintainer": "Ralf Herold <ralf.herold@mailbox.org>",
    "author": "Ralf Herold [aut, cre] (ORCID: <https://orcid.org/0000-0002-8148-6748>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nRich FitzJohn [aut],\nJeroen Ooms [aut],\nIvan Tarbakou [cph] (mongo-to-sql-converter library)",
    "url": "https://docs.ropensci.org/nodbi/,\nhttps://github.com/ropensci/nodbi",
    "bug_reports": "https://github.com/ropensci/nodbi/issues",
    "repository": "",
    "exports": [
      [
        "contacts"
      ],
      [
        "docdb_create"
      ],
      [
        "docdb_delete"
      ],
      [
        "docdb_exists"
      ],
      [
        "docdb_get"
      ],
      [
        "docdb_list"
      ],
      [
        "docdb_query"
      ],
      [
        "docdb_update"
      ],
      [
        "mapdata"
      ],
      [
        "src_couchdb"
      ],
      [
        "src_duckdb"
      ],
      [
        "src_elastic"
      ],
      [
        "src_mongo"
      ],
      [
        "src_postgres"
      ],
      [
        "src_sqlite"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "mongodb"
      ],
      [
        "elasticsearch"
      ],
      [
        "couchdb"
      ],
      [
        "sqlite"
      ],
      [
        "postgresql"
      ],
      [
        "duckdb"
      ],
      [
        "nosql"
      ],
      [
        "json"
      ],
      [
        "documents"
      ]
    ],
    "score": 8.1985,
    "stars": 75,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "nodbi 'NoSQL' Database Connector Simplified JSON document database access and manipulation,\nproviding a common API across supported 'NoSQL' databases\n'Elasticsearch', 'CouchDB', 'MongoDB' as well as\n'SQLite/JSON1', 'PostgreSQL', and 'DuckDB'. contacts docdb_create docdb_delete docdb_exists docdb_get docdb_list docdb_query docdb_update mapdata src_couchdb src_duckdb src_elastic src_mongo src_postgres src_sqlite database mongodb elasticsearch couchdb sqlite postgresql duckdb nosql json documents"
  },
  {
    "id": 733,
    "package_name": "juicyjuice",
    "title": "Inline CSS Properties into HTML Tags Using 'juice'",
    "description": "There are occasions where you need a piece of HTML with\nintegrated styles. A prime example of this is HTML email. This\ntransformation involves moving the CSS and associated\nformatting instructions from the style block in the head of\nyour document into the body of the HTML. Many prominent email\nclients require integrated styles in HTML email; otherwise a\nreceived HTML email will be displayed without any styling. This\npackage will quickly and precisely perform these CSS\ntransformations when given HTML text and it does so by using\nthe JavaScript 'juice' library.",
    "version": "0.1.0.9000",
    "maintainer": "Richard Iannone <rich@posit.co>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3925-190X>),\nAutomattic [cph] (juice library),\njuice contributors [ctb] (juice library),\nPosit Software, PBC [cph, fnd]",
    "url": "https://rstudio.github.io/juicyjuice/,\nhttps://github.com/rstudio/juicyjuice",
    "bug_reports": "https://github.com/rstudio/juicyjuice/issues",
    "repository": "",
    "exports": [
      [
        "css_inline"
      ]
    ],
    "topics": [
      [
        "css"
      ],
      [
        "html"
      ],
      [
        "inlining"
      ]
    ],
    "score": 8.1793,
    "stars": 3,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "juicyjuice Inline CSS Properties into HTML Tags Using 'juice' There are occasions where you need a piece of HTML with\nintegrated styles. A prime example of this is HTML email. This\ntransformation involves moving the CSS and associated\nformatting instructions from the style block in the head of\nyour document into the body of the HTML. Many prominent email\nclients require integrated styles in HTML email; otherwise a\nreceived HTML email will be displayed without any styling. This\npackage will quickly and precisely perform these CSS\ntransformations when given HTML text and it does so by using\nthe JavaScript 'juice' library. css_inline css html inlining"
  },
  {
    "id": 1115,
    "package_name": "ritis",
    "title": "Integrated Taxonomic Information System Client",
    "description": "An interface to the Integrated Taxonomic Information\nSystem ('ITIS') (<https://www.itis.gov>). Includes functions to\nwork with the 'ITIS' REST API methods\n(<https://www.itis.gov/ws_description.html>), as well as the\n'Solr' web service\n(<https://www.itis.gov/solr_documentation.html>).",
    "version": "1.0.0",
    "maintainer": "Julia Blum <juliablum@gmail.com>",
    "author": "Julia Blum [aut, cre] (ORCID: <https://orcid.org/0000-0002-2388-6612>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/ritis (devel)\nhttps://docs.ropensci.org/ritis/ (docs)",
    "bug_reports": "https://github.com/ropensci/ritis/issues",
    "repository": "",
    "exports": [
      [
        "accepted_names"
      ],
      [
        "any_match_count"
      ],
      [
        "comment_detail"
      ],
      [
        "common_names"
      ],
      [
        "core_metadata"
      ],
      [
        "coverage"
      ],
      [
        "credibility_rating"
      ],
      [
        "credibility_ratings"
      ],
      [
        "currency"
      ],
      [
        "date_data"
      ],
      [
        "description"
      ],
      [
        "experts"
      ],
      [
        "full_record"
      ],
      [
        "geographic_divisions"
      ],
      [
        "geographic_values"
      ],
      [
        "global_species_completeness"
      ],
      [
        "hierarchy_down"
      ],
      [
        "hierarchy_full"
      ],
      [
        "hierarchy_up"
      ],
      [
        "itis_facet"
      ],
      [
        "itis_group"
      ],
      [
        "itis_highlight"
      ],
      [
        "itis_search"
      ],
      [
        "jurisdiction_origin_values"
      ],
      [
        "jurisdiction_values"
      ],
      [
        "jurisdictional_origin"
      ],
      [
        "kingdom_name"
      ],
      [
        "kingdom_names"
      ],
      [
        "last_change_date"
      ],
      [
        "lsid2tsn"
      ],
      [
        "other_sources"
      ],
      [
        "parent_tsn"
      ],
      [
        "publications"
      ],
      [
        "rank_name"
      ],
      [
        "rank_names"
      ],
      [
        "record"
      ],
      [
        "review_year"
      ],
      [
        "scientific_name"
      ],
      [
        "search_any_match_paged"
      ],
      [
        "search_anymatch"
      ],
      [
        "search_common"
      ],
      [
        "search_scientific"
      ],
      [
        "synonym_names"
      ],
      [
        "taxon_authorship"
      ],
      [
        "terms"
      ],
      [
        "tsn_by_vernacular_language"
      ],
      [
        "tsn2lsid"
      ],
      [
        "unacceptability_reason"
      ],
      [
        "usage"
      ],
      [
        "vernacular_languages"
      ]
    ],
    "topics": [
      [
        "taxonomy"
      ],
      [
        "biology"
      ],
      [
        "nomenclature"
      ],
      [
        "json"
      ],
      [
        "api"
      ],
      [
        "web"
      ],
      [
        "api-client"
      ],
      [
        "identifiers"
      ],
      [
        "species"
      ],
      [
        "names"
      ],
      [
        "api-wrapper"
      ],
      [
        "itis"
      ],
      [
        "taxize"
      ]
    ],
    "score": 8.1029,
    "stars": 16,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "ritis Integrated Taxonomic Information System Client An interface to the Integrated Taxonomic Information\nSystem ('ITIS') (<https://www.itis.gov>). Includes functions to\nwork with the 'ITIS' REST API methods\n(<https://www.itis.gov/ws_description.html>), as well as the\n'Solr' web service\n(<https://www.itis.gov/solr_documentation.html>). accepted_names any_match_count comment_detail common_names core_metadata coverage credibility_rating credibility_ratings currency date_data description experts full_record geographic_divisions geographic_values global_species_completeness hierarchy_down hierarchy_full hierarchy_up itis_facet itis_group itis_highlight itis_search jurisdiction_origin_values jurisdiction_values jurisdictional_origin kingdom_name kingdom_names last_change_date lsid2tsn other_sources parent_tsn publications rank_name rank_names record review_year scientific_name search_any_match_paged search_anymatch search_common search_scientific synonym_names taxon_authorship terms tsn_by_vernacular_language tsn2lsid unacceptability_reason usage vernacular_languages taxonomy biology nomenclature json api web api-client identifiers species names api-wrapper itis taxize"
  },
  {
    "id": 1358,
    "package_name": "tinkr",
    "title": "Cast '(R)Markdown' Files to 'XML' and Back Again",
    "description": "Parsing '(R)Markdown' files with numerous regular\nexpressions can be fraught with peril, but it does not have to\nbe this way. Converting '(R)Markdown' files to 'XML' using the\n'commonmark' package allows in-memory editing via of 'markdown'\nelements via 'XPath' through the extensible 'R6' class called\n'yarn'. These modified 'XML' representations can be written to\n'(R)Markdown' documents via an 'xslt' stylesheet which\nimplements an extended version of 'GitHub'-flavoured 'markdown'\nso that you can tinker to your hearts content.",
    "version": "0.3.0",
    "maintainer": "Zhian N. Kamvar <zkamvar@gmail.com>",
    "author": "Ma\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nZhian N. Kamvar [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1458-7108>),\nJeroen Ooms [aut],\nNick Wellnhofer [cph] (Nick Wellnhofer wrote the XSLT stylesheet.),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>),\nPeter Daengeli [ctb]",
    "url": "https://docs.ropensci.org/tinkr/,\nhttps://github.com/ropensci/tinkr",
    "bug_reports": "https://github.com/ropensci/tinkr/issues",
    "repository": "",
    "exports": [
      [
        "find_between"
      ],
      [
        "get_protected"
      ],
      [
        "md_ns"
      ],
      [
        "protect_curly"
      ],
      [
        "protect_fences"
      ],
      [
        "protect_math"
      ],
      [
        "show_block"
      ],
      [
        "show_censor"
      ],
      [
        "show_list"
      ],
      [
        "stylesheet"
      ],
      [
        "to_md"
      ],
      [
        "to_md_vec"
      ],
      [
        "to_xml"
      ],
      [
        "yarn"
      ]
    ],
    "topics": [
      [
        "commonmark"
      ],
      [
        "xml"
      ],
      [
        "xpath"
      ]
    ],
    "score": 8.0406,
    "stars": 61,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tinkr Cast '(R)Markdown' Files to 'XML' and Back Again Parsing '(R)Markdown' files with numerous regular\nexpressions can be fraught with peril, but it does not have to\nbe this way. Converting '(R)Markdown' files to 'XML' using the\n'commonmark' package allows in-memory editing via of 'markdown'\nelements via 'XPath' through the extensible 'R6' class called\n'yarn'. These modified 'XML' representations can be written to\n'(R)Markdown' documents via an 'xslt' stylesheet which\nimplements an extended version of 'GitHub'-flavoured 'markdown'\nso that you can tinker to your hearts content. find_between get_protected md_ns protect_curly protect_fences protect_math show_block show_censor show_list stylesheet to_md to_md_vec to_xml yarn commonmark xml xpath"
  },
  {
    "id": 86,
    "package_name": "MODIStsp",
    "title": "Find, Download and Process MODIS Land Products Data",
    "description": "Allows automating the creation of time series of rasters\nderived from MODIS satellite land products data. It performs\nseveral typical preprocessing steps such as download,\nmosaicking, reprojecting and resizing data acquired on a\nspecified time period. All processing parameters can be set\nusing a user-friendly GUI. Users can select which layers of the\noriginal MODIS HDF files they want to process, which additional\nquality indicators should be extracted from aggregated MODIS\nquality assurance layers and, in the case of surface\nreflectance products, which spectral indexes should be computed\nfrom the original reflectance bands. For each output layer,\noutputs are saved as single-band raster files corresponding to\neach available acquisition date. Virtual files allowing access\nto the entire time series as a single file are also created.\nCommand-line execution exploiting a previously saved processing\noptions file is also possible, allowing users to automatically\nupdate time series related to a MODIS product whenever a new\nimage is available. For additional documentation refer to the\nfollowing article: Busetto and Ranghetti (2016)\n<doi:10.1016/j.cageo.2016.08.020>.",
    "version": "2.1.0.9001",
    "maintainer": "Luigi Ranghetti <rpackages.ranghetti@gmail.com>",
    "author": "Lorenzo Busetto [aut] (ORCID: <https://orcid.org/0000-0001-9634-6038>),\nLuigi Ranghetti [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6207-5188>),\nLeah Wasser [rev] (Leah Wasser reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/184),\nJeff Hanson [rev] (Jeff Hanson reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/184),\nBabak Naimi [ctb] (Babak Naimi wrote the function ModisDownload(), on\nwhich some MODIStsp internal functions are based)",
    "url": "https://github.com/ropensci/MODIStsp/,\nhttps://docs.ropensci.org/MODIStsp/",
    "bug_reports": "https://github.com/ropensci/MODIStsp/issues",
    "repository": "",
    "exports": [
      [
        "check_projection"
      ],
      [
        "install_MODIStsp_launcher"
      ],
      [
        "MODIStsp"
      ],
      [
        "MODIStsp_addindex"
      ],
      [
        "MODIStsp_extract"
      ],
      [
        "MODIStsp_get_prodlayers"
      ],
      [
        "MODIStsp_get_prodnames"
      ],
      [
        "MODIStsp_process"
      ],
      [
        "MODIStsp_resetindexes"
      ]
    ],
    "topics": [
      [
        "gdal"
      ],
      [
        "modis"
      ],
      [
        "modis-data"
      ],
      [
        "modis-land-products"
      ],
      [
        "peer-reviewed"
      ],
      [
        "preprocessing"
      ],
      [
        "remote-sensing"
      ],
      [
        "satellite-imagery"
      ],
      [
        "time-series"
      ]
    ],
    "score": 7.9501,
    "stars": 159,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "MODIStsp Find, Download and Process MODIS Land Products Data Allows automating the creation of time series of rasters\nderived from MODIS satellite land products data. It performs\nseveral typical preprocessing steps such as download,\nmosaicking, reprojecting and resizing data acquired on a\nspecified time period. All processing parameters can be set\nusing a user-friendly GUI. Users can select which layers of the\noriginal MODIS HDF files they want to process, which additional\nquality indicators should be extracted from aggregated MODIS\nquality assurance layers and, in the case of surface\nreflectance products, which spectral indexes should be computed\nfrom the original reflectance bands. For each output layer,\noutputs are saved as single-band raster files corresponding to\neach available acquisition date. Virtual files allowing access\nto the entire time series as a single file are also created.\nCommand-line execution exploiting a previously saved processing\noptions file is also possible, allowing users to automatically\nupdate time series related to a MODIS product whenever a new\nimage is available. For additional documentation refer to the\nfollowing article: Busetto and Ranghetti (2016)\n<doi:10.1016/j.cageo.2016.08.020>. check_projection install_MODIStsp_launcher MODIStsp MODIStsp_addindex MODIStsp_extract MODIStsp_get_prodlayers MODIStsp_get_prodnames MODIStsp_process MODIStsp_resetindexes gdal modis modis-data modis-land-products peer-reviewed preprocessing remote-sensing satellite-imagery time-series"
  },
  {
    "id": 887,
    "package_name": "nomnoml",
    "title": "Sassy 'UML' Diagrams",
    "description": "A tool for drawing sassy 'UML' (Unified Modeling Language)\ndiagrams based on a simple syntax, see\n<https://www.nomnoml.com>. Supports styling, R Markdown and\nexporting diagrams in the PNG format. Note: you need a chromium\nbased browser installed on your system.",
    "version": "0.3.0",
    "maintainer": "Andrie de Vries <apdevries@gmail.com>",
    "author": "Andrie de Vries [aut, cre],\nJavier Luraschi [aut],\nDaniel Kallin [cph] (nomnoml.js library, <https://nomnoml.com>),\nRStudio [cph, fnd]",
    "url": "https://rstudio.github.io/nomnoml/",
    "bug_reports": "https://github.com/rstudio/nomnoml/issues",
    "repository": "",
    "exports": [
      [
        "nomnoml"
      ],
      [
        "nomnoml_validate"
      ],
      [
        "nomnomlOutput"
      ],
      [
        "renderNomnoml"
      ]
    ],
    "topics": [
      [
        "diagrams"
      ],
      [
        "htmlwidgets"
      ],
      [
        "nomnoml"
      ],
      [
        "uml"
      ]
    ],
    "score": 7.8726,
    "stars": 220,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "nomnoml Sassy 'UML' Diagrams A tool for drawing sassy 'UML' (Unified Modeling Language)\ndiagrams based on a simple syntax, see\n<https://www.nomnoml.com>. Supports styling, R Markdown and\nexporting diagrams in the PNG format. Note: you need a chromium\nbased browser installed on your system. nomnoml nomnoml_validate nomnomlOutput renderNomnoml diagrams htmlwidgets nomnoml uml"
  },
  {
    "id": 918,
    "package_name": "orderly1",
    "title": "Lightweight Reproducible Reporting",
    "description": "Order, create and store reports from R.  By defining a\nlightweight interface around the inputs and outputs of an\nanalysis, a lot of the repetitive work for reproducible\nresearch can be automated.  We define a simple format for\norganising and describing work that facilitates collaborative\nreproducible research and acknowledges that all analyses are\nrun multiple times over their lifespans.",
    "version": "1.7.2",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\nRobert Ashton [aut],\nAlex Hill [aut],\nMartin Eden [aut],\nWes Hinsley [aut],\nEmma Russell [aut],\nJames Thompson [aut],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "https://www.vaccineimpact.org/orderly/,\nhttps://github.com/vimc/orderly",
    "bug_reports": "https://github.com/vimc/orderly/issues",
    "repository": "",
    "exports": [
      [
        "orderly_batch"
      ],
      [
        "orderly_bundle_import"
      ],
      [
        "orderly_bundle_import_remote"
      ],
      [
        "orderly_bundle_list"
      ],
      [
        "orderly_bundle_pack"
      ],
      [
        "orderly_bundle_pack_remote"
      ],
      [
        "orderly_bundle_run"
      ],
      [
        "orderly_cancel_remote"
      ],
      [
        "orderly_cleanup"
      ],
      [
        "orderly_commit"
      ],
      [
        "orderly_config"
      ],
      [
        "orderly_db"
      ],
      [
        "orderly_deduplicate"
      ],
      [
        "orderly_default_remote_get"
      ],
      [
        "orderly_default_remote_set"
      ],
      [
        "orderly_develop_clean"
      ],
      [
        "orderly_develop_start"
      ],
      [
        "orderly_develop_status"
      ],
      [
        "orderly_example"
      ],
      [
        "orderly_graph"
      ],
      [
        "orderly_graph_out_of_date"
      ],
      [
        "orderly_info"
      ],
      [
        "orderly_init"
      ],
      [
        "orderly_latest"
      ],
      [
        "orderly_list"
      ],
      [
        "orderly_list_archive"
      ],
      [
        "orderly_list_drafts"
      ],
      [
        "orderly_list_metadata"
      ],
      [
        "orderly_log"
      ],
      [
        "orderly_log_off"
      ],
      [
        "orderly_log_on"
      ],
      [
        "orderly_migrate"
      ],
      [
        "orderly_new"
      ],
      [
        "orderly_packages"
      ],
      [
        "orderly_pull_archive"
      ],
      [
        "orderly_pull_dependencies"
      ],
      [
        "orderly_push_archive"
      ],
      [
        "orderly_rebuild"
      ],
      [
        "orderly_remote"
      ],
      [
        "orderly_remote_path"
      ],
      [
        "orderly_remote_status"
      ],
      [
        "orderly_run"
      ],
      [
        "orderly_run_info"
      ],
      [
        "orderly_run_remote"
      ],
      [
        "orderly_search"
      ],
      [
        "orderly_test_check"
      ],
      [
        "orderly_test_start"
      ],
      [
        "orderly_use_gitignore"
      ],
      [
        "orderly_use_package"
      ],
      [
        "orderly_use_resource"
      ],
      [
        "orderly_use_source"
      ]
    ],
    "topics": [],
    "score": 7.8316,
    "stars": 116,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "orderly1 Lightweight Reproducible Reporting Order, create and store reports from R.  By defining a\nlightweight interface around the inputs and outputs of an\nanalysis, a lot of the repetitive work for reproducible\nresearch can be automated.  We define a simple format for\norganising and describing work that facilitates collaborative\nreproducible research and acknowledges that all analyses are\nrun multiple times over their lifespans. orderly_batch orderly_bundle_import orderly_bundle_import_remote orderly_bundle_list orderly_bundle_pack orderly_bundle_pack_remote orderly_bundle_run orderly_cancel_remote orderly_cleanup orderly_commit orderly_config orderly_db orderly_deduplicate orderly_default_remote_get orderly_default_remote_set orderly_develop_clean orderly_develop_start orderly_develop_status orderly_example orderly_graph orderly_graph_out_of_date orderly_info orderly_init orderly_latest orderly_list orderly_list_archive orderly_list_drafts orderly_list_metadata orderly_log orderly_log_off orderly_log_on orderly_migrate orderly_new orderly_packages orderly_pull_archive orderly_pull_dependencies orderly_push_archive orderly_rebuild orderly_remote orderly_remote_path orderly_remote_status orderly_run orderly_run_info orderly_run_remote orderly_search orderly_test_check orderly_test_start orderly_use_gitignore orderly_use_package orderly_use_resource orderly_use_source "
  },
  {
    "id": 551,
    "package_name": "ezknitr",
    "title": "Avoid the Typical Working Directory Pain When Using 'knitr'",
    "description": "An extension of 'knitr' that adds flexibility in several\nways. One common source of frustration with 'knitr' is that it\nassumes the directory where the source file lives should be the\nworking directory, which is often not true. 'ezknitr' addresses\nthis problem by giving you complete control over where all the\ninputs and outputs are, and adds several other convenient\nfeatures to make rendering markdown/HTML documents easier.",
    "version": "0.6.3",
    "maintainer": "Dean Attali <daattali@gmail.com>",
    "author": "Dean Attali [aut, cre]",
    "url": "https://docs.ropensci.org/ezknitr/,\nhttps://github.com/ropensci/ezknitr",
    "bug_reports": "https://github.com/ropensci/ezknitr/issues",
    "repository": "",
    "exports": [
      [
        "ezknit"
      ],
      [
        "ezspin"
      ],
      [
        "open_output_dir"
      ],
      [
        "set_default_params"
      ],
      [
        "setup_ezknit_test"
      ],
      [
        "setup_ezspin_test"
      ]
    ],
    "topics": [
      [
        "knitr"
      ],
      [
        "peer-reviewed"
      ],
      [
        "reproducibility"
      ],
      [
        "rmarkdown"
      ],
      [
        "rmd"
      ]
    ],
    "score": 7.818,
    "stars": 113,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "ezknitr Avoid the Typical Working Directory Pain When Using 'knitr' An extension of 'knitr' that adds flexibility in several\nways. One common source of frustration with 'knitr' is that it\nassumes the directory where the source file lives should be the\nworking directory, which is often not true. 'ezknitr' addresses\nthis problem by giving you complete control over where all the\ninputs and outputs are, and adds several other convenient\nfeatures to make rendering markdown/HTML documents easier. ezknit ezspin open_output_dir set_default_params setup_ezknit_test setup_ezspin_test knitr peer-reviewed reproducibility rmarkdown rmd"
  },
  {
    "id": 505,
    "package_name": "dynamite",
    "title": "Bayesian Modeling and Causal Inference for Multivariate\nLongitudinal Data",
    "description": "Easy-to-use and efficient interface for Bayesian inference\nof complex panel (time series) data using dynamic multivariate\npanel models by Helske and Tikka (2024)\n<doi:10.1016/j.alcr.2024.100617>. The package supports joint\nmodeling of multiple measurements per individual, time-varying\nand time-invariant effects, and a wide range of discrete and\ncontinuous distributions. Estimation of these dynamic\nmultivariate panel models is carried out via 'Stan'. For an\nin-depth tutorial of the package, see (Tikka and Helske, 2024)\n<doi:10.48550/arXiv.2302.01607>.",
    "version": "1.6.2",
    "maintainer": "Santtu Tikka <santtuth@gmail.com>",
    "author": "Santtu Tikka [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4039-4342>),\nJouni Helske [aut] (ORCID: <https://orcid.org/0000-0001-7130-793X>),\nNicholas Clark [rev],\nLucy D'Agostino McGowan [rev]",
    "url": "https://docs.ropensci.org/dynamite/,\nhttps://github.com/ropensci/dynamite/",
    "bug_reports": "https://github.com/ropensci/dynamite/issues/",
    "repository": "",
    "exports": [
      [
        "as_draws"
      ],
      [
        "as_draws_df"
      ],
      [
        "as.data.table"
      ],
      [
        "aux"
      ],
      [
        "dynamice"
      ],
      [
        "dynamite"
      ],
      [
        "dynamiteformula"
      ],
      [
        "get_algorithm"
      ],
      [
        "get_code"
      ],
      [
        "get_data"
      ],
      [
        "get_diagnostics"
      ],
      [
        "get_draws"
      ],
      [
        "get_elapsed_time"
      ],
      [
        "get_max_treedepth"
      ],
      [
        "get_model_code"
      ],
      [
        "get_nchains"
      ],
      [
        "get_ndraws"
      ],
      [
        "get_parameter_dims"
      ],
      [
        "get_parameter_names"
      ],
      [
        "get_parameter_types"
      ],
      [
        "get_pars_oi"
      ],
      [
        "get_priors"
      ],
      [
        "hmc_diagnostics"
      ],
      [
        "lags"
      ],
      [
        "lfactor"
      ],
      [
        "lfo"
      ],
      [
        "loo"
      ],
      [
        "mcmc_diagnostics"
      ],
      [
        "mice.impute.lag"
      ],
      [
        "mice.impute.lead"
      ],
      [
        "ndraws"
      ],
      [
        "obs"
      ],
      [
        "plot_betas"
      ],
      [
        "plot_deltas"
      ],
      [
        "plot_lambdas"
      ],
      [
        "plot_nus"
      ],
      [
        "plot_psis"
      ],
      [
        "random_spec"
      ],
      [
        "splines"
      ]
    ],
    "topics": [
      [
        "bayesian-inference"
      ],
      [
        "panel-data"
      ],
      [
        "stan"
      ],
      [
        "statistical-models"
      ],
      [
        "quarto"
      ]
    ],
    "score": 7.8126,
    "stars": 35,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "dynamite Bayesian Modeling and Causal Inference for Multivariate\nLongitudinal Data Easy-to-use and efficient interface for Bayesian inference\nof complex panel (time series) data using dynamic multivariate\npanel models by Helske and Tikka (2024)\n<doi:10.1016/j.alcr.2024.100617>. The package supports joint\nmodeling of multiple measurements per individual, time-varying\nand time-invariant effects, and a wide range of discrete and\ncontinuous distributions. Estimation of these dynamic\nmultivariate panel models is carried out via 'Stan'. For an\nin-depth tutorial of the package, see (Tikka and Helske, 2024)\n<doi:10.48550/arXiv.2302.01607>. as_draws as_draws_df as.data.table aux dynamice dynamite dynamiteformula get_algorithm get_code get_data get_diagnostics get_draws get_elapsed_time get_max_treedepth get_model_code get_nchains get_ndraws get_parameter_dims get_parameter_names get_parameter_types get_pars_oi get_priors hmc_diagnostics lags lfactor lfo loo mcmc_diagnostics mice.impute.lag mice.impute.lead ndraws obs plot_betas plot_deltas plot_lambdas plot_nus plot_psis random_spec splines bayesian-inference panel-data stan statistical-models quarto"
  },
  {
    "id": 1231,
    "package_name": "sivirep",
    "title": "Data Wrangling and Automated Reports from 'SIVIGILA' Source",
    "description": "Data wrangling, pre-processing, and generating automated\nreports from Colombia's epidemiological surveillance system,\n'SIVIGILA' <https://portalsivigila.ins.gov.co/>. It provides a\ncustomizable R Markdown template for analysis and automatic\ngeneration of epidemiological reports that can be adapted to\nlocal, regional, and national contexts. This tool offers a\nstandardized and reproducible workflow that helps to reduce\nmanual labor and potential errors in report generation,\nimproving their efficiency and consistency.",
    "version": "1.0.2",
    "maintainer": "Geraldine G\u00f3mez-Mill\u00e1n <geralidine.gomez@javeriana.edu.co>",
    "author": "Geraldine G\u00f3mez-Mill\u00e1n [aut, cre, ctb] (ORCID:\n<https://orcid.org/0009-0007-8701-0568>),\nZulma M. Cucunub\u00e1 [aut, ctb] (ORCID:\n<https://orcid.org/0000-0002-8165-3198>),\nJennifer A. Mendez-Romero [aut, ctb] (ORCID:\n<https://orcid.org/0009-0001-6138-0225>),\nClaudia Huguett-Arag\u00f3n [aut, ctb] (ORCID:\n<https://orcid.org/0000-0002-9814-2386>),\nHugo Gruson [ctb] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nJuanita Romero-Garc\u00e9s [ctb] (ORCID:\n<https://orcid.org/0000-0001-9796-6626>),\nJaime Pavlich-Mariscal [ctb] (ORCID:\n<https://orcid.org/0000-0002-3892-6680>),\nLaura G\u00f3mez Bermeo [ctb] (ORCID:\n<https://orcid.org/0000-0003-4028-2893>),\nAndr\u00e9s Moreno [ctb] (ORCID: <https://orcid.org/0000-0001-9266-731X>),\nMiguel G\u00e1mez [ctb],\nJohan Calder\u00f3n [ctb],\nLady Fl\u00f3rez-Tapiero [ctb],\nVer\u00f3nica Tangarife-Arredondo [ctb],\nGerard Alarcon [ctb],\nInternational Development Research Center (IDRC) [fnd] (ROR:\n<https://ror.org/0445x0472>),\nPontificia Universidad Javeriana [cph] (ROR:\n<https://ror.org/03etyjw28>)",
    "url": "https://epiverse-trace.github.io/sivirep/,\nhttps://github.com/epiverse-trace/sivirep",
    "bug_reports": "https://github.com/epiverse-trace/sivirep/issues",
    "repository": "",
    "exports": [
      [
        "agrupar_area_geo"
      ],
      [
        "agrupar_cols_casos"
      ],
      [
        "agrupar_dpto"
      ],
      [
        "agrupar_edad"
      ],
      [
        "agrupar_edad_sex"
      ],
      [
        "agrupar_eventos"
      ],
      [
        "agrupar_fecha_inisintomas"
      ],
      [
        "agrupar_mpio"
      ],
      [
        "agrupar_per_etn"
      ],
      [
        "agrupar_rango_edad"
      ],
      [
        "agrupar_semanaepi"
      ],
      [
        "agrupar_sex"
      ],
      [
        "agrupar_sex_semanaepi"
      ],
      [
        "agrupar_tipo_caso"
      ],
      [
        "agrupar_top_area_geo"
      ],
      [
        "agrupar_years"
      ],
      [
        "calcular_incidencia"
      ],
      [
        "calcular_incidencia_geo"
      ],
      [
        "calcular_incidencia_sex"
      ],
      [
        "convert_edad"
      ],
      [
        "estandarizar_geo_cods"
      ],
      [
        "geo_filtro"
      ],
      [
        "import_data_event"
      ],
      [
        "import_geo_cods"
      ],
      [
        "import_pob_incidencia"
      ],
      [
        "import_pob_proyecciones"
      ],
      [
        "import_pob_riesgo"
      ],
      [
        "limpiar_data_sivigila"
      ],
      [
        "limpiar_edad_event"
      ],
      [
        "limpiar_encabezado"
      ],
      [
        "limpiar_fecha_event"
      ],
      [
        "limpiar_val_atipic"
      ],
      [
        "list_events"
      ],
      [
        "obtener_cond_inciden_event"
      ],
      [
        "obtener_dptos"
      ],
      [
        "obtener_fila_mas_casos"
      ],
      [
        "obtener_info_depts"
      ],
      [
        "obtener_meses_mas_casos"
      ],
      [
        "obtener_nombre_dpto"
      ],
      [
        "obtener_nombre_mpio"
      ],
      [
        "obtener_text_sex"
      ],
      [
        "obtener_tip_ocurren_geo"
      ],
      [
        "obtener_val_config"
      ],
      [
        "plot_area_geo"
      ],
      [
        "plot_dptos"
      ],
      [
        "plot_edad"
      ],
      [
        "plot_edad_sex"
      ],
      [
        "plot_fecha_inisintomas"
      ],
      [
        "plot_map"
      ],
      [
        "plot_mpios"
      ],
      [
        "plot_per_etn"
      ],
      [
        "plot_sex"
      ],
      [
        "plot_sex_semanaepi"
      ],
      [
        "plot_tabla_incidencia_geo"
      ],
      [
        "plot_tabla_incidencia_sex"
      ],
      [
        "plot_tabla_tipos_event"
      ],
      [
        "plot_tipo_caso"
      ],
      [
        "plot_tipo_caso_years"
      ],
      [
        "plot_top_area_geo"
      ],
      [
        "plot_years"
      ]
    ],
    "topics": [
      [
        "colombia"
      ],
      [
        "epidemiological-surveillance"
      ],
      [
        "epiverse"
      ],
      [
        "public-health"
      ]
    ],
    "score": 7.7236,
    "stars": 42,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "sivirep Data Wrangling and Automated Reports from 'SIVIGILA' Source Data wrangling, pre-processing, and generating automated\nreports from Colombia's epidemiological surveillance system,\n'SIVIGILA' <https://portalsivigila.ins.gov.co/>. It provides a\ncustomizable R Markdown template for analysis and automatic\ngeneration of epidemiological reports that can be adapted to\nlocal, regional, and national contexts. This tool offers a\nstandardized and reproducible workflow that helps to reduce\nmanual labor and potential errors in report generation,\nimproving their efficiency and consistency. agrupar_area_geo agrupar_cols_casos agrupar_dpto agrupar_edad agrupar_edad_sex agrupar_eventos agrupar_fecha_inisintomas agrupar_mpio agrupar_per_etn agrupar_rango_edad agrupar_semanaepi agrupar_sex agrupar_sex_semanaepi agrupar_tipo_caso agrupar_top_area_geo agrupar_years calcular_incidencia calcular_incidencia_geo calcular_incidencia_sex convert_edad estandarizar_geo_cods geo_filtro import_data_event import_geo_cods import_pob_incidencia import_pob_proyecciones import_pob_riesgo limpiar_data_sivigila limpiar_edad_event limpiar_encabezado limpiar_fecha_event limpiar_val_atipic list_events obtener_cond_inciden_event obtener_dptos obtener_fila_mas_casos obtener_info_depts obtener_meses_mas_casos obtener_nombre_dpto obtener_nombre_mpio obtener_text_sex obtener_tip_ocurren_geo obtener_val_config plot_area_geo plot_dptos plot_edad plot_edad_sex plot_fecha_inisintomas plot_map plot_mpios plot_per_etn plot_sex plot_sex_semanaepi plot_tabla_incidencia_geo plot_tabla_incidencia_sex plot_tabla_tipos_event plot_tipo_caso plot_tipo_caso_years plot_top_area_geo plot_years colombia epidemiological-surveillance epiverse public-health"
  },
  {
    "id": 272,
    "package_name": "babelquarto",
    "title": "Renders a Multilingual Quarto Book",
    "description": "Automate rendering and cross-linking of Quarto books\nfollowing a prescribed structure.",
    "version": "0.1.0.9000",
    "maintainer": "Ma\u00eblle Salmon <msmaellesalmon@gmail.com>",
    "author": "Ma\u00eblle Salmon [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-2815-0399>),\nJohannes Ranke [ctb],\nJoel H. Nitta [ctb] (ORCID: <https://orcid.org/0000-0003-4719-7472>),\nPascal Burkhard [aut] (ORCID: <https://orcid.org/0009-0001-5504-5084>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>),\nElla Kaye [rev] (Ella reviewed the package (v. 0.0.1) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/720>., ORCID:\n<https://orcid.org/0000-0002-7300-3718>),\nJo\u00e3o Granja-Correia [rev] (Jo\u00e3o reviewed the package (v. 0.0.1) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/720>., ORCID:\n<https://orcid.org/0009-0003-8982-278X>)",
    "url": "https://docs.ropensci.org/babelquarto/,\nhttps://github.com/ropensci-review-tools/babelquarto",
    "bug_reports": "https://github.com/ropensci-review-tools/babelquarto/issues",
    "repository": "",
    "exports": [
      [
        "quarto_multilingual_book"
      ],
      [
        "quarto_multilingual_website"
      ],
      [
        "register_further_languages"
      ],
      [
        "register_main_language"
      ],
      [
        "render_book"
      ],
      [
        "render_website"
      ]
    ],
    "topics": [
      [
        "quarto"
      ]
    ],
    "score": 7.6409,
    "stars": 54,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "babelquarto Renders a Multilingual Quarto Book Automate rendering and cross-linking of Quarto books\nfollowing a prescribed structure. quarto_multilingual_book quarto_multilingual_website register_further_languages register_main_language render_book render_website quarto"
  },
  {
    "id": 372,
    "package_name": "cleanepi",
    "title": "Clean and Standardize Epidemiological Data",
    "description": "Cleaning and standardizing tabular data package, tailored\nspecifically for curating epidemiological data. It streamlines\nvarious data cleaning tasks that are typically expected when\nworking with datasets in epidemiology. It returns the processed\ndata in the same format, and generates a comprehensive report\ndetailing the outcomes of each cleaning task.",
    "version": "1.1.2.9000",
    "maintainer": "Bubacarr Bah <Bubacarr.Bah1@lshtm.ac.uk>",
    "author": "Karim Man\u00e9 [aut] (ORCID: <https://orcid.org/0000-0002-9892-2999>),\nThibaut Jombart [ctb] (Thibaut contributed in development of\ndate_guess().),\nAbdoelnaser Degoot [aut] (ORCID:\n<https://orcid.org/0000-0001-8788-2496>),\nBankol\u00e9 Ahadzie [aut],\nNuredin Mohammed [aut],\nBubacarr Bah [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3318-6668>),\nHugo Gruson [ctb, rev] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nPratik R. Gupte [rev] (ORCID: <https://orcid.org/0000-0001-5294-7819>),\nJames M. Azam [rev] (ORCID: <https://orcid.org/0000-0001-5782-7330>),\nJoshua W. Lambert [rev, ctb] (ORCID:\n<https://orcid.org/0000-0001-5218-3046>),\nChris Hartgerink [rev] (ORCID: <https://orcid.org/0000-0003-1050-6809>),\nAndree Valle-Campos [rev, ctb] (ORCID:\n<https://orcid.org/0000-0002-7779-481X>),\nLondon School of Hygiene and Tropical Medicine, LSHTM [cph] (ROR:\n<https://ror.org/00a0jsq62>),\ndata.org [fnd]",
    "url": "https://epiverse-trace.github.io/cleanepi/,\nhttps://github.com/epiverse-trace/cleanepi",
    "bug_reports": "https://github.com/epiverse-trace/cleanepi/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "add_to_dictionary"
      ],
      [
        "add_to_report"
      ],
      [
        "check_date_sequence"
      ],
      [
        "check_subject_ids"
      ],
      [
        "clean_data"
      ],
      [
        "clean_using_dictionary"
      ],
      [
        "convert_numeric_to_date"
      ],
      [
        "convert_to_numeric"
      ],
      [
        "correct_misspelled_values"
      ],
      [
        "correct_subject_ids"
      ],
      [
        "find_duplicates"
      ],
      [
        "get_default_params"
      ],
      [
        "print_report"
      ],
      [
        "remove_constants"
      ],
      [
        "remove_duplicates"
      ],
      [
        "replace_missing_values"
      ],
      [
        "scan_data"
      ],
      [
        "standardize_column_names"
      ],
      [
        "standardize_dates"
      ],
      [
        "timespan"
      ]
    ],
    "topics": [
      [
        "data-cleaning"
      ],
      [
        "epidemiology"
      ],
      [
        "epiverse"
      ]
    ],
    "score": 7.6232,
    "stars": 10,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "cleanepi Clean and Standardize Epidemiological Data Cleaning and standardizing tabular data package, tailored\nspecifically for curating epidemiological data. It streamlines\nvarious data cleaning tasks that are typically expected when\nworking with datasets in epidemiology. It returns the processed\ndata in the same format, and generates a comprehensive report\ndetailing the outcomes of each cleaning task. %>% add_to_dictionary add_to_report check_date_sequence check_subject_ids clean_data clean_using_dictionary convert_numeric_to_date convert_to_numeric correct_misspelled_values correct_subject_ids find_duplicates get_default_params print_report remove_constants remove_duplicates replace_missing_values scan_data standardize_column_names standardize_dates timespan data-cleaning epidemiology epiverse"
  },
  {
    "id": 899,
    "package_name": "occCite",
    "title": "Querying and Managing Large Biodiversity Occurrence Datasets",
    "description": "Facilitates the gathering of biodiversity occurrence data\nfrom disparate sources. Metadata is managed throughout the\nprocess to facilitate reporting and enhanced ability to repeat\nanalyses.",
    "version": "0.6.1",
    "maintainer": "Hannah L. Owens <hannah.owens@gmail.com>",
    "author": "Hannah L. Owens [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-0071-1745>),\nCory Merow [aut] (ORCID: <https://orcid.org/0000-0003-0561-053X>),\nBrian Maitner [aut] (ORCID: <https://orcid.org/0000-0002-2118-9880>),\nJamie M. Kass [aut] (ORCID: <https://orcid.org/0000-0002-9432-895X>),\nVijay Barve [aut] (ORCID: <https://orcid.org/0000-0002-4852-2567>),\nRobert P. Guralnick [aut] (ORCID:\n<https://orcid.org/0000-0001-6682-1504>),\nDamiano Oldoni [rev] (ORCID: <https://orcid.org/0000-0003-3445-7562>,\nDamiano reviewed the package (v. 0.5.2) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/407>)",
    "url": "https://docs.ropensci.org/occCite/",
    "bug_reports": "https://github.com/ropensci/occCite/issues",
    "repository": "",
    "exports": [
      [
        "GBIFLogin"
      ],
      [
        "GBIFLoginManager"
      ],
      [
        "getBIENpoints"
      ],
      [
        "getGBIFpoints"
      ],
      [
        "occCitation"
      ],
      [
        "occCiteData"
      ],
      [
        "occCiteMap"
      ],
      [
        "occQuery"
      ],
      [
        "prevGBIFdownload"
      ],
      [
        "studyTaxonList"
      ],
      [
        "taxonRectification"
      ]
    ],
    "topics": [
      [
        "biodiversity-data"
      ],
      [
        "biodiversity-informatics"
      ],
      [
        "biodiversity-standards"
      ],
      [
        "citations"
      ],
      [
        "museum-collection-specimens"
      ],
      [
        "museum-collections"
      ],
      [
        "museum-metadata"
      ]
    ],
    "score": 7.4928,
    "stars": 24,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "occCite Querying and Managing Large Biodiversity Occurrence Datasets Facilitates the gathering of biodiversity occurrence data\nfrom disparate sources. Metadata is managed throughout the\nprocess to facilitate reporting and enhanced ability to repeat\nanalyses. GBIFLogin GBIFLoginManager getBIENpoints getGBIFpoints occCitation occCiteData occCiteMap occQuery prevGBIFdownload studyTaxonList taxonRectification biodiversity-data biodiversity-informatics biodiversity-standards citations museum-collection-specimens museum-collections museum-metadata"
  },
  {
    "id": 729,
    "package_name": "jsonld",
    "title": "JSON for Linking Data",
    "description": "JSON-LD <https://www.w3.org/TR/json-ld/> is a light-weight\nsyntax for expressing linked data. It is primarily intended for\nweb-based programming environments, interoperable web services\nand for storing linked data in JSON-based databases. This\npackage provides bindings to the JavaScript library for\nconverting, expanding and compacting JSON-LD documents.",
    "version": "2.2.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://docs.ropensci.org/jsonld/,\nhttps://ropensci.r-universe.dev/jsonld",
    "bug_reports": "https://github.com/ropensci/jsonld/issues",
    "repository": "",
    "exports": [
      [
        "jsonld_compact"
      ],
      [
        "jsonld_expand"
      ],
      [
        "jsonld_flatten"
      ],
      [
        "jsonld_frame"
      ],
      [
        "jsonld_from_rdf"
      ],
      [
        "jsonld_normalize"
      ],
      [
        "jsonld_to_rdf"
      ]
    ],
    "topics": [
      [
        "json-ld"
      ]
    ],
    "score": 7.4136,
    "stars": 35,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "jsonld JSON for Linking Data JSON-LD <https://www.w3.org/TR/json-ld/> is a light-weight\nsyntax for expressing linked data. It is primarily intended for\nweb-based programming environments, interoperable web services\nand for storing linked data in JSON-based databases. This\npackage provides bindings to the JavaScript library for\nconverting, expanding and compacting JSON-LD documents. jsonld_compact jsonld_expand jsonld_flatten jsonld_frame jsonld_from_rdf jsonld_normalize jsonld_to_rdf json-ld"
  },
  {
    "id": 254,
    "package_name": "asciicast",
    "title": "Create 'Ascii' Screen Casts from R Scripts",
    "description": "Record 'asciicast' screen casts from R scripts. Convert\nthem to animated SVG images, to be used in 'README' files, or\nblog posts. Includes 'asciinema-player' as an 'HTML' widget,\nand an 'asciicast' 'knitr' engine, to embed 'ascii' screen\ncasts in 'Rmarkdown' documents.",
    "version": "2.3.1.9000",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi [aut, cre],\nRomain Francois [aut],\nMario Nebl [aut] (https://github.com/marionebl/svg-term author),\nMarcin Kulik [aut] (https://github.com/asciinema/asciinema-player\nauthor),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://asciicast.r-lib.org/, https://github.com/r-lib/asciicast",
    "bug_reports": "https://github.com/r-lib/asciicast/issues",
    "repository": "",
    "exports": [
      [
        "asciicast_options"
      ],
      [
        "asciicast_start_process"
      ],
      [
        "asciinema_player"
      ],
      [
        "clear_screen"
      ],
      [
        "default_theme"
      ],
      [
        "expect_snapshot_r_process"
      ],
      [
        "get_locales"
      ],
      [
        "init_knitr_engine"
      ],
      [
        "install_phantomjs"
      ],
      [
        "merge_casts"
      ],
      [
        "pause"
      ],
      [
        "play"
      ],
      [
        "read_cast"
      ],
      [
        "record"
      ],
      [
        "record_output"
      ],
      [
        "write_gif"
      ],
      [
        "write_html"
      ],
      [
        "write_json"
      ],
      [
        "write_svg"
      ]
    ],
    "topics": [],
    "score": 7.3286,
    "stars": 242,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "asciicast Create 'Ascii' Screen Casts from R Scripts Record 'asciicast' screen casts from R scripts. Convert\nthem to animated SVG images, to be used in 'README' files, or\nblog posts. Includes 'asciinema-player' as an 'HTML' widget,\nand an 'asciicast' 'knitr' engine, to embed 'ascii' screen\ncasts in 'Rmarkdown' documents. asciicast_options asciicast_start_process asciinema_player clear_screen default_theme expect_snapshot_r_process get_locales init_knitr_engine install_phantomjs merge_casts pause play read_cast record record_output write_gif write_html write_json write_svg "
  },
  {
    "id": 398,
    "package_name": "connectwidgets",
    "title": "Organize and Curate Your Content Within 'Posit Connect'",
    "description": "A collection of helper functions and 'htmlwidgets' to help\npublishers curate content collections on 'Posit Connect'. The\ncomponents, Card, Grid, Table, Search, and Filter can be used\nto produce a showcase page or gallery contained within a static\nor interactive R Markdown page.",
    "version": "0.2.1.9000",
    "maintainer": "Brian Smith <brian@rstudio.com>",
    "author": "Brian Smith [aut, cre],\nMarcos Navarro [aut],\nDavid Aja [ctb],\nKelly O'Briant [ctb],\nPosit [cph]",
    "url": "https://rstudio.github.io/connectwidgets/,\nhttps://github.com/rstudio/connectwidgets",
    "bug_reports": "https://github.com/rstudio/connectwidgets/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "by_owner"
      ],
      [
        "by_owners"
      ],
      [
        "by_tag"
      ],
      [
        "by_tags"
      ],
      [
        "connect"
      ],
      [
        "content"
      ],
      [
        "renderRsccard"
      ],
      [
        "renderRscfilter"
      ],
      [
        "renderRscgrid"
      ],
      [
        "renderRscsearch"
      ],
      [
        "rsc_card"
      ],
      [
        "rsc_cols"
      ],
      [
        "rsc_filter"
      ],
      [
        "rsc_grid"
      ],
      [
        "rsc_search"
      ],
      [
        "rsc_table"
      ],
      [
        "rsccardOutput"
      ],
      [
        "rscfilterOutput"
      ],
      [
        "rscgridOutput"
      ],
      [
        "rscsearchOutput"
      ]
    ],
    "topics": [],
    "score": 7.154,
    "stars": 22,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "connectwidgets Organize and Curate Your Content Within 'Posit Connect' A collection of helper functions and 'htmlwidgets' to help\npublishers curate content collections on 'Posit Connect'. The\ncomponents, Card, Grid, Table, Search, and Filter can be used\nto produce a showcase page or gallery contained within a static\nor interactive R Markdown page. %>% by_owner by_owners by_tag by_tags connect content renderRsccard renderRscfilter renderRscgrid renderRscsearch rsc_card rsc_cols rsc_filter rsc_grid rsc_search rsc_table rsccardOutput rscfilterOutput rscgridOutput rscsearchOutput "
  },
  {
    "id": 244,
    "package_name": "antiword",
    "title": "Extract Text from Microsoft Word Documents",
    "description": "Wraps the 'AntiWord' utility to extract text from\nMicrosoft Word documents. The utility only supports the old\n'doc' format, not the new xml based 'docx' format. Use the\n'xml2' package to read the latter.",
    "version": "1.3.4",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nAdri van Os [cph] (Author 'antiword' utility)",
    "url": "https://docs.ropensci.org/antiword/,\nhttps://ropensci.r-universe.dev/antiword",
    "bug_reports": "https://github.com/ropensci/antiword/issues",
    "repository": "",
    "exports": [
      [
        "antiword"
      ]
    ],
    "topics": [
      [
        "antiword"
      ],
      [
        "extract-text"
      ]
    ],
    "score": 7.057,
    "stars": 57,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "antiword Extract Text from Microsoft Word Documents Wraps the 'AntiWord' utility to extract text from\nMicrosoft Word documents. The utility only supports the old\n'doc' format, not the new xml based 'docx' format. Use the\n'xml2' package to read the latter. antiword antiword extract-text"
  },
  {
    "id": 1245,
    "package_name": "sofa",
    "title": "Connector to 'CouchDB'",
    "description": "Provides an interface to the 'NoSQL' database 'CouchDB'\n(<http://couchdb.apache.org>). Methods are provided for\nmanaging databases within 'CouchDB', including\ncreating/deleting/updating/transferring, and managing documents\nwithin databases. One can connect with a local 'CouchDB'\ninstance, or a remote 'CouchDB' databases such as 'Cloudant'.\nDocuments can be inserted directly from vectors, lists,\ndata.frames, and 'JSON'. Targeted at 'CouchDB' v2 or greater.",
    "version": "0.4.1",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-9200-1016>),\nEduard Sz\u00f6cs [aut] (ORCID: <https://orcid.org/0000-0003-1444-9135>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/sofa (devel)\nhttps://docs.ropensci.org/sofa (docs)",
    "bug_reports": "https://github.com/ropensci/sofa/issues",
    "repository": "",
    "exports": [
      [
        "active_tasks"
      ],
      [
        "attach_get"
      ],
      [
        "Cushion"
      ],
      [
        "db_alldocs"
      ],
      [
        "db_bulk_create"
      ],
      [
        "db_bulk_get"
      ],
      [
        "db_bulk_update"
      ],
      [
        "db_changes"
      ],
      [
        "db_compact"
      ],
      [
        "db_create"
      ],
      [
        "db_delete"
      ],
      [
        "db_explain"
      ],
      [
        "db_index"
      ],
      [
        "db_index_create"
      ],
      [
        "db_index_delete"
      ],
      [
        "db_info"
      ],
      [
        "db_list"
      ],
      [
        "db_query"
      ],
      [
        "db_replicate"
      ],
      [
        "db_revisions"
      ],
      [
        "design_create"
      ],
      [
        "design_create_"
      ],
      [
        "design_delete"
      ],
      [
        "design_get"
      ],
      [
        "design_head"
      ],
      [
        "design_info"
      ],
      [
        "design_search"
      ],
      [
        "design_search_many"
      ],
      [
        "doc_attach_create"
      ],
      [
        "doc_attach_delete"
      ],
      [
        "doc_attach_get"
      ],
      [
        "doc_attach_info"
      ],
      [
        "doc_create"
      ],
      [
        "doc_delete"
      ],
      [
        "doc_get"
      ],
      [
        "doc_head"
      ],
      [
        "doc_update"
      ],
      [
        "doc_upsert"
      ],
      [
        "membership"
      ],
      [
        "parse_df"
      ],
      [
        "ping"
      ],
      [
        "restart"
      ],
      [
        "session"
      ],
      [
        "uuids"
      ]
    ],
    "topics": [
      [
        "couchdb"
      ],
      [
        "database"
      ],
      [
        "nosql"
      ],
      [
        "documents"
      ],
      [
        "cloudant"
      ],
      [
        "couchdb-client"
      ]
    ],
    "score": 7.037,
    "stars": 33,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "sofa Connector to 'CouchDB' Provides an interface to the 'NoSQL' database 'CouchDB'\n(<http://couchdb.apache.org>). Methods are provided for\nmanaging databases within 'CouchDB', including\ncreating/deleting/updating/transferring, and managing documents\nwithin databases. One can connect with a local 'CouchDB'\ninstance, or a remote 'CouchDB' databases such as 'Cloudant'.\nDocuments can be inserted directly from vectors, lists,\ndata.frames, and 'JSON'. Targeted at 'CouchDB' v2 or greater. active_tasks attach_get Cushion db_alldocs db_bulk_create db_bulk_get db_bulk_update db_changes db_compact db_create db_delete db_explain db_index db_index_create db_index_delete db_info db_list db_query db_replicate db_revisions design_create design_create_ design_delete design_get design_head design_info design_search design_search_many doc_attach_create doc_attach_delete doc_attach_get doc_attach_info doc_create doc_delete doc_get doc_head doc_update doc_upsert membership parse_df ping restart session uuids couchdb database nosql documents cloudant couchdb-client"
  },
  {
    "id": 1272,
    "package_name": "stantargets",
    "title": "Targets for Stan Workflows",
    "description": "Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'stantargets' R package leverages 'targets'\nand 'cmdstanr' to ease these burdens. 'stantargets' makes it\nsuper easy to set up scalable Stan pipelines that automatically\nparallelize the computation and skip expensive steps when the\nresults are already up to date. Minimal custom code is\nrequired, and there is no need to manually configure branching,\nso usage is much easier than 'targets' alone. 'stantargets' can\naccess all of 'cmdstanr''s major algorithms (MCMC, variational\nBayes, and optimization) and it supports both single-fit\nworkflows and multi-rep simulation studies. For the statistical\nmethodology, please refer to 'Stan' documentation (Stan\nDevelopment Team 2020) <https://mc-stan.org/>.",
    "version": "0.1.2.9000",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1878-3253>),\nKrzysztof Sakrejda [rev],\nMatthew T. Warkentin [rev],\nEli Lilly and Company [cph]",
    "url": "https://docs.ropensci.org/stantargets/,\nhttps://github.com/ropensci/stantargets,\nhttps://r-multiverse.org/topics/bayesian.html",
    "bug_reports": "https://github.com/ropensci/stantargets/issues",
    "repository": "",
    "exports": [
      [
        "tar_stan_compile"
      ],
      [
        "tar_stan_compile_run"
      ],
      [
        "tar_stan_example_data"
      ],
      [
        "tar_stan_example_file"
      ],
      [
        "tar_stan_gq"
      ],
      [
        "tar_stan_gq_rep_draws"
      ],
      [
        "tar_stan_gq_rep_run"
      ],
      [
        "tar_stan_gq_rep_summary"
      ],
      [
        "tar_stan_gq_run"
      ],
      [
        "tar_stan_mcmc"
      ],
      [
        "tar_stan_mcmc_rep_diagnostics"
      ],
      [
        "tar_stan_mcmc_rep_draws"
      ],
      [
        "tar_stan_mcmc_rep_run"
      ],
      [
        "tar_stan_mcmc_rep_summary"
      ],
      [
        "tar_stan_mcmc_run"
      ],
      [
        "tar_stan_mle"
      ],
      [
        "tar_stan_mle_rep_draws"
      ],
      [
        "tar_stan_mle_rep_run"
      ],
      [
        "tar_stan_mle_rep_summary"
      ],
      [
        "tar_stan_mle_run"
      ],
      [
        "tar_stan_output"
      ],
      [
        "tar_stan_rep_data_batch"
      ],
      [
        "tar_stan_summary"
      ],
      [
        "tar_stan_summary_join_data"
      ],
      [
        "tar_stan_vb"
      ],
      [
        "tar_stan_vb_rep_draws"
      ],
      [
        "tar_stan_vb_rep_run"
      ],
      [
        "tar_stan_vb_rep_summary"
      ],
      [
        "tar_stan_vb_run"
      ]
    ],
    "topics": [
      [
        "bayesian"
      ],
      [
        "high-performance-computing"
      ],
      [
        "make"
      ],
      [
        "r-targetopia"
      ],
      [
        "reproducibility"
      ],
      [
        "stan"
      ],
      [
        "statistics"
      ],
      [
        "targets"
      ]
    ],
    "score": 7.0092,
    "stars": 50,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "stantargets Targets for Stan Workflows Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'stantargets' R package leverages 'targets'\nand 'cmdstanr' to ease these burdens. 'stantargets' makes it\nsuper easy to set up scalable Stan pipelines that automatically\nparallelize the computation and skip expensive steps when the\nresults are already up to date. Minimal custom code is\nrequired, and there is no need to manually configure branching,\nso usage is much easier than 'targets' alone. 'stantargets' can\naccess all of 'cmdstanr''s major algorithms (MCMC, variational\nBayes, and optimization) and it supports both single-fit\nworkflows and multi-rep simulation studies. For the statistical\nmethodology, please refer to 'Stan' documentation (Stan\nDevelopment Team 2020) <https://mc-stan.org/>. tar_stan_compile tar_stan_compile_run tar_stan_example_data tar_stan_example_file tar_stan_gq tar_stan_gq_rep_draws tar_stan_gq_rep_run tar_stan_gq_rep_summary tar_stan_gq_run tar_stan_mcmc tar_stan_mcmc_rep_diagnostics tar_stan_mcmc_rep_draws tar_stan_mcmc_rep_run tar_stan_mcmc_rep_summary tar_stan_mcmc_run tar_stan_mle tar_stan_mle_rep_draws tar_stan_mle_rep_run tar_stan_mle_rep_summary tar_stan_mle_run tar_stan_output tar_stan_rep_data_batch tar_stan_summary tar_stan_summary_join_data tar_stan_vb tar_stan_vb_rep_draws tar_stan_vb_rep_run tar_stan_vb_rep_summary tar_stan_vb_run bayesian high-performance-computing make r-targetopia reproducibility stan statistics targets"
  },
  {
    "id": 960,
    "package_name": "phosphoricons",
    "title": "'Phosphor' Icons for R",
    "description": "Use 'Phosphor' icons in 'shiny' applications or\n'rmarkdown' documents. Icons are available in 5 different\nweights and can be customized by setting color, size,\norientation and more.",
    "version": "0.2.1",
    "maintainer": "Victor Perrier <victor.perrier@dreamrs.fr>",
    "author": "Victor Perrier [aut, cre, cph],\nFanny Meyer [aut],\nPhosphor Icons [cph] (Phosphor Icons\n<https://github.com/phosphor-icons>)",
    "url": "https://dreamrs.github.io/phosphoricons/,\nhttps://github.com/dreamRs/phosphoricons",
    "bug_reports": "https://github.com/dreamRs/phosphoricons/issues",
    "repository": "",
    "exports": [
      [
        "html_dependency_phosphor"
      ],
      [
        "ph"
      ],
      [
        "ph_fill"
      ],
      [
        "ph_i"
      ],
      [
        "search_icon"
      ],
      [
        "waffle_icon"
      ]
    ],
    "topics": [],
    "score": 6.967,
    "stars": 31,
    "primary_category": "visualization",
    "source_universe": "dreamrs",
    "search_text": "phosphoricons 'Phosphor' Icons for R Use 'Phosphor' icons in 'shiny' applications or\n'rmarkdown' documents. Icons are available in 5 different\nweights and can be customized by setting color, size,\norientation and more. html_dependency_phosphor ph ph_fill ph_i search_icon waffle_icon "
  },
  {
    "id": 1136,
    "package_name": "roxygen2md",
    "title": "'Roxygen' to 'Markdown'",
    "description": "Converts elements of 'roxygen' documentation to\n'markdown'.",
    "version": "1.0.1.9016",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Kirill M\u00fcller [aut, cre],\nHeather Turner [ctb]",
    "url": "https://roxygen2md.r-lib.org/, https://github.com/r-lib/roxygen2md",
    "bug_reports": "https://github.com/r-lib/roxygen2md/issues",
    "repository": "",
    "exports": [
      [
        "find_rd"
      ],
      [
        "markdownify"
      ],
      [
        "roxygen2md"
      ]
    ],
    "topics": [
      [
        "documentation"
      ],
      [
        "markdown"
      ]
    ],
    "score": 6.9171,
    "stars": 68,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "roxygen2md 'Roxygen' to 'Markdown' Converts elements of 'roxygen' documentation to\n'markdown'. find_rd markdownify roxygen2md documentation markdown"
  },
  {
    "id": 570,
    "package_name": "filtro",
    "title": "Feature Selection Using Supervised Filter-Based Methods",
    "description": "Tidy tools to apply filter-based supervised feature\nselection methods. These methods score and rank feature\nrelevance using metrics such as p-values, correlation, and\nimportance scores (Kuhn and Johnson (2019)\n<doi:10.1201/9781315108230>).",
    "version": "0.2.0.9000",
    "maintainer": "Frances Lin <franceslinyc@gmail.com>",
    "author": "Frances Lin [aut, cre],\nMax Kuhn [aut] (ORCID: <https://orcid.org/0000-0003-2402-136X>),\nEmil Hvitfeldt [aut],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/tidymodels/filtro,\nhttps://filtro.tidymodels.org/",
    "bug_reports": "https://github.com/tidymodels/filtro/issues",
    "repository": "",
    "exports": [
      [
        "arrange_score"
      ],
      [
        "bind_scores"
      ],
      [
        "class_score"
      ],
      [
        "class_score_aov"
      ],
      [
        "class_score_cor"
      ],
      [
        "class_score_imp_rf"
      ],
      [
        "class_score_info_gain"
      ],
      [
        "class_score_list"
      ],
      [
        "class_score_roc_auc"
      ],
      [
        "class_score_xtab"
      ],
      [
        "dont_log_pvalues"
      ],
      [
        "fill_safe_value"
      ],
      [
        "fill_safe_values"
      ],
      [
        "fit"
      ],
      [
        "rank_best_score_dense"
      ],
      [
        "rank_best_score_min"
      ],
      [
        "required_pkgs"
      ],
      [
        "score_aov_fstat"
      ],
      [
        "score_aov_pval"
      ],
      [
        "score_cor_pearson"
      ],
      [
        "score_cor_spearman"
      ],
      [
        "score_gain_ratio"
      ],
      [
        "score_imp_rf"
      ],
      [
        "score_imp_rf_conditional"
      ],
      [
        "score_imp_rf_oblique"
      ],
      [
        "score_info_gain"
      ],
      [
        "score_roc_auc"
      ],
      [
        "score_sym_uncert"
      ],
      [
        "score_xtab_pval_chisq"
      ],
      [
        "score_xtab_pval_fisher"
      ],
      [
        "show_best_desirability_num"
      ],
      [
        "show_best_desirability_prop"
      ],
      [
        "show_best_score_cutoff"
      ],
      [
        "show_best_score_dual"
      ],
      [
        "show_best_score_num"
      ],
      [
        "show_best_score_prop"
      ]
    ],
    "topics": [
      [
        "quarto"
      ]
    ],
    "score": 6.8785,
    "stars": 7,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "filtro Feature Selection Using Supervised Filter-Based Methods Tidy tools to apply filter-based supervised feature\nselection methods. These methods score and rank feature\nrelevance using metrics such as p-values, correlation, and\nimportance scores (Kuhn and Johnson (2019)\n<doi:10.1201/9781315108230>). arrange_score bind_scores class_score class_score_aov class_score_cor class_score_imp_rf class_score_info_gain class_score_list class_score_roc_auc class_score_xtab dont_log_pvalues fill_safe_value fill_safe_values fit rank_best_score_dense rank_best_score_min required_pkgs score_aov_fstat score_aov_pval score_cor_pearson score_cor_spearman score_gain_ratio score_imp_rf score_imp_rf_conditional score_imp_rf_oblique score_info_gain score_roc_auc score_sym_uncert score_xtab_pval_chisq score_xtab_pval_fisher show_best_desirability_num show_best_desirability_prop show_best_score_cutoff show_best_score_dual show_best_score_num show_best_score_prop quarto"
  },
  {
    "id": 1166,
    "package_name": "ruODK",
    "title": "An R Client for the ODK Central API",
    "description": "Access and tidy up data from the 'ODK Central' API.  'ODK\nCentral' is a clearinghouse for digitally captured data using\nODK <https://docs.getodk.org/central-intro/>.  It manages user\naccounts and permissions, stores form definitions, and allows\ndata collection clients like 'ODK Collect' to connect to it for\nform download and submission upload. The 'ODK Central' API is\ndocumented at <https://docs.getodk.org/central-api/>.",
    "version": "1.5.2",
    "maintainer": "Florian W. Mayer <Florian.Mayer@dpc.wa.gov.au>",
    "author": "Florian W. Mayer [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4269-4242>),\nMa\u00eblle Salmon [rev] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nKarissa Whiting [rev] (ORCID: <https://orcid.org/0000-0002-4683-1868>),\nJason Taylor [rev],\nMarcelo Tyszler [ctb] (ORCID: <https://orcid.org/0000-0002-4573-0002>),\nH\u00e9l\u00e8ne Langet [ctb] (ORCID: <https://orcid.org/0000-0002-6758-2397>),\nDBCA [cph, fnd],\nNWSFTCP [fnd]",
    "url": "https://docs.ropensci.org/ruODK, https://github.com/ropensci/ruODK",
    "bug_reports": "https://github.com/ropensci/ruODK/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "attachment_get"
      ],
      [
        "attachment_link"
      ],
      [
        "attachment_list"
      ],
      [
        "audit_get"
      ],
      [
        "drop_null_coords"
      ],
      [
        "encryption_key_list"
      ],
      [
        "enexpr"
      ],
      [
        "enquo"
      ],
      [
        "ensym"
      ],
      [
        "entity_audits"
      ],
      [
        "entity_changes"
      ],
      [
        "entity_create"
      ],
      [
        "entity_delete"
      ],
      [
        "entity_detail"
      ],
      [
        "entity_list"
      ],
      [
        "entity_update"
      ],
      [
        "entity_versions"
      ],
      [
        "entitylist_detail"
      ],
      [
        "entitylist_download"
      ],
      [
        "entitylist_list"
      ],
      [
        "entitylist_update"
      ],
      [
        "expr"
      ],
      [
        "exprs"
      ],
      [
        "form_detail"
      ],
      [
        "form_list"
      ],
      [
        "form_schema"
      ],
      [
        "form_schema_ext"
      ],
      [
        "form_schema_parse"
      ],
      [
        "form_xml"
      ],
      [
        "get_default_fid"
      ],
      [
        "get_default_odkc_version"
      ],
      [
        "get_default_orders"
      ],
      [
        "get_default_pid"
      ],
      [
        "get_default_pp"
      ],
      [
        "get_default_pw"
      ],
      [
        "get_default_tz"
      ],
      [
        "get_default_un"
      ],
      [
        "get_default_url"
      ],
      [
        "get_one_attachment"
      ],
      [
        "get_one_submission"
      ],
      [
        "get_one_submission_att_list"
      ],
      [
        "get_one_submission_audit"
      ],
      [
        "get_retries"
      ],
      [
        "get_ru_verbose"
      ],
      [
        "get_test_fid"
      ],
      [
        "get_test_fid_att"
      ],
      [
        "get_test_fid_gap"
      ],
      [
        "get_test_fid_wkt"
      ],
      [
        "get_test_fid_zip"
      ],
      [
        "get_test_odkc_version"
      ],
      [
        "get_test_pid"
      ],
      [
        "get_test_pp"
      ],
      [
        "get_test_pw"
      ],
      [
        "get_test_un"
      ],
      [
        "get_test_url"
      ],
      [
        "handle_ru_attachments"
      ],
      [
        "handle_ru_datetimes"
      ],
      [
        "handle_ru_geopoints"
      ],
      [
        "handle_ru_geoshapes"
      ],
      [
        "handle_ru_geotraces"
      ],
      [
        "odata_entitylist_data_get"
      ],
      [
        "odata_entitylist_metadata_get"
      ],
      [
        "odata_entitylist_service_get"
      ],
      [
        "odata_metadata_get"
      ],
      [
        "odata_service_get"
      ],
      [
        "odata_submission_get"
      ],
      [
        "odata_submission_rectangle"
      ],
      [
        "odata_svc_parse"
      ],
      [
        "parse_odkc_version"
      ],
      [
        "project_create"
      ],
      [
        "project_detail"
      ],
      [
        "project_list"
      ],
      [
        "quo"
      ],
      [
        "quo_name"
      ],
      [
        "quos"
      ],
      [
        "ru_msg_abort"
      ],
      [
        "ru_msg_info"
      ],
      [
        "ru_msg_noop"
      ],
      [
        "ru_msg_success"
      ],
      [
        "ru_msg_warn"
      ],
      [
        "ru_settings"
      ],
      [
        "ru_setup"
      ],
      [
        "semver_gt"
      ],
      [
        "semver_lt"
      ],
      [
        "split_geopoint"
      ],
      [
        "split_geoshape"
      ],
      [
        "split_geotrace"
      ],
      [
        "submission_audit_get"
      ],
      [
        "submission_detail"
      ],
      [
        "submission_export"
      ],
      [
        "submission_get"
      ],
      [
        "submission_list"
      ],
      [
        "sym"
      ],
      [
        "syms"
      ],
      [
        "user_list"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "open-data"
      ],
      [
        "odk"
      ],
      [
        "api"
      ],
      [
        "data"
      ],
      [
        "dataset"
      ],
      [
        "odata"
      ],
      [
        "odata-client"
      ],
      [
        "odk-central"
      ],
      [
        "opendatakit"
      ]
    ],
    "score": 6.6257,
    "stars": 44,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "ruODK An R Client for the ODK Central API Access and tidy up data from the 'ODK Central' API.  'ODK\nCentral' is a clearinghouse for digitally captured data using\nODK <https://docs.getodk.org/central-intro/>.  It manages user\naccounts and permissions, stores form definitions, and allows\ndata collection clients like 'ODK Collect' to connect to it for\nform download and submission upload. The 'ODK Central' API is\ndocumented at <https://docs.getodk.org/central-api/>. %>% attachment_get attachment_link attachment_list audit_get drop_null_coords encryption_key_list enexpr enquo ensym entity_audits entity_changes entity_create entity_delete entity_detail entity_list entity_update entity_versions entitylist_detail entitylist_download entitylist_list entitylist_update expr exprs form_detail form_list form_schema form_schema_ext form_schema_parse form_xml get_default_fid get_default_odkc_version get_default_orders get_default_pid get_default_pp get_default_pw get_default_tz get_default_un get_default_url get_one_attachment get_one_submission get_one_submission_att_list get_one_submission_audit get_retries get_ru_verbose get_test_fid get_test_fid_att get_test_fid_gap get_test_fid_wkt get_test_fid_zip get_test_odkc_version get_test_pid get_test_pp get_test_pw get_test_un get_test_url handle_ru_attachments handle_ru_datetimes handle_ru_geopoints handle_ru_geoshapes handle_ru_geotraces odata_entitylist_data_get odata_entitylist_metadata_get odata_entitylist_service_get odata_metadata_get odata_service_get odata_submission_get odata_submission_rectangle odata_svc_parse parse_odkc_version project_create project_detail project_list quo quo_name quos ru_msg_abort ru_msg_info ru_msg_noop ru_msg_success ru_msg_warn ru_settings ru_setup semver_gt semver_lt split_geopoint split_geoshape split_geotrace submission_audit_get submission_detail submission_export submission_get submission_list sym syms user_list database open-data odk api data dataset odata odata-client odk-central opendatakit"
  },
  {
    "id": 939,
    "package_name": "paleobioDB",
    "title": "Download and Process Data from the Paleobiology Database",
    "description": "Includes functions to wrap most endpoints of the\n'PaleobioDB' API and to visualize and process the obtained\nfossil data. The API documentation for the Paleobiology\nDatabase can be found at <https://paleobiodb.org/data1.2/>.",
    "version": "1.0.1",
    "maintainer": "Adri\u00e1n Castro Insua <adrian.castro.insua@uvigo.gal>",
    "author": "Sara Varela [aut] (ORCID: <https://orcid.org/0000-0002-5756-5737>),\nJavier Gonz\u00e1lez Hern\u00e1ndez [aut],\nLuciano Fabris Sgarbi [aut] (ORCID:\n<https://orcid.org/0000-0002-0416-3275>),\nAdri\u00e1n Castro Insua [cre, ctb] (ORCID:\n<https://orcid.org/0000-0003-4184-8641>)",
    "url": "https://docs.ropensci.org/paleobioDB/,\nhttps://github.com/ropensci/paleobioDB",
    "bug_reports": "https://github.com/ropensci/paleobioDB/issues",
    "repository": "",
    "exports": [
      [
        "pbdb_collection"
      ],
      [
        "pbdb_collections"
      ],
      [
        "pbdb_collections_geo"
      ],
      [
        "pbdb_interval"
      ],
      [
        "pbdb_intervals"
      ],
      [
        "pbdb_map"
      ],
      [
        "pbdb_map_occur"
      ],
      [
        "pbdb_map_richness"
      ],
      [
        "pbdb_measurements"
      ],
      [
        "pbdb_occurrence"
      ],
      [
        "pbdb_occurrences"
      ],
      [
        "pbdb_opinion"
      ],
      [
        "pbdb_opinions"
      ],
      [
        "pbdb_opinions_taxa"
      ],
      [
        "pbdb_orig_ext"
      ],
      [
        "pbdb_ref_collections"
      ],
      [
        "pbdb_ref_occurrences"
      ],
      [
        "pbdb_ref_specimens"
      ],
      [
        "pbdb_ref_taxa"
      ],
      [
        "pbdb_reference"
      ],
      [
        "pbdb_references"
      ],
      [
        "pbdb_richness"
      ],
      [
        "pbdb_scale"
      ],
      [
        "pbdb_scales"
      ],
      [
        "pbdb_specimen"
      ],
      [
        "pbdb_specimens"
      ],
      [
        "pbdb_strata"
      ],
      [
        "pbdb_strata_auto"
      ],
      [
        "pbdb_subtaxa"
      ],
      [
        "pbdb_taxa"
      ],
      [
        "pbdb_taxa_auto"
      ],
      [
        "pbdb_taxon"
      ],
      [
        "pbdb_temp_range"
      ],
      [
        "pbdb_temporal_resolution"
      ]
    ],
    "topics": [],
    "score": 6.3893,
    "stars": 46,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "paleobioDB Download and Process Data from the Paleobiology Database Includes functions to wrap most endpoints of the\n'PaleobioDB' API and to visualize and process the obtained\nfossil data. The API documentation for the Paleobiology\nDatabase can be found at <https://paleobiodb.org/data1.2/>. pbdb_collection pbdb_collections pbdb_collections_geo pbdb_interval pbdb_intervals pbdb_map pbdb_map_occur pbdb_map_richness pbdb_measurements pbdb_occurrence pbdb_occurrences pbdb_opinion pbdb_opinions pbdb_opinions_taxa pbdb_orig_ext pbdb_ref_collections pbdb_ref_occurrences pbdb_ref_specimens pbdb_ref_taxa pbdb_reference pbdb_references pbdb_richness pbdb_scale pbdb_scales pbdb_specimen pbdb_specimens pbdb_strata pbdb_strata_auto pbdb_subtaxa pbdb_taxa pbdb_taxa_auto pbdb_taxon pbdb_temp_range pbdb_temporal_resolution "
  },
  {
    "id": 719,
    "package_name": "jagstargets",
    "title": "Targets for JAGS Pipelines",
    "description": "Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'jagstargets' R package is leverages\n'targets' and 'R2jags' to ease this burden. 'jagstargets' makes\nit super easy to set up scalable JAGS pipelines that\nautomatically parallelize the computation and skip expensive\nsteps when the results are already up to date. Minimal custom\ncode is required, and there is no need to manually configure\nbranching, so usage is much easier than 'targets' alone. For\nthe underlying methodology, please refer to the documentation\nof 'targets' <doi:10.21105/joss.02959> and 'JAGS' (Plummer\n2003)\n<https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf>.",
    "version": "1.2.3",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1878-3253>),\nDavid Lawrence Miller [rev],\nEli Lilly and Company [cph]",
    "url": "https://docs.ropensci.org/jagstargets/,\nhttps://github.com/ropensci/jagstargets,\nhttps://r-multiverse.org/topics/bayesian.html",
    "bug_reports": "https://github.com/ropensci/jagstargets/issues",
    "repository": "",
    "exports": [
      [
        "tar_jags"
      ],
      [
        "tar_jags_df"
      ],
      [
        "tar_jags_example_data"
      ],
      [
        "tar_jags_example_file"
      ],
      [
        "tar_jags_rep_data_batch"
      ],
      [
        "tar_jags_rep_dic"
      ],
      [
        "tar_jags_rep_draws"
      ],
      [
        "tar_jags_rep_run"
      ],
      [
        "tar_jags_rep_summary"
      ],
      [
        "tar_jags_run"
      ]
    ],
    "topics": [
      [
        "bayesian"
      ],
      [
        "high-performance-computing"
      ],
      [
        "jags"
      ],
      [
        "make"
      ],
      [
        "r-targetopia"
      ],
      [
        "reproducibility"
      ],
      [
        "rjags"
      ],
      [
        "statistics"
      ],
      [
        "targets"
      ],
      [
        "cpp"
      ]
    ],
    "score": 6.2455,
    "stars": 11,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "jagstargets Targets for JAGS Pipelines Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'jagstargets' R package is leverages\n'targets' and 'R2jags' to ease this burden. 'jagstargets' makes\nit super easy to set up scalable JAGS pipelines that\nautomatically parallelize the computation and skip expensive\nsteps when the results are already up to date. Minimal custom\ncode is required, and there is no need to manually configure\nbranching, so usage is much easier than 'targets' alone. For\nthe underlying methodology, please refer to the documentation\nof 'targets' <doi:10.21105/joss.02959> and 'JAGS' (Plummer\n2003)\n<https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf>. tar_jags tar_jags_df tar_jags_example_data tar_jags_example_file tar_jags_rep_data_batch tar_jags_rep_dic tar_jags_rep_draws tar_jags_rep_run tar_jags_rep_summary tar_jags_run bayesian high-performance-computing jags make r-targetopia reproducibility rjags statistics targets cpp"
  },
  {
    "id": 1412,
    "package_name": "vchartr",
    "title": "Interactive Charts with the 'JavaScript' 'VChart' Library",
    "description": "Provides an 'htmlwidgets' interface to 'VChart.js'.\n'VChart', more than just a cross-platform charting library, but\nalso an expressive data storyteller. 'VChart' examples and\ndocumentation are available here:\n<https://www.visactor.io/vchart>.",
    "version": "0.1.4.9100",
    "maintainer": "Victor Perrier <victor.perrier@dreamrs.fr>",
    "author": "Victor Perrier [aut, cre],\nFanny Meyer [aut]",
    "url": "https://dreamrs.github.io/vchartr/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "aes"
      ],
      [
        "format_date_dayjs"
      ],
      [
        "format_datetime_dayjs"
      ],
      [
        "format_num_d3"
      ],
      [
        "JS"
      ],
      [
        "label_format_date"
      ],
      [
        "label_format_datetime"
      ],
      [
        "label_value"
      ],
      [
        "renderVchart"
      ],
      [
        "v_area"
      ],
      [
        "v_bar"
      ],
      [
        "v_boxplot"
      ],
      [
        "v_circlepacking"
      ],
      [
        "v_event"
      ],
      [
        "v_facet_wrap"
      ],
      [
        "v_gauge"
      ],
      [
        "v_heatmap"
      ],
      [
        "v_hist"
      ],
      [
        "v_jitter"
      ],
      [
        "v_labs"
      ],
      [
        "v_line"
      ],
      [
        "v_mark_hline"
      ],
      [
        "v_mark_polygon"
      ],
      [
        "v_mark_rect"
      ],
      [
        "v_mark_segment"
      ],
      [
        "v_mark_vline"
      ],
      [
        "v_pie"
      ],
      [
        "v_progress"
      ],
      [
        "v_radar"
      ],
      [
        "v_sankey"
      ],
      [
        "v_scale_color_discrete"
      ],
      [
        "v_scale_color_manual"
      ],
      [
        "v_scale_colour_gradient"
      ],
      [
        "v_scale_fill_discrete"
      ],
      [
        "v_scale_fill_gradient"
      ],
      [
        "v_scale_fill_manual"
      ],
      [
        "v_scale_size"
      ],
      [
        "v_scale_x_continuous"
      ],
      [
        "v_scale_x_date"
      ],
      [
        "v_scale_x_datetime"
      ],
      [
        "v_scale_x_discrete"
      ],
      [
        "v_scale_x_log"
      ],
      [
        "v_scale_y_continuous"
      ],
      [
        "v_scale_y_date"
      ],
      [
        "v_scale_y_datetime"
      ],
      [
        "v_scale_y_discrete"
      ],
      [
        "v_scale_y_log"
      ],
      [
        "v_scatter"
      ],
      [
        "v_smooth"
      ],
      [
        "v_specs"
      ],
      [
        "v_specs_axes"
      ],
      [
        "v_specs_colors"
      ],
      [
        "v_specs_crosshair"
      ],
      [
        "v_specs_custom_mark"
      ],
      [
        "v_specs_datazoom"
      ],
      [
        "v_specs_indicator"
      ],
      [
        "v_specs_legend"
      ],
      [
        "v_specs_player"
      ],
      [
        "v_specs_tooltip"
      ],
      [
        "v_sunburst"
      ],
      [
        "v_theme"
      ],
      [
        "v_treemap"
      ],
      [
        "v_venn"
      ],
      [
        "v_waterfall"
      ],
      [
        "v_wordcloud"
      ],
      [
        "vars"
      ],
      [
        "vchart"
      ],
      [
        "vchartOutput"
      ],
      [
        "vmap"
      ]
    ],
    "topics": [
      [
        "chart"
      ],
      [
        "htmlwidgets"
      ],
      [
        "shiny"
      ],
      [
        "visualization"
      ]
    ],
    "score": 6.242,
    "stars": 9,
    "primary_category": "visualization",
    "source_universe": "dreamrs",
    "search_text": "vchartr Interactive Charts with the 'JavaScript' 'VChart' Library Provides an 'htmlwidgets' interface to 'VChart.js'.\n'VChart', more than just a cross-platform charting library, but\nalso an expressive data storyteller. 'VChart' examples and\ndocumentation are available here:\n<https://www.visactor.io/vchart>. %>% aes format_date_dayjs format_datetime_dayjs format_num_d3 JS label_format_date label_format_datetime label_value renderVchart v_area v_bar v_boxplot v_circlepacking v_event v_facet_wrap v_gauge v_heatmap v_hist v_jitter v_labs v_line v_mark_hline v_mark_polygon v_mark_rect v_mark_segment v_mark_vline v_pie v_progress v_radar v_sankey v_scale_color_discrete v_scale_color_manual v_scale_colour_gradient v_scale_fill_discrete v_scale_fill_gradient v_scale_fill_manual v_scale_size v_scale_x_continuous v_scale_x_date v_scale_x_datetime v_scale_x_discrete v_scale_x_log v_scale_y_continuous v_scale_y_date v_scale_y_datetime v_scale_y_discrete v_scale_y_log v_scatter v_smooth v_specs v_specs_axes v_specs_colors v_specs_crosshair v_specs_custom_mark v_specs_datazoom v_specs_indicator v_specs_legend v_specs_player v_specs_tooltip v_sunburst v_theme v_treemap v_venn v_waterfall v_wordcloud vars vchart vchartOutput vmap chart htmlwidgets shiny visualization"
  },
  {
    "id": 518,
    "package_name": "emodnet.wfs",
    "title": "Access 'EMODnet' Web Feature Service Data",
    "description": "Access and interrogate 'EMODnet' (European Marine\nObservation and Data Network) Web Feature Service data\n<https://emodnet.ec.europa.eu/en/emodnet-web-service-documentation#data-download-services>.\nThis includes listing existing data sources, and getting data\nfrom each of them.",
    "version": "2.1.1.9000",
    "maintainer": "Joana Beja <joana.beja@vliz.be>",
    "author": "Joana Beja [aut, cre] (ORCID: <https://orcid.org/0000-0002-5196-8447>),\nAnna Krystalli [aut] (ORCID: <https://orcid.org/0000-0002-2378-4915>),\nSalvador Fern\u00e1ndez-Bejarano [aut] (ORCID:\n<https://orcid.org/0000-0003-0535-7677>),\nThomas J Webb [ctb],\nEuropean Marine Observation Data Network (EMODnet) Biology project\nEuropean Commission's Directorate - General for Maritime Affairs\nand Fisheries (DG MARE) [cph],\nVLIZ (VLAAMS INSTITUUT VOOR DE ZEE) [fnd] (ROR:\n<https://ror.org/0496vr396>),\nMa\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nAlec L. Robitaille [rev] (ORCID:\n<https://orcid.org/0000-0002-4706-1762>, Reviewed the package\n(v2.0.2.9000) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/653>.),\nLiz Hare [rev] (ORCID: <https://orcid.org/0000-0002-3978-2543>,\nReviewed the package (v2.0.2.9000) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/653>.),\nFran\u00e7ois Michonneau [rev] (ORCID:\n<https://orcid.org/0000-0002-9092-966X>, Reviewed the package\n(v2.0.2.9000) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/653>.)",
    "url": "https://docs.ropensci.org/emodnet.wfs/,\nhttps://github.com/EMODnet/emodnet.wfs",
    "bug_reports": "https://github.com/EMODnet/emodnet.wfs/issues",
    "repository": "",
    "exports": [
      [
        "emodnet_get_all_wfs_info"
      ],
      [
        "emodnet_get_layer_info"
      ],
      [
        "emodnet_get_layers"
      ],
      [
        "emodnet_get_wfs_info"
      ],
      [
        "emodnet_init_wfs_client"
      ],
      [
        "emodnet_wfs"
      ],
      [
        "layer_attribute_descriptions"
      ],
      [
        "layer_attribute_inspect"
      ],
      [
        "layer_attributes_get_names"
      ],
      [
        "layer_attributes_summarise"
      ],
      [
        "layer_attributes_tbl"
      ],
      [
        "should_run_example"
      ]
    ],
    "topics": [
      [
        "biology"
      ],
      [
        "dataproducts"
      ],
      [
        "emodnet"
      ],
      [
        "marine-data"
      ],
      [
        "wfs"
      ]
    ],
    "score": 6.1796,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "emodnet.wfs Access 'EMODnet' Web Feature Service Data Access and interrogate 'EMODnet' (European Marine\nObservation and Data Network) Web Feature Service data\n<https://emodnet.ec.europa.eu/en/emodnet-web-service-documentation#data-download-services>.\nThis includes listing existing data sources, and getting data\nfrom each of them. emodnet_get_all_wfs_info emodnet_get_layer_info emodnet_get_layers emodnet_get_wfs_info emodnet_init_wfs_client emodnet_wfs layer_attribute_descriptions layer_attribute_inspect layer_attributes_get_names layer_attributes_summarise layer_attributes_tbl should_run_example biology dataproducts emodnet marine-data wfs"
  },
  {
    "id": 436,
    "package_name": "daiquiri",
    "title": "Data Quality Reporting for Temporal Datasets",
    "description": "Generate reports that enable quick visual review of\ntemporal shifts in record-level data. Time series plots showing\naggregated values are automatically created for each data field\n(column) depending on its contents (e.g. min/max/mean values\nfor numeric data, no. of distinct values for categorical data),\nas well as overviews for missing values, non-conformant values,\nand duplicated rows. The resulting reports are shareable and\ncan contribute to forming a transparent record of the entire\nanalysis process. It is designed with Electronic Health Records\nin mind, but can be used for any type of record-level temporal\ndata (i.e. tabular data where each row represents a single\n\"event\", one column contains the \"event date\", and other\ncolumns contain any associated values for the event).",
    "version": "1.2.1",
    "maintainer": "T. Phuong Quan <phuongquan567@outlook.com>",
    "author": "T. Phuong Quan [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8566-1817>),\nJack Cregan [ctb],\nUniversity of Oxford [cph],\nNational Institute for Health Research (NIHR) [fnd],\nBrad Cannell [rev]",
    "url": "https://github.com/ropensci/daiquiri,\nhttps://ropensci.github.io/daiquiri/",
    "bug_reports": "https://github.com/ropensci/daiquiri/issues",
    "repository": "",
    "exports": [
      [
        "aggregate_data"
      ],
      [
        "close_log"
      ],
      [
        "daiquiri_report"
      ],
      [
        "export_aggregated_data"
      ],
      [
        "field_types"
      ],
      [
        "field_types_advanced"
      ],
      [
        "ft_categorical"
      ],
      [
        "ft_datetime"
      ],
      [
        "ft_freetext"
      ],
      [
        "ft_ignore"
      ],
      [
        "ft_numeric"
      ],
      [
        "ft_simple"
      ],
      [
        "ft_strata"
      ],
      [
        "ft_timepoint"
      ],
      [
        "ft_uniqueidentifier"
      ],
      [
        "initialise_log"
      ],
      [
        "prepare_data"
      ],
      [
        "read_data"
      ],
      [
        "report_data"
      ],
      [
        "template_field_types"
      ]
    ],
    "topics": [
      [
        "data-quality"
      ],
      [
        "initial-data-analysis"
      ],
      [
        "reproducible-research"
      ],
      [
        "temporal-data"
      ],
      [
        "time-series"
      ]
    ],
    "score": 6.0682,
    "stars": 39,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "daiquiri Data Quality Reporting for Temporal Datasets Generate reports that enable quick visual review of\ntemporal shifts in record-level data. Time series plots showing\naggregated values are automatically created for each data field\n(column) depending on its contents (e.g. min/max/mean values\nfor numeric data, no. of distinct values for categorical data),\nas well as overviews for missing values, non-conformant values,\nand duplicated rows. The resulting reports are shareable and\ncan contribute to forming a transparent record of the entire\nanalysis process. It is designed with Electronic Health Records\nin mind, but can be used for any type of record-level temporal\ndata (i.e. tabular data where each row represents a single\n\"event\", one column contains the \"event date\", and other\ncolumns contain any associated values for the event). aggregate_data close_log daiquiri_report export_aggregated_data field_types field_types_advanced ft_categorical ft_datetime ft_freetext ft_ignore ft_numeric ft_simple ft_strata ft_timepoint ft_uniqueidentifier initialise_log prepare_data read_data report_data template_field_types data-quality initial-data-analysis reproducible-research temporal-data time-series"
  },
  {
    "id": 886,
    "package_name": "nomisr",
    "title": "Access 'Nomis' UK Labour Market Data",
    "description": "Access UK official statistics from the 'Nomis' database.\n'Nomis' includes data from the Census, the Labour Force Survey,\nDWP benefit statistics and other economic and demographic data\nfrom the Office for National Statistics, based around\nstatistical geographies. See\n<https://www.nomisweb.co.uk/api/v01/help> for full API\ndocumentation.",
    "version": "0.4.7",
    "maintainer": "Evan Odell <evanodell91@gmail.com>",
    "author": "Evan Odell [aut, cre] (ORCID: <https://orcid.org/0000-0003-1845-808X>),\nPaul Egeler [rev, ctb],\nChristophe Dervieux [rev] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nNina Robery [ctb] (Work and Health Indicators with nomisr vignette)",
    "url": "https://github.com/ropensci/nomisr,\nhttps://docs.evanodell.com/nomisr",
    "bug_reports": "https://github.com/ropensci/nomisr/issues",
    "repository": "",
    "exports": [
      [
        "nomis_api_key"
      ],
      [
        "nomis_codelist"
      ],
      [
        "nomis_content_type"
      ],
      [
        "nomis_data_info"
      ],
      [
        "nomis_get_data"
      ],
      [
        "nomis_get_metadata"
      ],
      [
        "nomis_overview"
      ],
      [
        "nomis_search"
      ]
    ],
    "topics": [
      [
        "api-client"
      ],
      [
        "census-data"
      ],
      [
        "demography"
      ],
      [
        "geographic-data"
      ],
      [
        "national-statistics"
      ],
      [
        "official-statistics"
      ],
      [
        "officialstatistics"
      ],
      [
        "peer-reviewed"
      ],
      [
        "uk"
      ]
    ],
    "score": 6.0617,
    "stars": 51,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "nomisr Access 'Nomis' UK Labour Market Data Access UK official statistics from the 'Nomis' database.\n'Nomis' includes data from the Census, the Labour Force Survey,\nDWP benefit statistics and other economic and demographic data\nfrom the Office for National Statistics, based around\nstatistical geographies. See\n<https://www.nomisweb.co.uk/api/v01/help> for full API\ndocumentation. nomis_api_key nomis_codelist nomis_content_type nomis_data_info nomis_get_data nomis_get_metadata nomis_overview nomis_search api-client census-data demography geographic-data national-statistics official-statistics officialstatistics peer-reviewed uk"
  },
  {
    "id": 1344,
    "package_name": "tidypmc",
    "title": "Parse Full Text XML Documents from PubMed Central",
    "description": "Parse XML documents from the Open Access subset of Europe\nPubMed Central <https://europepmc.org> including section\nparagraphs, tables, captions and references.",
    "version": "1.8",
    "maintainer": "Chris Stubben <chris.stubben@hci.utah.edu>",
    "author": "Chris Stubben [aut, cre]",
    "url": "https://docs.ropensci.org/tidypmc,\nhttps://github.com/ropensci/tidypmc",
    "bug_reports": "https://github.com/ropensci/tidypmc/issues",
    "repository": "",
    "exports": [
      [
        "collapse_rows"
      ],
      [
        "pmc_caption"
      ],
      [
        "pmc_metadata"
      ],
      [
        "pmc_reference"
      ],
      [
        "pmc_table"
      ],
      [
        "pmc_text"
      ],
      [
        "pmc_xml"
      ],
      [
        "separate_genes"
      ],
      [
        "separate_refs"
      ],
      [
        "separate_tags"
      ],
      [
        "separate_text"
      ]
    ],
    "topics": [],
    "score": 6.0354,
    "stars": 35,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tidypmc Parse Full Text XML Documents from PubMed Central Parse XML documents from the Open Access subset of Europe\nPubMed Central <https://europepmc.org> including section\nparagraphs, tables, captions and references. collapse_rows pmc_caption pmc_metadata pmc_reference pmc_table pmc_text pmc_xml separate_genes separate_refs separate_tags separate_text "
  },
  {
    "id": 1089,
    "package_name": "reportfactory",
    "title": "Lightweight Infrastructure for Handling Multiple R Markdown\nDocuments",
    "description": "Provides an infrastructure for handling multiple R\nMarkdown reports, including automated curation and\ntime-stamping of outputs, parameterisation and provision of\nhelper functions to manage dependencies.",
    "version": "0.4.1",
    "maintainer": "Thibaut Jombart <thibautjombart@gmail.com>",
    "author": "Thibaut Jombart [aut, cre],\nAmy Gimma [ctb],\nTim Taylor [aut] (ORCID: <https://orcid.org/0000-0002-8587-7113>)",
    "url": "https://github.com/reconverse/reportfactory",
    "bug_reports": "https://github.com/reconverse/reportfactory/issues",
    "repository": "",
    "exports": [
      [
        "compile_reports"
      ],
      [
        "factory_overview"
      ],
      [
        "install_deps"
      ],
      [
        "list_deps"
      ],
      [
        "list_outputs"
      ],
      [
        "list_reports"
      ],
      [
        "new_factory"
      ],
      [
        "validate_factory"
      ]
    ],
    "topics": [
      [
        "infrastructure"
      ],
      [
        "knitr"
      ],
      [
        "rmarkdown"
      ],
      [
        "rmarkdown-document"
      ]
    ],
    "score": 5.9891,
    "stars": 83,
    "primary_category": "epidemiology",
    "source_universe": "reconverse",
    "search_text": "reportfactory Lightweight Infrastructure for Handling Multiple R Markdown\nDocuments Provides an infrastructure for handling multiple R\nMarkdown reports, including automated curation and\ntime-stamping of outputs, parameterisation and provision of\nhelper functions to manage dependencies. compile_reports factory_overview install_deps list_deps list_outputs list_reports new_factory validate_factory infrastructure knitr rmarkdown rmarkdown-document"
  },
  {
    "id": 787,
    "package_name": "mantis",
    "title": "Multiple Time Series Scanner",
    "description": "Generate interactive html reports that enable quick visual\nreview of multiple related time series stored in a data frame.\nFor static datasets, this can help to identify any temporal\nartefacts that may affect the validity of subsequent analyses.\nFor live data feeds, regularly scheduled reports can help to\npro-actively identify data feed problems or unexpected trends\nthat may require action. The reports are self-contained and\nshareable without a web server.",
    "version": "1.0.0.9000",
    "maintainer": "T. Phuong Quan <phuongquan567@outlook.com>",
    "author": "T. Phuong Quan [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8566-1817>),\nUniversity of Oxford [cph],\nNational Institute for Health Research (NIHR) [fnd],\nRodrigo Pires [rev],\nMargaret Siple [rev]",
    "url": "https://github.com/ropensci/mantis,\nhttps://ropensci.github.io/mantis/",
    "bug_reports": "https://github.com/ropensci/mantis/issues",
    "repository": "",
    "exports": [
      [
        "alert_above"
      ],
      [
        "alert_below"
      ],
      [
        "alert_custom"
      ],
      [
        "alert_difference_above_perc"
      ],
      [
        "alert_difference_below_perc"
      ],
      [
        "alert_equals"
      ],
      [
        "alert_missing"
      ],
      [
        "alert_rules"
      ],
      [
        "alertspec"
      ],
      [
        "bespoke_rmd_alert_results"
      ],
      [
        "bespoke_rmd_initialise_widgets"
      ],
      [
        "bespoke_rmd_output"
      ],
      [
        "inputspec"
      ],
      [
        "mantis_alerts"
      ],
      [
        "mantis_report"
      ],
      [
        "outputspec_interactive"
      ],
      [
        "outputspec_static_heatmap"
      ],
      [
        "outputspec_static_multipanel"
      ]
    ],
    "topics": [],
    "score": 5.8573,
    "stars": 2,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "mantis Multiple Time Series Scanner Generate interactive html reports that enable quick visual\nreview of multiple related time series stored in a data frame.\nFor static datasets, this can help to identify any temporal\nartefacts that may affect the validity of subsequent analyses.\nFor live data feeds, regularly scheduled reports can help to\npro-actively identify data feed problems or unexpected trends\nthat may require action. The reports are self-contained and\nshareable without a web server. alert_above alert_below alert_custom alert_difference_above_perc alert_difference_below_perc alert_equals alert_missing alert_rules alertspec bespoke_rmd_alert_results bespoke_rmd_initialise_widgets bespoke_rmd_output inputspec mantis_alerts mantis_report outputspec_interactive outputspec_static_heatmap outputspec_static_multipanel "
  },
  {
    "id": 823,
    "package_name": "mlt.docreg",
    "title": "Most Likely Transformations: Documentation and Regression Tests",
    "description": "Additional documentation, a package vignette and\nregression tests for package mlt.",
    "version": "1.1-12",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "author": "Torsten Hothorn [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8301-0471>)",
    "url": "http://ctm.R-forge.R-project.org",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "checkGH"
      ]
    ],
    "topics": [],
    "score": 5.6806,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "mlt.docreg Most Likely Transformations: Documentation and Regression Tests Additional documentation, a package vignette and\nregression tests for package mlt. checkGH "
  },
  {
    "id": 131,
    "package_name": "RNetLogo",
    "title": "Provides an Interface to the Agent-Based Modelling Platform\n'NetLogo'",
    "description": "Interface to use and access Wilensky's 'NetLogo' (Wilensky\n1999) from R using either headless (no GUI) or interactive GUI\nmode. Provides functions to load models, execute commands, and\nget values from reporters. Mostly analogous to the 'NetLogo'\n'Mathematica' Link\n<https://github.com/NetLogo/Mathematica-Link>.",
    "version": "1.0-4",
    "maintainer": "Jan C. Thiele <rnetlogo@gmx.de>",
    "author": "Jan C. Thiele",
    "url": "http://rnetlogo.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "NLCommand"
      ],
      [
        "NLDfToList"
      ],
      [
        "NLDoCommand"
      ],
      [
        "NLDoCommandWhile"
      ],
      [
        "NLDoReport"
      ],
      [
        "NLDoReportWhile"
      ],
      [
        "NLGetAgentSet"
      ],
      [
        "NLGetGraph"
      ],
      [
        "NLGetPatches"
      ],
      [
        "NLLoadModel"
      ],
      [
        "NLQuit"
      ],
      [
        "NLReport"
      ],
      [
        "NLSetAgentSet"
      ],
      [
        "NLSetPatches"
      ],
      [
        "NLSetPatchSet"
      ],
      [
        "NLSourceFromString"
      ],
      [
        "NLStart"
      ]
    ],
    "topics": [
      [
        "openjdk"
      ]
    ],
    "score": 5.6294,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "RNetLogo Provides an Interface to the Agent-Based Modelling Platform\n'NetLogo' Interface to use and access Wilensky's 'NetLogo' (Wilensky\n1999) from R using either headless (no GUI) or interactive GUI\nmode. Provides functions to load models, execute commands, and\nget values from reporters. Mostly analogous to the 'NetLogo'\n'Mathematica' Link\n<https://github.com/NetLogo/Mathematica-Link>. NLCommand NLDfToList NLDoCommand NLDoCommandWhile NLDoReport NLDoReportWhile NLGetAgentSet NLGetGraph NLGetPatches NLLoadModel NLQuit NLReport NLSetAgentSet NLSetPatches NLSetPatchSet NLSourceFromString NLStart openjdk"
  },
  {
    "id": 331,
    "package_name": "c3dr",
    "title": "Read and Write C3D Motion Capture Files",
    "description": "A wrapper for the 'EZC3D' library to work with C3D motion\ncapture data.",
    "version": "0.2.0.9000",
    "maintainer": "Simon Nolte <s.nolte@dshs-koeln.de>",
    "author": "Simon Nolte [aut, cre] (ORCID: <https://orcid.org/0000-0003-1643-1860>),\nBenjamin Michaud [cph] (Author of included EZC3D library),\nGerman Sport University Cologne [fnd] (ROR:\n<https://ror.org/0189raq88>),\nAymeric Stamm [rev] (reviewed the package (v. 0.1.0) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/686>),\nJuly Pilowsky [rev] (reviewed the package (v. 0.1.0) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/686>)",
    "url": "https://github.com/ropensci/c3dr, https://docs.ropensci.org/c3dr/",
    "bug_reports": "https://github.com/ropensci/c3dr/issues",
    "repository": "",
    "exports": [
      [
        "c3d_analog"
      ],
      [
        "c3d_convert"
      ],
      [
        "c3d_data"
      ],
      [
        "c3d_example"
      ],
      [
        "c3d_read"
      ],
      [
        "c3d_setdata"
      ],
      [
        "c3d_write"
      ]
    ],
    "topics": [
      [
        "quarto"
      ],
      [
        "cpp"
      ]
    ],
    "score": 5.5185,
    "stars": 4,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "c3dr Read and Write C3D Motion Capture Files A wrapper for the 'EZC3D' library to work with C3D motion\ncapture data. c3d_analog c3d_convert c3d_data c3d_example c3d_read c3d_setdata c3d_write quarto cpp"
  },
  {
    "id": 46,
    "package_name": "EndoMineR",
    "title": "Functions to mine endoscopic and associated pathology datasets",
    "description": "This script comprises the functions that are used to clean\nup endoscopic reports and pathology reports as well as many of\nthe scripts used for analysis. The scripts assume the endoscopy\nand histopathology data set is merged already but it can also\nbe used of course with the unmerged datasets.",
    "version": "2.0.1.9000",
    "maintainer": "Sebastian Zeki <sebastiz@hotmail.com>",
    "author": "Sebastian Zeki [aut, cre]",
    "url": "https://docs.ropensci.org/EndoMineR,\nhttps://github.com/ropensci/EndoMineR",
    "bug_reports": "https://github.com/ropensci/EndoMineR/issues",
    "repository": "",
    "exports": [
      [
        "Barretts_FUType"
      ],
      [
        "Barretts_PathStage"
      ],
      [
        "Barretts_PragueScore"
      ],
      [
        "BarrettsAll"
      ],
      [
        "BarrettsBxQual"
      ],
      [
        "BarrettsParisEMR"
      ],
      [
        "BiopsyIndex"
      ],
      [
        "CategoricalByEndoscopist"
      ],
      [
        "ColumnCleanUp"
      ],
      [
        "dev_ExtrapolateOPCS4Prep"
      ],
      [
        "DictionaryInPlaceReplace"
      ],
      [
        "EndoBasicGraph"
      ],
      [
        "Endomerge2"
      ],
      [
        "EndoPaste"
      ],
      [
        "EndoscEndoscopist"
      ],
      [
        "EndoscInstrument"
      ],
      [
        "EndoscMeds"
      ],
      [
        "EndoscopyEvent"
      ],
      [
        "EntityPairs_OneSentence"
      ],
      [
        "EntityPairs_TwoSentence"
      ],
      [
        "Eosinophilics"
      ],
      [
        "EventList"
      ],
      [
        "Extractor"
      ],
      [
        "ExtrapolatefromDictionary"
      ],
      [
        "GISymptomsList"
      ],
      [
        "GRS_Type_Assess_By_Unit"
      ],
      [
        "HistolBxSize"
      ],
      [
        "HistolNumbOfBx"
      ],
      [
        "HistolType"
      ],
      [
        "HistolTypeAndSite"
      ],
      [
        "HowManyOverTime"
      ],
      [
        "IBD_Scores"
      ],
      [
        "ListLookup"
      ],
      [
        "LocationList"
      ],
      [
        "LocationListLower"
      ],
      [
        "LocationListUniversal"
      ],
      [
        "LocationListUpper"
      ],
      [
        "MetricByEndoscopist"
      ],
      [
        "MyImgLibrary"
      ],
      [
        "NegativeRemove"
      ],
      [
        "NegativeRemoveWrapper"
      ],
      [
        "PatientFlow_CircosPlots"
      ],
      [
        "PatientFlowIndividual"
      ],
      [
        "RFACath"
      ],
      [
        "sanity"
      ],
      [
        "scale_colour_Publication"
      ],
      [
        "scale_fill_Publication"
      ],
      [
        "SurveilFirstTest"
      ],
      [
        "SurveilLastTest"
      ],
      [
        "SurveilTimeByRow"
      ],
      [
        "SurveySankey"
      ],
      [
        "textPrep"
      ],
      [
        "theme_Publication"
      ],
      [
        "TimeToStatus"
      ],
      [
        "WordsToNumbers"
      ]
    ],
    "topics": [
      [
        "endoscopy"
      ],
      [
        "gastroenterology"
      ],
      [
        "peer-reviewed"
      ],
      [
        "semi-structured-data"
      ],
      [
        "text-mining"
      ]
    ],
    "score": 5.4804,
    "stars": 13,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "EndoMineR Functions to mine endoscopic and associated pathology datasets This script comprises the functions that are used to clean\nup endoscopic reports and pathology reports as well as many of\nthe scripts used for analysis. The scripts assume the endoscopy\nand histopathology data set is merged already but it can also\nbe used of course with the unmerged datasets. Barretts_FUType Barretts_PathStage Barretts_PragueScore BarrettsAll BarrettsBxQual BarrettsParisEMR BiopsyIndex CategoricalByEndoscopist ColumnCleanUp dev_ExtrapolateOPCS4Prep DictionaryInPlaceReplace EndoBasicGraph Endomerge2 EndoPaste EndoscEndoscopist EndoscInstrument EndoscMeds EndoscopyEvent EntityPairs_OneSentence EntityPairs_TwoSentence Eosinophilics EventList Extractor ExtrapolatefromDictionary GISymptomsList GRS_Type_Assess_By_Unit HistolBxSize HistolNumbOfBx HistolType HistolTypeAndSite HowManyOverTime IBD_Scores ListLookup LocationList LocationListLower LocationListUniversal LocationListUpper MetricByEndoscopist MyImgLibrary NegativeRemove NegativeRemoveWrapper PatientFlow_CircosPlots PatientFlowIndividual RFACath sanity scale_colour_Publication scale_fill_Publication SurveilFirstTest SurveilLastTest SurveilTimeByRow SurveySankey textPrep theme_Publication TimeToStatus WordsToNumbers endoscopy gastroenterology peer-reviewed semi-structured-data text-mining"
  },
  {
    "id": 1180,
    "package_name": "sasquatch",
    "title": "Use 'SAS', R, and 'quarto' Together",
    "description": "Use R and 'SAS' within reproducible multilingual 'quarto'\ndocuments. Run 'SAS' code blocks interactively, send data back\nand forth between 'SAS' and R, and render 'SAS' output within\nquarto documents. 'SAS' connections are established through a\ncombination of 'SASPy' and 'reticulate'.",
    "version": "0.1.0.9001",
    "maintainer": "Ryan Zomorrodi <rzomor2@uic.edu>",
    "author": "Ryan Zomorrodi [aut, cre, cph] (ORCID:\n<https://orcid.org/0009-0003-6417-5985>)",
    "url": "https://docs.ropensci.org/sasquatch,\nhttps://github.com/ropensci/sasquatch",
    "bug_reports": "https://github.com/ropensci/sasquatch/issues",
    "repository": "",
    "exports": [
      [
        "configure_saspy"
      ],
      [
        "install_saspy"
      ],
      [
        "sas_connect"
      ],
      [
        "sas_disconnect"
      ],
      [
        "sas_engine"
      ],
      [
        "sas_file_copy"
      ],
      [
        "sas_file_download"
      ],
      [
        "sas_file_exists"
      ],
      [
        "sas_file_remove"
      ],
      [
        "sas_file_upload"
      ],
      [
        "sas_from_r"
      ],
      [
        "sas_get_session"
      ],
      [
        "sas_list"
      ],
      [
        "sas_run_file"
      ],
      [
        "sas_run_selected"
      ],
      [
        "sas_run_string"
      ],
      [
        "sas_to_r"
      ]
    ],
    "topics": [],
    "score": 5.476,
    "stars": 16,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "sasquatch Use 'SAS', R, and 'quarto' Together Use R and 'SAS' within reproducible multilingual 'quarto'\ndocuments. Run 'SAS' code blocks interactively, send data back\nand forth between 'SAS' and R, and render 'SAS' output within\nquarto documents. 'SAS' connections are established through a\ncombination of 'SASPy' and 'reticulate'. configure_saspy install_saspy sas_connect sas_disconnect sas_engine sas_file_copy sas_file_download sas_file_exists sas_file_remove sas_file_upload sas_from_r sas_get_session sas_list sas_run_file sas_run_selected sas_run_string sas_to_r "
  },
  {
    "id": 522,
    "package_name": "epair",
    "title": "EPA Data Helper for R",
    "description": "Aid the user in making queries to the EPA API site found\nat https://aqs.epa.gov/aqsweb/documents/data_api. This package\ncombines API calling methods from various web scraping packages\nwith specific strings to retrieve data from the EPA API. It\nalso contains easy to use loaded variables that help a user\nnavigate services offered by the API and aid the user in\ndetermining the appropriate way to make a an API call.",
    "version": "1.1.0",
    "maintainer": "G.L. Orozco-Mulfinger <glo003@bucknell.edu>",
    "author": "G.L. Orozco-Mulfinger [aut, cre],\nMadyline Lawrence [aut],\nOwais Gilani [aut]",
    "url": "https://github.com/ropensci/epair",
    "bug_reports": "https://github.com/ropensci/epair/issues",
    "repository": "",
    "exports": [
      [
        "add.variables"
      ],
      [
        "clear.all.cached"
      ],
      [
        "clear.cached"
      ],
      [
        "create.authentication"
      ],
      [
        "get_all_mas"
      ],
      [
        "get_all_pqaos"
      ],
      [
        "get_aqs_key"
      ],
      [
        "get_cbsas"
      ],
      [
        "get_counties_in_state"
      ],
      [
        "get_daily_summary_in_bbox"
      ],
      [
        "get_daily_summary_in_cbsa"
      ],
      [
        "get_daily_summary_in_county"
      ],
      [
        "get_daily_summary_in_site"
      ],
      [
        "get_daily_summary_in_state"
      ],
      [
        "get_fields_by_service"
      ],
      [
        "get_known_issues"
      ],
      [
        "get_monitors_in_bbox"
      ],
      [
        "get_monitors_in_cbsa"
      ],
      [
        "get_monitors_in_county"
      ],
      [
        "get_monitors_in_site"
      ],
      [
        "get_monitors_in_state"
      ],
      [
        "get_parameter_classes"
      ],
      [
        "get_parameters_in_class"
      ],
      [
        "get_qa_ape_in_agency"
      ],
      [
        "get_qa_ape_in_county"
      ],
      [
        "get_qa_ape_in_pqao"
      ],
      [
        "get_qa_ape_in_site"
      ],
      [
        "get_qa_ape_in_state"
      ],
      [
        "get_qa_blanks_in_agency"
      ],
      [
        "get_qa_blanks_in_county"
      ],
      [
        "get_qa_blanks_in_pqao"
      ],
      [
        "get_qa_blanks_in_site"
      ],
      [
        "get_qa_blanks_in_state"
      ],
      [
        "get_qa_ca_in_agency"
      ],
      [
        "get_qa_ca_in_county"
      ],
      [
        "get_qa_ca_in_pqao"
      ],
      [
        "get_qa_ca_in_site"
      ],
      [
        "get_qa_ca_in_state"
      ],
      [
        "get_qa_fra_in_agency"
      ],
      [
        "get_qa_fra_in_county"
      ],
      [
        "get_qa_fra_in_pqao"
      ],
      [
        "get_qa_fra_in_site"
      ],
      [
        "get_qa_fra_in_state"
      ],
      [
        "get_qa_frv_in_agency"
      ],
      [
        "get_qa_frv_in_county"
      ],
      [
        "get_qa_frv_in_pqao"
      ],
      [
        "get_qa_frv_in_site"
      ],
      [
        "get_qa_frv_in_state"
      ],
      [
        "get_qa_pep_in_agency"
      ],
      [
        "get_qa_pep_in_county"
      ],
      [
        "get_qa_pep_in_pqao"
      ],
      [
        "get_qa_pep_in_site"
      ],
      [
        "get_qa_pep_in_state"
      ],
      [
        "get_qa_qc_in_agency"
      ],
      [
        "get_qa_qc_in_county"
      ],
      [
        "get_qa_qc_in_pqao"
      ],
      [
        "get_qa_qc_in_site"
      ],
      [
        "get_qa_qc_in_state"
      ],
      [
        "get_revision_history"
      ],
      [
        "get_sites_in_county"
      ],
      [
        "get_state_fips"
      ],
      [
        "get_tf_qa_ape_in_agency"
      ],
      [
        "get_tf_qa_ape_in_county"
      ],
      [
        "get_tf_qa_ape_in_pqao"
      ],
      [
        "get_tf_qa_ape_in_site"
      ],
      [
        "get_tf_qa_ape_in_state"
      ],
      [
        "get_tf_sample_in_agency"
      ],
      [
        "get_tf_sample_in_county"
      ],
      [
        "get_tf_sample_in_site"
      ],
      [
        "get_tf_sample_in_state"
      ],
      [
        "is_API_running"
      ],
      [
        "list.cached.data"
      ],
      [
        "non.cached.perform.call"
      ],
      [
        "perform.call"
      ],
      [
        "perform.call.raw"
      ],
      [
        "retrieve.cached.call"
      ],
      [
        "save.new.cached.call"
      ]
    ],
    "topics": [],
    "score": 5.2253,
    "stars": 8,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "epair EPA Data Helper for R Aid the user in making queries to the EPA API site found\nat https://aqs.epa.gov/aqsweb/documents/data_api. This package\ncombines API calling methods from various web scraping packages\nwith specific strings to retrieve data from the EPA API. It\nalso contains easy to use loaded variables that help a user\nnavigate services offered by the API and aid the user in\ndetermining the appropriate way to make a an API call. add.variables clear.all.cached clear.cached create.authentication get_all_mas get_all_pqaos get_aqs_key get_cbsas get_counties_in_state get_daily_summary_in_bbox get_daily_summary_in_cbsa get_daily_summary_in_county get_daily_summary_in_site get_daily_summary_in_state get_fields_by_service get_known_issues get_monitors_in_bbox get_monitors_in_cbsa get_monitors_in_county get_monitors_in_site get_monitors_in_state get_parameter_classes get_parameters_in_class get_qa_ape_in_agency get_qa_ape_in_county get_qa_ape_in_pqao get_qa_ape_in_site get_qa_ape_in_state get_qa_blanks_in_agency get_qa_blanks_in_county get_qa_blanks_in_pqao get_qa_blanks_in_site get_qa_blanks_in_state get_qa_ca_in_agency get_qa_ca_in_county get_qa_ca_in_pqao get_qa_ca_in_site get_qa_ca_in_state get_qa_fra_in_agency get_qa_fra_in_county get_qa_fra_in_pqao get_qa_fra_in_site get_qa_fra_in_state get_qa_frv_in_agency get_qa_frv_in_county get_qa_frv_in_pqao get_qa_frv_in_site get_qa_frv_in_state get_qa_pep_in_agency get_qa_pep_in_county get_qa_pep_in_pqao get_qa_pep_in_site get_qa_pep_in_state get_qa_qc_in_agency get_qa_qc_in_county get_qa_qc_in_pqao get_qa_qc_in_site get_qa_qc_in_state get_revision_history get_sites_in_county get_state_fips get_tf_qa_ape_in_agency get_tf_qa_ape_in_county get_tf_qa_ape_in_pqao get_tf_qa_ape_in_site get_tf_qa_ape_in_state get_tf_sample_in_agency get_tf_sample_in_county get_tf_sample_in_site get_tf_sample_in_state is_API_running list.cached.data non.cached.perform.call perform.call perform.call.raw retrieve.cached.call save.new.cached.call "
  },
  {
    "id": 352,
    "package_name": "censo2017",
    "title": "Base de Datos de Facil Acceso del Censo 2017 de Chile (2017\nChilean Census Easy Access Database)",
    "description": "Provee un acceso conveniente a mas de 17 millones de\nregistros de la base de datos del Censo 2017. Los datos fueron\nimportados desde el DVD oficial del INE usando el Convertidor\nREDATAM creado por Pablo De Grande. Esta paquete esta\ndocumentado intencionalmente en castellano asciificado para que\nfuncione sin problema en diferentes plataformas. (Provides\nconvenient access to more than 17 million records from the\nChilean Census 2017 database. The datasets were imported from\nthe official DVD provided by the Chilean National Bureau of\nStatistics by using the REDATAM converter created by Pablo De\nGrande and in addition it includes the maps accompanying these\ndatasets.)",
    "version": "0.6.1",
    "maintainer": "Mauricio Vargas <mavargas11@uc.cl>",
    "author": "Mauricio Vargas [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1017-7574>),\nJuan Correa [ctb],\nMaria Paula Caldas [rev] (rOpenSci),\nFrans van Dunn\u00e9 [rev] (rOpenSci),\nMelina Vidoni [rev] (rOpenSci),\nConstanza Manriquez [rev] (revision independiente de las vinietas),\nInstituto Nacional de Estadisticas (INE) [dtc]",
    "url": "https://docs.ropensci.org/censo2017/",
    "bug_reports": "https://github.com/ropensci/censo2017/issues/",
    "repository": "",
    "exports": [
      [
        "censo_conectar"
      ],
      [
        "censo_descargar"
      ],
      [
        "censo_desconectar"
      ],
      [
        "censo_eliminar"
      ],
      [
        "censo_tabla"
      ]
    ],
    "topics": [
      [
        "censo"
      ],
      [
        "census"
      ],
      [
        "chile"
      ],
      [
        "demografia"
      ],
      [
        "demographics"
      ],
      [
        "duckdb"
      ],
      [
        "redatam"
      ]
    ],
    "score": 5.1099,
    "stars": 28,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "censo2017 Base de Datos de Facil Acceso del Censo 2017 de Chile (2017\nChilean Census Easy Access Database) Provee un acceso conveniente a mas de 17 millones de\nregistros de la base de datos del Censo 2017. Los datos fueron\nimportados desde el DVD oficial del INE usando el Convertidor\nREDATAM creado por Pablo De Grande. Esta paquete esta\ndocumentado intencionalmente en castellano asciificado para que\nfuncione sin problema en diferentes plataformas. (Provides\nconvenient access to more than 17 million records from the\nChilean Census 2017 database. The datasets were imported from\nthe official DVD provided by the Chilean National Bureau of\nStatistics by using the REDATAM converter created by Pablo De\nGrande and in addition it includes the maps accompanying these\ndatasets.) censo_conectar censo_descargar censo_desconectar censo_eliminar censo_tabla censo census chile demografia demographics duckdb redatam"
  },
  {
    "id": 1445,
    "package_name": "wmm",
    "title": "World Magnetic Model",
    "description": "Calculate magnetic field at a given location and time\naccording to the World Magnetic Model (WMM). Both the main\nfield and secular variation components are returned. This\nfunctionality is useful for physicists and geophysicists who\nneed orthogonal components from WMM. Currently, this package\nsupports annualized time inputs between 2000-01-01 to\n2029-12-31. If desired, users can specify which WMM version to\nuse, e.g., the original WMM2015 release or the recent\nout-of-cycle WMM2015 release. Methods used to implement WMM,\nincluding the Gauss coefficients for each release, are\ndescribed in the following publications: NOAA NCEI Geomagnetic\nModeling Team and British Geological Survey (2024)\n<doi:10.25921/aqfd-sd83>, Chulliat et al (2020)\n<doi:10.25923/ytk1-yx35>, Chulliat et al (2019)\n<doi:10.25921/xhr3-0t19>, Chulliat et al (2015)\n<doi:10.7289/V5TB14V7>, Maus et al (2010)\n<https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/WMM2010_Report.pdf>,\nMcLean et al (2004)\n<https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/TRWMM_2005.pdf>,\nand Macmillian et al (2000)\n<https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/wmm2000.pdf>.",
    "version": "1.1.3",
    "maintainer": "Will Frierson <will.frierson@gmail.com>",
    "author": "Will Frierson [aut, cre]",
    "url": "https://github.com/wfrierson/wmm",
    "bug_reports": "https://github.com/wfrierson/wmm/issues",
    "repository": "",
    "exports": [
      [
        "GetMagneticFieldWMM"
      ],
      [
        "wmm"
      ]
    ],
    "topics": [],
    "score": 4.8451,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "wmm World Magnetic Model Calculate magnetic field at a given location and time\naccording to the World Magnetic Model (WMM). Both the main\nfield and secular variation components are returned. This\nfunctionality is useful for physicists and geophysicists who\nneed orthogonal components from WMM. Currently, this package\nsupports annualized time inputs between 2000-01-01 to\n2029-12-31. If desired, users can specify which WMM version to\nuse, e.g., the original WMM2015 release or the recent\nout-of-cycle WMM2015 release. Methods used to implement WMM,\nincluding the Gauss coefficients for each release, are\ndescribed in the following publications: NOAA NCEI Geomagnetic\nModeling Team and British Geological Survey (2024)\n<doi:10.25921/aqfd-sd83>, Chulliat et al (2020)\n<doi:10.25923/ytk1-yx35>, Chulliat et al (2019)\n<doi:10.25921/xhr3-0t19>, Chulliat et al (2015)\n<doi:10.7289/V5TB14V7>, Maus et al (2010)\n<https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/WMM2010_Report.pdf>,\nMcLean et al (2004)\n<https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/TRWMM_2005.pdf>,\nand Macmillian et al (2000)\n<https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/wmm2000.pdf>. GetMagneticFieldWMM wmm "
  },
  {
    "id": 1132,
    "package_name": "roreviewapi",
    "title": "Plumber API to report package structure and function",
    "description": "Plumber API to report package structure and function.",
    "version": "0.1.1.036",
    "maintainer": "Mark Padgham <mark.padgham@email.com>",
    "author": "Mark Padgham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2172-5265>)",
    "url": "https://docs.ropensci.org/roreviewapi,\nhttps://github.com/ropensci-review-tools/roreviewapi",
    "bug_reports": "https://github.com/ropensci-review-tools/roreviewapi/issues",
    "repository": "",
    "exports": [
      [
        "check_cache"
      ],
      [
        "check_issue_template"
      ],
      [
        "collate_editor_check"
      ],
      [
        "dl_gh_repo"
      ],
      [
        "editor_check"
      ],
      [
        "get_branch_from_url"
      ],
      [
        "get_subdir_from_url"
      ],
      [
        "is_user_authorized"
      ],
      [
        "pkgrep_install_deps"
      ],
      [
        "post_to_issue"
      ],
      [
        "push_to_gh_pages"
      ],
      [
        "readme_badge"
      ],
      [
        "readme_has_peer_review_badge"
      ],
      [
        "serve_api"
      ],
      [
        "srr_counts"
      ],
      [
        "srr_counts_from_report"
      ],
      [
        "srr_counts_summary"
      ],
      [
        "stats_badge"
      ],
      [
        "stdout_stderr_cache"
      ],
      [
        "symbol_crs"
      ],
      [
        "symbol_tck"
      ],
      [
        "url_is_r_pkg"
      ]
    ],
    "topics": [],
    "score": 4.7993,
    "stars": 3,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "roreviewapi Plumber API to report package structure and function Plumber API to report package structure and function. check_cache check_issue_template collate_editor_check dl_gh_repo editor_check get_branch_from_url get_subdir_from_url is_user_authorized pkgrep_install_deps post_to_issue push_to_gh_pages readme_badge readme_has_peer_review_badge serve_api srr_counts srr_counts_from_report srr_counts_summary stats_badge stdout_stderr_cache symbol_crs symbol_tck url_is_r_pkg "
  },
  {
    "id": 1263,
    "package_name": "squire",
    "title": "SEIR transmission model of COVID-19",
    "description": "An extended model of the SEIR model used in the Imperial\nCollege London Report into the global impact of COVID-19 and\nstrategies for mitigation and suppression\n(https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/covid-19/report-12-global-impact-covid-19/).\nExtensions now include healthcare treatment pathways and excess\nmortality.",
    "version": "0.7.1",
    "maintainer": "OJ Watson <o.watson15@imperial.ac.uk>",
    "author": "OJ Watson [aut, cre],\nPatrick Walker [aut],\nCharlie Whittaker [aut],\nPeter Winskill [aut],\nGiovanni Charles [aut],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "adjusted_eigen"
      ],
      [
        "align"
      ],
      [
        "beta_est_explicit"
      ],
      [
        "beta_est_simple"
      ],
      [
        "calibrate"
      ],
      [
        "create_master_chain"
      ],
      [
        "default_durations"
      ],
      [
        "default_probs"
      ],
      [
        "drjacoby_mcmc"
      ],
      [
        "explicit_model"
      ],
      [
        "extract_deaths"
      ],
      [
        "extract_hospital_occ"
      ],
      [
        "extract_ICU_occ"
      ],
      [
        "extract_infection_incidence"
      ],
      [
        "format_deterministic_output"
      ],
      [
        "format_output"
      ],
      [
        "format_output_simple_model"
      ],
      [
        "get_elderly_population"
      ],
      [
        "get_healthcare_capacity"
      ],
      [
        "get_lmic_countries"
      ],
      [
        "get_mixing_matrix"
      ],
      [
        "get_population"
      ],
      [
        "parameters_explicit_SEEIR"
      ],
      [
        "parameters_simple_SEEIR"
      ],
      [
        "pmcmc"
      ],
      [
        "projection_plotting"
      ],
      [
        "projections"
      ],
      [
        "run_deterministic_SEIR_model"
      ],
      [
        "run_explicit_SEEIR_model"
      ],
      [
        "run_simple_SEEIR_model"
      ],
      [
        "sample_drjacoby"
      ],
      [
        "trigger_projections"
      ]
    ],
    "topics": [],
    "score": 4.7952,
    "stars": 52,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "squire SEIR transmission model of COVID-19 An extended model of the SEIR model used in the Imperial\nCollege London Report into the global impact of COVID-19 and\nstrategies for mitigation and suppression\n(https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/covid-19/report-12-global-impact-covid-19/).\nExtensions now include healthcare treatment pathways and excess\nmortality. adjusted_eigen align beta_est_explicit beta_est_simple calibrate create_master_chain default_durations default_probs drjacoby_mcmc explicit_model extract_deaths extract_hospital_occ extract_ICU_occ extract_infection_incidence format_deterministic_output format_output format_output_simple_model get_elderly_population get_healthcare_capacity get_lmic_countries get_mixing_matrix get_population parameters_explicit_SEEIR parameters_simple_SEEIR pmcmc projection_plotting projections run_deterministic_SEIR_model run_explicit_SEEIR_model run_simple_SEEIR_model sample_drjacoby trigger_projections "
  },
  {
    "id": 50,
    "package_name": "EpiSoon",
    "title": "Forecast Cases Using Reproduction Numbers",
    "description": "To forecast the time-varying reproduction number and use\nthis to forecast reported case counts. Includes tools to\nevaluate a range of models across samples and time series using\nproper scoring rules.",
    "version": "0.3.1",
    "maintainer": "Sam Abbott <contact@samabbott.co.uk>",
    "author": "Sam Abbott [aut, cre] (ORCID: <https://orcid.org/0000-0001-8057-8037>),\nNikos Bosse [aut],\nJoel Hellewell [aut] (ORCID: <https://orcid.org/0000-0003-2683-0849>),\nKatharine Sherratt [aut],\nJames Munday [aut],\nRobin Thompson [aut],\nAurelien Chateigner [aut],\nSylvain Mareschal [aut],\nAndrea Rau [aut],\nNathalie Vialaneix [aut],\nMichael DeWitt [aut] (ORCID: <https://orcid.org/0000-0001-8940-1967>),\nSebastian Funk [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "brms_model"
      ],
      [
        "bsts_model"
      ],
      [
        "compare_models"
      ],
      [
        "compare_timeseries"
      ],
      [
        "draw_from_si_prob"
      ],
      [
        "evaluate_model"
      ],
      [
        "fable_model"
      ],
      [
        "forecast_cases"
      ],
      [
        "forecast_cases_directly"
      ],
      [
        "forecast_model"
      ],
      [
        "forecast_rt"
      ],
      [
        "forecastHybrid_model"
      ],
      [
        "iterative_case_forecast"
      ],
      [
        "iterative_direct_case_forecast"
      ],
      [
        "iterative_rt_forecast"
      ],
      [
        "lopensemble_model"
      ],
      [
        "plot_compare_timeseries"
      ],
      [
        "plot_forecast"
      ],
      [
        "plot_forecast_evaluation"
      ],
      [
        "plot_scores"
      ],
      [
        "predict_cases"
      ],
      [
        "predict_current_cases"
      ],
      [
        "score_case_forecast"
      ],
      [
        "score_forecast"
      ],
      [
        "summarise_case_forecast"
      ],
      [
        "summarise_forecast"
      ],
      [
        "summarise_scores"
      ]
    ],
    "topics": [
      [
        "case-forecasts"
      ],
      [
        "forecasts"
      ]
    ],
    "score": 4.7871,
    "stars": 7,
    "primary_category": "epidemiology",
    "source_universe": "epiforecasts",
    "search_text": "EpiSoon Forecast Cases Using Reproduction Numbers To forecast the time-varying reproduction number and use\nthis to forecast reported case counts. Includes tools to\nevaluate a range of models across samples and time series using\nproper scoring rules. %>% brms_model bsts_model compare_models compare_timeseries draw_from_si_prob evaluate_model fable_model forecast_cases forecast_cases_directly forecast_model forecast_rt forecastHybrid_model iterative_case_forecast iterative_direct_case_forecast iterative_rt_forecast lopensemble_model plot_compare_timeseries plot_forecast plot_forecast_evaluation plot_scores predict_cases predict_current_cases score_case_forecast score_forecast summarise_case_forecast summarise_forecast summarise_scores case-forecasts forecasts"
  },
  {
    "id": 566,
    "package_name": "fellingdater",
    "title": "Tree-ring dating and estimating felling dates of historical\ntimbers",
    "description": "fellingdater provides a comprehensive suite of functions\nfor dendrochronological and dendroarchaeological analysis,\ncovering a workflow from tree-ring **data processing** and\n**crossdating**, up to the estimation and reporting of\n**felling dates**. Originally designed to assist\ndendrochronologists in inferring felling dates from dated\ntree-ring series with partially preserved sapwood, the package\nhas been expanded to include a workflow for tree-ring dating,\nanalysis, and visualization.",
    "version": "1.2.0",
    "maintainer": "Kristof Haneca <Kristof.Haneca@vlaanderen.be>",
    "author": "Kristof Haneca [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-7719-8305>),\nKoen Van Daele [ctb] (ORCID: <https://orcid.org/0000-0002-8153-2978>),\nRonald Visser [ctb] (ORCID: <https://orcid.org/0000-0001-6966-1729>),\nAntonio Jesus P\u00e9rez-Luque [rev] (ORCID:\n<https://orcid.org/0000-0002-1747-0469>),\nNicholas John Tierney [rev] (ORCID:\n<https://orcid.org/0000-0003-1460-8722>)",
    "url": "https://docs.ropensci.org/fellingdater,\nhttps://github.com/ropensci/fellingdater,\nhttps://ropensci.github.io/fellingdater",
    "bug_reports": "https://github.com/ropensci/fellingdater/issues",
    "repository": "",
    "exports": [
      [
        "cor_table"
      ],
      [
        "fd_report"
      ],
      [
        "fh_header"
      ],
      [
        "hdi"
      ],
      [
        "mov_av"
      ],
      [
        "read_fh"
      ],
      [
        "sgc_for_plot"
      ],
      [
        "sw_combine"
      ],
      [
        "sw_combine_plot"
      ],
      [
        "sw_data_info"
      ],
      [
        "sw_data_overview"
      ],
      [
        "sw_interval"
      ],
      [
        "sw_interval_plot"
      ],
      [
        "sw_model"
      ],
      [
        "sw_model_plot"
      ],
      [
        "sw_sum"
      ],
      [
        "sw_sum_plot"
      ],
      [
        "trs_crossdate"
      ],
      [
        "trs_end_date"
      ],
      [
        "trs_plot_dated"
      ],
      [
        "trs_plot_rwl"
      ],
      [
        "trs_pseudo_rwl"
      ],
      [
        "trs_pv"
      ],
      [
        "trs_remove"
      ],
      [
        "trs_select"
      ],
      [
        "trs_tbp"
      ],
      [
        "trs_tbp_transform"
      ],
      [
        "trs_tho"
      ],
      [
        "trs_tho_transform"
      ],
      [
        "trs_trim"
      ],
      [
        "trs_tSt"
      ],
      [
        "trs_zscore"
      ]
    ],
    "topics": [
      [
        "dendrochronology"
      ],
      [
        "sapwood"
      ],
      [
        "tree-rings"
      ]
    ],
    "score": 4.7324,
    "stars": 9,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "fellingdater Tree-ring dating and estimating felling dates of historical\ntimbers fellingdater provides a comprehensive suite of functions\nfor dendrochronological and dendroarchaeological analysis,\ncovering a workflow from tree-ring **data processing** and\n**crossdating**, up to the estimation and reporting of\n**felling dates**. Originally designed to assist\ndendrochronologists in inferring felling dates from dated\ntree-ring series with partially preserved sapwood, the package\nhas been expanded to include a workflow for tree-ring dating,\nanalysis, and visualization. cor_table fd_report fh_header hdi mov_av read_fh sgc_for_plot sw_combine sw_combine_plot sw_data_info sw_data_overview sw_interval sw_interval_plot sw_model sw_model_plot sw_sum sw_sum_plot trs_crossdate trs_end_date trs_plot_dated trs_plot_rwl trs_pseudo_rwl trs_pv trs_remove trs_select trs_tbp trs_tbp_transform trs_tho trs_tho_transform trs_trim trs_tSt trs_zscore dendrochronology sapwood tree-rings"
  },
  {
    "id": 349,
    "package_name": "cde",
    "title": "Download Data from the Catchment Data Explorer Website",
    "description": "Facilitates searching, download and plotting of Water\nFramework Directive (WFD) reporting data for all waterbodies\nwithin the UK Environment Agency area. The types of data that\ncan be downloaded are: WFD status classification data, Reasons\nfor Not Achieving Good (RNAG) status, objectives set for\nwaterbodies, measures put in place to improve water quality and\ndetails of associated protected areas. The site accessed is\n<https://environment.data.gov.uk/catchment-planning/>. The data\nare made available under the Open Government Licence v3.0\n<https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/>.",
    "version": "0.4.1.9005",
    "maintainer": "Rob Briers <r.briers@napier.ac.uk>",
    "author": "Rob Briers [aut, cre] (ORCID: <https://orcid.org/0000-0003-0341-1203>)",
    "url": "https://docs.ropensci.org/cde, https://github.com/ropensci/cde",
    "bug_reports": "https://github.com/ropensci/cde/issues",
    "repository": "",
    "exports": [
      [
        "get_objectives"
      ],
      [
        "get_pa"
      ],
      [
        "get_rnag"
      ],
      [
        "get_status"
      ],
      [
        "search_names"
      ]
    ],
    "topics": [],
    "score": 4.5378,
    "stars": 5,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "cde Download Data from the Catchment Data Explorer Website Facilitates searching, download and plotting of Water\nFramework Directive (WFD) reporting data for all waterbodies\nwithin the UK Environment Agency area. The types of data that\ncan be downloaded are: WFD status classification data, Reasons\nfor Not Achieving Good (RNAG) status, objectives set for\nwaterbodies, measures put in place to improve water quality and\ndetails of associated protected areas. The site accessed is\n<https://environment.data.gov.uk/catchment-planning/>. The data\nare made available under the Open Government Licence v3.0\n<https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/>. get_objectives get_pa get_rnag get_status search_names "
  },
  {
    "id": 163,
    "package_name": "Rdatasets",
    "title": "Access Datasets from the Rdatasets Archive",
    "description": "Download and access datasets from the Rdatasets archive\n(<https://vincentarelbundock.github.io/Rdatasets/>). The\npackage provides functions to search, download, and view\ndocumentation for thousands of datasets from various R\npackages, available in both CSV and Parquet formats for\nefficient access.",
    "version": "0.0.1.1",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "author": "Vincent Arel-Bundock [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>)",
    "url": "https://vincentarelbundock.github.io/Rdatasetspkg/,\nhttps://vincentarelbundock.github.io/Rdatasets/",
    "bug_reports": "https://github.com/vincentarelbundock/Rdatasetspkg/issues",
    "repository": "",
    "exports": [
      [
        "rddata"
      ],
      [
        "rddocs"
      ],
      [
        "rdindex"
      ],
      [
        "rdsearch"
      ]
    ],
    "topics": [],
    "score": 4.2703,
    "stars": 2,
    "primary_category": "statistics",
    "source_universe": "vincentarelbundock",
    "search_text": "Rdatasets Access Datasets from the Rdatasets Archive Download and access datasets from the Rdatasets archive\n(<https://vincentarelbundock.github.io/Rdatasets/>). The\npackage provides functions to search, download, and view\ndocumentation for thousands of datasets from various R\npackages, available in both CSV and Parquet formats for\nefficient access. rddata rddocs rdindex rdsearch "
  },
  {
    "id": 271,
    "package_name": "babeldown",
    "title": "Helpers for Automatic Translation of Markdown-based Content",
    "description": "Provide workflows and guidance for automatic translation\nof Markdown-based R content using DeepL API.",
    "version": "0.0.0.9000",
    "maintainer": "Ma\u00eblle Salmon <msmaellesalmon@gmail.com>",
    "author": "Ma\u00eblle Salmon [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-2815-0399>),\nXavier Timbeau [ctb],\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci-review-tools/babeldown",
    "bug_reports": "https://github.com/ropensci-review-tools/babeldown/issues",
    "repository": "",
    "exports": [
      [
        "deepl_languages"
      ],
      [
        "deepl_translate"
      ],
      [
        "deepl_translate_hugo"
      ],
      [
        "deepl_translate_markdown_string"
      ],
      [
        "deepl_translate_quarto"
      ],
      [
        "deepl_translate_vtt"
      ],
      [
        "deepl_update"
      ],
      [
        "deepl_upsert_glossary"
      ],
      [
        "deepl_usage"
      ]
    ],
    "topics": [
      [
        "quarto"
      ]
    ],
    "score": 4.2601,
    "stars": 26,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "babeldown Helpers for Automatic Translation of Markdown-based Content Provide workflows and guidance for automatic translation\nof Markdown-based R content using DeepL API. deepl_languages deepl_translate deepl_translate_hugo deepl_translate_markdown_string deepl_translate_quarto deepl_translate_vtt deepl_update deepl_upsert_glossary deepl_usage quarto"
  },
  {
    "id": 488,
    "package_name": "docthis",
    "title": "RStudio Addin to Ease Writing Documentation",
    "description": "An RStudio addin that builds the skeleton of documentation\nfor an R function or dataframe using the roxygen2 syntax.",
    "version": "0.1.1",
    "maintainer": "Matthew Lincoln <matthew.d.lincoln@gmail.com>",
    "author": "Matthew Lincoln [aut, cre]",
    "url": "https://github.com/mdlincoln/docthis",
    "bug_reports": "https://github.com/mdlincoln/docthis/issues",
    "repository": "",
    "exports": [
      [
        "doc_this"
      ],
      [
        "doc_this_addin"
      ]
    ],
    "topics": [],
    "score": 4.2402,
    "stars": 61,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "docthis RStudio Addin to Ease Writing Documentation An RStudio addin that builds the skeleton of documentation\nfor an R function or dataframe using the roxygen2 syntax. doc_this doc_this_addin "
  },
  {
    "id": 1014,
    "package_name": "prismjs",
    "title": "Server-Side Syntax Highlighting",
    "description": "Prism <https://prismjs.com/> is a lightweight, extensible\nsyntax highlighter, built with modern web standards in mind.\nThis package provides server-side rendering in R using 'V8'\nsuch that no JavaScript library is required in the resulting\nHTML documents. Over 400 languages are supported.",
    "version": "2.1.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nLea Verou [cph] (Prism JavaScript library)",
    "url": "https://ropensci.r-universe.dev/prismjs\nhttps://docs.ropensci.org/prismjs/",
    "bug_reports": "https://github.com/ropensci/prismjs/issues",
    "repository": "",
    "exports": [
      [
        "prism_highlight_document"
      ],
      [
        "prism_highlight_text"
      ],
      [
        "prism_languages"
      ],
      [
        "prism_process_xmldoc"
      ]
    ],
    "topics": [],
    "score": 4.1761,
    "stars": 5,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "prismjs Server-Side Syntax Highlighting Prism <https://prismjs.com/> is a lightweight, extensible\nsyntax highlighter, built with modern web standards in mind.\nThis package provides server-side rendering in R using 'V8'\nsuch that no JavaScript library is required in the resulting\nHTML documents. Over 400 languages are supported. prism_highlight_document prism_highlight_text prism_languages prism_process_xmldoc "
  },
  {
    "id": 1246,
    "package_name": "softbib",
    "title": "Software Bibliographies for R Projects",
    "description": "Detect libraries used in a project and automatically\ncreate software bibliographies in 'PDF', 'Word', 'Rmarkdown',\nand 'BibTeX' formats.",
    "version": "0.0.2",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "author": "Vincent Arel-Bundock [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>)",
    "url": "https://github.com/vincentarelbundock/softbib",
    "bug_reports": "https://github.com/vincentarelbundock/softbib/issues",
    "repository": "",
    "exports": [
      [
        "softbib"
      ]
    ],
    "topics": [],
    "score": 4.1761,
    "stars": 30,
    "primary_category": "statistics",
    "source_universe": "vincentarelbundock",
    "search_text": "softbib Software Bibliographies for R Projects Detect libraries used in a project and automatically\ncreate software bibliographies in 'PDF', 'Word', 'Rmarkdown',\nand 'BibTeX' formats. softbib "
  },
  {
    "id": 1402,
    "package_name": "unrtf",
    "title": "Extract Text from Rich Text Format (RTF) Documents",
    "description": "Wraps the 'unrtf' utility\n<https://www.gnu.org/software/unrtf/> to extract text from RTF\nfiles. Supports document conversion to HTML, LaTeX or plain\ntext. Output in HTML is recommended because 'unrtf' has limited\nsupport for converting between character encodings.",
    "version": "1.4.7",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre],\nFree Software Foundation, Inc [cph]",
    "url": "https://docs.ropensci.org/unrtf/\nhttps://ropensci.r-universe.dev/unrtf",
    "bug_reports": "https://github.com/ropensci/unrtf/issues",
    "repository": "",
    "exports": [
      [
        "unrtf"
      ]
    ],
    "topics": [
      [
        "extract-text"
      ],
      [
        "rtf"
      ],
      [
        "unrtf"
      ]
    ],
    "score": 4.0512,
    "stars": 15,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "unrtf Extract Text from Rich Text Format (RTF) Documents Wraps the 'unrtf' utility\n<https://www.gnu.org/software/unrtf/> to extract text from RTF\nfiles. Supports document conversion to HTML, LaTeX or plain\ntext. Output in HTML is recommended because 'unrtf' has limited\nsupport for converting between character encodings. unrtf extract-text rtf unrtf"
  },
  {
    "id": 548,
    "package_name": "exoplanets",
    "title": "Access NASA's Exoplanet Archive Data",
    "description": "The goal of exoplanets is to provide access to NASA's\nExoplanet Archive TAP Service. For more information regarding\nthe API please read the documentation\n<https://exoplanetarchive.ipac.caltech.edu/index.html>.",
    "version": "0.2.3",
    "maintainer": "Tyler Littlefield <tylerlittlefield@hey.com>",
    "author": "Tyler Littlefield [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6020-1125>),\nMa\u00eblle Salmon [ctb, rev],\nMathida Chuk [ctb]",
    "url": "https://docs.ropensci.org/exoplanets/,\nhttps://github.com/ropensci/exoplanets",
    "bug_reports": "https://github.com/ropensci/exoplanets/issues",
    "repository": "",
    "exports": [
      [
        "exoplanets"
      ],
      [
        "forget_exoplanets"
      ]
    ],
    "topics": [
      [
        "api"
      ],
      [
        "exoplanets"
      ],
      [
        "nasa"
      ]
    ],
    "score": 3.959,
    "stars": 14,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "exoplanets Access NASA's Exoplanet Archive Data The goal of exoplanets is to provide access to NASA's\nExoplanet Archive TAP Service. For more information regarding\nthe API please read the documentation\n<https://exoplanetarchive.ipac.caltech.edu/index.html>. exoplanets forget_exoplanets api exoplanets nasa"
  },
  {
    "id": 1353,
    "package_name": "tif",
    "title": "Text Interchange Format",
    "description": "Provides validation functions for common interchange\nformats for representing text data in R. Includes formats for\ncorpus objects, document term matrices, and tokens. Other\nannotations can be stored by overloading the tokens structure.",
    "version": "0.4",
    "maintainer": "Taylor B. Arnold <tarnold2@richmond.edu>",
    "author": "Taylor Arnold [aut, cre],\nKen Benoit [aut],\nLincoln Mullen [aut],\nAdam Obeng [aut],\nrOpenSci Text Workshop Participants (2017) [aut]",
    "url": "https://docs.ropensci.org/tif, https://github.com/ropensci/tif",
    "bug_reports": "http://github.com/ropensci/tif/issues",
    "repository": "",
    "exports": [
      [
        "tif_as_corpus_character"
      ],
      [
        "tif_as_corpus_df"
      ],
      [
        "tif_as_tokens_df"
      ],
      [
        "tif_as_tokens_list"
      ],
      [
        "tif_is_corpus_character"
      ],
      [
        "tif_is_corpus_df"
      ],
      [
        "tif_is_dtm"
      ],
      [
        "tif_is_tokens_df"
      ],
      [
        "tif_is_tokens_list"
      ]
    ],
    "topics": [
      [
        "corpus"
      ],
      [
        "natural-language-processing"
      ],
      [
        "term-frequency"
      ],
      [
        "text-processing"
      ],
      [
        "tokenizer"
      ]
    ],
    "score": 3.9106,
    "stars": 37,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tif Text Interchange Format Provides validation functions for common interchange\nformats for representing text data in R. Includes formats for\ncorpus objects, document term matrices, and tokens. Other\nannotations can be stored by overloading the tokens structure. tif_as_corpus_character tif_as_corpus_df tif_as_tokens_df tif_as_tokens_list tif_is_corpus_character tif_is_corpus_df tif_is_dtm tif_is_tokens_df tif_is_tokens_list corpus natural-language-processing term-frequency text-processing tokenizer"
  },
  {
    "id": 479,
    "package_name": "distrDoc",
    "title": "Documentation for 'distr' Family of R Packages",
    "description": "Provides documentation in form of a common vignette to\npackages 'distr', 'distrEx', 'distrMod', 'distrSim',\n'distrTEst', 'distrTeach', and 'distrEllipse'.",
    "version": "2.8.5",
    "maintainer": "Peter Ruckdeschel <peter.ruckdeschel@uni-oldenburg.de>",
    "author": "Florian Camphausen [ctb] (contributed as student to the documented\npackages in the initial phase --2005),\nMatthias Kohl [aut, cph],\nPeter Ruckdeschel [cre, cph],\nThomas Stabla [ctb] (contributed as student to the documented packages\nin the initial phase --2005)",
    "url": "http://distr.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [],
    "topics": [],
    "score": 3.9031,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "distrDoc Documentation for 'distr' Family of R Packages Provides documentation in form of a common vignette to\npackages 'distr', 'distrEx', 'distrMod', 'distrSim',\n'distrTEst', 'distrTeach', and 'distrEllipse'.  "
  },
  {
    "id": 1005,
    "package_name": "postdoc",
    "title": "Minimal and Uncluttered Package Documentation",
    "description": "Generates simple and beautiful one-page HTML reference\nmanuals with package documentation. Math rendering and syntax\nhighlighting are done server-side in R such that no JavaScript\nlibraries are needed in the browser, which makes the\ndocumentation portable and fast to load.",
    "version": "1.4.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://ropensci.r-universe.dev/postdoc\nhttps://docs.ropensci.org/postdoc/",
    "bug_reports": "https://github.com/ropensci/postdoc/issues",
    "repository": "",
    "exports": [
      [
        "r_universe_link"
      ],
      [
        "render_base_manuals"
      ],
      [
        "render_package_manual"
      ]
    ],
    "topics": [],
    "score": 3.7782,
    "stars": 12,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "postdoc Minimal and Uncluttered Package Documentation Generates simple and beautiful one-page HTML reference\nmanuals with package documentation. Math rendering and syntax\nhighlighting are done server-side in R such that no JavaScript\nlibraries are needed in the browser, which makes the\ndocumentation portable and fast to load. r_universe_link render_base_manuals render_package_manual "
  },
  {
    "id": 225,
    "package_name": "aeolus",
    "title": "Unleash Useful Linebreaks in Markdown Documents",
    "description": "Add linebreaks at the end of sentences and remove other\nlinebreaks.",
    "version": "0.0.0.9000",
    "maintainer": "Ma\u00eblle Salmon <msmaellesalmon@gmail.com>",
    "author": "Ma\u00eblle Salmon [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-2815-0399>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://docs.ropensci.org/aeolus/,\nhttps://github.com/maelle/aeolus",
    "bug_reports": "https://github.com/maelle/aeolus/issues",
    "repository": "",
    "exports": [
      [
        "unleash"
      ],
      [
        "use_sentence_linebreaks"
      ]
    ],
    "topics": [],
    "score": 3.4472,
    "stars": 14,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "aeolus Unleash Useful Linebreaks in Markdown Documents Add linebreaks at the end of sentences and remove other\nlinebreaks. unleash use_sentence_linebreaks "
  },
  {
    "id": 44,
    "package_name": "Eagle",
    "title": "Multiple Locus Association Mapping on a Genome-Wide Scale",
    "description": "An implementation of multiple-locus association mapping on\na genome-wide scale. 'Eagle' can handle inbred or outbred study\npopulations, populations of arbitrary unknown complexity, and\ndata larger than the memory capacity of the computer. Since\n'Eagle' is based on linear mixed models, it is best suited to\nthe analysis of data on continuous traits. However, it can\ntolerate non-normal data. 'Eagle' reports, as its findings, the\nbest set of snp in strongest association with a trait. To\nperform an analysis, run 'OpenGUI()'. This opens a web browser\nto the menu-driven user interface for the input of data, and\nfor performing genome-wide analysis.",
    "version": "1.0.3",
    "maintainer": "Andrew George <andrew.george@csiro.au>",
    "author": "Andrew George [aut, cre], Joshua Bowden [ctb], Ryan Stephenson\n[ctb], Hyun Kang [ctb], Noah Zaitlen [ctb], Claire Wade [ctb],\nAndrew Kirby [ctb], David Heckerman [ctb], Mark Daly [ctb],\nEleazar Eskin [ctb]",
    "url": "http://eagle.r-forge.r-project.org",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "AM"
      ],
      [
        "OpenGUI"
      ],
      [
        "ReadMap"
      ],
      [
        "ReadMarker"
      ],
      [
        "ReadPheno"
      ],
      [
        "SummaryAM"
      ]
    ],
    "topics": [
      [
        "cpp"
      ],
      [
        "openmp"
      ]
    ],
    "score": 3.4314,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "Eagle Multiple Locus Association Mapping on a Genome-Wide Scale An implementation of multiple-locus association mapping on\na genome-wide scale. 'Eagle' can handle inbred or outbred study\npopulations, populations of arbitrary unknown complexity, and\ndata larger than the memory capacity of the computer. Since\n'Eagle' is based on linear mixed models, it is best suited to\nthe analysis of data on continuous traits. However, it can\ntolerate non-normal data. 'Eagle' reports, as its findings, the\nbest set of snp in strongest association with a trait. To\nperform an analysis, run 'OpenGUI()'. This opens a web browser\nto the menu-driven user interface for the input of data, and\nfor performing genome-wide analysis. AM OpenGUI ReadMap ReadMarker ReadPheno SummaryAM cpp openmp"
  },
  {
    "id": 266,
    "package_name": "awardFindR",
    "title": "awardFindR",
    "description": "Queries a number of scientific awards databases. Collects\nrelevant results based on keyword and date parameters, returns\nlist of projects that fit those criteria as a data frame.\nSources include: Arnold Ventures, Carnegie Corp, Federal\nRePORTER, Gates Foundation, MacArthur Foundation, Mellon\nFoundation, NEH, NIH, NSF, Open Philanthropy, Open Society\nFoundations, Rockefeller Foundation, Russell Sage Foundation,\nRobert Wood Johnson Foundation, Sloan Foundation, Social\nScience Research Council, John Templeton Foundation, and\nUSASpending.gov.",
    "version": "1.0.1",
    "maintainer": "Michael McCall <mimccall@syr.edu>",
    "author": "Michael C. McCall",
    "url": "https://github.com/PESData/awardFindR",
    "bug_reports": "https://github.com/PESData/awardFindR/issues",
    "repository": "",
    "exports": [
      [
        "create_templeton_df"
      ],
      [
        "get_arnold"
      ],
      [
        "get_carnegie"
      ],
      [
        "get_gates"
      ],
      [
        "get_macarthur"
      ],
      [
        "get_mellon"
      ],
      [
        "get_nih"
      ],
      [
        "get_nsf"
      ],
      [
        "get_ophil"
      ],
      [
        "get_osociety"
      ],
      [
        "get_rockefeller"
      ],
      [
        "get_rsf"
      ],
      [
        "get_rwjf"
      ],
      [
        "get_sloan"
      ],
      [
        "get_ssrc"
      ],
      [
        "get_templeton"
      ],
      [
        "get_usaspend"
      ],
      [
        "search_awards"
      ]
    ],
    "topics": [],
    "score": 3.4314,
    "stars": 18,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "awardFindR awardFindR Queries a number of scientific awards databases. Collects\nrelevant results based on keyword and date parameters, returns\nlist of projects that fit those criteria as a data frame.\nSources include: Arnold Ventures, Carnegie Corp, Federal\nRePORTER, Gates Foundation, MacArthur Foundation, Mellon\nFoundation, NEH, NIH, NSF, Open Philanthropy, Open Society\nFoundations, Rockefeller Foundation, Russell Sage Foundation,\nRobert Wood Johnson Foundation, Sloan Foundation, Social\nScience Research Council, John Templeton Foundation, and\nUSASpending.gov. create_templeton_df get_arnold get_carnegie get_gates get_macarthur get_mellon get_nih get_nsf get_ophil get_osociety get_rockefeller get_rsf get_rwjf get_sloan get_ssrc get_templeton get_usaspend search_awards "
  },
  {
    "id": 1044,
    "package_name": "quartificate",
    "title": "Transform Google Docs into Quarto Books",
    "description": "Automate the Transformation of a Google Document into a\nQuarto Book source.",
    "version": "0.0.0.9000",
    "maintainer": "Ma\u00eblle Salmon <msmaellesalmon@gmail.com>",
    "author": "Ma\u00eblle Salmon [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-2815-0399>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropenscilabs/quartificate",
    "bug_reports": "https://github.com/ropenscilabs/quartificate/issues",
    "repository": "",
    "exports": [
      [
        "quartificate"
      ]
    ],
    "topics": [
      [
        "quarto"
      ]
    ],
    "score": 3.3802,
    "stars": 48,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "quartificate Transform Google Docs into Quarto Books Automate the Transformation of a Google Document into a\nQuarto Book source. quartificate quarto"
  },
  {
    "id": 916,
    "package_name": "orderly.db",
    "title": "Database Support for 'orderly'",
    "description": "Access databases from 'orderly' while running reports.\nIncludes the basic 'SQL' support originally included in\n'orderly' for establishing connections and setting up data for\nuse within a report.",
    "version": "0.1.5",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "https://github.com/vimc/orderly.db",
    "bug_reports": "https://github.com/vimc/orderly.db/issues",
    "repository": "",
    "exports": [
      [
        "orderly_db_connection"
      ],
      [
        "orderly_db_query"
      ],
      [
        "orderly_db_view"
      ]
    ],
    "topics": [],
    "score": 3.301,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "orderly.db Database Support for 'orderly' Access databases from 'orderly' while running reports.\nIncludes the basic 'SQL' support originally included in\n'orderly' for establishing connections and setting up data for\nuse within a report. orderly_db_connection orderly_db_query orderly_db_view "
  },
  {
    "id": 1047,
    "package_name": "r2readthedocs",
    "title": "Convert R Package Documentation to a 'readthedocs' Website",
    "description": "Convert R package documentation to a 'readthedocs'\nwebsite.",
    "version": "0.1.0.002",
    "maintainer": "Mark Padgham <mark.padgham@email.com>",
    "author": "Mark Padgham [aut, cre]",
    "url": "https://github.com/ropenscilabs/r2readthedocs",
    "bug_reports": "https://github.com/ropenscilabs/r2readthedocs/issues",
    "repository": "",
    "exports": [
      [
        "r2readthedocs"
      ],
      [
        "rtd_build"
      ],
      [
        "rtd_clean"
      ],
      [
        "rtd_dummy_pkg"
      ],
      [
        "rtd_open"
      ]
    ],
    "topics": [],
    "score": 2.9031,
    "stars": 16,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "r2readthedocs Convert R Package Documentation to a 'readthedocs' Website Convert R package documentation to a 'readthedocs'\nwebsite. r2readthedocs rtd_build rtd_clean rtd_dummy_pkg rtd_open "
  },
  {
    "id": 920,
    "package_name": "orderlyweb",
    "title": "Orderly Support for 'OrderlyWeb'",
    "description": "Client for 'OrderlyWeb' for use from the 'orderly1'\npackage.  Allows downloading of reports, running remote reports\nand other interaction with the remote report repository.",
    "version": "0.1.16",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "https://github.com/vimc/orderlyweb",
    "bug_reports": "https://github.com/vimc/orderlyweb/issues",
    "repository": "",
    "exports": [
      [
        "orderlyweb"
      ],
      [
        "orderlyweb_api_client"
      ],
      [
        "orderlyweb_remote"
      ]
    ],
    "topics": [],
    "score": 2.7782,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "orderlyweb Orderly Support for 'OrderlyWeb' Client for 'OrderlyWeb' for use from the 'orderly1'\npackage.  Allows downloading of reports, running remote reports\nand other interaction with the remote report repository. orderlyweb orderlyweb_api_client orderlyweb_remote "
  },
  {
    "id": 456,
    "package_name": "defer",
    "title": "Defer Errors",
    "description": "Create errors that can be deferred until later.  With\ndeferrable errors, collections of errors that are not\nimmediately fatal can be collected and reported back at once.",
    "version": "0.1.0",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "https://github.com/mrc-ide/odin",
    "bug_reports": "https://github.com/mrc-ide/odin/issues",
    "repository": "",
    "exports": [
      [
        "defer_errors"
      ],
      [
        "deferrable_error"
      ],
      [
        "deferred_errors_flush"
      ]
    ],
    "topics": [],
    "score": 2.7324,
    "stars": 9,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "defer Defer Errors Create errors that can be deferred until later.  With\ndeferrable errors, collections of errors that are not\nimmediately fatal can be collected and reported back at once. defer_errors deferrable_error deferred_errors_flush "
  },
  {
    "id": 1319,
    "package_name": "testthat.buildkite",
    "title": "A testthat reporter for buildkite",
    "description": "A testthat reporter that prints progress output in a\nformat for use in buildkite logs.",
    "version": "0.0.1",
    "maintainer": "Robert Ashton <r.ashton@imperial.ac.uk>",
    "author": "Robert Ashton [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "BuildkiteReporter"
      ]
    ],
    "topics": [],
    "score": 2.5441,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "testthat.buildkite A testthat reporter for buildkite A testthat reporter that prints progress output in a\nformat for use in buildkite logs. BuildkiteReporter "
  },
  {
    "id": 11,
    "package_name": "BioCro",
    "title": "Modular Crop Growth Simulations",
    "description": "A cross-platform representation of models as sets of equations\n    that facilitates modularity in model building and allows users to harness\n    modern techniques for numerical integration and data visualization.\n    Documentation is provided by several vignettes included in this package;\n    also see Lochocki et al. (2022) <doi:10.1093/insilicoplants/diac003>.",
    "version": "3.3.0",
    "maintainer": "Justin M. McGrath <jmcgrath@illinois.edu>",
    "author": "Justin M. McGrath [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-7025-3906>),\n  Edward B. Lochocki [aut] (ORCID:\n    <https://orcid.org/0000-0002-4912-9783>),\n  Yufeng He [aut] (ORCID: <https://orcid.org/0000-0001-9895-1880>),\n  Scott W. Oswald [aut] (ORCID: <https://orcid.org/0000-0002-1906-0340>),\n  Scott Rohde [aut] (ORCID: <https://orcid.org/0000-0001-9030-0936>),\n  Deepak Jaiswal [aut] (ORCID: <https://orcid.org/0000-0002-4077-3919>),\n  Megan L. Matthews [aut] (ORCID:\n    <https://orcid.org/0000-0002-5513-9320>),\n  Fernando E. Miguez [aut] (ORCID:\n    <https://orcid.org/0000-0002-4627-8329>),\n  Stephen P. Long [aut] (ORCID: <https://orcid.org/0000-0002-8501-7164>),\n  Dan Wang [ctb],\n  David LeBauer [ctb] (ORCID: <https://orcid.org/0000-0001-7228-053X>),\n  BioCro authors [cph],\n  Boost Organization [cph] (Copyright holder of included Boost library)",
    "url": "https://github.com/biocro/biocro, https://biocro.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BioCro",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BioCro Modular Crop Growth Simulations A cross-platform representation of models as sets of equations\n    that facilitates modularity in model building and allows users to harness\n    modern techniques for numerical integration and data visualization.\n    Documentation is provided by several vignettes included in this package;\n    also see Lochocki et al. (2022) <doi:10.1093/insilicoplants/diac003>.  "
  },
  {
    "id": 30,
    "package_name": "DOIcreator",
    "title": "Append DOIs to References in Word Documents",
    "description": "Read 'Word' documents containing bibliographic references, search for corresponding DOIs using the 'Crossref' API, and append the retrieved DOIs directly to the references. Supports parallel processing for faster retrieval and produces a new 'Word' document with numbered references including DOIs.",
    "version": "0.1.0",
    "maintainer": "Umar Hussain <drumarhussain@gmail.com>",
    "author": "Umar Hussain [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DOIcreator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DOIcreator Append DOIs to References in Word Documents Read 'Word' documents containing bibliographic references, search for corresponding DOIs using the 'Crossref' API, and append the retrieved DOIs directly to the references. Supports parallel processing for faster retrieval and produces a new 'Word' document with numbered references including DOIs.  "
  },
  {
    "id": 87,
    "package_name": "MOTE",
    "title": "Effect Size and Confidence Interval Calculator",
    "description": "Measure of the Effect ('MOTE') is an effect size calculator, including a \n    wide variety of effect sizes in the mean differences family (all versions of d) and \n    the variance overlap family (eta, omega, epsilon, r). 'MOTE' provides non-central \n    confidence intervals for each effect size, relevant test statistics, and output \n    for reporting in APA Style (American Psychological Association, 2010, \n    <ISBN:1433805618>) with 'LaTeX'. In research, an over-reliance on p-values \n    may conceal the fact that a study is under-powered (Halsey, Curran-Everett, \n    Vowler, & Drummond, 2015 <doi:10.1038/nmeth.3288>). A test may be statistically \n    significant, yet practically inconsequential (Fritz, Scherndl, & K\u00fchberger, 2012 \n    <doi:10.1177/0959354312436870>). Although the American Psychological Association \n    has long advocated for the inclusion of effect sizes (Wilkinson & American \n    Psychological Association Task Force on Statistical Inference, 1999 \n    <doi:10.1037/0003-066X.54.8.594>), the vast majority of peer-reviewed, \n    published academic studies stop short of reporting effect sizes and confidence \n    intervals (Cumming, 2013, <doi:10.1177/0956797613504966>). 'MOTE' simplifies \n    the use and interpretation of effect sizes and confidence intervals. ",
    "version": "1.2.2",
    "maintainer": "Erin M. Buchanan <buchananlab@gmail.com>",
    "author": "Erin M. Buchanan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9689-4189>),\n  Amber M Gillenwaters [aut] (ORCID:\n    <https://orcid.org/0000-0002-7580-3591>),\n  John E. Scofield [aut] (ORCID: <https://orcid.org/0000-0001-6345-1181>),\n  K. D. Valentine [aut] (ORCID: <https://orcid.org/0000-0001-6349-5395>)",
    "url": "https://github.com/doomlab/MOTE",
    "bug_reports": "https://github.com/doomlab/MOTE/issues",
    "repository": "https://cran.r-project.org/package=MOTE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MOTE Effect Size and Confidence Interval Calculator Measure of the Effect ('MOTE') is an effect size calculator, including a \n    wide variety of effect sizes in the mean differences family (all versions of d) and \n    the variance overlap family (eta, omega, epsilon, r). 'MOTE' provides non-central \n    confidence intervals for each effect size, relevant test statistics, and output \n    for reporting in APA Style (American Psychological Association, 2010, \n    <ISBN:1433805618>) with 'LaTeX'. In research, an over-reliance on p-values \n    may conceal the fact that a study is under-powered (Halsey, Curran-Everett, \n    Vowler, & Drummond, 2015 <doi:10.1038/nmeth.3288>). A test may be statistically \n    significant, yet practically inconsequential (Fritz, Scherndl, & K\u00fchberger, 2012 \n    <doi:10.1177/0959354312436870>). Although the American Psychological Association \n    has long advocated for the inclusion of effect sizes (Wilkinson & American \n    Psychological Association Task Force on Statistical Inference, 1999 \n    <doi:10.1037/0003-066X.54.8.594>), the vast majority of peer-reviewed, \n    published academic studies stop short of reporting effect sizes and confidence \n    intervals (Cumming, 2013, <doi:10.1177/0956797613504966>). 'MOTE' simplifies \n    the use and interpretation of effect sizes and confidence intervals.   "
  },
  {
    "id": 98,
    "package_name": "OhdsiReportGenerator",
    "title": "Observational Health Data Sciences and Informatics Report\nGenerator",
    "description": "Extract results into R from the Observational Health Data Sciences and Informatics result database (see <https://ohdsi.github.io/Strategus/results-schema/index.html>) and generate reports/presentations via 'quarto' that summarize results in HTML format. Learn more about 'OhdsiReportGenerator' at <https://ohdsi.github.io/OhdsiReportGenerator/>.",
    "version": "2.0.2",
    "maintainer": "Jenna Reps <jreps@its.jnj.com>",
    "author": "Jenna Reps [aut, cre],\n  Anthony Sena [aut]",
    "url": "https://ohdsi.github.io/OhdsiReportGenerator/,\nhttps://github.com/OHDSI/OhdsiReportGenerator",
    "bug_reports": "https://github.com/OHDSI/OhdsiReportGenerator/issues",
    "repository": "https://cran.r-project.org/package=OhdsiReportGenerator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OhdsiReportGenerator Observational Health Data Sciences and Informatics Report\nGenerator Extract results into R from the Observational Health Data Sciences and Informatics result database (see <https://ohdsi.github.io/Strategus/results-schema/index.html>) and generate reports/presentations via 'quarto' that summarize results in HTML format. Learn more about 'OhdsiReportGenerator' at <https://ohdsi.github.io/OhdsiReportGenerator/>.  "
  },
  {
    "id": 107,
    "package_name": "PMwR",
    "title": "Portfolio Management with R",
    "description": "Tools for the practical management of financial\n  portfolios: backtesting investment and trading strategies,\n  computing profit/loss and returns, analysing trades,\n  handling lists of transactions, reporting, and more.  The\n  package provides a small set of reliable, efficient and\n  convenient tools for processing and analysing\n  trade/portfolio data.  The manual provides all the details;\n  it is available from\n  <https://enricoschumann.net/R/packages/PMwR/manual/PMwR.html>.\n  Examples and descriptions of new features are provided at\n  <https://enricoschumann.net/notes/PMwR/>.",
    "version": "1.2-0",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "https://enricoschumann.net/PMwR/ ,\nhttps://git.sr.ht/~enricoschumann/PMwR ,\nhttps://gitlab.com/enricoschumann/PMwR ,\nhttps://github.com/enricoschumann/PMwR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PMwR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PMwR Portfolio Management with R Tools for the practical management of financial\n  portfolios: backtesting investment and trading strategies,\n  computing profit/loss and returns, analysing trades,\n  handling lists of transactions, reporting, and more.  The\n  package provides a small set of reliable, efficient and\n  convenient tools for processing and analysing\n  trade/portfolio data.  The manual provides all the details;\n  it is available from\n  <https://enricoschumann.net/R/packages/PMwR/manual/PMwR.html>.\n  Examples and descriptions of new features are provided at\n  <https://enricoschumann.net/notes/PMwR/>.  "
  },
  {
    "id": 160,
    "package_name": "Rcpp",
    "title": "Seamless R and C++ Integration",
    "description": "The 'Rcpp' package provides R functions as well as C++ classes which\n offer a seamless integration of R and C++. Many R data types and objects can be\n mapped back and forth to C++ equivalents which facilitates both writing of new\n code as well as easier integration of third-party libraries. Documentation\n about 'Rcpp' is provided by several vignettes included in this package, via the\n 'Rcpp Gallery' site at <https://gallery.rcpp.org>, the paper by Eddelbuettel and\n Francois (2011, <doi:10.18637/jss.v040.i08>), the book by Eddelbuettel (2013,\n <doi:10.1007/978-1-4614-6868-4>) and the paper by Eddelbuettel and Balamuta (2018,\n <doi:10.1080/00031305.2017.1375990>); see 'citation(\"Rcpp\")' for details.",
    "version": "1.1.0",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Romain Francois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  JJ Allaire [aut] (ORCID: <https://orcid.org/0000-0003-0174-9868>),\n  Kevin Ushey [aut] (ORCID: <https://orcid.org/0000-0003-2880-7407>),\n  Qiang Kou [aut] (ORCID: <https://orcid.org/0000-0001-6786-5453>),\n  Nathan Russell [aut],\n  I\u00f1aki Ucar [aut] (ORCID: <https://orcid.org/0000-0001-6403-5550>),\n  Doug Bates [aut] (ORCID: <https://orcid.org/0000-0001-8316-9503>),\n  John Chambers [aut]",
    "url": "https://www.rcpp.org,\nhttps://dirk.eddelbuettel.com/code/rcpp.html,\nhttps://github.com/RcppCore/Rcpp",
    "bug_reports": "https://github.com/RcppCore/Rcpp/issues",
    "repository": "https://cran.r-project.org/package=Rcpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rcpp Seamless R and C++ Integration The 'Rcpp' package provides R functions as well as C++ classes which\n offer a seamless integration of R and C++. Many R data types and objects can be\n mapped back and forth to C++ equivalents which facilitates both writing of new\n code as well as easier integration of third-party libraries. Documentation\n about 'Rcpp' is provided by several vignettes included in this package, via the\n 'Rcpp Gallery' site at <https://gallery.rcpp.org>, the paper by Eddelbuettel and\n Francois (2011, <doi:10.18637/jss.v040.i08>), the book by Eddelbuettel (2013,\n <doi:10.1007/978-1-4614-6868-4>) and the paper by Eddelbuettel and Balamuta (2018,\n <doi:10.1080/00031305.2017.1375990>); see 'citation(\"Rcpp\")' for details.  "
  },
  {
    "id": 295,
    "package_name": "biodosetools",
    "title": "'shiny' Application for Biological Dosimetry",
    "description": "A tool to perform all different statistical tests and calculations\n    needed by Biological dosimetry Laboratories. Detailed documentation is\n    available in <https://biodosetools-team.github.io/documentation/>.",
    "version": "3.7.2",
    "maintainer": "Anna Frances-Abellan <anna.frances@uab.cat>",
    "author": "Alfredo Hern\u00e1ndez [aut] (ORCID:\n    <https://orcid.org/0000-0002-2660-4545>),\n  Anna Frances-Abellan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4738-1712>),\n  David Endesfelder [aut],\n  Pere Puig [aut] (ORCID: <https://orcid.org/0000-0002-6607-9642>)",
    "url": "https://biodosetools-team.github.io/biodosetools/,\nhttps://github.com/biodosetools-team/biodosetools/",
    "bug_reports": "https://github.com/biodosetools-team/biodosetools/issues/",
    "repository": "https://cran.r-project.org/package=biodosetools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biodosetools 'shiny' Application for Biological Dosimetry A tool to perform all different statistical tests and calculations\n    needed by Biological dosimetry Laboratories. Detailed documentation is\n    available in <https://biodosetools-team.github.io/documentation/>.  "
  },
  {
    "id": 314,
    "package_name": "bpvars",
    "title": "Forecasting with Bayesian Panel Vector Autoregressions",
    "description": "\n  Provides Bayesian estimation and forecasting of dynamic panel data using \n  Bayesian Panel Vector Autoregressions with hierarchical prior distributions. \n  The models include country-specific VARs that share a global prior \n  distribution that extend the model by Jaroci\u0144ski (2010) <doi:10.1002/jae.1082>. \n  Under this prior expected value, each country's system follows \n  a global VAR with country-invariant parameters. Further flexibility is \n  provided by the hierarchical prior structure that retains the Minnesota prior \n  interpretation for the global VAR and features estimated prior covariance \n  matrices, shrinkage, and persistence levels. Bayesian forecasting is developed \n  for models including exogenous variables, allowing conditional forecasts given \n  the future trajectories of some variables and restricted forecasts assuring \n  that rates are forecasted to stay positive and less than 100. The package \n  implements the model specification, estimation, and forecasting routines, \n  facilitating coherent workflows and reproducibility. It also includes automated\n  pseudo-out-of-sample forecasting and computation of forecasting performance \n  measures. Beautiful plots, \n  informative summary functions, and extensive documentation complement all \n  this. An extraordinary computational speed is achieved thanks to employing \n  frontier econometric and numerical techniques and algorithms written in 'C++'. \n  The 'bpvars' package is aligned regarding objects, workflows, and code \n  structure with the 'R' packages 'bsvars' by \n  Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars> and 'bsvarSIGNs' by \n  Wang & Wo\u017aniak (2025) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they \n  constitute an integrated toolset. Copyright: 2025 International Labour \n  Organization.",
    "version": "1.0",
    "maintainer": "Tomasz Wo\u017aniak <wozniak.tom@pm.me>",
    "author": "Tomasz Wo\u017aniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2212-2378>),\n  Miguel Sanchez-Martinez [ctb],\n  International Labour Organization [cph]",
    "url": "https://bsvars.org/bpvars/",
    "bug_reports": "https://github.com/bsvars/bpvars/issues",
    "repository": "https://cran.r-project.org/package=bpvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bpvars Forecasting with Bayesian Panel Vector Autoregressions \n  Provides Bayesian estimation and forecasting of dynamic panel data using \n  Bayesian Panel Vector Autoregressions with hierarchical prior distributions. \n  The models include country-specific VARs that share a global prior \n  distribution that extend the model by Jaroci\u0144ski (2010) <doi:10.1002/jae.1082>. \n  Under this prior expected value, each country's system follows \n  a global VAR with country-invariant parameters. Further flexibility is \n  provided by the hierarchical prior structure that retains the Minnesota prior \n  interpretation for the global VAR and features estimated prior covariance \n  matrices, shrinkage, and persistence levels. Bayesian forecasting is developed \n  for models including exogenous variables, allowing conditional forecasts given \n  the future trajectories of some variables and restricted forecasts assuring \n  that rates are forecasted to stay positive and less than 100. The package \n  implements the model specification, estimation, and forecasting routines, \n  facilitating coherent workflows and reproducibility. It also includes automated\n  pseudo-out-of-sample forecasting and computation of forecasting performance \n  measures. Beautiful plots, \n  informative summary functions, and extensive documentation complement all \n  this. An extraordinary computational speed is achieved thanks to employing \n  frontier econometric and numerical techniques and algorithms written in 'C++'. \n  The 'bpvars' package is aligned regarding objects, workflows, and code \n  structure with the 'R' packages 'bsvars' by \n  Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars> and 'bsvarSIGNs' by \n  Wang & Wo\u017aniak (2025) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they \n  constitute an integrated toolset. Copyright: 2025 International Labour \n  Organization.  "
  },
  {
    "id": 399,
    "package_name": "contentanalysis",
    "title": "Scientific Content and Citation Analysis from PDF Documents",
    "description": "Provides comprehensive tools for extracting and analyzing scientific \n    content from PDF documents, including citation extraction, reference matching, \n    text analysis, and bibliometric indicators. Supports multi-column PDF layouts,\n    'CrossRef' API <https://www.crossref.org/documentation/retrieve-metadata/rest-api/> integration, and advanced citation parsing.",
    "version": "0.2.1",
    "maintainer": "Massimo Aria <aria@unina.it>",
    "author": "Massimo Aria [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-8517-9411>),\n  Corrado Cuccurullo [aut] (ORCID:\n    <https://orcid.org/0000-0002-7401-8575>)",
    "url": "https://github.com/massimoaria/contentanalysis,",
    "bug_reports": "https://github.com/massimoaria/contentanalysis/issues",
    "repository": "https://cran.r-project.org/package=contentanalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "contentanalysis Scientific Content and Citation Analysis from PDF Documents Provides comprehensive tools for extracting and analyzing scientific \n    content from PDF documents, including citation extraction, reference matching, \n    text analysis, and bibliometric indicators. Supports multi-column PDF layouts,\n    'CrossRef' API <https://www.crossref.org/documentation/retrieve-metadata/rest-api/> integration, and advanced citation parsing.  "
  },
  {
    "id": 430,
    "package_name": "customknitrender",
    "title": "Easily Switch Output Format of 'Rmarkdown' Files with Shared\nFrontmatter",
    "description": "Define the output format of 'rmarkdown' files\n  with shared output 'yaml' frontmatter content. Rather than modifying\n  a shared 'yaml' file, use integers to easily switch output formats\n  for 'rmarkdown' files.",
    "version": "1.0.2",
    "maintainer": "Luis Martinez <lmmartinez3312@gmail.com>",
    "author": "Luis Martinez [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=customknitrender",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "customknitrender Easily Switch Output Format of 'Rmarkdown' Files with Shared\nFrontmatter Define the output format of 'rmarkdown' files\n  with shared output 'yaml' frontmatter content. Rather than modifying\n  a shared 'yaml' file, use integers to easily switch output formats\n  for 'rmarkdown' files.  "
  },
  {
    "id": 457,
    "package_name": "defineR",
    "title": "Creates Define XML Documents",
    "description": "Creates 'define.xml' documents used for regulatory \n    submissions based on spreadsheet metadata.  Can also help\n    create metadata and generate HTML data explorer. ",
    "version": "0.0.5",
    "maintainer": "David Bosak <dbosak01@gmail.com>",
    "author": "David Bosak [aut, cre],\n  Aheer Alvi [aut],\n  Jonathan Stallings [ctb]",
    "url": "https://defineR.r-sassy.org, https://github.com/dbosak01/defineR",
    "bug_reports": "https://github.com/dbosak01/defineR/issues",
    "repository": "https://cran.r-project.org/package=defineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "defineR Creates Define XML Documents Creates 'define.xml' documents used for regulatory \n    submissions based on spreadsheet metadata.  Can also help\n    create metadata and generate HTML data explorer.   "
  },
  {
    "id": 545,
    "package_name": "examly",
    "title": "Statistical Metrics and Reporting Tool",
    "description": "A 'Shiny'-based toolkit for item/test analysis. It is designed for multiple-choice, true-false, and open-ended questions. The toolkit is usable with datasets in 1-0 or other formats. Key analyses include difficulty, discrimination, response-option analysis, reports. The classical test theory methods used are described in Ebel & Frisbie (1991, ISBN:978-0132892314).",
    "version": "0.2.0",
    "maintainer": "Ahmet \u00c7al\u0131\u015fkan <ahmetcaliskan1987@gmail.com>",
    "author": "Ahmet \u00c7al\u0131\u015fkan [aut, cre],\n  Abdullah Faruk K\u0131l\u0131\u00e7 [aut]",
    "url": "https://github.com/ahmetcaliskan1987/examly",
    "bug_reports": "https://github.com/ahmetcaliskan1987/examly/issues",
    "repository": "https://cran.r-project.org/package=examly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "examly Statistical Metrics and Reporting Tool A 'Shiny'-based toolkit for item/test analysis. It is designed for multiple-choice, true-false, and open-ended questions. The toolkit is usable with datasets in 1-0 or other formats. Key analyses include difficulty, discrimination, response-option analysis, reports. The classical test theory methods used are described in Ebel & Frisbie (1991, ISBN:978-0132892314).  "
  },
  {
    "id": 616,
    "package_name": "geoutils",
    "title": "Geographic metadata for unifying multi-source data",
    "description": "Contains datasets with metadata for geographic entities (continents, WHO regions, countries/territories, etc.) with specified standards for codes and names, used for unifying data reported from multiple sources.",
    "version": "0.0.0.9000",
    "maintainer": "",
    "author": "",
    "url": "https://github.com/WorldHealthOrganization/geoutils",
    "bug_reports": "https://github.com/WorldHealthOrganization/geoutils/issues",
    "repository": "https://github.com/WorldHealthOrganization/geoutils",
    "exports": [],
    "topics": [
      "eios",
      "geography",
      "r"
    ],
    "score": "NA",
    "stars": 4,
    "primary_category": "epidemiology",
    "source_universe": "github:WorldHealthOrganization",
    "search_text": "geoutils Geographic metadata for unifying multi-source data Contains datasets with metadata for geographic entities (continents, WHO regions, countries/territories, etc.) with specified standards for codes and names, used for unifying data reported from multiple sources.  eios geography r"
  },
  {
    "id": 687,
    "package_name": "hitRcovid",
    "title": "Loads, filters, and analyzes the HIT-COVID database",
    "description": "Loads the HIT-COVID database and contains functions to filter and visualize the intervention data and pair it with epidemiological information. The Health Intervention Tracking for COVID-19 (HIT-COVID) project tracks the implementation and relaxation of public health and social measures (PHSMs) taken by governments to slow transmission of SARS-COV-2 globally. Each change in policy and corresponding date is documented at the first-level  administrative unit (e.g., states, districts) and nationally for all countries with more detailed  geographic resolution in some locations (e.g., counties in the US). For a detailed description of the methods see Zheng et al. (2020) <doi:10.1038/s41597-020-00610-2>.",
    "version": "0.0.0.9000",
    "maintainer": "Andrew Azman <azman@jhu.edu>",
    "author": "",
    "url": "https://github.com/HopkinsIDD/hitRcovid",
    "bug_reports": "https://github.com/HopkinsIDD/hitRcovid/issues",
    "repository": "https://github.com/HopkinsIDD/hitRcovid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 2,
    "primary_category": "epidemiology",
    "source_universe": "github:HopkinsIDD",
    "search_text": "hitRcovid Loads, filters, and analyzes the HIT-COVID database Loads the HIT-COVID database and contains functions to filter and visualize the intervention data and pair it with epidemiological information. The Health Intervention Tracking for COVID-19 (HIT-COVID) project tracks the implementation and relaxation of public health and social measures (PHSMs) taken by governments to slow transmission of SARS-COV-2 globally. Each change in policy and corresponding date is documented at the first-level  administrative unit (e.g., states, districts) and nationally for all countries with more detailed  geographic resolution in some locations (e.g., counties in the US). For a detailed description of the methods see Zheng et al. (2020) <doi:10.1038/s41597-020-00610-2>.  "
  },
  {
    "id": 713,
    "package_name": "interfacer",
    "title": "Define and Enforce Contracts for Dataframes as Function\nParameters",
    "description": "A dataframe validation framework for package builders who use\n  dataframes as function parameters. It performs checks on column names, coerces\n  data-types, and checks grouping to make sure user inputs conform to a\n  specification provided by the package author. It provides a mechanism for\n  package authors to automatically document supported dataframe inputs and\n  selectively dispatch to functions depending on the format of a dataframe much\n  like S3 does for classes. It also contains some developer tools to make\n  working with and documenting dataframe specifications easier. It helps package\n  developers to improve their documentation and simplifies parameter validation\n  where dataframes are used as function parameters.",
    "version": "0.4.0",
    "maintainer": "Robert Challen <rob.challen@bristol.ac.uk>",
    "author": "Robert Challen [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5504-7768>)",
    "url": "https://ai4ci.github.io/interfacer/,\nhttps://github.com/ai4ci/interfacer",
    "bug_reports": "https://github.com/ai4ci/interfacer/issues",
    "repository": "https://cran.r-project.org/package=interfacer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "interfacer Define and Enforce Contracts for Dataframes as Function\nParameters A dataframe validation framework for package builders who use\n  dataframes as function parameters. It performs checks on column names, coerces\n  data-types, and checks grouping to make sure user inputs conform to a\n  specification provided by the package author. It provides a mechanism for\n  package authors to automatically document supported dataframe inputs and\n  selectively dispatch to functions depending on the format of a dataframe much\n  like S3 does for classes. It also contains some developer tools to make\n  working with and documenting dataframe specifications easier. It helps package\n  developers to improve their documentation and simplifies parameter validation\n  where dataframes are used as function parameters.  "
  },
  {
    "id": 819,
    "package_name": "mixAR",
    "title": "Mixture Autoregressive Models",
    "description": "Model time series using mixture autoregressive (MAR)\n             models.  Implemented are frequentist (EM) and Bayesian\n             methods for estimation, prediction and model\n             evaluation. See Wong and Li (2002)\n             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)\n             <doi:10.1016/j.spl.2009.04.009>), and the extensive\n             references in the documentation.",
    "version": "0.22.9",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Georgi N. Boshnakov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>),\n  Davide Ravagli [aut] (ORCID: <https://orcid.org/0000-0001-7146-7685>)",
    "url": "https://geobosh.github.io/mixAR/ (doc),\nhttps://CRAN.R-project.org/package=mixAR/",
    "bug_reports": "https://github.com/GeoBosh/mixAR/issues",
    "repository": "https://cran.r-project.org/package=mixAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixAR Mixture Autoregressive Models Model time series using mixture autoregressive (MAR)\n             models.  Implemented are frequentist (EM) and Bayesian\n             methods for estimation, prediction and model\n             evaluation. See Wong and Li (2002)\n             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)\n             <doi:10.1016/j.spl.2009.04.009>), and the extensive\n             references in the documentation.  "
  },
  {
    "id": 851,
    "package_name": "mrgvalidate",
    "title": "Create Validation Docs For R Packages",
    "description": "R package for generating validation documents for other R packages developed by Metrum.",
    "version": "2.0.0",
    "maintainer": "",
    "author": "",
    "url": "https://github.com/metrumresearchgroup/mrgvalidate",
    "bug_reports": "https://github.com/metrumresearchgroup/mrgvalidate/issues",
    "repository": "https://github.com/metrumresearchgroup/mrgvalidate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 1,
    "primary_category": "pharmacometrics",
    "source_universe": "github:metrumresearchgroup",
    "search_text": "mrgvalidate Create Validation Docs For R Packages R package for generating validation documents for other R packages developed by Metrum.  "
  },
  {
    "id": 877,
    "package_name": "nhlscraper",
    "title": "Scraper for National Hockey League Data",
    "description": "Scrapes and cleans data from the 'NHL' and 'ESPN' APIs into data.frames and lists. Wraps 125+ endpoints documented in <https://github.com/RentoSaijo/nhlscraper/wiki> from high-level multi-season summaries and award winners to low-level decisecond replays and bookmakers' odds, making them more accessible. Features cleaning and visualization tools, primarily for play-by-plays.",
    "version": "0.4.1",
    "maintainer": "Rento Saijo <rentosaijo0527@gmail.com>",
    "author": "Rento Saijo [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0008-4919-7349>),\n  Lars Skytte [ctb]",
    "url": "https://rentosaijo.github.io/nhlscraper/,\nhttps://github.com/RentoSaijo/nhlscraper",
    "bug_reports": "https://github.com/RentoSaijo/nhlscraper/issues",
    "repository": "https://cran.r-project.org/package=nhlscraper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nhlscraper Scraper for National Hockey League Data Scrapes and cleans data from the 'NHL' and 'ESPN' APIs into data.frames and lists. Wraps 125+ endpoints documented in <https://github.com/RentoSaijo/nhlscraper/wiki> from high-level multi-season summaries and award winners to low-level decisecond replays and bookmakers' odds, making them more accessible. Features cleaning and visualization tools, primarily for play-by-plays.  "
  },
  {
    "id": 938,
    "package_name": "pakret",
    "title": "Cite 'R' Packages on the Fly in 'R Markdown' and 'Quarto'",
    "description": "References and cites 'R' and 'R' packages on the fly in 'R\n    Markdown' and 'Quarto'. 'pakret' provides a minimalist API that\n    generates preformatted citations for 'R' and 'R' packages, and adds\n    their references to a '.bib' file directly from within your document.",
    "version": "0.3.0",
    "maintainer": "Arnaud Gallou <arangacas@gmail.com>",
    "author": "Arnaud Gallou [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1002-4247>)",
    "url": "https://arnaudgallou.github.io/pakret/,\nhttps://github.com/arnaudgallou/pakret",
    "bug_reports": "https://github.com/arnaudgallou/pakret/issues",
    "repository": "https://cran.r-project.org/package=pakret",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pakret Cite 'R' Packages on the Fly in 'R Markdown' and 'Quarto' References and cites 'R' and 'R' packages on the fly in 'R\n    Markdown' and 'Quarto'. 'pakret' provides a minimalist API that\n    generates preformatted citations for 'R' and 'R' packages, and adds\n    their references to a '.bib' file directly from within your document.  "
  },
  {
    "id": 994,
    "package_name": "pmparams",
    "title": "R package for parameter tables",
    "description": "`pmparams` is a library written in R that generates clear, well-formatted parameter tables to report NONMEM model results.",
    "version": "0.3.2",
    "maintainer": "",
    "author": "",
    "url": "https://github.com/metrumresearchgroup/pmparams",
    "bug_reports": "https://github.com/metrumresearchgroup/pmparams/issues",
    "repository": "https://github.com/metrumresearchgroup/pmparams",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 1,
    "primary_category": "pharmacometrics",
    "source_universe": "github:metrumresearchgroup",
    "search_text": "pmparams R package for parameter tables `pmparams` is a library written in R that generates clear, well-formatted parameter tables to report NONMEM model results.  "
  },
  {
    "id": 1034,
    "package_name": "qol",
    "title": "Powerful 'SAS' Inspired Concepts for more Efficient Bigger\nOutputs",
    "description": "The main goal is to make descriptive evaluations easier to create bigger and more complex outputs in less time with less code. Introducing format containers with multilabels <https://documentation.sas.com/doc/en/pgmsascdc/v_067/proc/p06ciqes4eaqo6n0zyqtz9p21nfb.htm>, a more powerful summarise which is capable to output every possible combination of the provided grouping variables in one go <https://documentation.sas.com/doc/en/pgmsascdc/v_067/proc/p0jvbbqkt0gs2cn1lo4zndbqs1pe.htm>, tabulation functions which can create any table in different styles <https://documentation.sas.com/doc/en/pgmsascdc/v_067/proc/n1ql5xnu0k3kdtn11gwa5hc7u435.htm> and other more readable functions. The code is optimized to work fast even with datasets of over a million observations.",
    "version": "1.1.1",
    "maintainer": "Tim Siebenmorgen <qol_package@proton.me>",
    "author": "Tim Siebenmorgen [aut, cre, cph]",
    "url": "https://github.com/s3rdia/qol, https://s3rdia.github.io/qol/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qol",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qol Powerful 'SAS' Inspired Concepts for more Efficient Bigger\nOutputs The main goal is to make descriptive evaluations easier to create bigger and more complex outputs in less time with less code. Introducing format containers with multilabels <https://documentation.sas.com/doc/en/pgmsascdc/v_067/proc/p06ciqes4eaqo6n0zyqtz9p21nfb.htm>, a more powerful summarise which is capable to output every possible combination of the provided grouping variables in one go <https://documentation.sas.com/doc/en/pgmsascdc/v_067/proc/p0jvbbqkt0gs2cn1lo4zndbqs1pe.htm>, tabulation functions which can create any table in different styles <https://documentation.sas.com/doc/en/pgmsascdc/v_067/proc/n1ql5xnu0k3kdtn11gwa5hc7u435.htm> and other more readable functions. The code is optimized to work fast even with datasets of over a million observations.  "
  },
  {
    "id": 1042,
    "package_name": "quanteda.tidy",
    "title": "'tidyverse' Extensions for 'quanteda'",
    "description": "Enables 'tidyverse' operations on 'quanteda' corpus objects by extending 'dplyr' verbs to work directly with corpus objects and their document-level variables ('docvars'). Implements row operations for 'subsetting' and reordering documents; column operations for managing document variables; grouped operations; and two-table verbs for merging external data. For more on 'quanteda' see 'Benoit et al.' (2018) <doi:10.21105/joss.00774>. For 'dplyr' see 'Wickham et al.' (2023) <doi:10.32614/CRAN.package.dplyr>.",
    "version": "0.4",
    "maintainer": "Kenneth Benoit <kbenoit@smu.edu.sg>",
    "author": "Kenneth Benoit [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quanteda.tidy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda.tidy 'tidyverse' Extensions for 'quanteda' Enables 'tidyverse' operations on 'quanteda' corpus objects by extending 'dplyr' verbs to work directly with corpus objects and their document-level variables ('docvars'). Implements row operations for 'subsetting' and reordering documents; column operations for managing document variables; grouped operations; and two-table verbs for merging external data. For more on 'quanteda' see 'Benoit et al.' (2018) <doi:10.21105/joss.00774>. For 'dplyr' see 'Wickham et al.' (2023) <doi:10.32614/CRAN.package.dplyr>.  "
  },
  {
    "id": 1162,
    "package_name": "rtables",
    "title": "Reporting Tables",
    "description": "Reporting tables often have structure that goes beyond simple\n    rectangular data. The 'rtables' package provides a framework for\n    declaring complex multi-level tabulations and then applying them to\n    data. This framework models both tabulation and the resulting tables\n    as hierarchical, tree-like objects which support sibling sub-tables,\n    arbitrary splitting or grouping of data in row and column dimensions,\n    cells containing multiple values, and the concept of contextual\n    summary computations. A convenient pipe-able interface is provided for\n    declaring table layouts and the corresponding computations, and then\n    applying them to data.",
    "version": "0.6.15",
    "maintainer": "Joe Zhu <joe.zhu@roche.com>",
    "author": "Gabriel Becker [aut] (Original creator of the package),\n  Adrian Waddell [aut],\n  Daniel Saban\u00e9s Bov\u00e9 [ctb],\n  Maximilian Mordig [ctb],\n  Davide Garolini [aut] (ORCID: <https://orcid.org/0000-0002-1445-1369>),\n  Emily de la Rua [aut] (ORCID: <https://orcid.org/0009-0000-8738-5561>),\n  Abinaya Yogasekaram [ctb] (ORCID:\n    <https://orcid.org/0009-0005-2083-1105>),\n  Joe Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0001-7566-2787>),\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/insightsengineering/rtables,\nhttps://insightsengineering.github.io/rtables/",
    "bug_reports": "https://github.com/insightsengineering/rtables/issues",
    "repository": "https://cran.r-project.org/package=rtables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtables Reporting Tables Reporting tables often have structure that goes beyond simple\n    rectangular data. The 'rtables' package provides a framework for\n    declaring complex multi-level tabulations and then applying them to\n    data. This framework models both tabulation and the resulting tables\n    as hierarchical, tree-like objects which support sibling sub-tables,\n    arbitrary splitting or grouping of data in row and column dimensions,\n    cells containing multiple values, and the concept of contextual\n    summary computations. A convenient pipe-able interface is provided for\n    declaring table layouts and the corresponding computations, and then\n    applying them to data.  "
  },
  {
    "id": 1177,
    "package_name": "sarima",
    "title": "Simulation and Prediction with Seasonal ARIMA Models",
    "description": "Functions, classes and methods for time series modelling with ARIMA\n    and related models. The aim of the package is to provide consistent\n    interface for the user. For example, a single function autocorrelations()\n    computes various kinds of theoretical and sample autocorrelations. This is\n    work in progress, see the documentation and vignettes for the current\n    functionality.  Function sarima() fits extended multiplicative seasonal\n    ARIMA models with trends, exogenous variables and arbitrary roots on the\n    unit circle, which can be fixed or estimated (for the algebraic basis for\n    this see <doi:10.48550/arXiv.2208.05055>, a paper on the methodology is being prepared).",
    "version": "0.9.5",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Georgi N. Boshnakov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>),\n  Jamie Halliday [aut]",
    "url": "https://geobosh.github.io/sarima/ (doc)\nhttps://github.com/GeoBosh/sarima (devel)",
    "bug_reports": "https://github.com/GeoBosh/sarima/issues",
    "repository": "https://cran.r-project.org/package=sarima",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sarima Simulation and Prediction with Seasonal ARIMA Models Functions, classes and methods for time series modelling with ARIMA\n    and related models. The aim of the package is to provide consistent\n    interface for the user. For example, a single function autocorrelations()\n    computes various kinds of theoretical and sample autocorrelations. This is\n    work in progress, see the documentation and vignettes for the current\n    functionality.  Function sarima() fits extended multiplicative seasonal\n    ARIMA models with trends, exogenous variables and arbitrary roots on the\n    unit circle, which can be fixed or estimated (for the algebraic basis for\n    this see <doi:10.48550/arXiv.2208.05055>, a paper on the methodology is being prepared).  "
  },
  {
    "id": 1223,
    "package_name": "simFastBOIN",
    "title": "Fast Bayesian Optimal Interval Design for Phase I Dose-Finding\nTrials",
    "description": "\n    Conducting Bayesian Optimal Interval (BOIN) design for phase I \n    dose-finding trials. 'simFastBOIN' provides functions for pre-computing \n    decision tables, conducting trial simulations, and evaluating operating \n    characteristics. The package uses vectorized operations and the \n    Iso::pava() function for isotonic regression to achieve efficient \n    performance while maintaining full compatibility with BOIN methodology. \n    Version 1.3.2 adds p_saf and p_tox parameters for customizable safety and \n    toxicity thresholds. Version 1.3.1 fixes Date field. Version 1.2.1 adds \n    comprehensive 'roxygen2' documentation and enhanced print formatting with \n    flexible table output options. Version 1.2.0 integrated C-based PAVA for \n    isotonic regression. Version 1.1.0 introduced conservative MTD selection \n    (boundMTD) and flexible early stopping rules (n_earlystop_rule). Methods \n    are described in Liu and Yuan (2015) <doi:10.1111/rssc.12089>.",
    "version": "1.3.2",
    "maintainer": "Gosuke Homma <my.name.is.gosuke@gmail.com>",
    "author": "Gosuke Homma [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6854-5627>)",
    "url": "https://github.com/gosukehommaEX/simFastBOIN",
    "bug_reports": "https://github.com/gosukehommaEX/simFastBOIN/issues",
    "repository": "https://cran.r-project.org/package=simFastBOIN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simFastBOIN Fast Bayesian Optimal Interval Design for Phase I Dose-Finding\nTrials \n    Conducting Bayesian Optimal Interval (BOIN) design for phase I \n    dose-finding trials. 'simFastBOIN' provides functions for pre-computing \n    decision tables, conducting trial simulations, and evaluating operating \n    characteristics. The package uses vectorized operations and the \n    Iso::pava() function for isotonic regression to achieve efficient \n    performance while maintaining full compatibility with BOIN methodology. \n    Version 1.3.2 adds p_saf and p_tox parameters for customizable safety and \n    toxicity thresholds. Version 1.3.1 fixes Date field. Version 1.2.1 adds \n    comprehensive 'roxygen2' documentation and enhanced print formatting with \n    flexible table output options. Version 1.2.0 integrated C-based PAVA for \n    isotonic regression. Version 1.1.0 introduced conservative MTD selection \n    (boundMTD) and flexible early stopping rules (n_earlystop_rule). Methods \n    are described in Liu and Yuan (2015) <doi:10.1111/rssc.12089>.  "
  },
  {
    "id": 1303,
    "package_name": "tatooheene",
    "title": "Technology Appraisal Toolbox for Health Economic Evaluations in\nthe Netherlands",
    "description": "Functions to support economic modelling in R based on the methods of the \n    Dutch guideline for economic evaluations in healthcare <https://www.zorginstituutnederland.nl/documenten/2024/01/16/richtlijn-voor-het-uitvoeren-van-economische-evaluaties-in-de-gezondheidszorg>, \n    CBS data <https://www.cbs.nl/>, and OECD data <https://www.oecd.org/en.html>.",
    "version": "1.0.0",
    "maintainer": "Stijn Peeters <s.b.peeters@eshpm.eur.nl>",
    "author": "Stijn Peeters [aut, cre] (ORCID:\n    <https://orcid.org/0009-0004-3684-3584>),\n  Eline Krijkamp [aut] (ORCID: <https://orcid.org/0000-0003-3970-2205>),\n  Frederick Thielen [aut] (ORCID:\n    <https://orcid.org/0000-0002-0312-5891>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tatooheene",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tatooheene Technology Appraisal Toolbox for Health Economic Evaluations in\nthe Netherlands Functions to support economic modelling in R based on the methods of the \n    Dutch guideline for economic evaluations in healthcare <https://www.zorginstituutnederland.nl/documenten/2024/01/16/richtlijn-voor-het-uitvoeren-van-economische-evaluaties-in-de-gezondheidszorg>, \n    CBS data <https://www.cbs.nl/>, and OECD data <https://www.oecd.org/en.html>.  "
  },
  {
    "id": 1323,
    "package_name": "textutils",
    "title": "Utilities for Handling Strings and Text",
    "description": "Utilities for handling character vectors\n  that store human-readable text (either plain or with\n  markup, such as HTML or LaTeX). The package provides,\n  in particular, functions that help with the\n  preparation of plain-text reports, e.g. for expanding\n  and aligning strings that form the lines of such\n  reports. The package also provides generic functions for\n  transforming R objects to HTML and to plain text.",
    "version": "0.4-3",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "https://enricoschumann.net/R/packages/textutils/ ,\nhttps://git.sr.ht/~enricoschumann/textutils ,\nhttps://github.com/enricoschumann/textutils",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textutils Utilities for Handling Strings and Text Utilities for handling character vectors\n  that store human-readable text (either plain or with\n  markup, such as HTML or LaTeX). The package provides,\n  in particular, functions that help with the\n  preparation of plain-text reports, e.g. for expanding\n  and aligning strings that form the lines of such\n  reports. The package also provides generic functions for\n  transforming R objects to HTML and to plain text.  "
  },
  {
    "id": 1367,
    "package_name": "toxpiR",
    "title": "Create ToxPi Prioritization Models",
    "description": "\n  Enables users to build 'ToxPi' prioritization models and provides \n  functionality within the grid framework for plotting ToxPi graphs.\n  'toxpiR' allows for more customization than the 'ToxPi GUI' \n  (<https://toxpi.github.io/>) and integration into existing workflows for greater \n  ease-of-use, reproducibility, and transparency.\n  toxpiR package behaves nearly identically to the GUI; the package \n  documentation includes notes about all differences.\n  The vignettes download example files from \n  <https://github.com/ToxPi/ToxPi-example-files>.",
    "version": "1.3.1",
    "maintainer": "Jonathon F Fleming <jffleming0129@gmail.com>",
    "author": "Jonathon F Fleming [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2447-3139>),\n  Dayne L Filer [aut, fnd] (ORCID:\n    <https://orcid.org/0000-0002-3443-5315>),\n  Dillon T Lloyd [aut],\n  Preethi Thunga [aut] (ORCID: <https://orcid.org/0000-0001-5447-0129>),\n  Skylar W Marvel [aut],\n  Alison A Motsinger-Reif [fnd] (ORCID:\n    <https://orcid.org/0000-0003-1346-2493>),\n  David M Reif [aut, fnd] (ORCID:\n    <https://orcid.org/0000-0001-7815-6767>)",
    "url": "https://github.com/ToxPi/toxpiR, https://toxpi.github.io/toxpiR/",
    "bug_reports": "https://github.com/ToxPi/toxpiR/issues",
    "repository": "https://cran.r-project.org/package=toxpiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "toxpiR Create ToxPi Prioritization Models \n  Enables users to build 'ToxPi' prioritization models and provides \n  functionality within the grid framework for plotting ToxPi graphs.\n  'toxpiR' allows for more customization than the 'ToxPi GUI' \n  (<https://toxpi.github.io/>) and integration into existing workflows for greater \n  ease-of-use, reproducibility, and transparency.\n  toxpiR package behaves nearly identically to the GUI; the package \n  documentation includes notes about all differences.\n  The vignettes download example files from \n  <https://github.com/ToxPi/ToxPi-example-files>.  "
  },
  {
    "id": 1407,
    "package_name": "valdr",
    "title": "Access and Analyse 'VALD' Data via Our External 'APIs'",
    "description": "Provides helper functions and wrappers to simplify authentication, \n    data retrieval, and result processing from the 'VALD' 'APIs'. \n    Designed to streamline integration for analysts and researchers working \n    with 'VALD's external 'APIs'.\n    For further documentation on integrating with 'VALD' 'APIs', see:\n    <https://support.vald.com/hc/en-au/articles/23415335574553-How-to-integrate-with-VALD-APIs>.\n    For a step-by-step guide to using this package, see:\n    <https://support.vald.com/hc/en-au/articles/48730811824281-A-guide-to-using-the-valdr-R-package>.    ",
    "version": "2.1.0",
    "maintainer": "Kieran Harrison <k.harrison@vald.com>",
    "author": "Kieran Harrison [aut, cre],\n  VALD Support [ctb],\n  VALD [cph] (Copyright holder)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=valdr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "valdr Access and Analyse 'VALD' Data via Our External 'APIs' Provides helper functions and wrappers to simplify authentication, \n    data retrieval, and result processing from the 'VALD' 'APIs'. \n    Designed to streamline integration for analysts and researchers working \n    with 'VALD's external 'APIs'.\n    For further documentation on integrating with 'VALD' 'APIs', see:\n    <https://support.vald.com/hc/en-au/articles/23415335574553-How-to-integrate-with-VALD-APIs>.\n    For a step-by-step guide to using this package, see:\n    <https://support.vald.com/hc/en-au/articles/48730811824281-A-guide-to-using-the-valdr-R-package>.      "
  },
  {
    "id": 1443,
    "package_name": "wintime",
    "title": "Win Time Methods for Time-to-Event Data in Clinical Trials",
    "description": "Performs an analysis of time-to-event clinical trial data using various \"win time\" methods, \n  including 'ewt', 'ewtr', 'rmt', 'ewtp', 'rewtp', 'ewtpr', 'rewtpr', 'max', 'wtr', 'rwtr', 'pwt', and 'rpwt'. These methods are used to calculate and compare \n  treatment effects on ordered composite endpoints. The package handles event times, event indicators, and treatment \n  arm indicators and supports calculations on observed and resampled data. Detailed explanations of each method and \n  usage examples are provided in \"Use of win time for ordered composite endpoints in clinical trials,\" by Troendle et al. \n  (2024)<https://pubmed.ncbi.nlm.nih.gov/38417455/>. For more information, see the package documentation or the vignette titled \"Introduction to wintime.\"",
    "version": "0.4.2",
    "maintainer": "James Troendle <james.troendle@nih.gov>",
    "author": "James Troendle [aut, cre],\n  Samuel Lawrence [aut]",
    "url": "https://pubmed.ncbi.nlm.nih.gov/38417455/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wintime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wintime Win Time Methods for Time-to-Event Data in Clinical Trials Performs an analysis of time-to-event clinical trial data using various \"win time\" methods, \n  including 'ewt', 'ewtr', 'rmt', 'ewtp', 'rewtp', 'ewtpr', 'rewtpr', 'max', 'wtr', 'rwtr', 'pwt', and 'rpwt'. These methods are used to calculate and compare \n  treatment effects on ordered composite endpoints. The package handles event times, event indicators, and treatment \n  arm indicators and supports calculations on observed and resampled data. Detailed explanations of each method and \n  usage examples are provided in \"Use of win time for ordered composite endpoints in clinical trials,\" by Troendle et al. \n  (2024)<https://pubmed.ncbi.nlm.nih.gov/38417455/>. For more information, see the package documentation or the vignette titled \"Introduction to wintime.\"  "
  },
  {
    "id": 1467,
    "package_name": "yaml12",
    "title": "Fast 'YAML' 1.2 Parser and Formatter",
    "description": "A fast, correct, safe, and ergonomic 'YAML' 1.2 parser and\n    generator written in 'Rust'. Convert between 'YAML' and simple 'R'\n    objects with full support for multi-document streams, tags, anchors,\n    and aliases. Offers opt-in handlers for custom tag behavior and\n    round-trips common 'R' data structures. Implements the 'YAML' 1.2.2\n    specification from the 'YAML' Language Development Team (2021)\n    <https://yaml.org/spec/1.2.2/>. Proudly supported by Posit.",
    "version": "0.1.0",
    "maintainer": "Tomasz Kalinowski <tomasz@posit.co>",
    "author": "Tomasz Kalinowski [aut, cre],\n  Posit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>),\n  Authors of the dependency Rust crates [cph] (See inst/AUTHORS and\n    LICENSE.note for vendored Rust dependency authors and licenses.)",
    "url": "https://posit-dev.github.io/r-yaml12/,\nhttps://github.com/posit-dev/r-yaml12",
    "bug_reports": "https://github.com/posit-dev/r-yaml12/issues",
    "repository": "https://cran.r-project.org/package=yaml12",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yaml12 Fast 'YAML' 1.2 Parser and Formatter A fast, correct, safe, and ergonomic 'YAML' 1.2 parser and\n    generator written in 'Rust'. Convert between 'YAML' and simple 'R'\n    objects with full support for multi-document streams, tags, anchors,\n    and aliases. Offers opt-in handlers for custom tag behavior and\n    round-trips common 'R' data structures. Implements the 'YAML' 1.2.2\n    specification from the 'YAML' Language Development Team (2021)\n    <https://yaml.org/spec/1.2.2/>. Proudly supported by Posit.  "
  },
  {
    "id": 1482,
    "package_name": "ABCDscores",
    "title": "Summary Scores of the Adolescent Brain Cognitive Development\n(ABCD) Study",
    "description": "Provides functions to compute summary scores\n  (besides proprietary ones) reported in the tabulated data resource that is\n  released by the Adolescent Brain Cognitive Development (ABCD) study.",
    "version": "6.1.0",
    "maintainer": "Le Zhang <dairc.service@gmail.com>",
    "author": "Le Zhang [aut, cre] (ORCID: <https://orcid.org/0009-0008-0205-2150>),\n  Janosch Linkersdoerfer [aut] (ORCID:\n    <https://orcid.org/0000-0002-1577-1233>),\n  Olivier Celhay [aut] (ORCID: <https://orcid.org/0000-0002-2971-9110>),\n  Biplabendu Das [aut] (ORCID: <https://orcid.org/0000-0003-0855-262X>),\n  Sammy Berman [aut] (ORCID: <https://orcid.org/0000-0002-9803-019X>),\n  Laura Ziemer [aut] (ORCID: <https://orcid.org/0000-0003-0026-3823>)",
    "url": "https://software.nbdc-datahub.org/ABCDscores/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ABCDscores",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ABCDscores Summary Scores of the Adolescent Brain Cognitive Development\n(ABCD) Study Provides functions to compute summary scores\n  (besides proprietary ones) reported in the tabulated data resource that is\n  released by the Adolescent Brain Cognitive Development (ABCD) study.  "
  },
  {
    "id": 1536,
    "package_name": "AIFFtools",
    "title": "Read AIFF Files and Convert to WAVE Format",
    "description": "Functions are provided to read and convert AIFF audio files to WAVE (WAV) format.  This supports, for example, use of the 'tuneR' package, which does not currently handle AIFF files.  The AIFF file format is defined in  <https://web.archive.org/web/20080125221040/http://www.borg.com/~jglatt/tech/aiff.htm> and   <https://www.mmsp.ece.mcgill.ca/Documents/AudioFormats/AIFF/Docs/AIFF-1.3.pdf> . ",
    "version": "1.0",
    "maintainer": "Carl Witthoft <cellocgw@gmail.com>",
    "author": "Carl Witthoft [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AIFFtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AIFFtools Read AIFF Files and Convert to WAVE Format Functions are provided to read and convert AIFF audio files to WAVE (WAV) format.  This supports, for example, use of the 'tuneR' package, which does not currently handle AIFF files.  The AIFF file format is defined in  <https://web.archive.org/web/20080125221040/http://www.borg.com/~jglatt/tech/aiff.htm> and   <https://www.mmsp.ece.mcgill.ca/Documents/AudioFormats/AIFF/Docs/AIFF-1.3.pdf> .   "
  },
  {
    "id": 1560,
    "package_name": "ANOVAIREVA",
    "title": "Interactive Document for Working with Analysis of Variance",
    "description": "An interactive document on  the topic of one-way and two-way analysis of variance using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at <https://tinyurl.com/ANOVAStatsTool>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ANOVAIREVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ANOVAIREVA Interactive Document for Working with Analysis of Variance An interactive document on  the topic of one-way and two-way analysis of variance using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at <https://tinyurl.com/ANOVAStatsTool>.  "
  },
  {
    "id": 1561,
    "package_name": "ANOVAShiny",
    "title": "Interactive Document for Working with Analysis of Variance",
    "description": "An interactive document on  the topic of one-way and two-way analysis of variance using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/ANOVAShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ANOVAShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ANOVAShiny Interactive Document for Working with Analysis of Variance An interactive document on  the topic of one-way and two-way analysis of variance using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/ANOVAShiny/>.  "
  },
  {
    "id": 1562,
    "package_name": "ANOVAShiny2",
    "title": "Interactive Document for Working with Analysis of Variance",
    "description": "An interactive document on  the topic of one-way and two-way analysis of variance using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/ANOVAShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ANOVAShiny2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ANOVAShiny2 Interactive Document for Working with Analysis of Variance An interactive document on  the topic of one-way and two-way analysis of variance using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/ANOVAShiny/>.  "
  },
  {
    "id": 1627,
    "package_name": "Achilles",
    "title": "Achilles Data Source Characterization",
    "description": "Automated Characterization of Health Information at Large-Scale \n  Longitudinal Evidence Systems. Creates a descriptive statistics summary for \n  an Observational Medical Outcomes Partnership Common Data Model standardized \n  data source. This package includes functions for executing summary queries on \n  the specified data source and exporting reporting content for use across a \n  variety of Observational Health Data Sciences and Informatics community \n  applications. ",
    "version": "1.7.2",
    "maintainer": "Frank DeFalco <fdefalco@ohdsi.org>",
    "author": "Frank DeFalco [aut, cre],\n  Patrick Ryan [aut],\n  Martijn Schuemie [aut],\n  Vojtech Huser [aut],\n  Chris Knoll [aut],\n  Ajit Londhe [aut],\n  Taha Abdul-Basser [aut],\n  Anthony Molinaro [aut],\n  Observational Health Data Science and Informatics [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Achilles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Achilles Achilles Data Source Characterization Automated Characterization of Health Information at Large-Scale \n  Longitudinal Evidence Systems. Creates a descriptive statistics summary for \n  an Observational Medical Outcomes Partnership Common Data Model standardized \n  data source. This package includes functions for executing summary queries on \n  the specified data source and exporting reporting content for use across a \n  variety of Observational Health Data Sciences and Informatics community \n  applications.   "
  },
  {
    "id": 1664,
    "package_name": "AirMonitor",
    "title": "Air Quality Data Analysis",
    "description": "Utilities for working with hourly air quality monitoring data\n    with a focus on small particulates (PM2.5). A compact data model is \n    structured as a list with two dataframes. A 'meta' dataframe contains \n    spatial and measuring device metadata associated with deployments at known \n    locations. A 'data' dataframe contains a 'datetime' column followed by \n    columns of measurements associated with each \"device-deployment\".\n    Algorithms to calculate NowCast and the associated Air Quality Index (AQI)\n    are defined at the US Environmental Projection Agency AirNow program:\n    <https://document.airnow.gov/technical-assistance-document-for-the-reporting-of-daily-air-quailty.pdf>.",
    "version": "0.4.3",
    "maintainer": "Jonathan Callahan <jonathan.s.callahan@gmail.com>",
    "author": "Jonathan Callahan [aut, cre],\n  Spencer Pease [ctb],\n  Hans Martin [ctb],\n  Rex Thompson [ctb]",
    "url": "https://github.com/MazamaScience/AirMonitor,\nhttps://mazamascience.github.io/AirMonitor/",
    "bug_reports": "https://github.com/MazamaScience/AirMonitor/issues",
    "repository": "https://cran.r-project.org/package=AirMonitor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AirMonitor Air Quality Data Analysis Utilities for working with hourly air quality monitoring data\n    with a focus on small particulates (PM2.5). A compact data model is \n    structured as a list with two dataframes. A 'meta' dataframe contains \n    spatial and measuring device metadata associated with deployments at known \n    locations. A 'data' dataframe contains a 'datetime' column followed by \n    columns of measurements associated with each \"device-deployment\".\n    Algorithms to calculate NowCast and the associated Air Quality Index (AQI)\n    are defined at the US Environmental Projection Agency AirNow program:\n    <https://document.airnow.gov/technical-assistance-document-for-the-reporting-of-daily-air-quailty.pdf>.  "
  },
  {
    "id": 1674,
    "package_name": "AlleleShift",
    "title": "Predict and Visualize Population-Level Changes in Allele\nFrequencies in Response to Climate Change",
    "description": "Methods (<doi:10.7717/peerj.11534>) are provided of calibrating and predicting shifts in allele frequencies through redundancy analysis ('vegan::rda()') and generalized additive models ('mgcv::gam()'). Visualization functions for predicted changes in allele frequencies include 'shift.dot.ggplot()', 'shift.pie.ggplot()', 'shift.moon.ggplot()', 'shift.waffle.ggplot()' and 'shift.surf.ggplot()' that are made with input data sets that are prepared by helper functions for each visualization method. Examples in the documentation show how to prepare animated climate change graphics through a time series with the 'gganimate' package. Function 'amova.rda()' shows how Analysis of Molecular Variance can be directly conducted with the results from redundancy analysis.",
    "version": "1.1-3",
    "maintainer": "Roeland Kindt <RoelandCEKindt@gmail.com>",
    "author": "Roeland Kindt [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-7672-0712>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AlleleShift",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AlleleShift Predict and Visualize Population-Level Changes in Allele\nFrequencies in Response to Climate Change Methods (<doi:10.7717/peerj.11534>) are provided of calibrating and predicting shifts in allele frequencies through redundancy analysis ('vegan::rda()') and generalized additive models ('mgcv::gam()'). Visualization functions for predicted changes in allele frequencies include 'shift.dot.ggplot()', 'shift.pie.ggplot()', 'shift.moon.ggplot()', 'shift.waffle.ggplot()' and 'shift.surf.ggplot()' that are made with input data sets that are prepared by helper functions for each visualization method. Examples in the documentation show how to prepare animated climate change graphics through a time series with the 'gganimate' package. Function 'amova.rda()' shows how Analysis of Molecular Variance can be directly conducted with the results from redundancy analysis.  "
  },
  {
    "id": 1679,
    "package_name": "AlteredPQR",
    "title": "Detection of Altered Protein Quantitative Relationships",
    "description": "Inference of protein complex states from quantitative proteomics data. The package takes information on known stable protein interactions (i.e. protein components of the same complex) and assesses how protein quantitative ratios change between different conditions. It reports protein pairs for which relative protein quantities to each other have been significantly altered in the tested condition.",
    "version": "0.1.0",
    "maintainer": "Marija Buljan <marija.buljan@empa.ch>",
    "author": "Marija Buljan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AlteredPQR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AlteredPQR Detection of Altered Protein Quantitative Relationships Inference of protein complex states from quantitative proteomics data. The package takes information on known stable protein interactions (i.e. protein components of the same complex) and assesses how protein quantitative ratios change between different conditions. It reports protein pairs for which relative protein quantities to each other have been significantly altered in the tested condition.  "
  },
  {
    "id": 1744,
    "package_name": "AutoAds",
    "title": "Advertisement Metrics Calculation",
    "description": "Calculations of the most common metrics of automated advertisement and plotting of them with trend and forecast. Calculations and description of metrics is taken from different RTB platforms support documentation. Plotting and forecasting is based on packages 'forecast', described in Rob J Hyndman and George Athanasopoulos (2021) \"Forecasting: Principles and Practice\" <https://otexts.com/fpp3/> and Rob J Hyndman et al \"Documentation for 'forecast'\" (2003) <https://pkg.robjhyndman.com/forecast/>, and 'ggplot2', described in Hadley Wickham et al \"Documentation for 'ggplot2'\" (2015) <https://ggplot2.tidyverse.org/>, and Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen (2015) \"ggplot2: Elegant Graphics for Data Analysis\" <https://ggplot2-book.org/>.",
    "version": "0.1.0",
    "maintainer": "Ivan Nemtsev <nemtsev.v@gmail.com>",
    "author": "Ivan Nemtsev [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AutoAds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AutoAds Advertisement Metrics Calculation Calculations of the most common metrics of automated advertisement and plotting of them with trend and forecast. Calculations and description of metrics is taken from different RTB platforms support documentation. Plotting and forecasting is based on packages 'forecast', described in Rob J Hyndman and George Athanasopoulos (2021) \"Forecasting: Principles and Practice\" <https://otexts.com/fpp3/> and Rob J Hyndman et al \"Documentation for 'forecast'\" (2003) <https://pkg.robjhyndman.com/forecast/>, and 'ggplot2', described in Hadley Wickham et al \"Documentation for 'ggplot2'\" (2015) <https://ggplot2.tidyverse.org/>, and Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen (2015) \"ggplot2: Elegant Graphics for Data Analysis\" <https://ggplot2-book.org/>.  "
  },
  {
    "id": 1762,
    "package_name": "AzureCosmosR",
    "title": "Interface to the 'Azure Cosmos DB' 'NoSQL' Database Service",
    "description": "An interface to 'Azure CosmosDB': <https://azure.microsoft.com/en-us/services/cosmos-db/>. On the admin side, 'AzureCosmosR' provides functionality to create and manage 'Cosmos DB' instances in Microsoft's 'Azure' cloud. On the client side, it provides an interface to the 'Cosmos DB' SQL API, letting the user store and query documents and attachments in 'Cosmos DB'. Part of the 'AzureR' family of packages.",
    "version": "1.0.0",
    "maintainer": "Hong Ooi <hongooi73@gmail.com>",
    "author": "Hong Ooi [aut, cre],\n  Andrew Liu [ctb] (Assistance with Cosmos DB),\n  Microsoft [cph]",
    "url": "https://github.com/Azure/AzureCosmosR\nhttps://github.com/Azure/AzureR",
    "bug_reports": "https://github.com/Azure/AzureCosmosR/issues",
    "repository": "https://cran.r-project.org/package=AzureCosmosR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AzureCosmosR Interface to the 'Azure Cosmos DB' 'NoSQL' Database Service An interface to 'Azure CosmosDB': <https://azure.microsoft.com/en-us/services/cosmos-db/>. On the admin side, 'AzureCosmosR' provides functionality to create and manage 'Cosmos DB' instances in Microsoft's 'Azure' cloud. On the client side, it provides an interface to the 'Cosmos DB' SQL API, letting the user store and query documents and attachments in 'Cosmos DB'. Part of the 'AzureR' family of packages.  "
  },
  {
    "id": 1822,
    "package_name": "BET",
    "title": "Binary Expansion Testing",
    "description": "Nonparametric detection of nonuniformity and dependence with Binary Expansion Testing (BET). See Kai Zhang (2019) BET on Independence, Journal of the American Statistical Association, 114:528, 1620-1637, <DOI:10.1080/01621459.2018.1537921>, Kai Zhang, Wan Zhang, Zhigen Zhao, Wen Zhou. (2023). BEAUTY Powered BEAST, <doi:10.48550/arXiv.2103.00674>  and Wan Zhang, Zhigen Zhao, Michael Baiocchi, Yao Li, Kai Zhang. (2023) SorBET: A Fast and Powerful Algorithm to Test Dependence of Variables, Techinical report.",
    "version": "0.5.4",
    "maintainer": "Wan Zhang <wanz63@live.unc.edu>",
    "author": "Wan Zhang [aut, cre],\n  Zhigen Zhao [aut],\n  Michael Baiocchi [aut],\n  Kai Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BET Binary Expansion Testing Nonparametric detection of nonuniformity and dependence with Binary Expansion Testing (BET). See Kai Zhang (2019) BET on Independence, Journal of the American Statistical Association, 114:528, 1620-1637, <DOI:10.1080/01621459.2018.1537921>, Kai Zhang, Wan Zhang, Zhigen Zhao, Wen Zhou. (2023). BEAUTY Powered BEAST, <doi:10.48550/arXiv.2103.00674>  and Wan Zhang, Zhigen Zhao, Michael Baiocchi, Yao Li, Kai Zhang. (2023) SorBET: A Fast and Powerful Algorithm to Test Dependence of Variables, Techinical report.  "
  },
  {
    "id": 1823,
    "package_name": "BETS",
    "title": "Brazilian Economic Time Series",
    "description": "It provides access to and information about the most important\n    Brazilian economic time series - from the Getulio Vargas Foundation <http://portal.fgv.br/en>,\n    the Central Bank of Brazil <http://www.bcb.gov.br> and the Brazilian Institute of Geography\n    and Statistics <http://www.ibge.gov.br>. It also presents tools for managing, analysing (e.g.\n    generating dynamic reports with a complete analysis of a series) and exporting\n    these time series.",
    "version": "0.4.9",
    "maintainer": "Talitha Speranza <talitha.speranza@fgv.br>",
    "author": "Pedro Costa Ferreira [aut],\n  Talitha Speranza [aut, cre],\n  Jonatha Costa [aut],\n  Fernando Teixeira [ctb],\n  Daiane Marcolino [ctb]",
    "url": "https://github.com/nmecsys/BETS",
    "bug_reports": "https://github.com/nmecsys/BETS/issues",
    "repository": "https://cran.r-project.org/package=BETS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BETS Brazilian Economic Time Series It provides access to and information about the most important\n    Brazilian economic time series - from the Getulio Vargas Foundation <http://portal.fgv.br/en>,\n    the Central Bank of Brazil <http://www.bcb.gov.br> and the Brazilian Institute of Geography\n    and Statistics <http://www.ibge.gov.br>. It also presents tools for managing, analysing (e.g.\n    generating dynamic reports with a complete analysis of a series) and exporting\n    these time series.  "
  },
  {
    "id": 1866,
    "package_name": "BLRShiny",
    "title": "Interactive Document for Working with Binary Logistic Regression\nAnalysis",
    "description": "An interactive document on  the topic of binary logistic regression  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/BinaryLogisticRegressionModelling/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BLRShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BLRShiny Interactive Document for Working with Binary Logistic Regression\nAnalysis An interactive document on  the topic of binary logistic regression  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/BinaryLogisticRegressionModelling/>.  "
  },
  {
    "id": 1867,
    "package_name": "BLRShiny2",
    "title": "Interactive Document for Working with Binary Logistic Regression\nAnalysis",
    "description": "An interactive document on  the topic of binary logistic regression  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/BinaryLogisticRegressionModelling/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BLRShiny2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BLRShiny2 Interactive Document for Working with Binary Logistic Regression\nAnalysis An interactive document on  the topic of binary logistic regression  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/BinaryLogisticRegressionModelling/>.  "
  },
  {
    "id": 1918,
    "package_name": "BTM",
    "title": "Biterm Topic Models for Short Text",
    "description": "Biterm Topic Models find topics in collections of short texts. \n    It is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns which are called biterms.\n    This in contrast to traditional topic models like Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis \n    which are word-document co-occurrence topic models.\n    A biterm consists of two words co-occurring in the same short text window.  \n    This context window can for example be a twitter message, a short answer on a survey, a sentence of a text or a document identifier. \n    The techniques are explained in detail in the paper 'A Biterm Topic Model For Short Text' by Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng (2013) <https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf>.",
    "version": "0.3.8",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  Xiaohui Yan [ctb, cph] (BTM C++ library)",
    "url": "https://github.com/bnosac/BTM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BTM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BTM Biterm Topic Models for Short Text Biterm Topic Models find topics in collections of short texts. \n    It is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns which are called biterms.\n    This in contrast to traditional topic models like Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis \n    which are word-document co-occurrence topic models.\n    A biterm consists of two words co-occurring in the same short text window.  \n    This context window can for example be a twitter message, a short answer on a survey, a sentence of a text or a document identifier. \n    The techniques are explained in detail in the paper 'A Biterm Topic Model For Short Text' by Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng (2013) <https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf>.  "
  },
  {
    "id": 1928,
    "package_name": "BVSNLP",
    "title": "Bayesian Variable Selection in High Dimensional Settings using\nNonlocal Priors",
    "description": "Variable/Feature selection in high or ultra-high dimensional\n    settings has gained a lot of attention recently specially in cancer genomic\n    studies. This package provides a Bayesian approach to tackle this problem,\n    where it exploits mixture of point masses at zero and nonlocal priors to\n    improve the performance of variable selection and coefficient estimation.\n    product moment (pMOM) and product inverse moment (piMOM) nonlocal priors\n    are implemented and can be used for the analyses. This package performs\n    variable selection for binary response and survival time response datasets\n    which are widely used in biostatistic and bioinformatics community.\n    Benefiting from parallel computing ability, it reports necessary outcomes\n    of Bayesian variable selection such as Highest Posterior Probability Model\n    (HPPM), Median Probability Model (MPM) and posterior inclusion probability\n    for each of the covariates in the model. The option to use Bayesian Model\n    Averaging (BMA) is also part of this package that can be exploited for\n    predictive power measurements in real datasets.",
    "version": "1.1.9",
    "maintainer": "Amir Nikooienejad <amir.nikooienejad@gmail.com>",
    "author": "Amir Nikooienejad [aut, cre], Valen E. Johnson [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BVSNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BVSNLP Bayesian Variable Selection in High Dimensional Settings using\nNonlocal Priors Variable/Feature selection in high or ultra-high dimensional\n    settings has gained a lot of attention recently specially in cancer genomic\n    studies. This package provides a Bayesian approach to tackle this problem,\n    where it exploits mixture of point masses at zero and nonlocal priors to\n    improve the performance of variable selection and coefficient estimation.\n    product moment (pMOM) and product inverse moment (piMOM) nonlocal priors\n    are implemented and can be used for the analyses. This package performs\n    variable selection for binary response and survival time response datasets\n    which are widely used in biostatistic and bioinformatics community.\n    Benefiting from parallel computing ability, it reports necessary outcomes\n    of Bayesian variable selection such as Highest Posterior Probability Model\n    (HPPM), Median Probability Model (MPM) and posterior inclusion probability\n    for each of the covariates in the model. The option to use Bayesian Model\n    Averaging (BMA) is also part of this package that can be exploited for\n    predictive power measurements in real datasets.  "
  },
  {
    "id": 1991,
    "package_name": "BayesNetBP",
    "title": "Bayesian Network Belief Propagation",
    "description": "Belief propagation methods in Bayesian Networks to propagate evidence through the network. The implementation of these methods are based on the article: Cowell, RG (2005). Local Propagation in Conditional Gaussian Bayesian Networks <https://www.jmlr.org/papers/v6/cowell05a.html>. For details please see Yu et. al. (2020) BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian Networks <doi:10.18637/jss.v094.i03>. The optional 'cyjShiny' package for running the Shiny app is available at <https://github.com/cytoscape/cyjShiny>. Please see the example in the documentation of 'runBayesNetApp' function for installing 'cyjShiny' package from GitHub. ",
    "version": "1.6.1",
    "maintainer": "Han Yu <hyu9@buffalo.edu>",
    "author": "Han Yu, Rachael Blair, Janhavi Moharil, Andrew Yan",
    "url": "",
    "bug_reports": "https://github.com/hyu-ub/BayesNetBP/issues",
    "repository": "https://cran.r-project.org/package=BayesNetBP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesNetBP Bayesian Network Belief Propagation Belief propagation methods in Bayesian Networks to propagate evidence through the network. The implementation of these methods are based on the article: Cowell, RG (2005). Local Propagation in Conditional Gaussian Bayesian Networks <https://www.jmlr.org/papers/v6/cowell05a.html>. For details please see Yu et. al. (2020) BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian Networks <doi:10.18637/jss.v094.i03>. The optional 'cyjShiny' package for running the Shiny app is available at <https://github.com/cytoscape/cyjShiny>. Please see the example in the documentation of 'runBayesNetApp' function for installing 'cyjShiny' package from GitHub.   "
  },
  {
    "id": 2024,
    "package_name": "BayesianMediationA",
    "title": "Bayesian Mediation Analysis",
    "description": "We perform general mediation analysis in the Bayesian setting using the methods described in Yu and Li (2022, ISBN:9780367365479). With the package, the mediation analysis can be performed on different types of outcomes (e.g., continuous, binary, categorical, or time-to-event), with default or user-defined priors and predictive models. The Bayesian estimates and credible sets of mediation effects are reported as analytic results.",
    "version": "1.0.1",
    "maintainer": "Qingzhao Yu <qyu@lsuhsc.edu>",
    "author": "Qingzhao Yu [aut, cre, cph],\n  Bin Li [aut]",
    "url": "https://cran.r-project.org/package=BayesianMediationA,\nhttps://publichealth.lsuhsc.edu/Faculty_pages/qyu/index.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesianMediationA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesianMediationA Bayesian Mediation Analysis We perform general mediation analysis in the Bayesian setting using the methods described in Yu and Li (2022, ISBN:9780367365479). With the package, the mediation analysis can be performed on different types of outcomes (e.g., continuous, binary, categorical, or time-to-event), with default or user-defined priors and predictive models. The Bayesian estimates and credible sets of mediation effects are reported as analytic results.  "
  },
  {
    "id": 2055,
    "package_name": "BiObjClass",
    "title": "Classification of Algorithms",
    "description": "Implements the Bi-objective Lexicographical Classification method and Performance Assessment Ratio at 10% metric for algorithm classification. Constructs matrices representing algorithm performance under multiple criteria, facilitating decision-making in algorithm selection and evaluation. Analyzes and compares algorithm performance based on various metrics to identify the most suitable algorithms for specific tasks. This package includes methods for algorithm classification and evaluation, with examples provided in the documentation. Carvalho (2019) presents a statistical evaluation of algorithmic computational experimentation with infeasible solutions <doi:10.48550/arXiv.1902.00101>. Moreira and Carvalho (2023) analyze power in preprocessing methodologies for datasets with missing values <doi:10.1080/03610918.2023.2234683>.",
    "version": "0.1.0",
    "maintainer": "Tiago Costa Soares <tiagocsoares22@gmail.com>",
    "author": "Tiago Costa Soares [aut, cre],\n  Pedro Augusto Mendes [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BiObjClass",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BiObjClass Classification of Algorithms Implements the Bi-objective Lexicographical Classification method and Performance Assessment Ratio at 10% metric for algorithm classification. Constructs matrices representing algorithm performance under multiple criteria, facilitating decision-making in algorithm selection and evaluation. Analyzes and compares algorithm performance based on various metrics to identify the most suitable algorithms for specific tasks. This package includes methods for algorithm classification and evaluation, with examples provided in the documentation. Carvalho (2019) presents a statistical evaluation of algorithmic computational experimentation with infeasible solutions <doi:10.48550/arXiv.1902.00101>. Moreira and Carvalho (2023) analyze power in preprocessing methodologies for datasets with missing values <doi:10.1080/03610918.2023.2234683>.  "
  },
  {
    "id": 2080,
    "package_name": "BioCircos",
    "title": "Interactive Circular Visualization of Genomic Data using\n'htmlwidgets' and 'BioCircos.js'",
    "description": "Implement in 'R' interactive Circos-like visualizations of genomic data, to map information\n\tsuch as genetic variants, genomic fusions and aberrations to a circular genome, as proposed by the\n\t'JavaScript' library 'BioCircos.js', based on the 'JQuery' and 'D3' technologies. The output is by \n\tdefault displayed in stand-alone HTML documents or in the 'RStudio' viewer pane. Moreover it can be\n\tintegrated in 'R Markdown' documents and 'Shiny' applications.",
    "version": "0.3.4",
    "maintainer": "Loan Vulliard <lvulliard@cemm.at>",
    "author": "Loan Vulliard [trl, cre],\n  Xiaowei Chen [aut],\n  Ya Cui [aut]",
    "url": "https://github.com/lvulliard/BioCircos.R",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BioCircos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BioCircos Interactive Circular Visualization of Genomic Data using\n'htmlwidgets' and 'BioCircos.js' Implement in 'R' interactive Circos-like visualizations of genomic data, to map information\n\tsuch as genetic variants, genomic fusions and aberrations to a circular genome, as proposed by the\n\t'JavaScript' library 'BioCircos.js', based on the 'JQuery' and 'D3' technologies. The output is by \n\tdefault displayed in stand-alone HTML documents or in the 'RStudio' viewer pane. Moreover it can be\n\tintegrated in 'R Markdown' documents and 'Shiny' applications.  "
  },
  {
    "id": 2176,
    "package_name": "CALANGO",
    "title": "Comparative Analysis with Annotation-Based Genomic Components",
    "description": "A first-principle, phylogeny-aware comparative genomics tool for \n             investigating associations between terms used to annotate genomic\n             components (e.g., Pfam IDs, Gene Ontology terms,) with quantitative \n             or rank variables such as number of cell types, genome size, or \n             density of specific genomic elements. See the project website for \n             more information, documentation and examples, and \n             <doi:10.1016/j.patter.2023.100728> for the full paper.",
    "version": "1.0.20",
    "maintainer": "Felipe Campelo <fcampelo@gmail.com>",
    "author": "Francisco Lobo [aut],\n  Felipe Campelo [aut, cre],\n  Jorge Augusto Hongo [aut],\n  Giovanni Marques de Castro [aut],\n  Gabriel Almeida [sad, dnc]",
    "url": "https://labpackages.github.io/CALANGO/",
    "bug_reports": "https://github.com/fcampelo/CALANGO/issues/",
    "repository": "https://cran.r-project.org/package=CALANGO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CALANGO Comparative Analysis with Annotation-Based Genomic Components A first-principle, phylogeny-aware comparative genomics tool for \n             investigating associations between terms used to annotate genomic\n             components (e.g., Pfam IDs, Gene Ontology terms,) with quantitative \n             or rank variables such as number of cell types, genome size, or \n             density of specific genomic elements. See the project website for \n             more information, documentation and examples, and \n             <doi:10.1016/j.patter.2023.100728> for the full paper.  "
  },
  {
    "id": 2193,
    "package_name": "CATT",
    "title": "The Cochran-Armitage Trend Test",
    "description": "This function conducts the Cochran-Armitage trend test to a 2 by k contingency table. It will report the test statistic (Z) and p-value.A linear trend in the frequencies will be calculated, because the weights (0,1,2) will be used by default. ",
    "version": "2.0",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CATT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CATT The Cochran-Armitage Trend Test This function conducts the Cochran-Armitage trend test to a 2 by k contingency table. It will report the test statistic (Z) and p-value.A linear trend in the frequencies will be calculated, because the weights (0,1,2) will be used by default.   "
  },
  {
    "id": 2242,
    "package_name": "CFtime",
    "title": "Using CF-Compliant Calendars with Climate Projection Data",
    "description": "Support for all calendars as specified in the Climate and Forecast \n    (CF) Metadata Conventions for climate and forecasting data. The CF Metadata \n    Conventions is widely used for distributing files with climate observations \n    or projections, including the Coupled Model Intercomparison Project (CMIP) \n    data used by climate change scientists and the Intergovernmental Panel on\n    Climate Change (IPCC). This package specifically allows the user to work \n    with any of the CF-compliant calendars (many of which are not compliant with \n    POSIXt). The CF time coordinate is formally defined in the CF Metadata \n    Conventions document available at <https://cfconventions.org/Data/cf-conventions/cf-conventions-1.12/cf-conventions.html#time-coordinate>.",
    "version": "1.7.2",
    "maintainer": "Patrick Van Laake <patrick@vanlaake.net>",
    "author": "Patrick Van Laake [aut, cre, cph]",
    "url": "https://r-cf.github.io/CFtime/, https://github.com/R-CF/CFtime",
    "bug_reports": "https://github.com/R-CF/CFtime/issues",
    "repository": "https://cran.r-project.org/package=CFtime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CFtime Using CF-Compliant Calendars with Climate Projection Data Support for all calendars as specified in the Climate and Forecast \n    (CF) Metadata Conventions for climate and forecasting data. The CF Metadata \n    Conventions is widely used for distributing files with climate observations \n    or projections, including the Coupled Model Intercomparison Project (CMIP) \n    data used by climate change scientists and the Intergovernmental Panel on\n    Climate Change (IPCC). This package specifically allows the user to work \n    with any of the CF-compliant calendars (many of which are not compliant with \n    POSIXt). The CF time coordinate is formally defined in the CF Metadata \n    Conventions document available at <https://cfconventions.org/Data/cf-conventions/cf-conventions-1.12/cf-conventions.html#time-coordinate>.  "
  },
  {
    "id": 2281,
    "package_name": "CLSIEP15",
    "title": "Clinical and Laboratory Standards Institute (CLSI) EP15-A3\nCalculations",
    "description": "Calculations of \"EP15-A3 document. A manual for user verification of precision and estimation of bias\" CLSI (2014, ISBN:1-56238-966-1).",
    "version": "0.1.0",
    "maintainer": "Claucio Antonio Rank Filho <claucio.filho@hitechnologies.com.br>",
    "author": "Claucio Antonio Rank Filho [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CLSIEP15",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CLSIEP15 Clinical and Laboratory Standards Institute (CLSI) EP15-A3\nCalculations Calculations of \"EP15-A3 document. A manual for user verification of precision and estimation of bias\" CLSI (2014, ISBN:1-56238-966-1).  "
  },
  {
    "id": 2282,
    "package_name": "CLUSTShiny",
    "title": "Interactive Document for Working with Cluster Analysis",
    "description": "An interactive document on  the topic of cluster analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/ClusterAnalysis/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CLUSTShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CLUSTShiny Interactive Document for Working with Cluster Analysis An interactive document on  the topic of cluster analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/ClusterAnalysis/>.  "
  },
  {
    "id": 2294,
    "package_name": "CMShiny",
    "title": "Interactive Document for Working with Confusion Matrix",
    "description": "An interactive document on  the topic of confusion matrix analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at <https://predanalyticssessions1.shinyapps.io/ConfusionMatrixShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CMShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CMShiny Interactive Document for Working with Confusion Matrix An interactive document on  the topic of confusion matrix analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at <https://predanalyticssessions1.shinyapps.io/ConfusionMatrixShiny/>.  "
  },
  {
    "id": 2320,
    "package_name": "CORElearn",
    "title": "Classification, Regression and Feature Evaluation",
    "description": "A suite of machine learning algorithms written in C++ with the R \n interface contains several learning techniques for classification and regression.\n Predictive models include e.g., classification and regression trees with\n optional constructive induction and models in the leaves, random forests, kNN, \n naive Bayes, and locally weighted regression. All predictions obtained with these\n models can be explained and visualized with the 'ExplainPrediction' package.  \n This package is especially strong in feature evaluation where it contains several variants of\n Relief algorithm and many impurity based attribute evaluation functions, e.g., Gini, \n information gain, MDL, and DKM. These methods can be used for feature selection \n or discretization of numeric attributes.\n The OrdEval algorithm and its visualization is used for evaluation\n of data sets with ordinal features and class, enabling analysis according to the \n Kano model of customer satisfaction. \n Several algorithms support parallel multithreaded execution via OpenMP.  \n The top-level documentation is reachable through ?CORElearn.",
    "version": "1.57.3.1",
    "maintainer": "Marko Robnik-Sikonja <marko.robnik@fri.uni-lj.si>",
    "author": "Marko Robnik-Sikonja [aut, cre],\n  Petr Savicky [aut]",
    "url": "http://lkm.fri.uni-lj.si/rmarko/software/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CORElearn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CORElearn Classification, Regression and Feature Evaluation A suite of machine learning algorithms written in C++ with the R \n interface contains several learning techniques for classification and regression.\n Predictive models include e.g., classification and regression trees with\n optional constructive induction and models in the leaves, random forests, kNN, \n naive Bayes, and locally weighted regression. All predictions obtained with these\n models can be explained and visualized with the 'ExplainPrediction' package.  \n This package is especially strong in feature evaluation where it contains several variants of\n Relief algorithm and many impurity based attribute evaluation functions, e.g., Gini, \n information gain, MDL, and DKM. These methods can be used for feature selection \n or discretization of numeric attributes.\n The OrdEval algorithm and its visualization is used for evaluation\n of data sets with ordinal features and class, enabling analysis according to the \n Kano model of customer satisfaction. \n Several algorithms support parallel multithreaded execution via OpenMP.  \n The top-level documentation is reachable through ?CORElearn.  "
  },
  {
    "id": 2364,
    "package_name": "CTShiny",
    "title": "Interactive Document for Working with Classification Tree\nAnalysis",
    "description": "An interactive document on  the topic of classification tree analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/CTShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CTShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CTShiny Interactive Document for Working with Classification Tree\nAnalysis An interactive document on  the topic of classification tree analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/CTShiny/>.  "
  },
  {
    "id": 2365,
    "package_name": "CTShiny2",
    "title": "Interactive Document for Working with Classification Tree\nAnalysis",
    "description": "An interactive document on  the topic of classification tree analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/CTShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CTShiny2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CTShiny2 Interactive Document for Working with Classification Tree\nAnalysis An interactive document on  the topic of classification tree analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/CTShiny/>.  "
  },
  {
    "id": 2416,
    "package_name": "CausalImpact",
    "title": "Inferring Causal Effects using Bayesian Structural Time-Series\nModels",
    "description": "Implements a Bayesian approach to causal impact estimation in time\n  series, as described in Brodersen et al. (2015) <DOI:10.1214/14-AOAS788>.\n  See the package documentation on GitHub\n  <https://google.github.io/CausalImpact/> to get started.",
    "version": "1.4.1",
    "maintainer": "Alain Hauser <alhauser@google.com>",
    "author": "Kay H. Brodersen [aut],\n  Alain Hauser [aut, cre]",
    "url": "https://google.github.io/CausalImpact/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CausalImpact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CausalImpact Inferring Causal Effects using Bayesian Structural Time-Series\nModels Implements a Bayesian approach to causal impact estimation in time\n  series, as described in Brodersen et al. (2015) <DOI:10.1214/14-AOAS788>.\n  See the package documentation on GitHub\n  <https://google.github.io/CausalImpact/> to get started.  "
  },
  {
    "id": 2430,
    "package_name": "Certara.DarwinReporter",
    "title": "Data Visualization Utilities for 'pyDarwin' Machine Learning\nPharmacometric Model Development",
    "description": "Utilize the 'shiny' interface for visualizing results from a 'pyDarwin' (<https://certara.github.io/pyDarwin/>)\n    machine learning pharmacometric model search. It generates Goodness-of-Fit plots and summary tables for selected models,\n    allowing users to customize diagnostic outputs within the interface. The underlying R code for generating plots and\n    tables can be extracted for use outside the interactive session. Model diagnostics can also be incorporated into an\n    R Markdown document and rendered in various output formats.",
    "version": "2.0.1",
    "maintainer": "James Craig <james.craig@certara.com>",
    "author": "James Craig [aut, cre],\n  Michael Tomashevskiy [aut],\n  Mike Talley [aut],\n  Certara USA, Inc [cph, fnd]",
    "url": "https://certara.github.io/R-DarwinReporter/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Certara.DarwinReporter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Certara.DarwinReporter Data Visualization Utilities for 'pyDarwin' Machine Learning\nPharmacometric Model Development Utilize the 'shiny' interface for visualizing results from a 'pyDarwin' (<https://certara.github.io/pyDarwin/>)\n    machine learning pharmacometric model search. It generates Goodness-of-Fit plots and summary tables for selected models,\n    allowing users to customize diagnostic outputs within the interface. The underlying R code for generating plots and\n    tables can be extracted for use outside the interactive session. Model diagnostics can also be incorporated into an\n    R Markdown document and rendered in various output formats.  "
  },
  {
    "id": 2431,
    "package_name": "Certara.ModelResults",
    "title": "Generate Diagnostics for Pharmacometric Models Using 'shiny'",
    "description": "Utilize the 'shiny' interface to generate Goodness of Fit (GOF) plots and tables\n      for Non-Linear Mixed Effects (NLME / NONMEM) pharmacometric models. From the interface,\n      users can customize model diagnostics and generate the underlying R code to reproduce the \n      diagnostic plots and tables outside of the 'shiny' session. Model diagnostics can be included\n      in a 'rmarkdown' document and rendered to desired output format.",
    "version": "3.0.1",
    "maintainer": "James Craig <james.craig@certara.com>",
    "author": "James Craig [aut, cre],\n  Shuhua Hu [ctb],\n  Mike Talley [aut],\n  Certara USA, Inc [cph, fnd]",
    "url": "https://certara.github.io/R-model-results/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Certara.ModelResults",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Certara.ModelResults Generate Diagnostics for Pharmacometric Models Using 'shiny' Utilize the 'shiny' interface to generate Goodness of Fit (GOF) plots and tables\n      for Non-Linear Mixed Effects (NLME / NONMEM) pharmacometric models. From the interface,\n      users can customize model diagnostics and generate the underlying R code to reproduce the \n      diagnostic plots and tables outside of the 'shiny' session. Model diagnostics can be included\n      in a 'rmarkdown' document and rendered to desired output format.  "
  },
  {
    "id": 2473,
    "package_name": "CirceR",
    "title": "Construct Cohort Inclusion and Restriction Criteria Expressions",
    "description": "Wraps the 'CIRCE' (<https://github.com/ohdsi/circe-be>) 'Java'\n  library allowing cohort definition expressions to be edited and converted to \n  'Markdown' or 'SQL'.",
    "version": "1.3.3",
    "maintainer": "Chris Knoll <cknoll@ohdsi.org>",
    "author": "Chris Knoll [aut, cre],\n  Martijn Schuemie [aut]",
    "url": "https://ohdsi.github.io/CirceR/, https://github.com/OHDSI/CirceR/",
    "bug_reports": "https://github.com/OHDSI/CirceR/issues/",
    "repository": "https://cran.r-project.org/package=CirceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CirceR Construct Cohort Inclusion and Restriction Criteria Expressions Wraps the 'CIRCE' (<https://github.com/ohdsi/circe-be>) 'Java'\n  library allowing cohort definition expressions to be edited and converted to \n  'Markdown' or 'SQL'.  "
  },
  {
    "id": 2475,
    "package_name": "CircularDDM",
    "title": "Circular Drift-Diffusion Model",
    "description": "Circular drift-diffusion model for continuous reports.",
    "version": "0.1.0",
    "maintainer": "Yi-Shin Lin <yishin.lin@utas.edu.au>",
    "author": "Yi-Shin Lin [aut, cre],\n  Andrew Heathcote [aut],\n  Peter Kvam [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CircularDDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CircularDDM Circular Drift-Diffusion Model Circular drift-diffusion model for continuous reports.  "
  },
  {
    "id": 2481,
    "package_name": "ClassificationEnsembles",
    "title": "Automatically Builds 12 Classification Models",
    "description": "Automatically builds 12 classification models from data. The package returns 26 plots, 5 tables and a summary report.\n    The package automatically builds six individual classification models, including error (RMSE) and predictions. That data is used to create an ensemble, which is then modeled using six methods.\n    The process is repeated as many times as the user requests. The mean of the results are presented in a summary table. \n    The package returns the confusion matrices for all 12 models, tables of the correlation of the numeric data, the results of the variance inflation process, the head of the ensemble and the head of the data frame.",
    "version": "0.7.1",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "https://github.com/InfiniteCuriosity/ClassificationEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/ClassificationEnsembles/issues",
    "repository": "https://cran.r-project.org/package=ClassificationEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ClassificationEnsembles Automatically Builds 12 Classification Models Automatically builds 12 classification models from data. The package returns 26 plots, 5 tables and a summary report.\n    The package automatically builds six individual classification models, including error (RMSE) and predictions. That data is used to create an ensemble, which is then modeled using six methods.\n    The process is repeated as many times as the user requests. The mean of the results are presented in a summary table. \n    The package returns the confusion matrices for all 12 models, tables of the correlation of the numeric data, the results of the variance inflation process, the head of the ensemble and the head of the data frame.  "
  },
  {
    "id": 2556,
    "package_name": "CommonDataModel",
    "title": "OMOP CDM DDL and Documentation Generator",
    "description": "Generates the scripts required to create an Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database and associated documentation for supported database platforms. Leverages the 'SqlRender' package to convert the Data Definition Language (DDL) script written in parameterized Structured Query Language (SQL) to the other supported dialects.",
    "version": "1.0.1",
    "maintainer": "Clair Blacketer <mblacke@its.jnj.com>",
    "author": "Clair Blacketer [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CommonDataModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CommonDataModel OMOP CDM DDL and Documentation Generator Generates the scripts required to create an Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database and associated documentation for supported database platforms. Leverages the 'SqlRender' package to convert the Data Definition Language (DDL) script written in parameterized Structured Query Language (SQL) to the other supported dialects.  "
  },
  {
    "id": 2609,
    "package_name": "ConsReg",
    "title": "Fits Regression & ARMA Models Subject to Constraints to the\nCoefficient",
    "description": "Fits or generalized linear models either a regression with Autoregressive moving-average (ARMA) errors for time series data. \n       The package makes it easy to incorporate constraints into the model's coefficients. \n          The model is specified by an objective function (Gaussian, Binomial or Poisson) or an ARMA order (p,q), \n          a vector of bound constraints \n          for the coefficients (i.e beta1 > 0) and the possibility to incorporate restrictions\n          among coefficients (i.e beta1 > beta2).\n          The references of this packages are the same as 'stats' package for glm() and arima() functions.\n          See Brockwell, P. J. and Davis, R. A. (1996, ISBN-10: 9783319298528).\n          For the different optimizers implemented, it is recommended to consult the documentation of the corresponding packages. ",
    "version": "0.1.0",
    "maintainer": "Josep Puig <puigjos@gmail.com>",
    "author": "Josep Puig Sall\u00e9s",
    "url": "https://github.com/puigjos/ConsReg",
    "bug_reports": "https://github.com/puigjos/ConsReg/issues",
    "repository": "https://cran.r-project.org/package=ConsReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConsReg Fits Regression & ARMA Models Subject to Constraints to the\nCoefficient Fits or generalized linear models either a regression with Autoregressive moving-average (ARMA) errors for time series data. \n       The package makes it easy to incorporate constraints into the model's coefficients. \n          The model is specified by an objective function (Gaussian, Binomial or Poisson) or an ARMA order (p,q), \n          a vector of bound constraints \n          for the coefficients (i.e beta1 > 0) and the possibility to incorporate restrictions\n          among coefficients (i.e beta1 > beta2).\n          The references of this packages are the same as 'stats' package for glm() and arima() functions.\n          See Brockwell, P. J. and Davis, R. A. (1996, ISBN-10: 9783319298528).\n          For the different optimizers implemented, it is recommended to consult the documentation of the corresponding packages.   "
  },
  {
    "id": 2692,
    "package_name": "D4TAlink.light",
    "title": "GDP - Workflow Management",
    "description": "Tools, methods and processes for the management \n  of analysis workflows. These lightweight solutions facilitate \n  structuring R&D activities. These solutions were developed to comply  \n  with Good Documentation Practice (GDP), \n  with ALCOA+ principles as proposed by the U.S. FDA, and \n  with FAIR principles as discussed by Jacobsen et al. (2017) <doi:10.1162/dint_r_00024>. ",
    "version": "2.1.21",
    "maintainer": "Gregoire Thomas <gregoire.thomas@SQU4RE.com>",
    "author": "Gregoire Thomas [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6247-9438>)",
    "url": "https://bitbucket.org/SQ4/d4talink.light",
    "bug_reports": "https://bitbucket.org/SQ4/d4talink.light/issues",
    "repository": "https://cran.r-project.org/package=D4TAlink.light",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "D4TAlink.light GDP - Workflow Management Tools, methods and processes for the management \n  of analysis workflows. These lightweight solutions facilitate \n  structuring R&D activities. These solutions were developed to comply  \n  with Good Documentation Practice (GDP), \n  with ALCOA+ principles as proposed by the U.S. FDA, and \n  with FAIR principles as discussed by Jacobsen et al. (2017) <doi:10.1162/dint_r_00024>.   "
  },
  {
    "id": 2697,
    "package_name": "DAGassist",
    "title": "Test Robustness with Directed Acyclic Graphs",
    "description": "Provides robustness checks to align estimands with the identification \n    that they require. Given a 'dagitty' object and a model specification, \n    'DAGassist' classifies variables by causal roles, flags problematic controls, \n    and generates a report comparing the original model with minimal and canonical \n    adjustment sets. Exports publication-grade reports in 'LaTeX', 'Word', 'Excel',\n    'dotwhisker', or plain text/'markdown'. 'DAGassist' is built on 'dagitty', an \n    'R' package that uses the 'DAGitty' web tool (<https://dagitty.net/>) for \n    creating and analyzing DAGs. Methods draw on Pearl (2009) <doi:10.1017/CBO9780511803161>\n    and Textor et al. (2016) <doi:10.1093/ije/dyw341>.",
    "version": "0.2.7",
    "maintainer": "Graham Goff <goffgrahamc@gmail.com>",
    "author": "Graham Goff [aut, cre] (ORCID: <https://orcid.org/0000-0002-0717-6995>),\n  Michael Denly [aut] (ORCID: <https://orcid.org/0000-0002-7074-5011>)",
    "url": "https://github.com/grahamgoff/DAGassist,\nhttps://grahamgoff.github.io/DAGassist/",
    "bug_reports": "https://github.com/grahamgoff/DAGassist/issues",
    "repository": "https://cran.r-project.org/package=DAGassist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DAGassist Test Robustness with Directed Acyclic Graphs Provides robustness checks to align estimands with the identification \n    that they require. Given a 'dagitty' object and a model specification, \n    'DAGassist' classifies variables by causal roles, flags problematic controls, \n    and generates a report comparing the original model with minimal and canonical \n    adjustment sets. Exports publication-grade reports in 'LaTeX', 'Word', 'Excel',\n    'dotwhisker', or plain text/'markdown'. 'DAGassist' is built on 'dagitty', an \n    'R' package that uses the 'DAGitty' web tool (<https://dagitty.net/>) for \n    creating and analyzing DAGs. Methods draw on Pearl (2009) <doi:10.1017/CBO9780511803161>\n    and Textor et al. (2016) <doi:10.1093/ije/dyw341>.  "
  },
  {
    "id": 2734,
    "package_name": "DDIwR",
    "title": "DDI with R",
    "description": "Useful functions for various DDI (Data Documentation Initiative)\n    related inputs and outputs. Converts data files to and from DDI, SPSS,\n    Stata, SAS, R and Excel, including user declared missing values.",
    "version": "0.19",
    "maintainer": "Adrian Dusa <dusa.adrian@unibuc.ro>",
    "author": "Adrian Dusa [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3525-9253>)",
    "url": "https://github.com/dusadrian/DDIwR",
    "bug_reports": "https://github.com/dusadrian/DDIwR/issues",
    "repository": "https://cran.r-project.org/package=DDIwR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DDIwR DDI with R Useful functions for various DDI (Data Documentation Initiative)\n    related inputs and outputs. Converts data files to and from DDI, SPSS,\n    Stata, SAS, R and Excel, including user declared missing values.  "
  },
  {
    "id": 2737,
    "package_name": "DDPM",
    "title": "Data Sets for Discrete Probability Models",
    "description": "A wide collection of univariate discrete data sets from various applied domains related to distribution theory. The functions allow quick, easy, and efficient access to 100 univariate discrete data sets. The data are related to different applied domains, including medical, reliability analysis, engineering, manufacturing, occupational safety, geological sciences, terrorism, psychology, agriculture, environmental sciences, road traffic accidents, demography, actuarial science, law, and justice. The documentation, along with associated references for further details and uses, is presented.     ",
    "version": "0.1.0",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Christophe Chesneau [aut],\n  Muhammad Imran [aut, cre],\n  M.H Tahir [aut],\n  Farrukh Jamal [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DDPM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DDPM Data Sets for Discrete Probability Models A wide collection of univariate discrete data sets from various applied domains related to distribution theory. The functions allow quick, easy, and efficient access to 100 univariate discrete data sets. The data are related to different applied domains, including medical, reliability analysis, engineering, manufacturing, occupational safety, geological sciences, terrorism, psychology, agriculture, environmental sciences, road traffic accidents, demography, actuarial science, law, and justice. The documentation, along with associated references for further details and uses, is presented.       "
  },
  {
    "id": 2755,
    "package_name": "DEXiR",
    "title": "'DEXi' Library",
    "description": "A software package for using 'DEXi' models. 'DEXi' models are\n  hierarchical qualitative multi-criteria decision models developed according to the method\n  DEX (Decision EXpert, <https://dex.ijs.si/documentation/DEX_Method/DEX_Method.html>),\n  using the program 'DEXi' (<https://kt.ijs.si/MarkoBohanec/dexi.html>) or\n  'DEXiWin' (<https://dex.ijs.si/dexisuite/dexiwin.html>).\n  A typical workflow with 'DEXiR' consists of:\n  (1) reading a '.dxi' file, previously made using the 'DEXi' software (function read_dexi()),\n  (2) making a data frame containing input values of one or more decision alternatives,\n  (3) evaluating those alternatives (function evaluate()),\n  (4) analyzing alternatives (selective_explanation(), plus_minus(), compare_alternatives()),\n  (5) drawing charts.\n  'DEXiR' is restricted to using models produced externally by the 'DEXi' software and does not\n  provide functionality for creating and/or editing 'DEXi' models directly in 'R'.",
    "version": "1.0.2",
    "maintainer": "Marko Bohanec <marko.bohanec@ijs.si>",
    "author": "Marko Bohanec [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4317-2833>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DEXiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DEXiR 'DEXi' Library A software package for using 'DEXi' models. 'DEXi' models are\n  hierarchical qualitative multi-criteria decision models developed according to the method\n  DEX (Decision EXpert, <https://dex.ijs.si/documentation/DEX_Method/DEX_Method.html>),\n  using the program 'DEXi' (<https://kt.ijs.si/MarkoBohanec/dexi.html>) or\n  'DEXiWin' (<https://dex.ijs.si/dexisuite/dexiwin.html>).\n  A typical workflow with 'DEXiR' consists of:\n  (1) reading a '.dxi' file, previously made using the 'DEXi' software (function read_dexi()),\n  (2) making a data frame containing input values of one or more decision alternatives,\n  (3) evaluating those alternatives (function evaluate()),\n  (4) analyzing alternatives (selective_explanation(), plus_minus(), compare_alternatives()),\n  (5) drawing charts.\n  'DEXiR' is restricted to using models produced externally by the 'DEXi' software and does not\n  provide functionality for creating and/or editing 'DEXi' models directly in 'R'.  "
  },
  {
    "id": 2759,
    "package_name": "DEploid",
    "title": "Deconvolute Mixed Genomes with Unknown Proportions",
    "description": "Traditional phasing programs are limited to diploid organisms.\n Our method modifies Li and Stephens algorithm with Markov chain Monte Carlo\n (MCMC) approaches, and builds a generic framework that allows haplotype searches\n in a multiple infection setting. This package is primarily developed as part of\n the Pf3k project, which is a global collaboration using the latest\n sequencing technologies to provide a high-resolution view of natural variation\n in the malaria parasite Plasmodium falciparum. Parasite DNA are extracted from\n patient blood sample, which often contains more than one parasite strain, with\n unknown proportions. This package is used for deconvoluting mixed haplotypes,\n and reporting the mixture proportions from each sample.",
    "version": "0.5.7",
    "maintainer": "Joe Zhu <sha.joe.zhu@gmail.com>",
    "author": "Joe Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0001-7566-2787>),\n  Jacob Almagro-Garcia [aut],\n  Gil McVean [aut],\n  University of Oxford [cph],\n  Yinghan Liu [ctb],\n  CodeCogs Zyba Ltd [com, cph],\n  Deepak Bandyopadhyay [com, cph],\n  Lutz Kettner [com, cph]",
    "url": "https://github.com/DEploid-dev/DEploid-r",
    "bug_reports": "https://github.com/DEploid-dev/DEploid-r/issues",
    "repository": "https://cran.r-project.org/package=DEploid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DEploid Deconvolute Mixed Genomes with Unknown Proportions Traditional phasing programs are limited to diploid organisms.\n Our method modifies Li and Stephens algorithm with Markov chain Monte Carlo\n (MCMC) approaches, and builds a generic framework that allows haplotype searches\n in a multiple infection setting. This package is primarily developed as part of\n the Pf3k project, which is a global collaboration using the latest\n sequencing technologies to provide a high-resolution view of natural variation\n in the malaria parasite Plasmodium falciparum. Parasite DNA are extracted from\n patient blood sample, which often contains more than one parasite strain, with\n unknown proportions. This package is used for deconvoluting mixed haplotypes,\n and reporting the mixture proportions from each sample.  "
  },
  {
    "id": 2795,
    "package_name": "DIVINE",
    "title": "Curated Datasets and Tools for Epidemiological Data Analysis",
    "description": "Curated datasets and intuitive data management functions to streamline epidemiological data workflows. It is designed to support researchers in quickly accessing clean, structured data and applying essential cleaning, summarizing, visualization, and export operations with minimal effort. Whether you're preparing a cohort for analysis or creating reports, 'DIVINE' makes the process more efficient, transparent, and reproducible.",
    "version": "0.1.1",
    "maintainer": "Nat\u00e0lia Pallar\u00e8s <npallares@igtp.cat>",
    "author": "Nat\u00e0lia Pallar\u00e8s [aut, cre],\n  Jo\u00e3o Carmezim [aut],\n  Pau Satorra [aut],\n  Lucia Blanc [aut],\n  Cristian Teb\u00e9 [aut]",
    "url": "https://bruigtp.github.io/DIVINE/",
    "bug_reports": "https://github.com/bruigtp/DIVINE/issues",
    "repository": "https://cran.r-project.org/package=DIVINE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DIVINE Curated Datasets and Tools for Epidemiological Data Analysis Curated datasets and intuitive data management functions to streamline epidemiological data workflows. It is designed to support researchers in quickly accessing clean, structured data and applying essential cleaning, summarizing, visualization, and export operations with minimal effort. Whether you're preparing a cohort for analysis or creating reports, 'DIVINE' makes the process more efficient, transparent, and reproducible.  "
  },
  {
    "id": 2828,
    "package_name": "DOT",
    "title": "Render and Export DOT Graphs in R",
    "description": "Renders DOT diagram markup language in R and also provides the possibility to\n    export the graphs in PostScript and SVG (Scalable Vector Graphics) formats.\n    In addition, it supports literate programming packages such as 'knitr' and\n    'rmarkdown'.",
    "version": "0.1",
    "maintainer": "E. F. Haghish <haghish@imbi.uni-freiburg.de>",
    "author": "E. F. Haghish",
    "url": "http://haghish.com/dot",
    "bug_reports": "http://github.com/haghish/DOT",
    "repository": "https://cran.r-project.org/package=DOT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DOT Render and Export DOT Graphs in R Renders DOT diagram markup language in R and also provides the possibility to\n    export the graphs in PostScript and SVG (Scalable Vector Graphics) formats.\n    In addition, it supports literate programming packages such as 'knitr' and\n    'rmarkdown'.  "
  },
  {
    "id": 2861,
    "package_name": "DSOpal",
    "title": "'DataSHIELD' Implementation for 'Opal'",
    "description": "'DataSHIELD' is an infrastructure and series of R packages that \n    enables the remote and 'non-disclosive' analysis of sensitive research data.\n    This package is the 'DataSHIELD' interface implementation for 'Opal', which is\n    the data integration application for biobanks by 'OBiBa'. Participant data, once\n    collected from any data source, must be integrated and stored in a central\n    data repository under a uniform model. 'Opal' is such a central repository.\n    It can import, process, validate, query, analyze, report, and export data.\n    'Opal' is the reference implementation of the 'DataSHIELD' infrastructure.",
    "version": "1.5.0",
    "maintainer": "Yannick Marcon <yannick.marcon@obiba.org>",
    "author": "Yannick Marcon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0138-2023>),\n  Becca Wilson [ctb] (ORCID: <https://orcid.org/0000-0003-2294-593X>),\n  OBiBa group [cph]",
    "url": "https://github.com/datashield/DSOpal/,\nhttps://datashield.github.io/DSOpal/, https://www.obiba.org,\nhttps://www.obiba.org/pages/products/opal/,\nhttps://datashield.org/,\nhttps://academic.oup.com/ije/article/43/6/1929/707730,\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008880",
    "bug_reports": "https://github.com/datashield/DSOpal/issues/",
    "repository": "https://cran.r-project.org/package=DSOpal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DSOpal 'DataSHIELD' Implementation for 'Opal' 'DataSHIELD' is an infrastructure and series of R packages that \n    enables the remote and 'non-disclosive' analysis of sensitive research data.\n    This package is the 'DataSHIELD' interface implementation for 'Opal', which is\n    the data integration application for biobanks by 'OBiBa'. Participant data, once\n    collected from any data source, must be integrated and stored in a central\n    data repository under a uniform model. 'Opal' is such a central repository.\n    It can import, process, validate, query, analyze, report, and export data.\n    'Opal' is the reference implementation of the 'DataSHIELD' infrastructure.  "
  },
  {
    "id": 2866,
    "package_name": "DSpoty",
    "title": "Get 'Spotify' API Multiple Information",
    "description": "You can retrieve 'Spotify' API Information such as artists, albums, tracks, features tracks, recommendations or related artists.\n    This package allows you to search all the information by name and also includes a distance based algorithm to find similar songs.\n\tMore information: <https://developer.spotify.com/documentation/web-api/> .",
    "version": "0.1.0",
    "maintainer": "Alberto Almui\u00f1a <albertogonzalezalmuinha@gmail.com>",
    "author": "Alberto Almui\u00f1a <albertogonzalezalmuinha@gmail.com>",
    "url": "https://github.com/AlbertoAlmuinha/DSpoty",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DSpoty",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DSpoty Get 'Spotify' API Multiple Information You can retrieve 'Spotify' API Information such as artists, albums, tracks, features tracks, recommendations or related artists.\n    This package allows you to search all the information by name and also includes a distance based algorithm to find similar songs.\n\tMore information: <https://developer.spotify.com/documentation/web-api/> .  "
  },
  {
    "id": 2910,
    "package_name": "DataSetsUni",
    "title": "A Collection of Univariate Data Sets",
    "description": "A collection of widely used univariate data sets of various applied domains on applications of distribution theory. The functions allow researchers and practitioners to quickly, easily, and efficiently access and use these data sets. The data are related to different applied domains and as follows: Bio-medical, survival analysis, medicine, reliability analysis, hydrology, actuarial science, operational research, meteorology, extreme values, quality control, engineering, finance, sports and economics. The total 100 data sets are documented along with associated references for further details and uses.     ",
    "version": "0.1",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Muhammad Imran [aut, cre],\n  M.H Tahir [ctb],\n  Farrukh Jamal [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DataSetsUni",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataSetsUni A Collection of Univariate Data Sets A collection of widely used univariate data sets of various applied domains on applications of distribution theory. The functions allow researchers and practitioners to quickly, easily, and efficiently access and use these data sets. The data are related to different applied domains and as follows: Bio-medical, survival analysis, medicine, reliability analysis, hydrology, actuarial science, operational research, meteorology, extreme values, quality control, engineering, finance, sports and economics. The total 100 data sets are documented along with associated references for further details and uses.       "
  },
  {
    "id": 2911,
    "package_name": "DataSetsVerse",
    "title": "A Metapackage for Thematic and Domain-Specific Datasets",
    "description": "A metapackage that brings together a curated collection \n    of R packages containing domain-specific datasets. It includes time series data, \n    educational metrics, crime records, medical datasets, and oncology research data. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to structured and well-documented datasets, this metapackage \n    facilitates reproducible research, data exploration, and teaching applications across \n    a wide range of domains.\n    Included packages:\n    - 'timeSeriesDataSets': Time series data from economics, finance, energy, and healthcare.\n    - 'educationR': Datasets related to education, learning outcomes, and school metrics.\n    - 'crimedatasets': Datasets on global and local crime and criminal behavior.\n    - 'MedDataSets': Datasets related to medicine, public health, treatments, and clinical trials.\n    - 'OncoDataSets': Datasets focused on cancer research, survival, genetics, and biomarkers.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre]",
    "url": "https://github.com/lightbluetitan/datasetsverse,\nhttps://lightbluetitan.github.io/datasetsverse/",
    "bug_reports": "https://github.com/lightbluetitan/datasetsverse/issues",
    "repository": "https://cran.r-project.org/package=DataSetsVerse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataSetsVerse A Metapackage for Thematic and Domain-Specific Datasets A metapackage that brings together a curated collection \n    of R packages containing domain-specific datasets. It includes time series data, \n    educational metrics, crime records, medical datasets, and oncology research data. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to structured and well-documented datasets, this metapackage \n    facilitates reproducible research, data exploration, and teaching applications across \n    a wide range of domains.\n    Included packages:\n    - 'timeSeriesDataSets': Time series data from economics, finance, energy, and healthcare.\n    - 'educationR': Datasets related to education, learning outcomes, and school metrics.\n    - 'crimedatasets': Datasets on global and local crime and criminal behavior.\n    - 'MedDataSets': Datasets related to medicine, public health, treatments, and clinical trials.\n    - 'OncoDataSets': Datasets focused on cancer research, survival, genetics, and biomarkers.  "
  },
  {
    "id": 2919,
    "package_name": "DatastreamDSWS2R",
    "title": "Provides a Link Between the 'LSEG Datastream' System and R",
    "description": "Provides a set of functions and a class to connect, extract and\n    upload information from the 'LSEG Datastream' database. This\n    package uses the 'DSWS' API and server used by the 'Datastream DFO addin'.  \n    Details of this API are available at <https://www.lseg.com/en/data-analytics>.\n    Please report issues at <https://github.com/CharlesCara/DatastreamDSWS2R/issues>.",
    "version": "1.9.12",
    "maintainer": "Charles Cara <charles.cara@absolute-strategy.com>",
    "author": "Charles Cara [aut, cre]",
    "url": "https://github.com/CharlesCara/DatastreamDSWS2R",
    "bug_reports": "https://github.com/CharlesCara/DatastreamDSWS2R/issues",
    "repository": "https://cran.r-project.org/package=DatastreamDSWS2R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DatastreamDSWS2R Provides a Link Between the 'LSEG Datastream' System and R Provides a set of functions and a class to connect, extract and\n    upload information from the 'LSEG Datastream' database. This\n    package uses the 'DSWS' API and server used by the 'Datastream DFO addin'.  \n    Details of this API are available at <https://www.lseg.com/en/data-analytics>.\n    Please report issues at <https://github.com/CharlesCara/DatastreamDSWS2R/issues>.  "
  },
  {
    "id": 2923,
    "package_name": "DeBoinR",
    "title": "Box-Plots and Outlier Detection for Probability Density\nFunctions",
    "description": "Orders a data-set consisting of an ensemble of probability density functions on the same x-grid.  Visualizes a box-plot of these functions based on the notion of distance determined by the user.  Reports outliers based on the distance chosen and the scaling factor for an interquartile range rule.  For further details, see: Alexander C. Murph et al. (2023). \"Visualization and Outlier Detection for Probability Density Function Ensembles.\" <https://sirmurphalot.github.io/publications>.",
    "version": "1.0",
    "maintainer": "Alexander C. Murph <murph@lanl.gov>",
    "author": "Alexander C. Murph [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7170-867X>),\n  Justin D. Strait [aut] (ORCID: <https://orcid.org/0000-0003-4356-9443>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DeBoinR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DeBoinR Box-Plots and Outlier Detection for Probability Density\nFunctions Orders a data-set consisting of an ensemble of probability density functions on the same x-grid.  Visualizes a box-plot of these functions based on the notion of distance determined by the user.  Reports outliers based on the distance chosen and the scaling factor for an interquartile range rule.  For further details, see: Alexander C. Murph et al. (2023). \"Visualization and Outlier Detection for Probability Density Function Ensembles.\" <https://sirmurphalot.github.io/publications>.  "
  },
  {
    "id": 2948,
    "package_name": "DescTools",
    "title": "Tools for Descriptive Statistics",
    "description": "A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.",
    "version": "0.99.60",
    "maintainer": "Andri Signorell <andri@signorell.net>",
    "author": "Andri Signorell [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4311-1969>),\n  Ken Aho [ctb],\n  Andreas Alfons [ctb],\n  Nanina Anderegg [ctb],\n  Tomas Aragon [ctb],\n  Chandima Arachchige [ctb],\n  Antti Arppe [ctb],\n  Adrian Baddeley [ctb],\n  Kamil Barton [ctb],\n  Ben Bolker [ctb],\n  Hans W. Borchers [ctb],\n  Frederico Caeiro [ctb],\n  Stephane Champely [ctb],\n  Daniel Chessel [ctb],\n  Leanne Chhay [ctb],\n  Nicholas Cooper [ctb],\n  Clint Cummins [ctb],\n  Michael Dewey [ctb],\n  Harold C. Doran [ctb],\n  Stephane Dray [ctb],\n  Charles Dupont [ctb],\n  Dirk Eddelbuettel [ctb],\n  Claus Ekstrom [ctb],\n  Martin Elff [ctb],\n  Jeff Enos [ctb],\n  Richard W. Farebrother [ctb],\n  John Fox [ctb],\n  Romain Francois [ctb],\n  Michael Friendly [ctb],\n  Tal Galili [ctb],\n  Matthias Gamer [ctb],\n  Joseph L. Gastwirth [ctb],\n  Vilmantas Gegzna [ctb],\n  Yulia R. Gel [ctb],\n  Sereina Graber [ctb],\n  Juergen Gross [ctb],\n  Gabor Grothendieck [ctb],\n  Frank E. Harrell Jr [ctb],\n  Richard Heiberger [ctb],\n  Michael Hoehle [ctb],\n  Christian W. Hoffmann [ctb],\n  Soeren Hojsgaard [ctb],\n  Torsten Hothorn [ctb],\n  Markus Huerzeler [ctb],\n  Wallace W. Hui [ctb],\n  Pete Hurd [ctb],\n  Rob J. Hyndman [ctb],\n  Christopher Jackson [ctb],\n  Matthias Kohl [ctb],\n  Mikko Korpela [ctb],\n  Max Kuhn [ctb],\n  Detlew Labes [ctb],\n  Friederich Leisch [ctb],\n  Jim Lemon [ctb],\n  Dong Li [ctb],\n  Martin Maechler [ctb],\n  Arni Magnusson [ctb],\n  Ben Mainwaring [ctb],\n  Daniel Malter [ctb],\n  George Marsaglia [ctb],\n  John Marsaglia [ctb],\n  Alina Matei [ctb],\n  David Meyer [ctb],\n  Weiwen Miao [ctb],\n  Giovanni Millo [ctb],\n  Yongyi Min [ctb],\n  David Mitchell [ctb],\n  Cyril Flurin Moser [ctb],\n  Franziska Mueller [ctb],\n  Markus Naepflin [ctb],\n  Danielle Navarro [ctb],\n  Henric Nilsson [ctb],\n  Klaus Nordhausen [ctb],\n  Derek Ogle [ctb],\n  Hong Ooi [ctb],\n  Nick Parsons [ctb],\n  Sandrine Pavoine [ctb],\n  Tony Plate [ctb],\n  Luke Prendergast [ctb],\n  Roland Rapold [ctb],\n  William Revelle [ctb],\n  Tyler Rinker [ctb],\n  Brian D. Ripley [ctb],\n  Caroline Rodriguez [ctb],\n  Nathan Russell [ctb],\n  Nick Sabbe [ctb],\n  Ralph Scherer [ctb],\n  Venkatraman E. Seshan [ctb],\n  Michael Smithson [ctb],\n  Greg Snow [ctb],\n  Karline Soetaert [ctb],\n  Werner A. Stahel [ctb],\n  Alec Stephenson [ctb],\n  Mark Stevenson [ctb],\n  Ralf Stubner [ctb],\n  Matthias Templ [ctb],\n  Duncan Temple Lang [ctb],\n  Terry Therneau [ctb],\n  Yves Tille [ctb],\n  Luis Torgo [ctb],\n  Adrian Trapletti [ctb],\n  Joshua Ulrich [ctb],\n  Kevin Ushey [ctb],\n  Jeremy VanDerWal [ctb],\n  Bill Venables [ctb],\n  John Verzani [ctb],\n  Pablo J. Villacorta Iglesias [ctb],\n  Gregory R. Warnes [ctb],\n  Stefan Wellek [ctb],\n  Hadley Wickham [ctb],\n  Rand R. Wilcox [ctb],\n  Peter Wolf [ctb],\n  Daniel Wollschlaeger [ctb],\n  Joseph Wood [ctb],\n  Ying Wu [ctb],\n  Thomas Yee [ctb],\n  Achim Zeileis [ctb]",
    "url": "https://andrisignorell.github.io/DescTools/,\nhttps://github.com/AndriSignorell/DescTools/",
    "bug_reports": "https://github.com/AndriSignorell/DescTools/issues",
    "repository": "https://cran.r-project.org/package=DescTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DescTools Tools for Descriptive Statistics A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.  "
  },
  {
    "id": 2992,
    "package_name": "DisasterAlert",
    "title": "Disaster Alert and Sentiment Analysis",
    "description": "By systematically aggregating and processing textual reports from earthquakes, floods, storms,\n  wildfires, and other natural disasters, the framework enables a holistic assessment of crisis narratives.  \n  Intelligent cleaning and normalization techniques transform raw commentary into structured data, ensuring\n  precise extraction of disaster-specific insights. Collective sentiments of affected communities are  \n  quantitatively scored and qualitatively categorized, providing a multifaceted view of societal responses  \n  under duress. Interactive geographic maps and temporal charts illustrate the evolution and spatial dispersion\n  of emotional reactions and impact indicators. ",
    "version": "1.0.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Hossein Hassani [aut],\n  Nadejda Komendantova [aut],\n  Leila Marvian Mashhad [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DisasterAlert",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DisasterAlert Disaster Alert and Sentiment Analysis By systematically aggregating and processing textual reports from earthquakes, floods, storms,\n  wildfires, and other natural disasters, the framework enables a holistic assessment of crisis narratives.  \n  Intelligent cleaning and normalization techniques transform raw commentary into structured data, ensuring\n  precise extraction of disaster-specific insights. Collective sentiments of affected communities are  \n  quantitatively scored and qualitatively categorized, providing a multifaceted view of societal responses  \n  under duress. Interactive geographic maps and temporal charts illustrate the evolution and spatial dispersion\n  of emotional reactions and impact indicators.   "
  },
  {
    "id": 3013,
    "package_name": "Ditwah",
    "title": "Ditwah Storm Data and Tools for Storm Monitoring and Early\nWarning November 2025, Sri Lanka",
    "description": "\n The Ditwah storm began impacting Sri Lanka on 25 November 2025. Ditwah provides a collection of tidy, well-structured datasets to \n support storm data management, monitoring,  and early warning applications in Sri Lanka. \n The publicly available data were converted to tidy data format for easy analysis.\n The package processes weather data, flood data and situation report data (families affected, etc.).\n The package also includes functions for analyzing river level progression and load dashboard visualizations to enhance situational awareness. This is also developed for educational purposes to support learning in data wrangling, visualization, and disaster analytics.",
    "version": "1.0.0",
    "maintainer": "Thiyanga S. Talagala <ttalagala@sjp.ac.lk>",
    "author": "Thiyanga S. Talagala [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0656-9789>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Ditwah",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Ditwah Ditwah Storm Data and Tools for Storm Monitoring and Early\nWarning November 2025, Sri Lanka \n The Ditwah storm began impacting Sri Lanka on 25 November 2025. Ditwah provides a collection of tidy, well-structured datasets to \n support storm data management, monitoring,  and early warning applications in Sri Lanka. \n The publicly available data were converted to tidy data format for easy analysis.\n The package processes weather data, flood data and situation report data (families affected, etc.).\n The package also includes functions for analyzing river level progression and load dashboard visualizations to enhance situational awareness. This is also developed for educational purposes to support learning in data wrangling, visualization, and disaster analytics.  "
  },
  {
    "id": 3020,
    "package_name": "DoLa",
    "title": "Do Curr\u00edculo Lattes Para o Programa de P\u00f3s-Gradua\u00e7\u00e3o",
    "description": "Managing postgraduate programmes involves extracting information from Lattes CVs. This information can be used for strategic planning and self-evaluation, as well as for producing reports on the Sucupira Platform. Summary reports are produced for each period and course (specialisation, master's and doctorate), showing bibliographic production with and without student participation, as well as papers at events, technical or technological production, ongoing and completed supervision, research projects, exchanges (visiting professor, postdoctoral or short-term leave), awards and general activity indicators. Based on this information, a detailed report is then drawn up for each lecturer, taking into account their participation in exam boards, their research project contributions, their technical collaborations (e.g. advisory committee, editorial board) and the subjects they teach. For more details see Pagliosa and Nascimento (2021) <https://repositorio.ufsc.br/bitstream/handle/123456789/231602/ManualLattesGeociencias11_2021_versaobeta%20%281%29.pdf?sequence=1&isAllowed=y>.",
    "version": "0.1.0",
    "maintainer": "Paulo Pagliosa <paulo.pagliosa@ufsc.br>",
    "author": "Paulo Pagliosa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0834-2534>),\n  Fabiano Peruzzo Schwartz [aut] (ORCID:\n    <https://orcid.org/0000-0003-1727-9346>)",
    "url": "https://github.com/ppagliosa/DoLa",
    "bug_reports": "https://github.com/ppagliosa/DoLa/issues",
    "repository": "https://cran.r-project.org/package=DoLa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DoLa Do Curr\u00edculo Lattes Para o Programa de P\u00f3s-Gradua\u00e7\u00e3o Managing postgraduate programmes involves extracting information from Lattes CVs. This information can be used for strategic planning and self-evaluation, as well as for producing reports on the Sucupira Platform. Summary reports are produced for each period and course (specialisation, master's and doctorate), showing bibliographic production with and without student participation, as well as papers at events, technical or technological production, ongoing and completed supervision, research projects, exchanges (visiting professor, postdoctoral or short-term leave), awards and general activity indicators. Based on this information, a detailed report is then drawn up for each lecturer, taking into account their participation in exam boards, their research project contributions, their technical collaborations (e.g. advisory committee, editorial board) and the subjects they teach. For more details see Pagliosa and Nascimento (2021) <https://repositorio.ufsc.br/bitstream/handle/123456789/231602/ManualLattesGeociencias11_2021_versaobeta%20%281%29.pdf?sequence=1&isAllowed=y>.  "
  },
  {
    "id": 3055,
    "package_name": "DynareR",
    "title": "Bringing the Power of 'Dynare' to 'R', 'R Markdown', and\n'Quarto'",
    "description": "It allows running 'Dynare' program from base R, R Markdown and Quarto. 'Dynare' is a software platform for handling a wide class of economic models, in particular dynamic stochastic general equilibrium ('DSGE') and overlapping generations ('OLG') models.  This package does not only integrate R and Dynare but also serves as a 'Dynare' Knit-Engine for 'knitr' package. The package requires 'Dynare' (<https://www.dynare.org/>) and 'Octave' (<https://www.octave.org/download.html>).  Write all your 'Dynare' commands in R or R Markdown chunk.",
    "version": "0.1.5",
    "maintainer": "Sagiru Mati <sagirumati@gmail.com>",
    "author": "Sagiru Mati [aut, cre] (ORCID: <https://orcid.org/0000-0003-1413-3974>)",
    "url": "https://CRAN.R-project.org/package=DynareR",
    "bug_reports": "https://github.com/sagirumati/DynareR/issues",
    "repository": "https://cran.r-project.org/package=DynareR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DynareR Bringing the Power of 'Dynare' to 'R', 'R Markdown', and\n'Quarto' It allows running 'Dynare' program from base R, R Markdown and Quarto. 'Dynare' is a software platform for handling a wide class of economic models, in particular dynamic stochastic general equilibrium ('DSGE') and overlapping generations ('OLG') models.  This package does not only integrate R and Dynare but also serves as a 'Dynare' Knit-Engine for 'knitr' package. The package requires 'Dynare' (<https://www.dynare.org/>) and 'Octave' (<https://www.octave.org/download.html>).  Write all your 'Dynare' commands in R or R Markdown chunk.  "
  },
  {
    "id": 3060,
    "package_name": "EAVA",
    "title": "Deterministic Verbal Autopsy Coding with Expert Algorithm Verbal\nAutopsy",
    "description": "Expert Algorithm Verbal Autopsy assigns causes of death to 2016 WHO Verbal Autopsy Questionnaire data. odk2EAVA() converts data to a standard input format for cause of death determination building on the work of Thomas (2021) <https://cran.r-project.org/src/contrib/Archive/CrossVA/>. codEAVA() uses the presence and absence of signs and symptoms reported in the Verbal Autopsy interview to diagnose common causes of death. A deterministic algorithm assigns a single cause of death to each Verbal Autopsy interview record using a hierarchy of all common causes for neonates or children 1 to 59 months of age.",
    "version": "1.0.0",
    "maintainer": "Emily Wilson <wilsonem@gmail.com>",
    "author": "Emily Wilson [aut, cre],\n  Henry Kalter [aut],\n  Abhi Datta [aut],\n  Sandipan Pramanik [aut],\n  Robert Black [aut],\n  Gates Foundation [fnd]",
    "url": "",
    "bug_reports": "https://github.com/emilybrownwilson/EAVA/issues",
    "repository": "https://cran.r-project.org/package=EAVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EAVA Deterministic Verbal Autopsy Coding with Expert Algorithm Verbal\nAutopsy Expert Algorithm Verbal Autopsy assigns causes of death to 2016 WHO Verbal Autopsy Questionnaire data. odk2EAVA() converts data to a standard input format for cause of death determination building on the work of Thomas (2021) <https://cran.r-project.org/src/contrib/Archive/CrossVA/>. codEAVA() uses the presence and absence of signs and symptoms reported in the Verbal Autopsy interview to diagnose common causes of death. A deterministic algorithm assigns a single cause of death to each Verbal Autopsy interview record using a hierarchy of all common causes for neonates or children 1 to 59 months of age.  "
  },
  {
    "id": 3157,
    "package_name": "ERPeq",
    "title": "Probabilistic Hazard Assessment",
    "description": "Computes the probability density and cumulative distribution functions of fourteen distributions used for the probabilistic hazard assessment. Estimates the model parameters of the distributions using the maximum likelihood and reports the goodness-of-fit statistics. The recurrence interval estimations of earthquakes are computed for each distribution.",
    "version": "0.1.0",
    "maintainer": "Emrah Altun <emrahaltun123@gmail.com>",
    "author": "Emrah Altun [aut, cre, cph],\n  Gamze Ozel [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ERPeq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ERPeq Probabilistic Hazard Assessment Computes the probability density and cumulative distribution functions of fourteen distributions used for the probabilistic hazard assessment. Estimates the model parameters of the distributions using the maximum likelihood and reports the goodness-of-fit statistics. The recurrence interval estimations of earthquakes are computed for each distribution.  "
  },
  {
    "id": 3170,
    "package_name": "EVI",
    "title": "Epidemic Volatility Index as an Early-Warning Tool",
    "description": "\n    This is an R package implementing the epidemic volatility index (EVI), as \n    discussed by Kostoulas et. al. (2021) and variations by Pateras et. al. (2023). EVI is a new, conceptually simple, early warning tool for oncoming epidemic waves. \n    EVI is based on the volatility of newly reported cases per unit of time,\n    ideally per day, and issues an early warning when the volatility change rate exceeds a threshold.",
    "version": "0.2.0-0",
    "maintainer": "Konstantinos Pateras <kostas.pateras@gmail.com>",
    "author": "Eletherios Meletis [aut],\n  Konstantinos Pateras [aut, cre],\n  Paolo Eusebi [aut],\n  Matt Denwood [aut],\n  Polychronis Kostoulas [aut]",
    "url": "https://www.nature.com/articles/s41598-021-02622-3",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EVI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EVI Epidemic Volatility Index as an Early-Warning Tool \n    This is an R package implementing the epidemic volatility index (EVI), as \n    discussed by Kostoulas et. al. (2021) and variations by Pateras et. al. (2023). EVI is a new, conceptually simple, early warning tool for oncoming epidemic waves. \n    EVI is based on the volatility of newly reported cases per unit of time,\n    ideally per day, and issues an early warning when the volatility change rate exceeds a threshold.  "
  },
  {
    "id": 3203,
    "package_name": "EdSurvey",
    "title": "Analysis of NCES Education Survey and Assessment Data",
    "description": "Read in and analyze functions for education survey and assessment data from the National Center for Education Statistics (NCES) <https://nces.ed.gov/>, including National Assessment of Educational Progress (NAEP) data <https://nces.ed.gov/nationsreportcard/> and data from the International Assessment Database: Organisation for Economic Co-operation and Development (OECD) <https://www.oecd.org/>, including Programme for International Student Assessment (PISA), Teaching and Learning International Survey (TALIS), Programme for the International Assessment of Adult Competencies (PIAAC), and International Association for the Evaluation of Educational Achievement (IEA) <https://www.iea.nl/>, including Trends in International Mathematics and Science Study (TIMSS), TIMSS Advanced, Progress in International Reading Literacy Study (PIRLS), International Civic and Citizenship Study (ICCS), International Computer and Information Literacy Study (ICILS), and Civic Education Study (CivEd).",
    "version": "4.0.7",
    "maintainer": "Paul Bailey <pbailey@air.org>",
    "author": "Paul Bailey [aut, cre] (ORCID: <https://orcid.org/0000-0003-0989-8729>),\n  Ahmad Emad [aut],\n  Huade Huo [aut] (ORCID: <https://orcid.org/0009-0004-5014-646X>),\n  Michael Lee [aut] (ORCID: <https://orcid.org/0009-0006-0959-787X>),\n  Yuqi Liao [aut] (ORCID: <https://orcid.org/0000-0001-9359-6015>),\n  Alex Lishinski [aut] (ORCID: <https://orcid.org/0000-0003-4506-1600>),\n  Trang Nguyen [aut] (ORCID: <https://orcid.org/0009-0001-0167-8775>),\n  Qingshu Xie [aut],\n  Jiao Yu [aut],\n  Ting Zhang [aut] (ORCID: <https://orcid.org/0009-0001-1724-6141>),\n  Eric Buehler [aut] (ORCID: <https://orcid.org/0009-0004-6354-2015>),\n  Sun-joo Lee [aut],\n  Blue Webb [aut] (ORCID: <https://orcid.org/0009-0004-4080-9864>),\n  Thomas Fink [aut] (ORCID: <https://orcid.org/0009-0003-9308-2833>),\n  Emmanuel Sikali [pdr],\n  Claire Kelley [ctb],\n  Jeppe Bundsgaard [ctb],\n  Ren C'deBaca [ctb],\n  Anders Astrup Christensen [ctb]",
    "url": "https://www.air.org/project/nces-data-r-project-edsurvey",
    "bug_reports": "https://github.com/American-Institutes-for-Research/EdSurvey/issues",
    "repository": "https://cran.r-project.org/package=EdSurvey",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EdSurvey Analysis of NCES Education Survey and Assessment Data Read in and analyze functions for education survey and assessment data from the National Center for Education Statistics (NCES) <https://nces.ed.gov/>, including National Assessment of Educational Progress (NAEP) data <https://nces.ed.gov/nationsreportcard/> and data from the International Assessment Database: Organisation for Economic Co-operation and Development (OECD) <https://www.oecd.org/>, including Programme for International Student Assessment (PISA), Teaching and Learning International Survey (TALIS), Programme for the International Assessment of Adult Competencies (PIAAC), and International Association for the Evaluation of Educational Achievement (IEA) <https://www.iea.nl/>, including Trends in International Mathematics and Science Study (TIMSS), TIMSS Advanced, Progress in International Reading Literacy Study (PIRLS), International Civic and Citizenship Study (ICCS), International Computer and Information Literacy Study (ICILS), and Civic Education Study (CivEd).  "
  },
  {
    "id": 3234,
    "package_name": "EnvStats",
    "title": "Package for Environmental Statistics, Including US EPA Guidance",
    "description": "Graphical and statistical analyses of environmental data, with \n  focus on analyzing chemical concentrations and physical parameters, usually in \n  the context of mandated environmental monitoring.  Major environmental \n  statistical methods found in the literature and regulatory guidance documents, \n  with extensive help that explains what these methods do, how to use them, \n  and where to find them in the literature.  Numerous built-in data sets from \n  regulatory guidance documents and environmental statistics literature.  Includes \n  scripts reproducing analyses presented in the book \"EnvStats:  An R Package for \n  Environmental Statistics\" (Millard, 2013, Springer, ISBN 978-1-4614-8455-4, \n  <doi:10.1007/978-1-4614-8456-1>).",
    "version": "3.1.0",
    "maintainer": "Alexander Kowarik <alexander.kowarik@statistik.gv.at>",
    "author": "Steven P. Millard [aut],\n  Alexander Kowarik [ctb, cre] (ORCID:\n    <https://orcid.org/0000-0001-8598-4130>)",
    "url": "https://github.com/alexkowa/EnvStats,\nhttps://alexkowa.github.io/EnvStats/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EnvStats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EnvStats Package for Environmental Statistics, Including US EPA Guidance Graphical and statistical analyses of environmental data, with \n  focus on analyzing chemical concentrations and physical parameters, usually in \n  the context of mandated environmental monitoring.  Major environmental \n  statistical methods found in the literature and regulatory guidance documents, \n  with extensive help that explains what these methods do, how to use them, \n  and where to find them in the literature.  Numerous built-in data sets from \n  regulatory guidance documents and environmental statistics literature.  Includes \n  scripts reproducing analyses presented in the book \"EnvStats:  An R Package for \n  Environmental Statistics\" (Millard, 2013, Springer, ISBN 978-1-4614-8455-4, \n  <doi:10.1007/978-1-4614-8456-1>).  "
  },
  {
    "id": 3247,
    "package_name": "EpiReport",
    "title": "Epidemiological Report",
    "description": "Drafting an epidemiological report in 'Microsoft Word' format for a given disease,\n  similar to the Annual Epidemiological Reports published by the European Centre \n  for Disease Prevention and Control. Through standalone functions, it is specifically \n  designed to generate each disease specific output presented in these reports and includes:\n  - Table with the distribution of cases by Member State over the last five years;\n  - Seasonality plot with the distribution of cases at the European Union / European Economic Area level, \n  by month, over the past five years;\n  - Trend plot with the trend and number of cases at the European Union / European Economic Area level, \n  by month, over the past five years;\n  - Age and gender bar graph with the distribution of cases at the European Union / European Economic Area level.\n  Two types of datasets can be used:\n  - The default dataset of dengue 2015-2019 data;\n  - Any dataset specified as described in the vignette.",
    "version": "1.0.4",
    "maintainer": "Lore Merdrignac <l.merdrignac@epiconcept.fr>",
    "author": "Lore Merdrignac [aut, ctr, cre] (Author of the package and original\n    code),\n  Tommi Karki [aut, fnd],\n  Esther Kissling [aut, ctr],\n  Joana Gomes Dias [aut, fnd] (Project manager)",
    "url": "https://www.ecdc.europa.eu/en/publications-data/monitoring/all-annual-epidemiological-reports\nhttps://epiconcept-paris.github.io/EpiReport/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EpiReport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpiReport Epidemiological Report Drafting an epidemiological report in 'Microsoft Word' format for a given disease,\n  similar to the Annual Epidemiological Reports published by the European Centre \n  for Disease Prevention and Control. Through standalone functions, it is specifically \n  designed to generate each disease specific output presented in these reports and includes:\n  - Table with the distribution of cases by Member State over the last five years;\n  - Seasonality plot with the distribution of cases at the European Union / European Economic Area level, \n  by month, over the past five years;\n  - Trend plot with the trend and number of cases at the European Union / European Economic Area level, \n  by month, over the past five years;\n  - Age and gender bar graph with the distribution of cases at the European Union / European Economic Area level.\n  Two types of datasets can be used:\n  - The default dataset of dengue 2015-2019 data;\n  - Any dataset specified as described in the vignette.  "
  },
  {
    "id": 3249,
    "package_name": "EpiSignalDetection",
    "title": "Signal Detection Analysis",
    "description": "Exploring time series for signal detection. It is specifically designed\n    to detect possible outbreaks using infectious disease surveillance data\n    at the European Union / European Economic Area or country level.\n    Automatic detection tools used are presented in the paper\n    \"Monitoring count time series in R: aberration detection in public health surveillance\",\n    by Salmon (2016) <doi:10.18637/jss.v070.i10>.\n    The package includes:\n    - Signal Detection tool, an interactive 'shiny' application\n      in which the user can import external data and perform basic signal detection analyses;\n    - An automated report in HTML format, presenting the results of the time series analysis in tables and graphs.\n      This report can also be stratified by population characteristics (see 'Population' variable).\n    This project was funded by the European Centre for Disease Prevention and Control.",
    "version": "0.1.2",
    "maintainer": "Lore Merdrignac <l.merdrignac@epiconcept.fr>",
    "author": "Lore Merdrignac [aut, ctr, cre] (Author of the package and original\n    code),\n  Joana Gomes Dias [aut, fnd] (Project manager),\n  Esther Kissling [aut, ctr],\n  Tommi Karki [aut, fnd],\n  Margot Einoder-Moreno [ctb, fnd]",
    "url": "https://github.com/EU-ECDC/EpiSignalDetection",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EpiSignalDetection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpiSignalDetection Signal Detection Analysis Exploring time series for signal detection. It is specifically designed\n    to detect possible outbreaks using infectious disease surveillance data\n    at the European Union / European Economic Area or country level.\n    Automatic detection tools used are presented in the paper\n    \"Monitoring count time series in R: aberration detection in public health surveillance\",\n    by Salmon (2016) <doi:10.18637/jss.v070.i10>.\n    The package includes:\n    - Signal Detection tool, an interactive 'shiny' application\n      in which the user can import external data and perform basic signal detection analyses;\n    - An automated report in HTML format, presenting the results of the time series analysis in tables and graphs.\n      This report can also be stratified by population characteristics (see 'Population' variable).\n    This project was funded by the European Centre for Disease Prevention and Control.  "
  },
  {
    "id": 3280,
    "package_name": "EviewsR",
    "title": "A Seamless Integration of 'EViews' and R",
    "description": "It allows running 'EViews' (<https://eviews.com>) program from R, R Markdown and Quarto documents. 'EViews' (Econometric Views) is a statistical software for Econometric analysis.  This package integrates 'EViews' and R and also serves as an 'EViews' Knit-Engine for 'knitr' package. Write all your 'EViews' commands in R, R Markdown or Quarto documents. For details,  please consult our peer-review article Mati S., Civcir I. and Abba S.I (2023) <doi:10.32614/RJ-2023-045>.",
    "version": "0.1.6",
    "maintainer": "Sagiru Mati <sagirumati@gmail.com>",
    "author": "Sagiru Mati [aut, cre] (ORCID: <https://orcid.org/0000-0003-1413-3974>)",
    "url": "https://CRAN.R-project.org/package=EviewsR",
    "bug_reports": "https://github.com/sagirumati/EviewsR/issues",
    "repository": "https://cran.r-project.org/package=EviewsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EviewsR A Seamless Integration of 'EViews' and R It allows running 'EViews' (<https://eviews.com>) program from R, R Markdown and Quarto documents. 'EViews' (Econometric Views) is a statistical software for Econometric analysis.  This package integrates 'EViews' and R and also serves as an 'EViews' Knit-Engine for 'knitr' package. Write all your 'EViews' commands in R, R Markdown or Quarto documents. For details,  please consult our peer-review article Mati S., Civcir I. and Abba S.I (2023) <doi:10.32614/RJ-2023-045>.  "
  },
  {
    "id": 3300,
    "package_name": "ExclusionTable",
    "title": "Creating Tables of Excluded Observations",
    "description": "Instead of counting observations before and after a subset() \n    call, the ExclusionTable() function reports the number before and after \n    each subset() call together with the number of observations that have been \n    excluded. This is especially useful in observational studies for keeping \n    track how many observations have been excluded for each in-/ or \n    exclusion criteria. You just need to provide ExclusionTable() with a \n    dataset and a list of logical filter statements.",
    "version": "1.2.0",
    "maintainer": "Joshua P. Entrop <joshuaentrop@posteo.de>",
    "author": "Joshua P. Entrop [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1614-8096>),\n  Simon Steiger [ctb]",
    "url": "https://github.com/entjos/ExclusionTable/,\nhttps://entjos.github.io/ExclusionTable/",
    "bug_reports": "https://github.com/entjos/ExclusionTable/issues/",
    "repository": "https://cran.r-project.org/package=ExclusionTable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExclusionTable Creating Tables of Excluded Observations Instead of counting observations before and after a subset() \n    call, the ExclusionTable() function reports the number before and after \n    each subset() call together with the number of observations that have been \n    excluded. This is especially useful in observational studies for keeping \n    track how many observations have been excluded for each in-/ or \n    exclusion criteria. You just need to provide ExclusionTable() with a \n    dataset and a list of logical filter statements.  "
  },
  {
    "id": 3320,
    "package_name": "EzGP",
    "title": "Easy-to-Interpret Gaussian Process Models for Computer\nExperiments",
    "description": "Fit model for datasets with easy-to-interpret Gaussian process modeling, predict responses for new inputs.\n    The input variables of the datasets can be quantitative, qualitative/categorical or mixed.\n    The output variable of the datasets is a scalar (quantitative).\n    The optimization of the likelihood function can be chosen by the users (see the documentation of EzGP_fit()).\n    The modeling method is published in \"EzGP: Easy-to-Interpret Gaussian Process Models for Computer Experiments with Both Quantitative and Qualitative Factors\" \n    by Qian Xiao, Abhyuday Mandal, C. Devon Lin, and Xinwei Deng (2022) <doi:10.1137/19M1288462>. ",
    "version": "0.1.0",
    "maintainer": "Jiayi Li <jiayili0123@outlook.com>",
    "author": "Jiayi Li [cre, aut],\n  Qian Xiao [aut],\n  Abhyuday Mandal [aut],\n  C. Devon Lin [aut],\n  Xinwei Deng [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EzGP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EzGP Easy-to-Interpret Gaussian Process Models for Computer\nExperiments Fit model for datasets with easy-to-interpret Gaussian process modeling, predict responses for new inputs.\n    The input variables of the datasets can be quantitative, qualitative/categorical or mixed.\n    The output variable of the datasets is a scalar (quantitative).\n    The optimization of the likelihood function can be chosen by the users (see the documentation of EzGP_fit()).\n    The modeling method is published in \"EzGP: Easy-to-Interpret Gaussian Process Models for Computer Experiments with Both Quantitative and Qualitative Factors\" \n    by Qian Xiao, Abhyuday Mandal, C. Devon Lin, and Xinwei Deng (2022) <doi:10.1137/19M1288462>.   "
  },
  {
    "id": 3407,
    "package_name": "FRESA.CAD",
    "title": "Feature Selection Algorithms for Computer Aided Diagnosis",
    "description": "Contains a set of utilities for building and testing statistical models (linear, logistic,ordinal or COX) for Computer Aided Diagnosis/Prognosis applications. Utilities include data adjustment, univariate analysis, model building, model-validation, longitudinal analysis, reporting and visualization.",
    "version": "3.4.8",
    "maintainer": "Jose Gerardo Tamez-Pena <jose.tamezpena@tec.mx>",
    "author": "Jose Gerardo Tamez-Pena, Antonio Martinez-Torteya, Israel Alanis and Jorge Orozco",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FRESA.CAD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FRESA.CAD Feature Selection Algorithms for Computer Aided Diagnosis Contains a set of utilities for building and testing statistical models (linear, logistic,ordinal or COX) for Computer Aided Diagnosis/Prognosis applications. Utilities include data adjustment, univariate analysis, model building, model-validation, longitudinal analysis, reporting and visualization.  "
  },
  {
    "id": 3430,
    "package_name": "FactoInvestigate",
    "title": "Automatic Description of Factorial Analysis",
    "description": "Brings a set of tools to help and automatically realise the description of principal component analyses (from 'FactoMineR' functions). Detection of existing outliers, identification of the informative components, graphical views and dimensions description are performed threw dedicated functions. The Investigate() function performs all these functions in one, and returns the result as a report document (Word, PDF or HTML).",
    "version": "1.9",
    "maintainer": "Francois Husson <francois.husson@institut-agro.fr>",
    "author": "Simon Thuleau, Francois Husson",
    "url": "http://factominer.free.fr/reporting/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FactoInvestigate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FactoInvestigate Automatic Description of Factorial Analysis Brings a set of tools to help and automatically realise the description of principal component analyses (from 'FactoMineR' functions). Detection of existing outliers, identification of the informative components, graphical views and dimensions description are performed threw dedicated functions. The Investigate() function performs all these functions in one, and returns the result as a report document (Word, PDF or HTML).  "
  },
  {
    "id": 3474,
    "package_name": "FilmsGmooG",
    "title": "IMDb Film Ratings from the Summer of 2022",
    "description": "Average rating and number of votes reported by IMDb for films and shorts with over 100 votes in 2022.  The data are analysed in Chapter 3 of the Book 'Getting (more out of) Graphics' (Antony Unwin, CRC Press 2024).",
    "version": "0.1.0",
    "maintainer": "Antony Unwin <unwin@math.uni-augsburg.de>",
    "author": "Antony Unwin [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FilmsGmooG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FilmsGmooG IMDb Film Ratings from the Summer of 2022 Average rating and number of votes reported by IMDb for films and shorts with over 100 votes in 2022.  The data are analysed in Chapter 3 of the Book 'Getting (more out of) Graphics' (Antony Unwin, CRC Press 2024).  "
  },
  {
    "id": 3498,
    "package_name": "FlexScan",
    "title": "Flexible Scan Statistics",
    "description": "An easy way to conduct flexible scan.\n    Monte-Carlo method is used to test the spatial clusters given the cases, population, and shapefile.\n    A table with formal style and a map with clusters are included in the result report.\n    The method can be referenced at: Toshiro Tango and Kunihiko Takahashi (2005) <doi:10.1186/1476-072X-4-11>.",
    "version": "0.2.2",
    "maintainer": "Zhicheng Du <dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FlexScan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FlexScan Flexible Scan Statistics An easy way to conduct flexible scan.\n    Monte-Carlo method is used to test the spatial clusters given the cases, population, and shapefile.\n    A table with formal style and a map with clusters are included in the result report.\n    The method can be referenced at: Toshiro Tango and Kunihiko Takahashi (2005) <doi:10.1186/1476-072X-4-11>.  "
  },
  {
    "id": 3511,
    "package_name": "ForIT",
    "title": "Functions to Estimate Tree Volume and Phytomass in the Italian\nForest Inventory 2005",
    "description": "Tabacchi et al. (2011) published a very detailed study producing a uniform system of functions to estimate tree volume and\n    phytomass components (stem, branches, stool). The estimates of the 2005 Italian forest inventory (<https://www.inventarioforestale.org/it/>) \n    are based on these functions. The study documents the domain of applicability of each function and the equations to quantify estimates\n    accuracies for individual estimates as well as for aggregated estimates. This package makes the functions available in the R environment. \n    Version 2 exposes two distinct functions for individual and summary estimates. To facilitate access to the functions, tree species \n    identification is now based on EPPO species codes (<https://data.eppo.int/>).",
    "version": "2.5.2",
    "maintainer": "Nicola Puletti <nicola.puletti@gmail.com>",
    "author": "Nicola Puletti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2142-959X>),\n  Mirko Grotti [aut],\n  Roberto Scotti [aut] (ORCID: <https://orcid.org/0000-0001-7394-4473>)",
    "url": "https://gitlab.com/NuoroForestrySchool/ForIT.git,\nhttps://zenodo.org/records/5790157",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ForIT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ForIT Functions to Estimate Tree Volume and Phytomass in the Italian\nForest Inventory 2005 Tabacchi et al. (2011) published a very detailed study producing a uniform system of functions to estimate tree volume and\n    phytomass components (stem, branches, stool). The estimates of the 2005 Italian forest inventory (<https://www.inventarioforestale.org/it/>) \n    are based on these functions. The study documents the domain of applicability of each function and the equations to quantify estimates\n    accuracies for individual estimates as well as for aggregated estimates. This package makes the functions available in the R environment. \n    Version 2 exposes two distinct functions for individual and summary estimates. To facilitate access to the functions, tree species \n    identification is now based on EPPO species codes (<https://data.eppo.int/>).  "
  },
  {
    "id": 3517,
    "package_name": "ForecastingEnsembles",
    "title": "Time Series Forecasting Using 23 Individual Models",
    "description": "Runs multiple individual time series models, and combines them into an ensembles of time series models. This is mainly used to predict the results of the monthly labor market report from the \n    United States Bureau of Labor Statistics for virtually any part of the economy reported by the Bureau of Labor Statistics, but it can be easily modified to work with other types of time series data.\n    For example, the package was used to predict the winning men's and women's time for the 2024 London Marathon.",
    "version": "0.5.1",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "https://github.com/InfiniteCuriosity/ForecastingEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/ForecastingEnsembles/issues",
    "repository": "https://cran.r-project.org/package=ForecastingEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ForecastingEnsembles Time Series Forecasting Using 23 Individual Models Runs multiple individual time series models, and combines them into an ensembles of time series models. This is mainly used to predict the results of the monthly labor market report from the \n    United States Bureau of Labor Statistics for virtually any part of the economy reported by the Bureau of Labor Statistics, but it can be easily modified to work with other types of time series data.\n    For example, the package was used to predict the winning men's and women's time for the 2024 London Marathon.  "
  },
  {
    "id": 3519,
    "package_name": "ForestElementsR",
    "title": "Data Structures and Functions for Working with Forest Data",
    "description": "Provides generic data structures and algorithms for use with forest\n    mensuration data in a consistent framework. The functions and objects\n    included are a collection of broadly applicable tools. More specialized \n    applications should be implemented in separate packages that build on this\n    foundation. Documentation about 'ForestElementsR' is provided by three\n    vignettes included in this package. For an introduction to the field of \n    forest mensuration, refer to the textbooks by Kershaw et al. (2017)\n    <doi:10.1002/9781118902028>, and van Laar and Akca (2007)\n    <doi:10.1007/978-1-4020-5991-9>.",
    "version": "2.2.0",
    "maintainer": "Peter Biber <p.biber@tum.de>",
    "author": "Peter Biber [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9700-8708>),\n  Astor Tora\u00f1o Caicoya [aut] (ORCID:\n    <https://orcid.org/0000-0002-9658-8990>),\n  Torben Hilmers [ctb] (ORCID: <https://orcid.org/0000-0002-4982-8867>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ForestElementsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ForestElementsR Data Structures and Functions for Working with Forest Data Provides generic data structures and algorithms for use with forest\n    mensuration data in a consistent framework. The functions and objects\n    included are a collection of broadly applicable tools. More specialized \n    applications should be implemented in separate packages that build on this\n    foundation. Documentation about 'ForestElementsR' is provided by three\n    vignettes included in this package. For an introduction to the field of \n    forest mensuration, refer to the textbooks by Kershaw et al. (2017)\n    <doi:10.1002/9781118902028>, and van Laar and Akca (2007)\n    <doi:10.1007/978-1-4020-5991-9>.  "
  },
  {
    "id": 3573,
    "package_name": "GADAG",
    "title": "A Genetic Algorithm for Learning Directed Acyclic Graphs",
    "description": "Sparse large Directed Acyclic Graphs learning with a combination of a convex program and a tailored genetic algorithm (see Champion et al. (2017) <https://hal.archives-ouvertes.fr/hal-01172745v2/document>). ",
    "version": "0.99.0",
    "maintainer": "Magali Champion <magali.champion@parisdescartes.fr>",
    "author": "Magali Champion, Victor Picheny and Matthieu Vignes",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GADAG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GADAG A Genetic Algorithm for Learning Directed Acyclic Graphs Sparse large Directed Acyclic Graphs learning with a combination of a convex program and a tailored genetic algorithm (see Champion et al. (2017) <https://hal.archives-ouvertes.fr/hal-01172745v2/document>).   "
  },
  {
    "id": 3589,
    "package_name": "GB2",
    "title": "Generalized Beta Distribution of the Second Kind: Properties,\nLikelihood, Estimation",
    "description": "The GB2 package explores the Generalized Beta distribution of the second kind. Density, cumulative distribution function, quantiles and moments of the distribution are given. Functions for the full log-likelihood, the profile log-likelihood and the scores are provided. Formulas for various indicators of inequality and poverty under the GB2 are implemented. The GB2 is fitted by the methods of maximum pseudo-likelihood estimation using the full and profile log-likelihood, and non-linear least squares estimation of the model parameters. Various plots for the visualization and analysis of the results are provided. Variance estimation of the parameters is provided for the method of maximum pseudo-likelihood estimation. A mixture distribution based on the compounding property of the GB2 is presented (denoted as \"compound\" in the documentation). This mixture distribution is based on the discretization of the distribution of the underlying random scale parameter. The discretization can be left or right tail. Density, cumulative distribution function, moments and quantiles for the mixture distribution are provided. The compound mixture distribution is fitted using the method of maximum pseudo-likelihood estimation. The fit can also incorporate the use of auxiliary information. In this new version of the package, the mixture case is complemented with new functions for variance estimation by linearization and comparative density plots. ",
    "version": "2.1.2",
    "maintainer": "Desislava Nedyalkova <desislava.nedyalkova@gmail.com>",
    "author": "Monique Graf [aut],\n  Desislava Nedyalkova. [aut],\n  Desislava Nedyalkova [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GB2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GB2 Generalized Beta Distribution of the Second Kind: Properties,\nLikelihood, Estimation The GB2 package explores the Generalized Beta distribution of the second kind. Density, cumulative distribution function, quantiles and moments of the distribution are given. Functions for the full log-likelihood, the profile log-likelihood and the scores are provided. Formulas for various indicators of inequality and poverty under the GB2 are implemented. The GB2 is fitted by the methods of maximum pseudo-likelihood estimation using the full and profile log-likelihood, and non-linear least squares estimation of the model parameters. Various plots for the visualization and analysis of the results are provided. Variance estimation of the parameters is provided for the method of maximum pseudo-likelihood estimation. A mixture distribution based on the compounding property of the GB2 is presented (denoted as \"compound\" in the documentation). This mixture distribution is based on the discretization of the distribution of the underlying random scale parameter. The discretization can be left or right tail. Density, cumulative distribution function, moments and quantiles for the mixture distribution are provided. The compound mixture distribution is fitted using the method of maximum pseudo-likelihood estimation. The fit can also incorporate the use of auxiliary information. In this new version of the package, the mixture case is complemented with new functions for variance estimation by linearization and comparative density plots.   "
  },
  {
    "id": 3590,
    "package_name": "GB2group",
    "title": "Estimation of the Generalised Beta Distribution of the Second\nKind from Grouped Data",
    "description": "Estimation of the generalized beta distribution of the second \n    kind (GB2) and related models using grouped data in form of income shares. \n    The GB2 family is a general class of distributions that provides an accurate \n    fit to income data. 'GB2group' includes functions to estimate the GB2, the \n    Singh-Maddala, the Dagum, the Beta 2, the Lognormal and the Fisk distributions. \n    'GB2group' deploys two different econometric strategies to estimate these \n    parametric distributions, the equally weighted minimum distance (EWMD) estimator and the  \n    optimally weighted minimum distance (OMD) estimator. Asymptotic standard errors are reported for the \n    OMD estimates. Standard errors of the EWMD estimates are obtained by Monte \n    Carlo simulation. See Jorda et al. (2018) <arXiv:1808.09831> for a detailed \n    description of the estimation procedure. ",
    "version": "0.3.0",
    "maintainer": "Vanesa Jorda <jordav@unican.es>",
    "author": "Vanesa Jorda <jordav@unican.es>, Jose Maria Sarabia\n    <jose.sarabia@unican.es>, Markus J\u00e4ntti <markus.jantti@sofi.su.se>.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GB2group",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GB2group Estimation of the Generalised Beta Distribution of the Second\nKind from Grouped Data Estimation of the generalized beta distribution of the second \n    kind (GB2) and related models using grouped data in form of income shares. \n    The GB2 family is a general class of distributions that provides an accurate \n    fit to income data. 'GB2group' includes functions to estimate the GB2, the \n    Singh-Maddala, the Dagum, the Beta 2, the Lognormal and the Fisk distributions. \n    'GB2group' deploys two different econometric strategies to estimate these \n    parametric distributions, the equally weighted minimum distance (EWMD) estimator and the  \n    optimally weighted minimum distance (OMD) estimator. Asymptotic standard errors are reported for the \n    OMD estimates. Standard errors of the EWMD estimates are obtained by Monte \n    Carlo simulation. See Jorda et al. (2018) <arXiv:1808.09831> for a detailed \n    description of the estimation procedure.   "
  },
  {
    "id": 3649,
    "package_name": "GGenemy",
    "title": "Audit 'ggplot2' Visualizations for Accessibility and Best\nPractices",
    "description": "Audits 'ggplot2' visualizations for accessibility issues, misleading \n    practices, and readability problems. Checks for color accessibility concerns \n    including colorblind-unfriendly palettes, misleading scale manipulations such \n    as truncated axes and dual y-axes, text readability issues like small fonts \n    and overlapping labels, and general accessibility barriers. Provides \n    comprehensive audit reports with actionable suggestions for improvement.\n    Color vision deficiency simulation uses methods from the 'colorspace' \n    package Zeileis et al. (2020) <doi:10.18637/jss.v096.i01>. Contrast \n    calculations follow WCAG 2.1 guidelines (W3C 2018 \n    <https://www.w3.org/WAI/WCAG21/Understanding/contrast-minimum>).",
    "version": "0.1.0",
    "maintainer": "Andy Man Yeung Tai <andy.tai@stat.ubc.ca>",
    "author": "Andy Man Yeung Tai [aut, cre]",
    "url": "https://github.com/andytai7/GGenemy",
    "bug_reports": "https://github.com/andytai7/GGenemy/issues",
    "repository": "https://cran.r-project.org/package=GGenemy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GGenemy Audit 'ggplot2' Visualizations for Accessibility and Best\nPractices Audits 'ggplot2' visualizations for accessibility issues, misleading \n    practices, and readability problems. Checks for color accessibility concerns \n    including colorblind-unfriendly palettes, misleading scale manipulations such \n    as truncated axes and dual y-axes, text readability issues like small fonts \n    and overlapping labels, and general accessibility barriers. Provides \n    comprehensive audit reports with actionable suggestions for improvement.\n    Color vision deficiency simulation uses methods from the 'colorspace' \n    package Zeileis et al. (2020) <doi:10.18637/jss.v096.i01>. Contrast \n    calculations follow WCAG 2.1 guidelines (W3C 2018 \n    <https://www.w3.org/WAI/WCAG21/Understanding/contrast-minimum>).  "
  },
  {
    "id": 3657,
    "package_name": "GIFTr",
    "title": "GIFT Questions Format Generator from Dataframes",
    "description": "A framework and functions to create 'MOODLE' quizzes. 'GIFTr' takes dataframe of questions of\n    four types: multiple choices, numerical, true or false and short answer questions, and exports a text \n    file formatted in 'MOODLE' GIFT format. You can prepare a spreadsheet in any software and import \n    it into R to generate any number of questions with 'HTML', 'markdown' and 'LaTeX' support.",
    "version": "0.1.0",
    "maintainer": "Omar I. Elashkar <omar.ibrahim@miuegypt.edu.eg>",
    "author": "Omar I. Elashkar",
    "url": "https://github.com/omarelashkar/GIFTr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GIFTr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GIFTr GIFT Questions Format Generator from Dataframes A framework and functions to create 'MOODLE' quizzes. 'GIFTr' takes dataframe of questions of\n    four types: multiple choices, numerical, true or false and short answer questions, and exports a text \n    file formatted in 'MOODLE' GIFT format. You can prepare a spreadsheet in any software and import \n    it into R to generate any number of questions with 'HTML', 'markdown' and 'LaTeX' support.  "
  },
  {
    "id": 3695,
    "package_name": "GOFShiny",
    "title": "Interactive Document for Working with Goodness of Fit Analysis",
    "description": "An interactive document on  the topic of goodness of fit analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://predanalyticssessions1.shinyapps.io/ChiSquareGOF/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GOFShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GOFShiny Interactive Document for Working with Goodness of Fit Analysis An interactive document on  the topic of goodness of fit analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://predanalyticssessions1.shinyapps.io/ChiSquareGOF/>.  "
  },
  {
    "id": 3756,
    "package_name": "GTAPViz",
    "title": "Automating 'GTAP' Data Processing and Visualization",
    "description": "Tools to streamline the extraction, processing, and visualization of\n    Computable General Equilibrium (CGE) results from 'GTAP' models. Designed for\n    compatibility with both .har and .sl4 files, the package enables users to\n    automate data preparation, apply mapping metadata, and generate high-quality\n    plots and summary tables with minimal coding. 'GTAPViz' supports flexible export\n    options (e.g., Text, CSV, 'Stata', or 'Excel' formats).\n    This facilitates efficient post-simulation analysis for\n    economic research and policy reporting. Includes helper functions to filter,\n    format, and customize outputs with reproducible styling.",
    "version": "1.1.3",
    "maintainer": "Pattawee Puangchit <ppuangch@purdue.edu>",
    "author": "Pattawee Puangchit [aut, cre]",
    "url": "https://bodysbobb.github.io/GTAPViz/",
    "bug_reports": "https://github.com/bodysbobb/GTAPViz/issues/",
    "repository": "https://cran.r-project.org/package=GTAPViz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GTAPViz Automating 'GTAP' Data Processing and Visualization Tools to streamline the extraction, processing, and visualization of\n    Computable General Equilibrium (CGE) results from 'GTAP' models. Designed for\n    compatibility with both .har and .sl4 files, the package enables users to\n    automate data preparation, apply mapping metadata, and generate high-quality\n    plots and summary tables with minimal coding. 'GTAPViz' supports flexible export\n    options (e.g., Text, CSV, 'Stata', or 'Excel' formats).\n    This facilitates efficient post-simulation analysis for\n    economic research and policy reporting. Includes helper functions to filter,\n    format, and customize outputs with reproducible styling.  "
  },
  {
    "id": 3771,
    "package_name": "GWASinspector",
    "title": "Comprehensive and Easy to Use Quality Control of GWAS Results",
    "description": "When evaluating the results of a genome-wide association study (GWAS), it is important to perform a quality control to ensure that the results are valid, complete, correctly formatted, and, in case of meta-analysis, consistent with other studies that have applied the same analysis. This package was developed to facilitate and streamline this process and provide the user with a comprehensive report. ",
    "version": "1.7.3",
    "maintainer": "Alireza Ani <a.ani@umcg.nl>",
    "author": "Alireza Ani [aut, cre],\n  Peter J. van der Most [aut],\n  Ahmad Vaez [aut],\n  Ilja M. Nolte [aut]",
    "url": "https://GWASinspector.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GWASinspector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GWASinspector Comprehensive and Easy to Use Quality Control of GWAS Results When evaluating the results of a genome-wide association study (GWAS), it is important to perform a quality control to ensure that the results are valid, complete, correctly formatted, and, in case of meta-analysis, consistent with other studies that have applied the same analysis. This package was developed to facilitate and streamline this process and provide the user with a comprehensive report.   "
  },
  {
    "id": 3806,
    "package_name": "GenderInfer",
    "title": "This is a Collection of Functions to Analyse Gender Differences",
    "description": "Implementation of functions, which combines binomial calculation \n    and data visualisation, to analyse the differences in publishing authorship\n    by gender described in Day et al. (2020) <doi:10.1039/C9SC04090K>.\n    It should only be used when self-reported gender is unavailable.",
    "version": "0.1.0",
    "maintainer": "Rita Giordano <giordanor@rsc.org>",
    "author": "Rita Giordano [aut, cre],\n  Aileen Day [aut],\n  John Boyle [aut],\n  Colin Batchelor [ctb],\n  Royal Society of Chemistry [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GenderInfer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GenderInfer This is a Collection of Functions to Analyse Gender Differences Implementation of functions, which combines binomial calculation \n    and data visualisation, to analyse the differences in publishing authorship\n    by gender described in Day et al. (2020) <doi:10.1039/C9SC04090K>.\n    It should only be used when self-reported gender is unavailable.  "
  },
  {
    "id": 3840,
    "package_name": "GetDFPData",
    "title": "Reading Annual Financial Reports from Bovespa's DFP, FRE and FCA\nSystem",
    "description": "Reads annual financial reports including assets, liabilities, dividends history, stockholder composition and much more from Bovespa's DFP, FRE and FCA systems <http://www.b3.com.br/pt_br/produtos-e-servicos/negociacao/renda-variavel/empresas-listadas.htm>.\n These are web based interfaces for all financial reports of companies traded at Bovespa. The package is specially designed for large scale data importation, keeping a tabular (long) structure for easier processing.  ",
    "version": "1.6",
    "maintainer": "Marcelo Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo Perlin [aut, cre]",
    "url": "https://github.com/msperlin/GetDFPData/",
    "bug_reports": "https://github.com/msperlin/GetDFPData/issues",
    "repository": "https://cran.r-project.org/package=GetDFPData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GetDFPData Reading Annual Financial Reports from Bovespa's DFP, FRE and FCA\nSystem Reads annual financial reports including assets, liabilities, dividends history, stockholder composition and much more from Bovespa's DFP, FRE and FCA systems <http://www.b3.com.br/pt_br/produtos-e-servicos/negociacao/renda-variavel/empresas-listadas.htm>.\n These are web based interfaces for all financial reports of companies traded at Bovespa. The package is specially designed for large scale data importation, keeping a tabular (long) structure for easier processing.    "
  },
  {
    "id": 3841,
    "package_name": "GetDFPData2",
    "title": "Reading Annual and Quarterly Financial Reports from B3",
    "description": "Reads annual and quarterly financial reports from companies traded at B3, the Brazilian exchange \n            <https://www.b3.com.br/>. \n            All data is downloaded and imported from CVM's public ftp site <https://dados.cvm.gov.br/dados/CIA_ABERTA/>.",
    "version": "0.6.3",
    "maintainer": "Marcelo Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo Perlin [aut, cre],\n  Guilherme Kirch [aut]",
    "url": "https://github.com/msperlin/GetDFPData2/",
    "bug_reports": "https://github.com/msperlin/GetDFPData2/issues/",
    "repository": "https://cran.r-project.org/package=GetDFPData2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GetDFPData2 Reading Annual and Quarterly Financial Reports from B3 Reads annual and quarterly financial reports from companies traded at B3, the Brazilian exchange \n            <https://www.b3.com.br/>. \n            All data is downloaded and imported from CVM's public ftp site <https://dados.cvm.gov.br/dados/CIA_ABERTA/>.  "
  },
  {
    "id": 3853,
    "package_name": "GillespieSSA2",
    "title": "Gillespie's Stochastic Simulation Algorithm for Impatient People",
    "description": "A fast, scalable, and versatile framework for\n    simulating large systems with Gillespie's Stochastic Simulation\n    Algorithm ('SSA').  This package is the spiritual successor to the\n    'GillespieSSA' package originally written by Mario Pineda-Krch.\n    Benefits of this package include major speed improvements (>100x),\n    easier to understand documentation, and many unit tests that try to\n    ensure the package works as intended. Cannoodt and Saelens et al. (2021) \n    <doi:10.1038/s41467-021-24152-2>.",
    "version": "0.3.0",
    "maintainer": "Robrecht Cannoodt <rcannood@gmail.com>",
    "author": "Robrecht Cannoodt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3641-729X>),\n  Wouter Saelens [aut] (ORCID: <https://orcid.org/0000-0002-7114-6248>)",
    "url": "https://rcannood.github.io/GillespieSSA2/,\nhttps://github.com/rcannood/GillespieSSA2",
    "bug_reports": "https://github.com/rcannood/GillespieSSA2/issues",
    "repository": "https://cran.r-project.org/package=GillespieSSA2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GillespieSSA2 Gillespie's Stochastic Simulation Algorithm for Impatient People A fast, scalable, and versatile framework for\n    simulating large systems with Gillespie's Stochastic Simulation\n    Algorithm ('SSA').  This package is the spiritual successor to the\n    'GillespieSSA' package originally written by Mario Pineda-Krch.\n    Benefits of this package include major speed improvements (>100x),\n    easier to understand documentation, and many unit tests that try to\n    ensure the package works as intended. Cannoodt and Saelens et al. (2021) \n    <doi:10.1038/s41467-021-24152-2>.  "
  },
  {
    "id": 3872,
    "package_name": "GomoGomonoMi",
    "title": "Animate Text using the 'Animate.css' Library",
    "description": "Allows the user to animate text within 'rmarkdown' documents and 'shiny' applications. \n    The animations are activated using the 'Animate.css' library. See <https://animate.style/> for more information.",
    "version": "0.1.0",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden",
    "url": "https://github.com/feddelegrand7/GomoGomonoMi",
    "bug_reports": "https://github.com/feddelegrand7/GomoGomonoMi/issues",
    "repository": "https://cran.r-project.org/package=GomoGomonoMi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GomoGomonoMi Animate Text using the 'Animate.css' Library Allows the user to animate text within 'rmarkdown' documents and 'shiny' applications. \n    The animations are activated using the 'Animate.css' library. See <https://animate.style/> for more information.  "
  },
  {
    "id": 3886,
    "package_name": "GreedyExperimentalDesign",
    "title": "Greedy Experimental Design Construction",
    "description": "Computes experimental designs for a\n    two-arm experiment with covariates via a number of methods:\n    (0) complete randomization and randomization with forced-balance,\n    (1) Greedily optimizing a\n    balance objective function via pairwise switching. This optimization \n    provides lower variance for the treatment effect estimator (and higher \n    power) while preserving a design that is close to complete randomization.\n    We return all iterations of the designs for use in a permutation test,\n    (2) The second is via numerical optimization \n    (via 'gurobi' which must be installed, see <https://www.gurobi.com/documentation/9.1/quickstart_windows/r_ins_the_r_package.html>) \n    a la Bertsimas and Kallus, \n    (3) rerandomization, \n    (4) Karp's method for one covariate, \n    (5) exhaustive enumeration to find the \n    optimal solution (only for small sample sizes),\n    (6) Binary pair matching using the 'nbpMatching' library,\n    (7) Binary pair matching plus design number (1) to further optimize balance,\n    (8) Binary pair matching plus design number (3) to further optimize balance,\n    (9) Hadamard designs,\n    (10) Simultaneous Multiple Kernels.\n    In (1-9) we allow for three objective functions:\n    Mahalanobis distance,\n    Sum of absolute differences standardized and\n    Kernel distances via the 'kernlab' library. This package is the result of a stream of research that can be found in \n    Krieger, A, Azriel, D and Kapelner, A \"Nearly Random Designs with Greatly Improved Balance\" (2016) <arXiv:1612.02315>,\n    Krieger, A, Azriel, D and Kapelner, A \"Better Experimental Design by Hybridizing Binary Matching with Imbalance \n    Optimization\" (2021) <arXiv:2012.03330>.",
    "version": "1.5.6.1",
    "maintainer": "Adam Kapelner <kapelner@qc.cuny.edu>",
    "author": "Adam Kapelner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5985-6792>),\n  David Azriel [aut],\n  Abba Krieger [aut]",
    "url": "https://github.com/kapelner/GreedyExperimentalDesign",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GreedyExperimentalDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GreedyExperimentalDesign Greedy Experimental Design Construction Computes experimental designs for a\n    two-arm experiment with covariates via a number of methods:\n    (0) complete randomization and randomization with forced-balance,\n    (1) Greedily optimizing a\n    balance objective function via pairwise switching. This optimization \n    provides lower variance for the treatment effect estimator (and higher \n    power) while preserving a design that is close to complete randomization.\n    We return all iterations of the designs for use in a permutation test,\n    (2) The second is via numerical optimization \n    (via 'gurobi' which must be installed, see <https://www.gurobi.com/documentation/9.1/quickstart_windows/r_ins_the_r_package.html>) \n    a la Bertsimas and Kallus, \n    (3) rerandomization, \n    (4) Karp's method for one covariate, \n    (5) exhaustive enumeration to find the \n    optimal solution (only for small sample sizes),\n    (6) Binary pair matching using the 'nbpMatching' library,\n    (7) Binary pair matching plus design number (1) to further optimize balance,\n    (8) Binary pair matching plus design number (3) to further optimize balance,\n    (9) Hadamard designs,\n    (10) Simultaneous Multiple Kernels.\n    In (1-9) we allow for three objective functions:\n    Mahalanobis distance,\n    Sum of absolute differences standardized and\n    Kernel distances via the 'kernlab' library. This package is the result of a stream of research that can be found in \n    Krieger, A, Azriel, D and Kapelner, A \"Nearly Random Designs with Greatly Improved Balance\" (2016) <arXiv:1612.02315>,\n    Krieger, A, Azriel, D and Kapelner, A \"Better Experimental Design by Hybridizing Binary Matching with Imbalance \n    Optimization\" (2021) <arXiv:2012.03330>.  "
  },
  {
    "id": 3894,
    "package_name": "GroupComparisons",
    "title": "Paired/Unpaired Parametric/Non-Parametric Group Comparisons",
    "description": "Receives two vectors, computes appropriate function for group comparison (i.e., t-test, Mann-Whitney; equality of variances), and reports the findings (mean/median, standard deviation, test statistic, p-value, effect size) in APA format (Fay, M.P., & Proschan, M.A. (2010)<DOI: 10.1214/09-SS051>).",
    "version": "0.1.0",
    "maintainer": "Aaron England <aaron.england24@gmail.com>",
    "author": "Aaron England <aaron.england24@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GroupComparisons",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GroupComparisons Paired/Unpaired Parametric/Non-Parametric Group Comparisons Receives two vectors, computes appropriate function for group comparison (i.e., t-test, Mann-Whitney; equality of variances), and reports the findings (mean/median, standard deviation, test statistic, p-value, effect size) in APA format (Fay, M.P., & Proschan, M.A. (2010)<DOI: 10.1214/09-SS051>).  "
  },
  {
    "id": 3898,
    "package_name": "GrowthCurveME",
    "title": "Mixed-Effects Modeling for Growth Data",
    "description": "Simple and  user-friendly wrappers to the 'saemix' package for \n    performing linear and non-linear mixed-effects regression modeling for \n    growth data to account for clustering or longitudinal analysis via repeated\n    measurements. The package allows users to fit a variety of growth\n    models, including linear, exponential, logistic, and 'Gompertz'\n    functions. For non-linear models, starting values are automatically\n    calculated using initial least-squares estimates. The package includes \n    functions for summarizing models, visualizing data and results, \n    calculating doubling time and other key statistics, and generating model \n    diagnostic plots and residual summary statistics. It also provides \n    functions for generating publication-ready summary tables for reports. \n    Additionally, users can fit linear and non-linear least-squares \n    regression models if clustering is not applicable. The mixed-effects \n    modeling methods in this package are based on Comets, Lavenu, and \n    Lavielle (2017) <doi:10.18637/jss.v080.i03> as implemented in the \n    'saemix' package. Please contact us at models@dfci.harvard.edu \n    with any questions.",
    "version": "0.1.11",
    "maintainer": "Anand Panigrahy <anand_panigrahy@dfci.harvard.edu>",
    "author": "Anand Panigrahy [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2130-2089>),\n  Sonam Bhatia [ctb] (ORCID: <https://orcid.org/0000-0002-0124-2621>),\n  Thomas Quinn [dtc],\n  Aniket Shetty [rev],\n  Keith Ligon [fnd] (ORCID: <https://orcid.org/0000-0002-7733-600X>),\n  Center for Patient-Derived Models Dana-Farber Cancer Institute [cph]",
    "url": "https://github.com/cancermodels-org/GrowthCurveME",
    "bug_reports": "https://github.com/cancermodels-org/GrowthCurveME/issues",
    "repository": "https://cran.r-project.org/package=GrowthCurveME",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GrowthCurveME Mixed-Effects Modeling for Growth Data Simple and  user-friendly wrappers to the 'saemix' package for \n    performing linear and non-linear mixed-effects regression modeling for \n    growth data to account for clustering or longitudinal analysis via repeated\n    measurements. The package allows users to fit a variety of growth\n    models, including linear, exponential, logistic, and 'Gompertz'\n    functions. For non-linear models, starting values are automatically\n    calculated using initial least-squares estimates. The package includes \n    functions for summarizing models, visualizing data and results, \n    calculating doubling time and other key statistics, and generating model \n    diagnostic plots and residual summary statistics. It also provides \n    functions for generating publication-ready summary tables for reports. \n    Additionally, users can fit linear and non-linear least-squares \n    regression models if clustering is not applicable. The mixed-effects \n    modeling methods in this package are based on Comets, Lavenu, and \n    Lavielle (2017) <doi:10.18637/jss.v080.i03> as implemented in the \n    'saemix' package. Please contact us at models@dfci.harvard.edu \n    with any questions.  "
  },
  {
    "id": 3917,
    "package_name": "HCUPtools",
    "title": "Access and Work with HCUP Resources and Datasets",
    "description": "A comprehensive R package for accessing and working with publicly \n    available and free resources from the Agency for Healthcare Research and Quality \n    (AHRQ) Healthcare Cost and Utilization Project (HCUP). The package provides \n    streamlined access to HCUP's Clinical Classifications Software Refined (CCSR) \n    mapping files and Summary Trend Tables, enabling researchers and analysts to \n    efficiently map ICD-10-CM diagnosis codes and ICD-10-PCS procedure codes to \n    CCSR categories and access HCUP statistical reports. Key features include: \n    direct download from HCUP website, multiple output formats (long/wide/default), \n    cross-classification support, version management, citation generation, and \n    intelligent caching. The package does not redistribute HCUP data files but \n    facilitates direct download from the official HCUP website, ensuring users \n    always have access to the latest versions and maintain compliance with HCUP \n    data use policies. This package only accesses free public tools and reports; \n    it does NOT access HCUP databases (NIS, KID, SID, NEDS, etc.) that require \n    purchase. For more information, see <https://hcup-us.ahrq.gov/>.",
    "version": "1.0.0",
    "maintainer": "Vikrant Dev Rathore <rathore.vikrant@gmail.com>",
    "author": "Vikrant Dev Rathore [aut, cre]",
    "url": "https://github.com/vikrant31/HCUPtools,\nhttps://vikrant31.github.io/HCUPtools/",
    "bug_reports": "https://github.com/vikrant31/HCUPtools/issues",
    "repository": "https://cran.r-project.org/package=HCUPtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HCUPtools Access and Work with HCUP Resources and Datasets A comprehensive R package for accessing and working with publicly \n    available and free resources from the Agency for Healthcare Research and Quality \n    (AHRQ) Healthcare Cost and Utilization Project (HCUP). The package provides \n    streamlined access to HCUP's Clinical Classifications Software Refined (CCSR) \n    mapping files and Summary Trend Tables, enabling researchers and analysts to \n    efficiently map ICD-10-CM diagnosis codes and ICD-10-PCS procedure codes to \n    CCSR categories and access HCUP statistical reports. Key features include: \n    direct download from HCUP website, multiple output formats (long/wide/default), \n    cross-classification support, version management, citation generation, and \n    intelligent caching. The package does not redistribute HCUP data files but \n    facilitates direct download from the official HCUP website, ensuring users \n    always have access to the latest versions and maintain compliance with HCUP \n    data use policies. This package only accesses free public tools and reports; \n    it does NOT access HCUP databases (NIS, KID, SID, NEDS, etc.) that require \n    purchase. For more information, see <https://hcup-us.ahrq.gov/>.  "
  },
  {
    "id": 3977,
    "package_name": "HPZoneAPI",
    "title": "'HPZone' API Interface",
    "description": "Package that simplifies the use of the 'HPZone' API. Most of the annoying and labor-intensive parts of the interface are handled by wrapper functions. Note that the API and its details are not publicly available. Information can be found at <https://www.ggdghorkennisnet.nl/groep/726-platform-infectieziekte-epidemiologen/documenten/map/9609> for those with access. ",
    "version": "1.1.0",
    "maintainer": "Aart Dijkstra <a.dijkstra@ggdnog.nl>",
    "author": "Aart Dijkstra [aut, cre, cph]",
    "url": "https://github.com/ggdatascience/HPZoneAPI",
    "bug_reports": "https://github.com/ggdatascience/HPZoneAPI/issues",
    "repository": "https://cran.r-project.org/package=HPZoneAPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HPZoneAPI 'HPZone' API Interface Package that simplifies the use of the 'HPZone' API. Most of the annoying and labor-intensive parts of the interface are handled by wrapper functions. Note that the API and its details are not publicly available. Information can be found at <https://www.ggdghorkennisnet.nl/groep/726-platform-infectieziekte-epidemiologen/documenten/map/9609> for those with access.   "
  },
  {
    "id": 3990,
    "package_name": "HTMLUtils",
    "title": "Facilitates Automated HTML Report Creation",
    "description": "Facilitates automated HTML report creation, in particular\n        framed HTML pages and dynamically sortable tables.",
    "version": "0.1.9",
    "maintainer": "\"Markus Loecher, Berlin School of Economics and Law (BSEL)\" <markus.loecher@gmail.com>",
    "author": "\"Markus Loecher, Berlin School of Economics and Law (BSEL)\"  <markus.loecher@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HTMLUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HTMLUtils Facilitates Automated HTML Report Creation Facilitates automated HTML report creation, in particular\n        framed HTML pages and dynamically sortable tables.  "
  },
  {
    "id": 3999,
    "package_name": "HYDROCAL",
    "title": "Hydraulic Roughness Calculator",
    "description": "Estimates frictional constants for hydraulic analysis \n    of rivers. This HYDRaulic ROughness CALculator (HYDROCAL) was previously \n    developed as a spreadsheet tool and accompanying documentation by McKay and\n    Fischenich (2011, <https://erdc-library.erdc.dren.mil/jspui/bitstream/11681/2034/1/CHETN-VII-11.pdf>).",
    "version": "1.0.0",
    "maintainer": "Colton Shaw <shawcol@oregonstate.edu>",
    "author": "Colton Shaw [aut, cre] (ORCID: <https://orcid.org/0000-0002-4812-2555>),\n  S. Kyle McKay [aut] (ORCID: <https://orcid.org/0000-0003-2703-3841>)",
    "url": "GitHub (<https://github.com/USACE-WRISES>)",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HYDROCAL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HYDROCAL Hydraulic Roughness Calculator Estimates frictional constants for hydraulic analysis \n    of rivers. This HYDRaulic ROughness CALculator (HYDROCAL) was previously \n    developed as a spreadsheet tool and accompanying documentation by McKay and\n    Fischenich (2011, <https://erdc-library.erdc.dren.mil/jspui/bitstream/11681/2034/1/CHETN-VII-11.pdf>).  "
  },
  {
    "id": 4006,
    "package_name": "Hapi",
    "title": "Inference of Chromosome-Length Haplotypes Using Genomic Data of\nSingle Gamete Cells",
    "description": "Inference of chromosome-length haplotypes using a few haploid \n\tgametes of an individual. The gamete genotype data may be generated from various platforms \n\tincluding genotyping arrays and sequencing even with low-coverage. Hapi simply takes \n\tgenotype data of known hetSNPs in single gamete cells as input and report the high-resolution \n\thaplotypes as well as confidence of each phased hetSNPs. The package also includes a module \n\tallowing downstream analyses and visualization of identified crossovers in the gametes. ",
    "version": "0.0.3",
    "maintainer": "Ruidong Li <rli012@ucr.edu>",
    "author": "Ruidong Li,\n\tHan Qu,\n\tJinfeng Chen,\n\tShibo Wang,\n\tLe Zhang,\n\tJulong Wei,\n\tSergio Pietro Ferrante,\n\tMikeal L. Roose,\n\tZhenyu Jia",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Hapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Hapi Inference of Chromosome-Length Haplotypes Using Genomic Data of\nSingle Gamete Cells Inference of chromosome-length haplotypes using a few haploid \n\tgametes of an individual. The gamete genotype data may be generated from various platforms \n\tincluding genotyping arrays and sequencing even with low-coverage. Hapi simply takes \n\tgenotype data of known hetSNPs in single gamete cells as input and report the high-resolution \n\thaplotypes as well as confidence of each phased hetSNPs. The package also includes a module \n\tallowing downstream analyses and visualization of identified crossovers in the gametes.   "
  },
  {
    "id": 4012,
    "package_name": "Harvest.Tree",
    "title": "Harvest the Classification Tree",
    "description": "Aimed at applying the Harvest classification tree algorithm, modified algorithm of classic classification tree.The harvested tree has advantage of deleting redundant rules in trees, leading to a simplify and more efficient tree model.It was firstly used in drug discovery field, but it also performs well in other kinds of data, especially when the region of a class is disconnected. This package also improves the basic harvest classification tree algorithm by extending the field of data of algorithm to both continuous and categorical variables. To learn more about the harvest classification tree algorithm, you can go to http://www.stat.ubc.ca/Research/TechReports/techreports/220.pdf for more information. ",
    "version": "1.1",
    "maintainer": "Bingyuan Liu <adler1016@gmail.com>",
    "author": "Bingyuan Liu/Yan Yuan/Qian Shi",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Harvest.Tree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Harvest.Tree Harvest the Classification Tree Aimed at applying the Harvest classification tree algorithm, modified algorithm of classic classification tree.The harvested tree has advantage of deleting redundant rules in trees, leading to a simplify and more efficient tree model.It was firstly used in drug discovery field, but it also performs well in other kinds of data, especially when the region of a class is disconnected. This package also improves the basic harvest classification tree algorithm by extending the field of data of algorithm to both continuous and categorical variables. To learn more about the harvest classification tree algorithm, you can go to http://www.stat.ubc.ca/Research/TechReports/techreports/220.pdf for more information.   "
  },
  {
    "id": 4014,
    "package_name": "Hassani.Silva",
    "title": "A Test for Comparing the Predictive Accuracy of Two Sets of\nForecasts",
    "description": "A non-parametric test founded upon the principles of the Kolmogorov-Smirnov (KS) \n  test, referred to as the KS Predictive Accuracy (KSPA) test. The KSPA test is able to serve\n  two distinct purposes. Initially, the test seeks to determine whether there exists a \n  statistically significant difference between the distribution of forecast errors, and \n  secondly it exploits the principles of stochastic dominance to determine whether the \n  forecasts with the lower error also reports a stochastically smaller error than forecasts \n  from a competing model, and thereby enables distinguishing between the predictive accuracy \n  of forecasts. KSPA test has been described in : Hassani and Silva (2015) <doi:10.3390/econometrics3030590>.",
    "version": "1.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Hossein Hassani [aut],\n  Emmanuel Sirimal Silva [aut],\n  Leila Marvian Mashhad [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Hassani.Silva",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Hassani.Silva A Test for Comparing the Predictive Accuracy of Two Sets of\nForecasts A non-parametric test founded upon the principles of the Kolmogorov-Smirnov (KS) \n  test, referred to as the KS Predictive Accuracy (KSPA) test. The KSPA test is able to serve\n  two distinct purposes. Initially, the test seeks to determine whether there exists a \n  statistically significant difference between the distribution of forecast errors, and \n  secondly it exploits the principles of stochastic dominance to determine whether the \n  forecasts with the lower error also reports a stochastically smaller error than forecasts \n  from a competing model, and thereby enables distinguishing between the predictive accuracy \n  of forecasts. KSPA test has been described in : Hassani and Silva (2015) <doi:10.3390/econometrics3030590>.  "
  },
  {
    "id": 4042,
    "package_name": "HoRM",
    "title": "Supplemental Functions and Datasets for \"Handbook of Regression\nMethods\"",
    "description": "Supplement for the book \"Handbook of Regression Methods\" by D. S. Young.  Some datasets used in the book are included and documented.  Wrapper functions are included that simplify the examples in the textbook, such as code for constructing a regressogram and expanding ANOVA tables to reflect the total sum of squares.",
    "version": "0.1.4",
    "maintainer": "Derek S. Young <derek.young@uky.edu>",
    "author": "Derek S. Young [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3048-3803>)",
    "url": "https://github.com/dsy109/HoRM",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HoRM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HoRM Supplemental Functions and Datasets for \"Handbook of Regression\nMethods\" Supplement for the book \"Handbook of Regression Methods\" by D. S. Young.  Some datasets used in the book are included and documented.  Wrapper functions are included that simplify the examples in the textbook, such as code for constructing a regressogram and expanding ANOVA tables to reflect the total sum of squares.  "
  },
  {
    "id": 4084,
    "package_name": "ICD10gm",
    "title": "Metadata Processing for the German Modification of the ICD-10\nCoding System",
    "description": "Provides convenient access to the German modification of the International Classification of Diagnoses, 10th revision (ICD-10-GM). It provides functionality to aid in the identification, specification and historisation of ICD-10 codes. Its intended use is the analysis of routinely collected data in the context of epidemiology, medical research and health services research. The underlying metadata are released by the German Institute for Medical Documentation and Information <https://www.dimdi.de>, and are redistributed in accordance with their license.",
    "version": "1.2.5",
    "maintainer": "Ewan Donnachie <ewan@donnachie.net>",
    "author": "Ewan Donnachie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0668-0049>)",
    "url": "https://edonnachie.github.io/ICD10gm/,\nhttps://doi.org/10.5281/zenodo.2542833",
    "bug_reports": "https://github.com/edonnachie/ICD10gm/issues/",
    "repository": "https://cran.r-project.org/package=ICD10gm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ICD10gm Metadata Processing for the German Modification of the ICD-10\nCoding System Provides convenient access to the German modification of the International Classification of Diagnoses, 10th revision (ICD-10-GM). It provides functionality to aid in the identification, specification and historisation of ICD-10 codes. Its intended use is the analysis of routinely collected data in the context of epidemiology, medical research and health services research. The underlying metadata are released by the German Institute for Medical Documentation and Information <https://www.dimdi.de>, and are redistributed in accordance with their license.  "
  },
  {
    "id": 4112,
    "package_name": "IDEATools",
    "title": "Individual and Group Farm Sustainability Assessments using the\nIDEA4 Method",
    "description": "Collection of tools to automate the processing of data collected though the IDEA4 method (see Zahm et al. (2018) <doi:10.1051/cagri/2019004> ). Starting from the original data collecting files this packages provides functions to compute IDEA indicators, draw modern and aesthetic plots, and produce a wide range of reporting materials. ",
    "version": "3.5.2",
    "maintainer": "David Carayon <david.carayon@inrae.fr>",
    "author": "David Carayon [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0217-2175>)",
    "url": "https://davidcarayon.github.io/IDEATools/index.html,\nhttps://github.com/davidcarayon/IDEATools",
    "bug_reports": "https://github.com/davidcarayon/IDEATools/issues",
    "repository": "https://cran.r-project.org/package=IDEATools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IDEATools Individual and Group Farm Sustainability Assessments using the\nIDEA4 Method Collection of tools to automate the processing of data collected though the IDEA4 method (see Zahm et al. (2018) <doi:10.1051/cagri/2019004> ). Starting from the original data collecting files this packages provides functions to compute IDEA indicators, draw modern and aesthetic plots, and produce a wide range of reporting materials.   "
  },
  {
    "id": 4168,
    "package_name": "IPDfromKM",
    "title": "Map Digitized Survival Curves Back to Individual Patient Data",
    "description": "\n      An implementation to reconstruct individual patient data from Kaplan-Meier (K-M) survival curves, visualize and assess the accuracy of the reconstruction, then perform secondary analysis on the reconstructed data. We involve a simple function to extract the coordinates form the published K-M curves. The function is developed based on Poisot T. \u2019s digitize package (2011)  <doi:10.32614/RJ-2011-004> . For more complex and tangled together graphs, digitizing software, such as 'DigitizeIt' (for MAC or windows) or 'ScanIt'(for windows) can be used to get the coordinates. Additional information should also be involved to increase the accuracy, like numbers of patients at risk (often reported at 5-10 time points under the x-axis of the K-M graph), total number of patients, and total number of events. The package implements the modified iterative K-M estimation algorithm (modified-iKM) improved upon the approach proposed by Guyot (2012) <doi:10.1186/1471-2288-12-9> with some modifications. ",
    "version": "0.1.10",
    "maintainer": "Na Liu <nliu1104@gmail.com>",
    "author": "Na Liu [aut, cre],\n  J.Jack Lee [aut, ths],\n  Yanhong Zhou [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IPDfromKM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IPDfromKM Map Digitized Survival Curves Back to Individual Patient Data \n      An implementation to reconstruct individual patient data from Kaplan-Meier (K-M) survival curves, visualize and assess the accuracy of the reconstruction, then perform secondary analysis on the reconstructed data. We involve a simple function to extract the coordinates form the published K-M curves. The function is developed based on Poisot T. \u2019s digitize package (2011)  <doi:10.32614/RJ-2011-004> . For more complex and tangled together graphs, digitizing software, such as 'DigitizeIt' (for MAC or windows) or 'ScanIt'(for windows) can be used to get the coordinates. Additional information should also be involved to increase the accuracy, like numbers of patients at risk (often reported at 5-10 time points under the x-axis of the K-M graph), total number of patients, and total number of events. The package implements the modified iterative K-M estimation algorithm (modified-iKM) improved upon the approach proposed by Guyot (2012) <doi:10.1186/1471-2288-12-9> with some modifications.   "
  },
  {
    "id": 4176,
    "package_name": "IPV",
    "title": "Item Pool Visualization",
    "description": "Generate plots based on the Item Pool Visualization concept for\n    latent constructs. Item Pool Visualizations are used to display the\n    conceptual structure of a set of items (self-report or psychometric).\n    Dantlgraber, Stieger, & Reips (2019) <doi:10.1177/2059799119884283>.",
    "version": "1.0.0",
    "maintainer": "Nils Petras <nils.petras@mailbox.org>",
    "author": "Nils Petras [aut, cre],\n  Michael Dantlgraber [aut],\n  Ulf-Dietrich Reips [ctb],\n  Matthias Bannert [ctb]",
    "url": "https://github.com/NilsPetras/IPV",
    "bug_reports": "https://github.com/NilsPetras/IPV/issues",
    "repository": "https://cran.r-project.org/package=IPV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IPV Item Pool Visualization Generate plots based on the Item Pool Visualization concept for\n    latent constructs. Item Pool Visualizations are used to display the\n    conceptual structure of a set of items (self-report or psychometric).\n    Dantlgraber, Stieger, & Reips (2019) <doi:10.1177/2059799119884283>.  "
  },
  {
    "id": 4204,
    "package_name": "ISRaD",
    "title": "Tools and Data for the International Soil Radiocarbon Database",
    "description": "This is the central location for data and tools for the development,\n    maintenance, analysis, and deployment of the International Soil Radiocarbon Database\n    (ISRaD). ISRaD was developed as a collaboration between the U.S. Geological Survey\n    Powell Center and the Max Planck Institute for Biogeochemistry. This R package provides\n    tools for accessing and manipulating ISRaD data, compiling local data using the ISRaD\n    data structure, and simple query and reporting functions for ISRaD. For more detailed\n    information visit the ISRaD website at: <https://soilradiocarbon.org/>.",
    "version": "2.5.5",
    "maintainer": "Jeffrey Beem-Miller <jbeem@bgc-jena.mpg.de>",
    "author": "Alison Hoyt [aut],\n  Jeffrey Beem-Miller [aut, cre],\n  Shane Stoner [aut],\n  J. Grey Monroe [aut],\n  Caitlin Hicks-Pries [aut],\n  Paul A. Levine [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ISRaD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ISRaD Tools and Data for the International Soil Radiocarbon Database This is the central location for data and tools for the development,\n    maintenance, analysis, and deployment of the International Soil Radiocarbon Database\n    (ISRaD). ISRaD was developed as a collaboration between the U.S. Geological Survey\n    Powell Center and the Max Planck Institute for Biogeochemistry. This R package provides\n    tools for accessing and manipulating ISRaD data, compiling local data using the ISRaD\n    data structure, and simple query and reporting functions for ISRaD. For more detailed\n    information visit the ISRaD website at: <https://soilradiocarbon.org/>.  "
  },
  {
    "id": 4205,
    "package_name": "ISS",
    "title": "Isotonic Subgroup Selection",
    "description": "Methodology for subgroup selection in the context of isotonic regression including methods for sub-Gaussian errors, classification, homoscedastic Gaussian errors and quantile regression. See the documentation of ISS(). Details can be found in the paper by M\u00fcller, Reeve, Cannings and Samworth (2023) <arXiv:2305.04852v2>.",
    "version": "1.0.0",
    "maintainer": "Manuel M. M\u00fcller <mm2559@cam.ac.uk>",
    "author": "Manuel M. M\u00fcller [aut, cre],\n  Henry W. J. Reeve [aut],\n  Timothy I. Cannings [aut],\n  Richard J. Samworth [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ISS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ISS Isotonic Subgroup Selection Methodology for subgroup selection in the context of isotonic regression including methods for sub-Gaussian errors, classification, homoscedastic Gaussian errors and quantile regression. See the documentation of ISS(). Details can be found in the paper by M\u00fcller, Reeve, Cannings and Samworth (2023) <arXiv:2305.04852v2>.  "
  },
  {
    "id": 4254,
    "package_name": "InspectionPlanner",
    "title": "Phytosanitary Inspection Sampling Planner",
    "description": "A 'shiny' application to assist in phytosanitary inspections. It generates a diagram of pallets in a lot, highlights the units to be sampled, and documents them based on the selected sampling method (simple random or systematic sampling). ",
    "version": "1.2",
    "maintainer": "Gustavo Ramirez-Valverde <gustavoramirezvalverde@gmail.com>",
    "author": "Gustavo Ramirez-Valverde [aut, cre],\n  Luis Gabriel Otero-Prevost [aut],\n  Pedro Macias-Canales [ctb],\n  Juan A. Villanueva-Jimenez [ctb],\n  Jorge Luis Leyva-Vazquez [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=InspectionPlanner",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "InspectionPlanner Phytosanitary Inspection Sampling Planner A 'shiny' application to assist in phytosanitary inspections. It generates a diagram of pallets in a lot, highlights the units to be sampled, and documents them based on the selected sampling method (simple random or systematic sampling).   "
  },
  {
    "id": 4293,
    "package_name": "JATSdecoder",
    "title": "A Metadata and Text Extraction and Manipulation Tool Set",
    "description": "Provides a function collection to extract metadata, sectioned text and study characteristics from scientific articles in 'NISO-JATS' format. Articles in PDF format can be converted to 'NISO-JATS' with the 'Content ExtRactor and MINEr' ('CERMINE', <https://github.com/CeON/CERMINE>). For convenience, two functions bundle the extraction heuristics: JATSdecoder() converts 'NISO-JATS'-tagged XML files to a structured list with elements title, author, journal, history, 'DOI', abstract, sectioned text and reference list. study.character() extracts multiple study characteristics like number of included studies, statistical methods used, alpha error, power, statistical results, correction method for multiple testing, software used. The function get.stats() extracts all statistical results from text and recomputes p-values for many standard test statistics. It performs a consistency check of the reported with the recalculated p-values. An estimation of the involved sample size is performed based on textual reports within the abstract and the reported degrees of freedom within statistical results. In addition, the package contains some useful functions to process text (text2sentences(), text2num(), ngram(), strsplit2(), grep2()). See B\u00f6schen, I. (2021) <doi:10.1007/s11192-021-04162-z> B\u00f6schen, I. (2021) <doi:10.1038/s41598-021-98782-3>, B\u00f6schen, I. (2023) <doi:10.1038/s41598-022-27085-y>, and B\u00f6schen, I. (2024) <doi:10.48550/arXiv.2408.07948>.",
    "version": "1.2.1",
    "maintainer": "Ingmar B\u00f6schen <ingmar.boeschen@uni-hamburg.de>",
    "author": "Ingmar B\u00f6schen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1159-3991>)",
    "url": "https://github.com/ingmarboeschen/JATSdecoder",
    "bug_reports": "https://github.com/ingmarboeschen/JATSdecoder/issues",
    "repository": "https://cran.r-project.org/package=JATSdecoder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JATSdecoder A Metadata and Text Extraction and Manipulation Tool Set Provides a function collection to extract metadata, sectioned text and study characteristics from scientific articles in 'NISO-JATS' format. Articles in PDF format can be converted to 'NISO-JATS' with the 'Content ExtRactor and MINEr' ('CERMINE', <https://github.com/CeON/CERMINE>). For convenience, two functions bundle the extraction heuristics: JATSdecoder() converts 'NISO-JATS'-tagged XML files to a structured list with elements title, author, journal, history, 'DOI', abstract, sectioned text and reference list. study.character() extracts multiple study characteristics like number of included studies, statistical methods used, alpha error, power, statistical results, correction method for multiple testing, software used. The function get.stats() extracts all statistical results from text and recomputes p-values for many standard test statistics. It performs a consistency check of the reported with the recalculated p-values. An estimation of the involved sample size is performed based on textual reports within the abstract and the reported degrees of freedom within statistical results. In addition, the package contains some useful functions to process text (text2sentences(), text2num(), ngram(), strsplit2(), grep2()). See B\u00f6schen, I. (2021) <doi:10.1007/s11192-021-04162-z> B\u00f6schen, I. (2021) <doi:10.1038/s41598-021-98782-3>, B\u00f6schen, I. (2023) <doi:10.1038/s41598-022-27085-y>, and B\u00f6schen, I. (2024) <doi:10.48550/arXiv.2408.07948>.  "
  },
  {
    "id": 4294,
    "package_name": "JBrowseR",
    "title": "An R Interface to the JBrowse 2 Genome Browser",
    "description": "Provides an R interface to the JBrowse 2 genome browser.\n    Enables embedding a JB2 genome browser in a Shiny app or R Markdown\n    document. The browser can also be launched from an interactive R console.\n    The browser can be loaded with a variety of common genomics data types,\n    and can be used with a custom theme.",
    "version": "0.10.2",
    "maintainer": "Colin Diesh <colin.diesh@gmail.com>",
    "author": "Elliot Hershberg [aut] (ORCID: <https://orcid.org/0000-0003-2068-3366>),\n  Colin Diesh [aut, cre] (ORCID: <https://orcid.org/0000-0003-2629-7570>),\n  the JBrowse 2 Team [aut]",
    "url": "https://gmod.github.io/JBrowseR/ https://github.com/GMOD/JBrowseR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JBrowseR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JBrowseR An R Interface to the JBrowse 2 Genome Browser Provides an R interface to the JBrowse 2 genome browser.\n    Enables embedding a JB2 genome browser in a Shiny app or R Markdown\n    document. The browser can also be launched from an interactive R console.\n    The browser can be loaded with a variety of common genomics data types,\n    and can be used with a custom theme.  "
  },
  {
    "id": 4295,
    "package_name": "JCRImpactFactor",
    "title": "Journal Citation Reports ('JCR') Impact Factor by 'Clarivate'\n'Analytics'",
    "description": "The Impact Factor of a journal reported by Journal Citation Reports ('JCR') of  'Clarivate' 'Analytics' is provided. The impact factor is available for those journals only that were included Journal Citation Reports 'JCR'.",
    "version": "1.0.0",
    "maintainer": "Shahla Faisal <shahla_ramzan@yahoo.com>",
    "author": "Shahla Faisal ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JCRImpactFactor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JCRImpactFactor Journal Citation Reports ('JCR') Impact Factor by 'Clarivate'\n'Analytics' The Impact Factor of a journal reported by Journal Citation Reports ('JCR') of  'Clarivate' 'Analytics' is provided. The impact factor is available for those journals only that were included Journal Citation Reports 'JCR'.  "
  },
  {
    "id": 4296,
    "package_name": "JDCruncheR",
    "title": "Interface Between the 'JDemetra+' Cruncher and R, and Quality\nReport Generator",
    "description": "Tool for generating quality reports from cruncher outputs\n    (and calculating series scores). The latest version of the cruncher\n    can be downloaded here:\n    <https://github.com/jdemetra/jwsacruncher/releases>.",
    "version": "0.3.6",
    "maintainer": "Tanguy Barthelemy <tanguy.barthelemy@insee.fr>",
    "author": "Tanguy Barthelemy [aut, cre, art],\n  Alain Quartier-la-Tente [aut] (ORCID:\n    <https://orcid.org/0000-0001-7890-3857>),\n  Institut national de la statistique et des \u00e9tudes \u00e9conomiques [cph]\n    (https://www.insee.fr/),\n  Anna Smyk [aut]",
    "url": "https://github.com/InseeFr/JDCruncheR,\nhttps://inseefr.github.io/JDCruncheR/",
    "bug_reports": "https://github.com/InseeFr/JDCruncheR/issues",
    "repository": "https://cran.r-project.org/package=JDCruncheR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JDCruncheR Interface Between the 'JDemetra+' Cruncher and R, and Quality\nReport Generator Tool for generating quality reports from cruncher outputs\n    (and calculating series scores). The latest version of the cruncher\n    can be downloaded here:\n    <https://github.com/jdemetra/jwsacruncher/releases>.  "
  },
  {
    "id": 4333,
    "package_name": "JoSAE",
    "title": "Unit-Level and Area-Level Small Area Estimation",
    "description": "Implementation of some unit and area level EBLUP estimators as well as the estimators of their MSE also under heteroscedasticity. The package further documents the publications Breidenbach and Astrup (2012) <DOI:10.1007/s10342-012-0596-7>, Breidenbach et al. (2016) <DOI:10.1016/j.rse.2015.07.026> and Breidenbach et al. (2018 in press). The vignette further explains the use of the implemented functions.",
    "version": "0.3.0",
    "maintainer": "Johannes Breidenbach <job@nibio.no>",
    "author": "Johannes Breidenbach",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=JoSAE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JoSAE Unit-Level and Area-Level Small Area Estimation Implementation of some unit and area level EBLUP estimators as well as the estimators of their MSE also under heteroscedasticity. The package further documents the publications Breidenbach and Astrup (2012) <DOI:10.1007/s10342-012-0596-7>, Breidenbach et al. (2016) <DOI:10.1016/j.rse.2015.07.026> and Breidenbach et al. (2018 in press). The vignette further explains the use of the implemented functions.  "
  },
  {
    "id": 4352,
    "package_name": "KLINK",
    "title": "Kinship Analysis with Linked Markers",
    "description": "A 'shiny' application for forensic kinship testing, based on\n    the 'pedsuite' R packages. 'KLINK' is closely aligned with the (non-R)\n    software 'Familias' and 'FamLink', but offers several unique features,\n    including visualisations and automated report generation. The\n    calculation of likelihood ratios supports pairs of linked markers, and\n    all common mutation models.",
    "version": "1.1.0",
    "maintainer": "Magnus Dehli Vigeland <m.d.vigeland@medisin.uio.no>",
    "author": "Magnus Dehli Vigeland [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9134-4962>)",
    "url": "https://github.com/magnusdv/KLINK",
    "bug_reports": "https://github.com/magnusdv/KLINK/issues",
    "repository": "https://cran.r-project.org/package=KLINK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KLINK Kinship Analysis with Linked Markers A 'shiny' application for forensic kinship testing, based on\n    the 'pedsuite' R packages. 'KLINK' is closely aligned with the (non-R)\n    software 'Familias' and 'FamLink', but offers several unique features,\n    including visualisations and automated report generation. The\n    calculation of likelihood ratios supports pairs of linked markers, and\n    all common mutation models.  "
  },
  {
    "id": 4359,
    "package_name": "KNNShiny",
    "title": "Interactive Document for Working with KNN Analysis",
    "description": "An interactive document on  the topic of K-nearest neighbour (KNN) using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/KNNShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=KNNShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KNNShiny Interactive Document for Working with KNN Analysis An interactive document on  the topic of K-nearest neighbour (KNN) using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/KNNShiny/>.  "
  },
  {
    "id": 4385,
    "package_name": "Kendall",
    "title": "Kendall Rank Correlation and Mann-Kendall Trend Test",
    "description": "Computes the Kendall rank correlation and Mann-Kendall\n        trend test. See documentation for use of block bootstrap when\n        there is autocorrelation.",
    "version": "2.2.1",
    "maintainer": "A.I. McLeod <aimcleod@uwo.ca>",
    "author": "A.I. McLeod",
    "url": "http://www.stats.uwo.ca/faculty/aim",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Kendall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Kendall Kendall Rank Correlation and Mann-Kendall Trend Test Computes the Kendall rank correlation and Mann-Kendall\n        trend test. See documentation for use of block bootstrap when\n        there is autocorrelation.  "
  },
  {
    "id": 4390,
    "package_name": "Kernelheaping",
    "title": "Kernel Density Estimation for Heaped and Rounded Data",
    "description": "In self-reported or anonymised data the user often encounters\n    heaped data, i.e. data which are rounded (to a possibly different degree\n    of coarseness). While this is mostly a minor problem in parametric density\n    estimation the bias can be very large for non-parametric methods such as kernel\n    density estimation. This package implements a partly Bayesian algorithm treating\n    the true unknown values as additional parameters and estimates the rounding\n    parameters to give a corrected kernel density estimate. It supports various\n    standard bandwidth selection methods. Varying rounding probabilities (depending\n    on the true value) and asymmetric rounding is estimable as well: Gross, M. and Rendtel, U. (2016) (<doi:10.1093/jssam/smw011>).\n    Additionally, bivariate non-parametric density estimation for rounded data, Gross, M. et al. (2016) (<doi:10.1111/rssa.12179>),\n    as well as data aggregated on areas is supported.",
    "version": "2.3.0",
    "maintainer": "Marcus Gross <marcus.gross@inwt-statistics.de>",
    "author": "Marcus Gross [aut, cre],\n  Lukas Fuchs [aut],\n  Kerstin Erfurth [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Kernelheaping",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Kernelheaping Kernel Density Estimation for Heaped and Rounded Data In self-reported or anonymised data the user often encounters\n    heaped data, i.e. data which are rounded (to a possibly different degree\n    of coarseness). While this is mostly a minor problem in parametric density\n    estimation the bias can be very large for non-parametric methods such as kernel\n    density estimation. This package implements a partly Bayesian algorithm treating\n    the true unknown values as additional parameters and estimates the rounding\n    parameters to give a corrected kernel density estimate. It supports various\n    standard bandwidth selection methods. Varying rounding probabilities (depending\n    on the true value) and asymmetric rounding is estimable as well: Gross, M. and Rendtel, U. (2016) (<doi:10.1093/jssam/smw011>).\n    Additionally, bivariate non-parametric density estimation for rounded data, Gross, M. et al. (2016) (<doi:10.1111/rssa.12179>),\n    as well as data aggregated on areas is supported.  "
  },
  {
    "id": 4450,
    "package_name": "LEdecomp",
    "title": "Decompose Life Expectancy by Age (and Cause)",
    "description": "A set of all-cause and cause-specific life expectancy sensitivity and decomposition methods, including Arriaga (1984) <doi:10.2307/2061029>, others documented by Ponnapalli (2005) <doi:10.4054/DemRes.2005.12.7>, lifetable, numerical, and other algorithmic approaches such as Horiuchi et al (2008) <doi:10.1353/dem.0.0033>, or Andreev et al (2002) <doi:10.4054/DemRes.2002.7.14>. ",
    "version": "1.0.4",
    "maintainer": "Tim Riffe <tim.riffe@gmail.com>",
    "author": "Tim Riffe [aut, cre] (0000-0002-2673-4622),\n  David Atance [aut] (0000-0001-5860-0584),\n  Josep Lledo [aut] (0000-0002-7475-8549)",
    "url": "https://github.com/timriffe/LEdecomp",
    "bug_reports": "https://github.com/timriffe/LEdecomp/issues",
    "repository": "https://cran.r-project.org/package=LEdecomp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LEdecomp Decompose Life Expectancy by Age (and Cause) A set of all-cause and cause-specific life expectancy sensitivity and decomposition methods, including Arriaga (1984) <doi:10.2307/2061029>, others documented by Ponnapalli (2005) <doi:10.4054/DemRes.2005.12.7>, lifetable, numerical, and other algorithmic approaches such as Horiuchi et al (2008) <doi:10.1353/dem.0.0033>, or Andreev et al (2002) <doi:10.4054/DemRes.2002.7.14>.   "
  },
  {
    "id": 4454,
    "package_name": "LFDREmpiricalBayes",
    "title": "Estimating Local False Discovery Rates Using Empirical Bayes\nMethods",
    "description": "New empirical Bayes methods aiming at analyzing the association of single nucleotide polymorphisms (SNPs) to some particular disease are implemented in this package. The package uses local false discovery rate (LFDR) estimates of SNPs within a sample population defined as a  \"reference class\" and discovers if SNPs are associated with the corresponding disease. Although SNPs are used throughout this document, other biological data such as protein data and other gene data can be used. Karimnezhad, Ali and Bickel, D. R. (2016) <http://hdl.handle.net/10393/34889>.",
    "version": "1.0",
    "maintainer": "Ali Karimnezhad <ali_karimnezhad@yahoo.com>",
    "author": "Ali Karimnezhad, Johnary Kim, Anna Akpawu, Justin Chitpin and David R Bickel",
    "url": "https://davidbickel.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LFDREmpiricalBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LFDREmpiricalBayes Estimating Local False Discovery Rates Using Empirical Bayes\nMethods New empirical Bayes methods aiming at analyzing the association of single nucleotide polymorphisms (SNPs) to some particular disease are implemented in this package. The package uses local false discovery rate (LFDR) estimates of SNPs within a sample population defined as a  \"reference class\" and discovers if SNPs are associated with the corresponding disease. Although SNPs are used throughout this document, other biological data such as protein data and other gene data can be used. Karimnezhad, Ali and Bickel, D. R. (2016) <http://hdl.handle.net/10393/34889>.  "
  },
  {
    "id": 4462,
    "package_name": "LJexm",
    "title": "Extract, Convert, and Merge 'pdf' Files from 'zip' Files",
    "description": "Extracts 'zip' files, converts 'Word', 'Excel', and 'html'/'htm' files to 'pdf' format. 'Word' and 'Excel' conversion uses 'VBScript', while 'html'/'htm' conversion uses 'webshot' and 'PhantomJS'. Additionally, the package merges 'pdf' files into a single document. This package is only supported on 'Windows' due to 'VBScript' dependencies.",
    "version": "1.0.5",
    "maintainer": "Lijin Arakkandathil Thekkathil <lijin5673@gmail.com>",
    "author": "Lijin Arakkandathil Thekkathil [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LJexm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LJexm Extract, Convert, and Merge 'pdf' Files from 'zip' Files Extracts 'zip' files, converts 'Word', 'Excel', and 'html'/'htm' files to 'pdf' format. 'Word' and 'Excel' conversion uses 'VBScript', while 'html'/'htm' conversion uses 'webshot' and 'PhantomJS'. Additionally, the package merges 'pdf' files into a single document. This package is only supported on 'Windows' due to 'VBScript' dependencies.  "
  },
  {
    "id": 4464,
    "package_name": "LKT",
    "title": "Logistic Knowledge Tracing",
    "description": "Computes Logistic Knowledge Tracing ('LKT') which is a general method for tracking human learning in an educational software system. Please see Pavlik, Eglington, and Harrel-Williams (2021) <https://ieeexplore.ieee.org/document/9616435>. 'LKT' is a method to compute features of student data that are used as predictors of subsequent performance. 'LKT' allows great flexibility in the choice of predictive components and features computed for these predictive components. The system is built on top of 'LiblineaR', which enables extremely fast solutions compared to base glm() in R.",
    "version": "1.7.0",
    "maintainer": "Philip I. Pavlik Jr. <imrryr@gmail.com>",
    "author": "Philip I. Pavlik Jr. [aut, ctb, cre] (ORCID:\n    <https://orcid.org/0000-0001-6467-9452>),\n  Luke G. Eglington [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-8432-9203>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LKT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LKT Logistic Knowledge Tracing Computes Logistic Knowledge Tracing ('LKT') which is a general method for tracking human learning in an educational software system. Please see Pavlik, Eglington, and Harrel-Williams (2021) <https://ieeexplore.ieee.org/document/9616435>. 'LKT' is a method to compute features of student data that are used as predictors of subsequent performance. 'LKT' allows great flexibility in the choice of predictive components and features computed for these predictive components. The system is built on top of 'LiblineaR', which enables extremely fast solutions compared to base glm() in R.  "
  },
  {
    "id": 4466,
    "package_name": "LLMAgentR",
    "title": "Language Model Agents in R for AI Workflows and Research",
    "description": "Provides modular, graph-based agents powered by large language models (LLMs) for intelligent task execution in R. \n      Supports structured workflows for tasks such as forecasting, data visualization, feature engineering, data wrangling, data cleaning, 'SQL', code generation, weather reporting, and research-driven question answering. \n      Each agent performs iterative reasoning: recommending steps, generating R code, executing, debugging, and explaining results. \n      Includes built-in support for packages such as 'tidymodels', 'modeltime', 'plotly', 'ggplot2', and 'prophet'. Designed for analysts, developers, and teams building intelligent, reproducible AI workflows in R. \n      Compatible with LLM providers such as 'OpenAI', 'Anthropic', 'Groq', and 'Ollama'. Inspired by the Python package 'langagent'.",
    "version": "0.3.0",
    "maintainer": "Kwadwo Daddy Nyame Owusu Boakye <kwadwo.owusuboakye@outlook.com>",
    "author": "Kwadwo Daddy Nyame Owusu Boakye [aut, cre]",
    "url": "https://github.com/knowusuboaky/LLMAgentR,\nhttps://knowusuboaky.github.io/LLMAgentR/",
    "bug_reports": "https://github.com/knowusuboaky/LLMAgentR/issues",
    "repository": "https://cran.r-project.org/package=LLMAgentR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LLMAgentR Language Model Agents in R for AI Workflows and Research Provides modular, graph-based agents powered by large language models (LLMs) for intelligent task execution in R. \n      Supports structured workflows for tasks such as forecasting, data visualization, feature engineering, data wrangling, data cleaning, 'SQL', code generation, weather reporting, and research-driven question answering. \n      Each agent performs iterative reasoning: recommending steps, generating R code, executing, debugging, and explaining results. \n      Includes built-in support for packages such as 'tidymodels', 'modeltime', 'plotly', 'ggplot2', and 'prophet'. Designed for analysts, developers, and teams building intelligent, reproducible AI workflows in R. \n      Compatible with LLM providers such as 'OpenAI', 'Anthropic', 'Groq', and 'Ollama'. Inspired by the Python package 'langagent'.  "
  },
  {
    "id": 4468,
    "package_name": "LLMTranslate",
    "title": "'shiny' App for TRAPD/ISPOR Survey Translation with LLMs",
    "description": "A 'shiny' application to automate forward and back survey translation\n    with optional reconciliation using large language models (LLMs). Supports\n    OpenAI (GPT), Google Gemini, and Anthropic Claude models. It follows\n    the TRAPD (Translation, Review, Adjudication, Pretesting, Documentation)\n    framework and ISPOR (International Society for Pharmacoeconomics and\n    Outcomes Research) recommendations. See Harkness et al. (2010)\n    <doi:10.1002/9780470609927.ch7> and Wild et al. (2005)\n    <doi:10.1111/j.1524-4733.2005.04054.x>.",
    "version": "0.2.0",
    "maintainer": "Jonas R. Kunst <jonas.r.kunst@bi.no>",
    "author": "Jonas R. Kunst [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LLMTranslate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LLMTranslate 'shiny' App for TRAPD/ISPOR Survey Translation with LLMs A 'shiny' application to automate forward and back survey translation\n    with optional reconciliation using large language models (LLMs). Supports\n    OpenAI (GPT), Google Gemini, and Anthropic Claude models. It follows\n    the TRAPD (Translation, Review, Adjudication, Pretesting, Documentation)\n    framework and ISPOR (International Society for Pharmacoeconomics and\n    Outcomes Research) recommendations. See Harkness et al. (2010)\n    <doi:10.1002/9780470609927.ch7> and Wild et al. (2005)\n    <doi:10.1111/j.1524-4733.2005.04054.x>.  "
  },
  {
    "id": 4491,
    "package_name": "LPGraph",
    "title": "Nonparametric Smoothing of Laplacian Graph Spectra",
    "description": "A nonparametric method to approximate Laplacian graph spectra of a network with \n    ordered vertices. This provides a computationally efficient algorithm for obtaining an \n\taccurate and smooth estimate of the graph Laplacian basis. The approximation results can \n\tthen be used for tasks like change point detection, k-sample testing, and so on. The \n\tprimary reference is Mukhopadhyay, S. and Wang, K. (2018, Technical Report).",
    "version": "2.1",
    "maintainer": "Kaijun Wang <kaijun.wang@temple.edu>",
    "author": "Subhadeep Mukhopadhyay, Kaijun Wang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LPGraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LPGraph Nonparametric Smoothing of Laplacian Graph Spectra A nonparametric method to approximate Laplacian graph spectra of a network with \n    ordered vertices. This provides a computationally efficient algorithm for obtaining an \n\taccurate and smooth estimate of the graph Laplacian basis. The approximation results can \n\tthen be used for tasks like change point detection, k-sample testing, and so on. The \n\tprimary reference is Mukhopadhyay, S. and Wang, K. (2018, Technical Report).  "
  },
  {
    "id": 4508,
    "package_name": "LSDinterface",
    "title": "Interface Tools for LSD Simulation Results Files",
    "description": "Interfaces R with LSD simulation models. Reads object-oriented data in results files (.res[.gz]) produced by LSD and creates appropriate multi-dimensional arrays in R. Supports multiple core parallel threads of multi-file data reading for increased performance. Also provides functions to extract basic information and statistics from data files. LSD (Laboratory for Simulation Development) is free software developed by Marco Valente and Marcelo C. Pereira (documentation and downloads available at <https://www.labsimdev.org/>).",
    "version": "1.2.2",
    "maintainer": "Marcelo C. Pereira <mcper@unicamp.br>",
    "author": "Marcelo C. Pereira [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8069-2734>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LSDinterface",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSDinterface Interface Tools for LSD Simulation Results Files Interfaces R with LSD simulation models. Reads object-oriented data in results files (.res[.gz]) produced by LSD and creates appropriate multi-dimensional arrays in R. Supports multiple core parallel threads of multi-file data reading for increased performance. Also provides functions to extract basic information and statistics from data files. LSD (Laboratory for Simulation Development) is free software developed by Marco Valente and Marcelo C. Pereira (documentation and downloads available at <https://www.labsimdev.org/>).  "
  },
  {
    "id": 4510,
    "package_name": "LSDsensitivity",
    "title": "Sensitivity Analysis Tools for LSD Simulations",
    "description": "Tools for sensitivity analysis of LSD simulation models. Reads object-oriented data produced by LSD simulation models and performs screening and global sensitivity analysis (Sobol decomposition method, Saltelli et al. (2008) ISBN:9780470725177). A Kriging or polynomial meta-model (Kleijnen (2009) <doi:10.1016/j.ejor.2007.10.013>) is estimated using the simulation data to provide the data required by the Sobol decomposition. LSD (Laboratory for Simulation Development) is free software developed by Marco Valente and Marcelo C. Pereira (documentation and downloads available at <https://www.labsimdev.org/>).",
    "version": "1.3.0",
    "maintainer": "Marcelo C. Pereira <mcper@unicamp.br>",
    "author": "Marcelo C. Pereira [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8069-2734>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LSDsensitivity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSDsensitivity Sensitivity Analysis Tools for LSD Simulations Tools for sensitivity analysis of LSD simulation models. Reads object-oriented data produced by LSD simulation models and performs screening and global sensitivity analysis (Sobol decomposition method, Saltelli et al. (2008) ISBN:9780470725177). A Kriging or polynomial meta-model (Kleijnen (2009) <doi:10.1016/j.ejor.2007.10.013>) is estimated using the simulation data to provide the data required by the Sobol decomposition. LSD (Laboratory for Simulation Development) is free software developed by Marco Valente and Marcelo C. Pereira (documentation and downloads available at <https://www.labsimdev.org/>).  "
  },
  {
    "id": 4523,
    "package_name": "LSX",
    "title": "Semi-Supervised Algorithm for Document Scaling",
    "description": "A word embeddings-based semi-supervised model for document scaling Watanabe (2020) <doi:10.1080/19312458.2020.1832976>.\n    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).\n    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.",
    "version": "1.5.1",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph]",
    "url": "https://koheiw.github.io/LSX/",
    "bug_reports": "https://github.com/koheiw/LSX/issues",
    "repository": "https://cran.r-project.org/package=LSX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LSX Semi-Supervised Algorithm for Document Scaling A word embeddings-based semi-supervised model for document scaling Watanabe (2020) <doi:10.1080/19312458.2020.1832976>.\n    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).\n    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.  "
  },
  {
    "id": 4541,
    "package_name": "Lahman",
    "title": "Sean 'Lahman' Baseball Database",
    "description": "Provides the tables from the 'Sean Lahman Baseball Database' as\n    a set of R data.frames. It uses the data on pitching, hitting and fielding\n    performance and other tables from 1871 through 2024, as recorded in the 2025\n    version of the database. Documentation examples show how many baseball\n    questions can be investigated.",
    "version": "13.0-0",
    "maintainer": "Chris Dalzell <cdalzell@gmail.com>",
    "author": "Michael Friendly [aut],\n  Chris Dalzell [cre, aut],\n  Martin Monkman [aut],\n  Dennis Murphy [aut],\n  Vanessa Foot [ctb],\n  Justeena Zaki-Azat [ctb],\n  Daniel J Eck [ctb],\n  Sean Lahman [cph]",
    "url": "https://cdalzell.github.io/Lahman/,\nhttps://CRAN.R-project.org/package=Lahman",
    "bug_reports": "https://github.com/cdalzell/Lahman/issues",
    "repository": "https://cran.r-project.org/package=Lahman",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Lahman Sean 'Lahman' Baseball Database Provides the tables from the 'Sean Lahman Baseball Database' as\n    a set of R data.frames. It uses the data on pitching, hitting and fielding\n    performance and other tables from 1871 through 2024, as recorded in the 2025\n    version of the database. Documentation examples show how many baseball\n    questions can be investigated.  "
  },
  {
    "id": 4572,
    "package_name": "LexFindR",
    "title": "Find Related Items and Lexical Dimensions in a Lexicon",
    "description": "Implements code to identify lexical competitors in a given list\n  of words. We include many of the standard competitor types used in spoken word\n  recognition research, such as functions to find cohorts, neighbors, and\n  rhymes, amongst many others. The package includes documentation for using a\n  variety of lexicon files, including those with form codes made up of multiple\n  letters (i.e., phoneme codes) and also basic orthographies. Importantly, the\n  code makes use of multiple CPU cores and vectorization when possible, making\n  it extremely fast and able to handle large lexicons. Additionally, the package\n  contains documentation for users to easily write new functions, allowing\n  researchers to examine other relationships within a lexicon. \n  Preprint: <https://osf.io/preprints/psyarxiv/8dyru/>. Open access: <doi:10.3758/s13428-021-01667-6>. \n  Citation: Li, Z., Crinnion, A.M. & Magnuson, J.S. (2021). \n  <doi:10.3758/s13428-021-01667-6>.",
    "version": "1.1.0",
    "maintainer": "ZhaoBin Li <li_zhaobin@icloud.com>",
    "author": "ZhaoBin Li [aut, cre],\n  Anne Marie Crinnion [aut],\n  James S. Magnuson [aut, cph]",
    "url": "https://github.com/maglab-uconn/LexFindR",
    "bug_reports": "https://github.com/maglab-uconn/LexFindR/issues",
    "repository": "https://cran.r-project.org/package=LexFindR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LexFindR Find Related Items and Lexical Dimensions in a Lexicon Implements code to identify lexical competitors in a given list\n  of words. We include many of the standard competitor types used in spoken word\n  recognition research, such as functions to find cohorts, neighbors, and\n  rhymes, amongst many others. The package includes documentation for using a\n  variety of lexicon files, including those with form codes made up of multiple\n  letters (i.e., phoneme codes) and also basic orthographies. Importantly, the\n  code makes use of multiple CPU cores and vectorization when possible, making\n  it extremely fast and able to handle large lexicons. Additionally, the package\n  contains documentation for users to easily write new functions, allowing\n  researchers to examine other relationships within a lexicon. \n  Preprint: <https://osf.io/preprints/psyarxiv/8dyru/>. Open access: <doi:10.3758/s13428-021-01667-6>. \n  Citation: Li, Z., Crinnion, A.M. & Magnuson, J.S. (2021). \n  <doi:10.3758/s13428-021-01667-6>.  "
  },
  {
    "id": 4586,
    "package_name": "LikertMakeR",
    "title": "Synthesise and Correlate Likert Scale and Rating-Scale Data\nBased on Summary Statistics",
    "description": "Generate and correlate synthetic Likert and rating-scale data\n    with predefined means, standard deviations, Cronbach's Alpha, Factor\n    Loading table, coefficients, and other summary statistics.  \n    Worked examples and documentation are available in the package \n    articles, accessible via the package website,\n    <https://winzarh.github.io/LikertMakeR/>. ",
    "version": "1.3.0",
    "maintainer": "Hume Winzar <winzar@gmail.com>",
    "author": "Hume Winzar [cre, aut] (ORCID: <https://orcid.org/0000-0001-7475-2641>)",
    "url": "https://github.com/WinzarH/LikertMakeR/,\nhttps://winzarh.github.io/LikertMakeR/",
    "bug_reports": "https://github.com/WinzarH/LikertMakeR/issues",
    "repository": "https://cran.r-project.org/package=LikertMakeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LikertMakeR Synthesise and Correlate Likert Scale and Rating-Scale Data\nBased on Summary Statistics Generate and correlate synthetic Likert and rating-scale data\n    with predefined means, standard deviations, Cronbach's Alpha, Factor\n    Loading table, coefficients, and other summary statistics.  \n    Worked examples and documentation are available in the package \n    articles, accessible via the package website,\n    <https://winzarh.github.io/LikertMakeR/>.   "
  },
  {
    "id": 4588,
    "package_name": "LimnoPalettes",
    "title": "A Limnology Themed Palette Generator",
    "description": "Palettes generated from limnology based field and laboratory photos. Palettes can be used to generate color values to be used in any functions that calls for a color (i.e. ggplot(), plot(), flextable(), etc.). ",
    "version": "0.1.0",
    "maintainer": "Paul Julian <pauljulianphd@gmail.com>",
    "author": "Paul Julian [aut, cre] (ORCID: <https://orcid.org/0000-0002-7617-1354>)",
    "url": "https://github.com/SwampThingPaul/LimnoPalettes",
    "bug_reports": "https://github.com/SwampThingPaul/LimnoPalettes/issues",
    "repository": "https://cran.r-project.org/package=LimnoPalettes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LimnoPalettes A Limnology Themed Palette Generator Palettes generated from limnology based field and laboratory photos. Palettes can be used to generate color values to be used in any functions that calls for a color (i.e. ggplot(), plot(), flextable(), etc.).   "
  },
  {
    "id": 4595,
    "package_name": "LinkageMapView",
    "title": "Plot Linkage Group Maps with Quantitative Trait Loci",
    "description": "Produces high resolution, publication ready linkage maps\n    and quantitative trait loci maps. Input can be output from 'R/qtl',\n    simple text or comma delimited files. Output is currently\n    a portable document file.",
    "version": "2.1.2",
    "maintainer": "Lisa Ouellette <louellet@uncc.edu>",
    "author": "Lisa Ouellette [aut, cre]",
    "url": "https://github.com/louellette/LinkageMapView",
    "bug_reports": "https://github.com/louellette/LinkageMapView/issues",
    "repository": "https://cran.r-project.org/package=LinkageMapView",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LinkageMapView Plot Linkage Group Maps with Quantitative Trait Loci Produces high resolution, publication ready linkage maps\n    and quantitative trait loci maps. Input can be output from 'R/qtl',\n    simple text or comma delimited files. Output is currently\n    a portable document file.  "
  },
  {
    "id": 4617,
    "package_name": "LogisticEnsembles",
    "title": "Automatically Runs 24 Logistic Models (Individual and Ensembles)",
    "description": "Automatically returns 24 logistic models including 13 individual models and 11 ensembles of models of logistic data. The package also returns 25 plots, 5 tables, and a summary report. The package automatically\n    builds all 24 models, reports all results, and provides graphics to show how the models performed. This can be used for a wide range of data, such as sports or medical data. The package includes medical data (the Pima Indians data set), and\n    information about the performance of Lebron James. The package can be used to analyze many other examples, such as stock market data. The package automatically returns many values for each model, such as\n    True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate, Positive Predictive Value, Negative Predictive Value, F1 Score, Area Under the Curve. The package also returns 36 Receiver\n    Operating Characteristic (ROC) curves for each of the 24 models.",
    "version": "0.8.2",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "https://github.com/InfiniteCuriosity/LogisticEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/LogisticEnsembles/issues",
    "repository": "https://cran.r-project.org/package=LogisticEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LogisticEnsembles Automatically Runs 24 Logistic Models (Individual and Ensembles) Automatically returns 24 logistic models including 13 individual models and 11 ensembles of models of logistic data. The package also returns 25 plots, 5 tables, and a summary report. The package automatically\n    builds all 24 models, reports all results, and provides graphics to show how the models performed. This can be used for a wide range of data, such as sports or medical data. The package includes medical data (the Pima Indians data set), and\n    information about the performance of Lebron James. The package can be used to analyze many other examples, such as stock market data. The package automatically returns many values for each model, such as\n    True Positive Rate, True Negative Rate, False Positive Rate, False Negative Rate, Positive Predictive Value, Negative Predictive Value, F1 Score, Area Under the Curve. The package also returns 36 Receiver\n    Operating Characteristic (ROC) curves for each of the 24 models.  "
  },
  {
    "id": 4624,
    "package_name": "LoopDetectR",
    "title": "Comprehensive Feedback Loop Detection in ODE Models",
    "description": "Detect feedback loops (cycles, circuits) between species (nodes) in ordinary differential equation (ODE) models. Feedback loops are paths from a node to itself without visiting any other node twice, and they have important regulatory functions. Loops are reported with their order of participating nodes and their length, and whether the loop is a positive or a negative feedback loop. An upper limit of the number of feedback loops limits runtime (which scales with feedback loop count). Model parametrizations and values of the modelled variables are accounted for. Computation uses the characteristics of the Jacobian matrix as described e.g. in Thomas and Kaufman (2002) <doi:10.1016/s1631-0691(02)01452-x>. Input can be the Jacobian matrix of the ODE model or the ODE function definition; in the latter case, the Jacobian matrix is determined using 'numDeriv'. Graph-based algorithms from 'igraph' are employed for path detection. ",
    "version": "0.1.2",
    "maintainer": "Katharina Baum <katharina.baum@hpi.de>",
    "author": "Katharina Baum [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7256-0566>),\n  Sandra Kr\u00fcger [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LoopDetectR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LoopDetectR Comprehensive Feedback Loop Detection in ODE Models Detect feedback loops (cycles, circuits) between species (nodes) in ordinary differential equation (ODE) models. Feedback loops are paths from a node to itself without visiting any other node twice, and they have important regulatory functions. Loops are reported with their order of participating nodes and their length, and whether the loop is a positive or a negative feedback loop. An upper limit of the number of feedback loops limits runtime (which scales with feedback loop count). Model parametrizations and values of the modelled variables are accounted for. Computation uses the characteristics of the Jacobian matrix as described e.g. in Thomas and Kaufman (2002) <doi:10.1016/s1631-0691(02)01452-x>. Input can be the Jacobian matrix of the ODE model or the ODE function definition; in the latter case, the Jacobian matrix is determined using 'numDeriv'. Graph-based algorithms from 'igraph' are employed for path detection.   "
  },
  {
    "id": 4641,
    "package_name": "MAGMA.R",
    "title": "MAny-Group MAtching",
    "description": "Balancing quasi-experimental field research for effects of covariates is fundamental for drawing causal inference. Propensity Score Matching deals with this issue but current\n    techniques are restricted to binary treatment variables. Moreover, they provide several solutions without providing a comprehensive framework on choosing the best model. The\n    MAGMA R-package addresses these restrictions by offering nearest neighbor matching for two to four groups. It also includes the option to match data of a 2x2 design. In addition, \n    MAGMA includes a framework for evaluating the post-matching balance. The package includes functions for the matching process and matching reporting. We provide a tutorial on \n    MAGMA as vignette. More information on MAGMA can be found in Feuchter, M. D., Urban, J., Scherrer V., Breit, M. L., and Preckel F. (2022) <https://osf.io/p47nc/>.",
    "version": "1.0.4",
    "maintainer": "Julian Urban <urbanj@uni-trier.de>",
    "author": "Julian Urban [aut, cre],\n  Markus D. Feuchter [aut],\n  Vsevolod Scherrer [aut],\n  Moritz L. Breit [aut],\n  Franzis Preckel [aut]",
    "url": "",
    "bug_reports": "https://github.com/JulianUrban/MAGMA/issues",
    "repository": "https://cran.r-project.org/package=MAGMA.R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MAGMA.R MAny-Group MAtching Balancing quasi-experimental field research for effects of covariates is fundamental for drawing causal inference. Propensity Score Matching deals with this issue but current\n    techniques are restricted to binary treatment variables. Moreover, they provide several solutions without providing a comprehensive framework on choosing the best model. The\n    MAGMA R-package addresses these restrictions by offering nearest neighbor matching for two to four groups. It also includes the option to match data of a 2x2 design. In addition, \n    MAGMA includes a framework for evaluating the post-matching balance. The package includes functions for the matching process and matching reporting. We provide a tutorial on \n    MAGMA as vignette. More information on MAGMA can be found in Feuchter, M. D., Urban, J., Scherrer V., Breit, M. L., and Preckel F. (2022) <https://osf.io/p47nc/>.  "
  },
  {
    "id": 4716,
    "package_name": "MDMAPR",
    "title": "Molecular Detection Mapping and Analysis Platform",
    "description": "Runs a Shiny web application that merges raw 'qPCR' fluorescence data with related \n    metadata to visualize species presence/absence detection patterns and assess data quality. \n    The application calculates threshold values from raw fluorescence data using a method based \n    on the second derivative method, Luu-The et al (2005) <doi:10.2144/05382RR05>,  and utilizes \n    the \u2018chipPCR\u2019 package by R\u00f6diger, Burdukiewicz, & Schierack (2015) <doi:10.1093/bioinformatics/btv205> \n    to calculate Cq values. The application has the ability to connect to a custom developed MySQL \n    database to populate the applications interface. The application allows users to interact with \n    visualizations such as a dynamic map, amplification curves and standard curves, that allow for \n    zooming and/or filtering. It also enables the generation of customized exportable reports based\n    on filtered mapping data. ",
    "version": "0.2.3",
    "maintainer": "Alka Benawra <alkabenawra@rogers.com>",
    "author": "Alka Benawra <alkabenawra@rogers.com>",
    "url": "https://github.com/HannerLab/MDMAPR",
    "bug_reports": "https://github.com/HannerLab/MDMAPR/issues",
    "repository": "https://cran.r-project.org/package=MDMAPR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MDMAPR Molecular Detection Mapping and Analysis Platform Runs a Shiny web application that merges raw 'qPCR' fluorescence data with related \n    metadata to visualize species presence/absence detection patterns and assess data quality. \n    The application calculates threshold values from raw fluorescence data using a method based \n    on the second derivative method, Luu-The et al (2005) <doi:10.2144/05382RR05>,  and utilizes \n    the \u2018chipPCR\u2019 package by R\u00f6diger, Burdukiewicz, & Schierack (2015) <doi:10.1093/bioinformatics/btv205> \n    to calculate Cq values. The application has the ability to connect to a custom developed MySQL \n    database to populate the applications interface. The application allows users to interact with \n    visualizations such as a dynamic map, amplification curves and standard curves, that allow for \n    zooming and/or filtering. It also enables the generation of customized exportable reports based\n    on filtered mapping data.   "
  },
  {
    "id": 4723,
    "package_name": "MDSPCAShiny",
    "title": "Interactive Document for Working with Multidimensional Scaling\nand Principal Component Analysis",
    "description": "An interactive document on  the topic of multidimensional scaling and principal component analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/MDS_PCAShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MDSPCAShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MDSPCAShiny Interactive Document for Working with Multidimensional Scaling\nand Principal Component Analysis An interactive document on  the topic of multidimensional scaling and principal component analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyabolar.shinyapps.io/MDS_PCAShiny/>.  "
  },
  {
    "id": 4761,
    "package_name": "MHTrajectoryR",
    "title": "Bayesian Model Selection in Logistic Regression for the\nDetection of Adverse Drug Reactions",
    "description": "Spontaneous adverse event reports have a high potential for detecting adverse drug reactions. However, due to their dimension, the analysis of such databases requires statistical methods. We propose to use a logistic regression whose sparsity is viewed as a model selection challenge. Since the model space is huge, a Metropolis-Hastings algorithm carries out the model selection by maximizing the BIC criterion.",
    "version": "1.0.1",
    "maintainer": "Mohammed Sedki <Mohammed.sedki@u-psud.fr>",
    "author": "Matthieu Marbac and Mohammed Sedki",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MHTrajectoryR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MHTrajectoryR Bayesian Model Selection in Logistic Regression for the\nDetection of Adverse Drug Reactions Spontaneous adverse event reports have a high potential for detecting adverse drug reactions. However, due to their dimension, the analysis of such databases requires statistical methods. We propose to use a logistic regression whose sparsity is viewed as a model selection challenge. Since the model space is huge, a Metropolis-Hastings algorithm carries out the model selection by maximizing the BIC criterion.  "
  },
  {
    "id": 4817,
    "package_name": "MMD",
    "title": "Minimal Multilocus Distance (MMD) for Source Attribution and\nLoci Selection",
    "description": "The aim of the package is two-fold: (i) To implement \n    the MMD method for attribution of individuals to sources using \n    the Hamming distance between multilocus genotypes. (ii) To select \n    informative genetic markers based on information theory concepts \n    (entropy, mutual information and redundancy). The package implements the \n    functions introduced by Perez-Reche, F. J., Rotariu, O., Lopes, B. S., \n    Forbes, K. J. and Strachan, N. J. C. Mining whole genome sequence data to \n    efficiently attribute individuals to source populations. \n    Scientific Reports 10, 12124 (2020) <doi:10.1038/s41598-020-68740-6>.\n    See more details and examples in the README file.",
    "version": "1.0.0",
    "maintainer": "Francisco Perez-Reche <fperez-reche@abdn.ac.uk>",
    "author": "Francisco Perez-Reche [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MMD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MMD Minimal Multilocus Distance (MMD) for Source Attribution and\nLoci Selection The aim of the package is two-fold: (i) To implement \n    the MMD method for attribution of individuals to sources using \n    the Hamming distance between multilocus genotypes. (ii) To select \n    informative genetic markers based on information theory concepts \n    (entropy, mutual information and redundancy). The package implements the \n    functions introduced by Perez-Reche, F. J., Rotariu, O., Lopes, B. S., \n    Forbes, K. J. and Strachan, N. J. C. Mining whole genome sequence data to \n    efficiently attribute individuals to source populations. \n    Scientific Reports 10, 12124 (2020) <doi:10.1038/s41598-020-68740-6>.\n    See more details and examples in the README file.  "
  },
  {
    "id": 4833,
    "package_name": "MNS",
    "title": "Mixed Neighbourhood Selection",
    "description": "An implementation of the mixed neighbourhood selection (MNS) algorithm. The MNS algorithm can be used to estimate multiple related precision matrices. In particular, the motivation behind this work was driven by the need to understand functional connectivity networks across multiple subjects. This package also contains an implementation of a novel algorithm through which to simulate multiple related precision matrices which exhibit properties frequently reported in neuroimaging analysis. ",
    "version": "1.0",
    "maintainer": "Ricardo Pio Monti <ricardo.monti08@gmail.com>",
    "author": "Ricardo Pio Monti, Christoforos Anagnostopoulos and Giovanni Montana",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MNS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MNS Mixed Neighbourhood Selection An implementation of the mixed neighbourhood selection (MNS) algorithm. The MNS algorithm can be used to estimate multiple related precision matrices. In particular, the motivation behind this work was driven by the need to understand functional connectivity networks across multiple subjects. This package also contains an implementation of a novel algorithm through which to simulate multiple related precision matrices which exhibit properties frequently reported in neuroimaging analysis.   "
  },
  {
    "id": 4837,
    "package_name": "MOEADr",
    "title": "Component-Wise MOEA/D Implementation",
    "description": "Modular implementation of Multiobjective Evolutionary Algorithms \n              based on Decomposition (MOEA/D) [Zhang and Li (2007), \n              <DOI:10.1109/TEVC.2007.892759>] for quick assembling and \n              testing of new algorithmic components, as well as easy \n              replication of published MOEA/D proposals. The full framework is\n              documented in a paper published in the Journal of Statistical \n              Software [<doi:10.18637/jss.v092.i06>].",
    "version": "1.1.3",
    "maintainer": "Felipe Campelo <fcampelo@ufmg.br>",
    "author": "Felipe Campelo [aut, cre],\n  Lucas Batista [com],\n  Claus Aranha [aut]",
    "url": "https://fcampelo.github.io/MOEADr/",
    "bug_reports": "https://github.com/fcampelo/MOEADr/issues",
    "repository": "https://cran.r-project.org/package=MOEADr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MOEADr Component-Wise MOEA/D Implementation Modular implementation of Multiobjective Evolutionary Algorithms \n              based on Decomposition (MOEA/D) [Zhang and Li (2007), \n              <DOI:10.1109/TEVC.2007.892759>] for quick assembling and \n              testing of new algorithmic components, as well as easy \n              replication of published MOEA/D proposals. The full framework is\n              documented in a paper published in the Journal of Statistical \n              Software [<doi:10.18637/jss.v092.i06>].  "
  },
  {
    "id": 4842,
    "package_name": "MOQA",
    "title": "Basic Quality Data Assurance for Epidemiological Research",
    "description": "With the provision of several tools and templates the MOSAIC project (DFG-Grant Number HO 1937/2-1) supports the implementation of a central data management in epidemiological research projects. The 'MOQA' package enables epidemiologists with none or low experience in R to generate basic data quality reports for a wide range of application scenarios. See <https://mosaic-greifswald.de/> for more information. Please read and cite the corresponding open access publication (using the former package-name) in METHODS OF INFORMATION IN MEDICINE by M. Bialke, H. Rau, T. Schwaneberg, R. Walk, T. Bahls and W. Hoffmann (2017) <doi:10.3414/ME16-01-0123>. <https://methods.schattauer.de/en/contents/most-recent-articles/issue/2483/issue/special/manuscript/27573/show.html>.",
    "version": "2.0.0",
    "maintainer": "Martin Bialke <mosaic-projekt@uni-greifswald.de>",
    "author": "Martin Bialke <mosaic-projekt@uni-greifswald.de>, Thea Schwaneberg <thea.schwaneberg@uni-greifswald.de>, Rene Walk <rene.walk@uni-greifswald.de>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MOQA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MOQA Basic Quality Data Assurance for Epidemiological Research With the provision of several tools and templates the MOSAIC project (DFG-Grant Number HO 1937/2-1) supports the implementation of a central data management in epidemiological research projects. The 'MOQA' package enables epidemiologists with none or low experience in R to generate basic data quality reports for a wide range of application scenarios. See <https://mosaic-greifswald.de/> for more information. Please read and cite the corresponding open access publication (using the former package-name) in METHODS OF INFORMATION IN MEDICINE by M. Bialke, H. Rau, T. Schwaneberg, R. Walk, T. Bahls and W. Hoffmann (2017) <doi:10.3414/ME16-01-0123>. <https://methods.schattauer.de/en/contents/most-recent-articles/issue/2483/issue/special/manuscript/27573/show.html>.  "
  },
  {
    "id": 4897,
    "package_name": "MSUthemes",
    "title": "Michigan State University (MSU) Palettes and Themes",
    "description": "Defines colour palettes and themes for Michigan State\n  University (MSU) publications and presentations. Palettes and\n  themes are supported in both base R and 'ggplot2' graphics, and are intended\n  to provide consistency between those creating documents and presentations.",
    "version": "1.0.0",
    "maintainer": "Emilio Xavier Esposito <emilio.esposito@gmail.com>",
    "author": "Emilio Xavier Esposito [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6193-0485>, GitHub:\n    https://github.com/emilioxavier),\n  Nicola Rennie [aut],\n  Michigan State University [fnd]",
    "url": "https://github.com/emilioxavier/MSUthemes,\nhttps://emilioxavier.github.io/MSUthemes/",
    "bug_reports": "https://github.com/emilioxavier/MSUthemes/issues",
    "repository": "https://cran.r-project.org/package=MSUthemes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSUthemes Michigan State University (MSU) Palettes and Themes Defines colour palettes and themes for Michigan State\n  University (MSU) publications and presentations. Palettes and\n  themes are supported in both base R and 'ggplot2' graphics, and are intended\n  to provide consistency between those creating documents and presentations.  "
  },
  {
    "id": 4969,
    "package_name": "MassWateR",
    "title": "Quality Control and Analysis of Massachusetts Water Quality Data",
    "description": "Methods for quality control and exploratory analysis of surface \n  water quality data collected in Massachusetts, USA.  Functions are developed \n  to facilitate data formatting for the Water Quality Exchange Network \n  <https://www.epa.gov/waterdata/water-quality-data-upload-wqx> and reporting \n  of data quality objectives to state agencies. Quality control methods are \n  from Massachusetts Department of Environmental Protection (2020) \n  <https://www.mass.gov/orgs/massachusetts-department-of-environmental-protection>.",
    "version": "2.2.0",
    "maintainer": "Marcus Beck <mbeck@tbep.org>",
    "author": "Marcus Beck [aut, cre] (ORCID: <https://orcid.org/0000-0002-4996-0059>),\n  Jill Carr [aut],\n  Ben Wetherill [aut] (ORCID: <https://orcid.org/0000-0002-8791-4256>)",
    "url": "<https://github.com/massbays-tech/MassWateR>,\n<https://massbays-tech.github.io/MassWateR/>",
    "bug_reports": "https://github.com/massbays-tech/MassWateR/issues",
    "repository": "https://cran.r-project.org/package=MassWateR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MassWateR Quality Control and Analysis of Massachusetts Water Quality Data Methods for quality control and exploratory analysis of surface \n  water quality data collected in Massachusetts, USA.  Functions are developed \n  to facilitate data formatting for the Water Quality Exchange Network \n  <https://www.epa.gov/waterdata/water-quality-data-upload-wqx> and reporting \n  of data quality objectives to state agencies. Quality control methods are \n  from Massachusetts Department of Environmental Protection (2020) \n  <https://www.mass.gov/orgs/massachusetts-department-of-environmental-protection>.  "
  },
  {
    "id": 5003,
    "package_name": "MediaNews",
    "title": "Media News Extraction for Text Analysis",
    "description": "Extract textual data from different media channels \n    through its source based on users choice of keywords. \n    These data can be used to perform text analysis to \n    identify patterns in respective media reporting.\n    The media channels used in this package are print media.\n    The data (or news) used are publicly available to consumers.",
    "version": "0.2.1",
    "maintainer": "Vatsal Aima <vaima75@hotmail.com>",
    "author": "Vatsal Aima [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MediaNews",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MediaNews Media News Extraction for Text Analysis Extract textual data from different media channels \n    through its source based on users choice of keywords. \n    These data can be used to perform text analysis to \n    identify patterns in respective media reporting.\n    The media channels used in this package are print media.\n    The data (or news) used are publicly available to consumers.  "
  },
  {
    "id": 5006,
    "package_name": "MedxR",
    "title": "Access Drug Regulatory Data via FDA and Health Canada APIs",
    "description": "Provides functions to access drug regulatory data from public RESTful APIs \n    including the 'FDA Open API' and the 'Health Canada Drug Product Database API', \n    retrieving real-time or historical information on drug approvals, adverse events, \n    recalls, and product details. Additionally, the package includes a curated collection \n    of open datasets focused on drugs, pharmaceuticals, treatments, and clinical studies. \n    These datasets cover diverse topics such as treatment dosages, pharmacological studies, \n    placebo effects, drug reactions, misuses of pain relievers, and vaccine effectiveness. \n    The package supports reproducible research and teaching in pharmacology, medicine, \n    and healthcare by integrating reliable international APIs and structured datasets \n    from public, academic, and government sources. \n    For more information on the APIs, see: \n    'FDA API' <https://open.fda.gov/apis/> and \n    'Health Canada API' <https://health-products.canada.ca/api/documentation/dpd-documentation-en.html>.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/medxr,\nhttps://lightbluetitan.github.io/medxr/",
    "bug_reports": "https://github.com/lightbluetitan/medxr/issues",
    "repository": "https://cran.r-project.org/package=MedxR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MedxR Access Drug Regulatory Data via FDA and Health Canada APIs Provides functions to access drug regulatory data from public RESTful APIs \n    including the 'FDA Open API' and the 'Health Canada Drug Product Database API', \n    retrieving real-time or historical information on drug approvals, adverse events, \n    recalls, and product details. Additionally, the package includes a curated collection \n    of open datasets focused on drugs, pharmaceuticals, treatments, and clinical studies. \n    These datasets cover diverse topics such as treatment dosages, pharmacological studies, \n    placebo effects, drug reactions, misuses of pain relievers, and vaccine effectiveness. \n    The package supports reproducible research and teaching in pharmacology, medicine, \n    and healthcare by integrating reliable international APIs and structured datasets \n    from public, academic, and government sources. \n    For more information on the APIs, see: \n    'FDA API' <https://open.fda.gov/apis/> and \n    'Health Canada API' <https://health-products.canada.ca/api/documentation/dpd-documentation-en.html>.  "
  },
  {
    "id": 5020,
    "package_name": "MetaNLP",
    "title": "Natural Language Processing for Meta Analysis",
    "description": "Given a CSV file with titles and abstracts, the package creates a\n    document-term matrix that is lemmatized and stemmed and can directly be used to\n    train machine learning methods for automatic title-abstract screening in the\n    preparation of a meta analysis.",
    "version": "0.1.4",
    "maintainer": "Maximilian Pilz <maximilian.pilz@itwm.fraunhofer.de>",
    "author": "Nico Bruder [aut] (ORCID: <https://orcid.org/0009-0004-9522-2075>),\n  Samuel Zimmermann [aut] (ORCID:\n    <https://orcid.org/0009-0000-4828-9294>),\n  Johannes Vey [aut] (ORCID: <https://orcid.org/0000-0002-2610-9667>),\n  Maximilian Pilz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9685-1613>),\n  Institute of Medical Biometry - University of Heidelberg [cph]",
    "url": "https://github.com/imbi-heidelberg/MetaNLP",
    "bug_reports": "https://github.com/imbi-heidelberg/MetaNLP/issues",
    "repository": "https://cran.r-project.org/package=MetaNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MetaNLP Natural Language Processing for Meta Analysis Given a CSV file with titles and abstracts, the package creates a\n    document-term matrix that is lemmatized and stemmed and can directly be used to\n    train machine learning methods for automatic title-abstract screening in the\n    preparation of a meta analysis.  "
  },
  {
    "id": 5064,
    "package_name": "MisRepARMA",
    "title": "Misreported Time Series Analysis",
    "description": "Provides a simple and trustworthy methodology for the analysis of misreported continuous time series. See Mori\u00f1a, D, Fern\u00e1ndez-Fontelo, A, Caba\u00f1a, A, Puig P. (2021) <arXiv:2003.09202v2>.",
    "version": "0.0.2",
    "maintainer": "David Mori\u00f1a Soler <dmorina@ub.edu>",
    "author": "David Mori\u00f1a Soler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5949-7443>),\n  Amanda Fern\u00e1ndez-Fontelo [aut],\n  Alejandra Caba\u00f1a [aut],\n  Pedro Puig [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MisRepARMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MisRepARMA Misreported Time Series Analysis Provides a simple and trustworthy methodology for the analysis of misreported continuous time series. See Mori\u00f1a, D, Fern\u00e1ndez-Fontelo, A, Caba\u00f1a, A, Puig P. (2021) <arXiv:2003.09202v2>.  "
  },
  {
    "id": 5084,
    "package_name": "MixTwice",
    "title": "Large-Scale Hypothesis Testing by Variance Mixing",
    "description": "Implements large-scale hypothesis testing by variance mixing. It takes two statistics per testing unit -- an estimated effect and its associated squared standard error -- and fits a nonparametric, shape-constrained mixture separately on two latent parameters. It reports local false discovery rates (lfdr) and local false sign rates (lfsr). Manuscript describing algorithm of MixTwice: Zheng et al(2021) <doi: 10.1093/bioinformatics/btab162>.",
    "version": "2.0",
    "maintainer": "Zihao Zheng <zihao.zheng@wisc.edu>",
    "author": "Zihao Zheng and  Michael A.Newton",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MixTwice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MixTwice Large-Scale Hypothesis Testing by Variance Mixing Implements large-scale hypothesis testing by variance mixing. It takes two statistics per testing unit -- an estimated effect and its associated squared standard error -- and fits a nonparametric, shape-constrained mixture separately on two latent parameters. It reports local false discovery rates (lfdr) and local false sign rates (lfsr). Manuscript describing algorithm of MixTwice: Zheng et al(2021) <doi: 10.1093/bioinformatics/btab162>.  "
  },
  {
    "id": 5097,
    "package_name": "MoLE",
    "title": "Modeling Language Evolution",
    "description": "Model for simulating language evolution in terms of cultural evolution (Smith & Kirby (2008) <DOI:10.1098/rstb.2008.0145>; Deacon 1997). The focus is on the emergence of argument-marking systems (Dowty (1991) <DOI:10.1353/lan.1991.0021>, Van Valin 1999, Dryer 2002, Lestrade 2015a), i.e. noun marking (Aristar (1997) <DOI:10.1075/sl.21.2.04ari>, Lestrade (2010) <DOI:10.7282/T3ZG6R4S>), person indexing (Ariel 1999, Dahl (2000) <DOI:10.1075/fol.7.1.03dah>, Bhat 2004), and word order (Dryer 2013), but extensions are foreseen. Agents start out with a protolanguage (a language without grammar; Bickerton (1981) <DOI:10.17169/langsci.b91.109>, Jackendoff 2002, Arbib (2015) <DOI:10.1002/9781118346136.ch27>) and interact through language games (Steels 1997). Over time, grammatical constructions emerge that may or may not become obligatory (for which the tolerance principle is assumed; Yang 2016). Throughout the simulation, uniformitarianism of principles is assumed (Hopper (1987) <DOI:10.3765/bls.v13i0.1834>, Givon (1995) <DOI:10.1075/z.74>, Croft (2000), Saffran (2001) <DOI:10.1111/1467-8721.01243>, Heine & Kuteva 2007), in which maximal psychological validity is aimed at (Grice (1975) <DOI:10.1057/9780230005853_5>, Levelt 1989, Gaerdenfors 2000) and language representation is usage based (Tomasello 2003, Bybee 2010). In Lestrade (2015b) <DOI:10.15496/publikation-8640>, Lestrade (2015c) <DOI:10.1075/avt.32.08les>, and Lestrade (2016) <DOI:10.17617/2.2248195>), which reported on the results of preliminary versions, this package was announced as WDWTW (for who does what to whom), but for reasons of pronunciation and generalization the title was changed.",
    "version": "1.0.1",
    "maintainer": "Sander Lestrade <samlestrade@protonmail.com>",
    "author": "Sander Lestrade",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MoLE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MoLE Modeling Language Evolution Model for simulating language evolution in terms of cultural evolution (Smith & Kirby (2008) <DOI:10.1098/rstb.2008.0145>; Deacon 1997). The focus is on the emergence of argument-marking systems (Dowty (1991) <DOI:10.1353/lan.1991.0021>, Van Valin 1999, Dryer 2002, Lestrade 2015a), i.e. noun marking (Aristar (1997) <DOI:10.1075/sl.21.2.04ari>, Lestrade (2010) <DOI:10.7282/T3ZG6R4S>), person indexing (Ariel 1999, Dahl (2000) <DOI:10.1075/fol.7.1.03dah>, Bhat 2004), and word order (Dryer 2013), but extensions are foreseen. Agents start out with a protolanguage (a language without grammar; Bickerton (1981) <DOI:10.17169/langsci.b91.109>, Jackendoff 2002, Arbib (2015) <DOI:10.1002/9781118346136.ch27>) and interact through language games (Steels 1997). Over time, grammatical constructions emerge that may or may not become obligatory (for which the tolerance principle is assumed; Yang 2016). Throughout the simulation, uniformitarianism of principles is assumed (Hopper (1987) <DOI:10.3765/bls.v13i0.1834>, Givon (1995) <DOI:10.1075/z.74>, Croft (2000), Saffran (2001) <DOI:10.1111/1467-8721.01243>, Heine & Kuteva 2007), in which maximal psychological validity is aimed at (Grice (1975) <DOI:10.1057/9780230005853_5>, Levelt 1989, Gaerdenfors 2000) and language representation is usage based (Tomasello 2003, Bybee 2010). In Lestrade (2015b) <DOI:10.15496/publikation-8640>, Lestrade (2015c) <DOI:10.1075/avt.32.08les>, and Lestrade (2016) <DOI:10.17617/2.2248195>), which reported on the results of preliminary versions, this package was announced as WDWTW (for who does what to whom), but for reasons of pronunciation and generalization the title was changed.  "
  },
  {
    "id": 5149,
    "package_name": "MultiFit",
    "title": "Multiscale Fisher's Independence Test for Multivariate\nDependence",
    "description": "Test for independence of two random vectors, learn and report the dependency structure. For more information, see Gorsky, Shai and Li Ma, Multiscale Fisher's Independence Test for Multivariate Dependence, Biometrika, accepted, January 2022.",
    "version": "1.1.1",
    "maintainer": "S. Gorsky <sgorsky@umass.edu>",
    "author": "S. Gorsky, L. Ma",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultiFit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiFit Multiscale Fisher's Independence Test for Multivariate\nDependence Test for independence of two random vectors, learn and report the dependency structure. For more information, see Gorsky, Shai and Li Ma, Multiscale Fisher's Independence Test for Multivariate Dependence, Biometrika, accepted, January 2022.  "
  },
  {
    "id": 5190,
    "package_name": "NAEPirtparams",
    "title": "IRT Parameters for the National Assessment of Education Progress",
    "description": "This data package contains the Item Response Theory (IRT) parameters for the National Center for Education Statistics (NCES) items used on the National Assessment of Education Progress (NAEP) from 1990 to 2015. The values in these tables are used along with NAEP data to turn student item responses into scores and include information about item difficulty, discrimination, and guessing parameter for 3 parameter logit (3PL) items. Parameters for Generalized Partial Credit Model (GPCM) items are also included. The adjustments table contains the information regarding the treatment of items (e.g., deletion of an item or a collapsing of response categories), when these items did not appear to fit the item response models used to describe the NAEP data. Transformation constants change the score estimates that are obtained from the IRT scaling program to the NAEP reporting metric. Values from the years 2000 - 2013 were taken from the NCES website <https://nces.ed.gov/nationsreportcard/> and values from 1990 - 1998 and 2015 were extracted from their NAEP data files. All subtest names were reduced and homogenized to one word (e.g. \"Reading to gain information\" became \"information\"). The various subtest names for univariate transformation constants were all homogenized to \"univariate\".",
    "version": "1.0.0",
    "maintainer": "Sun-joo Lee <sjlee@air.org>",
    "author": "Sun-joo Lee [aut, cre],\n\t\tEric Buehler [aut],\n        Paul Bailey [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NAEPirtparams",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NAEPirtparams IRT Parameters for the National Assessment of Education Progress This data package contains the Item Response Theory (IRT) parameters for the National Center for Education Statistics (NCES) items used on the National Assessment of Education Progress (NAEP) from 1990 to 2015. The values in these tables are used along with NAEP data to turn student item responses into scores and include information about item difficulty, discrimination, and guessing parameter for 3 parameter logit (3PL) items. Parameters for Generalized Partial Credit Model (GPCM) items are also included. The adjustments table contains the information regarding the treatment of items (e.g., deletion of an item or a collapsing of response categories), when these items did not appear to fit the item response models used to describe the NAEP data. Transformation constants change the score estimates that are obtained from the IRT scaling program to the NAEP reporting metric. Values from the years 2000 - 2013 were taken from the NCES website <https://nces.ed.gov/nationsreportcard/> and values from 1990 - 1998 and 2015 were extracted from their NAEP data files. All subtest names were reduced and homogenized to one word (e.g. \"Reading to gain information\" became \"information\"). The various subtest names for univariate transformation constants were all homogenized to \"univariate\".  "
  },
  {
    "id": 5203,
    "package_name": "NBShiny",
    "title": "Interactive Document for Working with Naive Bayes Classification",
    "description": "An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NBShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NBShiny Interactive Document for Working with Naive Bayes Classification An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.  "
  },
  {
    "id": 5204,
    "package_name": "NBShiny2",
    "title": "Interactive Document for Working with Naive Bayes Classification",
    "description": "An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NBShiny2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NBShiny2 Interactive Document for Working with Naive Bayes Classification An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.  "
  },
  {
    "id": 5205,
    "package_name": "NBShiny3",
    "title": "Interactive Document for Working with Naive Bayes Classification",
    "description": "An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NBShiny3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NBShiny3 Interactive Document for Working with Naive Bayes Classification An interactive document on  the topic of naive Bayes classification  analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://kartikeyab.shinyapps.io/NBShiny/>.  "
  },
  {
    "id": 5213,
    "package_name": "NDP",
    "title": "Interactive Presentation for Working with Normal Distribution",
    "description": "An interactive presentation on  the topic of normal distribution using 'rmarkdown' and 'shiny' packages. It is helpful to those who want to learn normal distribution quickly and get a hands on experience. The presentation has a template for solving problems on normal distribution. Runtime examples are provided in the package function as well as at  <https://kartikeyastat.shinyapps.io/NormalDistribution/>. ",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NDP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NDP Interactive Presentation for Working with Normal Distribution An interactive presentation on  the topic of normal distribution using 'rmarkdown' and 'shiny' packages. It is helpful to those who want to learn normal distribution quickly and get a hands on experience. The presentation has a template for solving problems on normal distribution. Runtime examples are provided in the package function as well as at  <https://kartikeyastat.shinyapps.io/NormalDistribution/>.   "
  },
  {
    "id": 5256,
    "package_name": "NNbenchmark",
    "title": "Datasets and Functions to Benchmark Neural Network Packages",
    "description": "Datasets and functions to benchmark (convergence, speed, ease of use) R packages dedicated to regression with neural networks (no classification in this version). The templates for the tested packages are available in the R, R Markdown and HTML formats at <https://github.com/pkR-pkR/NNbenchmarkTemplates> and <https://theairbend3r.github.io/NNbenchmarkWeb/index.html>. The submitted article to the R-Journal can be read at <https://www.inmodelia.com/gsoc2020.html>.",
    "version": "3.2.0",
    "maintainer": "Patrice Kiener <rpackages@inmodelia.com>",
    "author": "Patrice Kiener [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0505-9920>),\n  Christophe Dutang [aut] (ORCID:\n    <https://orcid.org/0000-0001-6732-1501>),\n  Salsabila Mahdi [aut] (ORCID: <https://orcid.org/0000-0002-2559-4154>),\n  Akshaj Verma [aut] (ORCID: <https://orcid.org/0000-0002-3936-0033>),\n  Yifu Yan [ctb]",
    "url": "https://github.com/pkR-pkR/NNbenchmark",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NNbenchmark",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NNbenchmark Datasets and Functions to Benchmark Neural Network Packages Datasets and functions to benchmark (convergence, speed, ease of use) R packages dedicated to regression with neural networks (no classification in this version). The templates for the tested packages are available in the R, R Markdown and HTML formats at <https://github.com/pkR-pkR/NNbenchmarkTemplates> and <https://theairbend3r.github.io/NNbenchmarkWeb/index.html>. The submitted article to the R-Journal can be read at <https://www.inmodelia.com/gsoc2020.html>.  "
  },
  {
    "id": 5278,
    "package_name": "NST",
    "title": "Normalized Stochasticity Ratio",
    "description": "To estimate ecological stochasticity in community assembly. Understanding the community assembly mechanisms controlling biodiversity patterns is a central issue in ecology. Although it is generally accepted that both deterministic and stochastic processes play important roles in community assembly, quantifying their relative importance is challenging. The new index, normalized stochasticity ratio (NST), is to estimate ecological stochasticity, i.e. relative importance of stochastic processes, in community assembly. With functions in this package, NST can be calculated based on different similarity metrics and/or different null model algorithms, as well as some previous indexes, e.g. previous Stochasticity Ratio (ST), Standard Effect Size (SES), modified Raup-Crick metrics (RC). Functions for permutational test and bootstrapping analysis are also included. Previous ST is published by Zhou et al (2014) <doi:10.1073/pnas.1324044111>. NST is modified from ST by considering two alternative situations and normalizing the index to range from 0 to 1 (Ning et al 2019) <doi:10.1073/pnas.1904623116>. A modified version, MST, is a special case of NST, used in some recent or upcoming publications, e.g. Liang et al (2020) <doi:10.1016/j.soilbio.2020.108023>. SES is calculated as described in Kraft et al (2011) <doi:10.1126/science.1208584>. RC is calculated as reported by Chase et al (2011) <doi:10.1890/ES10-00117.1> and Stegen et al (2013) <doi:10.1038/ismej.2013.93>. Version 3 added NST based on phylogenetic beta diversity, used by Ning et al (2020) <doi:10.1038/s41467-020-18560-z>.",
    "version": "3.1.10",
    "maintainer": "Daliang Ning <ningdaliang@ou.edu>",
    "author": "Daliang Ning",
    "url": "https://github.com/DaliangNing/NST",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NST Normalized Stochasticity Ratio To estimate ecological stochasticity in community assembly. Understanding the community assembly mechanisms controlling biodiversity patterns is a central issue in ecology. Although it is generally accepted that both deterministic and stochastic processes play important roles in community assembly, quantifying their relative importance is challenging. The new index, normalized stochasticity ratio (NST), is to estimate ecological stochasticity, i.e. relative importance of stochastic processes, in community assembly. With functions in this package, NST can be calculated based on different similarity metrics and/or different null model algorithms, as well as some previous indexes, e.g. previous Stochasticity Ratio (ST), Standard Effect Size (SES), modified Raup-Crick metrics (RC). Functions for permutational test and bootstrapping analysis are also included. Previous ST is published by Zhou et al (2014) <doi:10.1073/pnas.1324044111>. NST is modified from ST by considering two alternative situations and normalizing the index to range from 0 to 1 (Ning et al 2019) <doi:10.1073/pnas.1904623116>. A modified version, MST, is a special case of NST, used in some recent or upcoming publications, e.g. Liang et al (2020) <doi:10.1016/j.soilbio.2020.108023>. SES is calculated as described in Kraft et al (2011) <doi:10.1126/science.1208584>. RC is calculated as reported by Chase et al (2011) <doi:10.1890/ES10-00117.1> and Stegen et al (2013) <doi:10.1038/ismej.2013.93>. Version 3 added NST based on phylogenetic beta diversity, used by Ning et al (2020) <doi:10.1038/s41467-020-18560-z>.  "
  },
  {
    "id": 5316,
    "package_name": "NetSimR",
    "title": "Actuarial Functions for Non-Life Insurance Modelling",
    "description": "Assists actuaries and other insurance modellers in pricing,\n    reserving and capital modelling for non-life insurance and\n    reinsurance modelling. Provides functions that help model\n    excess levels, capping and pure Incurred but not reported\n    claims (pure IBNR).\n    Includes capped mean, exposure curves and increased limit\n    factor curves (ILFs) for LogNormal, Gamma, Pareto, Sliced\n    LogNormal-Pareto and Sliced Gamma-Pareto distributions.\n    Includes mean, probability density function (pdf), cumulative\n    probability function (cdf) and inverse cumulative probability\n    function for Sliced LogNormal-Pareto and Sliced Gamma-Pareto\n    distributions.\n    Includes calculating pure IBNR exposure with LogNormal and\n    Gamma distribution for reporting delay.\n    Includes three shiny tools, one to simulate insurance claims applying\n    reinsurance structures, fit generalised linear models and fit claims\n    frequency or severity distributions.\n    Methods used in the package refer to\n    Free for All by Yiannis Parizas (2023) <https://www.theactuary.com/2023/03/02/free-all>;\n    Escaping the triangle by Yiannis Parizas (2019) <https://www.theactuary.com/features/2019/06/2019/06/05/escaping-triangle>;\n    Take to excess by Yiannis Parizas (2019) <https://www.theactuary.com/features/2019/03/2019/03/06/taken-excess>.",
    "version": "0.1.5",
    "maintainer": "Yiannis Parizas <yiannis.parizas@gmail.com>",
    "author": "Yiannis Parizas [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NetSimR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetSimR Actuarial Functions for Non-Life Insurance Modelling Assists actuaries and other insurance modellers in pricing,\n    reserving and capital modelling for non-life insurance and\n    reinsurance modelling. Provides functions that help model\n    excess levels, capping and pure Incurred but not reported\n    claims (pure IBNR).\n    Includes capped mean, exposure curves and increased limit\n    factor curves (ILFs) for LogNormal, Gamma, Pareto, Sliced\n    LogNormal-Pareto and Sliced Gamma-Pareto distributions.\n    Includes mean, probability density function (pdf), cumulative\n    probability function (cdf) and inverse cumulative probability\n    function for Sliced LogNormal-Pareto and Sliced Gamma-Pareto\n    distributions.\n    Includes calculating pure IBNR exposure with LogNormal and\n    Gamma distribution for reporting delay.\n    Includes three shiny tools, one to simulate insurance claims applying\n    reinsurance structures, fit generalised linear models and fit claims\n    frequency or severity distributions.\n    Methods used in the package refer to\n    Free for All by Yiannis Parizas (2023) <https://www.theactuary.com/2023/03/02/free-all>;\n    Escaping the triangle by Yiannis Parizas (2019) <https://www.theactuary.com/features/2019/06/2019/06/05/escaping-triangle>;\n    Take to excess by Yiannis Parizas (2019) <https://www.theactuary.com/features/2019/03/2019/03/06/taken-excess>.  "
  },
  {
    "id": 5339,
    "package_name": "NitrogenUptake2016",
    "title": "Data and Source Code From: Nitrogen Uptake and Allocation\nEstimates for Spartina Alterniflora and Distichlis Spicata",
    "description": "Contains data, code, and figures from Hill et al. 2018a (Journal of Experimental Marine Biology and Ecology; <DOI: 10.1016/j.jembe.2018.07.006>) and Hill et al. 2018b (Data In Brief <DOI: 10.1016/j.dib.2018.09.133>). Datasets document plant allometry, stem heights, nutrient and stable isotope content, and sediment denitrification enzyme assays. The data and analysis offer an examination of nitrogen uptake and allocation in two salt marsh plant species.",
    "version": "0.2.3",
    "maintainer": "Troy D. Hill <Hill.Troy@gmail.com>",
    "author": "Troy D. Hill, Nathalie R. Sommer, Caroline R. Kanaskie, Emily A. Santos, and Autumn J. Oczkowski",
    "url": "https://github.com/troyhill/NitrogenUptake2016",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NitrogenUptake2016",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NitrogenUptake2016 Data and Source Code From: Nitrogen Uptake and Allocation\nEstimates for Spartina Alterniflora and Distichlis Spicata Contains data, code, and figures from Hill et al. 2018a (Journal of Experimental Marine Biology and Ecology; <DOI: 10.1016/j.jembe.2018.07.006>) and Hill et al. 2018b (Data In Brief <DOI: 10.1016/j.dib.2018.09.133>). Datasets document plant allometry, stem heights, nutrient and stable isotope content, and sediment denitrification enzyme assays. The data and analysis offer an examination of nitrogen uptake and allocation in two salt marsh plant species.  "
  },
  {
    "id": 5345,
    "package_name": "NobBS",
    "title": "Nowcasting by Bayesian Smoothing",
    "description": "A Bayesian approach to estimate the number of occurred-but-not-yet-reported cases from incomplete, time-stamped reporting data for disease outbreaks. 'NobBS' learns the reporting delay distribution and the time evolution of the epidemic curve to produce smoothed nowcasts in both stable and time-varying case reporting settings, as described in McGough et al. (2020) <doi:10.1371/journal.pcbi.1007735>.  ",
    "version": "1.1.0",
    "maintainer": "Rami Yaari <ry2460@cumc.columbia.edu>",
    "author": "Rami Yaari [cre, aut],\n  Rodrigo Zepeda Tello [aut, ctb],\n  Sarah McGough [aut, ctb],\n  Nicolas Menzies [aut],\n  Marc Lipsitch [aut],\n  Michael Johansson [aut],\n  Teresa Yamana [ctb],\n  Matteo Perini [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NobBS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NobBS Nowcasting by Bayesian Smoothing A Bayesian approach to estimate the number of occurred-but-not-yet-reported cases from incomplete, time-stamped reporting data for disease outbreaks. 'NobBS' learns the reporting delay distribution and the time evolution of the epidemic curve to produce smoothed nowcasts in both stable and time-varying case reporting settings, as described in McGough et al. (2020) <doi:10.1371/journal.pcbi.1007735>.    "
  },
  {
    "id": 5360,
    "package_name": "NumericEnsembles",
    "title": "Automatically Runs 18 Individual and 14 Ensembles of Models",
    "description": "Automatically runs 18 individual models and 14 ensembles on numeric data, for a total of 32 models. The package automatically returns complete results on all 32 models,\n    30 charts and six tables. The user simply provides the tidy data, and answers a few questions (for example, how many times would you like to resample the data).\n    From there the package randomly splits the data into train, test and validation sets, fits each of models on the training data, makes predictions on the test and validation sets,\n    measures root mean squared error (RMSE), removes features above a user-set level of Variance Inflation Factor, and has several optional features including scaling\n    all numeric data, four different ways to handle strings in the data. Perhaps the most significant feature is the package's ability to make predictions\n    using the 32 pre trained models on totally new (untrained) data if the user selects that feature. This feature alone represents a very effective solution\n    to the issue of reproducibility of models in data science. The package can also randomly resample the data as many times as the user sets, thus giving more\n    accurate results than a single run. The graphs provide many results that are not typically found. For example, the package automatically calculates the Kolmogorov-Smirnov\n    test for each of the 32 models and plots a bar chart of the results, a bias bar chart of each of the 32 models, as well as several plots for exploratory data\n    analysis (automatic histograms of the numeric data, automatic histograms of the numeric data). The package also automatically creates a summary report\n    that can be both sorted and searched for each of the 32 models, including RMSE, bias, train RMSE, test RMSE, validation RMSE, overfitting and duration.\n    The best results on the holdout data typically beat the best results in data science competitions and published results for the same data set.",
    "version": "0.10.3",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "author": "Russ Conte [aut, cre, cph]",
    "url": "http://www.NumericEnsembles.com,\nhttps://github.com/InfiniteCuriosity/NumericEnsembles",
    "bug_reports": "https://github.com/InfiniteCuriosity/NumericEnsembles/issues",
    "repository": "https://cran.r-project.org/package=NumericEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NumericEnsembles Automatically Runs 18 Individual and 14 Ensembles of Models Automatically runs 18 individual models and 14 ensembles on numeric data, for a total of 32 models. The package automatically returns complete results on all 32 models,\n    30 charts and six tables. The user simply provides the tidy data, and answers a few questions (for example, how many times would you like to resample the data).\n    From there the package randomly splits the data into train, test and validation sets, fits each of models on the training data, makes predictions on the test and validation sets,\n    measures root mean squared error (RMSE), removes features above a user-set level of Variance Inflation Factor, and has several optional features including scaling\n    all numeric data, four different ways to handle strings in the data. Perhaps the most significant feature is the package's ability to make predictions\n    using the 32 pre trained models on totally new (untrained) data if the user selects that feature. This feature alone represents a very effective solution\n    to the issue of reproducibility of models in data science. The package can also randomly resample the data as many times as the user sets, thus giving more\n    accurate results than a single run. The graphs provide many results that are not typically found. For example, the package automatically calculates the Kolmogorov-Smirnov\n    test for each of the 32 models and plots a bar chart of the results, a bias bar chart of each of the 32 models, as well as several plots for exploratory data\n    analysis (automatic histograms of the numeric data, automatic histograms of the numeric data). The package also automatically creates a summary report\n    that can be both sorted and searched for each of the 32 models, including RMSE, bias, train RMSE, test RMSE, validation RMSE, overfitting and duration.\n    The best results on the holdout data typically beat the best results in data science competitions and published results for the same data set.  "
  },
  {
    "id": 5391,
    "package_name": "OPL",
    "title": "Optimal Policy Learning",
    "description": "Provides functions for optimal policy learning in socioeconomic applications helping users to learn the most effective policies based \n\ton data in order to maximize empirical welfare. Specifically, 'OPL' allows to find \"treatment assignment rules\" that maximize the overall \n\twelfare, defined as the sum  of the policy effects estimated over all the policy beneficiaries. Documentation about 'OPL' is provided by  \n\tseveral international articles via Athey et al (2021, <doi:10.3982/ECTA15732>), Kitagawa et al (2018, <doi:10.3982/ECTA13288>),\n        Cerulli (2022, <doi:10.1080/13504851.2022.2032577>), the paper by Cerulli (2021, <doi:10.1080/13504851.2020.1820939>) \n\tand the book by Gareth et al (2013, <doi:10.1007/978-1-4614-7138-7>).",
    "version": "1.0.2",
    "maintainer": "Federico Brogi <federicobrogi@gmail.com>",
    "author": "Federico Brogi [aut, cre],\n  Barbara Guardabascio [aut],\n  Giovanni Cerulli [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OPL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OPL Optimal Policy Learning Provides functions for optimal policy learning in socioeconomic applications helping users to learn the most effective policies based \n\ton data in order to maximize empirical welfare. Specifically, 'OPL' allows to find \"treatment assignment rules\" that maximize the overall \n\twelfare, defined as the sum  of the policy effects estimated over all the policy beneficiaries. Documentation about 'OPL' is provided by  \n\tseveral international articles via Athey et al (2021, <doi:10.3982/ECTA15732>), Kitagawa et al (2018, <doi:10.3982/ECTA13288>),\n        Cerulli (2022, <doi:10.1080/13504851.2022.2032577>), the paper by Cerulli (2021, <doi:10.1080/13504851.2020.1820939>) \n\tand the book by Gareth et al (2013, <doi:10.1007/978-1-4614-7138-7>).  "
  },
  {
    "id": 5407,
    "package_name": "OSLdecomposition",
    "title": "Signal Component Analysis for Optically Stimulated Luminescence",
    "description": "Function library for the identification and separation of exponentially\n    decaying signal components in continuous-wave optically stimulated luminescence measurements.\n    A special emphasis is laid on luminescence dating with quartz, which is known for\n    systematic errors due to signal components with unequal physical behaviour.\n    Also, this package enables an easy to use signal decomposition of\n    data sets imported and analysed with the R package 'Luminescence'.\n    This includes the optional automatic creation of HTML reports. Further information and tutorials\n    can be found at <https://luminescence.de>.",
    "version": "1.1.0",
    "maintainer": "Dirk Mittelstra\u00df <dirk.mittelstrass@luminescence.de>",
    "author": "Dirk Mittelstra\u00df [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9567-8791>),\n  Sebastian Kreutzer [aut] (ORCID:\n    <https://orcid.org/0000-0002-0734-2199>),\n  Christoph Schmidt [aut] (ORCID:\n    <https://orcid.org/0000-0002-2309-3209>),\n  Jan Beyer [ths] (ORCID: <https://orcid.org/0000-0002-1403-395X>),\n  Johannes Heitmann [ths],\n  Arno Straessner [ths] (ORCID: <https://orcid.org/0000-0003-2460-6659>)",
    "url": "",
    "bug_reports": "https://github.com/DirkMittelstrass/OSLdecomposition/issues",
    "repository": "https://cran.r-project.org/package=OSLdecomposition",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OSLdecomposition Signal Component Analysis for Optically Stimulated Luminescence Function library for the identification and separation of exponentially\n    decaying signal components in continuous-wave optically stimulated luminescence measurements.\n    A special emphasis is laid on luminescence dating with quartz, which is known for\n    systematic errors due to signal components with unequal physical behaviour.\n    Also, this package enables an easy to use signal decomposition of\n    data sets imported and analysed with the R package 'Luminescence'.\n    This includes the optional automatic creation of HTML reports. Further information and tutorials\n    can be found at <https://luminescence.de>.  "
  },
  {
    "id": 5433,
    "package_name": "OmicFlow",
    "title": "Fast and Efficient (Automated) Analysis of Sparse Omics Data",
    "description": "\n    A generalised data structure for fast and efficient loading and data munching of sparse omics data. The 'OmicFlow' requires an up-front validated metadata template from the user,\n    which serves as a guide to connect all the pieces together by aligning them into a single object that is defined as an 'omics' class. \n    Once this unified structure is established, users can perform manual subsetting, visualisation, and statistical analysis, or leverage the automated 'autoFlow' method to generate a comprehensive report.",
    "version": "1.4.2",
    "maintainer": "Alem Gusinac <alem.gusinac@gmail.com>",
    "author": "Alem Gusinac [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-1896-4176>),\n  Thomas Ederveen [aut] (ORCID: <https://orcid.org/0000-0003-0068-1275>),\n  Annemarie Boleij [aut, fnd] (ORCID:\n    <https://orcid.org/0000-0003-4495-5880>)",
    "url": "https://github.com/agusinac/OmicFlow",
    "bug_reports": "https://github.com/agusinac/OmicFlow/issues",
    "repository": "https://cran.r-project.org/package=OmicFlow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OmicFlow Fast and Efficient (Automated) Analysis of Sparse Omics Data \n    A generalised data structure for fast and efficient loading and data munching of sparse omics data. The 'OmicFlow' requires an up-front validated metadata template from the user,\n    which serves as a guide to connect all the pieces together by aligning them into a single object that is defined as an 'omics' class. \n    Once this unified structure is established, users can perform manual subsetting, visualisation, and statistical analysis, or leverage the automated 'autoFlow' method to generate a comprehensive report.  "
  },
  {
    "id": 5436,
    "package_name": "OmicsPLS",
    "title": "Data Integration with Two-Way Orthogonal Partial Least Squares",
    "description": "Performs the O2PLS data integration method for two datasets, yielding joint and data-specific parts for each dataset.\n    The algorithm automatically switches to a memory-efficient approach to fit O2PLS to high dimensional data.\n    It provides a rigorous and a faster alternative cross-validation method to select the number of components,\n    as well as functions to report proportions of explained variation and to construct plots of the results. \n    See the software article by el Bouhaddani et al (2018) <doi:10.1186/s12859-018-2371-3>, \n    and Trygg and Wold (2003) <doi:10.1002/cem.775>.\n    It also performs Sparse Group (Penalized) O2PLS, see Gu et al (2020) <doi:10.1186/s12859-021-03958-3> and cross-validation for the degree of sparsity. ",
    "version": "2.1.0",
    "maintainer": "Said el Bouhaddani <s.elbouhaddani@umcutrecht.nl>",
    "author": "Said el Bouhaddani [aut, cre],\n  Zander Gu [aut],\n  Jeanine Houwing-Duistermaat [aut],\n  Geurt Jongbloed [aut],\n  Szymon Kielbasa [aut],\n  Hae-Won Uh [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OmicsPLS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OmicsPLS Data Integration with Two-Way Orthogonal Partial Least Squares Performs the O2PLS data integration method for two datasets, yielding joint and data-specific parts for each dataset.\n    The algorithm automatically switches to a memory-efficient approach to fit O2PLS to high dimensional data.\n    It provides a rigorous and a faster alternative cross-validation method to select the number of components,\n    as well as functions to report proportions of explained variation and to construct plots of the results. \n    See the software article by el Bouhaddani et al (2018) <doi:10.1186/s12859-018-2371-3>, \n    and Trygg and Wold (2003) <doi:10.1002/cem.775>.\n    It also performs Sparse Group (Penalized) O2PLS, see Gu et al (2020) <doi:10.1186/s12859-021-03958-3> and cross-validation for the degree of sparsity.   "
  },
  {
    "id": 5459,
    "package_name": "OneTwoSamples",
    "title": "Deal with One and Two (Normal) Samples",
    "description": "We introduce an R function\n        one_two_sample() which can deal with one and two (normal)\n        samples, Ying-Ying Zhang, Yi Wei (2012) <doi:10.2991/asshm-13.2013.29>. \n        For one normal sample x, the function reports\n        descriptive statistics, plot, interval estimation and test of\n        hypothesis of x. For two normal samples x and y, the function\n        reports descriptive statistics, plot, interval estimation and\n        test of hypothesis of x and y, respectively. It also reports\n        interval estimation and test of hypothesis of mu1-mu2 (the\n        difference of the means of x and y) and sigma1^2 / sigma2^2\n        (the ratio of the variances of x and y), tests whether x and y\n        are from the same population, finds the correlation coefficient\n        of x and y if x and y have the same length.",
    "version": "1.2-0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Frederic Bertrand [cre] (ORCID:\n    <https://orcid.org/0000-0002-0837-8281>),\n  Ying-Ying Zhang (Robert) [aut]",
    "url": "https://github.com/fbertran/OneTwoSamples,\nhttps://fbertran.github.io/OneTwoSamples/",
    "bug_reports": "https://github.com/fbertran/OneTwoSamples/issues",
    "repository": "https://cran.r-project.org/package=OneTwoSamples",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OneTwoSamples Deal with One and Two (Normal) Samples We introduce an R function\n        one_two_sample() which can deal with one and two (normal)\n        samples, Ying-Ying Zhang, Yi Wei (2012) <doi:10.2991/asshm-13.2013.29>. \n        For one normal sample x, the function reports\n        descriptive statistics, plot, interval estimation and test of\n        hypothesis of x. For two normal samples x and y, the function\n        reports descriptive statistics, plot, interval estimation and\n        test of hypothesis of x and y, respectively. It also reports\n        interval estimation and test of hypothesis of mu1-mu2 (the\n        difference of the means of x and y) and sigma1^2 / sigma2^2\n        (the ratio of the variances of x and y), tests whether x and y\n        are from the same population, finds the correlation coefficient\n        of x and y if x and y have the same length.  "
  },
  {
    "id": 5465,
    "package_name": "OpenImageR",
    "title": "An Image Processing Toolkit",
    "description": "Incorporates functions for image preprocessing, filtering and image recognition. The package takes advantage of 'RcppArmadillo' to speed up computationally intensive functions. The histogram of oriented gradients descriptor is a modification of the 'findHOGFeatures' function of the 'SimpleCV' computer vision platform, the average_hash(), dhash() and phash() functions are based on the 'ImageHash' python library. The Gabor Feature Extraction functions are based on 'Matlab' code of the paper, \"CloudID: Trustworthy cloud-based and cross-enterprise biometric identification\" by M. Haghighat, S. Zonouz, M. Abdel-Mottaleb, Expert Systems with Applications, vol. 42, no. 21, pp. 7905-7916, 2015, <doi:10.1016/j.eswa.2015.06.025>. The 'SLIC' and 'SLICO' superpixel algorithms were explained in detail in (i) \"SLIC Superpixels Compared to State-of-the-art Superpixel Methods\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, num. 11, p. 2274-2282, May 2012, <doi:10.1109/TPAMI.2012.120> and (ii) \"SLIC Superpixels\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, EPFL Technical Report no. 149300, June 2010.",
    "version": "1.3.0",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>),\n  Sight Machine [cph] (findHOGFeatures function of the SimpleCV computer\n    vision platform),\n  Johannes Buchner [cph] (average_hash, dhash and phash functions of the\n    ImageHash python library),\n  Mohammad Haghighat [cph] (Gabor Feature Extraction),\n  Radhakrishna Achanta [cph] (Author of the C++ code of the SLIC and\n    SLICO algorithms (for commercial use please contact the author)),\n  Oleh Onyshchak [cph] (Author of the Python code of the WarpAffine\n    function)",
    "url": "https://github.com/mlampros/OpenImageR",
    "bug_reports": "https://github.com/mlampros/OpenImageR/issues",
    "repository": "https://cran.r-project.org/package=OpenImageR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OpenImageR An Image Processing Toolkit Incorporates functions for image preprocessing, filtering and image recognition. The package takes advantage of 'RcppArmadillo' to speed up computationally intensive functions. The histogram of oriented gradients descriptor is a modification of the 'findHOGFeatures' function of the 'SimpleCV' computer vision platform, the average_hash(), dhash() and phash() functions are based on the 'ImageHash' python library. The Gabor Feature Extraction functions are based on 'Matlab' code of the paper, \"CloudID: Trustworthy cloud-based and cross-enterprise biometric identification\" by M. Haghighat, S. Zonouz, M. Abdel-Mottaleb, Expert Systems with Applications, vol. 42, no. 21, pp. 7905-7916, 2015, <doi:10.1016/j.eswa.2015.06.025>. The 'SLIC' and 'SLICO' superpixel algorithms were explained in detail in (i) \"SLIC Superpixels Compared to State-of-the-art Superpixel Methods\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, num. 11, p. 2274-2282, May 2012, <doi:10.1109/TPAMI.2012.120> and (ii) \"SLIC Superpixels\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, EPFL Technical Report no. 149300, June 2010.  "
  },
  {
    "id": 5520,
    "package_name": "PAMhm",
    "title": "Generate Heatmaps Based on Partitioning Around Medoids (PAM)",
    "description": "Data are partitioned (clustered) into k clusters \"around medoids\", which is\n    a more robust version of K-means implemented in the function pam() in the 'cluster' package.\n    The PAM algorithm is described in Kaufman and Rousseeuw (1990) <doi:10.1002/9780470316801>.\n    Please refer to the pam() function documentation for more references.\n    Clustered data is plotted as a split heatmap allowing visualisation of representative\n    \"group-clusters\" (medoids) in the data as separated fractions of the graph while those\n    \"sub-clusters\" are visualised as a traditional heatmap based on hierarchical clustering.",
    "version": "0.1.2",
    "maintainer": "Vidal Fey <vidal.fey@gmail.com>",
    "author": "Vidal Fey [aut, cre],\n  Henri Sara [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PAMhm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PAMhm Generate Heatmaps Based on Partitioning Around Medoids (PAM) Data are partitioned (clustered) into k clusters \"around medoids\", which is\n    a more robust version of K-means implemented in the function pam() in the 'cluster' package.\n    The PAM algorithm is described in Kaufman and Rousseeuw (1990) <doi:10.1002/9780470316801>.\n    Please refer to the pam() function documentation for more references.\n    Clustered data is plotted as a split heatmap allowing visualisation of representative\n    \"group-clusters\" (medoids) in the data as separated fractions of the graph while those\n    \"sub-clusters\" are visualised as a traditional heatmap based on hierarchical clustering.  "
  },
  {
    "id": 5522,
    "package_name": "PAMpal",
    "title": "Load and Process Passive Acoustic Data",
    "description": "Tools for loading and processing passive acoustic data. Read in data\n    that has been processed in 'Pamguard' (<https://www.pamguard.org/>), apply a suite\n    processing functions, and export data for reports or external modeling tools. Parameter \n    calculations implement methods by Oswald et al (2007) <doi:10.1121/1.2743157>,\n    Griffiths et al (2020) <doi:10.1121/10.0001229> and Baumann-Pickering et al (2010)\n    <doi:10.1121/1.3479549>.",
    "version": "1.4.4",
    "maintainer": "Taiki Sakai <taiki.sakai@noaa.gov>",
    "author": "Taiki Sakai [aut, cre],\n  Jay Barlow [ctb],\n  Emily Griffiths [ctb],\n  Michael Oswald [ctb],\n  Simone Baumann-Pickering [ctb],\n  Julie Oswald [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PAMpal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PAMpal Load and Process Passive Acoustic Data Tools for loading and processing passive acoustic data. Read in data\n    that has been processed in 'Pamguard' (<https://www.pamguard.org/>), apply a suite\n    processing functions, and export data for reports or external modeling tools. Parameter \n    calculations implement methods by Oswald et al (2007) <doi:10.1121/1.2743157>,\n    Griffiths et al (2020) <doi:10.1121/10.0001229> and Baumann-Pickering et al (2010)\n    <doi:10.1121/1.3479549>.  "
  },
  {
    "id": 5555,
    "package_name": "PCMBase",
    "title": "Simulation and Likelihood Calculation of Phylogenetic\nComparative Models",
    "description": "Phylogenetic comparative methods represent models of continuous trait \n  data associated with the tips of a phylogenetic tree. Examples of such models \n  are Gaussian continuous time branching stochastic processes such as Brownian \n  motion (BM) and Ornstein-Uhlenbeck (OU) processes, which regard the data at the \n  tips of the tree as an observed (final) state of a Markov process starting from \n  an initial state at the root and evolving along the branches of the tree. The \n  PCMBase R package provides a general framework for manipulating such models. \n  This framework consists of an application programming interface for specifying \n  data and model parameters, and efficient algorithms for simulating trait evolution \n  under a model and calculating the likelihood of model parameters for an assumed\n  model and trait data. The package implements a growing collection of models, \n  which currently includes BM, OU, BM/OU with jumps, two-speed OU as well as mixed \n  Gaussian models, in which different types of the above models can be associated \n  with different branches of the tree. The PCMBase package is limited to \n  trait-simulation and likelihood calculation of (mixed) Gaussian phylogenetic \n  models. The PCMFit package provides functionality for inference of \n  these models to tree and trait data. The package web-site \n  <https://venelin.github.io/PCMBase/>\n  provides access to the documentation and other resources. ",
    "version": "1.2.15",
    "maintainer": "Venelin Mitov <vmitov@gmail.com>",
    "author": "Venelin Mitov [aut, cre, cph] (<a\n    href=\"https://venelin.github.io\">venelin.github.io</a>),\n  Krzysztof Bartoszek [ctb],\n  Georgios Asimomitis [ctb],\n  Tanja Stadler [ths]",
    "url": "https://venelin.github.io/PCMBase/, https://venelin.github.io",
    "bug_reports": "https://github.com/venelin/PCMBase/issues",
    "repository": "https://cran.r-project.org/package=PCMBase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCMBase Simulation and Likelihood Calculation of Phylogenetic\nComparative Models Phylogenetic comparative methods represent models of continuous trait \n  data associated with the tips of a phylogenetic tree. Examples of such models \n  are Gaussian continuous time branching stochastic processes such as Brownian \n  motion (BM) and Ornstein-Uhlenbeck (OU) processes, which regard the data at the \n  tips of the tree as an observed (final) state of a Markov process starting from \n  an initial state at the root and evolving along the branches of the tree. The \n  PCMBase R package provides a general framework for manipulating such models. \n  This framework consists of an application programming interface for specifying \n  data and model parameters, and efficient algorithms for simulating trait evolution \n  under a model and calculating the likelihood of model parameters for an assumed\n  model and trait data. The package implements a growing collection of models, \n  which currently includes BM, OU, BM/OU with jumps, two-speed OU as well as mixed \n  Gaussian models, in which different types of the above models can be associated \n  with different branches of the tree. The PCMBase package is limited to \n  trait-simulation and likelihood calculation of (mixed) Gaussian phylogenetic \n  models. The PCMFit package provides functionality for inference of \n  these models to tree and trait data. The package web-site \n  <https://venelin.github.io/PCMBase/>\n  provides access to the documentation and other resources.   "
  },
  {
    "id": 5566,
    "package_name": "PDE",
    "title": "Extract Tables and Sentences from PDFs with User Interface",
    "description": "The PDE (Pdf Data Extractor) allows the extraction of \n    information and tables optionally based on search words from \n    PDF (Portable Document Format) files and enables the visualization \n    of the results, both by providing a convenient user-interface. ",
    "version": "1.4.10",
    "maintainer": "Erik Stricker <erik.stricker@gmx.com>",
    "author": "Erik Stricker [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PDE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PDE Extract Tables and Sentences from PDFs with User Interface The PDE (Pdf Data Extractor) allows the extraction of \n    information and tables optionally based on search words from \n    PDF (Portable Document Format) files and enables the visualization \n    of the results, both by providing a convenient user-interface.   "
  },
  {
    "id": 5584,
    "package_name": "PERSUADE",
    "title": "Parametric Survival Model Selection for Decision-Analytic Models",
    "description": "Provides a standardized framework to support the selection\n    and evaluation of parametric survival models for time-to-event data.\n    Includes tools for visualizing survival data, checking proportional\n    hazards assumptions (Grambsch and Therneau, 1994,\n    <doi:10.1093/biomet/81.3.515>), comparing parametric (Ishak and\n    colleagues, 2013, <doi:10.1007/s40273-013-0064-3>), spline (Royston\n    and Parmar, 2002, <doi:10.1002/sim.1203>) and cure models, examining\n    hazard functions, and evaluating model extrapolation. Methods are\n    consistent with recommendations in the NICE Decision Support Unit\n    Technical Support Documents (14 and 21\n    <https://sheffield.ac.uk/nice-dsu/tsds/survival-analysis>). Results\n    are structured to facilitate integration into decision-analytic\n    models, and reports can be generated with 'rmarkdown'. The package\n    builds on existing tools including 'flexsurv' (Jackson, 2016,\n    <doi:10.18637/jss.v070.i08>)) and 'flexsurvcure' for estimating cure\n    models.",
    "version": "0.1.2",
    "maintainer": "Bram Ramaekers <bram.ramaekers@mumc.nl>",
    "author": "Bram Ramaekers [aut, cre, ths] (ORCID:\n    <https://orcid.org/0000-0001-5785-9228>),\n  Xavier Pouwels [ctb] (ORCID: <https://orcid.org/0000-0003-3563-0013>),\n  Sabine Grimm [ths] (ORCID: <https://orcid.org/0000-0002-2175-7999>),\n  Manuela Joore [ths] (ORCID: <https://orcid.org/0000-0002-5649-6768>)",
    "url": "https://github.com/Bram-R/PERSUADE",
    "bug_reports": "https://github.com/Bram-R/PERSUADE/issues",
    "repository": "https://cran.r-project.org/package=PERSUADE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PERSUADE Parametric Survival Model Selection for Decision-Analytic Models Provides a standardized framework to support the selection\n    and evaluation of parametric survival models for time-to-event data.\n    Includes tools for visualizing survival data, checking proportional\n    hazards assumptions (Grambsch and Therneau, 1994,\n    <doi:10.1093/biomet/81.3.515>), comparing parametric (Ishak and\n    colleagues, 2013, <doi:10.1007/s40273-013-0064-3>), spline (Royston\n    and Parmar, 2002, <doi:10.1002/sim.1203>) and cure models, examining\n    hazard functions, and evaluating model extrapolation. Methods are\n    consistent with recommendations in the NICE Decision Support Unit\n    Technical Support Documents (14 and 21\n    <https://sheffield.ac.uk/nice-dsu/tsds/survival-analysis>). Results\n    are structured to facilitate integration into decision-analytic\n    models, and reports can be generated with 'rmarkdown'. The package\n    builds on existing tools including 'flexsurv' (Jackson, 2016,\n    <doi:10.18637/jss.v070.i08>)) and 'flexsurvcure' for estimating cure\n    models.  "
  },
  {
    "id": 5596,
    "package_name": "PHInfiniteEstimates",
    "title": "Tools for Inference in the Presence of a Monotone Likelihood",
    "description": "Proportional hazards estimation in the presence of a partially monotone likelihood has difficulties, in that finite estimators do not exist.  These difficulties are related to those arising from logistic and multinomial regression.  References for methods are given in the separate function documents.  Supported by grant NSF DMS 1712839.",
    "version": "2.9.5",
    "maintainer": "John E. Kolassa <kolassa@stat.rutgers.edu>",
    "author": "John E. Kolassa and Juan Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PHInfiniteEstimates",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PHInfiniteEstimates Tools for Inference in the Presence of a Monotone Likelihood Proportional hazards estimation in the presence of a partially monotone likelihood has difficulties, in that finite estimators do not exist.  These difficulties are related to those arising from logistic and multinomial regression.  References for methods are given in the separate function documents.  Supported by grant NSF DMS 1712839.  "
  },
  {
    "id": 5605,
    "package_name": "PJCcalculator",
    "title": "PROs-Joint Contrast (PJC) Calculator",
    "description": "Computes the Patient-Reported Outcomes (PROs) Joint Contrast (PJC), \n a residual-based summary that captures information left over after accounting \n for the clinical Disease Activity index for Psoriatic Arthritis (cDAPSA). \n PROs (pain and patient global assessment) and joint counts (swollen and tender) \n are standardized, then each component is adjusted for standardized cDAPSA using \n natural spline coefficients that were derived from previously published models. \n The resulting residuals are standardized and combined using fixed principal \n component loadings, to yield a continuous PJC score and quartile groupings. \n This package provides a calculator for applying those published coefficients to new datasets; \n it does not itself estimate spline models or principal components.",
    "version": "0.1.3",
    "maintainer": "Ning Meng <nmeng2@jh.edu>",
    "author": "Ning Meng [aut, cre],\n  Ji Soo Kim [aut],\n  Ana-Maria Orbai [aut],\n  Scott L. Zeger [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PJCcalculator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PJCcalculator PROs-Joint Contrast (PJC) Calculator Computes the Patient-Reported Outcomes (PROs) Joint Contrast (PJC), \n a residual-based summary that captures information left over after accounting \n for the clinical Disease Activity index for Psoriatic Arthritis (cDAPSA). \n PROs (pain and patient global assessment) and joint counts (swollen and tender) \n are standardized, then each component is adjusted for standardized cDAPSA using \n natural spline coefficients that were derived from previously published models. \n The resulting residuals are standardized and combined using fixed principal \n component loadings, to yield a continuous PJC score and quartile groupings. \n This package provides a calculator for applying those published coefficients to new datasets; \n it does not itself estimate spline models or principal components.  "
  },
  {
    "id": 5658,
    "package_name": "PPMiss",
    "title": "Copula-Based Estimator for Long-Range Dependent Processes under\nMissing Data",
    "description": "Implements the copula-based estimator for univariate long-range dependent processes, introduced in Pumi et al. (2023) <doi:10.1007/s00362-023-01418-z>. Notably, this estimator is capable of handling missing data and has been shown to perform exceptionally well, even when up to 70% of data is missing (as reported in <arXiv:2303.04754>) and has been found to outperform several other commonly applied estimators.",
    "version": "0.1.1",
    "maintainer": "Taiane Schaedler Prass <taianeprass@gmail.com>",
    "author": "Taiane Schaedler Prass [aut, cre, com] (ORCID:\n    <https://orcid.org/0000-0003-3136-909X>),\n  Guilherme Pumi [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-6256-3170>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PPMiss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PPMiss Copula-Based Estimator for Long-Range Dependent Processes under\nMissing Data Implements the copula-based estimator for univariate long-range dependent processes, introduced in Pumi et al. (2023) <doi:10.1007/s00362-023-01418-z>. Notably, this estimator is capable of handling missing data and has been shown to perform exceptionally well, even when up to 70% of data is missing (as reported in <arXiv:2303.04754>) and has been found to outperform several other commonly applied estimators.  "
  },
  {
    "id": 5670,
    "package_name": "PREPShiny",
    "title": "Interactive Document for Preprocessing the Dataset",
    "description": "An interactive document for preprocessing the dataset using  'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/PREPShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PREPShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PREPShiny Interactive Document for Preprocessing the Dataset An interactive document for preprocessing the dataset using  'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/PREPShiny/>.  "
  },
  {
    "id": 5676,
    "package_name": "PRISMA2020",
    "title": "Make Interactive 'PRISMA' Flow Diagrams",
    "description": "Systematic reviews should be described in a high degree of \n    methodological detail. The 'PRISMA' Statement calls for a high level of \n    reporting detail in systematic reviews and meta-analyses. An integral part \n    of the methodological description of a review is a flow diagram. \n    This package produces an interactive flow diagram that conforms to the \n    'PRISMA2020' preprint. When made interactive, the reader/user can click \n    on each box and be directed to another website or file online (e.g. a \n    detailed description of the screening methods, or a list of excluded full \n    texts), with a mouse-over tool tip that describes the information linked \n    to in more detail. Interactive versions can be saved as HTML files, \n    whilst static versions for inclusion in manuscripts can be saved as \n    HTML, PDF, PNG, SVG, PS or WEBP files.",
    "version": "1.1.1",
    "maintainer": "Chris Pritchard <chris.pritchard@ntu.ac.uk>",
    "author": "Neal Haddaway [aut] (ORCID: <https://orcid.org/0000-0003-3902-2234>),\n  Luke McGuinness [aut] (ORCID: <https://orcid.org/0000-0001-8730-9761>),\n  Chris Pritchard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1143-9751>),\n  Brennan Chapman [ctb],\n  Hossam Hammady [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PRISMA2020",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PRISMA2020 Make Interactive 'PRISMA' Flow Diagrams Systematic reviews should be described in a high degree of \n    methodological detail. The 'PRISMA' Statement calls for a high level of \n    reporting detail in systematic reviews and meta-analyses. An integral part \n    of the methodological description of a review is a flow diagram. \n    This package produces an interactive flow diagram that conforms to the \n    'PRISMA2020' preprint. When made interactive, the reader/user can click \n    on each box and be directed to another website or file online (e.g. a \n    detailed description of the screening methods, or a list of excluded full \n    texts), with a mouse-over tool tip that describes the information linked \n    to in more detail. Interactive versions can be saved as HTML files, \n    whilst static versions for inclusion in manuscripts can be saved as \n    HTML, PDF, PNG, SVG, PS or WEBP files.  "
  },
  {
    "id": 5679,
    "package_name": "PROBShiny",
    "title": "Interactive Document for Working with Basic Probability",
    "description": "An interactive document on  the topic of basic probability using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/BayesShiny/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PROBShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROBShiny Interactive Document for Working with Basic Probability An interactive document on  the topic of basic probability using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://analyticmodels.shinyapps.io/BayesShiny/>.  "
  },
  {
    "id": 5683,
    "package_name": "PROreg",
    "title": "Patient Reported Outcomes Regression Analysis",
    "description": "It offers a wide variety of techniques, such as graphics, recoding, or regression models, for a comprehensive analysis of patient-reported outcomes (PRO). Especially novel is the broad range of regression models based on the beta-binomial distribution useful for analyzing binomial data with over-dispersion in cross-sectional, longitudinal, or multidimensional response studies (see Najera-Zuloaga J., Lee D.-J. and Arostegui I. (2019) <doi:10.1002/bimj.201700251>).",
    "version": "1.3.2",
    "maintainer": "Josu Najera-Zuloaga <josu.najera@ehu.eus>",
    "author": "Josu Najera-Zuloaga [aut, cre],\n  Dae-Jin Lee [aut],\n  Inmaculada Arostegui [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PROreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROreg Patient Reported Outcomes Regression Analysis It offers a wide variety of techniques, such as graphics, recoding, or regression models, for a comprehensive analysis of patient-reported outcomes (PRO). Especially novel is the broad range of regression models based on the beta-binomial distribution useful for analyzing binomial data with over-dispersion in cross-sectional, longitudinal, or multidimensional response studies (see Najera-Zuloaga J., Lee D.-J. and Arostegui I. (2019) <doi:10.1002/bimj.201700251>).  "
  },
  {
    "id": 5684,
    "package_name": "PROscorer",
    "title": "Functions to Score Commonly-Used Patient-Reported Outcome (PRO)\nMeasures and Other Psychometric Instruments",
    "description": "An extensible repository of accurate, up-to-date functions to\n    score commonly used patient-reported outcome (PRO), quality of life\n    (QOL), and other psychometric and psychological measures.\n    'PROscorer', together with the 'PROscorerTools' package, is a system\n    to facilitate the incorporation of PRO measures into research studies\n    and clinical settings in a scientifically rigorous and reproducible\n    manner.  These packages and their vignettes are intended to help\n    establish and promote best practices for scoring PRO and PRO-like \n    measures in research.  The 'PROscorer' Instrument Descriptions vignette \n    contains descriptions of each instrument scored by 'PROscorer', complete \n    with references.  These instrument descriptions are suitable for inclusion \n    in formal study protocol documents, grant proposals, and manuscript Method\n    sections.  Each 'PROscorer' function is composed of helper functions\n    from the 'PROscorerTools' package, and users are encouraged to\n    contribute new functions to 'PROscorer'.  More scoring functions are\n    currently in development and will be added in future updates.",
    "version": "0.0.4",
    "maintainer": "Ray Baser <ray.stats@gmail.com>",
    "author": "Ray Baser [aut, cre]",
    "url": "https://github.com/raybaser/PROscorer",
    "bug_reports": "https://github.com/raybaser/PROscorer/issues",
    "repository": "https://cran.r-project.org/package=PROscorer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROscorer Functions to Score Commonly-Used Patient-Reported Outcome (PRO)\nMeasures and Other Psychometric Instruments An extensible repository of accurate, up-to-date functions to\n    score commonly used patient-reported outcome (PRO), quality of life\n    (QOL), and other psychometric and psychological measures.\n    'PROscorer', together with the 'PROscorerTools' package, is a system\n    to facilitate the incorporation of PRO measures into research studies\n    and clinical settings in a scientifically rigorous and reproducible\n    manner.  These packages and their vignettes are intended to help\n    establish and promote best practices for scoring PRO and PRO-like \n    measures in research.  The 'PROscorer' Instrument Descriptions vignette \n    contains descriptions of each instrument scored by 'PROscorer', complete \n    with references.  These instrument descriptions are suitable for inclusion \n    in formal study protocol documents, grant proposals, and manuscript Method\n    sections.  Each 'PROscorer' function is composed of helper functions\n    from the 'PROscorerTools' package, and users are encouraged to\n    contribute new functions to 'PROscorer'.  More scoring functions are\n    currently in development and will be added in future updates.  "
  },
  {
    "id": 5685,
    "package_name": "PROscorerTools",
    "title": "Tools to Score Patient-Reported Outcome (PRO) and Other\nPsychometric Measures",
    "description": "Provides a reliable and flexible toolbox to score \n    patient-reported outcome (PRO), Quality of Life (QOL), and other \n    psychometric measures. The guiding philosophy is that scoring errors can \n    be eliminated by using a limited number of well-tested, well-behaved \n    functions to score PRO-like measures. The workhorse of the package is \n    the 'scoreScale' function, which can be used to score most single-scale \n    measures. It can reverse code items that need to be reversed before \n    scoring and pro-rate scores for missing item data. Currently, three \n    different types of scores can be output: summed item scores, mean item \n    scores, and scores scaled to range from 0 to 100. The 'PROscorerTools' \n    functions can be used to write new functions that score more complex \n    measures. In fact, 'PROscorerTools' functions are the building blocks of \n    the scoring functions in the 'PROscorer' package (which is a repository \n    of functions that score specific commonly-used instruments). Users are \n    encouraged to use 'PROscorerTools' to write scoring functions for their \n    favorite PRO-like instruments, and to submit these functions for \n    inclusion in 'PROscorer' (a tutorial vignette will be added soon). The \n    long-term vision for the 'PROscorerTools' and 'PROscorer' packages is to \n    provide an easy-to-use system to facilitate the incorporation of PRO \n    measures into research studies in a scientifically rigorous and \n    reproducible manner. These packages and their vignettes are intended to \n    help establish and promote \"best practices\" for scoring and describing \n    PRO-like measures in research. ",
    "version": "0.0.4",
    "maintainer": "Ray Baser <ray.stats@gmail.com>",
    "author": "Ray Baser [aut, cre]",
    "url": "https://github.com/MSKCC-Epi-Bio/PROscorerTools",
    "bug_reports": "https://github.com/MSKCC-Epi-Bio/PROscorerTools/issues",
    "repository": "https://cran.r-project.org/package=PROscorerTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROscorerTools Tools to Score Patient-Reported Outcome (PRO) and Other\nPsychometric Measures Provides a reliable and flexible toolbox to score \n    patient-reported outcome (PRO), Quality of Life (QOL), and other \n    psychometric measures. The guiding philosophy is that scoring errors can \n    be eliminated by using a limited number of well-tested, well-behaved \n    functions to score PRO-like measures. The workhorse of the package is \n    the 'scoreScale' function, which can be used to score most single-scale \n    measures. It can reverse code items that need to be reversed before \n    scoring and pro-rate scores for missing item data. Currently, three \n    different types of scores can be output: summed item scores, mean item \n    scores, and scores scaled to range from 0 to 100. The 'PROscorerTools' \n    functions can be used to write new functions that score more complex \n    measures. In fact, 'PROscorerTools' functions are the building blocks of \n    the scoring functions in the 'PROscorer' package (which is a repository \n    of functions that score specific commonly-used instruments). Users are \n    encouraged to use 'PROscorerTools' to write scoring functions for their \n    favorite PRO-like instruments, and to submit these functions for \n    inclusion in 'PROscorer' (a tutorial vignette will be added soon). The \n    long-term vision for the 'PROscorerTools' and 'PROscorer' packages is to \n    provide an easy-to-use system to facilitate the incorporation of PRO \n    measures into research studies in a scientifically rigorous and \n    reproducible manner. These packages and their vignettes are intended to \n    help establish and promote \"best practices\" for scoring and describing \n    PRO-like measures in research.   "
  },
  {
    "id": 5686,
    "package_name": "PROsetta",
    "title": "Linking Patient-Reported Outcomes Measures",
    "description": "Perform scale linking to establish relationships between instruments\n    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>.",
    "version": "0.4.2",
    "maintainer": "Seung W. Choi <schoi@austin.utexas.edu>",
    "author": "Seung W. Choi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4777-5420>),\n  Sangdon Lim [aut] (ORCID: <https://orcid.org/0000-0002-2988-014X>),\n  Benjamin D. Schalet [ctb],\n  Aaron J. Kaat [ctb],\n  David Cella [ctb]",
    "url": "https://www.prosettastone.org/ (project description),\nhttps://choi-phd.github.io/PROsetta/ (documentation)",
    "bug_reports": "https://github.com/choi-phd/PROsetta/issues",
    "repository": "https://cran.r-project.org/package=PROsetta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PROsetta Linking Patient-Reported Outcomes Measures Perform scale linking to establish relationships between instruments\n    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>.  "
  },
  {
    "id": 5718,
    "package_name": "PTXQC",
    "title": "Quality Report Generation for MaxQuant and mzTab Results",
    "description": "Generates Proteomics (PTX) quality control (QC) reports for shotgun LC-MS data analyzed with the \n             MaxQuant software suite (from .txt files) or mzTab files (ideally from OpenMS 'QualityControl' tool).\n             Reports are customizable (target thresholds, subsetting) and available in HTML or PDF format.\n             Published in J. Proteome Res., Proteomics Quality Control: Quality Control Software for MaxQuant Results (2015)\n             <doi:10.1021/acs.jproteome.5b00780>.",
    "version": "1.1.3",
    "maintainer": "Chris Bielow <chris.bielow@bsc.fu-berlin.de>",
    "author": "Chris Bielow [aut, cre],\n  Juliane Schmachtenberg [ctb],\n  Swenja Wagner [ctb],\n  Patricia Scheil [ctb],\n  Tom Waschischek [ctb],\n  Guido Mastrobuoni [dtc, rev]",
    "url": "https://github.com/cbielow/PTXQC",
    "bug_reports": "https://github.com/cbielow/PTXQC/issues",
    "repository": "https://cran.r-project.org/package=PTXQC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PTXQC Quality Report Generation for MaxQuant and mzTab Results Generates Proteomics (PTX) quality control (QC) reports for shotgun LC-MS data analyzed with the \n             MaxQuant software suite (from .txt files) or mzTab files (ideally from OpenMS 'QualityControl' tool).\n             Reports are customizable (target thresholds, subsetting) and available in HTML or PDF format.\n             Published in J. Proteome Res., Proteomics Quality Control: Quality Control Software for MaxQuant Results (2015)\n             <doi:10.1021/acs.jproteome.5b00780>.  "
  },
  {
    "id": 5735,
    "package_name": "PacketLLM",
    "title": "Interactive 'OpenAI' Model Integration in 'RStudio'",
    "description": "Offers an interactive 'RStudio' gadget interface for communicating \n    with 'OpenAI' large language models (e.g., 'gpt-5', 'gpt-5-mini', 'gpt-5-nano') \n    (<https://platform.openai.com/docs/api-reference>). \n    Enables users to conduct multiple chat conversations simultaneously in \n    separate tabs. Supports uploading local files (R, PDF, DOCX) to provide \n    context for the models. Allows per-conversation configuration of system \n    messages (where supported by the model). API interactions via the 'httr' \n    package are performed asynchronously using 'promises' and 'future' to avoid \n    blocking the R console. Useful for tasks like code generation, text \n    summarization, and document analysis directly within the 'RStudio' environment. \n    Requires an 'OpenAI' API key set as an environment variable.",
    "version": "0.1.1",
    "maintainer": "Antoni Czolgowski <antoni.czolgowski@gmail.com>",
    "author": "Antoni Czolgowski [aut, cre]",
    "url": "https://github.com/AntoniCzolgowski/PacketLLM",
    "bug_reports": "https://github.com/AntoniCzolgowski/PacketLLM/issues",
    "repository": "https://cran.r-project.org/package=PacketLLM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PacketLLM Interactive 'OpenAI' Model Integration in 'RStudio' Offers an interactive 'RStudio' gadget interface for communicating \n    with 'OpenAI' large language models (e.g., 'gpt-5', 'gpt-5-mini', 'gpt-5-nano') \n    (<https://platform.openai.com/docs/api-reference>). \n    Enables users to conduct multiple chat conversations simultaneously in \n    separate tabs. Supports uploading local files (R, PDF, DOCX) to provide \n    context for the models. Allows per-conversation configuration of system \n    messages (where supported by the model). API interactions via the 'httr' \n    package are performed asynchronously using 'promises' and 'future' to avoid \n    blocking the R console. Useful for tasks like code generation, text \n    summarization, and document analysis directly within the 'RStudio' environment. \n    Requires an 'OpenAI' API key set as an environment variable.  "
  },
  {
    "id": 5807,
    "package_name": "Petersen",
    "title": "Estimators for Two-Sample Capture-Recapture Studies",
    "description": "A comprehensive implementation of Petersen-type estimators\n    and its many variants for two-sample capture-recapture studies.\n    A conditional likelihood approach is used that allows\n    for tag loss; non reporting of tags; reward tags; categorical, geographical and temporal stratification;\n    partial stratification; reverse capture-recapture;\n    and continuous variables in modeling the probability of capture.\n    Many examples from fisheries management are presented.",
    "version": "2025.3.1",
    "maintainer": "Carl Schwarz <cschwarz.stat.sfu.ca@gmail.com>",
    "author": "Carl Schwarz [aut, cre]",
    "url": "https://github.com/cschwarz-stat-sfu-ca/Petersen",
    "bug_reports": "https://github.com/cschwarz-stat-sfu-ca/Petersen/issues",
    "repository": "https://cran.r-project.org/package=Petersen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Petersen Estimators for Two-Sample Capture-Recapture Studies A comprehensive implementation of Petersen-type estimators\n    and its many variants for two-sample capture-recapture studies.\n    A conditional likelihood approach is used that allows\n    for tag loss; non reporting of tags; reward tags; categorical, geographical and temporal stratification;\n    partial stratification; reverse capture-recapture;\n    and continuous variables in modeling the probability of capture.\n    Many examples from fisheries management are presented.  "
  },
  {
    "id": 5822,
    "package_name": "PhilipsHue",
    "title": "R Interface to the Philips Hue API",
    "description": "Control Philips Hue smart lighting. Use this package to\n    connect to a Hue bridge on your local network (remote authentication\n    not yet supported) and control your smart lights through the Philips\n    Hue API. All API V1 endpoints are supported. See API documentation at\n    <https://developers.meethue.com/>.",
    "version": "1.0.0",
    "maintainer": "Justin Brantley <fascinatingfingers@icloud.com>",
    "author": "Justin Brantley [aut, cre]",
    "url": "https://fascinatingfingers.gitlab.io/philipshue,\nhttps://gitlab.com/fascinatingfingers/philipshue",
    "bug_reports": "https://gitlab.com/fascinatingfingers/philipshue/-/issues",
    "repository": "https://cran.r-project.org/package=PhilipsHue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PhilipsHue R Interface to the Philips Hue API Control Philips Hue smart lighting. Use this package to\n    connect to a Hue bridge on your local network (remote authentication\n    not yet supported) and control your smart lights through the Philips\n    Hue API. All API V1 endpoints are supported. See API documentation at\n    <https://developers.meethue.com/>.  "
  },
  {
    "id": 5824,
    "package_name": "PhoneValidator",
    "title": "Client for 'GenderAPI.io' Phone Number Validation and Formatter\nAPI",
    "description": "Provides an interface to the 'GenderAPI.io' Phone Number Validation & Formatter API (<https://www.genderapi.io>) for validating international phone numbers, detecting number type (mobile, landline, Voice over Internet Protocol (VoIP)), retrieving region and country metadata, and formatting numbers to E.164 or national format. Designed to simplify integration into R workflows for data validation, Customer Relationship Management (CRM) data cleaning, and analytics tasks. Full documentation is available at <https://www.genderapi.io/docs-phone-validation-formatter-api>.",
    "version": "1.0.1",
    "maintainer": "Onur Ozturk <onurozturk1980@gmail.com>",
    "author": "Onur Ozturk [aut, cre]",
    "url": "https://github.com/GenderAPI/PhoneValidator-R",
    "bug_reports": "https://github.com/GenderAPI/PhoneValidator-R/issues",
    "repository": "https://cran.r-project.org/package=PhoneValidator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PhoneValidator Client for 'GenderAPI.io' Phone Number Validation and Formatter\nAPI Provides an interface to the 'GenderAPI.io' Phone Number Validation & Formatter API (<https://www.genderapi.io>) for validating international phone numbers, detecting number type (mobile, landline, Voice over Internet Protocol (VoIP)), retrieving region and country metadata, and formatting numbers to E.164 or national format. Designed to simplify integration into R workflows for data validation, Customer Relationship Management (CRM) data cleaning, and analytics tasks. Full documentation is available at <https://www.genderapi.io/docs-phone-validation-formatter-api>.  "
  },
  {
    "id": 5825,
    "package_name": "PhotoGEA",
    "title": "Photosynthetic Gas Exchange Analysis",
    "description": "Read, process, fit, and analyze photosynthetic gas exchange\n    measurements. Documentation is provided by several vignettes; also see\n    Lochocki, Salesse-Smith, & McGrath (2025) <doi:10.1111/pce.15501>.",
    "version": "1.4.0",
    "maintainer": "Edward B. Lochocki <eloch@illinois.edu>",
    "author": "Edward B. Lochocki [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-4912-9783>),\n  Coralie E. Salesse-Smith [aut] (ORCID:\n    <https://orcid.org/0000-0002-2856-4217>),\n  Justin M. McGrath [aut] (ORCID:\n    <https://orcid.org/0000-0002-7025-3906>),\n  PhotoGEA authors [cph]",
    "url": "https://github.com/eloch216/PhotoGEA,\nhttps://eloch216.github.io/PhotoGEA/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PhotoGEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PhotoGEA Photosynthetic Gas Exchange Analysis Read, process, fit, and analyze photosynthetic gas exchange\n    measurements. Documentation is provided by several vignettes; also see\n    Lochocki, Salesse-Smith, & McGrath (2025) <doi:10.1111/pce.15501>.  "
  },
  {
    "id": 5833,
    "package_name": "PhytosanitaryCalculator",
    "title": "Phytosanitary Calculator for Inspection Plans Based on Risks",
    "description": "A 'Shiny' application for calculating phytosanitary inspection plans based on risks. It generates a diagram of pallets in a lot, highlights the units to be sampled, and documents them based on the selected sampling method (simple random or systematic sampling). ",
    "version": "1.1.3",
    "maintainer": "Gustavo Ramirez-Valverde <gustavoramirezvalverde@gmail.com>",
    "author": "Gustavo Ramirez-Valverde [aut, cre],\n  Luis Gabriel Otero-Prevost [aut],\n  Pedro Macias-Canales [ctb],\n  Juan A. Villanueva-Jimenez [ctb],\n  Jorge Luis Leyva-Vazquez [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PhytosanitaryCalculator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PhytosanitaryCalculator Phytosanitary Calculator for Inspection Plans Based on Risks A 'Shiny' application for calculating phytosanitary inspection plans based on risks. It generates a diagram of pallets in a lot, highlights the units to be sampled, and documents them based on the selected sampling method (simple random or systematic sampling).   "
  },
  {
    "id": 5837,
    "package_name": "Pinference",
    "title": "Probability Inference for Propositional Logic",
    "description": "Implementation of T. Hailperin's procedure to calculate lower and upper bounds of the probability for a propositional-logic expression, given equality and inequality constraints on the probabilities for other expressions. Truth-valuation is included as a special case. Applications range from decision-making and probabilistic reasoning, to pedagogical for probability and logic courses. For more details see T. Hailperin (1965) <doi:10.1080/00029890.1965.11970533>, T. Hailperin (1996) \"Sentential Probability Logic\" ISBN:0-934223-45-9, and package documentation. Requires the 'lpSolve' package.",
    "version": "0.2.6",
    "maintainer": "PierGianLuca Porta Mana <pgl@portamana.org>",
    "author": "PierGianLuca Porta Mana [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6070-0784>)",
    "url": "https://pglpm.github.io/Pinference/,\nhttps://github.com/pglpm/Pinference/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Pinference",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Pinference Probability Inference for Propositional Logic Implementation of T. Hailperin's procedure to calculate lower and upper bounds of the probability for a propositional-logic expression, given equality and inequality constraints on the probabilities for other expressions. Truth-valuation is included as a special case. Applications range from decision-making and probabilistic reasoning, to pedagogical for probability and logic courses. For more details see T. Hailperin (1965) <doi:10.1080/00029890.1965.11970533>, T. Hailperin (1996) \"Sentential Probability Logic\" ISBN:0-934223-45-9, and package documentation. Requires the 'lpSolve' package.  "
  },
  {
    "id": 5847,
    "package_name": "PlayerRatings",
    "title": "Dynamic Updating Methods for Player Ratings Estimation",
    "description": "Implements schemes for estimating player or \n  team skill based on dynamic updating. Implemented methods include \n  Elo, Glicko, Glicko-2 and Stephenson. Contains pdf documentation \n  of a reproducible analysis using approximately two million chess \n  matches. Also contains an Elo based method for multi-player games\n  where the result is a placing or a score. This includes zero-sum\n  games such as poker and mahjong.",
    "version": "1.1-0",
    "maintainer": "Alec Stephenson <alec_stephenson@hotmail.com>",
    "author": "Alec Stephenson and Jeff Sonas.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PlayerRatings",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PlayerRatings Dynamic Updating Methods for Player Ratings Estimation Implements schemes for estimating player or \n  team skill based on dynamic updating. Implemented methods include \n  Elo, Glicko, Glicko-2 and Stephenson. Contains pdf documentation \n  of a reproducible analysis using approximately two million chess \n  matches. Also contains an Elo based method for multi-player games\n  where the result is a placing or a score. This includes zero-sum\n  games such as poker and mahjong.  "
  },
  {
    "id": 5876,
    "package_name": "Poly4AT",
    "title": "Access 'INVEKOS' API for Field Polygons",
    "description": "A 'shiny' app that allows to access and use the 'INVEKOS' API for field polygons in Austria. API documentation is available at <https://gis.lfrz.gv.at/api/geodata/i009501/ogc/features/v1/>.",
    "version": "1.0.1",
    "maintainer": "Sebastian Wieser <poly4at@gmail.com>",
    "author": "Sebastian Wieser [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0008-8940-7937>)",
    "url": "https://github.com/farmse988/Poly4AT",
    "bug_reports": "https://github.com/farmse988/Poly4AT/issues",
    "repository": "https://cran.r-project.org/package=Poly4AT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Poly4AT Access 'INVEKOS' API for Field Polygons A 'shiny' app that allows to access and use the 'INVEKOS' API for field polygons in Austria. API documentation is available at <https://gis.lfrz.gv.at/api/geodata/i009501/ogc/features/v1/>.  "
  },
  {
    "id": 5891,
    "package_name": "PopGenReport",
    "title": "A Simple Framework to Analyse Population and Landscape Genetic\nData",
    "description": "Provides beginner friendly framework to analyse population genetic\n    data. Based on 'adegenet' objects it uses 'knitr' to create comprehensive reports on spatial genetic data. \n    For detailed information how to use the package refer to the comprehensive\n    tutorials or visit <http://www.popgenreport.org/>.",
    "version": "3.1.3",
    "maintainer": "Bernd Gruber <bernd.gruber@canberra.edu.au>",
    "author": "Bernd Gruber [aut, cre],\n  Aaron Adamack [aut]",
    "url": "https://github.com/green-striped-gecko/PopGenReport",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PopGenReport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PopGenReport A Simple Framework to Analyse Population and Landscape Genetic\nData Provides beginner friendly framework to analyse population genetic\n    data. Based on 'adegenet' objects it uses 'knitr' to create comprehensive reports on spatial genetic data. \n    For detailed information how to use the package refer to the comprehensive\n    tutorials or visit <http://www.popgenreport.org/>.  "
  },
  {
    "id": 5903,
    "package_name": "PoweR",
    "title": "Computation of Power and Level Tables for Hypothesis Tests",
    "description": "Computes power and level tables for goodness-of-fit tests for the normal, Laplace, and uniform distributions. Generates output in 'LaTeX' format to facilitate reporting and reproducibility. Explanatory graphs help visualize the statistical power of test statistics under various alternatives. For more details, see Lafaye De Micheaux and Tran (2016) <doi:10.18637/jss.v069.i03>.",
    "version": "1.1.4",
    "maintainer": "Pierre Lafaye De Micheaux <lafaye@unsw.edu.au>",
    "author": "Pierre Lafaye De Micheaux [aut, cre],\n  Viet Anh Tran [aut],\n  Alain Desgagne [aut],\n  Frederic Ouimet [aut],\n  Steven G. Johnson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PoweR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PoweR Computation of Power and Level Tables for Hypothesis Tests Computes power and level tables for goodness-of-fit tests for the normal, Laplace, and uniform distributions. Generates output in 'LaTeX' format to facilitate reporting and reproducibility. Explanatory graphs help visualize the statistical power of test statistics under various alternatives. For more details, see Lafaye De Micheaux and Tran (2016) <doi:10.18637/jss.v069.i03>.  "
  },
  {
    "id": 5912,
    "package_name": "PreKnitPostHTMLRender",
    "title": "Pre-Knitting Processing and Post HTML-Rendering Processing",
    "description": "Dynamize headers or R code within 'Rmd' files to prevent proliferation of 'Rmd' files for similar reports. Add in external HTML document within 'rmarkdown' rendered HTML doc.",
    "version": "0.1.0",
    "maintainer": "Chin Soon Lim <chinsoon12@hotmail.com>",
    "author": "Chin Soon Lim [aut]",
    "url": "https://github.com/chinsoon12/PreKnitPostHTMLRender",
    "bug_reports": "https://github.com/chinsoon12/PreKnitPostHTMLRender",
    "repository": "https://cran.r-project.org/package=PreKnitPostHTMLRender",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PreKnitPostHTMLRender Pre-Knitting Processing and Post HTML-Rendering Processing Dynamize headers or R code within 'Rmd' files to prevent proliferation of 'Rmd' files for similar reports. Add in external HTML document within 'rmarkdown' rendered HTML doc.  "
  },
  {
    "id": 5926,
    "package_name": "ProAE",
    "title": "PRO-CTCAE Scoring, Analysis, and Graphical Tools",
    "description": "A collection of tools to facilitate standardized analysis \n    and graphical procedures when using the National Cancer Institute\u2019s \n    Patient-Reported Outcomes version of the Common Terminology Criteria for \n    Adverse Events (PRO-CTCAE) and other PRO measurements.",
    "version": "1.0.3",
    "maintainer": "Blake Langlais <langlais.blake@mayo.edu>",
    "author": "Blake Langlais [aut, cre],\n  Brie Noble [ctb],\n  Mia Truman [ctb],\n  Molly Voss [ctb],\n  Amylou Dueck [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ProAE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProAE PRO-CTCAE Scoring, Analysis, and Graphical Tools A collection of tools to facilitate standardized analysis \n    and graphical procedures when using the National Cancer Institute\u2019s \n    Patient-Reported Outcomes version of the Common Terminology Criteria for \n    Adverse Events (PRO-CTCAE) and other PRO measurements.  "
  },
  {
    "id": 5931,
    "package_name": "ProSportsDraftData",
    "title": "Professional Sports Draft Data",
    "description": "We provide comprehensive draft data for major professional sports leagues, including the National Football League (NFL), National Basketball Association (NBA), and National Hockey League (NHL). It offers access to both historical and current draft data, allowing for detailed analysis and research on player biases and player performance. The package is useful for sports fans and researchers interested in identifying biases and trends within scouting reports. Created by web scraping data from leading websites that cover professional sports player scouting reports, the package allows users to filter and summarize data for analytical purposes. For further details on the methods used, please refer to Wickham (2022) \"rvest: Easily Harvest (Scrape) Web Pages\" <https://CRAN.R-project.org/package=rvest> and Harrison (2023) \"RSelenium: R Bindings for Selenium WebDriver\" <https://CRAN.R-project.org/package=RSelenium>.",
    "version": "1.0.3",
    "maintainer": "Benjamin Ginsburg <benjamin.ginsburg@du.edu>",
    "author": "Benjamin Ginsburg [aut, cre]",
    "url": "https://github.com/Ginsburg1/ProSportsDraftData",
    "bug_reports": "https://github.com/Ginsburg1/ProSportsDraftData/issues",
    "repository": "https://cran.r-project.org/package=ProSportsDraftData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProSportsDraftData Professional Sports Draft Data We provide comprehensive draft data for major professional sports leagues, including the National Football League (NFL), National Basketball Association (NBA), and National Hockey League (NHL). It offers access to both historical and current draft data, allowing for detailed analysis and research on player biases and player performance. The package is useful for sports fans and researchers interested in identifying biases and trends within scouting reports. Created by web scraping data from leading websites that cover professional sports player scouting reports, the package allows users to filter and summarize data for analytical purposes. For further details on the methods used, please refer to Wickham (2022) \"rvest: Easily Harvest (Scrape) Web Pages\" <https://CRAN.R-project.org/package=rvest> and Harrison (2023) \"RSelenium: R Bindings for Selenium WebDriver\" <https://CRAN.R-project.org/package=RSelenium>.  "
  },
  {
    "id": 5967,
    "package_name": "Publish",
    "title": "Format Output of Various Routines in a Suitable Way for Reports\nand Publication",
    "description": "A bunch of convenience functions that transform the results of some basic statistical analyses\n       into table format nearly ready for publication. This includes descriptive tables, tables of\n       logistic regression and Cox regression results as well as forest plots. ",
    "version": "2025.07.24",
    "maintainer": "Thomas A. Gerds <tag@biostat.ku.dk>",
    "author": "Thomas A. Gerds [aut, cre],\n  Christian Torp-Pedersen [ctb],\n  Klaus K Holst [ctb],\n  Brice Ozenne [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Publish",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Publish Format Output of Various Routines in a Suitable Way for Reports\nand Publication A bunch of convenience functions that transform the results of some basic statistical analyses\n       into table format nearly ready for publication. This includes descriptive tables, tables of\n       logistic regression and Cox regression results as well as forest plots.   "
  },
  {
    "id": 5990,
    "package_name": "QDComparison",
    "title": "Modern Nonparametric Tools for Two-Sample Quantile and\nDistribution Comparisons",
    "description": "Allows practitioners to determine (i) if two univariate distributions (which can be continuous, discrete, or even mixed) are equal, (ii) how two distributions differ (shape differences, e.g., location, scale, etc.), and (iii) where two distributions differ (at which quantiles), all using nonparametric LP statistics. The primary reference is Jungreis, D. (2019, Technical Report).",
    "version": "3.0",
    "maintainer": "David Jungreis <dbjungreis@gmail.com>",
    "author": "David Jungreis, Subhadeep Mukhopadhyay",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QDComparison",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QDComparison Modern Nonparametric Tools for Two-Sample Quantile and\nDistribution Comparisons Allows practitioners to determine (i) if two univariate distributions (which can be continuous, discrete, or even mixed) are equal, (ii) how two distributions differ (shape differences, e.g., location, scale, etc.), and (iii) where two distributions differ (at which quantiles), all using nonparametric LP statistics. The primary reference is Jungreis, D. (2019, Technical Report).  "
  },
  {
    "id": 6040,
    "package_name": "QuantileGradeR",
    "title": "Quantile-Adjusted Restaurant Grading",
    "description": "Implementation of the food safety restaurant grading system adopted by Public Health - Seattle & King County (see Ashwood, Z.C., Elias, B., and Ho. D.E. \"Improving the Reliability of Food Safety Disclosure: A Quantile Adjusted Restaurant Grading System for Seattle-King County\" (working paper)). As reported in the accompanying paper, this package allows jurisdictions to easily implement refinements that address common challenges with unadjusted grading systems. First, in contrast to unadjusted grading, where the most recent single routine inspection is the primary determinant of a grade, grading inputs are allowed to be flexible. For instance, it is straightforward to base the grade on average inspection scores across multiple inspection cycles. Second, the package can identify quantile cutoffs by inputting substantively meaningful regulatory thresholds (e.g., the proportion of establishments receiving sufficient violation points to warrant a return visit). Third, the quantile adjustment equalizes the proportion of establishments in a flexible number of grading categories (e.g., A/B/C) across areas (e.g., ZIP codes, inspector areas) to account for inspector differences. Fourth, the package implements a refined quantile adjustment that addresses two limitations with the stats::quantile() function when applied to inspection score datasets with large numbers of score ties. The quantile adjustment algorithm iterates over quantiles until, over all restaurants in all areas, grading proportions are within a tolerance of desired global proportions. In addition the package allows a modified definition of \"quantile\" from \"Nearest Rank\". Instead of requiring that at least p[1]% of restaurants receive the top grade and at least (p[1]+p[2])% of restaurants receive the top or second best grade for quantiles p, the algorithm searches for cutoffs so that as close as possible p[1]% of restaurants receive the top grade, and as close as possible to p[2]% of restaurants receive the second top grade.",
    "version": "0.1.1",
    "maintainer": "Zoe Ashwood <zashwood@law.stanford.edu>",
    "author": "Zoe Ashwood <zashwood@law.stanford.edu>,\n    Becky Elias <Becky.Elias@kingcounty.gov>,\n    Daniel E. Ho <dho@law.stanford.edu>",
    "url": "http://www.kingcounty.gov/depts/health/environmental-health/food-safety/inspection-system/food-safety-rating.aspx",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QuantileGradeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QuantileGradeR Quantile-Adjusted Restaurant Grading Implementation of the food safety restaurant grading system adopted by Public Health - Seattle & King County (see Ashwood, Z.C., Elias, B., and Ho. D.E. \"Improving the Reliability of Food Safety Disclosure: A Quantile Adjusted Restaurant Grading System for Seattle-King County\" (working paper)). As reported in the accompanying paper, this package allows jurisdictions to easily implement refinements that address common challenges with unadjusted grading systems. First, in contrast to unadjusted grading, where the most recent single routine inspection is the primary determinant of a grade, grading inputs are allowed to be flexible. For instance, it is straightforward to base the grade on average inspection scores across multiple inspection cycles. Second, the package can identify quantile cutoffs by inputting substantively meaningful regulatory thresholds (e.g., the proportion of establishments receiving sufficient violation points to warrant a return visit). Third, the quantile adjustment equalizes the proportion of establishments in a flexible number of grading categories (e.g., A/B/C) across areas (e.g., ZIP codes, inspector areas) to account for inspector differences. Fourth, the package implements a refined quantile adjustment that addresses two limitations with the stats::quantile() function when applied to inspection score datasets with large numbers of score ties. The quantile adjustment algorithm iterates over quantiles until, over all restaurants in all areas, grading proportions are within a tolerance of desired global proportions. In addition the package allows a modified definition of \"quantile\" from \"Nearest Rank\". Instead of requiring that at least p[1]% of restaurants receive the top grade and at least (p[1]+p[2])% of restaurants receive the top or second best grade for quantiles p, the algorithm searches for cutoffs so that as close as possible p[1]% of restaurants receive the top grade, and as close as possible to p[2]% of restaurants receive the second top grade.  "
  },
  {
    "id": 6046,
    "package_name": "QurvE",
    "title": "Robust and User-Friendly Analysis of Growth and Fluorescence\nCurves",
    "description": "High-throughput analysis of growth curves and fluorescence\n    data using three methods: linear regression, growth model fitting, and\n    smooth spline fit. Analysis of dose-response relationships via\n    smoothing splines or dose-response models. Complete data analysis\n    workflows can be executed in a single step via user-friendly wrapper\n    functions. The results of these workflows are summarized in detailed\n    reports as well as intuitively navigable 'R' data containers. A 'shiny'\n    application provides access to all features without\n    requiring any programming knowledge. The package is described in further\n    detail in Wirth et al. (2023) <doi:10.1038/s41596-023-00850-7>.",
    "version": "1.1.2",
    "maintainer": "Nicolas T. Wirth <mail.nicowirth@gmail.com>",
    "author": "Nicolas T. Wirth [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0799-1321>),\n  Jonathan Funk [aut] (contribution: Co-developer of shiny app.),\n  Matthias Kahm [ctb] (contribution: Author of 'grofit' package, whose\n    general data structure was adopted for QurvE.),\n  Maik Kschischo [ctb] (contribution: Author of 'grofit' package, whose\n    general data structure was adopted for QurvE.),\n  Thomas Petzoldt [ctb] (ORCID: <https://orcid.org/0000-0002-4951-6468>,\n    contribution: Creator of the package 'growthrates', whose function\n    for calculating linear regressions served as a template in QurvE.),\n  Andrew Stein [ctb] (contribution: Creator of 'xgxr' package from which\n    QurvE adopted code to plot axis ticks on log10 scale.),\n  Michael W. Kearney [ctb] (contribution: Creator of 'tfse' package from\n    which QurvE adopted the match_arg function.),\n  Santiago I. Hurtado [ctb] (contribution: Creator of 'RobustLinearReg'\n    package from which QurvE adopted the Theil Sehn Regression method.),\n  Mark Heckmann [ctb] (contribution: Creator of the 'zipFastener'\n    function; source:\n    https://ryouready.wordpress.com/2009/03/27/r-zip-fastener-for-two-data-frames-combining-rows-or-columns-of-two-dataframes-in-an-alternating-manner/),\n  Nicholas Hamilton [ctb] (contribution: Creator of the 'colFmt'\n    function.),\n  Evan Friedland [ctb] (contribution: Creator of the 'inflect' function.),\n  Heather Turner [ctb] (contribution: Creator of the 'base_breaks'\n    function.),\n  Georgi N. Boshnakov [ctb] (contribution: Creator of 'gbRd' package from\n    which functions are used to display function help pages within the\n    shiny app.)",
    "url": "https://github.com/NicWir/QurvE, https://nicwir.github.io/QurvE/",
    "bug_reports": "https://github.com/NicWir/QurvE/issues",
    "repository": "https://cran.r-project.org/package=QurvE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QurvE Robust and User-Friendly Analysis of Growth and Fluorescence\nCurves High-throughput analysis of growth curves and fluorescence\n    data using three methods: linear regression, growth model fitting, and\n    smooth spline fit. Analysis of dose-response relationships via\n    smoothing splines or dose-response models. Complete data analysis\n    workflows can be executed in a single step via user-friendly wrapper\n    functions. The results of these workflows are summarized in detailed\n    reports as well as intuitively navigable 'R' data containers. A 'shiny'\n    application provides access to all features without\n    requiring any programming knowledge. The package is described in further\n    detail in Wirth et al. (2023) <doi:10.1038/s41596-023-00850-7>.  "
  },
  {
    "id": 6055,
    "package_name": "R.rsp",
    "title": "Dynamic Generation of Scientific Reports",
    "description": "The RSP markup language makes any text-based document come alive.  RSP provides a powerful markup for controlling the content and output of LaTeX, HTML, Markdown, AsciiDoc, Sweave and knitr documents (and more), e.g. 'Today's date is <%=Sys.Date()%>'.  Contrary to many other literate programming languages, with RSP it is straightforward to loop over mixtures of code and text sections, e.g. in month-by-month summaries.  RSP has also several preprocessing directives for incorporating static and dynamic contents of external files (local or online) among other things.  Functions rstring() and rcat() make it easy to process RSP strings, rsource() sources an RSP file as it was an R script, while rfile() compiles it (even online) into its final output format, e.g. rfile('report.tex.rsp') generates 'report.pdf' and rfile('report.md.rsp') generates 'report.html'.  RSP is ideal for self-contained scientific reports and R package vignettes.  It's easy to use - if you know how to write an R script, you'll be up and running within minutes.",
    "version": "0.46.0",
    "maintainer": "Henrik Bengtsson <henrikb@braju.com>",
    "author": "Henrik Bengtsson [aut, cre, cph]",
    "url": "https://henrikbengtsson.github.io/R.rsp/,\nhttps://github.com/HenrikBengtsson/R.rsp",
    "bug_reports": "https://github.com/HenrikBengtsson/R.rsp/issues",
    "repository": "https://cran.r-project.org/package=R.rsp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R.rsp Dynamic Generation of Scientific Reports The RSP markup language makes any text-based document come alive.  RSP provides a powerful markup for controlling the content and output of LaTeX, HTML, Markdown, AsciiDoc, Sweave and knitr documents (and more), e.g. 'Today's date is <%=Sys.Date()%>'.  Contrary to many other literate programming languages, with RSP it is straightforward to loop over mixtures of code and text sections, e.g. in month-by-month summaries.  RSP has also several preprocessing directives for incorporating static and dynamic contents of external files (local or online) among other things.  Functions rstring() and rcat() make it easy to process RSP strings, rsource() sources an RSP file as it was an R script, while rfile() compiles it (even online) into its final output format, e.g. rfile('report.tex.rsp') generates 'report.pdf' and rfile('report.md.rsp') generates 'report.html'.  RSP is ideal for self-contained scientific reports and R package vignettes.  It's easy to use - if you know how to write an R script, you'll be up and running within minutes.  "
  },
  {
    "id": 6056,
    "package_name": "R.temis",
    "title": "Integrated Text Mining Solution",
    "description": "An integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, lexical summary, terms\n    co-occurrences and documents similarity measures, graphs of terms,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",
    "version": "0.1.4",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre],\n  Gilles Bastin [aut],\n  Antoine Chollet [aut]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=R.temis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R.temis Integrated Text Mining Solution An integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, lexical summary, terms\n    co-occurrences and documents similarity measures, graphs of terms,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.  "
  },
  {
    "id": 6058,
    "package_name": "R0",
    "title": "Estimation of R0 and Real-Time Reproduction Number from\nEpidemics",
    "description": "Estimation of reproduction numbers for disease outbreak, based on\n    incidence data. The R0 package implements several documented methods. It is\n    therefore possible to compare estimations according to the methods used.\n    Depending on the methods requested by user, basic reproduction number\n    (commonly denoted as R0) or real-time reproduction number (referred to as\n    R(t)) is computed, along with a 95% Confidence Interval. Plotting outputs\n    will give different graphs depending on the methods requested : basic\n    reproductive number estimations will only show the epidemic curve\n    (collected data) and an adjusted model, whereas real-time methods will also\n    show the R(t) variations throughout the outbreak time period. Sensitivity\n    analysis tools are also provided, and allow for investigating effects of\n    varying Generation Time distribution or time window on estimates.",
    "version": "1.3-1",
    "maintainer": "Thomas Obadia <thomas.obadia@pasteur.fr>",
    "author": "Pierre-Yves Boelle, Thomas Obadia",
    "url": "https://github.com/tobadia/R0",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=R0",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R0 Estimation of R0 and Real-Time Reproduction Number from\nEpidemics Estimation of reproduction numbers for disease outbreak, based on\n    incidence data. The R0 package implements several documented methods. It is\n    therefore possible to compare estimations according to the methods used.\n    Depending on the methods requested by user, basic reproduction number\n    (commonly denoted as R0) or real-time reproduction number (referred to as\n    R(t)) is computed, along with a 95% Confidence Interval. Plotting outputs\n    will give different graphs depending on the methods requested : basic\n    reproductive number estimations will only show the epidemic curve\n    (collected data) and an adjusted model, whereas real-time methods will also\n    show the R(t) variations throughout the outbreak time period. Sensitivity\n    analysis tools are also provided, and allow for investigating effects of\n    varying Generation Time distribution or time window on estimates.  "
  },
  {
    "id": 6064,
    "package_name": "R2HTML",
    "title": "HTML Exportation for R Objects",
    "description": "Includes HTML function and methods to write in an HTML\n        file. Thus, making HTML reports is easy. Includes a function\n        that allows redirection on the fly, which appears to be very\n        useful for teaching purpose, as the student can keep a copy of\n        the produced output to keep all that he did during the course.\n        Package comes with a vignette describing how to write HTML\n        reports for statistical analysis. Finally, a driver for 'Sweave'\n        allows to parse HTML flat files containing R code and to\n        automatically write the corresponding outputs (tables and\n        graphs).",
    "version": "2.3.4",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Eric Lecoutre [aut],\n  Milan Bouchet-Valat [cre, ctb],\n  Thomas Friedrichsmeier [ctb]",
    "url": "https://github.com/nalimilan/R2HTML",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=R2HTML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R2HTML HTML Exportation for R Objects Includes HTML function and methods to write in an HTML\n        file. Thus, making HTML reports is easy. Includes a function\n        that allows redirection on the fly, which appears to be very\n        useful for teaching purpose, as the student can keep a copy of\n        the produced output to keep all that he did during the course.\n        Package comes with a vignette describing how to write HTML\n        reports for statistical analysis. Finally, a driver for 'Sweave'\n        allows to parse HTML flat files containing R code and to\n        automatically write the corresponding outputs (tables and\n        graphs).  "
  },
  {
    "id": 6073,
    "package_name": "R2wd",
    "title": "Write MS-Word documents from R",
    "description": "This package uses either the statconnDCOM server (via the\n        rcom package) or the RDCOMClient to communicate with MS-Word\n        via the COM interface.",
    "version": "1.5",
    "maintainer": "Christian Ritter <R2wd@ridaco.be>",
    "author": "Christian Ritter",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=R2wd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R2wd Write MS-Word documents from R This package uses either the statconnDCOM server (via the\n        rcom package) or the RDCOMClient to communicate with MS-Word\n        via the COM interface.  "
  },
  {
    "id": 6074,
    "package_name": "R3port",
    "title": "Report Functions to Create HTML and PDF Files",
    "description": "Create and combine HTML and PDF reports from within R.\n    Possibility to design tables and listings for reporting and also include R plots.",
    "version": "0.3.1",
    "maintainer": "Richard Hooijmaijers <richardhooijmaijers@gmail.com>",
    "author": "Richard Hooijmaijers [aut, cre],\n  Richard Hooijmaijers [cph]",
    "url": "https://github.com/RichardHooijmaijers/R3port",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=R3port",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R3port Report Functions to Create HTML and PDF Files Create and combine HTML and PDF reports from within R.\n    Possibility to design tables and listings for reporting and also include R plots.  "
  },
  {
    "id": 6075,
    "package_name": "R4CouchDB",
    "title": "A R Convenience Layer for CouchDB 2.0",
    "description": "Provides a collection of functions for basic\n    database and document management operations such as add, get, list access\n    or delete. Every cdbFunction() gets and returns a list() containing the\n    connection setup. Such a list can be generated by cdbIni().",
    "version": "0.7.5",
    "maintainer": "Thomas Bock <thsteinbock@web.de>",
    "author": "Thomas Bock",
    "url": "https://github.com/wactbprot/R4CouchDB",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=R4CouchDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R4CouchDB A R Convenience Layer for CouchDB 2.0 Provides a collection of functions for basic\n    database and document management operations such as add, get, list access\n    or delete. Every cdbFunction() gets and returns a list() containing the\n    connection setup. Such a list can be generated by cdbIni().  "
  },
  {
    "id": 6097,
    "package_name": "RAQSAPI",
    "title": "A Simple Interface to the US EPA Air Quality System Data Mart\nAPI",
    "description": "Retrieve air monitoring data and associated metadata from the US\n  Environmental Protection Agency's Air Quality System service using functions.\n  See <https://aqs.epa.gov/aqsweb/documents/data_api.html> for details about\n  the US EPA Data Mart API.",
    "version": "2.0.5",
    "maintainer": "Clinton Mccrowey <mccrowey.clinton@epa.gov>",
    "author": "Clinton Mccrowey [cre, aut] (United States Environmental Protection\n    Agency Region 3 Air and Radiation Division),\n  Timothy Sharac [ctb, rev] (United States Environmental Protection\n    Agency),\n  Nick Mangus [rev] (United States Environmental Protection Agency),\n  Doug Jager [ctb, rev] (United States Environmental Protection Agency),\n  Ryan Brown [ctb, rev] (United States Environmental Protection Agency),\n  Daniel Garver [ctb, rev] (United States Environmental Protection\n    Agency),\n  Benjamin Wells [ctb, rev] (United States Environmental Protection\n    Agency),\n  Hayley Brittingham [crr, ctr] (Neptune and Company),\n  Jeffrey Hollister [rev] (United States Environmental Protection Agency,\n    ORCID: <https://orcid.org/0000-0002-9254-9740>),\n  Edward Andrews [rev, ctb] (West Virginia Department of Environmental\n    Protection, Division of Air Quality)",
    "url": "https://github.com/USEPA/RAQSAPI/,\nhttps://aqs.epa.gov/aqsweb/documents/data_api.html",
    "bug_reports": "https://github.com/USEPA/RAQSAPI/issues",
    "repository": "https://cran.r-project.org/package=RAQSAPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAQSAPI A Simple Interface to the US EPA Air Quality System Data Mart\nAPI Retrieve air monitoring data and associated metadata from the US\n  Environmental Protection Agency's Air Quality System service using functions.\n  See <https://aqs.epa.gov/aqsweb/documents/data_api.html> for details about\n  the US EPA Data Mart API.  "
  },
  {
    "id": 6102,
    "package_name": "RAdwords",
    "title": "Loading Google Adwords Data into R",
    "description": "Aims at loading Google Adwords data into R. Adwords is an online\n    advertising service that enables advertisers to display advertising copy to web\n    users (see <https://developers.google.com/adwords/> for more information). \n    Therefore the package implements three main features. First, the package\n    provides an authentication process for R with the Google Adwords API (see \n    <https://developers.google.com/adwords/api/> for more information) via OAUTH2.\n    Second, the package offers an interface to apply the Adwords query language in\n    R and query the Adwords API with ad-hoc reports. Third, the received data are\n    transformed into suitable data formats for further data processing and data\n    analysis.",
    "version": "0.1.18",
    "maintainer": "Johannes Burkhardt <johannes.burkhardt@gmail.com>",
    "author": "Johannes Burkhardt <johannes.burkhardt@gmail.com>, Matthias Bannert\n    <matthias.bannert@gmail.com>",
    "url": "https://github.com/jburkhardt/RAdwords,\nhttps://developers.google.com/adwords,\nhttps://developers.google.com/adwords/api/",
    "bug_reports": "https://github.com/jburkhardt/RAdwords/issues",
    "repository": "https://cran.r-project.org/package=RAdwords",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAdwords Loading Google Adwords Data into R Aims at loading Google Adwords data into R. Adwords is an online\n    advertising service that enables advertisers to display advertising copy to web\n    users (see <https://developers.google.com/adwords/> for more information). \n    Therefore the package implements three main features. First, the package\n    provides an authentication process for R with the Google Adwords API (see \n    <https://developers.google.com/adwords/api/> for more information) via OAUTH2.\n    Second, the package offers an interface to apply the Adwords query language in\n    R and query the Adwords API with ad-hoc reports. Third, the received data are\n    transformed into suitable data formats for further data processing and data\n    analysis.  "
  },
  {
    "id": 6106,
    "package_name": "RAthena",
    "title": "Connect to 'AWS Athena' using 'Boto3' ('DBI' Interface)",
    "description": "Designed to be compatible with the R package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this 'Python' 'Boto3' Software Development Kit ('SDK')\n    <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html> is used as a driver.",
    "version": "2.6.3",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "Dyfan Jones [aut, cre]",
    "url": "https://dyfanjones.github.io/RAthena/,\nhttps://github.com/DyfanJones/RAthena",
    "bug_reports": "https://github.com/DyfanJones/RAthena/issues",
    "repository": "https://cran.r-project.org/package=RAthena",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAthena Connect to 'AWS Athena' using 'Boto3' ('DBI' Interface) Designed to be compatible with the R package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this 'Python' 'Boto3' Software Development Kit ('SDK')\n    <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html> is used as a driver.  "
  },
  {
    "id": 6166,
    "package_name": "REDCapDM",
    "title": "'REDCap' Data Management",
    "description": "REDCap Data Management - 'REDCap' (Research Electronic Data CAPture; <https://projectredcap.org>) is a web application developed at Vanderbilt University, designed for creating and managing online surveys and databases and the REDCap API is an interface that allows external applications to connect to REDCap remotely, and is used to programmatically retrieve or modify project data or settings within REDCap, such as importing or exporting data. REDCapDM is an R package that allows users to manage data exported directly from REDCap or using an API connection. This package includes several functions designed for pre-processing data, generating reports of queries such as outliers or missing values, and following up on previously identified queries. ",
    "version": "1.0.0",
    "maintainer": "Jo\u00e3o Carmezim <jcarmezim@igtp.cat>",
    "author": "Jo\u00e3o Carmezim [aut, cre],\n  Pau Satorra [aut],\n  Judith Pe\u00f1afiel [aut],\n  Esther Garc\u00eda [aut],\n  Nat\u00e0lia Pallar\u00e8s [aut],\n  Cristian Teb\u00e9 [aut]",
    "url": "https://bruigtp.github.io/REDCapDM/,\nhttps://doi.org/10.1186/s12874-024-02178-6",
    "bug_reports": "https://github.com/bruigtp/REDCapDM/issues",
    "repository": "https://cran.r-project.org/package=REDCapDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDCapDM 'REDCap' Data Management REDCap Data Management - 'REDCap' (Research Electronic Data CAPture; <https://projectredcap.org>) is a web application developed at Vanderbilt University, designed for creating and managing online surveys and databases and the REDCap API is an interface that allows external applications to connect to REDCap remotely, and is used to programmatically retrieve or modify project data or settings within REDCap, such as importing or exporting data. REDCapDM is an R package that allows users to manage data exported directly from REDCap or using an API connection. This package includes several functions designed for pre-processing data, generating reports of queries such as outliers or missing values, and following up on previously identified queries.   "
  },
  {
    "id": 6167,
    "package_name": "REDCapExporter",
    "title": "Automated Construction of R Data Packages from REDCap Projects",
    "description": "Export all data, including metadata, from a REDCap (Research\n    Electronic Data Capture) Project via the REDCap API\n    <https://projectredcap.org/wp-content/resources/REDCapTechnicalOverview.pdf>.\n    The exported (meta)data will be processed and formatted into a stand alone R\n    data package which can be installed and shared between researchers.  Several\n    default reports are generated as vignettes in the resulting package.",
    "version": "0.3.2",
    "maintainer": "Peter DeWitt <peter.dewitt@cuanschutz.edu>",
    "author": "Peter DeWitt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6391-0795>)",
    "url": "https://github.com/dewittpe/REDCapExporter,\nhttp://www.peteredewitt.com/REDCapExporter/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=REDCapExporter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDCapExporter Automated Construction of R Data Packages from REDCap Projects Export all data, including metadata, from a REDCap (Research\n    Electronic Data Capture) Project via the REDCap API\n    <https://projectredcap.org/wp-content/resources/REDCapTechnicalOverview.pdf>.\n    The exported (meta)data will be processed and formatted into a stand alone R\n    data package which can be installed and shared between researchers.  Several\n    default reports are generated as vignettes in the resulting package.  "
  },
  {
    "id": 6177,
    "package_name": "REPLesentR",
    "title": "Presentations in the REPL",
    "description": "Create presentations and display them inside the R 'REPL'\n    (Read-Eval-Print loop), aka the R console. Presentations can be written in\n    'RMarkdown' or any other text format. A set of convenient navigation options\n    as well as code evaluation during a presentation is provided. It is great\n    for tech talks with live coding examples and tutorials. While this is not a\n    replacement for standard presentation formats, it's old-school looks might\n    just be what sets it apart. This project has been inspired by the\n    'REPLesent' project for presentations in the 'Scala' 'REPL'.",
    "version": "0.4.1",
    "maintainer": "Sebastian Warnholz <wahani@gmail.com>",
    "author": "Sebastian Warnholz [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/wahani/REPLesentR/issues",
    "repository": "https://cran.r-project.org/package=REPLesentR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REPLesentR Presentations in the REPL Create presentations and display them inside the R 'REPL'\n    (Read-Eval-Print loop), aka the R console. Presentations can be written in\n    'RMarkdown' or any other text format. A set of convenient navigation options\n    as well as code evaluation during a presentation is provided. It is great\n    for tech talks with live coding examples and tutorials. While this is not a\n    replacement for standard presentation formats, it's old-school looks might\n    just be what sets it apart. This project has been inspired by the\n    'REPLesent' project for presentations in the 'Scala' 'REPL'.  "
  },
  {
    "id": 6186,
    "package_name": "RESS",
    "title": "Integrates R and Essentia",
    "description": "Contains three functions that query AuriQ Systems' Essentia Database and return the results in R. 'essQuery' takes a single Essentia command and captures the output in R, where you can save the output to a dataframe or stream it directly into additional analysis. 'read.essentia' takes an Essentia script and captures the output csv data into R, where you can save the output to a dataframe or stream it directly into additional analysis. 'capture.essentia' takes a file containing any number of Essentia commands and captures the output of the specified statements into R dataframes. Essentia can be downloaded for free at http://www.auriq.com/documentation/source/install/index.html.",
    "version": "1.3",
    "maintainer": "Ben Waxer <bwaxer@auriq.com>",
    "author": "Ben Waxer",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RESS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RESS Integrates R and Essentia Contains three functions that query AuriQ Systems' Essentia Database and return the results in R. 'essQuery' takes a single Essentia command and captures the output in R, where you can save the output to a dataframe or stream it directly into additional analysis. 'read.essentia' takes an Essentia script and captures the output csv data into R, where you can save the output to a dataframe or stream it directly into additional analysis. 'capture.essentia' takes a file containing any number of Essentia commands and captures the output of the specified statements into R dataframes. Essentia can be downloaded for free at http://www.auriq.com/documentation/source/install/index.html.  "
  },
  {
    "id": 6194,
    "package_name": "RFLPtools",
    "title": "Tools to Analyse RFLP Data",
    "description": "Provides functions to analyse DNA fragment samples (i.e. derived from RFLP-analysis) and standalone BLAST report files (i.e. DNA sequence analysis).",
    "version": "2.0",
    "maintainer": "Matthias Kohl <Matthias.Kohl@stamats.de>",
    "author": "Fabienne Flessa [aut],\n  Alexandra Kehl [aut] (ORCID: <https://orcid.org/0000-0002-1547-0713>),\n  Mohammed Aslam Imtiaz [aut],\n  Matthias Kohl [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9514-8910>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RFLPtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RFLPtools Tools to Analyse RFLP Data Provides functions to analyse DNA fragment samples (i.e. derived from RFLP-analysis) and standalone BLAST report files (i.e. DNA sequence analysis).  "
  },
  {
    "id": 6218,
    "package_name": "RGoogleAnalyticsPremium",
    "title": "Unsampled Data in R for Google Analytics Premium Accounts",
    "description": "It fires a query to the API to get the unsampled data in R for Google Analytics Premium Accounts. It retrieves data from the Google drive document and stores it into the local drive. The path to the excel file is returned by this package. The user can read data from the excel file into R using read.csv() function.",
    "version": "0.1.1",
    "maintainer": "Jalpa Joshi Dave <jalpa@tatvic.com>",
    "author": "Jalpa Joshi Dave",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RGoogleAnalyticsPremium",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RGoogleAnalyticsPremium Unsampled Data in R for Google Analytics Premium Accounts It fires a query to the API to get the unsampled data in R for Google Analytics Premium Accounts. It retrieves data from the Google drive document and stores it into the local drive. The path to the excel file is returned by this package. The user can read data from the excel file into R using read.csv() function.  "
  },
  {
    "id": 6245,
    "package_name": "RInside",
    "title": "C++ Classes to Embed R in C++ (and C) Applications",
    "description": "C++ classes to embed R in C++ (and C) applications\n A C++ class providing the R interpreter is offered by this package\n making it easier to have \"R inside\" your C++ application. As R itself\n is embedded into your application, a shared library build of R is\n required. This works on Linux, OS X and even on Windows provided you\n use the same tools used to build R itself. Numerous examples are\n provided in the nine subdirectories of the examples/ directory of\n the installed package: standard, 'mpi' (for parallel computing), 'qt'\n (showing how to embed 'RInside' inside a Qt GUI application), 'wt'\n (showing how to build a \"web-application\" using the Wt toolkit),\n 'armadillo' (for 'RInside' use with 'RcppArmadillo'), 'eigen' (for\n 'RInside' use with 'RcppEigen'), and 'c_interface' for a basic C\n interface and 'Ruby' illustration.  The examples use 'GNUmakefile(s)'\n with GNU extensions, so a GNU make is required (and will use the\n 'GNUmakefile' automatically). 'Doxygen'-generated documentation of\n the C++ classes is available at the 'RInside' website as well.",
    "version": "0.2.19",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Romain Francois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Lance Bachmeier [ctb]",
    "url": "https://github.com/eddelbuettel/rinside/,\nhttps://dirk.eddelbuettel.com/code/rinside.html",
    "bug_reports": "https://github.com/eddelbuettel/rinside/issues",
    "repository": "https://cran.r-project.org/package=RInside",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RInside C++ Classes to Embed R in C++ (and C) Applications C++ classes to embed R in C++ (and C) applications\n A C++ class providing the R interpreter is offered by this package\n making it easier to have \"R inside\" your C++ application. As R itself\n is embedded into your application, a shared library build of R is\n required. This works on Linux, OS X and even on Windows provided you\n use the same tools used to build R itself. Numerous examples are\n provided in the nine subdirectories of the examples/ directory of\n the installed package: standard, 'mpi' (for parallel computing), 'qt'\n (showing how to embed 'RInside' inside a Qt GUI application), 'wt'\n (showing how to build a \"web-application\" using the Wt toolkit),\n 'armadillo' (for 'RInside' use with 'RcppArmadillo'), 'eigen' (for\n 'RInside' use with 'RcppEigen'), and 'c_interface' for a basic C\n interface and 'Ruby' illustration.  The examples use 'GNUmakefile(s)'\n with GNU extensions, so a GNU make is required (and will use the\n 'GNUmakefile' automatically). 'Doxygen'-generated documentation of\n the C++ classes is available at the 'RInside' website as well.  "
  },
  {
    "id": 6252,
    "package_name": "RJafroc",
    "title": "Artificial Intelligence Systems and Observer Performance",
    "description": "Analyzing the performance of artificial intelligence\n (AI) systems/algorithms characterized by a 'search-and-report'\n strategy. Historically observer performance has dealt with\n measuring radiologists' performances in search tasks, e.g., searching\n for lesions in medical images and reporting them, but the implicit\n location information has been ignored. The implemented methods apply\n to analyzing the absolute and relative performances of AI systems,\n comparing AI performance to a group of human readers or optimizing the\n reporting threshold of an AI system. In addition to performing historical\n receiver operating receiver operating characteristic (ROC) analysis\n (localization information ignored), the software also performs\n free-response receiver operating characteristic (FROC)\n analysis, where lesion localization information is used. A book\n using the software has been published: Chakraborty DP: Observer\n Performance Methods for Diagnostic Imaging - Foundations, Modeling,\n and Applications with R-Based Examples, Taylor-Francis LLC; 2017:\n <https://www.routledge.com/Observer-Performance-Methods-for-Diagnostic-Imaging-Foundations-Modeling/Chakraborty/p/book/9781482214840>.\n Online updates to this book, which use the software, are at\n <https://dpc10ster.github.io/RJafrocQuickStart/>,\n <https://dpc10ster.github.io/RJafrocRocBook/> and at\n <https://dpc10ster.github.io/RJafrocFrocBook/>. Supported data\n collection paradigms are the ROC, FROC and the location ROC (LROC).\n ROC data consists of single ratings per images, where a rating is\n the perceived confidence level that the image is that of a diseased\n patient. An ROC curve is a plot of true positive fraction vs. false\n positive fraction. FROC data consists of a variable number (zero or\n more) of mark-rating pairs per image, where a mark is the location\n of a reported suspicious region and the rating is the confidence\n level that it is a real lesion. LROC data consists of a rating and a\n location of the most suspicious region, for every image. Four models\n of observer performance, and curve-fitting software, are implemented:\n the binormal model (BM), the contaminated binormal model (CBM), the\n correlated contaminated binormal model (CORCBM), and the radiological\n search model (RSM). Unlike the binormal model, CBM, CORCBM and RSM\n predict 'proper' ROC curves that do not inappropriately cross the\n chance diagonal. Additionally, RSM parameters are related to search\n performance (not measured in conventional ROC analysis) and\n classification performance. Search performance refers to finding\n lesions, i.e., true positives, while simultaneously not finding false\n positive locations. Classification performance measures the ability to\n distinguish between true and false positive locations. Knowing these\n separate performances allows principled optimization of reader or AI\n system performance. This package supersedes Windows JAFROC (jackknife\n alternative FROC) software V4.2.1,\n <https://github.com/dpc10ster/WindowsJafroc>. Package functions are\n organized as follows. Data file related function names are preceded\n by 'Df', curve fitting functions by 'Fit', included data sets by 'dataset',\n plotting functions by 'Plot', significance testing functions by 'St',\n sample size related functions by 'Ss', data simulation functions by\n 'Simulate' and utility functions by 'Util'. Implemented are figures of\n merit (FOMs) for quantifying performance and functions for visualizing\n empirical or fitted operating characteristics: e.g., ROC, FROC, alternative\n FROC (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study\n designs significance testing of reader-averaged FOM differences between\n modalities is implemented via either Dorfman-Berbaum-Metz or the\n Obuchowski-Rockette methods. Also implemented is single treatment analysis,\n which allows comparison of performance of a group of radiologists to a\n specified value, or comparison of AI to a group of radiologists interpreting\n the same cases. Crossed-modality analysis is implemented wherein there are\n two crossed treatment factors and the aim is to determined performance in\n each treatment factor averaged over all levels of the second factor. Sample\n size estimation tools are provided for ROC and FROC studies; these use\n estimates of the relevant variances from a pilot study to predict required\n numbers of readers and cases in a pivotal study to achieve the desired power.\n Utility and data file manipulation functions allow data to be read in any of\n the currently used input formats, including Excel, and the results of the\n analysis can be viewed in text or Excel output files. The methods are\n illustrated with several included datasets from the author's collaborations.\n This update includes improvements to the code, some as a result of\n user-reported bugs and new feature requests, and others discovered during\n ongoing testing and code simplification.",
    "version": "2.1.2",
    "maintainer": "Dev Chakraborty <dpc10ster@gmail.com>",
    "author": "Dev Chakraborty [cre, aut, cph],\n  Peter Phillips [ctb],\n  Xuetong Zhai [aut]",
    "url": "https://dpc10ster.github.io/RJafroc/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RJafroc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RJafroc Artificial Intelligence Systems and Observer Performance Analyzing the performance of artificial intelligence\n (AI) systems/algorithms characterized by a 'search-and-report'\n strategy. Historically observer performance has dealt with\n measuring radiologists' performances in search tasks, e.g., searching\n for lesions in medical images and reporting them, but the implicit\n location information has been ignored. The implemented methods apply\n to analyzing the absolute and relative performances of AI systems,\n comparing AI performance to a group of human readers or optimizing the\n reporting threshold of an AI system. In addition to performing historical\n receiver operating receiver operating characteristic (ROC) analysis\n (localization information ignored), the software also performs\n free-response receiver operating characteristic (FROC)\n analysis, where lesion localization information is used. A book\n using the software has been published: Chakraborty DP: Observer\n Performance Methods for Diagnostic Imaging - Foundations, Modeling,\n and Applications with R-Based Examples, Taylor-Francis LLC; 2017:\n <https://www.routledge.com/Observer-Performance-Methods-for-Diagnostic-Imaging-Foundations-Modeling/Chakraborty/p/book/9781482214840>.\n Online updates to this book, which use the software, are at\n <https://dpc10ster.github.io/RJafrocQuickStart/>,\n <https://dpc10ster.github.io/RJafrocRocBook/> and at\n <https://dpc10ster.github.io/RJafrocFrocBook/>. Supported data\n collection paradigms are the ROC, FROC and the location ROC (LROC).\n ROC data consists of single ratings per images, where a rating is\n the perceived confidence level that the image is that of a diseased\n patient. An ROC curve is a plot of true positive fraction vs. false\n positive fraction. FROC data consists of a variable number (zero or\n more) of mark-rating pairs per image, where a mark is the location\n of a reported suspicious region and the rating is the confidence\n level that it is a real lesion. LROC data consists of a rating and a\n location of the most suspicious region, for every image. Four models\n of observer performance, and curve-fitting software, are implemented:\n the binormal model (BM), the contaminated binormal model (CBM), the\n correlated contaminated binormal model (CORCBM), and the radiological\n search model (RSM). Unlike the binormal model, CBM, CORCBM and RSM\n predict 'proper' ROC curves that do not inappropriately cross the\n chance diagonal. Additionally, RSM parameters are related to search\n performance (not measured in conventional ROC analysis) and\n classification performance. Search performance refers to finding\n lesions, i.e., true positives, while simultaneously not finding false\n positive locations. Classification performance measures the ability to\n distinguish between true and false positive locations. Knowing these\n separate performances allows principled optimization of reader or AI\n system performance. This package supersedes Windows JAFROC (jackknife\n alternative FROC) software V4.2.1,\n <https://github.com/dpc10ster/WindowsJafroc>. Package functions are\n organized as follows. Data file related function names are preceded\n by 'Df', curve fitting functions by 'Fit', included data sets by 'dataset',\n plotting functions by 'Plot', significance testing functions by 'St',\n sample size related functions by 'Ss', data simulation functions by\n 'Simulate' and utility functions by 'Util'. Implemented are figures of\n merit (FOMs) for quantifying performance and functions for visualizing\n empirical or fitted operating characteristics: e.g., ROC, FROC, alternative\n FROC (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study\n designs significance testing of reader-averaged FOM differences between\n modalities is implemented via either Dorfman-Berbaum-Metz or the\n Obuchowski-Rockette methods. Also implemented is single treatment analysis,\n which allows comparison of performance of a group of radiologists to a\n specified value, or comparison of AI to a group of radiologists interpreting\n the same cases. Crossed-modality analysis is implemented wherein there are\n two crossed treatment factors and the aim is to determined performance in\n each treatment factor averaged over all levels of the second factor. Sample\n size estimation tools are provided for ROC and FROC studies; these use\n estimates of the relevant variances from a pilot study to predict required\n numbers of readers and cases in a pivotal study to achieve the desired power.\n Utility and data file manipulation functions allow data to be read in any of\n the currently used input formats, including Excel, and the results of the\n analysis can be viewed in text or Excel output files. The methods are\n illustrated with several included datasets from the author's collaborations.\n This update includes improvements to the code, some as a result of\n user-reported bugs and new feature requests, and others discovered during\n ongoing testing and code simplification.  "
  },
  {
    "id": 6254,
    "package_name": "RKEA",
    "title": "R/KEA Interface",
    "description": "An R interface to KEA (Version 5.0).\n  KEA (for Keyphrase Extraction Algorithm) allows for extracting\n  keyphrases from text documents. It can be either used for free\n  indexing or for indexing with a controlled vocabulary. For more\n  information see <http://www.nzdl.org/Kea/>.",
    "version": "0.0-6",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Ingo Feinerer [aut],\n  Kurt Hornik [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RKEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RKEA R/KEA Interface An R interface to KEA (Version 5.0).\n  KEA (for Keyphrase Extraction Algorithm) allows for extracting\n  keyphrases from text documents. It can be either used for free\n  indexing or for indexing with a controlled vocabulary. For more\n  information see <http://www.nzdl.org/Kea/>.  "
  },
  {
    "id": 6364,
    "package_name": "RProtoBuf",
    "title": "R Interface to the 'Protocol Buffers' 'API' (Version 2 or 3)",
    "description": "Protocol Buffers are a way of encoding structured data in an\n efficient yet extensible format. Google uses Protocol Buffers for almost all\n of its internal 'RPC' protocols and file formats.  Additional documentation\n is available in two included vignettes one of which corresponds to our 'JSS'\n paper (2016, <doi:10.18637/jss.v071.i02>. A sufficiently recent version of\n 'Protocol Buffers' library is required; currently version 3.3.0 from 2017\n is the stated minimum.",
    "version": "0.4.24",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Romain Francois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Murray Stokely [aut] (ORCID: <https://orcid.org/0009-0008-3390-1338>),\n  Jeroen Ooms [aut] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://github.com/eddelbuettel/rprotobuf,\nhttps://dirk.eddelbuettel.com/code/rprotobuf.html",
    "bug_reports": "https://github.com/eddelbuettel/rprotobuf/issues",
    "repository": "https://cran.r-project.org/package=RProtoBuf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RProtoBuf R Interface to the 'Protocol Buffers' 'API' (Version 2 or 3) Protocol Buffers are a way of encoding structured data in an\n efficient yet extensible format. Google uses Protocol Buffers for almost all\n of its internal 'RPC' protocols and file formats.  Additional documentation\n is available in two included vignettes one of which corresponds to our 'JSS'\n paper (2016, <doi:10.18637/jss.v071.i02>. A sufficiently recent version of\n 'Protocol Buffers' library is required; currently version 3.3.0 from 2017\n is the stated minimum.  "
  },
  {
    "id": 6375,
    "package_name": "RRMLRfMC",
    "title": "Reduced-Rank Multinomial Logistic Regression for Markov Chains",
    "description": "Fit the reduced-rank multinomial logistic regression model for Markov\n    chains developed by Wang, Abner, Fardo, Schmitt, Jicha, Eldik and Kryscio\n    (2021)<doi:10.1002/sim.8923> in R. It combines the ideas of multinomial\n    logistic regression in Markov chains and reduced-rank. It is very useful in \n    a study where multi-states model is assumed and each transition among the \n    states is controlled by a series of covariates. The key advantage is to \n    reduce the number of parameters to be estimated. The final coefficients for \n    all the covariates and the p-values for the interested covariates will be \n    reported. The p-values for the whole coefficient matrix can be calculated by \n    two bootstrap methods.",
    "version": "0.4.0",
    "maintainer": "Pei Wang <wangp33@miamioh.edu>",
    "author": "Pei Wang [aut, cre],\n  Richard Kryscio [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RRMLRfMC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RRMLRfMC Reduced-Rank Multinomial Logistic Regression for Markov Chains Fit the reduced-rank multinomial logistic regression model for Markov\n    chains developed by Wang, Abner, Fardo, Schmitt, Jicha, Eldik and Kryscio\n    (2021)<doi:10.1002/sim.8923> in R. It combines the ideas of multinomial\n    logistic regression in Markov chains and reduced-rank. It is very useful in \n    a study where multi-states model is assumed and each transition among the \n    states is controlled by a series of covariates. The key advantage is to \n    reduce the number of parameters to be estimated. The final coefficients for \n    all the covariates and the p-values for the interested covariates will be \n    reported. The p-values for the whole coefficient matrix can be calculated by \n    two bootstrap methods.  "
  },
  {
    "id": 6421,
    "package_name": "RSurveillance",
    "title": "Design and Analysis of Disease Surveillance Activities",
    "description": "A range of functions for the design and\n    analysis of disease surveillance activities. These functions were\n    originally developed for animal health surveillance activities but can be\n    equally applied to aquatic animal, wildlife, plant and human health\n    surveillance activities. Utilities are included for sample size calculation\n    and analysis of representative surveys for disease freedom, risk-based\n    studies for disease freedom and for prevalence estimation.\n    This package is based on Cameron A., Conraths F., Frohlich A., Schauer B.,\n    Schulz K., Sergeant E., Sonnenburg J., Staubach C. (2015). R package of \n    functions for risk-based surveillance. Deliverable 6.24, WP 6 - Decision \n    making tools for implementing risk-based surveillance, Grant Number \n    no. 310806, RISKSUR (<https://www.fp7-risksur.eu/sites/default/files/documents/Deliverables/RISKSUR_%28310806%29_D6.24.pdf>). \n    Many of the 'RSurveillance' functions are incorporated into the 'epitools'\n    website: Sergeant, ESG, 2019. Epitools epidemiological calculators. \n    Ausvet Pty Ltd. Available at: <http://epitools.ausvet.com.au>.",
    "version": "0.2.1",
    "maintainer": "Rohan Sadler <rohan.sadler@ausvet.com.au>",
    "author": "Evan Sergeant",
    "url": "https://github.com/roStats/RSurveillance",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSurveillance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSurveillance Design and Analysis of Disease Surveillance Activities A range of functions for the design and\n    analysis of disease surveillance activities. These functions were\n    originally developed for animal health surveillance activities but can be\n    equally applied to aquatic animal, wildlife, plant and human health\n    surveillance activities. Utilities are included for sample size calculation\n    and analysis of representative surveys for disease freedom, risk-based\n    studies for disease freedom and for prevalence estimation.\n    This package is based on Cameron A., Conraths F., Frohlich A., Schauer B.,\n    Schulz K., Sergeant E., Sonnenburg J., Staubach C. (2015). R package of \n    functions for risk-based surveillance. Deliverable 6.24, WP 6 - Decision \n    making tools for implementing risk-based surveillance, Grant Number \n    no. 310806, RISKSUR (<https://www.fp7-risksur.eu/sites/default/files/documents/Deliverables/RISKSUR_%28310806%29_D6.24.pdf>). \n    Many of the 'RSurveillance' functions are incorporated into the 'epitools'\n    website: Sergeant, ESG, 2019. Epitools epidemiological calculators. \n    Ausvet Pty Ltd. Available at: <http://epitools.ausvet.com.au>.  "
  },
  {
    "id": 6427,
    "package_name": "RTLknitr",
    "title": "Right to Left Dynamic Documents Using 'knitr'",
    "description": "Provide seamless support for right-to-left (RTL) languages, such as Persian and Arabic,\n  in R Markdown documents and 'LaTeX' output. It includes functions and hooks that enable easy\n  integration of RTL language content, allowing users to create documents that adhere to RTL writing\n  conventions. For in-depth insights into dynamic documents and the 'knitr' package, consider referring\n  to Xie, Y (2014) <ISBN: 978-1-482-20353-0>.",
    "version": "1.1.1",
    "maintainer": "Foad Esmaeili <foadesmaeili5@gmail.com>",
    "author": "Foad Esmaeili [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9638-0807>)",
    "url": "https://github.com/FoadEsmaeili5/RTLknitr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RTLknitr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTLknitr Right to Left Dynamic Documents Using 'knitr' Provide seamless support for right-to-left (RTL) languages, such as Persian and Arabic,\n  in R Markdown documents and 'LaTeX' output. It includes functions and hooks that enable easy\n  integration of RTL language content, allowing users to create documents that adhere to RTL writing\n  conventions. For in-depth insights into dynamic documents and the 'knitr' package, consider referring\n  to Xie, Y (2014) <ISBN: 978-1-482-20353-0>.  "
  },
  {
    "id": 6433,
    "package_name": "RTextTools",
    "title": "Automatic Text Classification via Supervised Learning",
    "description": "A machine learning package for automatic text classification \n\tthat makes it simple for novice users to get started with machine \n\tlearning, while allowing experienced users to easily experiment \n\twith different settings and algorithm combinations. The package \n\tincludes eight algorithms for ensemble classification (svm, slda, \n\tboosting, bagging, random forests, glmnet, decision trees, neural \n\tnetworks), comprehensive analytics, and thorough documentation.",
    "version": "1.4.3",
    "maintainer": "Loren Collingwood <loren.collingwood@gmail.com>",
    "author": "Timothy P. Jurka, Loren Collingwood, Amber E. Boydstun,\n        Emiliano Grossman, Wouter van Atteveldt",
    "url": "http://www.rtexttools.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RTextTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTextTools Automatic Text Classification via Supervised Learning A machine learning package for automatic text classification \n\tthat makes it simple for novice users to get started with machine \n\tlearning, while allowing experienced users to easily experiment \n\twith different settings and algorithm combinations. The package \n\tincludes eight algorithms for ensemble classification (svm, slda, \n\tboosting, bagging, random forests, glmnet, decision trees, neural \n\tnetworks), comprehensive analytics, and thorough documentation.  "
  },
  {
    "id": 6436,
    "package_name": "RUnit",
    "title": "R Unit Test Framework",
    "description": "R functions implementing a standard Unit Testing\n        framework, with additional code inspection and report\n        generation tools.",
    "version": "0.4.33.1",
    "maintainer": "Roman Zenka <zenka.roman@mayo.edu>",
    "author": "Matthias Burger [aut],\n  Klaus Juenemann [aut],\n  Thomas Koenig [aut],\n  Roman Zenka [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RUnit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RUnit R Unit Test Framework R functions implementing a standard Unit Testing\n        framework, with additional code inspection and report\n        generation tools.  "
  },
  {
    "id": 6451,
    "package_name": "RWsearch",
    "title": "Lazy Search in R Packages, Task Views, CRAN, the Web. All-in-One\nDownload",
    "description": "Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the package documentation (html index, README, NEWS, pdf manual, vignettes, source code, binaries) with a single instruction. Visualize the package dependencies and CRAN checks. Compare the package versions, unload and install the packages and their dependencies in a safe order. Explore CRAN archives. Use the above functions for task view maintenance. Access web search engines from the console thanks to 80+ bookmarks. All functions accept standard and non-standard evaluation.",
    "version": "5.2.6",
    "maintainer": "Patrice Kiener <rpackages@inmodelia.com>",
    "author": "Patrice Kiener [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0505-9920>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RWsearch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RWsearch Lazy Search in R Packages, Task Views, CRAN, the Web. All-in-One\nDownload Search by keywords in R packages, task views, CRAN, the web and display the results in the console or in txt, html or pdf files. Download the package documentation (html index, README, NEWS, pdf manual, vignettes, source code, binaries) with a single instruction. Visualize the package dependencies and CRAN checks. Compare the package versions, unload and install the packages and their dependencies in a safe order. Explore CRAN archives. Use the above functions for task view maintenance. Access web search engines from the console thanks to 80+ bookmarks. All functions accept standard and non-standard evaluation.  "
  },
  {
    "id": 6467,
    "package_name": "RagGrid",
    "title": "A Wrapper of the 'JavaScript' Library 'agGrid'",
    "description": "Data objects in 'R' can be rendered as 'HTML' tables using the\n    'JavaScript' library 'ag-grid' (typically via 'R Markdown' or 'Shiny'). The\n    'ag-grid' library has been included in this 'R' package. The package name\n    'RagGrid' is an abbreviation of 'R agGrid'.",
    "version": "0.2.0",
    "maintainer": "Srikkanth M <srikkanth18@gmail.com>",
    "author": "Srikkanth M [aut, cre],\n  Praveen N [aut, ctb]",
    "url": "https://github.com/no-types/RagGrid/",
    "bug_reports": "https://github.com/no-types/RagGrid/issues",
    "repository": "https://cran.r-project.org/package=RagGrid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RagGrid A Wrapper of the 'JavaScript' Library 'agGrid' Data objects in 'R' can be rendered as 'HTML' tables using the\n    'JavaScript' library 'ag-grid' (typically via 'R Markdown' or 'Shiny'). The\n    'ag-grid' library has been included in this 'R' package. The package name\n    'RagGrid' is an abbreviation of 'R agGrid'.  "
  },
  {
    "id": 6492,
    "package_name": "RastaRocket",
    "title": "Rocket-Fast Clinical Research Reporting",
    "description": "Description of the tables, both grouped and not grouped, with some associated data management actions, \n             such as sorting the terms of the variables and deleting terms with zero numbers.",
    "version": "1.0.0",
    "maintainer": "USMR CHU de Bordeaux <astreinte.usmr@chu-bordeaux.fr>",
    "author": "USMR CHU de Bordeaux [aut, cre],\n  Valentine Renaudeau [aut],\n  Marion Kret [aut],\n  Matisse Decilap [aut],\n  Sahardid Mohamed Houssein [aut],\n  Thomas Fert\u00e9 [aut]",
    "url": "https://github.com/biostatusmr/RastaRocket,\nhttps://biostatusmr.github.io/RastaRocket/",
    "bug_reports": "https://github.com/biostatusmr/RastaRocket/issues",
    "repository": "https://cran.r-project.org/package=RastaRocket",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RastaRocket Rocket-Fast Clinical Research Reporting Description of the tables, both grouped and not grouped, with some associated data management actions, \n             such as sorting the terms of the variables and deleting terms with zero numbers.  "
  },
  {
    "id": 6493,
    "package_name": "Rata",
    "title": "Automated Test Assembly",
    "description": "Automated test assembly of linear and adaptive tests using the \n    mixed-integer programming. The full documentation and tutorials are at \n    <https://github.com/xluo11/Rata>.",
    "version": "0.0.2",
    "maintainer": "Xiao Luo <xluo1986@gmail.com>",
    "author": "Xiao Luo [aut, cre]",
    "url": "https://github.com/xluo11/Rata",
    "bug_reports": "https://github.com/xluo11/Rata/issues",
    "repository": "https://cran.r-project.org/package=Rata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rata Automated Test Assembly Automated test assembly of linear and adaptive tests using the \n    mixed-integer programming. The full documentation and tutorials are at \n    <https://github.com/xluo11/Rata>.  "
  },
  {
    "id": 6501,
    "package_name": "Rbeast",
    "title": "Bayesian Change-Point Detection and Time Series Decomposition",
    "description": "Interpretation of time series data is affected by model choices. Different models can give different or even contradicting estimates of patterns, trends, and mechanisms for the same data--a limitation alleviated by the Bayesian estimator of abrupt change,seasonality, and trend (BEAST) of this package. BEAST seeks to improve time series decomposition by forgoing the \"single-best-model\" concept and embracing all competing models into the inference via a Bayesian model averaging scheme. It is a flexible tool to uncover abrupt changes (i.e., change-points, breakpoints, structural breaks, or join-points), cyclic variations (e.g., seasonality), and nonlinear trends in time-series observations. BEAST not just tells when changes occur but also quantifies how likely the detected changes are true. It detects not just piecewise linear trends but also arbitrary nonlinear trends. BEAST is applicable to real-valued time series data of all kinds, be it for remote sensing, economics, climate sciences, ecology, and hydrology. Example applications include its use to identify regime shifts in ecological data, map forest disturbance and land degradation from satellite imagery, detect market trends in economic data, pinpoint anomaly and extreme events in climate data, and unravel system dynamics in biological data. Details on BEAST are reported in Zhao et al. (2019) <doi:10.1016/j.rse.2019.04.034>.",
    "version": "1.0.1",
    "maintainer": "Kaiguang Zhao <zhao.1423@osu.edu>",
    "author": "Tongxi Hu [aut],\n  Yang Li [aut],\n  Xuesong Zhang [aut],\n  Kaiguang Zhao [aut, cre],\n  Jack Dongarra [ctb],\n  Cleve Moler [ctb]",
    "url": "https://github.com/zhaokg/Rbeast",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rbeast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rbeast Bayesian Change-Point Detection and Time Series Decomposition Interpretation of time series data is affected by model choices. Different models can give different or even contradicting estimates of patterns, trends, and mechanisms for the same data--a limitation alleviated by the Bayesian estimator of abrupt change,seasonality, and trend (BEAST) of this package. BEAST seeks to improve time series decomposition by forgoing the \"single-best-model\" concept and embracing all competing models into the inference via a Bayesian model averaging scheme. It is a flexible tool to uncover abrupt changes (i.e., change-points, breakpoints, structural breaks, or join-points), cyclic variations (e.g., seasonality), and nonlinear trends in time-series observations. BEAST not just tells when changes occur but also quantifies how likely the detected changes are true. It detects not just piecewise linear trends but also arbitrary nonlinear trends. BEAST is applicable to real-valued time series data of all kinds, be it for remote sensing, economics, climate sciences, ecology, and hydrology. Example applications include its use to identify regime shifts in ecological data, map forest disturbance and land degradation from satellite imagery, detect market trends in economic data, pinpoint anomaly and extreme events in climate data, and unravel system dynamics in biological data. Details on BEAST are reported in Zhao et al. (2019) <doi:10.1016/j.rse.2019.04.034>.  "
  },
  {
    "id": 6526,
    "package_name": "RcmdrPlugin.EZR",
    "title": "R Commander Plug-in for the EZR (Easy R) Package",
    "description": "EZR (Easy R) adds a variety of statistical functions, including survival analyses, ROC analyses, metaanalyses, sample size calculation, and so on, to the R commander. EZR enables point-and-click easy access to statistical functions, especially for medical statistics. EZR is platform-independent and runs on Windows, Mac OS X, and UNIX. Its complete manual is available only in Japanese (Chugai Igakusha, ISBN: 978-4-498-10918-6, Nankodo, ISBN: 978-4-524-21861-5, Ohmsha, ISBN: 978-4-274-22632-8), but an report that introduced the investigation of EZR was published in Bone Marrow Transplantation (Nature Publishing Group) as an Open article. This report can be used as a simple manual. It can be freely downloaded from the journal website as shown below. This report has been cited in more than 14,000 scientific articles.",
    "version": "1.70",
    "maintainer": "Yoshinobu Kanda <ycanda-tky@umin.ac.jp>",
    "author": "Yoshinobu Kanda [aut, cre]",
    "url": "https://www.nature.com/articles/bmt2012244.pdf\nhttps://www.jichi.ac.jp/usr/hema/EZR/statmedEN.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RcmdrPlugin.EZR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcmdrPlugin.EZR R Commander Plug-in for the EZR (Easy R) Package EZR (Easy R) adds a variety of statistical functions, including survival analyses, ROC analyses, metaanalyses, sample size calculation, and so on, to the R commander. EZR enables point-and-click easy access to statistical functions, especially for medical statistics. EZR is platform-independent and runs on Windows, Mac OS X, and UNIX. Its complete manual is available only in Japanese (Chugai Igakusha, ISBN: 978-4-498-10918-6, Nankodo, ISBN: 978-4-524-21861-5, Ohmsha, ISBN: 978-4-274-22632-8), but an report that introduced the investigation of EZR was published in Bone Marrow Transplantation (Nature Publishing Group) as an Open article. This report can be used as a simple manual. It can be freely downloaded from the journal website as shown below. This report has been cited in more than 14,000 scientific articles.  "
  },
  {
    "id": 6527,
    "package_name": "RcmdrPlugin.Export",
    "title": "Export R Output to LaTeX or HTML",
    "description": "Export Rcmdr output to LaTeX or HTML code. The\n        plug-in was originally intended to facilitate exporting Rcmdr\n        output to formats other than ASCII text and to provide R\n        novices with an easy-to-use, easy-to-access reference on\n        exporting R objects to formats suited for printed output. The\n        package documentation contains several pointers on creating\n        reports, either by using conventional word processors or\n        LaTeX/LyX.",
    "version": "0.3-1",
    "maintainer": "Liviu Andronic <landronimirc@gmail.com>",
    "author": "Liviu Andronic",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RcmdrPlugin.Export",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcmdrPlugin.Export Export R Output to LaTeX or HTML Export Rcmdr output to LaTeX or HTML code. The\n        plug-in was originally intended to facilitate exporting Rcmdr\n        output to formats other than ASCII text and to provide R\n        novices with an easy-to-use, easy-to-access reference on\n        exporting R objects to formats suited for printed output. The\n        package documentation contains several pointers on creating\n        reports, either by using conventional word processors or\n        LaTeX/LyX.  "
  },
  {
    "id": 6547,
    "package_name": "RcmdrPlugin.temis",
    "title": "Graphical Integrated Text Mining Solution",
    "description": "An 'R Commander' plug-in providing an integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, vocabulary tables, terms\n    co-occurrences and documents similarity measures, time series analysis,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",
    "version": "0.7.12",
    "maintainer": "Milan Bouchet-Valat <nalimilan@club.fr>",
    "author": "Milan Bouchet-Valat [aut, cre],\n  Gilles Bastin [aut]",
    "url": "https://github.com/nalimilan/R.TeMiS",
    "bug_reports": "https://github.com/nalimilan/R.TeMiS/issues",
    "repository": "https://cran.r-project.org/package=RcmdrPlugin.temis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcmdrPlugin.temis Graphical Integrated Text Mining Solution An 'R Commander' plug-in providing an integrated solution to perform\n    a series of text mining tasks such as importing and cleaning a corpus, and\n    analyses like terms and documents counts, vocabulary tables, terms\n    co-occurrences and documents similarity measures, time series analysis,\n    correspondence analysis and hierarchical clustering. Corpora can be imported\n    from spreadsheet-like files, directories of raw text files,\n    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.  "
  },
  {
    "id": 6559,
    "package_name": "RcppCGAL",
    "title": "'Rcpp' Integration for 'CGAL'",
    "description": "Creates a header only package to link to the 'CGAL' \n  (Computational Geometry Algorithms Library)\n  header files in 'Rcpp'. There are a variety of potential uses for \n  the software such as Hilbert sorting, K-D Tree nearest neighbors, \n  and convex hull algorithms. For more information about how to use the header files, \n  see the 'CGAL' documentation at <https://www.cgal.org>. Currently\n  downloads version 6.1 of the 'CGAL' header files.",
    "version": "6.1",
    "maintainer": "Eric Dunipace <edunipace@mail.harvard.edu>",
    "author": "Eric Dunipace [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8909-213X>),\n  Tyler Morgan-Wall [ctb],\n  The CGAL Project [cph]",
    "url": "https://github.com/ericdunipace/RcppCGAL",
    "bug_reports": "https://github.com/ericdunipace/RcppCGAL/issues",
    "repository": "https://cran.r-project.org/package=RcppCGAL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppCGAL 'Rcpp' Integration for 'CGAL' Creates a header only package to link to the 'CGAL' \n  (Computational Geometry Algorithms Library)\n  header files in 'Rcpp'. There are a variety of potential uses for \n  the software such as Hilbert sorting, K-D Tree nearest neighbors, \n  and convex hull algorithms. For more information about how to use the header files, \n  see the 'CGAL' documentation at <https://www.cgal.org>. Currently\n  downloads version 6.1 of the 'CGAL' header files.  "
  },
  {
    "id": 6575,
    "package_name": "RcppExamples",
    "title": "Examples using 'Rcpp' to Interface R and C++",
    "description": "Examples for Seamless R and C++ integration\n The 'Rcpp' package contains a C++ library that facilitates the integration of\n R and C++ in various ways. This package provides some usage examples.\n Note that the documentation in this package currently does not cover all the\n features in the package. The site <https://gallery.rcpp.org> regroups a large\n number of examples for 'Rcpp'.",
    "version": "0.1.10",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Romain Francois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>)",
    "url": "https://github.com/eddelbuettel/rcppexamples,\nhttps://dirk.eddelbuettel.com/code/rcpp.examples.html",
    "bug_reports": "https://github.com/eddelbuettel/rcppexamples/issues",
    "repository": "https://cran.r-project.org/package=RcppExamples",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppExamples Examples using 'Rcpp' to Interface R and C++ Examples for Seamless R and C++ integration\n The 'Rcpp' package contains a C++ library that facilitates the integration of\n R and C++ in various ways. This package provides some usage examples.\n Note that the documentation in this package currently does not cover all the\n features in the package. The site <https://gallery.rcpp.org> regroups a large\n number of examples for 'Rcpp'.  "
  },
  {
    "id": 6615,
    "package_name": "Rd2md",
    "title": "Markdown Reference Manuals",
    "description": "Native R only allows PDF exports of reference manuals.\n    The 'Rd2md' package converts the package documentation files into\n    markdown files and combines them into a markdown version of the package\n    reference manual.",
    "version": "1.0.1",
    "maintainer": "Julian Busch <jb@quants.ch>",
    "author": "Julian Busch [aut, cre]",
    "url": "https://github.com/quantsch/rd2md",
    "bug_reports": "https://github.com/quantsch/rd2md/issues",
    "repository": "https://cran.r-project.org/package=Rd2md",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rd2md Markdown Reference Manuals Native R only allows PDF exports of reference manuals.\n    The 'Rd2md' package converts the package documentation files into\n    markdown files and combines them into a markdown version of the package\n    reference manual.  "
  },
  {
    "id": 6616,
    "package_name": "Rd2roxygen",
    "title": "Convert Rd to 'Roxygen' Documentation",
    "description": "Functions to convert Rd to 'roxygen' documentation. It can parse an\n    Rd file to a list, create the 'roxygen' documentation and update the original\n    R script (e.g. the one containing the definition of the function)\n    accordingly. This package also provides utilities that can help developers\n    build packages using 'roxygen' more easily. The 'formatR' package can be used\n    to reformat the R code in the examples sections so that the code will be\n    more readable.",
    "version": "1.17",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Hadley Wickham [aut],\n  Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>)",
    "url": "https://github.com/yihui/Rd2roxygen",
    "bug_reports": "https://github.com/yihui/Rd2roxygen/issues",
    "repository": "https://cran.r-project.org/package=Rd2roxygen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rd2roxygen Convert Rd to 'Roxygen' Documentation Functions to convert Rd to 'roxygen' documentation. It can parse an\n    Rd file to a list, create the 'roxygen' documentation and update the original\n    R script (e.g. the one containing the definition of the function)\n    accordingly. This package also provides utilities that can help developers\n    build packages using 'roxygen' more easily. The 'formatR' package can be used\n    to reformat the R code in the examples sections so that the code will be\n    more readable.  "
  },
  {
    "id": 6621,
    "package_name": "Rdpack",
    "title": "Update and Manipulate Rd Documentation Objects",
    "description": "Functions for manipulation of R documentation objects,\n    including functions reprompt() and ereprompt() for updating 'Rd'\n    documentation for functions, methods and classes; 'Rd' macros for\n    citations and import of references from 'bibtex' files for use in\n    'Rd' files and 'roxygen2' comments; 'Rd' macros for evaluating and\n    inserting snippets of 'R' code and the results of its evaluation or\n    creating graphics on the fly; and many functions for manipulation of\n    references and Rd files.",
    "version": "2.6.4",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Georgi N. Boshnakov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>),\n  Duncan Murdoch [ctb]",
    "url": "https://geobosh.github.io/Rdpack/ (doc),\nhttps://github.com/GeoBosh/Rdpack (devel)",
    "bug_reports": "https://github.com/GeoBosh/Rdpack/issues",
    "repository": "https://cran.r-project.org/package=Rdpack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rdpack Update and Manipulate Rd Documentation Objects Functions for manipulation of R documentation objects,\n    including functions reprompt() and ereprompt() for updating 'Rd'\n    documentation for functions, methods and classes; 'Rd' macros for\n    citations and import of references from 'bibtex' files for use in\n    'Rd' files and 'roxygen2' comments; 'Rd' macros for evaluating and\n    inserting snippets of 'R' code and the results of its evaluation or\n    creating graphics on the fly; and many functions for manipulation of\n    references and Rd files.  "
  },
  {
    "id": 6678,
    "package_name": "ReportSubtotal",
    "title": "Adds Subtotals to Data Reports",
    "description": "Adds subtotal rows / sections (a la the 'SAS' 'Proc Tabulate' All option) to a Group By output by running a series of Group By functions with partial sets of the same variables and combining the results with the original. Can be used to add comprehensive information to a data report or to quickly aggregate Group By outputs used to gain a greater understanding of data.",
    "version": "0.1.2",
    "maintainer": "Yoni Aboody <yoniaboody@gmail.com>",
    "author": "Yoni Aboody [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ReportSubtotal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReportSubtotal Adds Subtotals to Data Reports Adds subtotal rows / sections (a la the 'SAS' 'Proc Tabulate' All option) to a Group By output by running a series of Group By functions with partial sets of the same variables and combining the results with the original. Can be used to add comprehensive information to a data report or to quickly aggregate Group By outputs used to gain a greater understanding of data.  "
  },
  {
    "id": 6679,
    "package_name": "ReporterScore",
    "title": "Generalized Reporter Score-Based Enrichment Analysis for Omics\nData",
    "description": "Inspired by the classic 'RSA', we developed the improved 'Generalized Reporter \n    Score-based Analysis (GRSA)' method, implemented in the R package 'ReporterScore', along \n    with comprehensive visualization methods and pathway databases. 'GRSA' is a threshold-free \n    method that works well with all types of biomedical features, such as genes, chemical compounds, \n    and microbial species. Importantly, the 'GRSA' supports multi-group and longitudinal experimental \n    designs, because of the included multi-group-compatible statistical methods. ",
    "version": "0.1.9",
    "maintainer": "Chen Peng <pengchen2001@zju.edu.cn>",
    "author": "Chen Peng [aut, cre] (ORCID: <https://orcid.org/0000-0002-9449-7606>)",
    "url": "https://github.com/Asa12138/ReporterScore",
    "bug_reports": "https://github.com/Asa12138/ReporterScore/issues",
    "repository": "https://cran.r-project.org/package=ReporterScore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReporterScore Generalized Reporter Score-Based Enrichment Analysis for Omics\nData Inspired by the classic 'RSA', we developed the improved 'Generalized Reporter \n    Score-based Analysis (GRSA)' method, implemented in the R package 'ReporterScore', along \n    with comprehensive visualization methods and pathway databases. 'GRSA' is a threshold-free \n    method that works well with all types of biomedical features, such as genes, chemical compounds, \n    and microbial species. Importantly, the 'GRSA' supports multi-group and longitudinal experimental \n    designs, because of the included multi-group-compatible statistical methods.   "
  },
  {
    "id": 6694,
    "package_name": "ReviewR",
    "title": "A Light-Weight, Portable Tool for Reviewing Individual Patient\nRecords",
    "description": "A portable Shiny tool to explore patient-level electronic health record data \n    and perform chart review in a single integrated framework. This tool supports \n    browsing clinical data in many different formats including multiple versions \n    of the 'OMOP' common data model as well as the 'MIMIC-III' data model. In \n    addition, chart review information is captured and stored securely via the \n    Shiny interface in a 'REDCap' (Research Electronic Data Capture) project \n    using the 'REDCap' API. See the 'ReviewR' website for additional information, \n    documentation, and examples.",
    "version": "2.3.10",
    "maintainer": "David Mayer <david.mayer@cuanschutz.edu>",
    "author": "Laura Wiley [aut] (ORCID: <https://orcid.org/0000-0001-6681-9754>),\n  Luke Rasmussen [aut] (ORCID: <https://orcid.org/0000-0002-4497-8049>),\n  David Mayer [cre, aut] (ORCID: <https://orcid.org/0000-0002-6056-9771>),\n  The Wiley Lab [cph, fnd]",
    "url": "https://reviewr.thewileylab.org/,\nhttps://github.com/thewileylab/ReviewR/",
    "bug_reports": "https://github.com/thewileylab/ReviewR/issues",
    "repository": "https://cran.r-project.org/package=ReviewR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReviewR A Light-Weight, Portable Tool for Reviewing Individual Patient\nRecords A portable Shiny tool to explore patient-level electronic health record data \n    and perform chart review in a single integrated framework. This tool supports \n    browsing clinical data in many different formats including multiple versions \n    of the 'OMOP' common data model as well as the 'MIMIC-III' data model. In \n    addition, chart review information is captured and stored securely via the \n    Shiny interface in a 'REDCap' (Research Electronic Data Capture) project \n    using the 'REDCap' API. See the 'ReviewR' website for additional information, \n    documentation, and examples.  "
  },
  {
    "id": 6717,
    "package_name": "Rigma",
    "title": "Access to the 'Figma' API",
    "description": "The goal of Rigma is to provide a user friendly client to the\n    'Figma' API <https://www.figma.com/developers/api>. It uses the latest\n    `httr2` for a stable interface with the REST API. More than 20 methods\n    are provided to interact with 'Figma' files, and teams. Get design\n    data into R by reading published components and styles, converting and\n    downloading images, getting access to the full 'Figma' file as a\n    hierarchical data structure, and much more. Enhance your creativity\n    and streamline the application development by automating the\n    extraction, transformation, and loading of design data to your\n    applications and 'HTML' documents.",
    "version": "0.3.0",
    "maintainer": "Alexandros Kouretsis <alexandros@appsilon.com>",
    "author": "Alexandros Kouretsis [aut, cre],\n  Eli Pousson [aut] (ORCID: <https://orcid.org/0000-0001-8280-1706>)",
    "url": "https://github.com/AleKoure/Rigma,\nhttps://AleKoure.github.io/Rigma/",
    "bug_reports": "https://github.com/AleKoure/Rigma/issues",
    "repository": "https://cran.r-project.org/package=Rigma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rigma Access to the 'Figma' API The goal of Rigma is to provide a user friendly client to the\n    'Figma' API <https://www.figma.com/developers/api>. It uses the latest\n    `httr2` for a stable interface with the REST API. More than 20 methods\n    are provided to interact with 'Figma' files, and teams. Get design\n    data into R by reading published components and styles, converting and\n    downloading images, getting access to the full 'Figma' file as a\n    hierarchical data structure, and much more. Enhance your creativity\n    and streamline the application development by automating the\n    extraction, transformation, and loading of design data to your\n    applications and 'HTML' documents.  "
  },
  {
    "id": 6719,
    "package_name": "Rirt",
    "title": "Data Analysis and Parameter Estimation Using Item Response\nTheory",
    "description": "Parameter estimation, computation of probability, information, and \n    (log-)likelihood, and visualization of item/test characteristic curves and\n    item/test information functions for three uni-dimensional item response theory\n    models: the 3-parameter-logistic model, generalized partial credit model, \n    and graded response model. The full documentation and tutorials are at \n    <https://github.com/xluo11/Rirt>.",
    "version": "0.0.2",
    "maintainer": "Xiao Luo <xluo1986@gmail.com>",
    "author": "Xiao Luo [aut, cre]",
    "url": "https://github.com/xluo11/Rirt",
    "bug_reports": "https://github.com/xluo11/Rirt/issues",
    "repository": "https://cran.r-project.org/package=Rirt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rirt Data Analysis and Parameter Estimation Using Item Response\nTheory Parameter estimation, computation of probability, information, and \n    (log-)likelihood, and visualization of item/test characteristic curves and\n    item/test information functions for three uni-dimensional item response theory\n    models: the 3-parameter-logistic model, generalized partial credit model, \n    and graded response model. The full documentation and tutorials are at \n    <https://github.com/xluo11/Rirt>.  "
  },
  {
    "id": 6724,
    "package_name": "Rita",
    "title": "Automated Transformations, Normality Testing, and Reporting",
    "description": "\n    Automated performance of common transformations used to fulfill parametric\n    assumptions of normality and identification of the best performing method \n    for the user. Output for various normality tests (Thode, 2002) corresponding \n    to the best performing method and a descriptive statistical report of the \n    input data in its original units (5-number summary and mathematical moments) \n    are also presented. Lastly, the Rankit, an empirical normal quantile transformation \n    (ENQT) (Soloman & Sawilowsky, 2009), is provided to accommodate non-standard \n    use cases and facilitate adoption.  \n    <DOI: 10.1201/9780203910894>.\n    <DOI: 10.22237/jmasm/1257034080>.",
    "version": "1.2.0",
    "maintainer": "Daniel Mattei <DMattei@live.com>",
    "author": "Daniel Mattei [aut, cre],\n  John Ruscio [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rita",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rita Automated Transformations, Normality Testing, and Reporting \n    Automated performance of common transformations used to fulfill parametric\n    assumptions of normality and identification of the best performing method \n    for the user. Output for various normality tests (Thode, 2002) corresponding \n    to the best performing method and a descriptive statistical report of the \n    input data in its original units (5-number summary and mathematical moments) \n    are also presented. Lastly, the Rankit, an empirical normal quantile transformation \n    (ENQT) (Soloman & Sawilowsky, 2009), is provided to accommodate non-standard \n    use cases and facilitate adoption.  \n    <DOI: 10.1201/9780203910894>.\n    <DOI: 10.22237/jmasm/1257034080>.  "
  },
  {
    "id": 6739,
    "package_name": "RmdConcord",
    "title": "Concordances for 'R Markdown'",
    "description": "Supports concordances in 'R Markdown' \n  documents.  This currently allows the original source\n  location in the '.Rmd' file of errors detected by 'HTML tidy'\n  to be found more easily, and potentially allows forward\n  and reverse search in 'HTML' and 'LaTeX' documents\n  produced from 'R Markdown'.  The 'LaTeX' support\n  has been included in the most recent development \n  version of the 'patchDVI' package.",
    "version": "0.3",
    "maintainer": "Duncan Murdoch <murdoch.duncan@gmail.com>",
    "author": "Duncan Murdoch [aut, cre],\n  Heather Turner [ctb]",
    "url": "https://github.com/dmurdoch/RmdConcord,\nhttps://dmurdoch.github.io/RmdConcord/",
    "bug_reports": "https://github.com/dmurdoch/RmdConcord/issues",
    "repository": "https://cran.r-project.org/package=RmdConcord",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RmdConcord Concordances for 'R Markdown' Supports concordances in 'R Markdown' \n  documents.  This currently allows the original source\n  location in the '.Rmd' file of errors detected by 'HTML tidy'\n  to be found more easily, and potentially allows forward\n  and reverse search in 'HTML' and 'LaTeX' documents\n  produced from 'R Markdown'.  The 'LaTeX' support\n  has been included in the most recent development \n  version of the 'patchDVI' package.  "
  },
  {
    "id": 6745,
    "package_name": "Rmoji",
    "title": "Interactively Insert Emojis in 'R' Documents",
    "description": "Provides an intuitive and user-friendly interface for working with emojis in 'R'. It allows users to search, insert, and manage emojis by keyword, category, or through an interactive 'shiny'-based drop-down. The package enables integration of emojis into 'R' scripts, 'R Markdown', 'Quarto', 'shiny' apps, and 'ggplot2' plots. Also includes built-in mappings for commit messages, useful for version control. It builds on established emoji libraries and Unicode standards, adding expressiveness and visual cues to documentation, user interfaces, and reports. For more details see 'Emojipedia' (2024) <https://emojipedia.org> and GitHub Emoji Cheat Sheet <https://github.com/ikatyang/emoji-cheat-sheet/tree/master>.",
    "version": "0.1.0",
    "maintainer": "Berhe Etsay Tesfay <berhe.etsay@gmail.com>",
    "author": "Berhe Etsay Tesfay [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4625-1265>)",
    "url": "https://github.com/3p1d3m/Rmoji",
    "bug_reports": "https://github.com/3p1d3m/Rmoji/issues",
    "repository": "https://cran.r-project.org/package=Rmoji",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rmoji Interactively Insert Emojis in 'R' Documents Provides an intuitive and user-friendly interface for working with emojis in 'R'. It allows users to search, insert, and manage emojis by keyword, category, or through an interactive 'shiny'-based drop-down. The package enables integration of emojis into 'R' scripts, 'R Markdown', 'Quarto', 'shiny' apps, and 'ggplot2' plots. Also includes built-in mappings for commit messages, useful for version control. It builds on established emoji libraries and Unicode standards, adding expressiveness and visual cues to documentation, user interfaces, and reports. For more details see 'Emojipedia' (2024) <https://emojipedia.org> and GitHub Emoji Cheat Sheet <https://github.com/ikatyang/emoji-cheat-sheet/tree/master>.  "
  },
  {
    "id": 6747,
    "package_name": "Rmonize",
    "title": "Tools for Data Harmonization",
    "description": "Integrated tools to support rigorous and well documented data \n    harmonization based on Maelstrom Research guidelines. The package includes \n    functions to assess and prepare input elements, apply specified processing \n    rules to generate harmonized datasets, validate data processing and identify \n    processing errors, and document and summarize harmonized outputs. The \n    harmonization process is defined and structured by two key user-generated \n    documents: the DataSchema (specifying the list of harmonized variables to \n    generate across datasets) and the Data Processing Elements (specifying the \n    input elements and processing algorithms to generate harmonized variables \n    in DataSchema formats). The package was developed to address key  challenges \n    of retrospective data harmonization in epidemiology (as described in \n    Fortier I and al. (2017) <doi:10.1093/ije/dyw075>) but can be used for any \n    data harmonization initiative.",
    "version": "2.0.0",
    "maintainer": "Guillaume Fabre <guijoseph.fabre@gmail.com>",
    "author": "Guillaume Fabre [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0124-9970>),\n  Maelstrom Research [aut, fnd, cph]",
    "url": "https://github.com/maelstrom-research/Rmonize/",
    "bug_reports": "https://github.com/maelstrom-research/Rmonize/issues",
    "repository": "https://cran.r-project.org/package=Rmonize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rmonize Tools for Data Harmonization Integrated tools to support rigorous and well documented data \n    harmonization based on Maelstrom Research guidelines. The package includes \n    functions to assess and prepare input elements, apply specified processing \n    rules to generate harmonized datasets, validate data processing and identify \n    processing errors, and document and summarize harmonized outputs. The \n    harmonization process is defined and structured by two key user-generated \n    documents: the DataSchema (specifying the list of harmonized variables to \n    generate across datasets) and the Data Processing Elements (specifying the \n    input elements and processing algorithms to generate harmonized variables \n    in DataSchema formats). The package was developed to address key  challenges \n    of retrospective data harmonization in epidemiology (as described in \n    Fortier I and al. (2017) <doi:10.1093/ije/dyw075>) but can be used for any \n    data harmonization initiative.  "
  },
  {
    "id": 6750,
    "package_name": "Rmst",
    "title": "Computerized Adaptive Multistage Testing",
    "description": "Assemble the panels of computerized adaptive multistage testing by the \n    bottom-up and the top-down approach, and simulate the administration of the assembled \n    panels. The full documentation and tutorials are at <https://github.com/xluo11/Rmst>.\n    Reference: Luo and Kim (2018) <doi:10.1111/jedm.12174>.",
    "version": "0.0.3",
    "maintainer": "Xiao Luo <xluo1986@gmail.com>",
    "author": "Xiao Luo [aut, cre]",
    "url": "https://github.com/xluo11/Rmst",
    "bug_reports": "https://github.com/xluo11/Rmst/issues",
    "repository": "https://cran.r-project.org/package=Rmst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rmst Computerized Adaptive Multistage Testing Assemble the panels of computerized adaptive multistage testing by the \n    bottom-up and the top-down approach, and simulate the administration of the assembled \n    panels. The full documentation and tutorials are at <https://github.com/xluo11/Rmst>.\n    Reference: Luo and Kim (2018) <doi:10.1111/jedm.12174>.  "
  },
  {
    "id": 6791,
    "package_name": "Rook",
    "title": "HTTP Web Server for R",
    "description": "An HTTP web server for R with a documented API to interface between R and the server. The documentation contains the Rook specification and details for building and running Rook applications. To get started, be sure and read the 'Rook' help file first.",
    "version": "1.2",
    "maintainer": "Evan Biederstedt <evan.biederstedt@gmail.com>",
    "author": "Jeffrey Horner [aut], Evan Biederstedt [aut, cre]",
    "url": "https://github.com/evanbiederstedt/rook",
    "bug_reports": "https://github.com/evanbiederstedt/rook/issues",
    "repository": "https://cran.r-project.org/package=Rook",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rook HTTP Web Server for R An HTTP web server for R with a documented API to interface between R and the server. The documentation contains the Rook specification and details for building and running Rook applications. To get started, be sure and read the 'Rook' help file first.  "
  },
  {
    "id": 6826,
    "package_name": "Rspotify",
    "title": "Access to Spotify API",
    "description": "Provides an interface to the Spotify API <https://developer.spotify.com/documentation/web-api/>.",
    "version": "0.1.2",
    "maintainer": "Tiago Mendes Dantas <t.mendesdantas@gmail.com>",
    "author": "Tiago Mendes Dantas",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rspotify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rspotify Access to Spotify API Provides an interface to the Spotify API <https://developer.spotify.com/documentation/web-api/>.  "
  },
  {
    "id": 6873,
    "package_name": "SAMtool",
    "title": "Stock Assessment Methods Toolkit",
    "description": "Simulation tools for closed-loop simulation are provided for the 'MSEtool' operating model to inform data-rich fisheries. \n  'SAMtool' provides a conditioning model, assessment models of varying complexity with standardized reporting, \n  model-based management procedures, and diagnostic tools for evaluating assessments inside closed-loop simulation.",
    "version": "1.9.0",
    "maintainer": "Quang Huynh <quang@bluematterscience.com>",
    "author": "Quang Huynh [aut, cre],\n  Tom Carruthers [aut],\n  Adrian Hordyk [aut]",
    "url": "https://openmse.com, https://samtool.openmse.com,\nhttps://github.com/Blue-Matter/SAMtool",
    "bug_reports": "https://github.com/Blue-Matter/SAMtool/issues",
    "repository": "https://cran.r-project.org/package=SAMtool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SAMtool Stock Assessment Methods Toolkit Simulation tools for closed-loop simulation are provided for the 'MSEtool' operating model to inform data-rich fisheries. \n  'SAMtool' provides a conditioning model, assessment models of varying complexity with standardized reporting, \n  model-based management procedures, and diagnostic tools for evaluating assessments inside closed-loop simulation.  "
  },
  {
    "id": 6883,
    "package_name": "SASmarkdown",
    "title": "'SAS' Markdown",
    "description": "Settings and functions to extend the 'knitr' 'SAS' engine.",
    "version": "0.8.7",
    "maintainer": "Doug Hemken <d_hemken@yahoo.com>",
    "author": "Doug Hemken [aut, cre] (SSCC, Univ. of Wisconsin-Madison (retired)),\n  Chao Cheng [ctb] (Statistician, Simcere Pharmaceutical Group Limited)",
    "url": "https://www.ssc.wisc.edu/~hemken/SASworkshops/sas.html#writing-sas-documentation",
    "bug_reports": "https://github.com/Hemken/SASmarkdown/issues",
    "repository": "https://cran.r-project.org/package=SASmarkdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SASmarkdown 'SAS' Markdown Settings and functions to extend the 'knitr' 'SAS' engine.  "
  },
  {
    "id": 6897,
    "package_name": "SC2API",
    "title": "Blizzard SC2 API Wrapper",
    "description": "A wrapper for Blizzard's Starcraft II (a 2010 real-time strategy game) Application Programming Interface (API). All documented API calls are implemented in an easy-to-use and consistent manner.",
    "version": "1.0.0",
    "maintainer": "Samuel Morrissette <samuel.morrissette01@gmail.com>",
    "author": "Samuel Morrissette [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SC2API",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SC2API Blizzard SC2 API Wrapper A wrapper for Blizzard's Starcraft II (a 2010 real-time strategy game) Application Programming Interface (API). All documented API calls are implemented in an easy-to-use and consistent manner.  "
  },
  {
    "id": 6941,
    "package_name": "SEAHORS",
    "title": "Spatial Exploration of ArcHaeological Objects in R Shiny",
    "description": "An R 'Shiny' application dedicated to the intra-site spatial analysis of piece-plotted archaeological remains, making the two and three-dimensional spatial exploration of archaeological data as user-friendly as possible.  Documentation about 'SEAHORS' is provided by the vignette included in this package and by the companion scientific paper: Royer, Discamps, Plutniak, Thomas (2023, PCI Archaeology, <doi:10.5281/zenodo.7674698>).",
    "version": "1.9.0",
    "maintainer": "Sebastien Plutniak <sebastien.plutniak@posteo.net>",
    "author": "Aurelien Royer [aut] (ORCID: <https://orcid.org/0000-0002-0139-8765>),\n  Sebastien Plutniak [cre] (ORCID:\n    <https://orcid.org/0000-0002-6674-3806>),\n  Emmanuel Discamps [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2464-0761>),\n  Marc Thomas [ctb] (ORCID: <https://orcid.org/0000-0002-8160-1910>)",
    "url": "https://github.com/AurelienRoyer/SEAHORS",
    "bug_reports": "https://github.com/AurelienRoyer/SEAHORS/issues",
    "repository": "https://cran.r-project.org/package=SEAHORS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SEAHORS Spatial Exploration of ArcHaeological Objects in R Shiny An R 'Shiny' application dedicated to the intra-site spatial analysis of piece-plotted archaeological remains, making the two and three-dimensional spatial exploration of archaeological data as user-friendly as possible.  Documentation about 'SEAHORS' is provided by the vignette included in this package and by the companion scientific paper: Royer, Discamps, Plutniak, Thomas (2023, PCI Archaeology, <doi:10.5281/zenodo.7674698>).  "
  },
  {
    "id": 7053,
    "package_name": "SNPfiltR",
    "title": "Interactively Filter SNP Datasets",
    "description": "Is designed to interactively and reproducibly visualize and filter SNP\n\t(single-nucleotide polymorphism) datasets. This R-based implementation of SNP\n\tand genotype filters facilitates \n\tan interactive and iterative SNP filtering pipeline, which can be documented\n\treproducibly via 'rmarkdown'. 'SNPfiltR' contains functions for visualizing \n\tvarious quality and missing data metrics for a SNP dataset, and then filtering\n\tthe dataset based on user specified cutoffs.\n\tAll functions take 'vcfR' objects as input, which can easily be\n\tgenerated by reading standard vcf (variant call format) files into R using \n\tthe R package 'vcfR' authored by Knaus and Gr\u00fcnwald (2017) <doi:10.1111/1755-0998.12549>. \n\tEach 'SNPfiltR' function can return a newly filtered 'vcfR' object, which can then be\n\twritten to a local directory in standard vcf format using the 'vcfR' package,\n\tfor downstream population genetic and phylogenetic analyses.",
    "version": "1.0.7",
    "maintainer": "Devon DeRaad <devonderaad@gmail.com>",
    "author": "Devon DeRaad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3105-985X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SNPfiltR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SNPfiltR Interactively Filter SNP Datasets Is designed to interactively and reproducibly visualize and filter SNP\n\t(single-nucleotide polymorphism) datasets. This R-based implementation of SNP\n\tand genotype filters facilitates \n\tan interactive and iterative SNP filtering pipeline, which can be documented\n\treproducibly via 'rmarkdown'. 'SNPfiltR' contains functions for visualizing \n\tvarious quality and missing data metrics for a SNP dataset, and then filtering\n\tthe dataset based on user specified cutoffs.\n\tAll functions take 'vcfR' objects as input, which can easily be\n\tgenerated by reading standard vcf (variant call format) files into R using \n\tthe R package 'vcfR' authored by Knaus and Gr\u00fcnwald (2017) <doi:10.1111/1755-0998.12549>. \n\tEach 'SNPfiltR' function can return a newly filtered 'vcfR' object, which can then be\n\twritten to a local directory in standard vcf format using the 'vcfR' package,\n\tfor downstream population genetic and phylogenetic analyses.  "
  },
  {
    "id": 7055,
    "package_name": "SNSFdatasets",
    "title": "Download Datasets from the Swiss National Science Foundation\n(SNF, FNS, SNSF)",
    "description": "Download and read datasets from the Swiss National\n     Science Foundation (SNF, FNS, SNSF; <https://snf.ch>).  The\n     package is lightweight and without dependencies.  Downloaded\n     data can optionally be cached, to avoid repeated downloads of\n     the same files.  There are also utilities for comparing\n     different versions of datasets, i.e. to report added, removed\n     and changed entries.",
    "version": "0.1.1",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Silvia Martens [ctb] (ORCID: <https://orcid.org/0009-0001-7554-3195>),\n  Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "http://enricoschumann.net/R/packages/SNSFdatasets/ ,\nhttps://git.sr.ht/~enricoschumann/SNSFdatasets ,\nhttps://github.com/enricoschumann/SNSFdatasets",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SNSFdatasets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SNSFdatasets Download Datasets from the Swiss National Science Foundation\n(SNF, FNS, SNSF) Download and read datasets from the Swiss National\n     Science Foundation (SNF, FNS, SNSF; <https://snf.ch>).  The\n     package is lightweight and without dependencies.  Downloaded\n     data can optionally be cached, to avoid repeated downloads of\n     the same files.  There are also utilities for comparing\n     different versions of datasets, i.e. to report added, removed\n     and changed entries.  "
  },
  {
    "id": 7061,
    "package_name": "SOAs",
    "title": "Creation of Stratum Orthogonal Arrays",
    "description": "Creates stratum orthogonal arrays (also known as strong orthogonal arrays). These are arrays with more levels per column than the typical orthogonal array, and whose low order projections behave like orthogonal arrays, when collapsing levels to coarser strata. Details are described in Groemping (2022) \"A unifying implementation of stratum (aka strong) orthogonal arrays\" <http://www1.bht-berlin.de/FB_II/reports/Report-2022-002.pdf>.",
    "version": "1.4-1",
    "maintainer": "Ulrike Groemping <ulrike.groemping@bht-berlin.de>",
    "author": "Ulrike Groemping [aut, cre],\n  Rob Carnell [ctb],\n  Hongquan Xu [ctb]",
    "url": "https://github.com/bertcarnell/SOAs",
    "bug_reports": "https://github.com/bertcarnell/SOAs/issues",
    "repository": "https://cran.r-project.org/package=SOAs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SOAs Creation of Stratum Orthogonal Arrays Creates stratum orthogonal arrays (also known as strong orthogonal arrays). These are arrays with more levels per column than the typical orthogonal array, and whose low order projections behave like orthogonal arrays, when collapsing levels to coarser strata. Details are described in Groemping (2022) \"A unifying implementation of stratum (aka strong) orthogonal arrays\" <http://www1.bht-berlin.de/FB_II/reports/Report-2022-002.pdf>.  "
  },
  {
    "id": 7080,
    "package_name": "SPB",
    "title": "Simple Progress Bars for Procedural Coding",
    "description": "Provides a simple progress bar to use for basic and advanced users that suits all those who prefer procedural programming. It is especially useful for integration into markdown files thanks to the progress bar's customisable appearance.",
    "version": "1.0",
    "maintainer": "Fabio Ashtar Telarico <Fabio-Ashtar.Telarico@fdv.uni-lj.si>",
    "author": "Fabio Ashtar Telarico [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8740-7078>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPB Simple Progress Bars for Procedural Coding Provides a simple progress bar to use for basic and advanced users that suits all those who prefer procedural programming. It is especially useful for integration into markdown files thanks to the progress bar's customisable appearance.  "
  },
  {
    "id": 7094,
    "package_name": "SPORTSCausal",
    "title": "Spillover Time Series Causal Inference",
    "description": "A time series causal inference model for Randomized Controlled Trial (RCT) under spillover effect. 'SPORTSCausal' (Spillover Time Series Causal Inference) separates treatment effect and spillover effect from given responses of experiment group and control group by predicting the response without treatment. It reports both effects by fitting the Bayesian Structural Time Series (BSTS) model based on 'CausalImpact', as described in Brodersen et al. (2015) <doi:10.1214/14-AOAS788>. ",
    "version": "1.0",
    "maintainer": "Feiyu Yue <yuefyopals@gmail.com>",
    "author": "Zihao Zheng and Feiyu Yue",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPORTSCausal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPORTSCausal Spillover Time Series Causal Inference A time series causal inference model for Randomized Controlled Trial (RCT) under spillover effect. 'SPORTSCausal' (Spillover Time Series Causal Inference) separates treatment effect and spillover effect from given responses of experiment group and control group by predicting the response without treatment. It reports both effects by fitting the Bayesian Structural Time Series (BSTS) model based on 'CausalImpact', as described in Brodersen et al. (2015) <doi:10.1214/14-AOAS788>.   "
  },
  {
    "id": 7143,
    "package_name": "STAT",
    "title": "Interactive Document for Working with Basic Statistical Analysis",
    "description": "An interactive document on  the topic of basic statistical analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://jarvisatharva.shinyapps.io/StatisticsPrimer/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=STAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "STAT Interactive Document for Working with Basic Statistical Analysis An interactive document on  the topic of basic statistical analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://jarvisatharva.shinyapps.io/StatisticsPrimer/>.  "
  },
  {
    "id": 7144,
    "package_name": "STAT2",
    "title": "Interactive Document for Working with Basic Statistical Analysis",
    "description": "An interactive document on  the topic of basic statistical analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://jarvisatharva.shinyapps.io/StatisticsPrimer/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=STAT2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "STAT2 Interactive Document for Working with Basic Statistical Analysis An interactive document on  the topic of basic statistical analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://jarvisatharva.shinyapps.io/StatisticsPrimer/>.  "
  },
  {
    "id": 7145,
    "package_name": "STATcubeR",
    "title": "R Interface for the 'STATcube' REST API and Open Government Data",
    "description": "Import data from the 'STATcube' REST API or from the open data\n    portal of Statistics Austria. This package includes a client for API\n    requests as well as parsing utilities for data which originates from\n    'STATcube'. Documentation about 'STATcubeR' is provided by several vignettes \n    included in the package as well as on the public 'pkgdown' page at \n    <https://statistikat.github.io/STATcubeR/>.",
    "version": "1.0.0",
    "maintainer": "Bernhard Meindl <Bernhard.Meindl@statistik.gv.at>",
    "author": "Bernhard Meindl [ctb, cre],\n  Alexander Kowarik [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8598-4130>),\n  Gregor de Cillia [aut]",
    "url": "https://statistikat.github.io/STATcubeR/,\nhttps://github.com/statistikat/STATcubeR",
    "bug_reports": "https://github.com/statistikat/STATcubeR/issues",
    "repository": "https://cran.r-project.org/package=STATcubeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "STATcubeR R Interface for the 'STATcube' REST API and Open Government Data Import data from the 'STATcube' REST API or from the open data\n    portal of Statistics Austria. This package includes a client for API\n    requests as well as parsing utilities for data which originates from\n    'STATcube'. Documentation about 'STATcubeR' is provided by several vignettes \n    included in the package as well as on the public 'pkgdown' page at \n    <https://statistikat.github.io/STATcubeR/>.  "
  },
  {
    "id": 7160,
    "package_name": "SUMMER",
    "title": "Small-Area-Estimation Unit/Area Models and Methods for\nEstimation in R",
    "description": "Provides methods for spatial and spatio-temporal smoothing of demographic and health indicators using survey data, with particular focus on estimating and projecting under-five mortality rates, described in Mercer et al. (2015) <doi:10.1214/15-AOAS872>, Li et al. (2019) <doi:10.1371/journal.pone.0210645>, Wu et al. (DHS Spatial Analysis Reports No. 21, 2021), and Li et al. (2023) <doi:10.48550/arXiv.2007.05117>. ",
    "version": "2.0.0",
    "maintainer": "Zehang R Li <lizehang@gmail.com>",
    "author": "Zehang R Li [cre, aut],\n  Bryan D Martin [aut],\n  Yuan Hsiao [aut],\n  Jessica Godwin [aut],\n  John Paige [aut],\n  Peter Gao [aut],\n  Jon Wakefield [aut],\n  Samuel J Clark [aut],\n  Geir-Arne Fuglstad [aut],\n  Andrea Riebler [aut]",
    "url": "https://github.com/richardli/SUMMER,\nhttps://richardli.github.io/SUMMER/",
    "bug_reports": "https://github.com/richardli/SUMMER/issues",
    "repository": "https://cran.r-project.org/package=SUMMER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SUMMER Small-Area-Estimation Unit/Area Models and Methods for\nEstimation in R Provides methods for spatial and spatio-temporal smoothing of demographic and health indicators using survey data, with particular focus on estimating and projecting under-five mortality rates, described in Mercer et al. (2015) <doi:10.1214/15-AOAS872>, Li et al. (2019) <doi:10.1371/journal.pone.0210645>, Wu et al. (DHS Spatial Analysis Reports No. 21, 2021), and Li et al. (2023) <doi:10.48550/arXiv.2007.05117>.   "
  },
  {
    "id": 7199,
    "package_name": "SeaSondeR",
    "title": "Radial Metrics from SeaSonde HF-Radar Data",
    "description": "Read CODAR's SeaSonde High-Frequency Radar spectra files, compute radial metrics, and generate plots for spectra and antenna pattern data. Implementation is based in technical manuals, publications and patents, please refer to the following documents for more information: Barrick and Lipa (1999) <https://codar.com/images/about/patents/05990834.PDF>; CODAR Ocean Sensors (2002) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Docs/Informative/FirstOrder_Settings.pdf>; Lipa et al. (2006) <doi:10.1109/joe.2006.886104>; Paolo et al. (2007) <doi:10.1109/oceans.2007.4449265>; CODAR Ocean Sensors (2009a) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Docs/GuidesToFileFormats/File_AntennaPattern.pdf>; CODAR Ocean Sensors (2009b) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Docs/GuidesToFileFormats/File_CrossSpectraReduced.pdf>; CODAR Ocean Sensors (2016a) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Manuals_Documentation_Release_8/File_Formats/File_Cross_Spectra_V6.pdf>; CODAR Ocean Sensors (2016b) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Manuals_Documentation_Release_8/File_Formats/FIle_Reduced_Spectra.pdf>; CODAR Ocean Sensors (2016c) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Manuals_Documentation_Release_8/Application_Guides/Guide_SpectraPlotterMap.pdf>; Bushnell and Worthington (2022) <doi:10.25923/4c5x-g538>.",
    "version": "0.2.8",
    "maintainer": "Juan Luis Herrera Cortijo <juan.luis.herrera.cortijo@gmail.com>",
    "author": "Juan Luis Herrera Cortijo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4206-2459>),\n  Ramiro A. Varela Benvenuto [aut] (ORCID:\n    <https://orcid.org/0000-0002-9212-577X>),\n  Adri\u00e1n Fern\u00e1ndez Baladr\u00f3n [aut] (ORCID:\n    <https://orcid.org/0000-0001-6795-4261>)",
    "url": "https://github.com/GOFUVI/SeaSondeR,\nhttps://gofuvi.github.io/SeaSondeR/",
    "bug_reports": "https://github.com/GOFUVI/SeaSondeR/issues",
    "repository": "https://cran.r-project.org/package=SeaSondeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SeaSondeR Radial Metrics from SeaSonde HF-Radar Data Read CODAR's SeaSonde High-Frequency Radar spectra files, compute radial metrics, and generate plots for spectra and antenna pattern data. Implementation is based in technical manuals, publications and patents, please refer to the following documents for more information: Barrick and Lipa (1999) <https://codar.com/images/about/patents/05990834.PDF>; CODAR Ocean Sensors (2002) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Docs/Informative/FirstOrder_Settings.pdf>; Lipa et al. (2006) <doi:10.1109/joe.2006.886104>; Paolo et al. (2007) <doi:10.1109/oceans.2007.4449265>; CODAR Ocean Sensors (2009a) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Docs/GuidesToFileFormats/File_AntennaPattern.pdf>; CODAR Ocean Sensors (2009b) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Docs/GuidesToFileFormats/File_CrossSpectraReduced.pdf>; CODAR Ocean Sensors (2016a) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Manuals_Documentation_Release_8/File_Formats/File_Cross_Spectra_V6.pdf>; CODAR Ocean Sensors (2016b) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Manuals_Documentation_Release_8/File_Formats/FIle_Reduced_Spectra.pdf>; CODAR Ocean Sensors (2016c) <http://support.codar.com/Technicians_Information_Page_for_SeaSondes/Manuals_Documentation_Release_8/Application_Guides/Guide_SpectraPlotterMap.pdf>; Bushnell and Worthington (2022) <doi:10.25923/4c5x-g538>.  "
  },
  {
    "id": 7226,
    "package_name": "SenSrivastava",
    "title": "Datasets from Sen & Srivastava",
    "description": "Collection of datasets from Sen & Srivastava: \"Regression\n        Analysis, Theory, Methods and Applications\", Springer.  Sources\n        for individual data files are more fully documented in the\n        book.",
    "version": "2015.6.25.1",
    "maintainer": "Kjetil B Halvorsen <kjetil1001@gmail.com>",
    "author": "Kjetil B Halvorsen <kjetil1001@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SenSrivastava",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SenSrivastava Datasets from Sen & Srivastava Collection of datasets from Sen & Srivastava: \"Regression\n        Analysis, Theory, Methods and Applications\", Springer.  Sources\n        for individual data files are more fully documented in the\n        book.  "
  },
  {
    "id": 7237,
    "package_name": "SeqKat",
    "title": "Detection of Kataegis",
    "description": "Kataegis is a localized hypermutation occurring when a region is enriched in somatic SNVs. Kataegis can result from multiple cytosine deaminations catalyzed by the AID/APOBEC family of proteins. This package contains functions to detect kataegis from SNVs in BED format. This package reports two scores per kataegic event, a hypermutation score and an APOBEC mediated kataegic score. Yousif, F. et al.; The Origins and Consequences of Localized and Global Somatic Hypermutation; Biorxiv 2018 <doi:10.1101/287839>.",
    "version": "0.0.8",
    "maintainer": "Paul C. Boutros <pboutros@mednet.ucla.edu>",
    "author": "Fouad Yousif, Xihui Lin, Fan Fan, Christopher Lalansingh, John Macdonald",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SeqKat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SeqKat Detection of Kataegis Kataegis is a localized hypermutation occurring when a region is enriched in somatic SNVs. Kataegis can result from multiple cytosine deaminations catalyzed by the AID/APOBEC family of proteins. This package contains functions to detect kataegis from SNVs in BED format. This package reports two scores per kataegic event, a hypermutation score and an APOBEC mediated kataegic score. Yousif, F. et al.; The Origins and Consequences of Localized and Global Somatic Hypermutation; Biorxiv 2018 <doi:10.1101/287839>.  "
  },
  {
    "id": 7248,
    "package_name": "ShapePattern",
    "title": "Tools for Analyzing Shapes and Patterns",
    "description": "This is an evolving and growing collection of tools for the quantification, assessment, and comparison of shape and pattern. This collection provides tools for: (1) the spatial decomposition of planar shapes using 'ShrinkShape' to incrementally shrink shapes to extinction while computing area, perimeter, and number of parts at each iteration of shrinking; the spectra of results are returned in graphic and tabular formats (Remmel 2015) <doi:10.1111/cag.12222>, (2) simulating landscape patterns, (3) provision of tools for estimating composition and configuration parameters from a categorical (binary) landscape map (grid) and then simulates a selected number of statistically similar landscapes. Class-focused pattern metrics are computed for each simulated map to produce empirical distributions against which statistical comparisons can be made. The code permits the analysis of single maps or pairs of maps (Remmel and Fortin 2013) <doi:10.1007/s10980-013-9905-x>, (4) counting the number of each first-order pattern element and converting that information into both frequency and empirical probability vectors (Remmel 2020) <doi:10.3390/e22040420>, and (5) computing the porosity of raster patches <doi:10.3390/su10103413>. NOTE: This is a consolidation of existing packages ('PatternClass', 'ShapePattern') to begin warehousing all shape and pattern code in a common package. Additional utility tools for handling data are provided and this package will be added to as more tools are created, cleaned-up, and documented.  Note that all future developments will appear in this package and that 'PatternClass' will eventually be archived.",
    "version": "3.1.0",
    "maintainer": "Tarmo K. Remmel <remmelt@yorku.ca>",
    "author": "Tarmo K. Remmel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6251-876X>),\n  Marie-Josee Fortin [ctb],\n  Ferenc Csillag [ctb],\n  Sandor Kabos [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ShapePattern",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShapePattern Tools for Analyzing Shapes and Patterns This is an evolving and growing collection of tools for the quantification, assessment, and comparison of shape and pattern. This collection provides tools for: (1) the spatial decomposition of planar shapes using 'ShrinkShape' to incrementally shrink shapes to extinction while computing area, perimeter, and number of parts at each iteration of shrinking; the spectra of results are returned in graphic and tabular formats (Remmel 2015) <doi:10.1111/cag.12222>, (2) simulating landscape patterns, (3) provision of tools for estimating composition and configuration parameters from a categorical (binary) landscape map (grid) and then simulates a selected number of statistically similar landscapes. Class-focused pattern metrics are computed for each simulated map to produce empirical distributions against which statistical comparisons can be made. The code permits the analysis of single maps or pairs of maps (Remmel and Fortin 2013) <doi:10.1007/s10980-013-9905-x>, (4) counting the number of each first-order pattern element and converting that information into both frequency and empirical probability vectors (Remmel 2020) <doi:10.3390/e22040420>, and (5) computing the porosity of raster patches <doi:10.3390/su10103413>. NOTE: This is a consolidation of existing packages ('PatternClass', 'ShapePattern') to begin warehousing all shape and pattern code in a common package. Additional utility tools for handling data are provided and this package will be added to as more tools are created, cleaned-up, and documented.  Note that all future developments will appear in this package and that 'PatternClass' will eventually be archived.  "
  },
  {
    "id": 7249,
    "package_name": "ShapeSelectForest",
    "title": "Shape Selection for Landsat Time Series of Forest Dynamics",
    "description": "Landsat satellites collect important data about global forest conditions. Documentation about Landsat's role in forest disturbance estimation is available at the site <https://landsat.gsfc.nasa.gov/>. By constrained quadratic B-splines, this package delivers an optimal shape-restricted trajectory to a time series of Landsat imagery for the purpose of modeling annual forest disturbance dynamics to behave in an ecologically sensible manner assuming one of seven possible \"shapes\", namely, flat, decreasing, one-jump (decreasing, jump up, decreasing), inverted vee (increasing then decreasing), vee (decreasing then increasing), linear increasing, and double-jump (decreasing, jump up, decreasing, jump up, decreasing). The main routine selects the best shape according to the minimum Bayes information criterion (BIC) or the cone information criterion (CIC), which is defined as the log of the estimated predictive squared error. The package also provides parameters summarizing the temporal pattern including year(s) of inflection, magnitude of change, pre- and post-inflection rates of growth or recovery. In addition, it contains routines for converting a flat map of disturbance agents to time-series disturbance maps and a graphical routine displaying the fitted trajectory of Landsat imagery. ",
    "version": "1.7",
    "maintainer": "Xiyue Liao <xliao@sdsu.edu>",
    "author": "Xiyue Liao [aut, cre] (ORCID: <https://orcid.org/0000-0002-4508-9219>),\n  Mary Meyer [aut],\n  Elizabeth Freeman [aut],\n  Gretchen Moisen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ShapeSelectForest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ShapeSelectForest Shape Selection for Landsat Time Series of Forest Dynamics Landsat satellites collect important data about global forest conditions. Documentation about Landsat's role in forest disturbance estimation is available at the site <https://landsat.gsfc.nasa.gov/>. By constrained quadratic B-splines, this package delivers an optimal shape-restricted trajectory to a time series of Landsat imagery for the purpose of modeling annual forest disturbance dynamics to behave in an ecologically sensible manner assuming one of seven possible \"shapes\", namely, flat, decreasing, one-jump (decreasing, jump up, decreasing), inverted vee (increasing then decreasing), vee (decreasing then increasing), linear increasing, and double-jump (decreasing, jump up, decreasing, jump up, decreasing). The main routine selects the best shape according to the minimum Bayes information criterion (BIC) or the cone information criterion (CIC), which is defined as the log of the estimated predictive squared error. The package also provides parameters summarizing the temporal pattern including year(s) of inflection, magnitude of change, pre- and post-inflection rates of growth or recovery. In addition, it contains routines for converting a flat map of disturbance agents to time-series disturbance maps and a graphical routine displaying the fitted trajectory of Landsat imagery.   "
  },
  {
    "id": 7290,
    "package_name": "SimEngine",
    "title": "A Modular Framework for Statistical Simulations in R",
    "description": "An open-source R package for structuring, maintaining, running, and debugging statistical simulations on both local and cluster-based computing environments.See full documentation at <https://avi-kenny.github.io/SimEngine/>.",
    "version": "1.4.0",
    "maintainer": "Avi Kenny <avi.kenny@gmail.com>",
    "author": "Avi Kenny [aut, cre],\n  Charles Wolock [aut]",
    "url": "https://avi-kenny.github.io/SimEngine/",
    "bug_reports": "https://github.com/Avi-Kenny/SimEngine/issues",
    "repository": "https://cran.r-project.org/package=SimEngine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimEngine A Modular Framework for Statistical Simulations in R An open-source R package for structuring, maintaining, running, and debugging statistical simulations on both local and cluster-based computing environments.See full documentation at <https://avi-kenny.github.io/SimEngine/>.  "
  },
  {
    "id": 7295,
    "package_name": "SimKid",
    "title": "Simulate Virtual Pediatrics using Anthropometric Growth Charts",
    "description": "Simulate a virtual population of subjects that has demographic distributions (height, weight, and BMI) and correlations (height and weight), by sex and age, which mimic those reported in real-world anthropometric growth charts (CDC, WHO, or Fenton).",
    "version": "1.0.0",
    "maintainer": "Andrew Santulli <asantulli@epd-llc.com>",
    "author": "Andrew Santulli [aut, cre],\n  Enhanced Pharmacodynamics LLC [cph, fnd]",
    "url": "https://github.com/Andy00000000000/SimKid",
    "bug_reports": "https://github.com/Andy00000000000/SimKid/issues",
    "repository": "https://cran.r-project.org/package=SimKid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimKid Simulate Virtual Pediatrics using Anthropometric Growth Charts Simulate a virtual population of subjects that has demographic distributions (height, weight, and BMI) and correlations (height and weight), by sex and age, which mimic those reported in real-world anthropometric growth charts (CDC, WHO, or Fenton).  "
  },
  {
    "id": 7304,
    "package_name": "SimTOST",
    "title": "Sample Size Estimation for Bio-Equivalence Trials Through\nSimulation",
    "description": "\n    Sample size estimation for bio-equivalence trials is supported through a simulation-based approach \n    that extends the Two One-Sided Tests (TOST) procedure. The methodology provides flexibility in \n    hypothesis testing, accommodates multiple treatment comparisons, and accounts for correlated endpoints. \n    Users can model complex trial scenarios, including parallel and crossover designs, intra-subject variability, \n    and different equivalence margins. Monte Carlo simulations enable accurate estimation of power and type I error \n    rates, ensuring well-calibrated study designs. The statistical framework builds on established methods for \n    equivalence testing and multiple hypothesis testing in bio-equivalence studies, as described in Schuirmann (1987) \n    <doi:10.1007/BF01068419>, Mielke et al. (2018) <doi:10.1080/19466315.2017.1371071>, Shieh (2022) \n    <doi:10.1371/journal.pone.0269128>, and Sozu et al. (2015) <doi:10.1007/978-3-319-22005-5>. \n    Comprehensive documentation and vignettes guide users through implementation and interpretation of results.",
    "version": "1.0.2",
    "maintainer": "Thomas Debray <tdebray@fromdatatowisdom.com>",
    "author": "Thomas Debray [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1790-2719>),\n  Johanna Munoz [aut],\n  Dewi Amaliah [ctb],\n  Wei Wei [ctb],\n  Marian Mitroiu [ctb],\n  Scott McDonald [ctb],\n  Biogen Inc [cph, fnd]",
    "url": "https://smartdata-analysis-and-statistics.github.io/SimTOST/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimTOST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimTOST Sample Size Estimation for Bio-Equivalence Trials Through\nSimulation \n    Sample size estimation for bio-equivalence trials is supported through a simulation-based approach \n    that extends the Two One-Sided Tests (TOST) procedure. The methodology provides flexibility in \n    hypothesis testing, accommodates multiple treatment comparisons, and accounts for correlated endpoints. \n    Users can model complex trial scenarios, including parallel and crossover designs, intra-subject variability, \n    and different equivalence margins. Monte Carlo simulations enable accurate estimation of power and type I error \n    rates, ensuring well-calibrated study designs. The statistical framework builds on established methods for \n    equivalence testing and multiple hypothesis testing in bio-equivalence studies, as described in Schuirmann (1987) \n    <doi:10.1007/BF01068419>, Mielke et al. (2018) <doi:10.1080/19466315.2017.1371071>, Shieh (2022) \n    <doi:10.1371/journal.pone.0269128>, and Sozu et al. (2015) <doi:10.1007/978-3-319-22005-5>. \n    Comprehensive documentation and vignettes guide users through implementation and interpretation of results.  "
  },
  {
    "id": 7322,
    "package_name": "SlideCNA",
    "title": "Calls Copy Number Alterations from Slide-Seq Data",
    "description": "This takes spatial single-cell-type RNA-seq data (specifically designed for Slide-seq v2) that calls copy number alterations (CNAs) using pseudo-spatial binning, clusters cellular units (e.g. beads) based on CNA profile, and visualizes spatial CNA patterns. Documentation about 'SlideCNA' is included in the the pre-print by Zhang et al. (2022, <doi:10.1101/2022.11.25.517982>). The package 'enrichR' (>= 3.0), conditionally used to annotate SlideCNA-determined clusters with gene ontology terms, can be installed at <https://github.com/wjawaid/enrichR> or with install_github(\"wjawaid/enrichR\").",
    "version": "0.1.0",
    "maintainer": "Diane Zhang <dkzhang711@gmail.com>",
    "author": "Diane Zhang [aut, cre] (ORCID: <https://orcid.org/0000-0002-0268-4413>),\n  Johanna Klughammer [aut] (ORCID:\n    <https://orcid.org/0000-0002-3628-9278>),\n  Jan Watter [aut] (ORCID: <https://orcid.org/0000-0001-7519-731X>),\n  Broad Institute of MIT and Harvard [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SlideCNA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SlideCNA Calls Copy Number Alterations from Slide-Seq Data This takes spatial single-cell-type RNA-seq data (specifically designed for Slide-seq v2) that calls copy number alterations (CNAs) using pseudo-spatial binning, clusters cellular units (e.g. beads) based on CNA profile, and visualizes spatial CNA patterns. Documentation about 'SlideCNA' is included in the the pre-print by Zhang et al. (2022, <doi:10.1101/2022.11.25.517982>). The package 'enrichR' (>= 3.0), conditionally used to annotate SlideCNA-determined clusters with gene ontology terms, can be installed at <https://github.com/wjawaid/enrichR> or with install_github(\"wjawaid/enrichR\").  "
  },
  {
    "id": 7349,
    "package_name": "SoilTaxonomy",
    "title": "A System of Soil Classification for Making and Interpreting Soil\nSurveys",
    "description": "Taxonomic dictionaries, formative element lists, and functions related to the maintenance, development and application of U.S. Soil Taxonomy. \n   Data and functionality are based on official U.S. Department of Agriculture sources including the latest edition of the Keys to Soil Taxonomy. Descriptions and metadata are obtained from the National Soil Information System or Soil Survey Geographic databases. Other sources are referenced in the data documentation. \n   Provides tools for understanding and interacting with concepts in the U.S. Soil Taxonomic System. Most of the current utilities are for working with taxonomic concepts at the \"higher\" taxonomic levels: Order, Suborder, Great Group, and Subgroup.",
    "version": "0.2.8",
    "maintainer": "Andrew Brown <andrew.g.brown@usda.gov>",
    "author": "Andrew Brown [aut, cre],\n  Dylan Beaudette [aut]",
    "url": "https://github.com/ncss-tech/SoilTaxonomy,\nhttps://ncss-tech.github.io/SoilTaxonomy/",
    "bug_reports": "https://github.com/ncss-tech/SoilTaxonomy/issues",
    "repository": "https://cran.r-project.org/package=SoilTaxonomy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SoilTaxonomy A System of Soil Classification for Making and Interpreting Soil\nSurveys Taxonomic dictionaries, formative element lists, and functions related to the maintenance, development and application of U.S. Soil Taxonomy. \n   Data and functionality are based on official U.S. Department of Agriculture sources including the latest edition of the Keys to Soil Taxonomy. Descriptions and metadata are obtained from the National Soil Information System or Soil Survey Geographic databases. Other sources are referenced in the data documentation. \n   Provides tools for understanding and interacting with concepts in the U.S. Soil Taxonomic System. Most of the current utilities are for working with taxonomic concepts at the \"higher\" taxonomic levels: Order, Suborder, Great Group, and Subgroup.  "
  },
  {
    "id": 7387,
    "package_name": "SpatFD",
    "title": "Functional Geostatistics: Univariate and Multivariate Functional\nSpatial Prediction",
    "description": "Performance of functional kriging, cokriging, optimal sampling and simulation for spatial prediction of functional data. The framework of spatial prediction, optimal sampling and simulation are extended from scalar to functional data. 'SpatFD' is based on the Karhunen-Lo\u00e8ve expansion that allows to represent the observed functions in terms of its empirical functional principal components. Based on this approach, the functional auto-covariances and cross-covariances required for  spatial functional predictions and optimal sampling, are completely determined by the sum of the spatial auto-covariances and cross-covariances of the respective score components. The package provides new classes of data and functions for modeling spatial dependence structure among curves. The spatial prediction of curves at unsampled locations can be carried out using two types of predictors, and both of them report, the respective variances of the prediction error. In addition, there is a function for the determination of spatial locations sampling configuration that ensures minimum variance of spatial functional prediction. There are also two functions for plotting predicted curves at each location and mapping the surface at each time point, respectively. References Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s10260-015-0340-9>, Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s00477-016-1266-y>, Bohorquez M., Giraldo R. and Mateu J. (2021) <doi:10.1002/9781119387916>.",
    "version": "0.0.1",
    "maintainer": "Martha Patricia Bohorquez Casta\u00f1eda <mpbohorquezc@unal.edu.co>",
    "author": "Martha Patricia Bohorquez Casta\u00f1eda [aut, cre],\n  Diego Alejandro Sandoval Skinner [aut],\n  Angie Villamil [aut],\n  Samuel Hernando Sanchez Gutierrez [aut],\n  Nathaly Vergel Serrano [ctb],\n  Miguel Angel Munoz Layton [ctb],\n  Valeria Bejarano Salcedo [ctb],\n  Venus Celeste Puertas [ctb],\n  Ruben Dario Guevara Gonzalez [aut],\n  Joan Nicolas Castro Cortes [ctb],\n  Ramon Giraldo Henao [aut],\n  Jorge Mateu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatFD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatFD Functional Geostatistics: Univariate and Multivariate Functional\nSpatial Prediction Performance of functional kriging, cokriging, optimal sampling and simulation for spatial prediction of functional data. The framework of spatial prediction, optimal sampling and simulation are extended from scalar to functional data. 'SpatFD' is based on the Karhunen-Lo\u00e8ve expansion that allows to represent the observed functions in terms of its empirical functional principal components. Based on this approach, the functional auto-covariances and cross-covariances required for  spatial functional predictions and optimal sampling, are completely determined by the sum of the spatial auto-covariances and cross-covariances of the respective score components. The package provides new classes of data and functions for modeling spatial dependence structure among curves. The spatial prediction of curves at unsampled locations can be carried out using two types of predictors, and both of them report, the respective variances of the prediction error. In addition, there is a function for the determination of spatial locations sampling configuration that ensures minimum variance of spatial functional prediction. There are also two functions for plotting predicted curves at each location and mapping the surface at each time point, respectively. References Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s10260-015-0340-9>, Bohorquez, M., Giraldo, R., and Mateu, J. (2016) <doi:10.1007/s00477-016-1266-y>, Bohorquez M., Giraldo R. and Mateu J. (2021) <doi:10.1002/9781119387916>.  "
  },
  {
    "id": 7396,
    "package_name": "SpatialGraph",
    "title": "The SpatialGraph Class and Utilities",
    "description": "Provision of the S4 SpatialGraph class built on top of objects provided by 'igraph' and 'sp' packages, and associated utilities. See the documentation of the SpatialGraph-class within this package for further description. An example of how from a few points one can arrive to a SpatialGraph is provided in the function sl2sg().  ",
    "version": "1.0-4",
    "maintainer": "Javier Garcia-Pintado <jgarciapintado@marum.de>",
    "author": "Javier Garcia-Pintado",
    "url": "https://github.com/garciapintado/SpatialGraph",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatialGraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatialGraph The SpatialGraph Class and Utilities Provision of the S4 SpatialGraph class built on top of objects provided by 'igraph' and 'sp' packages, and associated utilities. See the documentation of the SpatialGraph-class within this package for further description. An example of how from a few points one can arrive to a SpatialGraph is provided in the function sl2sg().    "
  },
  {
    "id": 7458,
    "package_name": "Statamarkdown",
    "title": "'Stata' Markdown",
    "description": "Settings and functions to extend the 'knitr' 'Stata' engine.",
    "version": "0.9.6",
    "maintainer": "Doug Hemken <d_hemken@yahoo.com>",
    "author": "Doug Hemken [aut, cre] (SSCC, Univ. of Wisconsin-Madison (retired)),\n  Tom Palmer [ctb] (MacOS, linux),\n  Philipp Lepert [ctb]",
    "url": "",
    "bug_reports": "https://github.com/Hemken/Statamarkdown/issues",
    "repository": "https://cran.r-project.org/package=Statamarkdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Statamarkdown 'Stata' Markdown Settings and functions to extend the 'knitr' 'Stata' engine.  "
  },
  {
    "id": 7550,
    "package_name": "SwimmeR",
    "title": "Data Import, Cleaning, and Conversions for Swimming Results",
    "description": "The goal of the 'SwimmeR' package is to provide means of acquiring, and then analyzing, data from swimming (and diving) competitions.  To that end 'SwimmeR' allows results to be read in from .html sources, like 'Hy-Tek' real time results pages, '.pdf' files, 'ISL' results, 'Omega' results, and (on a development basis) '.hy3' files.  Once read in, 'SwimmeR' can convert swimming times (performances) between the computationally useful format of seconds reported to the '100ths' place (e.g. 95.37), and the conventional reporting format (1:35.37) used in the swimming community.  'SwimmeR' can also score meets in a variety of formats with user defined point values, convert times between courses ('LCM', 'SCM', 'SCY') and draw single elimination brackets, as well as providing a suite of tools for working cleaning swimming data.  This is a developmental package, not yet mature.",
    "version": "0.14.2",
    "maintainer": "Greg Pilgrim <gpilgrim2670@gmail.com>",
    "author": "Greg Pilgrim [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7831-442X>),\n  Caitlin Baldwin [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SwimmeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SwimmeR Data Import, Cleaning, and Conversions for Swimming Results The goal of the 'SwimmeR' package is to provide means of acquiring, and then analyzing, data from swimming (and diving) competitions.  To that end 'SwimmeR' allows results to be read in from .html sources, like 'Hy-Tek' real time results pages, '.pdf' files, 'ISL' results, 'Omega' results, and (on a development basis) '.hy3' files.  Once read in, 'SwimmeR' can convert swimming times (performances) between the computationally useful format of seconds reported to the '100ths' place (e.g. 95.37), and the conventional reporting format (1:35.37) used in the swimming community.  'SwimmeR' can also score meets in a variety of formats with user defined point values, convert times between courses ('LCM', 'SCM', 'SCY') and draw single elimination brackets, as well as providing a suite of tools for working cleaning swimming data.  This is a developmental package, not yet mature.  "
  },
  {
    "id": 7571,
    "package_name": "TAF",
    "title": "Transparent Assessment Framework for Reproducible Research",
    "description": "General framework to organize data, methods, and results used in\n  reproducible scientific analyses. A TAF analysis consists of four scripts\n  (data.R, model.R, output.R, report.R) that are run sequentially. Each script\n  starts by reading files from a previous step and ends with writing out files\n  for the next step. Convenience functions are provided to version control the\n  required data and software, run analyses, clean residues from previous runs,\n  manage files, manipulate tables, and produce figures. With a focus on\n  stability and reproducible analyses, the TAF package comes with no\n  dependencies. TAF forms a base layer for the 'icesTAF' package and other\n  scientific applications.",
    "version": "4.3.1",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Arni Magnusson [aut, cre],\n  Colin Millar [aut],\n  Iago Mosqueira [aut],\n  Alexandros Kokkalis [ctb],\n  Ibrahim Umar [ctb],\n  Hjalte Parner [ctb]",
    "url": "https://github.com/ices-tools-prod/TAF",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TAF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TAF Transparent Assessment Framework for Reproducible Research General framework to organize data, methods, and results used in\n  reproducible scientific analyses. A TAF analysis consists of four scripts\n  (data.R, model.R, output.R, report.R) that are run sequentially. Each script\n  starts by reading files from a previous step and ends with writing out files\n  for the next step. Convenience functions are provided to version control the\n  required data and software, run analyses, clean residues from previous runs,\n  manage files, manipulate tables, and produce figures. With a focus on\n  stability and reproducible analyses, the TAF package comes with no\n  dependencies. TAF forms a base layer for the 'icesTAF' package and other\n  scientific applications.  "
  },
  {
    "id": 7582,
    "package_name": "TBox",
    "title": "Useful Functions for Programming and Generating Documents",
    "description": "Tools to help developers and producers manipulate R objects\n    and outputs. It includes tools for displaying results and objects, and\n    for formatting them in the correct format.",
    "version": "0.2.2",
    "maintainer": "Tanguy Barthelemy <tanguy.barthelemy@insee.fr>",
    "author": "Tanguy Barthelemy [aut, cre]",
    "url": "https://github.com/TanguyBarthelemy/TBox,\nhttps://tanguybarthelemy.github.io/TBox/",
    "bug_reports": "https://github.com/TanguyBarthelemy/TBox/issues",
    "repository": "https://cran.r-project.org/package=TBox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TBox Useful Functions for Programming and Generating Documents Tools to help developers and producers manipulate R objects\n    and outputs. It includes tools for displaying results and objects, and\n    for formatting them in the correct format.  "
  },
  {
    "id": 7587,
    "package_name": "TCIU",
    "title": "Spacekime Analytics, Time Complexity and Inferential Uncertainty",
    "description": "Provide the core functionality to transform longitudinal data to\n    complex-time (kime) data using analytic and numerical techniques, visualize the original \n    time-series and reconstructed kime-surfaces, perform model based (e.g., tensor-linear regression)\n    and model-free classification and clustering methods in the book Dinov, ID and Velev, MV. (2021)\n    \"Data Science: Time Complexity, Inferential Uncertainty, and Spacekime Analytics\", De Gruyter STEM Series,\n    ISBN 978-3-11-069780-3. <https://www.degruyter.com/view/title/576646>.\n    The package includes 18 core functions which can be separated into three groups.\n    1) draw longitudinal data, such as Functional magnetic resonance imaging(fMRI) time-series, and forecast or transform the time-series data.\n    2) simulate real-valued time-series data, e.g., fMRI time-courses, detect the activated areas,\n    report the corresponding p-values, and visualize the p-values in the 3D brain space.\n    3) Laplace transform and kimesurface reconstructions of the fMRI data.",
    "version": "1.2.7",
    "maintainer": "Yueyang Shen <petersyy@umich.edu>",
    "author": "Yongkai Qiu [aut],\n  Zhe Yin [aut],\n  Jinwen Cao [aut],\n  Yupeng Zhang [aut],\n  Yuyao Liu [aut],\n  Rongqian Zhang [aut],\n  Yueyang Shen [aut, cre],\n  Rouben Rostamian [ctb],\n  Ranjan Maitra [ctb],\n  Daniel Rowe [ctb],\n  Daniel Adrian [ctb] (gLRT method for complex-valued fMRI statistics),\n  Yunjie Guo [aut],\n  Ivo Dinov [aut]",
    "url": "https://github.com/SOCR/TCIU,\nhttps://www.socr.umich.edu/spacekime/,\nhttps://www.socr.umich.edu/TCIU/",
    "bug_reports": "https://github.com/SOCR/TCIU/issues",
    "repository": "https://cran.r-project.org/package=TCIU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TCIU Spacekime Analytics, Time Complexity and Inferential Uncertainty Provide the core functionality to transform longitudinal data to\n    complex-time (kime) data using analytic and numerical techniques, visualize the original \n    time-series and reconstructed kime-surfaces, perform model based (e.g., tensor-linear regression)\n    and model-free classification and clustering methods in the book Dinov, ID and Velev, MV. (2021)\n    \"Data Science: Time Complexity, Inferential Uncertainty, and Spacekime Analytics\", De Gruyter STEM Series,\n    ISBN 978-3-11-069780-3. <https://www.degruyter.com/view/title/576646>.\n    The package includes 18 core functions which can be separated into three groups.\n    1) draw longitudinal data, such as Functional magnetic resonance imaging(fMRI) time-series, and forecast or transform the time-series data.\n    2) simulate real-valued time-series data, e.g., fMRI time-courses, detect the activated areas,\n    report the corresponding p-values, and visualize the p-values in the 3D brain space.\n    3) Laplace transform and kimesurface reconstructions of the fMRI data.  "
  },
  {
    "id": 7620,
    "package_name": "TKCat",
    "title": "Tailored Knowledge Catalog",
    "description": "Facilitate the management of data from knowledge\n   resources that are frequently used alone or together\n   in research environments.\n   In 'TKCat', knowledge resources are manipulated as modeled database (MDB)\n   objects. These objects provide access to the data tables along with a general\n   description of the resource and a detail data model documenting the\n   tables, their fields and their relationships.\n   These MDBs are then gathered in catalogs that can be easily\n   explored an shared.\n   Finally, 'TKCat' provides tools to easily subset, filter and combine MDBs and\n   create new catalogs suited for specific needs.",
    "version": "1.1.14",
    "maintainer": "Patrice Godard <patrice.godard@gmail.com>",
    "author": "Patrice Godard [aut, cre, cph]",
    "url": "https://patzaw.github.io/TKCat/, https://github.com/patzaw/TKCat/",
    "bug_reports": "https://github.com/patzaw/TKCat/issues",
    "repository": "https://cran.r-project.org/package=TKCat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TKCat Tailored Knowledge Catalog Facilitate the management of data from knowledge\n   resources that are frequently used alone or together\n   in research environments.\n   In 'TKCat', knowledge resources are manipulated as modeled database (MDB)\n   objects. These objects provide access to the data tables along with a general\n   description of the resource and a detail data model documenting the\n   tables, their fields and their relationships.\n   These MDBs are then gathered in catalogs that can be easily\n   explored an shared.\n   Finally, 'TKCat' provides tools to easily subset, filter and combine MDBs and\n   create new catalogs suited for specific needs.  "
  },
  {
    "id": 7660,
    "package_name": "TSEind",
    "title": "Total Survey Error (Independent Samples)",
    "description": "Calculates total survey error (TSE) for one or more surveys, using both scale-dependent and scale-independent metrics.  Package works directly from the data set, with no hand calculations required: just upload a properly structured data set (see TESTIND and its documentation), properly input column names (see functions documentation), and run your functions.  For more on TSE, see: Weisberg, Herbert (2005, ISBN:0-226-89128-3); Biemer, Paul (2010) <doi:10.1093/poq/nfq058>; Biemer, Paul et.al. (2017, ISBN:9781119041672); etc.",
    "version": "0.1.0",
    "maintainer": "Joshua Miller <joshlmiller@msn.com>",
    "author": "Joshua Miller [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSEind",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSEind Total Survey Error (Independent Samples) Calculates total survey error (TSE) for one or more surveys, using both scale-dependent and scale-independent metrics.  Package works directly from the data set, with no hand calculations required: just upload a properly structured data set (see TESTIND and its documentation), properly input column names (see functions documentation), and run your functions.  For more on TSE, see: Weisberg, Herbert (2005, ISBN:0-226-89128-3); Biemer, Paul (2010) <doi:10.1093/poq/nfq058>; Biemer, Paul et.al. (2017, ISBN:9781119041672); etc.  "
  },
  {
    "id": 7663,
    "package_name": "TSEwgt",
    "title": "Total Survey Error Under Multiple, Different Weighting Schemes",
    "description": "Calculates total survey error (TSE) for a survey under multiple, different weighting schemes, using both scale-dependent and scale-independent metrics.  Package works directly from the data set, with no hand calculations required: just upload a properly structured data set (see TESTWGT and its documentation), properly input column names (see functions documentation), and run your functions.  For more on TSE, see: Weisberg, Herbert (2005, ISBN:0-226-89128-3); Biemer, Paul (2010) <doi:10.1093/poq/nfq058>; Biemer, Paul et.al. (2017, ISBN:9781119041672); etc.",
    "version": "0.1.0",
    "maintainer": "Joshua Miller <joshlmiller@msn.com>",
    "author": "Joshua Miller [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSEwgt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSEwgt Total Survey Error Under Multiple, Different Weighting Schemes Calculates total survey error (TSE) for a survey under multiple, different weighting schemes, using both scale-dependent and scale-independent metrics.  Package works directly from the data set, with no hand calculations required: just upload a properly structured data set (see TESTWGT and its documentation), properly input column names (see functions documentation), and run your functions.  For more on TSE, see: Weisberg, Herbert (2005, ISBN:0-226-89128-3); Biemer, Paul (2010) <doi:10.1093/poq/nfq058>; Biemer, Paul et.al. (2017, ISBN:9781119041672); etc.  "
  },
  {
    "id": 7718,
    "package_name": "TeXCheckR",
    "title": "Parses LaTeX Documents for Errors",
    "description": "Checks LaTeX documents and .bib files for typing errors, such as spelling errors, incorrect quotation marks. Also provides useful functions for parsing and linting bibliography files.",
    "version": "0.8.1",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre]",
    "url": "https://github.com/HughParsonage/TeXCheckR",
    "bug_reports": "https://github.com/HughParsonage/TeXCheckR/issues",
    "repository": "https://cran.r-project.org/package=TeXCheckR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TeXCheckR Parses LaTeX Documents for Errors Checks LaTeX documents and .bib files for typing errors, such as spelling errors, incorrect quotation marks. Also provides useful functions for parsing and linting bibliography files.  "
  },
  {
    "id": 7724,
    "package_name": "TempStable",
    "title": "A Collection of Methods to Estimate Parameters of Different\nTempered Stable Distributions",
    "description": "\n  A collection of methods to estimate parameters of different tempered stable \n  distributions (TSD). Currently, there are seven different tempered stable \n  distributions to choose from: Tempered stable subordinator distribution, \n  classical TSD, generalized classical TSD, normal TSD, modified TSD, rapid \n  decreasing TSD, and Kim-Rachev TSD.\n  The package also provides functions to compute density and probability \n  functions and tools to run Monte Carlo simulations.\n  This package has already been used for the estimation of tempered stable \n  distributions (Massing (2023) <arXiv:2303.07060>).\n  The following references form the theoretical background for various functions\n  in this package. References for each function are explicitly listed in its \n  documentation:\n  Bianchi et al. (2010) <doi:10.1007/978-88-470-1481-7_4>\n  Bianchi et al. (2011) <doi:10.1137/S0040585X97984632>\n  Carrasco (2017) <doi:10.1017/S0266466616000025>\n  Feuerverger (1981) <doi:10.1111/j.2517-6161.1981.tb01143.x>\n  Hansen et al. (1996) <doi:10.1080/07350015.1996.10524656>\n  Hansen (1982) <doi:10.2307/1912775>\n  Hofert (2011) <doi:10.1145/2043635.2043638>\n  Kawai & Masuda (2011) <doi:10.1016/j.cam.2010.12.014>\n  Kim et al. (2008) <doi:10.1016/j.jbankfin.2007.11.004>\n  Kim et al. (2009) <doi:10.1007/978-3-7908-2050-8_5>\n  Kim et al. (2010) <doi:10.1016/j.jbankfin.2010.01.015>\n  Kuechler & Tappe (2013) <doi:10.1016/j.spa.2013.06.012>\n  Rachev et al. (2011) <doi:10.1002/9781118268070>.",
    "version": "0.2.2",
    "maintainer": "Till Massing <till.massing@uni-due.de>",
    "author": "Till Massing [cre, aut],\n  Cedric Maximilian Juessen [aut]",
    "url": "https://github.com/TMoek/TempStable",
    "bug_reports": "https://github.com/TMoek/TempStable/issues",
    "repository": "https://cran.r-project.org/package=TempStable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TempStable A Collection of Methods to Estimate Parameters of Different\nTempered Stable Distributions \n  A collection of methods to estimate parameters of different tempered stable \n  distributions (TSD). Currently, there are seven different tempered stable \n  distributions to choose from: Tempered stable subordinator distribution, \n  classical TSD, generalized classical TSD, normal TSD, modified TSD, rapid \n  decreasing TSD, and Kim-Rachev TSD.\n  The package also provides functions to compute density and probability \n  functions and tools to run Monte Carlo simulations.\n  This package has already been used for the estimation of tempered stable \n  distributions (Massing (2023) <arXiv:2303.07060>).\n  The following references form the theoretical background for various functions\n  in this package. References for each function are explicitly listed in its \n  documentation:\n  Bianchi et al. (2010) <doi:10.1007/978-88-470-1481-7_4>\n  Bianchi et al. (2011) <doi:10.1137/S0040585X97984632>\n  Carrasco (2017) <doi:10.1017/S0266466616000025>\n  Feuerverger (1981) <doi:10.1111/j.2517-6161.1981.tb01143.x>\n  Hansen et al. (1996) <doi:10.1080/07350015.1996.10524656>\n  Hansen (1982) <doi:10.2307/1912775>\n  Hofert (2011) <doi:10.1145/2043635.2043638>\n  Kawai & Masuda (2011) <doi:10.1016/j.cam.2010.12.014>\n  Kim et al. (2008) <doi:10.1016/j.jbankfin.2007.11.004>\n  Kim et al. (2009) <doi:10.1007/978-3-7908-2050-8_5>\n  Kim et al. (2010) <doi:10.1016/j.jbankfin.2010.01.015>\n  Kuechler & Tappe (2013) <doi:10.1016/j.spa.2013.06.012>\n  Rachev et al. (2011) <doi:10.1002/9781118268070>.  "
  },
  {
    "id": 7733,
    "package_name": "TestAnaAPP",
    "title": "A 'shiny' App for Test Analysis and Visualization",
    "description": "This application provides exploratory and confirmatory factor analysis, classical test theory, unidimensional and multidimensional item response theory, and continuous item response model analysis, through the 'shiny' interactive interface. In addition,  it offers rich functionalities for visualizing and downloading results. Users can download figures, tables, and analysis reports via the interactive interface. ",
    "version": "1.1.2",
    "maintainer": "Youxiang Jiang <jiangyouxiang34@163.com>",
    "author": "Youxiang Jiang [cre, aut, ths] (0000-0003-4557-5038),\n  Qing Zeng [aut, ths],\n  Hongbo Wen [aut, ths] (0000-0001-8620-9734)",
    "url": "https://github.com/jiangyouxiang/TestAnaAPP",
    "bug_reports": "https://github.com/jiangyouxiang/TestAnaAPP/issues",
    "repository": "https://cran.r-project.org/package=TestAnaAPP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TestAnaAPP A 'shiny' App for Test Analysis and Visualization This application provides exploratory and confirmatory factor analysis, classical test theory, unidimensional and multidimensional item response theory, and continuous item response model analysis, through the 'shiny' interactive interface. In addition,  it offers rich functionalities for visualizing and downloading results. Users can download figures, tables, and analysis reports via the interactive interface.   "
  },
  {
    "id": 7746,
    "package_name": "TexExamRandomizer",
    "title": "Personalizes and Randomizes Exams Written in 'LaTeX'",
    "description": "Randomizing exams with 'LaTeX'.\n    If you can compile your main document with 'LaTeX', the program should be able to compile the randomized\n    versions without much extra effort when creating the document.",
    "version": "1.2.7",
    "maintainer": "Alejandro Gonzalez Recuenco <alejandrogonzalezrecuenco@gmail.com>",
    "author": "Alejandro Gonzalez Recuenco",
    "url": "https://github.com/alexrecuenco/TexExamRandomizer,\nhttps://alexrecuenco.github.io/TexExamRandomizer/",
    "bug_reports": "https://github.com/alexrecuenco/TexExamRandomizer/issues",
    "repository": "https://cran.r-project.org/package=TexExamRandomizer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TexExamRandomizer Personalizes and Randomizes Exams Written in 'LaTeX' Randomizing exams with 'LaTeX'.\n    If you can compile your main document with 'LaTeX', the program should be able to compile the randomized\n    versions without much extra effort when creating the document.  "
  },
  {
    "id": 7770,
    "package_name": "TidyMultiqc",
    "title": "Converts 'MultiQC' Reports into Tidy Data Frames",
    "description": "Provides the means to convert 'multiqc_data.json' files,\n    produced by the wonderful 'MultiQC' tool, into tidy data frames for downstream\n    analysis in R. This analysis might involve cohort analysis, quality control visualisation,\n    change-point detection, statistical process control, clustering, or any other\n    type of quality analysis.",
    "version": "1.0.3",
    "maintainer": "Michael Milton <michael.r.milton@gmail.com>",
    "author": "Michael Milton",
    "url": "https://multimeric.github.io/TidyMultiqc/,\nhttps://github.com/multimeric/TidyMultiqc,\nhttps://cran.r-project.org/package=TidyMultiqc",
    "bug_reports": "https://github.com/multimeric/TidyMultiqc/issues",
    "repository": "https://cran.r-project.org/package=TidyMultiqc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TidyMultiqc Converts 'MultiQC' Reports into Tidy Data Frames Provides the means to convert 'multiqc_data.json' files,\n    produced by the wonderful 'MultiQC' tool, into tidy data frames for downstream\n    analysis in R. This analysis might involve cohort analysis, quality control visualisation,\n    change-point detection, statistical process control, clustering, or any other\n    type of quality analysis.  "
  },
  {
    "id": 7783,
    "package_name": "TopDom",
    "title": "An Efficient and Deterministic Method for Identifying\nTopological Domains in Genomes",
    "description": "The 'TopDom' method identifies topological domains in genomes from Hi-C sequence data (Shin et al., 2016 <doi:10.1093/nar/gkv1505>).  The authors published an implementation of their method as an R script (two different versions; also available in this package).  This package originates from those original 'TopDom' R scripts and provides help pages adopted from the original 'TopDom' PDF documentation.  It also provides a small number of bug fixes to the original code.",
    "version": "0.10.1",
    "maintainer": "Henrik Bengtsson <henrikb@braju.com>",
    "author": "Henrik Bengtsson [aut, cre, cph],\n  Hanjun Shin [aut, ctr, cph],\n  Harris Lazaris [ctr, cph] (PhD Student, NYU),\n  Gangqing Hu [ctr, cph] (Staff Scientist, NIH),\n  Xianghong Zhou [ctr]",
    "url": "https://github.com/HenrikBengtsson/TopDom",
    "bug_reports": "https://github.com/HenrikBengtsson/TopDom/issues",
    "repository": "https://cran.r-project.org/package=TopDom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TopDom An Efficient and Deterministic Method for Identifying\nTopological Domains in Genomes The 'TopDom' method identifies topological domains in genomes from Hi-C sequence data (Shin et al., 2016 <doi:10.1093/nar/gkv1505>).  The authors published an implementation of their method as an R script (two different versions; also available in this package).  This package originates from those original 'TopDom' R scripts and provides help pages adopted from the original 'TopDom' PDF documentation.  It also provides a small number of bug fixes to the original code.  "
  },
  {
    "id": 7786,
    "package_name": "TopicScore",
    "title": "The Topic SCORE Algorithm to Fit Topic Models",
    "description": "Provides implementation of the \"Topic SCORE\" algorithm that is\n\t     proposed by Tracy Ke and Minzhe Wang. The singular value decomposition\n\t     step is optimized through the usage of svds() function in 'RSpectra'\n\t     package, on a 'dgRMatrix' sparse matrix. Also provides a column-wise\n\t     error measure in the word-topic matrix A, and an algorithm for\n\t     recovering the topic-document matrix W given A and D based on\n\t     quadratic programming.\n\t\tThe details about the techniques are explained in the paper \"A new SVD approach to optimal topic estimation\" by Tracy Ke and Minzhe Wang (2017) <arXiv:1704.07016>.",
    "version": "0.0.1",
    "maintainer": "Minzhe Wang <minzhew@uchicago.edu>",
    "author": "Minzhe Wang [aut, cre],\n  Tracy Ke [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TopicScore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TopicScore The Topic SCORE Algorithm to Fit Topic Models Provides implementation of the \"Topic SCORE\" algorithm that is\n\t     proposed by Tracy Ke and Minzhe Wang. The singular value decomposition\n\t     step is optimized through the usage of svds() function in 'RSpectra'\n\t     package, on a 'dgRMatrix' sparse matrix. Also provides a column-wise\n\t     error measure in the word-topic matrix A, and an algorithm for\n\t     recovering the topic-document matrix W given A and D based on\n\t     quadratic programming.\n\t\tThe details about the techniques are explained in the paper \"A new SVD approach to optimal topic estimation\" by Tracy Ke and Minzhe Wang (2017) <arXiv:1704.07016>.  "
  },
  {
    "id": 7801,
    "package_name": "TransProR",
    "title": "Analysis and Visualization of Multi-Omics Data",
    "description": "A tool for comprehensive transcriptomic data analysis, with a focus on transcript-level data preprocessing, expression profiling, differential expression analysis, and functional enrichment. It enables researchers to identify key biological processes, disease biomarkers, and gene regulatory mechanisms. 'TransProR' is aimed at researchers and bioinformaticians working with RNA-Seq data, providing an intuitive framework for in-depth analysis and visualization of transcriptomic datasets. The package includes comprehensive documentation and usage examples to guide users through the entire analysis pipeline. The differential expression analysis methods incorporated in the package include 'limma' (Ritchie et al., 2015, <doi:10.1093/nar/gkv007>; Smyth, 2005, <doi:10.1007/0-387-29362-0_23>), 'edgeR' (Robinson et al., 2010, <doi:10.1093/bioinformatics/btp616>), 'DESeq2' (Love et al., 2014, <doi:10.1186/s13059-014-0550-8>), and Wilcoxon tests (Li et al., 2022, <doi:10.1186/s13059-022-02648-4>), providing flexible and robust approaches to RNA-Seq data analysis. For more information, refer to the package vignettes and related publications.",
    "version": "1.0.7",
    "maintainer": "Dongyue Yu <yudongyue@mail.nankai.edu.cn>",
    "author": "Dongyue Yu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7041-2073>)",
    "url": "https://github.com/SSSYDYSSS/TransProRBook",
    "bug_reports": "https://github.com/SSSYDYSSS/TransProR/issues",
    "repository": "https://cran.r-project.org/package=TransProR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TransProR Analysis and Visualization of Multi-Omics Data A tool for comprehensive transcriptomic data analysis, with a focus on transcript-level data preprocessing, expression profiling, differential expression analysis, and functional enrichment. It enables researchers to identify key biological processes, disease biomarkers, and gene regulatory mechanisms. 'TransProR' is aimed at researchers and bioinformaticians working with RNA-Seq data, providing an intuitive framework for in-depth analysis and visualization of transcriptomic datasets. The package includes comprehensive documentation and usage examples to guide users through the entire analysis pipeline. The differential expression analysis methods incorporated in the package include 'limma' (Ritchie et al., 2015, <doi:10.1093/nar/gkv007>; Smyth, 2005, <doi:10.1007/0-387-29362-0_23>), 'edgeR' (Robinson et al., 2010, <doi:10.1093/bioinformatics/btp616>), 'DESeq2' (Love et al., 2014, <doi:10.1186/s13059-014-0550-8>), and Wilcoxon tests (Li et al., 2022, <doi:10.1186/s13059-022-02648-4>), providing flexible and robust approaches to RNA-Seq data analysis. For more information, refer to the package vignettes and related publications.  "
  },
  {
    "id": 7807,
    "package_name": "TreeBUGS",
    "title": "Hierarchical Multinomial Processing Tree Modeling",
    "description": "User-friendly analysis of hierarchical multinomial processing tree (MPT) \n    models that are often used in cognitive psychology. Implements the latent-trait \n    MPT approach (Klauer, 2010) <DOI:10.1007/s11336-009-9141-0> and the beta-MPT \n    approach (Smith & Batchelder, 2010) <DOI:10.1016/j.jmp.2009.06.007> to model \n    heterogeneity of participants. MPT models are conveniently specified by an\n    .eqn-file as used by other MPT software and data are provided by a .csv-file \n    or directly in R. Models are either fitted by calling JAGS or by an MPT-tailored \n    Gibbs sampler in C++ (only for nonhierarchical and beta MPT models). Provides \n    tests of heterogeneity and MPT-tailored summaries and plotting functions.\n    A detailed documentation is available in Heck, Arnold, & Arnold (2018) \n    <DOI:10.3758/s13428-017-0869-7> and a tutorial on MPT modeling can be found \n    in Schmidt, Erdfelder, & Heck (2023) <DOI:10.1037/met0000561>.",
    "version": "1.5.3",
    "maintainer": "Daniel W. Heck <daniel.heck@uni-marburg.de>",
    "author": "Daniel W. Heck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6302-9252>),\n  Nina R. Arnold [aut, dtc],\n  Denis Arnold [aut],\n  Alexander Ly [ctb],\n  Marius Barth [ctb] (ORCID: <https://orcid.org/0000-0002-3421-6665>)",
    "url": "https://github.com/danheck/TreeBUGS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TreeBUGS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TreeBUGS Hierarchical Multinomial Processing Tree Modeling User-friendly analysis of hierarchical multinomial processing tree (MPT) \n    models that are often used in cognitive psychology. Implements the latent-trait \n    MPT approach (Klauer, 2010) <DOI:10.1007/s11336-009-9141-0> and the beta-MPT \n    approach (Smith & Batchelder, 2010) <DOI:10.1016/j.jmp.2009.06.007> to model \n    heterogeneity of participants. MPT models are conveniently specified by an\n    .eqn-file as used by other MPT software and data are provided by a .csv-file \n    or directly in R. Models are either fitted by calling JAGS or by an MPT-tailored \n    Gibbs sampler in C++ (only for nonhierarchical and beta MPT models). Provides \n    tests of heterogeneity and MPT-tailored summaries and plotting functions.\n    A detailed documentation is available in Heck, Arnold, & Arnold (2018) \n    <DOI:10.3758/s13428-017-0869-7> and a tutorial on MPT modeling can be found \n    in Schmidt, Erdfelder, & Heck (2023) <DOI:10.1037/met0000561>.  "
  },
  {
    "id": 7845,
    "package_name": "Tushare",
    "title": "Interface to 'Tushare Pro' API",
    "description": "Helps the R users to get data from 'Tushare Pro'<https://tushare.pro>.\n    'Tushare Pro' is a platform as well as a community with a lot of staffs working in financial area.\n    We support financial data such as stock price, financial report statements and digital coins data.",
    "version": "0.1.4",
    "maintainer": "Feifei ZHANG<tushare_pro@163.com>",
    "author": "Feifei ZHANG",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Tushare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Tushare Interface to 'Tushare Pro' API Helps the R users to get data from 'Tushare Pro'<https://tushare.pro>.\n    'Tushare Pro' is a platform as well as a community with a lot of staffs working in financial area.\n    We support financial data such as stock price, financial report statements and digital coins data.  "
  },
  {
    "id": 7847,
    "package_name": "TwoDcDAPSA",
    "title": "Calculate TwoDcDAPSA: PROs-Joint Contrast (PJC) Score and\nQuartiles",
    "description": "Provides a calculator for the two-dimensional clinical Disease Activity \n    index for Psoriatic Arthritis (TwoDcDAPSA), a principal component-derived measure \n    that complements the conventional clinical DAPSA score. The TwoDcDAPSA captures \n    residual variation in patient-reported outcomes (pain and patient global assessment) \n    and joint counts (swollen and tender) after adjusting for standardized cDAPSA using \n    natural spline coefficients derived from published models. Residuals are \n    standardized and combined with fixed principal component loadings to yield a \n    continuous PROs-Joint Contrast (PJC) score and quartile groupings. The package \n    applies pre-specified coefficients and loadings to new datasets but does not \n    estimate spline models or principal components itself.",
    "version": "0.1.0",
    "maintainer": "Ning Meng <nmeng2@jh.edu>",
    "author": "Ning Meng [aut, cre],\n  Ji Soo Kim [aut],\n  Ana-Maria Orbai [aut],\n  Scott L. Zeger [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TwoDcDAPSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TwoDcDAPSA Calculate TwoDcDAPSA: PROs-Joint Contrast (PJC) Score and\nQuartiles Provides a calculator for the two-dimensional clinical Disease Activity \n    index for Psoriatic Arthritis (TwoDcDAPSA), a principal component-derived measure \n    that complements the conventional clinical DAPSA score. The TwoDcDAPSA captures \n    residual variation in patient-reported outcomes (pain and patient global assessment) \n    and joint counts (swollen and tender) after adjusting for standardized cDAPSA using \n    natural spline coefficients derived from published models. Residuals are \n    standardized and combined with fixed principal component loadings to yield a \n    continuous PROs-Joint Contrast (PJC) score and quartile groupings. The package \n    applies pre-specified coefficients and loadings to new datasets but does not \n    estimate spline models or principal components itself.  "
  },
  {
    "id": 7849,
    "package_name": "TwoRegression",
    "title": "Develop and Apply Two-Regression Algorithms",
    "description": "Facilitates development and application of two-regression\n    algorithms for research-grade wearable devices. It provides an easy\n    way for users to access previously-developed algorithms, and also to\n    develop their own. Initial motivation came from Hibbing PR, LaMunion SR,\n    Kaplan AS, & Crouter SE (2018) <doi:10.1249/MSS.0000000000001532>.\n    However, other algorithms are now supported. Please see the associated\n    references in the package documentation for full details of the algorithms\n    that are supported.",
    "version": "1.1.1",
    "maintainer": "Paul R. Hibbing <paulhibbing@gmail.com>",
    "author": "Paul R. Hibbing [aut, cre],\n  Vincent T. van Hees [ctb]",
    "url": "https://github.com/paulhibbing/TwoRegression",
    "bug_reports": "https://github.com/paulhibbing/TwoRegression/issues",
    "repository": "https://cran.r-project.org/package=TwoRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TwoRegression Develop and Apply Two-Regression Algorithms Facilitates development and application of two-regression\n    algorithms for research-grade wearable devices. It provides an easy\n    way for users to access previously-developed algorithms, and also to\n    develop their own. Initial motivation came from Hibbing PR, LaMunion SR,\n    Kaplan AS, & Crouter SE (2018) <doi:10.1249/MSS.0000000000001532>.\n    However, other algorithms are now supported. Please see the associated\n    references in the package documentation for full details of the algorithms\n    that are supported.  "
  },
  {
    "id": 7850,
    "package_name": "TwoSampleTest.HD",
    "title": "A Two-Sample Test for the Equality of Distributions for\nHigh-Dimensional Data",
    "description": "For high-dimensional data whose main feature is a large number, p, of variables but a small sample size, the null hypothesis that the marginal distributions of p variables are the same for two groups is tested. We propose a test statistic motivated by the simple idea of comparing, for each of the p variables, the empirical characteristic functions computed from the two samples. If one rejects this global null hypothesis of no differences in distributions between the two groups, a set of permutation p-values is reported to identify which variables are not equally distributed in both groups.",
    "version": "1.2",
    "maintainer": "Marta Cousido Rocha <martacousido@uvigo.es>",
    "author": "Marta Cousido Rocha [aut, cre],\n  Jos\u00e9 Carlos Soage Gonz\u00e1lez [ctr],\n  Jacobo de U\u00f1a \u00c1lvarez [aut, ths],\n  Jeffrey D. Hart [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TwoSampleTest.HD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TwoSampleTest.HD A Two-Sample Test for the Equality of Distributions for\nHigh-Dimensional Data For high-dimensional data whose main feature is a large number, p, of variables but a small sample size, the null hypothesis that the marginal distributions of p variables are the same for two groups is tested. We propose a test statistic motivated by the simple idea of comparing, for each of the p variables, the empirical characteristic functions computed from the two samples. If one rejects this global null hypothesis of no differences in distributions between the two groups, a set of permutation p-values is reported to identify which variables are not equally distributed in both groups.  "
  },
  {
    "id": 7861,
    "package_name": "UCR.ColumnNames",
    "title": "Fixes Column Names for Uniform Crime Report \"Offenses Known and\nClearance by Arrest\" Datasets",
    "description": "Changes the column names of the inputted dataset to the correct\n    names from the Uniform Crime Report codebook for the \"Offenses Known and\n    Clearance by Arrest\" datasets from 1998-2014.",
    "version": "0.1.0",
    "maintainer": "Jacob Kaplan <jacobkap@sas.upenn.edu>",
    "author": "Jacob Kaplan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UCR.ColumnNames",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UCR.ColumnNames Fixes Column Names for Uniform Crime Report \"Offenses Known and\nClearance by Arrest\" Datasets Changes the column names of the inputted dataset to the correct\n    names from the Uniform Crime Report codebook for the \"Offenses Known and\n    Clearance by Arrest\" datasets from 1998-2014.  "
  },
  {
    "id": 7867,
    "package_name": "UKFE",
    "title": "UK Flood Estimation",
    "description": "Functions to implement the methods of the Flood Estimation Handbook (FEH), associated updates and the revitalised flood hydrograph model (ReFH). Currently the package uses NRFA peak flow dataset version 14. Aside from FEH functionality, further hydrological functions are available. Most of the methods implemented in this package are described in one or more of the following: \"Flood Estimation Handbook\", Centre for Ecology & Hydrology (1999, ISBN:0 948540 94 X). \"Flood Estimation Handbook Supplementary Report No. 1\", Kjeldsen (2007, ISBN:0 903741 15 7). \"Regional Frequency Analysis - an approach based on L-moments\", Hosking & Wallis (1997, ISBN: 978 0 521 01940 8). \"Making better use of local data in flood frequency estimation\", Environment Agency (2017, ISBN: 978 1 84911 387 8). \"Sampling uncertainty of UK design flood estimation\" , Hammond (2021, <doi:10.2166/nh.2021.059>). \"The FEH 2025 statistical method update\", UK Centre for Ecology and Hydrology (2025). \"Low flow estimation in the United Kingdom\", Institute of Hydrology (1992, ISBN 0 948540 45 1). Data from the UK National River Flow Archive (<https://nrfa.ceh.ac.uk/>, terms and conditions: <https://nrfa.ceh.ac.uk/help/costs-terms-and-conditions>).",
    "version": "2.0.0",
    "maintainer": "Anthony Hammond <agqhammond@gmail.com>",
    "author": "Anthony Hammond [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UKFE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UKFE UK Flood Estimation Functions to implement the methods of the Flood Estimation Handbook (FEH), associated updates and the revitalised flood hydrograph model (ReFH). Currently the package uses NRFA peak flow dataset version 14. Aside from FEH functionality, further hydrological functions are available. Most of the methods implemented in this package are described in one or more of the following: \"Flood Estimation Handbook\", Centre for Ecology & Hydrology (1999, ISBN:0 948540 94 X). \"Flood Estimation Handbook Supplementary Report No. 1\", Kjeldsen (2007, ISBN:0 903741 15 7). \"Regional Frequency Analysis - an approach based on L-moments\", Hosking & Wallis (1997, ISBN: 978 0 521 01940 8). \"Making better use of local data in flood frequency estimation\", Environment Agency (2017, ISBN: 978 1 84911 387 8). \"Sampling uncertainty of UK design flood estimation\" , Hammond (2021, <doi:10.2166/nh.2021.059>). \"The FEH 2025 statistical method update\", UK Centre for Ecology and Hydrology (2025). \"Low flow estimation in the United Kingdom\", Institute of Hydrology (1992, ISBN 0 948540 45 1). Data from the UK National River Flow Archive (<https://nrfa.ceh.ac.uk/>, terms and conditions: <https://nrfa.ceh.ac.uk/help/costs-terms-and-conditions>).  "
  },
  {
    "id": 7874,
    "package_name": "URooTab",
    "title": "Tabular Reporting of 'EViews' Unit Root Tests",
    "description": "Conduct unit root tests based on 'EViews' (<https://eviews.com>) routines and report them in tables. 'EViews' (Econometric Views) is a commercial software for econometrics.",
    "version": "0.1.0",
    "maintainer": "Sagiru Mati <sagirumati@gmail.com>",
    "author": "Sagiru Mati [aut, cre] (ORCID: <https://orcid.org/0000-0003-1413-3974>)",
    "url": "https://github.com/sagirumati/URooTab",
    "bug_reports": "https://github.com/sagirumati/URooTab/issues",
    "repository": "https://cran.r-project.org/package=URooTab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "URooTab Tabular Reporting of 'EViews' Unit Root Tests Conduct unit root tests based on 'EViews' (<https://eviews.com>) routines and report them in tables. 'EViews' (Econometric Views) is a commercial software for econometrics.  "
  },
  {
    "id": 7939,
    "package_name": "VIC5",
    "title": "The Variable Infiltration Capacity (VIC) Hydrological Model",
    "description": "The Variable Infiltration Capacity (VIC) model is a macroscale\n    hydrologic model that solves full water and energy balances, originally\n    developed by Xu Liang at the University of Washington (UW).\n    The version of VIC source code used is of 5.0.1 on <https://github.com/UW-Hydro/VIC/>,\n    see Hamman et al. (2018).\n    Development and maintenance of the current official version\n    of the VIC model at present is led by the UW Hydro (Computational Hydrology\n    group) in the Department of Civil and Environmental Engineering at UW. VIC is\n    a research model and in its various forms it has been applied to most of the\n    major river basins around the world, as well as globally \n    <http://vic.readthedocs.io/en/master/Documentation/References/>. \n    References: \"Liang, X., D. P. Lettenmaier, E. F. Wood, and\n    S. J. Burges (1994), A simple hydrologically based model of land surface water\n    and energy fluxes for general circulation models, J. Geophys. Res., 99(D7),\n    14415-14428, <doi:10.1029/94JD00483>\"; \n    \"Hamman, J. J., Nijssen, B., Bohn, T. J., Gergel, D. R., and Mao, Y. (2018), The\n    Variable Infiltration Capacity model version 5 (VIC-5): infrastructure improvements\n    for new applications and reproducibility, Geosci. Model Dev., 11, 3481-3496, \n    <doi:10.5194/gmd-11-3481-2018>\".",
    "version": "0.2.6",
    "maintainer": "Dongdong Kong <kongdd.sysu@gmail.com>",
    "author": "Ruida Zhong [aut, ctb],\n  Dongdong Kong [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1836-8172>),\n  Xiaohong Chen [aut, ctb],\n  Zhaoli Wang [aut, ctb],\n  Chengguang Lai [aut, ctb],\n  Joseph J Hamman [ctb] (Contributor and maintainer of VIC source code),\n  Keith Cherkauer [ctb]",
    "url": "https://github.com/rpkgs/VIC5",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VIC5",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VIC5 The Variable Infiltration Capacity (VIC) Hydrological Model The Variable Infiltration Capacity (VIC) model is a macroscale\n    hydrologic model that solves full water and energy balances, originally\n    developed by Xu Liang at the University of Washington (UW).\n    The version of VIC source code used is of 5.0.1 on <https://github.com/UW-Hydro/VIC/>,\n    see Hamman et al. (2018).\n    Development and maintenance of the current official version\n    of the VIC model at present is led by the UW Hydro (Computational Hydrology\n    group) in the Department of Civil and Environmental Engineering at UW. VIC is\n    a research model and in its various forms it has been applied to most of the\n    major river basins around the world, as well as globally \n    <http://vic.readthedocs.io/en/master/Documentation/References/>. \n    References: \"Liang, X., D. P. Lettenmaier, E. F. Wood, and\n    S. J. Burges (1994), A simple hydrologically based model of land surface water\n    and energy fluxes for general circulation models, J. Geophys. Res., 99(D7),\n    14415-14428, <doi:10.1029/94JD00483>\"; \n    \"Hamman, J. J., Nijssen, B., Bohn, T. J., Gergel, D. R., and Mao, Y. (2018), The\n    Variable Infiltration Capacity model version 5 (VIC-5): infrastructure improvements\n    for new applications and reproducibility, Geosci. Model Dev., 11, 3481-3496, \n    <doi:10.5194/gmd-11-3481-2018>\".  "
  },
  {
    "id": 7957,
    "package_name": "VTShiny",
    "title": "Interactive Document for Working with Variance Analysis",
    "description": "An interactive document on  the topic of variance analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://predanalyticssessions1.shinyapps.io/chisquareVarianceTest/>.",
    "version": "0.1.0",
    "maintainer": "Kartikeya Bolar <kartikeya.bolar@tapmi.edu.in>",
    "author": "Kartikeya Bolar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VTShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VTShiny Interactive Document for Working with Variance Analysis An interactive document on  the topic of variance analysis using 'rmarkdown' and 'shiny' packages. Runtime examples are provided in the package function as well as at  <https://predanalyticssessions1.shinyapps.io/chisquareVarianceTest/>.  "
  },
  {
    "id": 7959,
    "package_name": "VWPre",
    "title": "Tools for Preprocessing Visual World Data",
    "description": "Gaze data from the Visual World Paradigm requires significant\n    preprocessing prior to plotting and analyzing the data. This package \n    provides functions for preparing visual world eye-tracking data for \n    statistical analysis and plotting. It can prepare data for linear \n    analyses (e.g., ANOVA, Gaussian-family LMER, Gaussian-family GAMM) as\n    well as logistic analyses (e.g., binomial-family LMER and binomial-family GAMM).\n    Additionally, it contains various plotting functions for creating grand average and\n    conditional average plots. See the vignette for samples of the functionality.\n    Currently, the functions in this package are designed for handling data\n    collected with SR Research Eyelink eye trackers using Sample Reports created\n    in SR Research Data Viewer.  While we would like to add functionality \n    for data collected with other systems in the future, the current package is \n    considered to be feature-complete; further updates will mainly entail maintenance\n\tand the addition of minor functionality.",
    "version": "1.2.5",
    "maintainer": "Vincent Porretta <vincentporretta@gmail.com>",
    "author": "Vincent Porretta [aut, cre],\n  Aki-Juhani Kyr\u00f6l\u00e4inen [aut],\n  Jacolien van Rij [ctb],\n  Juhani J\u00e4rvikivi [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VWPre",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VWPre Tools for Preprocessing Visual World Data Gaze data from the Visual World Paradigm requires significant\n    preprocessing prior to plotting and analyzing the data. This package \n    provides functions for preparing visual world eye-tracking data for \n    statistical analysis and plotting. It can prepare data for linear \n    analyses (e.g., ANOVA, Gaussian-family LMER, Gaussian-family GAMM) as\n    well as logistic analyses (e.g., binomial-family LMER and binomial-family GAMM).\n    Additionally, it contains various plotting functions for creating grand average and\n    conditional average plots. See the vignette for samples of the functionality.\n    Currently, the functions in this package are designed for handling data\n    collected with SR Research Eyelink eye trackers using Sample Reports created\n    in SR Research Data Viewer.  While we would like to add functionality \n    for data collected with other systems in the future, the current package is \n    considered to be feature-complete; further updates will mainly entail maintenance\n\tand the addition of minor functionality.  "
  },
  {
    "id": 8048,
    "package_name": "WaverideR",
    "title": "Extracting Signals from Wavelet Spectra",
    "description": "The continuous wavelet transform enables the observation of transient/non-stationary cyclicity in time-series. The goal of cyclostratigraphic studies is to define frequency/period in the depth/time domain. By conducting the continuous wavelet transform on cyclostratigraphic data series one can observe and extract cyclic signals/signatures from signals. These results can then be visualized and interpreted enabling one to identify/interpret cyclicity in the geological record, which can be used to construct astrochronological age-models and identify and interpret cyclicity in past and present climate systems. The 'WaverideR' R package builds upon existing literature and existing codebase. The list of articles which are relevant can be grouped in four subjects; cyclostratigraphic data analysis,example data sets,the (continuous) wavelet transform and astronomical solutions. References for the cyclostratigraphic data analysis articles are: Stephen Meyers (2019) <doi:10.1016/j.earscirev.2018.11.015>. Mingsong Li, Linda Hinnov, Lee Kump (2019) <doi:10.1016/j.cageo.2019.02.011> Stephen Meyers (2012)<doi:10.1029/2012PA002307> Mingsong Li, Lee R. Kump, Linda A. Hinnov, Michael E. Mann (2018) <doi:10.1016/j.epsl.2018.08.041>. Wouters, S., Crucifix, M., Sinnesael, M., Da Silva, A.C., Zeeden, C., Zivanovic, M., Boulvain, F., Devleeschouwer, X. (2022) <doi:10.1016/j.earscirev.2021.103894>. Wouters, S., Da Silva, A.-C., Boulvain, F., and Devleeschouwer, X. (2021) <doi:10.32614/RJ-2021-039>. Huang, Norden E., Zhaohua Wu, Steven R. Long, Kenneth C. Arnold, Xianyao Chen, and Karin Blank  (2009) <doi:10.1142/S1793536909000096>. Cleveland, W. S. (1979)<doi:10.1080/01621459.1979.10481038> Hurvich, C.M., Simonoff, J.S., and Tsai, C.L. (1998) <doi:10.1111/1467-9868.00125>, Golub, G., Heath, M. and Wahba, G. (1979) <doi:10.2307/1268518>. References for the example data articles are: Damien Pas, Linda Hinnov, James E. (Jed) Day, Kenneth Kodama, Matthias Sinnesael, Wei Liu (2018) <doi:10.1016/j.epsl.2018.02.010>. Steinhilber, Friedhelm, Abreu, Jacksiel, Beer, Juerg , Brunner, Irene, Christl, Marcus, Fischer, Hubertus, HeikkilA, U., Kubik,  Peter, Mann, Mathias, Mccracken, K. , Miller, Heinrich, Miyahara, Hiroko, Oerter, Hans , Wilhelms, Frank. (2012 <doi:10.1073/pnas.1118965109>. Christian Zeeden, Frederik Hilgen, Thomas Westerhold, Lucas Lourens, Ursula R\u00f6hl, Torsten Bickert (2013) <doi:10.1016/j.palaeo.2012.11.009>. References for the (continuous) wavelet transform articles are: Morlet, Jean, Georges Arens, Eliane Fourgeau, and Dominique Glard  (1982a) <doi:10.1190/1.1441328>. J. Morlet, G. Arens, E. Fourgeau, D. Giard (1982b) <doi:10.1190/1.1441329>. Torrence, C., and G. P. Compo (1998)<https://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf>, Gouhier TC, Grinsted A, Simko V (2021) <https://github.com/tgouhier/biwavelet>. Angi Roesch and Harald Schmidbauer (2018) <https://CRAN.R-project.org/package=WaveletComp>. Russell, Brian, and Jiajun Han (2016)<https://www.crewes.org/Documents/ResearchReports/2016/CRR201668.pdf>. Gabor, Dennis (1946) <http://genesis.eecg.toronto.edu/gabor1946.pdf>. J. Laskar, P. Robutel, F. Joutel, M. Gastineau, A.C.M. Correia, and B. Levrard, B. (2004) <doi:10.1051/0004-6361:20041335>. Laskar, J., Fienga, A., Gastineau, M., Manche, H. (2011a) <doi:10.1051/0004-6361/201116836>. References for the astronomical solutions articles are: Laskar, J., Gastineau, M., Delisle, J.-B., Farres, A., Fienga, A. (2011b <doi:10.1051/0004-6361/201117504>. J. Laskar (2019) <doi:10.1016/B978-0-12-824360-2.00004-8>. Zeebe, Richard E (2017) <doi:10.3847/1538-3881/aa8cce>. Zeebe, R. E. and Lourens, L. J. (2019) <doi:10.1016/j.epsl.2022.117595>. Richard E. Zeebe Lucas J. Lourens (2022) <doi:10.1126/science.aax0612>.",
    "version": "0.4.1",
    "maintainer": "Michiel Arts <michiel.arts@stratigraphy.eu>",
    "author": "Michiel Arts [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3181-4608>)",
    "url": "https://github.com/stratigraphy/WaverideR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WaverideR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WaverideR Extracting Signals from Wavelet Spectra The continuous wavelet transform enables the observation of transient/non-stationary cyclicity in time-series. The goal of cyclostratigraphic studies is to define frequency/period in the depth/time domain. By conducting the continuous wavelet transform on cyclostratigraphic data series one can observe and extract cyclic signals/signatures from signals. These results can then be visualized and interpreted enabling one to identify/interpret cyclicity in the geological record, which can be used to construct astrochronological age-models and identify and interpret cyclicity in past and present climate systems. The 'WaverideR' R package builds upon existing literature and existing codebase. The list of articles which are relevant can be grouped in four subjects; cyclostratigraphic data analysis,example data sets,the (continuous) wavelet transform and astronomical solutions. References for the cyclostratigraphic data analysis articles are: Stephen Meyers (2019) <doi:10.1016/j.earscirev.2018.11.015>. Mingsong Li, Linda Hinnov, Lee Kump (2019) <doi:10.1016/j.cageo.2019.02.011> Stephen Meyers (2012)<doi:10.1029/2012PA002307> Mingsong Li, Lee R. Kump, Linda A. Hinnov, Michael E. Mann (2018) <doi:10.1016/j.epsl.2018.08.041>. Wouters, S., Crucifix, M., Sinnesael, M., Da Silva, A.C., Zeeden, C., Zivanovic, M., Boulvain, F., Devleeschouwer, X. (2022) <doi:10.1016/j.earscirev.2021.103894>. Wouters, S., Da Silva, A.-C., Boulvain, F., and Devleeschouwer, X. (2021) <doi:10.32614/RJ-2021-039>. Huang, Norden E., Zhaohua Wu, Steven R. Long, Kenneth C. Arnold, Xianyao Chen, and Karin Blank  (2009) <doi:10.1142/S1793536909000096>. Cleveland, W. S. (1979)<doi:10.1080/01621459.1979.10481038> Hurvich, C.M., Simonoff, J.S., and Tsai, C.L. (1998) <doi:10.1111/1467-9868.00125>, Golub, G., Heath, M. and Wahba, G. (1979) <doi:10.2307/1268518>. References for the example data articles are: Damien Pas, Linda Hinnov, James E. (Jed) Day, Kenneth Kodama, Matthias Sinnesael, Wei Liu (2018) <doi:10.1016/j.epsl.2018.02.010>. Steinhilber, Friedhelm, Abreu, Jacksiel, Beer, Juerg , Brunner, Irene, Christl, Marcus, Fischer, Hubertus, HeikkilA, U., Kubik,  Peter, Mann, Mathias, Mccracken, K. , Miller, Heinrich, Miyahara, Hiroko, Oerter, Hans , Wilhelms, Frank. (2012 <doi:10.1073/pnas.1118965109>. Christian Zeeden, Frederik Hilgen, Thomas Westerhold, Lucas Lourens, Ursula R\u00f6hl, Torsten Bickert (2013) <doi:10.1016/j.palaeo.2012.11.009>. References for the (continuous) wavelet transform articles are: Morlet, Jean, Georges Arens, Eliane Fourgeau, and Dominique Glard  (1982a) <doi:10.1190/1.1441328>. J. Morlet, G. Arens, E. Fourgeau, D. Giard (1982b) <doi:10.1190/1.1441329>. Torrence, C., and G. P. Compo (1998)<https://paos.colorado.edu/research/wavelets/bams_79_01_0061.pdf>, Gouhier TC, Grinsted A, Simko V (2021) <https://github.com/tgouhier/biwavelet>. Angi Roesch and Harald Schmidbauer (2018) <https://CRAN.R-project.org/package=WaveletComp>. Russell, Brian, and Jiajun Han (2016)<https://www.crewes.org/Documents/ResearchReports/2016/CRR201668.pdf>. Gabor, Dennis (1946) <http://genesis.eecg.toronto.edu/gabor1946.pdf>. J. Laskar, P. Robutel, F. Joutel, M. Gastineau, A.C.M. Correia, and B. Levrard, B. (2004) <doi:10.1051/0004-6361:20041335>. Laskar, J., Fienga, A., Gastineau, M., Manche, H. (2011a) <doi:10.1051/0004-6361/201116836>. References for the astronomical solutions articles are: Laskar, J., Gastineau, M., Delisle, J.-B., Farres, A., Fienga, A. (2011b <doi:10.1051/0004-6361/201117504>. J. Laskar (2019) <doi:10.1016/B978-0-12-824360-2.00004-8>. Zeebe, Richard E (2017) <doi:10.3847/1538-3881/aa8cce>. Zeebe, R. E. and Lourens, L. J. (2019) <doi:10.1016/j.epsl.2022.117595>. Richard E. Zeebe Lucas J. Lourens (2022) <doi:10.1126/science.aax0612>.  "
  },
  {
    "id": 8055,
    "package_name": "WebGestaltR",
    "title": "Gene Set Analysis Toolkit WebGestaltR",
    "description": "The web version WebGestalt <https://www.webgestalt.org> supports 12 organisms, 354 gene identifiers and 321,251 function categories. Users can upload the data and functional categories with their own gene identifiers. In addition to the Over-Representation Analysis, WebGestalt also supports Gene Set Enrichment Analysis and Network Topology Analysis. The user-friendly output report allows interactive and efficient exploration of enrichment results. The WebGestaltR package not only supports all above functions but also can be integrated into other pipeline or simultaneously analyze multiple gene lists.",
    "version": "0.4.6",
    "maintainer": "Yuxing Liao <yuxingliao@gmail.com>",
    "author": "Jing Wang [aut],\n  Yuxing Liao [aut, cre],\n  Eric Jaehnig [ctb],\n  Zhiao Shi [ctb],\n  Quanhu Sheng [ctb]",
    "url": "https://github.com/bzhanglab/WebGestaltR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WebGestaltR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WebGestaltR Gene Set Analysis Toolkit WebGestaltR The web version WebGestalt <https://www.webgestalt.org> supports 12 organisms, 354 gene identifiers and 321,251 function categories. Users can upload the data and functional categories with their own gene identifiers. In addition to the Over-Representation Analysis, WebGestalt also supports Gene Set Enrichment Analysis and Network Topology Analysis. The user-friendly output report allows interactive and efficient exploration of enrichment results. The WebGestaltR package not only supports all above functions but also can be integrated into other pipeline or simultaneously analyze multiple gene lists.  "
  },
  {
    "id": 8069,
    "package_name": "WeightedTreemaps",
    "title": "Generate and Plot Voronoi or Sunburst Treemaps from Hierarchical\nData",
    "description": "Treemaps are a visually appealing graphical representation of\n    numerical data using a space-filling approach. A plane or 'map' is\n    subdivided into smaller areas called cells.  The cells in the map are\n    scaled according to an underlying metric which allows to grasp the\n    hierarchical organization and relative importance of many objects at\n    once. This package contains two different implementations of treemaps,\n    Voronoi treemaps and Sunburst treemaps.  The Voronoi treemap function\n    subdivides the plot area in polygonal cells according to the highest\n    hierarchical level, then continues to subdivide those parental cells\n    on the next lower hierarchical level, and so on. The Sunburst treemap\n    is a computationally less demanding treemap that does not require\n    iterative refinement, but simply generates circle sectors that are\n    sized according to predefined weights.  The Voronoi tesselation is\n    based on functions from Paul Murrell (2012)\n    <https://www.stat.auckland.ac.nz/~paul/Reports/VoronoiTreemap/voronoiTreeMap.html>.",
    "version": "0.1.4",
    "maintainer": "Michael Jahn <jahn@mpusp.mpg.de>",
    "author": "Michael Jahn [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3913-153X>),\n  David Leslie [aut],\n  Ahmadou Dicko [aut] (ORCID: <https://orcid.org/0000-0002-9654-7582>),\n  Dunipace Eric [aut] (ORCID: <https://orcid.org/0000-0001-8909-213X>),\n  Paul Murrell [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-3224-8858>)",
    "url": "https://github.com/m-jahn/WeightedTreemaps",
    "bug_reports": "https://github.com/m-jahn/WeightedTreemaps/issues",
    "repository": "https://cran.r-project.org/package=WeightedTreemaps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WeightedTreemaps Generate and Plot Voronoi or Sunburst Treemaps from Hierarchical\nData Treemaps are a visually appealing graphical representation of\n    numerical data using a space-filling approach. A plane or 'map' is\n    subdivided into smaller areas called cells.  The cells in the map are\n    scaled according to an underlying metric which allows to grasp the\n    hierarchical organization and relative importance of many objects at\n    once. This package contains two different implementations of treemaps,\n    Voronoi treemaps and Sunburst treemaps.  The Voronoi treemap function\n    subdivides the plot area in polygonal cells according to the highest\n    hierarchical level, then continues to subdivide those parental cells\n    on the next lower hierarchical level, and so on. The Sunburst treemap\n    is a computationally less demanding treemap that does not require\n    iterative refinement, but simply generates circle sectors that are\n    sized according to predefined weights.  The Voronoi tesselation is\n    based on functions from Paul Murrell (2012)\n    <https://www.stat.auckland.ac.nz/~paul/Reports/VoronoiTreemap/voronoiTreeMap.html>.  "
  },
  {
    "id": 8073,
    "package_name": "WhiteLabRt",
    "title": "Novel Methods for Reproduction Number Estimation,\nBack-Calculation, and Forecasting",
    "description": "A collection of functions related to novel methods for estimating R(t), \n  created by the lab of Professor Laura White. Currently implemented methods include \n  two-step Bayesian back-calculation and now-casting for line-list data with missing reporting delays, \n  adapted in 'STAN' from Li (2021) <doi:10.1371/journal.pcbi.1009210>, \n  and calculation of time-varying reproduction number assuming a flux between various adjacent states, adapted into 'STAN' from \n  Zhou (2021) <doi:10.1371/journal.pcbi.1010434>.",
    "version": "1.0.1",
    "maintainer": "Chad Milando <cmilando@bu.edu>",
    "author": "Chad Milando [aut, cre],\n  Tenglong Li [ctb],\n  Zhenwei Zhou [ctb],\n  Laura White [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WhiteLabRt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WhiteLabRt Novel Methods for Reproduction Number Estimation,\nBack-Calculation, and Forecasting A collection of functions related to novel methods for estimating R(t), \n  created by the lab of Professor Laura White. Currently implemented methods include \n  two-step Bayesian back-calculation and now-casting for line-list data with missing reporting delays, \n  adapted in 'STAN' from Li (2021) <doi:10.1371/journal.pcbi.1009210>, \n  and calculation of time-varying reproduction number assuming a flux between various adjacent states, adapted into 'STAN' from \n  Zhou (2021) <doi:10.1371/journal.pcbi.1010434>.  "
  },
  {
    "id": 8086,
    "package_name": "WordR",
    "title": "Rendering Word Documents with R Inline Code",
    "description": "Serves for rendering MS Word documents with R inline code and inserting tables and plots.",
    "version": "0.3.6",
    "maintainer": "Tomas Hovorka <tomashovorka@seznam.cz>",
    "author": "Tomas Hovorka",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WordR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WordR Rendering Word Documents with R Inline Code Serves for rendering MS Word documents with R inline code and inserting tables and plots.  "
  },
  {
    "id": 8095,
    "package_name": "WufooR",
    "title": "R Wrapper for the 'Wufoo.com' - The Form Building Service",
    "description": "Allows form managers to download entries from their respondents\n    using Wufoo JSON API (<https://www.wufoo.com>). Additionally, the Wufoo reports - when public - can be\n    also acquired programmatically. Note that building new forms within this package\n    is not supported.",
    "version": "1.0.1",
    "maintainer": "John Malc <cincenko@outlook.com>",
    "author": "John Malc [aut, cre] (@dmpe),\n  Maksim Pecherskiy [ctb] (@MrMaksimize)",
    "url": "https://github.com/dmpe/wufoor",
    "bug_reports": "https://github.com/dmpe/wufoor/issues",
    "repository": "https://cran.r-project.org/package=WufooR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WufooR R Wrapper for the 'Wufoo.com' - The Form Building Service Allows form managers to download entries from their respondents\n    using Wufoo JSON API (<https://www.wufoo.com>). Additionally, the Wufoo reports - when public - can be\n    also acquired programmatically. Note that building new forms within this package\n    is not supported.  "
  },
  {
    "id": 8101,
    "package_name": "XML",
    "title": "Tools for Parsing and Generating XML Within R and S-Plus",
    "description": "Many approaches for both reading and\n        creating XML (and HTML) documents (including DTDs), both local\n        and accessible via HTTP or FTP.  Also offers access to an\n        'XPath' \"interpreter\".",
    "version": "3.99-0.20",
    "maintainer": "CRAN Team <CRAN@r-project.org>",
    "author": "CRAN Team [ctb, cre] (de facto maintainer since 2013),\n  Duncan Temple Lang [aut] (ORCID:\n    <https://orcid.org/0000-0003-0159-1546>),\n  Tomas Kalibera [ctb],\n  Ivan Krylov [ctb]",
    "url": "https://www.omegahat.net/RSXML/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=XML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "XML Tools for Parsing and Generating XML Within R and S-Plus Many approaches for both reading and\n        creating XML (and HTML) documents (including DTDs), both local\n        and accessible via HTTP or FTP.  Also offers access to an\n        'XPath' \"interpreter\".  "
  },
  {
    "id": 8109,
    "package_name": "Xplortext",
    "title": "Statistical Analysis of Textual Data",
    "description": "Provides a set of functions devoted to multivariate exploratory statistics on textual data. Classical methods such as correspondence analysis and agglomerative hierarchical clustering are available. Chronologically constrained agglomerative hierarchical clustering enriched with labelled-by-words trees is offered. Given a division of the corpus into parts, their characteristic words and documents are identified. Further, accessing to 'FactoMineR' functions is very easy. Two of them are relevant in textual domain. MFA() addresses multiple lexical table allowing applications such as dealing with multilingual corpora as well as simultaneously analyzing both open-ended and closed questions in surveys. See <http://xplortext.unileon.es> for examples.",
    "version": "1.5.5",
    "maintainer": "Ram\u00f3n Alvarez-Esteban <ramon.alvarez@unileon.es>",
    "author": "Ram\u00f3n Alvarez-Esteban [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4751-2797>),\n  M\u00f3nica B\u00e9cue-Bertaut [aut] (ORCID:\n    <https://orcid.org/0000-0002-6027-3655>),\n  Josep-Anton S\u00e1nchez-Espigares [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8195-1913>),\n  Belchin Adriyanov Kostov [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2126-3892>)",
    "url": "https://xplortext.unileon.es",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Xplortext",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Xplortext Statistical Analysis of Textual Data Provides a set of functions devoted to multivariate exploratory statistics on textual data. Classical methods such as correspondence analysis and agglomerative hierarchical clustering are available. Chronologically constrained agglomerative hierarchical clustering enriched with labelled-by-words trees is offered. Given a division of the corpus into parts, their characteristic words and documents are identified. Further, accessing to 'FactoMineR' functions is very easy. Two of them are relevant in textual domain. MFA() addresses multiple lexical table allowing applications such as dealing with multilingual corpora as well as simultaneously analyzing both open-ended and closed questions in surveys. See <http://xplortext.unileon.es> for examples.  "
  },
  {
    "id": 8141,
    "package_name": "ZillowR",
    "title": "R Interface to Zillow Real Estate and Mortgage Data API",
    "description": "Zillow, an online real estate company, provides real estate\n    and mortgage data for the United States through a REST API. The\n    ZillowR package provides an R function for each API service, making it\n    easy to make API calls and process the response into convenient,\n    R-friendly data structures.  See\n    <https://www.zillow.com/howto/api/APIOverview.htm> for the Zillow API\n    Documentation. NOTE: Zillow deprecated their API on 2021-09-30, and\n    this package is now deprecated as a result.",
    "version": "1.0.0",
    "maintainer": "Justin Brantley <fascinatingfingers@icloud.com>",
    "author": "Justin Brantley [aut, cre]",
    "url": "https://fascinatingfingers.gitlab.io/zillowr,\nhttps://gitlab.com/fascinatingfingers/zillowr",
    "bug_reports": "https://gitlab.com/fascinatingfingers/zillowr/-/issues",
    "repository": "https://cran.r-project.org/package=ZillowR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZillowR R Interface to Zillow Real Estate and Mortgage Data API Zillow, an online real estate company, provides real estate\n    and mortgage data for the United States through a REST API. The\n    ZillowR package provides an R function for each API service, making it\n    easy to make API calls and process the response into convenient,\n    R-friendly data structures.  See\n    <https://www.zillow.com/howto/api/APIOverview.htm> for the Zillow API\n    Documentation. NOTE: Zillow deprecated their API on 2021-09-30, and\n    this package is now deprecated as a result.  "
  },
  {
    "id": 8148,
    "package_name": "aNCA",
    "title": "(Pre-)Clinical NCA in a Dynamic Shiny App",
    "description": "An interactive 'shiny' application for performing non-compartmental analysis (NCA) on\n  pre-clinical and clinical pharmacokinetic data. The package builds on 'PKNCA' for core estimators and provides interactive visualizations, CDISC\n  outputs ('ADNCA', 'PP', 'ADPP') and configurable TLGs (tables, listings, and graphs). Typical use cases include exploratory\n  analysis, validation, reporting or teaching/demonstration of NCA methods. Methods and core\n  estimators are described in Denney, Duvvuri, and Buckeridge (2015) \"Simple, Automatic\n  Noncompartmental Analysis: The PKNCA R Package\" <doi:10.1007/s10928-015-9432-2>.",
    "version": "0.1.0",
    "maintainer": "Valentin Legras <anca.pharmaverse@gmail.com>",
    "author": "Ercan Suekuer [aut] (ORCID: <https://orcid.org/0009-0001-1626-1526>),\n  Gerardo Jose Rodriguez [aut] (ORCID:\n    <https://orcid.org/0000-0003-1413-0060>),\n  Pascal Baertschi [aut] (ORCID: <https://orcid.org/0000-0002-6533-0399>),\n  Jana Spinner [aut] (ORCID: <https://orcid.org/0009-0009-2197-9530>),\n  Mateusz Kolomanski [aut] (ORCID:\n    <https://orcid.org/0000-0001-7424-3919>),\n  Lucy Aspridis [aut] (ORCID: <https://orcid.org/0009-0003-1300-3622>),\n  Valentin Legras [aut, cre],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://pharmaverse.github.io/aNCA/,\nhttps://github.com/pharmaverse/aNCA",
    "bug_reports": "https://github.com/pharmaverse/aNCA/issues",
    "repository": "https://cran.r-project.org/package=aNCA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aNCA (Pre-)Clinical NCA in a Dynamic Shiny App An interactive 'shiny' application for performing non-compartmental analysis (NCA) on\n  pre-clinical and clinical pharmacokinetic data. The package builds on 'PKNCA' for core estimators and provides interactive visualizations, CDISC\n  outputs ('ADNCA', 'PP', 'ADPP') and configurable TLGs (tables, listings, and graphs). Typical use cases include exploratory\n  analysis, validation, reporting or teaching/demonstration of NCA methods. Methods and core\n  estimators are described in Denney, Duvvuri, and Buckeridge (2015) \"Simple, Automatic\n  Noncompartmental Analysis: The PKNCA R Package\" <doi:10.1007/s10928-015-9432-2>.  "
  },
  {
    "id": 8195,
    "package_name": "accessr",
    "title": "Command Line Tools to Produce Accessible Documents using 'R\nMarkdown'",
    "description": "Provides functions to produce accessible 'HTML' slides, 'HTML', \n    'Word' and 'PDF' documents from input 'R markdown' files. Accessible 'PDF' \n    files are produced only on a 'Windows' Operating System. One aspect of \n    accessibility is providing a headings structure that is recognised by a \n    screen reader, providing a navigational tool for a blind or \n    partially-sighted person. A key aim is to produce documents of different \n    formats easily from each of a collection of 'R markdown' source files. \n    Input 'R markdown' files are rendered using the render() function from the \n    'rmarkdown' package <https://cran.r-project.org/package=rmarkdown>. A 'zip' \n    file containing multiple output files can be produced from one function \n    call. A user-supplied template 'Word' document can be used to determine the \n    formatting of an output 'Word' document. Accessible 'PDF' files are \n    produced from 'Word' documents using 'OfficeToPDF' \n    <https://github.com/cognidox/OfficeToPDF>. A convenience function, \n    install_otp() is provided to install this software. The option to print \n    'HTML' output to (non-accessible) 'PDF' files is also available.  ",
    "version": "1.1.3",
    "maintainer": "Paul J. Northrop <p.northrop@ucl.ac.uk>",
    "author": "Paul J. Northrop [aut, cre, cph]",
    "url": "https://paulnorthrop.github.io/accessr/,\nhttps://github.com/paulnorthrop/accessr",
    "bug_reports": "https://github.com/paulnorthrop/accessr/issues",
    "repository": "https://cran.r-project.org/package=accessr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "accessr Command Line Tools to Produce Accessible Documents using 'R\nMarkdown' Provides functions to produce accessible 'HTML' slides, 'HTML', \n    'Word' and 'PDF' documents from input 'R markdown' files. Accessible 'PDF' \n    files are produced only on a 'Windows' Operating System. One aspect of \n    accessibility is providing a headings structure that is recognised by a \n    screen reader, providing a navigational tool for a blind or \n    partially-sighted person. A key aim is to produce documents of different \n    formats easily from each of a collection of 'R markdown' source files. \n    Input 'R markdown' files are rendered using the render() function from the \n    'rmarkdown' package <https://cran.r-project.org/package=rmarkdown>. A 'zip' \n    file containing multiple output files can be produced from one function \n    call. A user-supplied template 'Word' document can be used to determine the \n    formatting of an output 'Word' document. Accessible 'PDF' files are \n    produced from 'Word' documents using 'OfficeToPDF' \n    <https://github.com/cognidox/OfficeToPDF>. A convenience function, \n    install_otp() is provided to install this software. The option to print \n    'HTML' output to (non-accessible) 'PDF' files is also available.    "
  },
  {
    "id": 8196,
    "package_name": "accessrmd",
    "title": "Improving the Accessibility of 'rmarkdown' Documents",
    "description": "Provides a simple method to improve the accessibility of\n    'rmarkdown' documents. The package provides functions for creating or\n    modifying 'rmarkdown' documents, resolving known errors and alerts that\n    result in accessibility issues for screen reader users.",
    "version": "1.0.0",
    "maintainer": "Rich Leyshon <richard.leyshon@ons.gov.uk>",
    "author": "Rich Leyshon [aut, cre],\n  Crown Copyright 2021 [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=accessrmd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "accessrmd Improving the Accessibility of 'rmarkdown' Documents Provides a simple method to improve the accessibility of\n    'rmarkdown' documents. The package provides functions for creating or\n    modifying 'rmarkdown' documents, resolving known errors and alerts that\n    result in accessibility issues for screen reader users.  "
  },
  {
    "id": 8226,
    "package_name": "activAnalyzer",
    "title": "A 'Shiny' App to Analyze Accelerometer-Measured Daily Physical\nBehavior Data",
    "description": "A tool to analyse 'ActiGraph' accelerometer data and to implement \n    the use of the PROactive Physical Activity in COPD (chronic obstructive pulmonary disease) instruments. Once analysis\n    is completed, the app allows to export results to .csv files and to generate\n    a report of the measurement. All the configured inputs relevant for interpreting\n    the results are recorded in the report. In addition to the existing 'R' packages \n    that are fully integrated with the app, the app uses some functions from the \n    'actigraph.sleepr' package developed by Petkova (2021) <https://github.com/dipetkov/actigraph.sleepr/>.",
    "version": "2.1.2",
    "maintainer": "Pierre-Yves de M\u00fcllenheim <pydemull@uco.fr>",
    "author": "Pierre-Yves de M\u00fcllenheim [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-9157-7371>)",
    "url": "https://pydemull.github.io/activAnalyzer/,\nhttps://github.com/pydemull/activAnalyzer",
    "bug_reports": "https://github.com/pydemull/activAnalyzer/issues",
    "repository": "https://cran.r-project.org/package=activAnalyzer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "activAnalyzer A 'Shiny' App to Analyze Accelerometer-Measured Daily Physical\nBehavior Data A tool to analyse 'ActiGraph' accelerometer data and to implement \n    the use of the PROactive Physical Activity in COPD (chronic obstructive pulmonary disease) instruments. Once analysis\n    is completed, the app allows to export results to .csv files and to generate\n    a report of the measurement. All the configured inputs relevant for interpreting\n    the results are recorded in the report. In addition to the existing 'R' packages \n    that are fully integrated with the app, the app uses some functions from the \n    'actigraph.sleepr' package developed by Petkova (2021) <https://github.com/dipetkov/actigraph.sleepr/>.  "
  },
  {
    "id": 8235,
    "package_name": "actuaryr",
    "title": "Develop Actuarial Models",
    "description": "Actuarial reports are prepared for the last day of a specific \n    period, such as a month, a quarter or a year. Actuarial models assume that \n    certain events happen at the beginning or end of periods. The package \n    contains functions to easily refer to the first or last (working) day \n    within a specific period relative to a base date to facilitate actuarial \n    reporting and to compare results.",
    "version": "1.1.1",
    "maintainer": "Zuzanna Chmielewska <zchmielewska@gmail.com>",
    "author": "Zuzanna Chmielewska [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7622-3321>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=actuaryr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "actuaryr Develop Actuarial Models Actuarial reports are prepared for the last day of a specific \n    period, such as a month, a quarter or a year. Actuarial models assume that \n    certain events happen at the beginning or end of periods. The package \n    contains functions to easily refer to the first or last (working) day \n    within a specific period relative to a base date to facilitate actuarial \n    reporting and to compare results.  "
  },
  {
    "id": 8236,
    "package_name": "actxps",
    "title": "Create Actuarial Experience Studies: Prepare Data, Summarize\nResults, and Create Reports",
    "description": "Experience studies are used by actuaries to explore historical\n    experience across blocks of business and to inform assumption setting\n    activities. This package provides functions for preparing data, creating \n    studies, visualizing results, and beginning assumption development. \n    Experience study methods, including exposure calculations, are described in:\n    Atkinson & McGarry (2016) \"Experience Study Calculations\" \n    <https://www.soa.org/49378a/globalassets/assets/files/research/experience-study-calculations.pdf>.\n    The limited fluctuation credibility method used by the 'exp_stats()'\n    function is described in: Herzog (1999, ISBN:1-56698-374-6) \n    \"Introduction to Credibility Theory\".",
    "version": "1.6.1",
    "maintainer": "Matt Heaphy <mattrmattrs@gmail.com>",
    "author": "Matt Heaphy [aut, cre]",
    "url": "https://github.com/mattheaphy/actxps/,\nhttps://mattheaphy.github.io/actxps/",
    "bug_reports": "https://github.com/mattheaphy/actxps/issues",
    "repository": "https://cran.r-project.org/package=actxps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "actxps Create Actuarial Experience Studies: Prepare Data, Summarize\nResults, and Create Reports Experience studies are used by actuaries to explore historical\n    experience across blocks of business and to inform assumption setting\n    activities. This package provides functions for preparing data, creating \n    studies, visualizing results, and beginning assumption development. \n    Experience study methods, including exposure calculations, are described in:\n    Atkinson & McGarry (2016) \"Experience Study Calculations\" \n    <https://www.soa.org/49378a/globalassets/assets/files/research/experience-study-calculations.pdf>.\n    The limited fluctuation credibility method used by the 'exp_stats()'\n    function is described in: Herzog (1999, ISBN:1-56698-374-6) \n    \"Introduction to Credibility Theory\".  "
  },
  {
    "id": 8238,
    "package_name": "ada",
    "title": "The R Package Ada for Stochastic Boosting",
    "description": "Performs discrete, real, and gentle boost under both exponential and \n             logistic loss on a given data set.  The package ada provides a straightforward, \n             well-documented, and broad boosting routine for classification, ideally suited \n             for small to moderate-sized data sets.",
    "version": "2.0-5",
    "maintainer": "Mark Culp <mvculp@mail.wvu.edu>",
    "author": "Mark Culp, Kjell Johnson, and George Michailidis",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ada",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ada The R Package Ada for Stochastic Boosting Performs discrete, real, and gentle boost under both exponential and \n             logistic loss on a given data set.  The package ada provides a straightforward, \n             well-documented, and broad boosting routine for classification, ideally suited \n             for small to moderate-sized data sets.  "
  },
  {
    "id": 8266,
    "package_name": "adcontabil",
    "title": "Accounting Analysis",
    "description": "Provides methods for processing corporate balance sheets with a focus on the Brazilian reporting format. Includes data standardization, classification by accounting categories, and aggregation of values. Supports accounting and financial analyses of companies, improving efficiency and ensuring reproducibility of empirical studies.",
    "version": "1.1.8",
    "maintainer": "Lissandro Costa de Sousa <lisandrosousa54@gmail.com>",
    "author": "Lissandro Costa de Sousa [cre, aut],\n  Francisco Gildemir Ferreira da Silva [ths, aut]",
    "url": "https://github.com/LissandroSousa/adcontabil.R",
    "bug_reports": "https://github.com/LissandroSousa/adcontabil.R/issues",
    "repository": "https://cran.r-project.org/package=adcontabil",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adcontabil Accounting Analysis Provides methods for processing corporate balance sheets with a focus on the Brazilian reporting format. Includes data standardization, classification by accounting categories, and aggregation of values. Supports accounting and financial analyses of companies, improving efficiency and ensuring reproducibility of empirical studies.  "
  },
  {
    "id": 8271,
    "package_name": "addinsOutline",
    "title": "'RStudio' Addins for Show Outline of a R Markdown/'LaTeX'\nProject",
    "description": "'RStudio' allows to show and navigate for the outline of a \n    R Markdown file, but not for R Markdown projects with multiple \n    files. For this reason, I have developed several 'RStudio' addins capable \n    of show project outline. Each addin is specialized in showing projects \n    of different types: R Markdown project, 'bookdown' package project \n    and 'LaTeX' project. There is a configuration file that allows you \n    to customize additional searches.",
    "version": "0.1.6",
    "maintainer": "Pedro L. Luque-Calvo <calvo@us.es>",
    "author": "Pedro L. Luque-Calvo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6660-5326>)",
    "url": "https://github.com/calote/addinsOutline",
    "bug_reports": "https://github.com/calote/addinsOutline/issues",
    "repository": "https://cran.r-project.org/package=addinsOutline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "addinsOutline 'RStudio' Addins for Show Outline of a R Markdown/'LaTeX'\nProject 'RStudio' allows to show and navigate for the outline of a \n    R Markdown file, but not for R Markdown projects with multiple \n    files. For this reason, I have developed several 'RStudio' addins capable \n    of show project outline. Each addin is specialized in showing projects \n    of different types: R Markdown project, 'bookdown' package project \n    and 'LaTeX' project. There is a configuration file that allows you \n    to customize additional searches.  "
  },
  {
    "id": 8293,
    "package_name": "adheRenceRX",
    "title": "Assess Medication Adherence from Pharmaceutical Claims Data",
    "description": "A (mildly) opinionated set of functions to help assess medication adherence for researchers working with medication claims data.\n    Medication adherence analyses have several complex steps that are often convoluted and can be time-intensive. The focus is to create a \n    set of functions using \"tidy principles\" geared towards transparency, speed, and flexibility while working with adherence metrics. All functions perform exactly one task \n    with an intuitive name so that a researcher can handle details (often achieved with vectorized solutions) while we handle non-vectorized tasks common to most \n    adherence calculations such as adjusting fill dates and determining episodes of care. The methodologies in referenced in this package come from\n    Canfield SL, et al (2019) \"Navigating the Wild West of Medication Adherence Reporting in Specialty Pharmacy\" <doi:10.18553/jmcp.2019.25.10.1073>.",
    "version": "1.0.0",
    "maintainer": "Brennan Beal <brennanbeal@gmail.com>",
    "author": "Brennan Beal [aut, cre]",
    "url": "https://github.com/btbeal/adheRenceRX",
    "bug_reports": "https://github.com/btbeal/adheRenceRX/issues",
    "repository": "https://cran.r-project.org/package=adheRenceRX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adheRenceRX Assess Medication Adherence from Pharmaceutical Claims Data A (mildly) opinionated set of functions to help assess medication adherence for researchers working with medication claims data.\n    Medication adherence analyses have several complex steps that are often convoluted and can be time-intensive. The focus is to create a \n    set of functions using \"tidy principles\" geared towards transparency, speed, and flexibility while working with adherence metrics. All functions perform exactly one task \n    with an intuitive name so that a researcher can handle details (often achieved with vectorized solutions) while we handle non-vectorized tasks common to most \n    adherence calculations such as adjusting fill dates and determining episodes of care. The methodologies in referenced in this package come from\n    Canfield SL, et al (2019) \"Navigating the Wild West of Medication Adherence Reporting in Specialty Pharmacy\" <doi:10.18553/jmcp.2019.25.10.1073>.  "
  },
  {
    "id": 8305,
    "package_name": "admiraldev",
    "title": "Utility Functions and Development Tools for the Admiral Package\nFamily",
    "description": "Utility functions to check data, variables and conditions for\n    functions used in 'admiral' and 'admiral' extension packages.\n    Additional utility helper functions to assist developers with\n    maintaining documentation, testing and general upkeep of 'admiral' and\n    'admiral' extension packages.",
    "version": "1.3.1",
    "maintainer": "Ben Straub <ben.x.straub@gsk.com>",
    "author": "Ben Straub [aut, cre],\n  Stefan Bundfuss [aut] (ORCID: <https://orcid.org/0009-0005-0027-1198>),\n  Jeffrey Dickinson [aut],\n  Ross Farrugia [aut],\n  Fanny Gautier [aut],\n  Edoardo Mancini [aut] (ORCID: <https://orcid.org/0009-0006-4899-8641>),\n  Gordon Miller [aut],\n  Daniel Sjoberg [aut] (ORCID: <https://orcid.org/0000-0003-0862-2018>),\n  Stefan Thoma [aut] (ORCID: <https://orcid.org/0000-0002-5553-9252>),\n  F. Hoffmann-La Roche AG [cph, fnd],\n  GlaxoSmithKline LLC [cph, fnd]",
    "url": "https://pharmaverse.github.io/admiraldev/,\nhttps://github.com/pharmaverse/admiraldev/",
    "bug_reports": "https://github.com/pharmaverse/admiraldev/issues",
    "repository": "https://cran.r-project.org/package=admiraldev",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiraldev Utility Functions and Development Tools for the Admiral Package\nFamily Utility functions to check data, variables and conditions for\n    functions used in 'admiral' and 'admiral' extension packages.\n    Additional utility helper functions to assist developers with\n    maintaining documentation, testing and general upkeep of 'admiral' and\n    'admiral' extension packages.  "
  },
  {
    "id": 8332,
    "package_name": "adwordsR",
    "title": "Access the 'Google Adwords' API",
    "description": "Allows access to selected services that are part of the \n    'Google Adwords' API <https://developers.google.com/adwords/api/docs/guides/start>.\n\t'Google Adwords' is an online advertising service by 'Google', that delivers Ads \n\tto users. This package offers a authentication process using 'OAUTH2'.  Currently, \n\tthere are two methods of data of accessing the API, depending on the type of \n\trequest. One method uses 'SOAP' requests which require building an 'XML' structure \n\tand then sent to the API. These are used for the 'ManagedCustomerService' and \n\tthe 'TargetingIdeaService'. The second method is by building 'AWQL' queries for \n\tthe reporting side of the 'Google Adwords' API.",
    "version": "0.3.1",
    "maintainer": "Sean Longthorpe <sean.longthorpe@branded3.com>",
    "author": "Sean Longthorpe [aut, cre, cph],\n  Johannes Burkhardt [ctb, cph]",
    "url": "https://www.branded3.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=adwordsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adwordsR Access the 'Google Adwords' API Allows access to selected services that are part of the \n    'Google Adwords' API <https://developers.google.com/adwords/api/docs/guides/start>.\n\t'Google Adwords' is an online advertising service by 'Google', that delivers Ads \n\tto users. This package offers a authentication process using 'OAUTH2'.  Currently, \n\tthere are two methods of data of accessing the API, depending on the type of \n\trequest. One method uses 'SOAP' requests which require building an 'XML' structure \n\tand then sent to the API. These are used for the 'ManagedCustomerService' and \n\tthe 'TargetingIdeaService'. The second method is by building 'AWQL' queries for \n\tthe reporting side of the 'Google Adwords' API.  "
  },
  {
    "id": 8378,
    "package_name": "agrobox",
    "title": "Data Visualization and Statistical Tools for Agroindustrial\nExperiments",
    "description": "Set of functions to create clear graphics and run common statistical analyses for agricultural experiments (ANOVA with post-hoc tests such as Tukey HSD and Duncan MRR, coefficient of variation, and simple power calculations), streamlining exploratory analysis and reporting. Functions build on 'ggplot2' and base 'stats' and follow methods widely used in agronomy (field trials, plant breeding). Key references include Tukey (1949) <doi:10.2307/3001913>, Duncan (1955) <doi:10.2307/3001478>, Cohen (1988, ISBN:9781138892899); see also 'agricolae' <https://CRAN.R-project.org/package=agricolae> and Wickham (2016, ISBN:9783319242750) for 'ggplot2'. Versi\u00f3n en espa\u00f1ol: Conjunto de funciones para generar gr\u00e1ficos claros y ejecutar an\u00e1lisis habituales en ensayos agr\u00edcolas (ANOVA con pruebas post-hoc como Tukey HSD y Duncan MRR, coeficiente de variaci\u00f3n y c\u00e1lculos simples de potencia), facilitando el an\u00e1lisis exploratorio y la elaboraci\u00f3n de reportes. Los m\u00e9todos implementados se basan en Tukey (1949) <doi:10.2307/3001913>, Duncan (1955) <doi:10.2307/3001478> y Cohen (1988, ISBN:9781138892899); ver tambi\u00e9n 'agricolae' <https://CRAN.R-project.org/package=agricolae> y Wickham (2016, ISBN:9783319242750) para 'ggplot2'.",
    "version": "0.1.0",
    "maintainer": "Joaquin Alejandro Salinas Angeles <joaquinsa03@gmail.com>",
    "author": "Joaquin Alejandro Salinas Angeles [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-6174-9714>)",
    "url": "https://github.com/Joa3aquin50/agrobox",
    "bug_reports": "https://github.com/Joa3aquin50/agrobox/issues",
    "repository": "https://cran.r-project.org/package=agrobox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "agrobox Data Visualization and Statistical Tools for Agroindustrial\nExperiments Set of functions to create clear graphics and run common statistical analyses for agricultural experiments (ANOVA with post-hoc tests such as Tukey HSD and Duncan MRR, coefficient of variation, and simple power calculations), streamlining exploratory analysis and reporting. Functions build on 'ggplot2' and base 'stats' and follow methods widely used in agronomy (field trials, plant breeding). Key references include Tukey (1949) <doi:10.2307/3001913>, Duncan (1955) <doi:10.2307/3001478>, Cohen (1988, ISBN:9781138892899); see also 'agricolae' <https://CRAN.R-project.org/package=agricolae> and Wickham (2016, ISBN:9783319242750) for 'ggplot2'. Versi\u00f3n en espa\u00f1ol: Conjunto de funciones para generar gr\u00e1ficos claros y ejecutar an\u00e1lisis habituales en ensayos agr\u00edcolas (ANOVA con pruebas post-hoc como Tukey HSD y Duncan MRR, coeficiente de variaci\u00f3n y c\u00e1lculos simples de potencia), facilitando el an\u00e1lisis exploratorio y la elaboraci\u00f3n de reportes. Los m\u00e9todos implementados se basan en Tukey (1949) <doi:10.2307/3001913>, Duncan (1955) <doi:10.2307/3001478> y Cohen (1988, ISBN:9781138892899); ver tambi\u00e9n 'agricolae' <https://CRAN.R-project.org/package=agricolae> y Wickham (2016, ISBN:9783319242750) para 'ggplot2'.  "
  },
  {
    "id": 8422,
    "package_name": "alfr",
    "title": "Connectivity to 'Alfresco' Content Management Repositories",
    "description": "Allows you to connect to an 'Alfresco' content management repository and interact\n  with its contents using simple and intuitive functions.  You will be able to establish a connection session to the 'Alfresco' repository,\n  read and upload content and manage folder hierarchies.  For more details on the 'Alfresco' content management repository\n  see <https://www.alfresco.com/ecm-software/document-management>.",
    "version": "1.2.1",
    "maintainer": "Roy Wetherall <rwetherall@gmail.com>",
    "author": "Roy Wetherall <rwetherall@gmail.com>",
    "url": "https://github.com/rwetherall/alfr,\nhttps://rwetherall.github.io/alfr/",
    "bug_reports": "https://github.com/rwetherall/alfr/issues",
    "repository": "https://cran.r-project.org/package=alfr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "alfr Connectivity to 'Alfresco' Content Management Repositories Allows you to connect to an 'Alfresco' content management repository and interact\n  with its contents using simple and intuitive functions.  You will be able to establish a connection session to the 'Alfresco' repository,\n  read and upload content and manage folder hierarchies.  For more details on the 'Alfresco' content management repository\n  see <https://www.alfresco.com/ecm-software/document-management>.  "
  },
  {
    "id": 8433,
    "package_name": "allelematch",
    "title": "Identifying Unique Multilocus Genotypes where Genotyping Error\nand Missing Data may be Present",
    "description": "Tools for the identification of unique of multilocus genotypes when both genotyping error and missing data may be present; targeted for use with large datasets and databases containing multiple samples of each individual (a common situation in conservation genetics, particularly in non-invasive wildlife sampling applications). Functions explicitly incorporate missing data and can tolerate allele mismatches created by genotyping error. If you use this package, please cite the original publication in Molecular Ecology Resources (Galpern et al., 2012), the details for which can be generated using citation('allelematch'). For a complete vignette, please access via the Data S1 Supplementary documentation and tutorials (PDF) located at <doi:10.1111/j.1755-0998.2012.03137.x>.",
    "version": "2.5.5",
    "maintainer": "Todd Cross <todd.cross@gmail.com>",
    "author": "Paul Galpern [aut],\n  Micheline Manseau [aut],\n  Pete Hettinga [aut],\n  Karen Smith [aut],\n  Paul Wilson [aut],\n  Todd Cross [cre]",
    "url": "<doi:10.1111%2Fj.1755-0998.2012.03137.x>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=allelematch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "allelematch Identifying Unique Multilocus Genotypes where Genotyping Error\nand Missing Data may be Present Tools for the identification of unique of multilocus genotypes when both genotyping error and missing data may be present; targeted for use with large datasets and databases containing multiple samples of each individual (a common situation in conservation genetics, particularly in non-invasive wildlife sampling applications). Functions explicitly incorporate missing data and can tolerate allele mismatches created by genotyping error. If you use this package, please cite the original publication in Molecular Ecology Resources (Galpern et al., 2012), the details for which can be generated using citation('allelematch'). For a complete vignette, please access via the Data S1 Supplementary documentation and tutorials (PDF) located at <doi:10.1111/j.1755-0998.2012.03137.x>.  "
  },
  {
    "id": 8458,
    "package_name": "altdoc",
    "title": "Package Documentation Websites with 'Quarto', 'Docsify',\n'Docute', or 'MkDocs'",
    "description": "This is a simple and powerful package to create, render, preview,\n    and deploy documentation websites for 'R' packages. It is a lightweight and\n    flexible alternative to 'pkgdown', with support for many documentation\n    generators, including 'Quarto', 'Docute', 'Docsify', and 'MkDocs'.",
    "version": "0.7.0",
    "maintainer": "Etienne Bacher <etienne.bacher@protonmail.com>",
    "author": "Etienne Bacher [aut, cre, cph],\n  Vincent Arel-Bundock [aut] (ORCID:\n    <https://orcid.org/0000-0003-2042-7063>)",
    "url": "https://altdoc.etiennebacher.com,\nhttps://github.com/etiennebacher/altdoc",
    "bug_reports": "https://github.com/etiennebacher/altdoc/issues",
    "repository": "https://cran.r-project.org/package=altdoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "altdoc Package Documentation Websites with 'Quarto', 'Docsify',\n'Docute', or 'MkDocs' This is a simple and powerful package to create, render, preview,\n    and deploy documentation websites for 'R' packages. It is a lightweight and\n    flexible alternative to 'pkgdown', with support for many documentation\n    generators, including 'Quarto', 'Docute', 'Docsify', and 'MkDocs'.  "
  },
  {
    "id": 8462,
    "package_name": "amVennDiagram5",
    "title": "Interactive Venn Diagrams",
    "description": "Creates interactive Venn diagrams using the 'amCharts5' library for \n    'JavaScript'. They can be used directly from the R console, from 'RStudio', \n    in 'shiny' applications, and in 'rmarkdown' documents.",
    "version": "1.0.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  amCharts team [cph]",
    "url": "https://github.com/stla/amVennDiagram5",
    "bug_reports": "https://github.com/stla/amVennDiagram5/issues",
    "repository": "https://cran.r-project.org/package=amVennDiagram5",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amVennDiagram5 Interactive Venn Diagrams Creates interactive Venn diagrams using the 'amCharts5' library for \n    'JavaScript'. They can be used directly from the R console, from 'RStudio', \n    in 'shiny' applications, and in 'rmarkdown' documents.  "
  },
  {
    "id": 8464,
    "package_name": "amanida",
    "title": "Meta-Analysis for Non-Integral Data",
    "description": "Combination of results for meta-analysis using significance\n    and effect size only. P-values and fold-change are combined to obtain\n    a global significance on each metabolite. Produces a volcano plot\n    summarising the relevant results from meta-analysis. Vote-counting\n    reports for metabolites. And explore plot to detect discrepancies\n    between studies at a first glance. Methodology is described in the\n    Llambrich et al. (2021) <doi:10.1093/bioinformatics/btab591>.  ",
    "version": "0.3.0",
    "maintainer": "Maria Llambrich <maria.llambrich@urv.cat>",
    "author": "Maria Llambrich [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8418-0982>),\n  Eudald Correig [aut],\n  Raquel Cumeras [aut]",
    "url": "https://github.com/mariallr/amanida",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=amanida",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amanida Meta-Analysis for Non-Integral Data Combination of results for meta-analysis using significance\n    and effect size only. P-values and fold-change are combined to obtain\n    a global significance on each metabolite. Produces a volcano plot\n    summarising the relevant results from meta-analysis. Vote-counting\n    reports for metabolites. And explore plot to detect discrepancies\n    between studies at a first glance. Methodology is described in the\n    Llambrich et al. (2021) <doi:10.1093/bioinformatics/btab591>.    "
  },
  {
    "id": 8472,
    "package_name": "amberr",
    "title": "'Amber' Electronic Data Capture Client",
    "description": "'Amber' is a server application for capturing electronic data records.\n    Rich forms are used to collect data. This 'Amber' client allows\n    to perform data extraction for reporting or data transfer at persistent location\n    purposes.",
    "version": "1.2.0",
    "maintainer": "Yannick Marcon <yannick.marcon@obiba.org>",
    "author": "Yannick Marcon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0138-2023>),\n  OBiBa group [cph]",
    "url": "https://github.com/obiba/amberr/, https://www.obiba.org/amberr/,\nhttps://www.obiba.org/pages/products/amber/",
    "bug_reports": "https://github.com/obiba/amberr/issues/",
    "repository": "https://cran.r-project.org/package=amberr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amberr 'Amber' Electronic Data Capture Client 'Amber' is a server application for capturing electronic data records.\n    Rich forms are used to collect data. This 'Amber' client allows\n    to perform data extraction for reporting or data transfer at persistent location\n    purposes.  "
  },
  {
    "id": 8485,
    "package_name": "amregtest",
    "title": "Runs Allelematch Regression Tests",
    "description": "Automates regression testing of package 'allelematch'. Over\n    2500 tests covers all functions in 'allelematch', reproduces the\n    examples from the documentation and includes negative tests. The\n    implementation is based on 'testthat'.",
    "version": "1.0.5",
    "maintainer": "Torvald Staxler <torvald.staxler@telia.com>",
    "author": "Department of Wildlife, Fish and Environmental Studies at Swedish\n    University of Agricultural Sciences [cph],\n  G\u00f6ran Spong [cph] (Senior Lecturer at the Department of Wildlife, Fish\n    and Environmental Studies),\n  Paul Galpern [ctb] (Author of the excellent package 'allelematch' from\n    which the five amExample data files have been copied under MIT\n    license),\n  Torvald Staxler [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=amregtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amregtest Runs Allelematch Regression Tests Automates regression testing of package 'allelematch'. Over\n    2500 tests covers all functions in 'allelematch', reproduces the\n    examples from the documentation and includes negative tests. The\n    implementation is based on 'testthat'.  "
  },
  {
    "id": 8505,
    "package_name": "animate",
    "title": "A Web-Based Graphics Device for Animated Visualisations",
    "description": "Implements a web-based graphics device for animated visualisations.\n  Modelled on the 'base' syntax, it extends the 'base' graphics functions to\n  support frame-by-frame animation and keyframes animation.\n  The target use cases are real-time animated visualisations, including agent-based\n  models, dynamical systems, and animated diagrams.\n  The generated visualisations can be deployed as GIF images / MP4 videos, as\n  'Shiny' apps (with interactivity) or as HTML documents through embedding into\n  R Markdown documents.",
    "version": "0.3.9.4",
    "maintainer": "Chun Fung Kwok <kcf.jackson@gmail.com>",
    "author": "Chun Fung Kwok [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0716-3879>),\n  Davis McCarthy [ctb] (ORCID: <https://orcid.org/0000-0002-2218-6833>)",
    "url": "https://kcf-jackson.github.io/animate/",
    "bug_reports": "https://github.com/kcf-jackson/animate/issues",
    "repository": "https://cran.r-project.org/package=animate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "animate A Web-Based Graphics Device for Animated Visualisations Implements a web-based graphics device for animated visualisations.\n  Modelled on the 'base' syntax, it extends the 'base' graphics functions to\n  support frame-by-frame animation and keyframes animation.\n  The target use cases are real-time animated visualisations, including agent-based\n  models, dynamical systems, and animated diagrams.\n  The generated visualisations can be deployed as GIF images / MP4 videos, as\n  'Shiny' apps (with interactivity) or as HTML documents through embedding into\n  R Markdown documents.  "
  },
  {
    "id": 8506,
    "package_name": "animation",
    "title": "A Gallery of Animations in Statistics and Utilities to Create\nAnimations",
    "description": "Provides functions for animations in statistics, covering topics\n    in probability theory, mathematical statistics, multivariate statistics,\n    non-parametric statistics, sampling survey, linear models, time series,\n    computational statistics, data mining and machine learning. These functions\n    may be helpful in teaching statistics and data analysis. Also provided in this\n    package are a series of functions to save animations to various formats, e.g.\n    Flash, 'GIF', HTML pages, 'PDF' and videos. 'PDF' animations can be inserted\n    into 'Sweave' / 'knitr' easily.",
    "version": "2.8",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>,\n    URL: https://yihui.org),\n  Christian Mueller [ctb],\n  Lijia Yu [ctb],\n  Xinyuan Chu [ctb],\n  Weicheng Zhu [ctb]",
    "url": "https://yihui.org/animation/",
    "bug_reports": "https://github.com/yihui/animation/issues",
    "repository": "https://cran.r-project.org/package=animation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "animation A Gallery of Animations in Statistics and Utilities to Create\nAnimations Provides functions for animations in statistics, covering topics\n    in probability theory, mathematical statistics, multivariate statistics,\n    non-parametric statistics, sampling survey, linear models, time series,\n    computational statistics, data mining and machine learning. These functions\n    may be helpful in teaching statistics and data analysis. Also provided in this\n    package are a series of functions to save animations to various formats, e.g.\n    Flash, 'GIF', HTML pages, 'PDF' and videos. 'PDF' animations can be inserted\n    into 'Sweave' / 'knitr' easily.  "
  },
  {
    "id": 8511,
    "package_name": "aniview",
    "title": "Animate Shiny and R Markdown Content when it Comes into View",
    "description": "Animate Shiny and R Markdown content when it comes into view using 'animate-css' effects thanks to 'jQuery AniView'.",
    "version": "0.1.0",
    "maintainer": "F\u00e9lix Luginbuhl <felix.luginbuhl@protonmail.ch>",
    "author": "F\u00e9lix Luginbuhl [aut, cre]",
    "url": "https://felixluginbuhl.com/aniview,\nhttps://github.com/lgnbhl/aniview",
    "bug_reports": "https://github.com/lgnbhl/aniview/issues",
    "repository": "https://cran.r-project.org/package=aniview",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aniview Animate Shiny and R Markdown Content when it Comes into View Animate Shiny and R Markdown content when it comes into view using 'animate-css' effects thanks to 'jQuery AniView'.  "
  },
  {
    "id": 8538,
    "package_name": "aos",
    "title": "Animate on Scroll Library for 'shiny'",
    "description": "Trigger animation effects on scroll on any HTML element \n    of 'shiny' and 'rmarkdown', such as any text or plot, thanks to \n    the 'AOS' Animate On Scroll jQuery library.",
    "version": "0.1.0",
    "maintainer": "F\u00e9lix Luginbuhl <felix.luginbuhl@protonmail.ch>",
    "author": "F\u00e9lix Luginbuhl [aut, cre]",
    "url": "https://felixluginbuhl.com/aos, https://github.com/lgnbhl/aos",
    "bug_reports": "https://github.com/lgnbhl/aos/issues",
    "repository": "https://cran.r-project.org/package=aos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aos Animate on Scroll Library for 'shiny' Trigger animation effects on scroll on any HTML element \n    of 'shiny' and 'rmarkdown', such as any text or plot, thanks to \n    the 'AOS' Animate On Scroll jQuery library.  "
  },
  {
    "id": 8540,
    "package_name": "apa7",
    "title": "Facilitate Writing Documents in American Psychological\nAssociation Style, Seventh Edition",
    "description": "Create American Psychological Association Style, Seventh\n    Edition documents. Format numbers and text consistent with APA style.\n    Create tables that comply with APA style by extending flextable\n    functions.",
    "version": "0.1.0",
    "maintainer": "W. Joel Schneider <w.joel.schneider@gmail.com>",
    "author": "W. Joel Schneider [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8393-5316>)",
    "url": "https://github.com/wjschne/apa7, https://wjschne.github.io/apa7/",
    "bug_reports": "https://github.com/wjschne/apa7/issues",
    "repository": "https://cran.r-project.org/package=apa7",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apa7 Facilitate Writing Documents in American Psychological\nAssociation Style, Seventh Edition Create American Psychological Association Style, Seventh\n    Edition documents. Format numbers and text consistent with APA style.\n    Create tables that comply with APA style by extending flextable\n    functions.  "
  },
  {
    "id": 8542,
    "package_name": "apaText",
    "title": "Create R Markdown Text for Results in the Style of the American\nPsychological Association (APA)",
    "description": "Create APA style text from analyses for use within R Markdown documents. Descriptive statistics, confidence intervals, and cell sizes are reported.",
    "version": "0.1.7",
    "maintainer": "David Stanley <dstanley@uoguelph.ca>",
    "author": "David Stanley [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=apaText",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apaText Create R Markdown Text for Results in the Style of the American\nPsychological Association (APA) Create APA style text from analyses for use within R Markdown documents. Descriptive statistics, confidence intervals, and cell sizes are reported.  "
  },
  {
    "id": 8544,
    "package_name": "apathe",
    "title": "American Psychological Association Thesis Templates for R\nMarkdown",
    "description": "Facilitates writing computationally reproducible student theses in PDF format that conform to the American Psychological Association (APA) manuscript guidelines (6th Edition). The package currently provides two R Markdown templates for homework and theses at the Psychology Department of the University of Cologne. The package builds on the package 'papaja' but is tailored to the requirements of student theses and omits features for simplicity.",
    "version": "0.1.0",
    "maintainer": "Frederik Aust <frederik.aust@uni-koeln.de>",
    "author": "Frederik Aust [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4900-788X>)",
    "url": "https://github.com/crsh/apathe",
    "bug_reports": "https://github.com/crsh/apathe/issues",
    "repository": "https://cran.r-project.org/package=apathe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apathe American Psychological Association Thesis Templates for R\nMarkdown Facilitates writing computationally reproducible student theses in PDF format that conform to the American Psychological Association (APA) manuscript guidelines (6th Edition). The package currently provides two R Markdown templates for homework and theses at the Psychology Department of the University of Cologne. The package builds on the package 'papaja' but is tailored to the requirements of student theses and omits features for simplicity.  "
  },
  {
    "id": 8562,
    "package_name": "apmx",
    "title": "Automated Population Pharmacokinetic Dataset Assembly",
    "description": "Automated methods to assemble population PK (pharmacokinetic) and\n    PKPD (pharmacodynamic) datasets for analysis in 'NONMEM' (non-linear mixed effects\n    modeling) by Bauer (2019) <doi:10.1002/psp4.12404>. The package includes functions\n    to build datasets from SDTM (study data tabulation module)\n    <https://www.cdisc.org/standards/foundational/sdtm>, ADaM (analysis dataset\n    module) <https://www.cdisc.org/standards/foundational/adam>, or other dataset\n    formats. The package will combine population datasets, add covariates, and\n    create documentation to support regulatory submission and internal communication.",
    "version": "1.1.1",
    "maintainer": "Stephen Amori <stephen.amori@amadorbio.com>",
    "author": "Stephen Amori [aut, cre, cph],\n  Ethan DellaMaestra [aut],\n  Michael Dick [aut],\n  Daniel Litow [ctb],\n  Jonah Lyon [ctb]",
    "url": "https://github.com/stephen-amori/apmx",
    "bug_reports": "https://github.com/stephen-amori/apmx/issues",
    "repository": "https://cran.r-project.org/package=apmx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apmx Automated Population Pharmacokinetic Dataset Assembly Automated methods to assemble population PK (pharmacokinetic) and\n    PKPD (pharmacodynamic) datasets for analysis in 'NONMEM' (non-linear mixed effects\n    modeling) by Bauer (2019) <doi:10.1002/psp4.12404>. The package includes functions\n    to build datasets from SDTM (study data tabulation module)\n    <https://www.cdisc.org/standards/foundational/sdtm>, ADaM (analysis dataset\n    module) <https://www.cdisc.org/standards/foundational/adam>, or other dataset\n    formats. The package will combine population datasets, add covariates, and\n    create documentation to support regulatory submission and internal communication.  "
  },
  {
    "id": 8582,
    "package_name": "aqp",
    "title": "Algorithms for Quantitative Pedology",
    "description": "The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice.",
    "version": "2.2-1",
    "maintainer": "Dylan Beaudette <dylan.beaudette@usda.gov>",
    "author": "Dylan Beaudette [aut, cre],\n  Pierre Roudier [aut, ctb],\n  Andrew Brown [aut, ctb]",
    "url": "https://ncss-tech.github.io/aqp/, https://ncss-tech.github.io/AQP/",
    "bug_reports": "https://github.com/ncss-tech/aqp/issues",
    "repository": "https://cran.r-project.org/package=aqp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aqp Algorithms for Quantitative Pedology The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice.  "
  },
  {
    "id": 8598,
    "package_name": "archeofrag",
    "title": "Spatial Analysis in Archaeology from Refitting Fragments",
    "description": "Methods to analyse spatial units in archaeology from the relationships between refitting fragmented objects scattered in these units (e.g. stratigraphic layers). Graphs are used to model archaeological observations. The package is mainly based on the 'igraph' package for graph analysis. Functions can: 1) create, manipulate, visualise, and simulate fragmentation graphs, 2) measure the cohesion and admixture of archaeological spatial units, and 3) characterise the topology of a specific set of refitting relationships. A series of published empirical datasets is included. Documentation about 'archeofrag' is provided by a vignette and by the accompanying scientific papers: Plutniak (2021, Journal of Archaeological Science, <doi:10.1016/j.jas.2021.105501>) and Plutniak (2022, Journal of Open Source Software, <doi:10.21105/joss.04335>). This package is complemented by the 'archeofrag.gui' R package, a companion GUI application available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.",
    "version": "1.2.3",
    "maintainer": "Sebastien Plutniak <sebastien.plutniak@posteo.net>",
    "author": "Sebastien Plutniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6674-3806>)",
    "url": "https://github.com/sebastien-plutniak/archeofrag",
    "bug_reports": "https://github.com/sebastien-plutniak/archeofrag/issues",
    "repository": "https://cran.r-project.org/package=archeofrag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "archeofrag Spatial Analysis in Archaeology from Refitting Fragments Methods to analyse spatial units in archaeology from the relationships between refitting fragmented objects scattered in these units (e.g. stratigraphic layers). Graphs are used to model archaeological observations. The package is mainly based on the 'igraph' package for graph analysis. Functions can: 1) create, manipulate, visualise, and simulate fragmentation graphs, 2) measure the cohesion and admixture of archaeological spatial units, and 3) characterise the topology of a specific set of refitting relationships. A series of published empirical datasets is included. Documentation about 'archeofrag' is provided by a vignette and by the accompanying scientific papers: Plutniak (2021, Journal of Archaeological Science, <doi:10.1016/j.jas.2021.105501>) and Plutniak (2022, Journal of Open Source Software, <doi:10.21105/joss.04335>). This package is complemented by the 'archeofrag.gui' R package, a companion GUI application available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.  "
  },
  {
    "id": 8599,
    "package_name": "archeofrag.gui",
    "title": "Spatial Analysis in Archaeology from Refitting Fragments (GUI)",
    "description": "A 'Shiny' application to access the functionalities and datasets of the 'archeofrag' package for spatial analysis in archaeology from refitting data. Quick and seamless exploration of archaeological refitting datasets, focusing on physical refits only. Features include: built-in documentation and convenient workflow, plot generation and exports, exploration of spatial units merging solutions, simulation of archaeological site formation processes, support for parallel computing, R code generation to re-execute simulations and ensure reproducibility, code generation for the 'openMOLE' model exploration software. A demonstration of the app is available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.",
    "version": "1.1.0",
    "maintainer": "Sebastien Plutniak <sebastien.plutniak@posteo.net>",
    "author": "Sebastien Plutniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6674-3806>)",
    "url": "https://github.com/sebastien-plutniak/archeofrag.gui",
    "bug_reports": "https://github.com/sebastien-plutniak/archeofrag.gui/issues",
    "repository": "https://cran.r-project.org/package=archeofrag.gui",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "archeofrag.gui Spatial Analysis in Archaeology from Refitting Fragments (GUI) A 'Shiny' application to access the functionalities and datasets of the 'archeofrag' package for spatial analysis in archaeology from refitting data. Quick and seamless exploration of archaeological refitting datasets, focusing on physical refits only. Features include: built-in documentation and convenient workflow, plot generation and exports, exploration of spatial units merging solutions, simulation of archaeological site formation processes, support for parallel computing, R code generation to re-execute simulations and ensure reproducibility, code generation for the 'openMOLE' model exploration software. A demonstration of the app is available at <https://analytics.huma-num.fr/Sebastien.Plutniak/archeofrag/>.  "
  },
  {
    "id": 8619,
    "package_name": "arfima",
    "title": "Fractional ARIMA (and Other Long Memory) Time Series Modeling",
    "description": "Simulates, fits, and predicts long-memory and anti-persistent\n\ttime series, possibly mixed with ARMA, regression, transfer-function\n\tcomponents.\n\tExact methods (MLE, forecasting, simulation) are used.\n\tBug reports should be done via GitHub (at\n\t<https://github.com/JQVeenstra/arfima>), where the development version\n\tof this package lives; it can be installed using devtools.",
    "version": "1.8-2",
    "maintainer": "JQ Veenstra <jqveenstra@gmail.com>",
    "author": "JQ Veenstra [aut, cre] (Justin),\n  A.I. McLeod [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=arfima",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arfima Fractional ARIMA (and Other Long Memory) Time Series Modeling Simulates, fits, and predicts long-memory and anti-persistent\n\ttime series, possibly mixed with ARMA, regression, transfer-function\n\tcomponents.\n\tExact methods (MLE, forecasting, simulation) are used.\n\tBug reports should be done via GitHub (at\n\t<https://github.com/JQVeenstra/arfima>), where the development version\n\tof this package lives; it can be installed using devtools.  "
  },
  {
    "id": 8629,
    "package_name": "ari",
    "title": "Automated R Instructor",
    "description": "Create videos from 'R Markdown' documents, or images and audio\n    files. These images can come from image files or HTML slides, and the audio\n    files can be provided by the user or computer voice narration can be created\n    using 'Amazon Polly'. The purpose of this package is to allow users to create\n    accessible, translatable, and reproducible lecture videos. See\n    <https://aws.amazon.com/polly/> for more information.",
    "version": "0.3.5",
    "maintainer": "Sean Kross <sean@seankross.com>",
    "author": "Sean Kross [aut, cre],\n  John Muschelli [ctb]",
    "url": "http://github.com/seankross/ari",
    "bug_reports": "http://github.com/seankross/ari/issues",
    "repository": "https://cran.r-project.org/package=ari",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ari Automated R Instructor Create videos from 'R Markdown' documents, or images and audio\n    files. These images can come from image files or HTML slides, and the audio\n    files can be provided by the user or computer voice narration can be created\n    using 'Amazon Polly'. The purpose of this package is to allow users to create\n    accessible, translatable, and reproducible lecture videos. See\n    <https://aws.amazon.com/polly/> for more information.  "
  },
  {
    "id": 8650,
    "package_name": "arsenal",
    "title": "An Arsenal of 'R' Functions for Large-Scale Statistical\nSummaries",
    "description": "An Arsenal of 'R' functions for large-scale statistical summaries,\n  which are streamlined to work within the latest reporting tools in 'R' and\n  'RStudio' and which use formulas and versatile summary statistics for summary\n  tables and models. The primary functions include tableby(), a Table-1-like\n  summary of multiple variable types 'by' the levels of one or more categorical\n  variables; paired(), a Table-1-like summary of multiple variable types paired across\n  two time points; modelsum(), which performs simple model fits on one or more endpoints\n  for many variables (univariate or adjusted for covariates);\n  freqlist(), a powerful frequency table across many categorical variables;\n  comparedf(), a function for comparing data.frames; and\n  write2(), a function to output tables to a document.",
    "version": "3.6.3",
    "maintainer": "Ethan Heinzen <heinzen.ethan@mayo.edu>",
    "author": "Ethan Heinzen [aut, cre],\n  Jason Sinnwell [aut],\n  Elizabeth Atkinson [aut],\n  Tina Gunderson [aut],\n  Gregory Dougherty [aut],\n  Patrick Votruba [ctb],\n  Ryan Lennon [ctb],\n  Andrew Hanson [ctb],\n  Krista Goergen [ctb],\n  Emily Lundt [ctb],\n  Brendan Broderick [ctb],\n  Maddie McCullough [art]",
    "url": "https://github.com/mayoverse/arsenal,\nhttps://cran.r-project.org/package=arsenal,\nhttps://mayoverse.github.io/arsenal/",
    "bug_reports": "https://github.com/mayoverse/arsenal/issues",
    "repository": "https://cran.r-project.org/package=arsenal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arsenal An Arsenal of 'R' Functions for Large-Scale Statistical\nSummaries An Arsenal of 'R' functions for large-scale statistical summaries,\n  which are streamlined to work within the latest reporting tools in 'R' and\n  'RStudio' and which use formulas and versatile summary statistics for summary\n  tables and models. The primary functions include tableby(), a Table-1-like\n  summary of multiple variable types 'by' the levels of one or more categorical\n  variables; paired(), a Table-1-like summary of multiple variable types paired across\n  two time points; modelsum(), which performs simple model fits on one or more endpoints\n  for many variables (univariate or adjusted for covariates);\n  freqlist(), a powerful frequency table across many categorical variables;\n  comparedf(), a function for comparing data.frames; and\n  write2(), a function to output tables to a document.  "
  },
  {
    "id": 8693,
    "package_name": "asteRisk",
    "title": "Computation of Satellite Position",
    "description": "Provides basic functionalities to calculate the position of\n    satellites given a known state vector. The package includes implementations\n    of the SGP4 and SDP4 simplified perturbation models to propagate orbital\n    state vectors, as well as utilities to read TLE files and convert coordinates\n    between different frames of reference. Several of the functionalities of the\n    package (including the high-precision numerical orbit propagator) require\n    the coefficients and data included in the 'asteRiskData' package, available\n    in a 'drat' repository. To install this data package, run \n    'install.packages(\"asteRiskData\", repos=\"https://rafael-ayala.github.io/drat/\")'.\n    Felix R. Hoots, Ronald L. Roehrich and T.S. Kelso (1988) <https://celestrak.org/NORAD/documentation/spacetrk.pdf>.\n    David Vallado, Paul Crawford, Richard Hujsak and T.S. Kelso (2012) <doi:10.2514/6.2006-6753>.\n    Felix R. Hoots, Paul W. Schumacher Jr. and Robert A. Glover (2014) <doi:10.2514/1.9161>.",
    "version": "1.4.5",
    "maintainer": "Rafael Ayala <rafaelayalahernandez@gmail.com>",
    "author": "Rafael Ayala [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9332-4623>),\n  Daniel Ayala [aut] (ORCID: <https://orcid.org/0000-0003-2095-1009>),\n  David Ruiz [aut] (ORCID: <https://orcid.org/0000-0003-4460-5493>),\n  Pablo Hernandez [aut] (ORCID: <https://orcid.org/0009-0000-9279-6744>),\n  Lara Selles Vidal [aut] (ORCID:\n    <https://orcid.org/0000-0003-2537-6824>)",
    "url": "",
    "bug_reports": "https://github.com/Rafael-Ayala/asteRisk/issues",
    "repository": "https://cran.r-project.org/package=asteRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "asteRisk Computation of Satellite Position Provides basic functionalities to calculate the position of\n    satellites given a known state vector. The package includes implementations\n    of the SGP4 and SDP4 simplified perturbation models to propagate orbital\n    state vectors, as well as utilities to read TLE files and convert coordinates\n    between different frames of reference. Several of the functionalities of the\n    package (including the high-precision numerical orbit propagator) require\n    the coefficients and data included in the 'asteRiskData' package, available\n    in a 'drat' repository. To install this data package, run \n    'install.packages(\"asteRiskData\", repos=\"https://rafael-ayala.github.io/drat/\")'.\n    Felix R. Hoots, Ronald L. Roehrich and T.S. Kelso (1988) <https://celestrak.org/NORAD/documentation/spacetrk.pdf>.\n    David Vallado, Paul Crawford, Richard Hujsak and T.S. Kelso (2012) <doi:10.2514/6.2006-6753>.\n    Felix R. Hoots, Paul W. Schumacher Jr. and Robert A. Glover (2014) <doi:10.2514/1.9161>.  "
  },
  {
    "id": 8709,
    "package_name": "atable",
    "title": "Create Tables for Reporting Clinical Trials",
    "description": "Create Tables for Reporting Clinical Trials.\n  Calculates descriptive statistics and hypothesis tests, \n  arranges the results in a table ready for reporting with LaTeX, HTML or Word.",
    "version": "0.1.15",
    "maintainer": "Armin Str\u00f6bel <arminstroebel@web.de>",
    "author": "Armin Str\u00f6bel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6873-5332>),\n  Alan Haynes [aut] (ORCID: <https://orcid.org/0000-0003-1374-081X>)",
    "url": "https://github.com/arminstroebel/atable",
    "bug_reports": "https://github.com/arminstroebel/atable/issues",
    "repository": "https://cran.r-project.org/package=atable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "atable Create Tables for Reporting Clinical Trials Create Tables for Reporting Clinical Trials.\n  Calculates descriptive statistics and hypothesis tests, \n  arranges the results in a table ready for reporting with LaTeX, HTML or Word.  "
  },
  {
    "id": 8713,
    "package_name": "atlasapprox",
    "title": "Cell Atlas Approximations",
    "description": "Provides an interface in R to cell atlas approximations. See the vignette under \"Getting started\" for instructions. You can also explore the reference documentation for specific functions. Additional interfaces and resources are available at <https://atlasapprox.readthedocs.io>.",
    "version": "0.1.0",
    "maintainer": "Ying Xu <ying.xu3@unsw.edu.au>",
    "author": "Fabio Zanini [aut] (ORCID: <https://orcid.org/0000-0001-7097-8539>),\n  Ying Xu [aut, cre]",
    "url": "https://atlasapprox.readthedocs.io/en/latest/R/index.html,\nhttps://github.com/fabilab/cell_atlas_approximations_API",
    "bug_reports": "https://github.com/fabilab/cell_atlas_approximations_API/issues",
    "repository": "https://cran.r-project.org/package=atlasapprox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "atlasapprox Cell Atlas Approximations Provides an interface in R to cell atlas approximations. See the vignette under \"Getting started\" for instructions. You can also explore the reference documentation for specific functions. Additional interfaces and resources are available at <https://atlasapprox.readthedocs.io>.  "
  },
  {
    "id": 8718,
    "package_name": "attachment",
    "title": "Deal with Dependencies",
    "description": "Manage dependencies during package development. This can\n    retrieve all dependencies that are used in \".R\" files in the \"R/\"\n    directory, in \".Rmd\" files in \"vignettes/\" directory and in 'roxygen2'\n    documentation of functions. There is a function to update the\n    \"DESCRIPTION\" file of your package with 'CRAN' packages or any other\n    remote package.  All functions to retrieve dependencies of \".R\"\n    scripts and \".Rmd\" or \".qmd\" files can be used independently of a\n    package development.",
    "version": "0.4.5",
    "maintainer": "Vincent Guyader <vincent@thinkr.fr>",
    "author": "Vincent Guyader [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-0671-9270>),\n  S\u00e9bastien Rochette [aut] (ORCID:\n    <https://orcid.org/0000-0002-1565-9313>, previous maintainer),\n  Murielle Delmotte [aut] (ORCID:\n    <https://orcid.org/0000-0002-1339-2424>),\n  Swann Floc'hlay [aut] (ORCID: <https://orcid.org/0000-0003-1477-830X>),\n  ThinkR [cph, fnd]",
    "url": "https://thinkr-open.github.io/attachment/,\nhttps://github.com/ThinkR-open/attachment",
    "bug_reports": "https://github.com/ThinkR-open/attachment/issues",
    "repository": "https://cran.r-project.org/package=attachment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "attachment Deal with Dependencies Manage dependencies during package development. This can\n    retrieve all dependencies that are used in \".R\" files in the \"R/\"\n    directory, in \".Rmd\" files in \"vignettes/\" directory and in 'roxygen2'\n    documentation of functions. There is a function to update the\n    \"DESCRIPTION\" file of your package with 'CRAN' packages or any other\n    remote package.  All functions to retrieve dependencies of \".R\"\n    scripts and \".Rmd\" or \".qmd\" files can be used independently of a\n    package development.  "
  },
  {
    "id": 8732,
    "package_name": "augmentedRCBD",
    "title": "Analysis of Augmented Randomised Complete Block Designs",
    "description": "Functions for analysis of data generated from experiments in\n    augmented randomised complete block design according to Federer, W.T.\n    (1961) <doi:10.2307/2527837>. Computes analysis of variance, adjusted\n    means, descriptive statistics, genetic variability statistics etc.\n    Further includes data visualization and report generation functions.",
    "version": "0.1.7",
    "maintainer": "J. Aravind <j.aravind@icar.gov.in>",
    "author": "J. Aravind [aut, cre] (ORCID: <https://orcid.org/0000-0002-4791-442X>),\n  S. Mukesh Sankar [aut] (ORCID: <https://orcid.org/0000-0001-5459-392X>),\n  Dhammaprakash Pandhari Wankhede [aut] (ORCID:\n    <https://orcid.org/0000-0001-6384-8664>),\n  Vikender Kaur [aut] (ORCID: <https://orcid.org/0000-0001-9698-7329>),\n  ICAR-NBGPR [cph] (url: www.nbpgr.ernet.in)",
    "url": "https://github.com/aravind-j/augmentedRCBD\nhttps://CRAN.R-project.org/package=augmentedRCBD\nhttps://aravind-j.github.io/augmentedRCBD/\nhttps://doi.org/10.5281/zenodo.1310011",
    "bug_reports": "https://github.com/aravind-j/augmentedRCBD/issues",
    "repository": "https://cran.r-project.org/package=augmentedRCBD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "augmentedRCBD Analysis of Augmented Randomised Complete Block Designs Functions for analysis of data generated from experiments in\n    augmented randomised complete block design according to Federer, W.T.\n    (1961) <doi:10.2307/2527837>. Computes analysis of variance, adjusted\n    means, descriptive statistics, genetic variability statistics etc.\n    Further includes data visualization and report generation functions.  "
  },
  {
    "id": 8745,
    "package_name": "autoReg",
    "title": "Automatic Linear and Logistic Regression and Survival Analysis",
    "description": "Make summary tables for descriptive statistics and select explanatory variables \n    automatically in various regression models. Support linear models, generalized linear \n    models and cox-proportional hazard models. Generate publication-ready tables summarizing \n    result of regression analysis and plots. The tables and plots can be exported in \"HTML\", \n    \"pdf('LaTex')\", \"docx('MS Word')\" and \"pptx('MS Powerpoint')\" documents.",
    "version": "0.3.3",
    "maintainer": "Keon-Woong Moon <cardiomoon@gmail.com>",
    "author": "Keon-Woong Moon [aut, cre]",
    "url": "https://github.com/cardiomoon/autoReg,\nhttps://cardiomoon.github.io/autoReg/",
    "bug_reports": "https://github.com/cardiomoon/autoReg/issues",
    "repository": "https://cran.r-project.org/package=autoReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "autoReg Automatic Linear and Logistic Regression and Survival Analysis Make summary tables for descriptive statistics and select explanatory variables \n    automatically in various regression models. Support linear models, generalized linear \n    models and cox-proportional hazard models. Generate publication-ready tables summarizing \n    result of regression analysis and plots. The tables and plots can be exported in \"HTML\", \n    \"pdf('LaTex')\", \"docx('MS Word')\" and \"pptx('MS Powerpoint')\" documents.  "
  },
  {
    "id": 8752,
    "package_name": "autoharp",
    "title": "Semi-Automatic Grading of R and Rmd Scripts",
    "description": "A customisable set of tools for assessing and grading \n    R or R-markdown scripts from students. It allows for checking correctness \n    of code output, runtime statistics and static code analysis. The latter \n    feature is made possible by representing R expressions using a tree\n    structure.",
    "version": "0.0.13",
    "maintainer": "Agrawal Naman <naman.a@nus.edu.sg>",
    "author": "Vik Gopal [aut],\n  Agrawal Naman [aut, cre],\n  Samuel Seah [aut],\n  Viknesh Jeya Kumar [aut],\n  Gabriel Ang [aut],\n  Ruofan Liu [ctb],\n  National University of Singapore [cph]",
    "url": "https://singator.github.io/autoharp-docs/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=autoharp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "autoharp Semi-Automatic Grading of R and Rmd Scripts A customisable set of tools for assessing and grading \n    R or R-markdown scripts from students. It allows for checking correctness \n    of code output, runtime statistics and static code analysis. The latter \n    feature is made possible by representing R expressions using a tree\n    structure.  "
  },
  {
    "id": 8757,
    "package_name": "automagic",
    "title": "Automagically Document and Install Packages Necessary to Run R\nCode",
    "description": "Parse R code in a given directory for R packages and attempt to install them from CRAN or GitHub. Optionally use a dependencies file for tighter control over which package versions to install.",
    "version": "0.5.1",
    "maintainer": "Cole Brokamp <cole.brokamp@gmail.com>",
    "author": "Cole Brokamp [aut, cre],\n  Steph Locke [ctb]",
    "url": "https://github.com/cole-brokamp/automagic",
    "bug_reports": "https://github.com/cole-brokamp/automagic/issues",
    "repository": "https://cran.r-project.org/package=automagic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "automagic Automagically Document and Install Packages Necessary to Run R\nCode Parse R code in a given directory for R packages and attempt to install them from CRAN or GitHub. Optionally use a dependencies file for tighter control over which package versions to install.  "
  },
  {
    "id": 8777,
    "package_name": "avlm",
    "title": "Safe Anytime Valid Inference for Linear Models",
    "description": "Anytime-valid inference for linear models, namely, sequential t-tests, sequential F-tests, and confidence sequences with time-uniform Type-I error and coverage guarantees. This allows hypotheses to be continuously tested without sacrificing false positive guarantees. It is based on the methods documented in Lindon et al. (2022) <doi:10.48550/arXiv.2210.08589>.",
    "version": "0.1.0",
    "maintainer": "Michael Lindon <michael.s.lindon@gmail.com>",
    "author": "Michael Lindon [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=avlm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "avlm Safe Anytime Valid Inference for Linear Models Anytime-valid inference for linear models, namely, sequential t-tests, sequential F-tests, and confidence sequences with time-uniform Type-I error and coverage guarantees. This allows hypotheses to be continuously tested without sacrificing false positive guarantees. It is based on the methods documented in Lindon et al. (2022) <doi:10.48550/arXiv.2210.08589>.  "
  },
  {
    "id": 8778,
    "package_name": "avocado",
    "title": "Weekly Hass Avocado Sales Summary",
    "description": "Provides a weekly summary of Hass Avocado sales for the \n    contiguous US from January 2017 through December 20204. \n    See the package website for more information, documentation,\n    and examples. Data source: Haas Avocado Board \n    <https://hassavocadoboard.com/category-data/>.",
    "version": "0.2.0",
    "maintainer": "Nikhil Agarwal <gitnik@niks.me>",
    "author": "Nikhil Agarwal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1470-7472>)",
    "url": "https://github.com/nikdata/avocado",
    "bug_reports": "https://github.com/nikdata/avocado/issues",
    "repository": "https://cran.r-project.org/package=avocado",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "avocado Weekly Hass Avocado Sales Summary Provides a weekly summary of Hass Avocado sales for the \n    contiguous US from January 2017 through December 20204. \n    See the package website for more information, documentation,\n    and examples. Data source: Haas Avocado Board \n    <https://hassavocadoboard.com/category-data/>.  "
  },
  {
    "id": 8789,
    "package_name": "aws.ecx",
    "title": "Communicating with AWS EC2 and ECS using AWS REST APIs",
    "description": "\n    Providing the functions for communicating with Amazon Web Services(AWS)\n    Elastic Compute Cloud(EC2) and Elastic Container Service(ECS).\n    The functions will have the prefix 'ecs_' or 'ec2_' depending on the class \n    of the API. The request will be sent via the REST API and the parameters are\n    given by the function argument. The credentials can be set via 'aws_set_credentials'.\n    The EC2 documentation can be found at <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Welcome.html>\n    and ECS can be found at <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/Welcome.html>.",
    "version": "1.0.5",
    "maintainer": "Jiefei Wang <szwjf08@gmail.com>",
    "author": "Jiefei Wang [aut, cre],\n  Martin Morgan [aut]",
    "url": "https://github.com/Jiefei-Wang/aws.ecx",
    "bug_reports": "https://github.com/Jiefei-Wang/aws.ecx/issues",
    "repository": "https://cran.r-project.org/package=aws.ecx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aws.ecx Communicating with AWS EC2 and ECS using AWS REST APIs \n    Providing the functions for communicating with Amazon Web Services(AWS)\n    Elastic Compute Cloud(EC2) and Elastic Container Service(ECS).\n    The functions will have the prefix 'ecs_' or 'ec2_' depending on the class \n    of the API. The request will be sent via the REST API and the parameters are\n    given by the function argument. The credentials can be set via 'aws_set_credentials'.\n    The EC2 documentation can be found at <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Welcome.html>\n    and ECS can be found at <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/Welcome.html>.  "
  },
  {
    "id": 8793,
    "package_name": "aws.polly",
    "title": "Client for AWS Polly",
    "description": "A client for AWS Polly <http://aws.amazon.com/documentation/polly>, a speech synthesis service.",
    "version": "0.1.5",
    "maintainer": "Antoine Sachet <antoine.sac@gmail.com>",
    "author": "Thomas J. Leeper [aut] (ORCID: <https://orcid.org/0000-0003-4097-6326>),\n  Antoine Sachet [cre]",
    "url": "https://github.com/cloudyr/aws.polly",
    "bug_reports": "https://github.com/cloudyr/aws.polly/issues",
    "repository": "https://cran.r-project.org/package=aws.polly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aws.polly Client for AWS Polly A client for AWS Polly <http://aws.amazon.com/documentation/polly>, a speech synthesis service.  "
  },
  {
    "id": 8795,
    "package_name": "aws.transcribe",
    "title": "Client for 'AWS Transcribe'",
    "description": "Client for 'AWS Transcribe' <https://aws.amazon.com/documentation/transcribe>, a cloud transcription service that can convert an audio media file in English and other languages into a text transcript.",
    "version": "0.1.3",
    "maintainer": "Antoine Sachet <antoine.sac@gmail.com>",
    "author": "Thomas J. Leeper [aut] (ORCID: <https://orcid.org/0000-0003-4097-6326>),\n  Antoine Sachet [cre]",
    "url": "https://github.com/cloudyr/aws.transcribe",
    "bug_reports": "https://github.com/cloudyr/aws.transcribe/issues",
    "repository": "https://cran.r-project.org/package=aws.transcribe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aws.transcribe Client for 'AWS Transcribe' Client for 'AWS Transcribe' <https://aws.amazon.com/documentation/transcribe>, a cloud transcription service that can convert an audio media file in English and other languages into a text transcript.  "
  },
  {
    "id": 8796,
    "package_name": "aws.translate",
    "title": "Client for 'AWS Translate'",
    "description": "A client for 'AWS Translate' <https://aws.amazon.com/documentation/translate>, a machine translation service that will convert a text input in one language into a text output in another language.",
    "version": "0.1.4",
    "maintainer": "Antoine Sachet <antoine.sac@gmail.com>",
    "author": "Thomas J. Leeper [aut] (ORCID: <https://orcid.org/0000-0003-4097-6326>),\n  Antoine Sachet [cre]",
    "url": "https://github.com/cloudyr/aws.translate",
    "bug_reports": "https://github.com/cloudyr/aws.translate/issues",
    "repository": "https://cran.r-project.org/package=aws.translate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aws.translate Client for 'AWS Translate' A client for 'AWS Translate' <https://aws.amazon.com/documentation/translate>, a machine translation service that will convert a text input in one language into a text output in another language.  "
  },
  {
    "id": 8832,
    "package_name": "bagyo",
    "title": "Philippine Tropical Cyclones Data",
    "description": "The Philippines frequently experiences tropical cyclones (called\n    'bagyo' in the Filipino language) because of its geographical position.\n    These cyclones typically bring heavy rainfall, leading to widespread\n    flooding, as well as strong winds that cause significant damage to human\n    life, crops, and property. Data on cyclones are collected and curated by the\n    Philippine Atmospheric, Geophysical, and Astronomical Services\n    Administration or 'PAGASA' and made available through its website\n    <https://bagong.pagasa.dost.gov.ph/tropical-cyclone/publications/annual-report>.\n    This package contains Philippine tropical cyclones data in a\n    machine-readable format. It is hoped that this data package provides an\n    interesting and unique dataset for data exploration and visualisation.",
    "version": "0.1.1",
    "maintainer": "Ernest Guevarra <ernest@guevarra.io>",
    "author": "Ernest Guevarra [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4887-4415>)",
    "url": "https://panukatan.io/bagyo/, https://github.com/panukatan/bagyo",
    "bug_reports": "https://github.com/panukatan/bagyo/issues",
    "repository": "https://cran.r-project.org/package=bagyo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bagyo Philippine Tropical Cyclones Data The Philippines frequently experiences tropical cyclones (called\n    'bagyo' in the Filipino language) because of its geographical position.\n    These cyclones typically bring heavy rainfall, leading to widespread\n    flooding, as well as strong winds that cause significant damage to human\n    life, crops, and property. Data on cyclones are collected and curated by the\n    Philippine Atmospheric, Geophysical, and Astronomical Services\n    Administration or 'PAGASA' and made available through its website\n    <https://bagong.pagasa.dost.gov.ph/tropical-cyclone/publications/annual-report>.\n    This package contains Philippine tropical cyclones data in a\n    machine-readable format. It is hoped that this data package provides an\n    interesting and unique dataset for data exploration and visualisation.  "
  },
  {
    "id": 8841,
    "package_name": "bambooHR",
    "title": "A Wrapper to the 'BambooHR' API",
    "description": "Enables a user to consume the 'BambooHR' API endpoints using R. The\n  actual URL of the API will depend on your company domain, and will be handled\n  by the package automatically once you setup the config file. The API documentation\n  can be found here <https://documentation.bamboohr.com/docs>.",
    "version": "0.1.1",
    "maintainer": "Tom Bowling <tom.bowling@ascent.io>",
    "author": "Tom Bowling [aut, cre],\n  Tim Fry [aut],\n  Harry Alexander [ctb],\n  Andrew Little [ctb],\n  Mark Druffel [ctb]",
    "url": "https://mangothecat.github.io/bambooHR/",
    "bug_reports": "https://github.com/MangoTheCat/bambooHR/issues",
    "repository": "https://cran.r-project.org/package=bambooHR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bambooHR A Wrapper to the 'BambooHR' API Enables a user to consume the 'BambooHR' API endpoints using R. The\n  actual URL of the API will depend on your company domain, and will be handled\n  by the package automatically once you setup the config file. The API documentation\n  can be found here <https://documentation.bamboohr.com/docs>.  "
  },
  {
    "id": 8852,
    "package_name": "banffIT",
    "title": "Automated Standardized Assignment of the Banff Classification",
    "description": "Assigns standardized diagnoses using the Banff Classification\n    (Category 1 to 6 diagnoses, including Acute and Chronic active T-cell\n    mediated rejection as well as Active, Chronic active, and Chronic antibody\n    mediated rejection). The main function considers a minimal dataset\n    containing biopsies information in a specific format (described by a data\n    dictionary), verifies its content and format (based on the data dictionary),\n    assigns diagnoses, and creates a summary report. The package is developed on\n    the reference guide to the Banff classification of renal allograft pathology\n    Roufosse C, Simmonds N, Clahsen-van Groningen M, et al. A (2018) <doi:10.1097/TP.0000000000002366>.\n    The full description of the Banff classification is available at <https://banfffoundation.org/>.",
    "version": "2.0.0",
    "maintainer": "Guillaume Fabre <guijoseph.fabre@gmail.com>",
    "author": "Guillaume Fabre [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0124-9970>),\n  Ruth Sapir-Pichhadze [fnd, cph, ctb],\n  Jan Ulrich Becker [ctb],\n  Samuel El Bouza\u00efdi Tiali [ctb],\n  J\u00e9r\u00f4me Laforme [ctb],\n  Tina Wey [ctb],\n  Edden Gitelman [ctb]",
    "url": "https://github.com/PersonalizedTransplantCare/banffIT",
    "bug_reports": "https://github.com/PersonalizedTransplantCare/banffIT/issues",
    "repository": "https://cran.r-project.org/package=banffIT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "banffIT Automated Standardized Assignment of the Banff Classification Assigns standardized diagnoses using the Banff Classification\n    (Category 1 to 6 diagnoses, including Acute and Chronic active T-cell\n    mediated rejection as well as Active, Chronic active, and Chronic antibody\n    mediated rejection). The main function considers a minimal dataset\n    containing biopsies information in a specific format (described by a data\n    dictionary), verifies its content and format (based on the data dictionary),\n    assigns diagnoses, and creates a summary report. The package is developed on\n    the reference guide to the Banff classification of renal allograft pathology\n    Roufosse C, Simmonds N, Clahsen-van Groningen M, et al. A (2018) <doi:10.1097/TP.0000000000002366>.\n    The full description of the Banff classification is available at <https://banfffoundation.org/>.  "
  },
  {
    "id": 8994,
    "package_name": "bdc",
    "title": "Biodiversity Data Cleaning",
    "description": "It brings together several aspects of biodiversity\n    data-cleaning in one place. 'bdc' is organized in thematic modules\n    related to different biodiversity dimensions, including 1) Merge\n    datasets: standardization and integration of different datasets; 2)\n    Pre-filter: flagging and removal of invalid or non-interpretable\n    information, followed by data amendments; 3) Taxonomy: cleaning,\n    parsing, and harmonization of scientific names from several taxonomic\n    groups against taxonomic databases locally stored through the\n    application of exact and partial matching algorithms; 4) Space:\n    flagging of erroneous, suspect, and low-precision geographic\n    coordinates; and 5) Time: flagging and, whenever possible, correction\n    of inconsistent collection date. In addition, it contains\n    features to visualize, document, and report data quality \u2013 which is\n    essential for making data quality assessment transparent and\n    reproducible. The reference for the methodology is Bruno et al. (2022)\n    <doi:10.1111/2041-210X.13868>.",
    "version": "1.1.5",
    "maintainer": "Bruno Ribeiro <ribeiro.brr@gmail.com>",
    "author": "Bruno Ribeiro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7755-6715>),\n  Santiago Velazco [aut] (ORCID: <https://orcid.org/0000-0002-7527-0967>),\n  Karlo Guidoni-Martins [aut] (ORCID:\n    <https://orcid.org/0000-0002-8458-8467>),\n  Geiziane Tessarolo [aut] (ORCID:\n    <https://orcid.org/0000-0003-1361-0062>),\n  Lucas Jardim [aut] (ORCID: <https://orcid.org/0000-0003-2602-5575>),\n  Steven Bachman [ctb] (ORCID: <https://orcid.org/0000-0003-1085-6075>),\n  Rafael Loyola [ctb] (ORCID: <https://orcid.org/0000-0001-5323-2735>)",
    "url": "https://brunobrr.github.io/bdc/ (website)\nhttps://github.com/brunobrr/bdc",
    "bug_reports": "https://github.com/brunobrr/bdc/issues",
    "repository": "https://cran.r-project.org/package=bdc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bdc Biodiversity Data Cleaning It brings together several aspects of biodiversity\n    data-cleaning in one place. 'bdc' is organized in thematic modules\n    related to different biodiversity dimensions, including 1) Merge\n    datasets: standardization and integration of different datasets; 2)\n    Pre-filter: flagging and removal of invalid or non-interpretable\n    information, followed by data amendments; 3) Taxonomy: cleaning,\n    parsing, and harmonization of scientific names from several taxonomic\n    groups against taxonomic databases locally stored through the\n    application of exact and partial matching algorithms; 4) Space:\n    flagging of erroneous, suspect, and low-precision geographic\n    coordinates; and 5) Time: flagging and, whenever possible, correction\n    of inconsistent collection date. In addition, it contains\n    features to visualize, document, and report data quality \u2013 which is\n    essential for making data quality assessment transparent and\n    reproducible. The reference for the methodology is Bruno et al. (2022)\n    <doi:10.1111/2041-210X.13868>.  "
  },
  {
    "id": 9027,
    "package_name": "behaviorchange",
    "title": "Tools for Behavior Change Researchers and Professionals",
    "description": "Contains specialised analyses and\n    visualisation tools for behavior change science.\n    These facilitate conducting determinant studies\n    (for example, using confidence interval-based\n    estimation of relevance, CIBER, or CIBERlite\n    plots, see Crutzen, Noijen & Peters (2017)\n    <doi:10/ghtfz9>),\n    systematically developing, reporting,\n    and analysing interventions (for example, using\n    Acyclic Behavior Change Diagrams), and reporting\n    about intervention effectiveness (for example, using\n    the Numbers Needed for Change, see Gruijters & Peters\n    (2017) <doi:10/jzkt>), and computing the\n    required sample size (using the Meaningful Change\n    Definition, see Gruijters & Peters (2020)\n    <doi:10/ghpnx8>).\n    This package is especially useful for\n    researchers in the field of behavior change or\n    health psychology and to behavior change\n    professionals such as intervention developers and\n    prevention workers.",
    "version": "25.8.0",
    "maintainer": "Gjalt-Jorn Peters <behaviorchange@opens.science>",
    "author": "Gjalt-Jorn Peters [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0336-9589>),\n  Rik Crutzen [ctb] (ORCID: <https://orcid.org/0000-0002-3731-6610>),\n  Jeroen Bruinsma [ctb] (ORCID: <https://orcid.org/0000-0002-7964-0267>),\n  Stefan Gruijters [ctb] (ORCID: <https://orcid.org/0000-0003-0141-0071>)",
    "url": "https://behaviorchange.opens.science",
    "bug_reports": "https://codeberg.org/R-packages/behaviorchange/issues",
    "repository": "https://cran.r-project.org/package=behaviorchange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "behaviorchange Tools for Behavior Change Researchers and Professionals Contains specialised analyses and\n    visualisation tools for behavior change science.\n    These facilitate conducting determinant studies\n    (for example, using confidence interval-based\n    estimation of relevance, CIBER, or CIBERlite\n    plots, see Crutzen, Noijen & Peters (2017)\n    <doi:10/ghtfz9>),\n    systematically developing, reporting,\n    and analysing interventions (for example, using\n    Acyclic Behavior Change Diagrams), and reporting\n    about intervention effectiveness (for example, using\n    the Numbers Needed for Change, see Gruijters & Peters\n    (2017) <doi:10/jzkt>), and computing the\n    required sample size (using the Meaningful Change\n    Definition, see Gruijters & Peters (2020)\n    <doi:10/ghpnx8>).\n    This package is especially useful for\n    researchers in the field of behavior change or\n    health psychology and to behavior change\n    professionals such as intervention developers and\n    prevention workers.  "
  },
  {
    "id": 9038,
    "package_name": "bennu",
    "title": "Bayesian Estimation of Naloxone Kit Number Under-Reporting",
    "description": "Bayesian model and associated tools for generating estimates of \n    total naloxone kit numbers distributed and used from naloxone kit orders \n    data. Provides functions for generating simulated data of naloxone kit use\n    and functions for generating samples from the posterior.",
    "version": "0.3.2",
    "maintainer": "Mike Irvine <mike.irvine@bccdc.ca>",
    "author": "Mike Irvine [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4785-8998>),\n  Samantha Bardwell [ctb],\n  Andrew Johnson [ctb]",
    "url": "https://sempwn.github.io/bennu/",
    "bug_reports": "https://github.com/sempwn/bennu/issues",
    "repository": "https://cran.r-project.org/package=bennu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bennu Bayesian Estimation of Naloxone Kit Number Under-Reporting Bayesian model and associated tools for generating estimates of \n    total naloxone kit numbers distributed and used from naloxone kit orders \n    data. Provides functions for generating simulated data of naloxone kit use\n    and functions for generating samples from the posterior.  "
  },
  {
    "id": 9055,
    "package_name": "betafunctions",
    "title": "Functions for Working with Two- And Four-Parameter Beta\nProbability Distributions and Psychometric Analysis of\nClassifications",
    "description": "Package providing a number of functions for working with Two- and \n    Four-parameter Beta and closely related distributions (i.e., the Gamma-\n    Binomial-, and Beta-Binomial distributions).\n        Includes, among other things: \n    - d/p/q/r functions for Four-Parameter Beta distributions and Generalized\n    \"Binomial\" (continuous) distributions, and d/p/r- functions for Beta-\n    Binomial distributions.\n    - d/p/q/r functions for Two- and Four-Parameter Beta distributions\n    parameterized in terms of their means and variances rather than their\n    shape-parameters.\n    - Moment generating functions for Binomial distributions, Beta-Binomial \n    distributions, and observed value distributions.\n    - Functions for estimating classification accuracy and consistency, \n    making use of the Classical Test-Theory based 'Livingston and Lewis' (L&L) \n    and 'Hanson and Brennan' approaches.\n      A shiny app is available, providing a GUI for the L&L approach when used \n    for binary classifications. For url to the app, see documentation for the \n    LL.CA() function.\n    Livingston and Lewis (1995) <doi:10.1111/j.1745-3984.1995.tb00462.x>.\n    Lord (1965) <doi:10.1007/BF02289490>.\n    Hanson (1991) <https://files.eric.ed.gov/fulltext/ED344945.pdf>.",
    "version": "1.9.0",
    "maintainer": "Haakon Eidem Haakstad <h.e.haakstad@gmail.com>",
    "author": "Haakon Eidem Haakstad",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=betafunctions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "betafunctions Functions for Working with Two- And Four-Parameter Beta\nProbability Distributions and Psychometric Analysis of\nClassifications Package providing a number of functions for working with Two- and \n    Four-parameter Beta and closely related distributions (i.e., the Gamma-\n    Binomial-, and Beta-Binomial distributions).\n        Includes, among other things: \n    - d/p/q/r functions for Four-Parameter Beta distributions and Generalized\n    \"Binomial\" (continuous) distributions, and d/p/r- functions for Beta-\n    Binomial distributions.\n    - d/p/q/r functions for Two- and Four-Parameter Beta distributions\n    parameterized in terms of their means and variances rather than their\n    shape-parameters.\n    - Moment generating functions for Binomial distributions, Beta-Binomial \n    distributions, and observed value distributions.\n    - Functions for estimating classification accuracy and consistency, \n    making use of the Classical Test-Theory based 'Livingston and Lewis' (L&L) \n    and 'Hanson and Brennan' approaches.\n      A shiny app is available, providing a GUI for the L&L approach when used \n    for binary classifications. For url to the app, see documentation for the \n    LL.CA() function.\n    Livingston and Lewis (1995) <doi:10.1111/j.1745-3984.1995.tb00462.x>.\n    Lord (1965) <doi:10.1007/BF02289490>.\n    Hanson (1991) <https://files.eric.ed.gov/fulltext/ED344945.pdf>.  "
  },
  {
    "id": 9093,
    "package_name": "biblioverlap",
    "title": "Document-Level Matching Between Bibliographic Datasets",
    "description": "Identifies and visualizes document overlap in any number of bibliographic datasets.\n    This package implements the identification of overlapping documents through the exact match of a unique \n    identifier (e.g. Digital Object Identifier - DOI) and, for records where the identifier is absent, through a score calculated \n    from a set of fields commonly found in bibliographic datasets (Title, Source, Authors and Publication Year).\n    Additionally, it provides functions to visualize the results of the document matching through a Venn diagram \n    and/or UpSet plot, as well as a summary of the matching procedure. ",
    "version": "1.0.2",
    "maintainer": "Gabriel Vieira <gabriel.vieira@bioqmed.ufrj.br>",
    "author": "Gabriel Vieira [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5529-7628>),\n  Jacqueline Leta [ctb] (ORCID: <https://orcid.org/0000-0002-3271-7749>)",
    "url": "https://github.com/gavieira/biblioverlap",
    "bug_reports": "https://github.com/gavieira/biblioverlap/issues",
    "repository": "https://cran.r-project.org/package=biblioverlap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biblioverlap Document-Level Matching Between Bibliographic Datasets Identifies and visualizes document overlap in any number of bibliographic datasets.\n    This package implements the identification of overlapping documents through the exact match of a unique \n    identifier (e.g. Digital Object Identifier - DOI) and, for records where the identifier is absent, through a score calculated \n    from a set of fields commonly found in bibliographic datasets (Title, Source, Authors and Publication Year).\n    Additionally, it provides functions to visualize the results of the document matching through a Venn diagram \n    and/or UpSet plot, as well as a summary of the matching procedure.   "
  },
  {
    "id": 9098,
    "package_name": "bidux",
    "title": "Behavioral Insight Design: A Toolkit for Integrating Behavioral\nScience in UI/UX Design",
    "description": "Provides a framework and toolkit to guide R dashboard developers in\n    implementing the Behavioral Insight Design (BID) framework. The package offers\n    functions for documenting each of the five stages (Interpret, Notice,\n    Anticipate, Structure, and Validate), along with a comprehensive concept\n    dictionary. Works with both 'shiny' applications and 'Quarto' dashboards.",
    "version": "0.3.3",
    "maintainer": "Jeremy Winget <contact@jrwinget.com>",
    "author": "Jeremy Winget [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3783-4354>)",
    "url": "https://jrwinget.github.io/bidux/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bidux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bidux Behavioral Insight Design: A Toolkit for Integrating Behavioral\nScience in UI/UX Design Provides a framework and toolkit to guide R dashboard developers in\n    implementing the Behavioral Insight Design (BID) framework. The package offers\n    functions for documenting each of the five stages (Interpret, Notice,\n    Anticipate, Structure, and Validate), along with a comprehensive concept\n    dictionary. Works with both 'shiny' applications and 'Quarto' dashboards.  "
  },
  {
    "id": 9106,
    "package_name": "bigPLSR",
    "title": "Partial Least Squares Regression Models with Big Matrices",
    "description": "\n  Fast partial least squares (PLS) for dense and out-of-core data.\n  Provides SIMPLS (straightforward implementation of a statistically inspired \n  modification of the PLS method) and NIPALS (non-linear iterative partial least-squares) solvers, \n  plus kernel-style PLS variants ('kernelpls' and 'widekernelpls') with parity to 'pls'. Optimized for\n  'bigmemory'-backed matrices with streamed cross-products and chunked BLAS (Basic Linear Algebra Subprograms)\n  (XtX/XtY and XXt/YX), optional file-backed score sinks, and deterministic\n  testing helpers. Includes an auto-selection strategy that chooses between\n  XtX SIMPLS, XXt (wide) SIMPLS, and NIPALS based on (n, p) and a configurable\n  memory budget. About the package, Bertrand and Maumy (2023) <https://hal.science/hal-05352069>, \n  and  <https://hal.science/hal-05352061> highlighted fitting and cross-validating \n  PLS regression models to big data. For more details about some of the techniques \n  featured in the package, Dayal and MacGregor (1997) \n  <doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM435%3E3.0.CO;2-%23>,\n  Rosipal & Trejo (2001) <https://www.jmlr.org/papers/v2/rosipal01a.html>,\n  Tenenhaus, Viennet, and Saporta (2007) <doi:10.1016/j.csda.2007.01.004>,\n  Rosipal (2004) <doi:10.1007/978-3-540-45167-9_17>,\n  Rosipal (2019) <https://ieeexplore.ieee.org/document/8616346>,\n  Song, Wang, and Bai (2024) <doi:10.1016/j.chemolab.2024.105238>.\n  Includes kernel logistic PLS with 'C++'-accelerated alternating iteratively \n  reweighted least squares (IRLS) updates, streamed reproducing kernel Hilbert space (RKHS) \n  solvers with reusable centering statistics, and bootstrap\n  diagnostics with graphical summaries for coefficients, scores, and\n  cross-validation workflows, alongside dedicated plotting utilities for\n  individuals, variables, ellipses, and biplots.\n  The streaming backend uses far less memory and keeps memory bounded across data sizes.\n\tFor PLS1, streaming is often fast enough while preserving a small memory footprint; \n\tfor PLS2 it remains competitive with a bounded footprint.\n\tOn small problems that fit comfortably in RAM (random-access memory), dense in-memory \n\tsolvers are slightly faster; the crossover occurs as n or p grow and the Gram/cross-product cost dominates.",
    "version": "0.7.2",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Frederic Bertrand [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-0837-8281>),\n  Myriam Maumy [aut] (ORCID: <https://orcid.org/0000-0002-4615-1512>)",
    "url": "https://fbertran.github.io/bigPLSR/,\nhttps://github.com/fbertran/bigPLSR",
    "bug_reports": "https://github.com/fbertran/bigPLSR/issues",
    "repository": "https://cran.r-project.org/package=bigPLSR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigPLSR Partial Least Squares Regression Models with Big Matrices \n  Fast partial least squares (PLS) for dense and out-of-core data.\n  Provides SIMPLS (straightforward implementation of a statistically inspired \n  modification of the PLS method) and NIPALS (non-linear iterative partial least-squares) solvers, \n  plus kernel-style PLS variants ('kernelpls' and 'widekernelpls') with parity to 'pls'. Optimized for\n  'bigmemory'-backed matrices with streamed cross-products and chunked BLAS (Basic Linear Algebra Subprograms)\n  (XtX/XtY and XXt/YX), optional file-backed score sinks, and deterministic\n  testing helpers. Includes an auto-selection strategy that chooses between\n  XtX SIMPLS, XXt (wide) SIMPLS, and NIPALS based on (n, p) and a configurable\n  memory budget. About the package, Bertrand and Maumy (2023) <https://hal.science/hal-05352069>, \n  and  <https://hal.science/hal-05352061> highlighted fitting and cross-validating \n  PLS regression models to big data. For more details about some of the techniques \n  featured in the package, Dayal and MacGregor (1997) \n  <doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM435%3E3.0.CO;2-%23>,\n  Rosipal & Trejo (2001) <https://www.jmlr.org/papers/v2/rosipal01a.html>,\n  Tenenhaus, Viennet, and Saporta (2007) <doi:10.1016/j.csda.2007.01.004>,\n  Rosipal (2004) <doi:10.1007/978-3-540-45167-9_17>,\n  Rosipal (2019) <https://ieeexplore.ieee.org/document/8616346>,\n  Song, Wang, and Bai (2024) <doi:10.1016/j.chemolab.2024.105238>.\n  Includes kernel logistic PLS with 'C++'-accelerated alternating iteratively \n  reweighted least squares (IRLS) updates, streamed reproducing kernel Hilbert space (RKHS) \n  solvers with reusable centering statistics, and bootstrap\n  diagnostics with graphical summaries for coefficients, scores, and\n  cross-validation workflows, alongside dedicated plotting utilities for\n  individuals, variables, ellipses, and biplots.\n  The streaming backend uses far less memory and keeps memory bounded across data sizes.\n\tFor PLS1, streaming is often fast enough while preserving a small memory footprint; \n\tfor PLS2 it remains competitive with a bounded footprint.\n\tOn small problems that fit comfortably in RAM (random-access memory), dense in-memory \n\tsolvers are slightly faster; the crossover occurs as n or p grow and the Gram/cross-product cost dominates.  "
  },
  {
    "id": 9151,
    "package_name": "binb",
    "title": "'binb' is not 'Beamer'",
    "description": "A collection of 'LaTeX' styles using 'Beamer' customization for\n pdf-based presentation slides in 'RMarkdown'. At present it contains\n 'RMarkdown' adaptations of the LaTeX themes 'Metropolis' (formerly 'mtheme')\n theme by Matthias Vogelgesang and others (now included in 'TeXLive'), the\n 'IQSS' by Ista Zahn (which is included here), and the 'Monash' theme by\n Rob J Hyndman. Additional (free) fonts may be needed: 'Metropolis' prefers\n 'Fira', and 'IQSS' requires 'Libertinus'.",
    "version": "0.0.7",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel, Ista Zahn and Rob Hyndman",
    "url": "https://github.com/eddelbuettel/binb,\nhttps://dirk.eddelbuettel.com/code/binb.html",
    "bug_reports": "https://github.com/eddelbuettel/binb/issues",
    "repository": "https://cran.r-project.org/package=binb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binb 'binb' is not 'Beamer' A collection of 'LaTeX' styles using 'Beamer' customization for\n pdf-based presentation slides in 'RMarkdown'. At present it contains\n 'RMarkdown' adaptations of the LaTeX themes 'Metropolis' (formerly 'mtheme')\n theme by Matthias Vogelgesang and others (now included in 'TeXLive'), the\n 'IQSS' by Ista Zahn (which is included here), and the 'Monash' theme by\n Rob J Hyndman. Additional (free) fonts may be needed: 'Metropolis' prefers\n 'Fira', and 'IQSS' requires 'Libertinus'.  "
  },
  {
    "id": 9157,
    "package_name": "binford",
    "title": "Binford's Hunter-Gatherer Data",
    "description": "Binford's hunter-gatherer data includes more than 200 variables\n    coding aspects of hunter-gatherer subsistence, mobility, and social organization\n    for 339 ethnographically documented groups of hunter-gatherers.",
    "version": "0.1.0",
    "maintainer": "Ben Marwick <benmarwick@gmail.com>",
    "author": "Ben Marwick [aut, cre],\n  Amber Johnson [aut],\n  Doug White [aut],\n  E. Anthon Eff [aut]",
    "url": "http://github.com/benmarwick/binford",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=binford",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binford Binford's Hunter-Gatherer Data Binford's hunter-gatherer data includes more than 200 variables\n    coding aspects of hunter-gatherer subsistence, mobility, and social organization\n    for 339 ethnographically documented groups of hunter-gatherers.  "
  },
  {
    "id": 9177,
    "package_name": "bioC.logs",
    "title": "BioConductor Package Downloads Stats",
    "description": "Download stats reported from the BioConductor.org stats website.",
    "version": "1.2.1",
    "maintainer": "Marcelo Ponce <m.ponce@utoronto.ca>",
    "author": "Marcelo Ponce [aut, cre]",
    "url": "https://github.com/mponce0/bioC.logs",
    "bug_reports": "https://github.com/mponce0/bioC.logs/issues",
    "repository": "https://cran.r-project.org/package=bioC.logs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bioC.logs BioConductor Package Downloads Stats Download stats reported from the BioConductor.org stats website.  "
  },
  {
    "id": 9181,
    "package_name": "biobricks",
    "title": "Access Data Dependencies Installed Through 'Biobricks.ai'",
    "description": "Provides an integrated data management solution for assets installed via the 'Biobricks.ai' platform. Streamlines the process of loading and interacting with diverse datasets in a consistent manner. A list of bricks is available at <https://status.biobricks.ai>. Documentation for 'Biobricks.ai' is available at <https://docs.biobricks.ai>.",
    "version": "0.2.2",
    "maintainer": "Thomas Luechtefeld <tom@insilica.co>",
    "author": "Thomas Luechtefeld [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biobricks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biobricks Access Data Dependencies Installed Through 'Biobricks.ai' Provides an integrated data management solution for assets installed via the 'Biobricks.ai' platform. Streamlines the process of loading and interacting with diverse datasets in a consistent manner. A list of bricks is available at <https://status.biobricks.ai>. Documentation for 'Biobricks.ai' is available at <https://docs.biobricks.ai>.  "
  },
  {
    "id": 9182,
    "package_name": "bioclim",
    "title": "Bioclimatic Analysis and Classification",
    "description": "Using numeric or raster data, this package contains functions to \n    calculate: complete water balance, bioclimatic balance, bioclimatic \n    intensities, reports for individual locations, multi-layered rasters for\n    spatial analysis.",
    "version": "0.4.0",
    "maintainer": "Roberto Serrano-Notivoli <roberto.serrano@unizar.es>",
    "author": "Roberto Serrano-Notivoli",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bioclim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bioclim Bioclimatic Analysis and Classification Using numeric or raster data, this package contains functions to \n    calculate: complete water balance, bioclimatic balance, bioclimatic \n    intensities, reports for individual locations, multi-layered rasters for\n    spatial analysis.  "
  },
  {
    "id": 9183,
    "package_name": "biocompute",
    "title": "Create and Manipulate BioCompute Objects",
    "description": "Tools to create, validate, and export BioCompute Objects\n    described in King et al. (2019) <doi:10.17605/osf.io/h59uh>.\n    Users can encode information in data frames, and compose\n    BioCompute Objects from the domains defined by the standard.\n    A checksum validator and a JSON schema validator are provided.\n    This package also supports exporting BioCompute Objects as JSON,\n    PDF, HTML, or 'Word' documents, and exporting to cloud-based platforms.",
    "version": "1.1.1",
    "maintainer": "Soner Koc <soner.koc@sevenbridges.com>",
    "author": "Soner Koc [aut, cre] (ORCID: <https://orcid.org/0000-0002-0772-6600>),\n  Jeffrey Grover [aut] (ORCID: <https://orcid.org/0000-0001-6246-1767>),\n  Nan Xiao [aut] (ORCID: <https://orcid.org/0000-0002-0250-5673>),\n  Dennis Dean [aut] (ORCID: <https://orcid.org/0000-0002-7621-9717>),\n  Seven Bridges Genomics [cph, fnd]",
    "url": "https://sbg.github.io/biocompute/,\nhttps://github.com/sbg/biocompute",
    "bug_reports": "https://github.com/sbg/biocompute/issues",
    "repository": "https://cran.r-project.org/package=biocompute",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biocompute Create and Manipulate BioCompute Objects Tools to create, validate, and export BioCompute Objects\n    described in King et al. (2019) <doi:10.17605/osf.io/h59uh>.\n    Users can encode information in data frames, and compose\n    BioCompute Objects from the domains defined by the standard.\n    A checksum validator and a JSON schema validator are provided.\n    This package also supports exporting BioCompute Objects as JSON,\n    PDF, HTML, or 'Word' documents, and exporting to cloud-based platforms.  "
  },
  {
    "id": 9184,
    "package_name": "biogas",
    "title": "Process Biogas Data and Predict Biogas Production",
    "description": "Functions for calculating biochemical methane potential (BMP) from laboratory measurements and other types of data processing and prediction useful for biogas research. Raw laboratory measurements for diverse methods (volumetric, manometric, gravimetric, gas density) can be processed to calculate BMP. Theoretical maximum BMP or methane or biogas yield can be predicted from various measures of substrate composition. Molar mass and calculated oxygen demand (COD') can be determined from a chemical formula. Measured gas volume can be corrected for water vapor and to standard (or user-defined) temperature and pressure. Gas quantity can be converted between volume, mass, and moles. A function for planning BMP experiments can consider multiple constraints in suggesting substrate or inoculum quantities, and check for problems. Inoculum and substrate mass can be determined for planning BMP experiments. Finally, a set of first-order models can be fit to measured methane production rate or cumulative yield in order to extract estimates of ultimate yield and kinetic constants. See Hafner et al. (2018) <doi:10.1016/j.softx.2018.06.005> for details. OBA is a web application that provides access to some of the package functionality: <https://biotransformers.shinyapps.io/oba1/>. The Standard BMP Methods website documents the calculations in detail: <https://www.dbfz.de/en/BMP>.",
    "version": "1.64.0",
    "maintainer": "Sasha D. Hafner <sasha.hafner@bce.au.dk>",
    "author": "Sasha D. Hafner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0955-0327>),\n  Charlotte Rennuit [aut],\n  Camilla Justesen [aut],\n  Nanna Lojborg [aut],\n  Jacob Mortensen [aut],\n  Jonas Ohlsson [aut],\n  Sergi Astals [ctb],\n  Konrad Koch [ctb],\n  Soeren Weinrich [ctb],\n  Jin Mi Triolo [ctb],\n  Ali Heidarzadeh Vazifehkhoran [ctb]",
    "url": "https://github.com/sashahafner/biogas/, https://www.dbfz.de/en/BMP",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biogas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biogas Process Biogas Data and Predict Biogas Production Functions for calculating biochemical methane potential (BMP) from laboratory measurements and other types of data processing and prediction useful for biogas research. Raw laboratory measurements for diverse methods (volumetric, manometric, gravimetric, gas density) can be processed to calculate BMP. Theoretical maximum BMP or methane or biogas yield can be predicted from various measures of substrate composition. Molar mass and calculated oxygen demand (COD') can be determined from a chemical formula. Measured gas volume can be corrected for water vapor and to standard (or user-defined) temperature and pressure. Gas quantity can be converted between volume, mass, and moles. A function for planning BMP experiments can consider multiple constraints in suggesting substrate or inoculum quantities, and check for problems. Inoculum and substrate mass can be determined for planning BMP experiments. Finally, a set of first-order models can be fit to measured methane production rate or cumulative yield in order to extract estimates of ultimate yield and kinetic constants. See Hafner et al. (2018) <doi:10.1016/j.softx.2018.06.005> for details. OBA is a web application that provides access to some of the package functionality: <https://biotransformers.shinyapps.io/oba1/>. The Standard BMP Methods website documents the calculations in detail: <https://www.dbfz.de/en/BMP>.  "
  },
  {
    "id": 9190,
    "package_name": "biolink",
    "title": "Create Hyperlinks to Biological Databases and Resources",
    "description": "Generate urls and hyperlinks to commonly used biological databases\n    and resources based on standard identifiers. This is primarily useful when\n    writing dynamic reports that reference things like gene symbols in text or\n    tables, allowing you to, for example, convert gene identifiers to hyperlinks\n    pointing to their entry in the 'NCBI' Gene database. Currently supports\n    'NCBI' Gene, 'PubMed', Gene Ontology, 'KEGG', CRAN and Bioconductor.",
    "version": "0.1.8",
    "maintainer": "Aaron Wolen <aaron@wolen.com>",
    "author": "Aaron Wolen [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biolink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biolink Create Hyperlinks to Biological Databases and Resources Generate urls and hyperlinks to commonly used biological databases\n    and resources based on standard identifiers. This is primarily useful when\n    writing dynamic reports that reference things like gene symbols in text or\n    tables, allowing you to, for example, convert gene identifiers to hyperlinks\n    pointing to their entry in the 'NCBI' Gene database. Currently supports\n    'NCBI' Gene, 'PubMed', Gene Ontology, 'KEGG', CRAN and Bioconductor.  "
  },
  {
    "id": 9193,
    "package_name": "biometryassist",
    "title": "Functions to Assist Design and Analysis of Agronomic Experiments",
    "description": "Provides functions to aid in the design and analysis of\n    agronomic and agricultural experiments through easy access to\n    documentation and helper functions, especially for users who are\n    learning these concepts. While not required for most functionality,\n    this package enhances the `asreml` package which provides a \n    computationally efficient algorithm for fitting mixed models \n    using Residual Maximum Likelihood. It is a commercial package \n    that can be purchased as 'asreml-R' from 'VSNi' \n    <https://vsni.co.uk/>, who will supply a zip file for local \n    installation/updating (see <https://asreml.kb.vsni.co.uk/>). ",
    "version": "1.3.3",
    "maintainer": "Sam Rogers <biometrytraining@adelaide.edu.au>",
    "author": "Sharon Nielsen [aut],\n  Sam Rogers [aut, cre],\n  Annie Conway [aut],\n  University of Adelaide [cph, fnd] (https://adelaide.edu.au/),\n  Grains Research and Development Corporation [cph, fnd]\n    (https://grdc.com.au/)",
    "url": "https://biometryhub.github.io/biometryassist/",
    "bug_reports": "https://github.com/biometryhub/biometryassist/issues",
    "repository": "https://cran.r-project.org/package=biometryassist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biometryassist Functions to Assist Design and Analysis of Agronomic Experiments Provides functions to aid in the design and analysis of\n    agronomic and agricultural experiments through easy access to\n    documentation and helper functions, especially for users who are\n    learning these concepts. While not required for most functionality,\n    this package enhances the `asreml` package which provides a \n    computationally efficient algorithm for fitting mixed models \n    using Residual Maximum Likelihood. It is a commercial package \n    that can be purchased as 'asreml-R' from 'VSNi' \n    <https://vsni.co.uk/>, who will supply a zip file for local \n    installation/updating (see <https://asreml.kb.vsni.co.uk/>).   "
  },
  {
    "id": 9295,
    "package_name": "bnRep",
    "title": "A Repository of Bayesian Networks from the Academic Literature",
    "description": "A collection of Bayesian networks (discrete, Gaussian, and conditional linear Gaussian) collated from recent academic literature. The 'bnRep_summary' object provides an overview of the Bayesian networks in the repository and the package documentation includes details about the variables in each network. A Shiny app to explore the repository can be launched with 'bnRep_app()' and is available online at <https://manueleleonelli.shinyapps.io/bnRep>. Reference: 'M. Leonelli' (2025) <doi:10.1016/j.neucom.2025.129502>.",
    "version": "0.0.6",
    "maintainer": "Manuele Leonelli <manuele.leonelli@ie.edu>",
    "author": "Manuele Leonelli [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2562-5192>)",
    "url": "https://github.com/manueleleonelli/bnRep",
    "bug_reports": "https://github.com/manueleleonelli/bnRep/issues",
    "repository": "https://cran.r-project.org/package=bnRep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bnRep A Repository of Bayesian Networks from the Academic Literature A collection of Bayesian networks (discrete, Gaussian, and conditional linear Gaussian) collated from recent academic literature. The 'bnRep_summary' object provides an overview of the Bayesian networks in the repository and the package documentation includes details about the variables in each network. A Shiny app to explore the repository can be launched with 'bnRep_app()' and is available online at <https://manueleleonelli.shinyapps.io/bnRep>. Reference: 'M. Leonelli' (2025) <doi:10.1016/j.neucom.2025.129502>.  "
  },
  {
    "id": 9315,
    "package_name": "boilerplate",
    "title": "Managing and Compiling Manuscript Templates",
    "description": "Managing and generating standardised text for methods and results sections of scientific reports. It handles template variable substitution and supports hierarchical organisation of text through dot-separated paths. The package supports both RDS and JSON database formats, enabling version control and cross-language compatibility.",
    "version": "1.3.0",
    "maintainer": "Joseph Bulbulia <joseph.bulbulia@gmail.com>",
    "author": "Joseph Bulbulia [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5861-2056>)",
    "url": "https://go-bayes.github.io/boilerplate/,\nhttps://github.com/go-bayes/boilerplate",
    "bug_reports": "https://github.com/go-bayes/boilerplate/issues",
    "repository": "https://cran.r-project.org/package=boilerplate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "boilerplate Managing and Compiling Manuscript Templates Managing and generating standardised text for methods and results sections of scientific reports. It handles template variable substitution and supports hierarchical organisation of text through dot-separated paths. The package supports both RDS and JSON database formats, enabling version control and cross-language compatibility.  "
  },
  {
    "id": 9324,
    "package_name": "boodd",
    "title": "Functions for the Book \"Bootstrap for Dependent Data, with an R\nPackage\"",
    "description": "Companion package, functions, data sets, examples for the book \n        Patrice Bertail and Anna Dudek (2025), Bootstrap for Dependent Data, with an R package (by Bernard Desgraupes and Karolina Marek) - submitted.\n    Kreiss, J.-P. and Paparoditis, E. (2003) <doi:10.1214/aos/1074290332>\n    Politis, D.N., and White, H. (2004) <doi:10.1081/ETC-120028836>\n    Patton, A., Politis, D.N., and White, H. (2009) <doi:10.1080/07474930802459016>\n    Tsybakov, A. B. (2018) <doi:10.1007/b13794>\n    Bickel, P., and Sakov, A. (2008) <doi:10.1214/18-AOS1803>\n    G\u00f6tze, F. and Ra\u010dkauskas, A. (2001) <doi:10.1214/lnms/1215090074>\n    Politis, D. N., Romano, J. P., & Wolf, M. (1999, ISBN:978-0-387-98854-2)\n    Carlstein E. (1986) <doi:10.1214/aos/1176350057>\n    K\u00fcnsch, H. (1989) <doi:10.1214/aos/1176347265>\n    Liu, R. and Singh, K. (1992) <https://www.stat.purdue.edu/docs/research/tech-reports/1991/tr91-07.pdf>\n    Politis, D.N. and Romano, J.P. (1994) <doi:10.1080/01621459.1994.10476870>\n    Politis, D.N. and Romano, J.P. (1992) <https://www.stat.purdue.edu/docs/research/tech-reports/1991/tr91-07.pdf>\n    Patrice Bertail, Anna E. Dudek. (2022) <doi:10.3150/23-BEJ1683>\n    Dudek, A.E., Le\u015bkow, J., Paparoditis, E. and Politis, D. (2014a) <https://ideas.repec.org/a/bla/jtsera/v35y2014i2p89-114.html>\n    Beran, R. (1997) <doi:10.1023/A:1003114420352>\n    B. Efron, and Tibshirani, R. (1993, ISBN:9780429246593)\n    Bickel, P. J., G\u00f6tze, F. and van Zwet, W. R. (1997) <doi:10.1007/978-1-4614-1314-1_17>\n    A. C. Davison, D. Hinkley (1997) <doi:10.2307/1271471>\n    Falk, M., & Reiss, R. D. (1989) <doi:10.1007/BF00354758>\n    Lahiri, S. N. (2003) <doi:10.1007/978-1-4757-3803-2>\n    Shimizu, K. .(2017) <doi:10.1007/978-3-8348-9778-7>\n    Park, J.Y. (2003) <doi:10.1111/1468-0262.00471>\n    Kirch, C. and Politis, D. N. (2011) <doi:10.48550/arXiv.1211.4732>\n    Bertail, P. and Dudek, A.E. (2024) <doi:10.3150/23-BEJ1683>\n    Dudek, A. E. (2015) <doi:10.1007/s00184-014-0505-9>\n    Dudek, A. E. (2018) <doi:10.1080/10485252.2017.1404060>\n    Bertail, P., Cl\u00e9men\u00e7on, S. (2006a) <https://ideas.repec.org/p/crs/wpaper/2004-47.html>\n    Bertail, P. and Cl\u00e9men\u00e7on, S. (2006, ISBN:978-0-387-36062-1)\n    Radulovi\u0107, D. (2006) <doi:10.1007/BF02603005>\n    Bertail, P. Politis, D. N. Rhomari, N. (2000) <doi:10.1080/02331880008802701>\n    Nordman, D.J. Lahiri, S.N.(2004) <doi:10.1214/009053604000000779>\n    Politis, D.N. Romano, J.P. (1993) <doi:10.1006/jmva.1993.1085>\n    Hurvich, C. M. and Zeger, S. L. (1987, ISBN:978-1-4612-0099-4)\n    Bertail, P. and Dudek, A. (2021) <doi:10.1214/20-EJS1787>\n    Bertail, P., Cl\u00e9men\u00e7on, S. and Tressou, J. (2015) <doi:10.1111/jtsa.12105>\n    Asmussen, S. (1987) <doi:10.1007/978-3-662-11657-9>\n    Efron, B. (1979) <doi:10.1214/aos/1176344552>\n    Gray, H., Schucany, W. and Watkins, T. (1972) <doi:10.2307/2335521>\n    Quenouille, M.H. (1949) <doi:10.1111/j.2517-6161.1949.tb00023.x>\n    Quenouille, M. H. (1956) <doi:10.2307/2332914>\n    Prakasa Rao, B. L. S. and Kulperger, R. J. (1989) <https://www.jstor.org/stable/25050735>\n    Rajarshi, M.B. (1990) <doi:10.1007/BF00050835>\n    Dudek, A.E. Maiz, S. and Elbadaoui, M. (2014) <doi:10.1016/j.sigpro.2014.04.022>\n    Beran R. (1986) <doi:10.1214/aos/1176349847>\n    Maritz, J. S. and Jarrett, R. G. (1978) <doi:10.2307/2286545>\n    Bertail, P., Politis, D., Romano, J. (1999) <doi:10.2307/2670177>\n    Bertail, P. and Cl\u00e9men\u00e7on, S. (2006b) <doi:10.1007/0-387-36062-X_1>\n    Radulovi\u0107, D. (2004) <doi:10.1007/BF02603005>\n    Hurd, H.L., Miamee, A.G. (2007) <doi:10.1002/9780470182833>\n    B\u00fchlmann, P. (1997) <doi:10.2307/3318584>\n    Choi, E., Hall, P. (2000) <doi:10.1111/1467-9868.00244>\n    Efron, B., Tibshirani, R. (1993, ISBN:9780429246593)\n    Bertail, P., Cl\u00e9men\u00e7on, S. and Tressou, J. (2009) <doi:10.1007/s10687-009-0081-y>\n    Bertail, P., Medina-Garay, A., De Lima-Medina, F. and Jales, I. (2024) <doi:10.1080/02331888.2024.2344670>.",
    "version": "0.1",
    "maintainer": "Karolina Marek <karolina.marek10@gmail.com>",
    "author": "Patrice Bertail [aut, ctb],\n  Bernard Desgraupes [aut],\n  Anna Dudek [aut, ctb],\n  Karolina Marek [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=boodd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "boodd Functions for the Book \"Bootstrap for Dependent Data, with an R\nPackage\" Companion package, functions, data sets, examples for the book \n        Patrice Bertail and Anna Dudek (2025), Bootstrap for Dependent Data, with an R package (by Bernard Desgraupes and Karolina Marek) - submitted.\n    Kreiss, J.-P. and Paparoditis, E. (2003) <doi:10.1214/aos/1074290332>\n    Politis, D.N., and White, H. (2004) <doi:10.1081/ETC-120028836>\n    Patton, A., Politis, D.N., and White, H. (2009) <doi:10.1080/07474930802459016>\n    Tsybakov, A. B. (2018) <doi:10.1007/b13794>\n    Bickel, P., and Sakov, A. (2008) <doi:10.1214/18-AOS1803>\n    G\u00f6tze, F. and Ra\u010dkauskas, A. (2001) <doi:10.1214/lnms/1215090074>\n    Politis, D. N., Romano, J. P., & Wolf, M. (1999, ISBN:978-0-387-98854-2)\n    Carlstein E. (1986) <doi:10.1214/aos/1176350057>\n    K\u00fcnsch, H. (1989) <doi:10.1214/aos/1176347265>\n    Liu, R. and Singh, K. (1992) <https://www.stat.purdue.edu/docs/research/tech-reports/1991/tr91-07.pdf>\n    Politis, D.N. and Romano, J.P. (1994) <doi:10.1080/01621459.1994.10476870>\n    Politis, D.N. and Romano, J.P. (1992) <https://www.stat.purdue.edu/docs/research/tech-reports/1991/tr91-07.pdf>\n    Patrice Bertail, Anna E. Dudek. (2022) <doi:10.3150/23-BEJ1683>\n    Dudek, A.E., Le\u015bkow, J., Paparoditis, E. and Politis, D. (2014a) <https://ideas.repec.org/a/bla/jtsera/v35y2014i2p89-114.html>\n    Beran, R. (1997) <doi:10.1023/A:1003114420352>\n    B. Efron, and Tibshirani, R. (1993, ISBN:9780429246593)\n    Bickel, P. J., G\u00f6tze, F. and van Zwet, W. R. (1997) <doi:10.1007/978-1-4614-1314-1_17>\n    A. C. Davison, D. Hinkley (1997) <doi:10.2307/1271471>\n    Falk, M., & Reiss, R. D. (1989) <doi:10.1007/BF00354758>\n    Lahiri, S. N. (2003) <doi:10.1007/978-1-4757-3803-2>\n    Shimizu, K. .(2017) <doi:10.1007/978-3-8348-9778-7>\n    Park, J.Y. (2003) <doi:10.1111/1468-0262.00471>\n    Kirch, C. and Politis, D. N. (2011) <doi:10.48550/arXiv.1211.4732>\n    Bertail, P. and Dudek, A.E. (2024) <doi:10.3150/23-BEJ1683>\n    Dudek, A. E. (2015) <doi:10.1007/s00184-014-0505-9>\n    Dudek, A. E. (2018) <doi:10.1080/10485252.2017.1404060>\n    Bertail, P., Cl\u00e9men\u00e7on, S. (2006a) <https://ideas.repec.org/p/crs/wpaper/2004-47.html>\n    Bertail, P. and Cl\u00e9men\u00e7on, S. (2006, ISBN:978-0-387-36062-1)\n    Radulovi\u0107, D. (2006) <doi:10.1007/BF02603005>\n    Bertail, P. Politis, D. N. Rhomari, N. (2000) <doi:10.1080/02331880008802701>\n    Nordman, D.J. Lahiri, S.N.(2004) <doi:10.1214/009053604000000779>\n    Politis, D.N. Romano, J.P. (1993) <doi:10.1006/jmva.1993.1085>\n    Hurvich, C. M. and Zeger, S. L. (1987, ISBN:978-1-4612-0099-4)\n    Bertail, P. and Dudek, A. (2021) <doi:10.1214/20-EJS1787>\n    Bertail, P., Cl\u00e9men\u00e7on, S. and Tressou, J. (2015) <doi:10.1111/jtsa.12105>\n    Asmussen, S. (1987) <doi:10.1007/978-3-662-11657-9>\n    Efron, B. (1979) <doi:10.1214/aos/1176344552>\n    Gray, H., Schucany, W. and Watkins, T. (1972) <doi:10.2307/2335521>\n    Quenouille, M.H. (1949) <doi:10.1111/j.2517-6161.1949.tb00023.x>\n    Quenouille, M. H. (1956) <doi:10.2307/2332914>\n    Prakasa Rao, B. L. S. and Kulperger, R. J. (1989) <https://www.jstor.org/stable/25050735>\n    Rajarshi, M.B. (1990) <doi:10.1007/BF00050835>\n    Dudek, A.E. Maiz, S. and Elbadaoui, M. (2014) <doi:10.1016/j.sigpro.2014.04.022>\n    Beran R. (1986) <doi:10.1214/aos/1176349847>\n    Maritz, J. S. and Jarrett, R. G. (1978) <doi:10.2307/2286545>\n    Bertail, P., Politis, D., Romano, J. (1999) <doi:10.2307/2670177>\n    Bertail, P. and Cl\u00e9men\u00e7on, S. (2006b) <doi:10.1007/0-387-36062-X_1>\n    Radulovi\u0107, D. (2004) <doi:10.1007/BF02603005>\n    Hurd, H.L., Miamee, A.G. (2007) <doi:10.1002/9780470182833>\n    B\u00fchlmann, P. (1997) <doi:10.2307/3318584>\n    Choi, E., Hall, P. (2000) <doi:10.1111/1467-9868.00244>\n    Efron, B., Tibshirani, R. (1993, ISBN:9780429246593)\n    Bertail, P., Cl\u00e9men\u00e7on, S. and Tressou, J. (2009) <doi:10.1007/s10687-009-0081-y>\n    Bertail, P., Medina-Garay, A., De Lima-Medina, F. and Jales, I. (2024) <doi:10.1080/02331888.2024.2344670>.  "
  },
  {
    "id": 9351,
    "package_name": "bootwar",
    "title": "Nonparametric Bootstrap Test with Pooled Resampling Card Game",
    "description": "The card game War is simple in its rules but can be lengthy. In\n    another domain, the nonparametric bootstrap test with pooled resampling\n    (nbpr) methods, as outlined in Dwivedi, Mallawaarachchi, and Alvarado (2017) <doi:10.1002/sim.7263>,\n    is optimal for comparing paired or unpaired means in non-normal data,\n    especially for small sample size studies. However, many researchers are\n    unfamiliar with these methods. The 'bootwar' package bridges this gap by\n    enabling users to grasp the concepts of nbpr via Boot War, a variation of the\n    card game War designed for small samples. The package provides functions like\n    score_keeper() and play_round() to streamline gameplay and scoring. Once a\n    predetermined number of rounds concludes, users can employ the analyze_game()\n    function to derive game results. This function leverages the 'npboottprm'\n    package's nonparboot() to report nbpr results and, for comparative analysis,\n    also reports results from the 'stats' package's t.test() function. Additionally,\n    'bootwar' features an interactive 'shiny' web application, bootwar(). This\n    offers a user-centric interface to experience Boot War, enhancing understanding\n    of nbpr methods across various distributions, sample sizes, number of bootstrap\n    resamples, and confidence intervals.",
    "version": "0.2.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/bootwar",
    "bug_reports": "https://github.com/mightymetrika/bootwar/issues",
    "repository": "https://cran.r-project.org/package=bootwar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bootwar Nonparametric Bootstrap Test with Pooled Resampling Card Game The card game War is simple in its rules but can be lengthy. In\n    another domain, the nonparametric bootstrap test with pooled resampling\n    (nbpr) methods, as outlined in Dwivedi, Mallawaarachchi, and Alvarado (2017) <doi:10.1002/sim.7263>,\n    is optimal for comparing paired or unpaired means in non-normal data,\n    especially for small sample size studies. However, many researchers are\n    unfamiliar with these methods. The 'bootwar' package bridges this gap by\n    enabling users to grasp the concepts of nbpr via Boot War, a variation of the\n    card game War designed for small samples. The package provides functions like\n    score_keeper() and play_round() to streamline gameplay and scoring. Once a\n    predetermined number of rounds concludes, users can employ the analyze_game()\n    function to derive game results. This function leverages the 'npboottprm'\n    package's nonparboot() to report nbpr results and, for comparative analysis,\n    also reports results from the 'stats' package's t.test() function. Additionally,\n    'bootwar' features an interactive 'shiny' web application, bootwar(). This\n    offers a user-centric interface to experience Boot War, enhancing understanding\n    of nbpr methods across various distributions, sample sizes, number of bootstrap\n    resamples, and confidence intervals.  "
  },
  {
    "id": 9359,
    "package_name": "boussinesq",
    "title": "Analytic Solutions for (Ground-Water) Boussinesq Equation",
    "description": "A collection of R functions were implemented\n        from published and available analytic solutions for the\n        One-Dimensional Boussinesq Equation (ground-water). In\n        particular, the function \"beq.lin()\" is the analytic solution of\n        the linearized form of Boussinesq Equation between two\n        different head-based boundary (Dirichlet) conditions;\n        \"beq.song\" is the non-linear power-series analytic solution of\n        the motion of a wetting front over a dry bedrock (Song at al,\n        2007, see complete reference on function documentation).\n        Bugs/comments/questions/collaboration of any kind are warmly\n        welcomed.",
    "version": "1.0.6",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "author": "Emanuele Cordano",
    "url": "https://github.com/ecor/boussinesq,https://agupubs.onlinelibrary.wiley.com/doi/10.1002/wrcr.20072",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=boussinesq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "boussinesq Analytic Solutions for (Ground-Water) Boussinesq Equation A collection of R functions were implemented\n        from published and available analytic solutions for the\n        One-Dimensional Boussinesq Equation (ground-water). In\n        particular, the function \"beq.lin()\" is the analytic solution of\n        the linearized form of Boussinesq Equation between two\n        different head-based boundary (Dirichlet) conditions;\n        \"beq.song\" is the non-linear power-series analytic solution of\n        the motion of a wetting front over a dry bedrock (Song at al,\n        2007, see complete reference on function documentation).\n        Bugs/comments/questions/collaboration of any kind are warmly\n        welcomed.  "
  },
  {
    "id": 9390,
    "package_name": "braidReports",
    "title": "Visualize Combined Action Response Surfaces and Report BRAID\nAnalyses",
    "description": "Provides functions to visualize combined action data in 'ggplot2'.\n    Also provides functions for producing full BRAID analysis reports with \n    custom layouts and aesthetics, using the BRAID method originally described \n    in Twarog et al. (2016) <doi:10.1038/srep25523>.",
    "version": "1.0.5",
    "maintainer": "Nathaniel R. Twarog <nathaniel.twarog@stjude.org>",
    "author": "Anang Shelat [aut],\n  Nathaniel R. Twarog [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=braidReports",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "braidReports Visualize Combined Action Response Surfaces and Report BRAID\nAnalyses Provides functions to visualize combined action data in 'ggplot2'.\n    Also provides functions for producing full BRAID analysis reports with \n    custom layouts and aesthetics, using the BRAID method originally described \n    in Twarog et al. (2016) <doi:10.1038/srep25523>.  "
  },
  {
    "id": 9397,
    "package_name": "brand.yml",
    "title": "Unified Branding with a Simple YAML File",
    "description": "Read and process 'brand.yml' 'YAML' files. 'brand.yml' is a\n    simple, portable 'YAML' file that codifies your company's brand\n    guidelines into a format that can be used by 'Quarto', 'Shiny' and 'R'\n    tooling to create branded outputs. Maintain unified, branded theming\n    for web applications to printed reports to dashboards and\n    presentations with a consistent look and feel.",
    "version": "0.1.0",
    "maintainer": "Garrick Aden-Buie <garrick@posit.co>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>),\n  Posit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://posit-dev.github.io/brand-yml/pkg/r/,\nhttps://github.com/posit-dev/brand-yml",
    "bug_reports": "https://github.com/posit-dev/brand-yml/issues",
    "repository": "https://cran.r-project.org/package=brand.yml",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brand.yml Unified Branding with a Simple YAML File Read and process 'brand.yml' 'YAML' files. 'brand.yml' is a\n    simple, portable 'YAML' file that codifies your company's brand\n    guidelines into a format that can be used by 'Quarto', 'Shiny' and 'R'\n    tooling to create branded outputs. Maintain unified, branded theming\n    for web applications to printed reports to dashboards and\n    presentations with a consistent look and feel.  "
  },
  {
    "id": 9412,
    "package_name": "brew",
    "title": "Templating Framework for Report Generation",
    "description": "Implements a templating framework for mixing text and R code\n    for report generation. brew template syntax is similar to PHP, Ruby's\n    erb module, Java Server Pages, and Python's psp module.",
    "version": "1.0-10",
    "maintainer": "Greg Hunt <greg@firmansyah.com>",
    "author": "Jeffrey Horner [aut, cph],\n  Greg Hunt [aut, cre, cph]",
    "url": "https://github.com/gregfrog/brew",
    "bug_reports": "https://github.com/gregfrog/brew/issues",
    "repository": "https://cran.r-project.org/package=brew",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brew Templating Framework for Report Generation Implements a templating framework for mixing text and R code\n    for report generation. brew template syntax is similar to PHP, Ruby's\n    erb module, Java Server Pages, and Python's psp module.  "
  },
  {
    "id": 9417,
    "package_name": "brickset",
    "title": "Interface with the Brickset API for Getting Data About LEGO Sets",
    "description": "Interface with the 'Brickset' API\n    <https://brickset.com/article/52664/api-version-3-documentation> for\n    getting data about LEGO sets. Data sets that can be used for teaching\n    and learning without the need of a 'Brickset' account and API key are\n    also included. Includes all LEGO since through the end of 2023.",
    "version": "2025.0.0",
    "maintainer": "Jason Bryer <jason@bryer.org>",
    "author": "Jason Bryer [aut, cre] (ORCID: <https://orcid.org/0000-0002-2454-0402>)",
    "url": "https://github.com/jbryer/brickset,\nhttps://jbryer.github.io/brickset/",
    "bug_reports": "https://github.com/jbryer/brickset/issues",
    "repository": "https://cran.r-project.org/package=brickset",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brickset Interface with the Brickset API for Getting Data About LEGO Sets Interface with the 'Brickset' API\n    <https://brickset.com/article/52664/api-version-3-documentation> for\n    getting data about LEGO sets. Data sets that can be used for teaching\n    and learning without the need of a 'Brickset' account and API key are\n    also included. Includes all LEGO since through the end of 2023.  "
  },
  {
    "id": 9431,
    "package_name": "brokenstick",
    "title": "Broken Stick Model for Irregular Longitudinal Data",
    "description": "Data on multiple individuals through time are often sampled at \n    times that differ between persons. Irregular observation times can severely \n    complicate the statistical analysis of the data. The broken stick model \n    approximates each subject\u2019s trajectory by one or more connected line segments. \n    The times at which segments connect (breakpoints) are identical for all \n    subjects and under control of the user. A well-fitting broken stick model \n    effectively transforms individual measurements made at irregular times into \n    regular trajectories with common observation times. Specification of the \n    model requires three variables: time, measurement and subject. The \n    model is a special case of the linear mixed model, with time as a linear \n    B-spline and subject as the grouping factor. The main assumptions are: \n    subjects are exchangeable, trajectories between consecutive breakpoints are \n    straight, random effects follow a multivariate normal distribution, and \n    unobserved data are missing at random. The package contains functions for \n    fitting the broken stick model to data, for predicting curves in new data \n    and for plotting broken stick estimates. The package supports two \n    optimization methods, and includes options to structure the \n    variance-covariance matrix of the random effects. The analyst may use the \n    software to smooth growth curves by a series of connected straight lines, to \n    align irregularly observed curves to a common time grid, to create synthetic \n    curves at a user-specified set of breakpoints, to estimate the time-to-time \n    correlation matrix and to predict future observations. See \n    <doi:10.18637/jss.v106.i07> for additional documentation on background, \n    methodology and applications.",
    "version": "2.6.0",
    "maintainer": "Stef van Buuren <stef.vanbuuren@tno.nl>",
    "author": "Stef van Buuren [aut, cre]",
    "url": "doi:10.18637/jss.v106.i07, https://growthcharts.org/brokenstick/",
    "bug_reports": "https://github.com/growthcharts/brokenstick/issues",
    "repository": "https://cran.r-project.org/package=brokenstick",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brokenstick Broken Stick Model for Irregular Longitudinal Data Data on multiple individuals through time are often sampled at \n    times that differ between persons. Irregular observation times can severely \n    complicate the statistical analysis of the data. The broken stick model \n    approximates each subject\u2019s trajectory by one or more connected line segments. \n    The times at which segments connect (breakpoints) are identical for all \n    subjects and under control of the user. A well-fitting broken stick model \n    effectively transforms individual measurements made at irregular times into \n    regular trajectories with common observation times. Specification of the \n    model requires three variables: time, measurement and subject. The \n    model is a special case of the linear mixed model, with time as a linear \n    B-spline and subject as the grouping factor. The main assumptions are: \n    subjects are exchangeable, trajectories between consecutive breakpoints are \n    straight, random effects follow a multivariate normal distribution, and \n    unobserved data are missing at random. The package contains functions for \n    fitting the broken stick model to data, for predicting curves in new data \n    and for plotting broken stick estimates. The package supports two \n    optimization methods, and includes options to structure the \n    variance-covariance matrix of the random effects. The analyst may use the \n    software to smooth growth curves by a series of connected straight lines, to \n    align irregularly observed curves to a common time grid, to create synthetic \n    curves at a user-specified set of breakpoints, to estimate the time-to-time \n    correlation matrix and to predict future observations. See \n    <doi:10.18637/jss.v106.i07> for additional documentation on background, \n    methodology and applications.  "
  },
  {
    "id": 9438,
    "package_name": "bruceR",
    "title": "Broadly Useful Convenient and Efficient R Functions",
    "description": "\n    Broadly useful convenient and efficient R functions\n    that bring users concise and elegant R data analyses.\n    This package includes easy-to-use functions for\n    (1) basic R programming\n    (e.g., set working directory to the path of currently opened file;\n    import/export data from/to files in any format;\n    print tables to Microsoft Word);\n    (2) multivariate computation\n    (e.g., compute scale sums/means/... with reverse scoring);\n    (3) reliability analyses and factor analyses;\n    (4) descriptive statistics and correlation analyses;\n    (5) t-test, multi-factor analysis of variance (ANOVA),\n    simple-effect analysis, and post-hoc multiple comparison;\n    (6) tidy report of statistical models\n    (to R Console and Microsoft Word);\n    (7) mediation and moderation analyses (PROCESS);\n    and (8) additional toolbox for statistics and graphics.",
    "version": "2025.11",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/bruceR/",
    "bug_reports": "https://github.com/psychbruce/bruceR/issues",
    "repository": "https://cran.r-project.org/package=bruceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bruceR Broadly Useful Convenient and Efficient R Functions \n    Broadly useful convenient and efficient R functions\n    that bring users concise and elegant R data analyses.\n    This package includes easy-to-use functions for\n    (1) basic R programming\n    (e.g., set working directory to the path of currently opened file;\n    import/export data from/to files in any format;\n    print tables to Microsoft Word);\n    (2) multivariate computation\n    (e.g., compute scale sums/means/... with reverse scoring);\n    (3) reliability analyses and factor analyses;\n    (4) descriptive statistics and correlation analyses;\n    (5) t-test, multi-factor analysis of variance (ANOVA),\n    simple-effect analysis, and post-hoc multiple comparison;\n    (6) tidy report of statistical models\n    (to R Console and Microsoft Word);\n    (7) mediation and moderation analyses (PROCESS);\n    and (8) additional toolbox for statistics and graphics.  "
  },
  {
    "id": 9443,
    "package_name": "bs4cards",
    "title": "Generate Bootstrap Cards",
    "description": "Allows the user to generate bootstrap cards within\n    R markdown documents. Intended for use in conjunction with\n    R markdown HTML outputs and other formats that support the \n    bootstrap 4 library.",
    "version": "0.1.1",
    "maintainer": "Danielle Navarro <djnavarro@protonmail.com>",
    "author": "Danielle Navarro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7648-6578>)",
    "url": "https://github.com/djnavarro/bs4cards",
    "bug_reports": "https://github.com/djnavarro/bs4cards/issues",
    "repository": "https://cran.r-project.org/package=bs4cards",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bs4cards Generate Bootstrap Cards Allows the user to generate bootstrap cards within\n    R markdown documents. Intended for use in conjunction with\n    R markdown HTML outputs and other formats that support the \n    bootstrap 4 library.  "
  },
  {
    "id": 9447,
    "package_name": "bscui",
    "title": "Build SVG Custom User Interface",
    "description": "Render SVG as interactive figures to display contextual\n   information, with selectable and clickable user interface elements.\n   These figures can be seamlessly integrated into 'rmarkdown' and 'Quarto'\n   documents, as well as 'shiny' applications, allowing manipulation of elements\n   and reporting actions performed on them.\n   Additional features include pan, zoom in/out functionality, and the ability\n   to export the figures in SVG or PNG formats.",
    "version": "0.1.6",
    "maintainer": "Patrice Godard <patrice.godard@gmail.com>",
    "author": "Patrice Godard [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6257-9730>)",
    "url": "https://patzaw.github.io/bscui/, https://github.com/patzaw/bscui/",
    "bug_reports": "https://github.com/patzaw/bscui/issues",
    "repository": "https://cran.r-project.org/package=bscui",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bscui Build SVG Custom User Interface Render SVG as interactive figures to display contextual\n   information, with selectable and clickable user interface elements.\n   These figures can be seamlessly integrated into 'rmarkdown' and 'Quarto'\n   documents, as well as 'shiny' applications, allowing manipulation of elements\n   and reporting actions performed on them.\n   Additional features include pan, zoom in/out functionality, and the ability\n   to export the figures in SVG or PNG formats.  "
  },
  {
    "id": 9452,
    "package_name": "bskyr",
    "title": "Interact with 'Bluesky' Social",
    "description": "Collect data from and make posts on 'Bluesky' Social\n    via the Hypertext Transfer Protocol (HTTP) Application Programming \n    Interface (API), as documented at <https://atproto.com/specs/xrpc>. This \n    further supports broader queries to the Authenticated Transfer (AT) Protocol\n    <https://atproto.com/> which 'Bluesky' Social relies on. Data is returned in a\n    tidy format and posts can be made using a simple interface.",
    "version": "0.4.0",
    "maintainer": "Christopher T. Kenny <ctkenny@proton.me>",
    "author": "Christopher T. Kenny [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9386-6860>)",
    "url": "https://github.com/christopherkenny/bskyr,\nhttp://christophertkenny.com/bskyr/",
    "bug_reports": "https://github.com/christopherkenny/bskyr/issues",
    "repository": "https://cran.r-project.org/package=bskyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bskyr Interact with 'Bluesky' Social Collect data from and make posts on 'Bluesky' Social\n    via the Hypertext Transfer Protocol (HTTP) Application Programming \n    Interface (API), as documented at <https://atproto.com/specs/xrpc>. This \n    further supports broader queries to the Authenticated Transfer (AT) Protocol\n    <https://atproto.com/> which 'Bluesky' Social relies on. Data is returned in a\n    tidy format and posts can be made using a simple interface.  "
  },
  {
    "id": 9457,
    "package_name": "bsplus",
    "title": "Adds Functionality to the R Markdown + Shiny Bootstrap Framework",
    "description": "The Bootstrap framework lets you add some JavaScript functionality to your web site by\n  adding attributes to your HTML tags - Bootstrap takes care of the JavaScript\n  <https://getbootstrap.com/docs/3.3/javascript/>. If you are using R Markdown or Shiny, you can\n  use these functions to create collapsible sections, accordion panels, modals, tooltips,\n  popovers, and an accordion sidebar framework (not described at Bootstrap site).\n  Please note this package was designed for Bootstrap 3.3.",
    "version": "0.1.5",
    "maintainer": "Ian Lyttle <ijlyttle@me.com>",
    "author": "Ian Lyttle [aut, cre] (ORCID: <https://orcid.org/0000-0001-9962-4849>),\n  Alex Shum [ctb],\n  Emerson Berry [ctb]",
    "url": "https://ijlyttle.github.io/bsplus/,\nhttps://github.com/ijlyttle/bsplus",
    "bug_reports": "https://github.com/ijlyttle/bsplus/issues",
    "repository": "https://cran.r-project.org/package=bsplus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsplus Adds Functionality to the R Markdown + Shiny Bootstrap Framework The Bootstrap framework lets you add some JavaScript functionality to your web site by\n  adding attributes to your HTML tags - Bootstrap takes care of the JavaScript\n  <https://getbootstrap.com/docs/3.3/javascript/>. If you are using R Markdown or Shiny, you can\n  use these functions to create collapsible sections, accordion panels, modals, tooltips,\n  popovers, and an accordion sidebar framework (not described at Bootstrap site).\n  Please note this package was designed for Bootstrap 3.3.  "
  },
  {
    "id": 9467,
    "package_name": "bsvarSIGNs",
    "title": "Bayesian SVARs with Sign, Zero, and Narrative Restrictions",
    "description": "Implements state-of-the-art algorithms for the Bayesian analysis of Structural Vector Autoregressions (SVARs) identified by sign, zero, and narrative restrictions. The core model is based on a flexible Vector Autoregression with estimated hyper-parameters of the Minnesota prior and the dummy observation priors as in Giannone, Lenza, Primiceri (2015) <doi:10.1162/REST_a_00483>. The sign restrictions are implemented employing the methods proposed by Rubio-Ram\u00edrez, Waggoner & Zha (2010) <doi:10.1111/j.1467-937X.2009.00578.x>, while identification through sign and zero restrictions follows the approach developed by Arias, Rubio-Ram\u00edrez, & Waggoner (2018) <doi:10.3982/ECTA14468>. Furthermore, our tool provides algorithms for identification via sign and narrative restrictions, in line with the methods introduced by Antol\u00edn-D\u00edaz and Rubio-Ram\u00edrez (2018) <doi:10.1257/aer.20161852>. Users can also estimate a model with sign, zero, and narrative restrictions imposed at once. The package facilitates predictive and structural analyses using impulse responses, forecast error variance and historical decompositions, forecasting and conditional forecasting, as well as analyses of structural shocks and fitted values. All this is complemented by colourful plots, user-friendly summary functions, and comprehensive documentation including the vignette by Wang & Wo\u017aniak (2024) <doi:10.48550/arXiv.2501.16711>. The 'bsvarSIGNs' package is aligned regarding objects, workflows, and code structure with the R package 'bsvars' by Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars>, and they constitute an integrated toolset. It was granted the Di Cook Open-Source Statistical Software Award by the Statistical Society of Australia in 2024.",
    "version": "2.0",
    "maintainer": "Xiaolei Wang <adamwang15@gmail.com>",
    "author": "Xiaolei Wang [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-6192-9061>),\n  Tomasz Wo\u017aniak [aut] (ORCID: <https://orcid.org/0000-0003-2212-2378>)",
    "url": "https://bsvars.org/bsvarSIGNs/",
    "bug_reports": "https://github.com/bsvars/bsvarSIGNs/issues",
    "repository": "https://cran.r-project.org/package=bsvarSIGNs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsvarSIGNs Bayesian SVARs with Sign, Zero, and Narrative Restrictions Implements state-of-the-art algorithms for the Bayesian analysis of Structural Vector Autoregressions (SVARs) identified by sign, zero, and narrative restrictions. The core model is based on a flexible Vector Autoregression with estimated hyper-parameters of the Minnesota prior and the dummy observation priors as in Giannone, Lenza, Primiceri (2015) <doi:10.1162/REST_a_00483>. The sign restrictions are implemented employing the methods proposed by Rubio-Ram\u00edrez, Waggoner & Zha (2010) <doi:10.1111/j.1467-937X.2009.00578.x>, while identification through sign and zero restrictions follows the approach developed by Arias, Rubio-Ram\u00edrez, & Waggoner (2018) <doi:10.3982/ECTA14468>. Furthermore, our tool provides algorithms for identification via sign and narrative restrictions, in line with the methods introduced by Antol\u00edn-D\u00edaz and Rubio-Ram\u00edrez (2018) <doi:10.1257/aer.20161852>. Users can also estimate a model with sign, zero, and narrative restrictions imposed at once. The package facilitates predictive and structural analyses using impulse responses, forecast error variance and historical decompositions, forecasting and conditional forecasting, as well as analyses of structural shocks and fitted values. All this is complemented by colourful plots, user-friendly summary functions, and comprehensive documentation including the vignette by Wang & Wo\u017aniak (2024) <doi:10.48550/arXiv.2501.16711>. The 'bsvarSIGNs' package is aligned regarding objects, workflows, and code structure with the R package 'bsvars' by Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvars>, and they constitute an integrated toolset. It was granted the Di Cook Open-Source Statistical Software Award by the Statistical Society of Australia in 2024.  "
  },
  {
    "id": 9468,
    "package_name": "bsvars",
    "title": "Bayesian Estimation of Structural Vector Autoregressive Models",
    "description": "Provides fast and efficient procedures for Bayesian analysis of Structural Vector Autoregressions. This package estimates a wide range of models, including homo-, heteroskedastic, and non-normal specifications. Structural models can be identified by adjustable exclusion restrictions, time-varying volatility, or non-normality. They all include a flexible three-level equation-specific local-global hierarchical prior distribution for the estimated level of shrinkage for autoregressive and structural parameters. Additionally, the package facilitates predictive and structural analyses such as impulse responses, forecast error variance and historical decompositions, forecasting, verification of heteroskedasticity, non-normality, and hypotheses on autoregressive parameters, as well as analyses of structural shocks, volatilities, and fitted values. Beautiful plots, informative summary functions, and extensive documentation including the vignette by Wo\u017aniak (2024) <doi:10.48550/arXiv.2410.15090> complement all this. The implemented techniques align closely with those presented in L\u00fctkepohl, Shang, Uzeda, & Wo\u017aniak (2024) <doi:10.48550/arXiv.2404.11057>, L\u00fctkepohl & Wo\u017aniak (2020) <doi:10.1016/j.jedc.2020.103862>, and Song & Wo\u017aniak (2021) <doi:10.1093/acrefore/9780190625979.013.174>. The 'bsvars' package is aligned regarding objects, workflows, and code structure with the R package 'bsvarSIGNs' by Wang & Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they constitute an integrated toolset.",
    "version": "3.2",
    "maintainer": "Tomasz Wo\u017aniak <wozniak.tom@pm.me>",
    "author": "Tomasz Wo\u017aniak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2212-2378>)",
    "url": "https://bsvars.org/bsvars/",
    "bug_reports": "https://github.com/bsvars/bsvars/issues",
    "repository": "https://cran.r-project.org/package=bsvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bsvars Bayesian Estimation of Structural Vector Autoregressive Models Provides fast and efficient procedures for Bayesian analysis of Structural Vector Autoregressions. This package estimates a wide range of models, including homo-, heteroskedastic, and non-normal specifications. Structural models can be identified by adjustable exclusion restrictions, time-varying volatility, or non-normality. They all include a flexible three-level equation-specific local-global hierarchical prior distribution for the estimated level of shrinkage for autoregressive and structural parameters. Additionally, the package facilitates predictive and structural analyses such as impulse responses, forecast error variance and historical decompositions, forecasting, verification of heteroskedasticity, non-normality, and hypotheses on autoregressive parameters, as well as analyses of structural shocks, volatilities, and fitted values. Beautiful plots, informative summary functions, and extensive documentation including the vignette by Wo\u017aniak (2024) <doi:10.48550/arXiv.2410.15090> complement all this. The implemented techniques align closely with those presented in L\u00fctkepohl, Shang, Uzeda, & Wo\u017aniak (2024) <doi:10.48550/arXiv.2404.11057>, L\u00fctkepohl & Wo\u017aniak (2020) <doi:10.1016/j.jedc.2020.103862>, and Song & Wo\u017aniak (2021) <doi:10.1093/acrefore/9780190625979.013.174>. The 'bsvars' package is aligned regarding objects, workflows, and code structure with the R package 'bsvarSIGNs' by Wang & Wo\u017aniak (2024) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they constitute an integrated toolset.  "
  },
  {
    "id": 9474,
    "package_name": "btw",
    "title": "A Toolkit for Connecting R and Large Language Models",
    "description": "A complete toolkit for connecting 'R' environments with Large\n    Language Models (LLMs). Provides utilities for describing 'R' objects,\n    package documentation, and workspace state in plain text formats\n    optimized for LLM consumption. Supports multiple workflows:\n    interactive copy-paste to external chat interfaces, programmatic tool\n    registration with 'ellmer' chat clients, batteries-included chat\n    applications via 'shinychat', and exposure to external coding agents\n    through the Model Context Protocol. Project configuration files enable\n    stable, repeatable conversations with project-specific context and\n    preferred LLM settings.",
    "version": "1.0.0",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>),\n  Simon Couch [aut] (ORCID: <https://orcid.org/0000-0001-5676-5107>),\n  Joe Cheng [aut],\n  Posit Software, PBC [cph, fnd],\n  Google [cph] (Material Design Icons),\n  Jamie Perkins [cph] (countUp.js author)",
    "url": "https://github.com/posit-dev/btw, https://posit-dev.github.io/btw/",
    "bug_reports": "https://github.com/posit-dev/btw/issues",
    "repository": "https://cran.r-project.org/package=btw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "btw A Toolkit for Connecting R and Large Language Models A complete toolkit for connecting 'R' environments with Large\n    Language Models (LLMs). Provides utilities for describing 'R' objects,\n    package documentation, and workspace state in plain text formats\n    optimized for LLM consumption. Supports multiple workflows:\n    interactive copy-paste to external chat interfaces, programmatic tool\n    registration with 'ellmer' chat clients, batteries-included chat\n    applications via 'shinychat', and exposure to external coding agents\n    through the Model Context Protocol. Project configuration files enable\n    stable, repeatable conversations with project-specific context and\n    preferred LLM settings.  "
  },
  {
    "id": 9476,
    "package_name": "bubblyr",
    "title": "Beautiful Bubbles for 'shiny' and 'rmarkdown' Backgrounds",
    "description": "Creates bubbles within 'shiny' and 'rmarkdown' backgrounds using the 'bubbly-bg' 'JavaScript' library.",
    "version": "0.1.2",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden [aut, cre],\n  David \u00c5se [cph] (bubbly-bg library)",
    "url": "https://github.com/feddelegrand7/bubblyr",
    "bug_reports": "https://github.com/feddelegrand7/bubblyr/issues",
    "repository": "https://cran.r-project.org/package=bubblyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bubblyr Beautiful Bubbles for 'shiny' and 'rmarkdown' Backgrounds Creates bubbles within 'shiny' and 'rmarkdown' backgrounds using the 'bubbly-bg' 'JavaScript' library.  "
  },
  {
    "id": 9488,
    "package_name": "bumblebee",
    "title": "Quantify Disease Transmission Within and Between Population\nGroups",
    "description": "A simple tool to quantify the amount of transmission\n   of an infectious disease of interest occurring within and between \n   population groups. 'bumblebee' uses counts of observed directed \n   transmission pairs, identified phylogenetically from deep-sequence data or \n   from epidemiological contacts, to quantify transmission flows within and \n   between population groups accounting for sampling heterogeneity. Population \n   groups might include: geographical areas (e.g. communities, regions), \n   demographic groups (e.g. age, gender) or arms of a randomized clinical \n   trial. See the 'bumblebee' website for statistical theory, documentation \n   and examples <https://magosil86.github.io/bumblebee/>.",
    "version": "0.1.0",
    "maintainer": "Lerato E Magosi <magosil86@gmail.com>",
    "author": "Lerato E Magosi [aut] (ORCID: <https://orcid.org/0000-0002-3388-9892>),\n  Marc Lipsitch [aut],\n  Lerato E Magosi [cre]",
    "url": "https://magosil86.github.io/bumblebee/",
    "bug_reports": "https://github.com/magosil86/bumblebee/issues",
    "repository": "https://cran.r-project.org/package=bumblebee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bumblebee Quantify Disease Transmission Within and Between Population\nGroups A simple tool to quantify the amount of transmission\n   of an infectious disease of interest occurring within and between \n   population groups. 'bumblebee' uses counts of observed directed \n   transmission pairs, identified phylogenetically from deep-sequence data or \n   from epidemiological contacts, to quantify transmission flows within and \n   between population groups accounting for sampling heterogeneity. Population \n   groups might include: geographical areas (e.g. communities, regions), \n   demographic groups (e.g. age, gender) or arms of a randomized clinical \n   trial. See the 'bumblebee' website for statistical theory, documentation \n   and examples <https://magosil86.github.io/bumblebee/>.  "
  },
  {
    "id": 9502,
    "package_name": "businessPlanR",
    "title": "Simple Modelling Tools for Business Plans",
    "description": "A collection of S4 classes, methods and functions to create\n        and visualize business plans. Different types of cash flows can be\n        defined, which can then be used and tabulated to create profit and\n        loss statements, cash flow plans, investment and depreciation\n        schedules, loan amortization schedules, etc. The methods are\n        designed to produce handsome tables in both PDF and HTML using\n        'RMarkdown' or 'Shiny'.",
    "version": "0.1-0",
    "maintainer": "Meik Michalke <meik.michalke@c3s.cc>",
    "author": "Meik Michalke [aut, cre]",
    "url": "https://www.c3s.cc",
    "bug_reports": "https://github.com/C3S/businessPlanR/issues",
    "repository": "https://cran.r-project.org/package=businessPlanR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "businessPlanR Simple Modelling Tools for Business Plans A collection of S4 classes, methods and functions to create\n        and visualize business plans. Different types of cash flows can be\n        defined, which can then be used and tabulated to create profit and\n        loss statements, cash flow plans, investment and depreciation\n        schedules, loan amortization schedules, etc. The methods are\n        designed to produce handsome tables in both PDF and HTML using\n        'RMarkdown' or 'Shiny'.  "
  },
  {
    "id": 9520,
    "package_name": "c3",
    "title": "'C3.js' Chart Library",
    "description": "Create interactive charts with the 'C3.js' <http://c3js.org/> charting library. All plot \n    types in 'C3.js' are available and include line, bar, scatter, and mixed geometry plots. Plot \n    annotations, labels and axis are highly adjustable. Interactive web based charts can be embedded \n    in R Markdown documents or Shiny web applications. ",
    "version": "0.3.0",
    "maintainer": "Matt Johnson <mrjoh3@gmail.com>",
    "author": "Matt Johnson [aut, cre]",
    "url": "https://github.com/mrjoh3/c3",
    "bug_reports": "https://github.com/mrjoh3/c3/issues",
    "repository": "https://cran.r-project.org/package=c3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "c3 'C3.js' Chart Library Create interactive charts with the 'C3.js' <http://c3js.org/> charting library. All plot \n    types in 'C3.js' are available and include line, bar, scatter, and mixed geometry plots. Plot \n    annotations, labels and axis are highly adjustable. Interactive web based charts can be embedded \n    in R Markdown documents or Shiny web applications.   "
  },
  {
    "id": 9537,
    "package_name": "cabootcrs",
    "title": "Bootstrap Confidence Regions for Simple and Multiple\nCorrespondence Analysis",
    "description": "Performs simple correspondence analysis on a two-way contingency table, \n    or multiple correspondence analysis (homogeneity analysis) \n    on data with p categorical variables,\n    and produces bootstrap-based elliptical confidence regions around the \n    projected coordinates for the category points. \n    Includes routines to plot the results in a variety of styles. \n    Also reports the standard numerical output for correspondence analysis.",
    "version": "2.1.0",
    "maintainer": "Trevor Ringrose <t.j.ringrose@cranfield.ac.uk>",
    "author": "Trevor Ringrose",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cabootcrs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cabootcrs Bootstrap Confidence Regions for Simple and Multiple\nCorrespondence Analysis Performs simple correspondence analysis on a two-way contingency table, \n    or multiple correspondence analysis (homogeneity analysis) \n    on data with p categorical variables,\n    and produces bootstrap-based elliptical confidence regions around the \n    projected coordinates for the category points. \n    Includes routines to plot the results in a variety of styles. \n    Also reports the standard numerical output for correspondence analysis.  "
  },
  {
    "id": 9540,
    "package_name": "cache",
    "title": "Cache and Retrieve Computation Results",
    "description": "Easily cache and retrieve computation results. The package works seamlessly across interactive R sessions, R scripts and Rmarkdown documents.",
    "version": "0.0.3",
    "maintainer": "Olivier Binette <olivier.binette@gmail.com>",
    "author": "Olivier Binette [aut, cre]",
    "url": "https://github.com/OlivierBinette/cache",
    "bug_reports": "https://github.com/OlivierBinette/cache/issues",
    "repository": "https://cran.r-project.org/package=cache",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cache Cache and Retrieve Computation Results Easily cache and retrieve computation results. The package works seamlessly across interactive R sessions, R scripts and Rmarkdown documents.  "
  },
  {
    "id": 9557,
    "package_name": "calidad",
    "title": "Assesses the Quality of Estimates Made by Complex Sample Designs",
    "description": "Assesses the quality of estimates made by complex sample designs,\n  following the methodology developed by the National Institute of Statistics Chile (Household Survey Standard 2020, <https://www.ine.cl/docs/default-source/institucionalidad/buenas-pr%C3%A1cticas/clasificaciones-y-estandares/est%C3%A1ndar-evaluaci%C3%B3n-de-calidad-de-estimaciones-publicaci%C3%B3n-27022020.pdf>), (Economics Survey Standard 2024, <https://www.ine.gob.cl/docs/default-source/buenas-practicas/directrices-metodologicas/estandares/documentos/est%C3%A1ndar-evaluaci%C3%B3n-de-calidad-de-estimaciones-econ%C3%B3micas.pdf?sfvrsn=201fbeb9_2>)\n  and by Economic Commission  for Latin America and \n  Caribbean (2020, <https://repositorio.cepal.org/bitstream/handle/11362/45681/1/S2000293_es.pdf>), (2024, <https://repositorio.cepal.org/server/api/core/bitstreams/f04569e6-4f38-42e7-a32b-e0b298e0ab9c/content>).",
    "version": "0.8.2",
    "maintainer": "Klaus Lehmann <klehmann@fen.uchile.cl>",
    "author": "Klaus Lehmann [aut, cre],\n  Ricardo Pizarro [aut],\n  Ignacio Agloni [ctb],\n  Andrea L\u00f3pez [ctb],\n  Javiera Preuss [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=calidad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "calidad Assesses the Quality of Estimates Made by Complex Sample Designs Assesses the quality of estimates made by complex sample designs,\n  following the methodology developed by the National Institute of Statistics Chile (Household Survey Standard 2020, <https://www.ine.cl/docs/default-source/institucionalidad/buenas-pr%C3%A1cticas/clasificaciones-y-estandares/est%C3%A1ndar-evaluaci%C3%B3n-de-calidad-de-estimaciones-publicaci%C3%B3n-27022020.pdf>), (Economics Survey Standard 2024, <https://www.ine.gob.cl/docs/default-source/buenas-practicas/directrices-metodologicas/estandares/documentos/est%C3%A1ndar-evaluaci%C3%B3n-de-calidad-de-estimaciones-econ%C3%B3micas.pdf?sfvrsn=201fbeb9_2>)\n  and by Economic Commission  for Latin America and \n  Caribbean (2020, <https://repositorio.cepal.org/bitstream/handle/11362/45681/1/S2000293_es.pdf>), (2024, <https://repositorio.cepal.org/server/api/core/bitstreams/f04569e6-4f38-42e7-a32b-e0b298e0ab9c/content>).  "
  },
  {
    "id": 9566,
    "package_name": "camerondata",
    "title": "Datasets from \"Microeconometrics: Methods and Applications\" by\nCameron and Trivedi",
    "description": "Quick and easy access to datasets that let you replicate the\n    empirical examples in Cameron and Trivedi (2005) \"Microeconometrics: Methods and\n    Applications\" (ISBN: 9780521848053).The data are available as soon as you install\n    and load the package (lazy-loading) as data frames. The documentation includes\n    reference to chapter sections and page numbers where the datasets are used. ",
    "version": "1.0.0",
    "maintainer": "Juliana Vega-Lacorte <jv@jv-lacorte.de>",
    "author": "Juliana Vega-Lacorte [aut, cre]",
    "url": "https://github.com/juvlac/camerondata",
    "bug_reports": "https://github.com/juvlac/camerondata/issues",
    "repository": "https://cran.r-project.org/package=camerondata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "camerondata Datasets from \"Microeconometrics: Methods and Applications\" by\nCameron and Trivedi Quick and easy access to datasets that let you replicate the\n    empirical examples in Cameron and Trivedi (2005) \"Microeconometrics: Methods and\n    Applications\" (ISBN: 9780521848053).The data are available as soon as you install\n    and load the package (lazy-loading) as data frames. The documentation includes\n    reference to chapter sections and page numbers where the datasets are used.   "
  },
  {
    "id": 9568,
    "package_name": "campfin",
    "title": "Wrangle Campaign Finance Data",
    "description": "Explore and normalize American campaign finance\n    data. Created by the Investigative Reporting Workshop to facilitate\n    work on The Accountability Project, an effort to collect public data\n    into a central, standard database that is more easily searched:\n    <https://publicaccountability.org/>.",
    "version": "1.0.11",
    "maintainer": "Kiernan Nicholls <kiernann@protonmail.com>",
    "author": "Kiernan Nicholls [aut, cre, cph],\n  Investigative Reporting Workshop [cph],\n  Yanqi Xu [aut],\n  Schuyler Erle [cph]",
    "url": "https://github.com/irworkshop/campfin,\nhttps://irworkshop.github.io/campfin/",
    "bug_reports": "https://github.com/irworkshop/campfin/issues",
    "repository": "https://cran.r-project.org/package=campfin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "campfin Wrangle Campaign Finance Data Explore and normalize American campaign finance\n    data. Created by the Investigative Reporting Workshop to facilitate\n    work on The Accountability Project, an effort to collect public data\n    into a central, standard database that is more easily searched:\n    <https://publicaccountability.org/>.  "
  },
  {
    "id": 9571,
    "package_name": "camtrapR",
    "title": "Camera Trap Data Management and Analysis Framework",
    "description": "Management and analysis of camera trap wildlife data through an integrated workflow. Provides functions for image/video organization and metadata extraction, species/individual identification. Creates detection histories for occupancy and spatial capture-recapture analyses, with support for multi-season studies. Includes tools for fitting community occupancy models in JAGS and NIMBLE, and an interactive dashboard for survey data visualization and analysis. Features visualization of species distributions and activity patterns, plus export capabilities for GIS and reports. Emphasizes automation and reproducibility while maintaining flexibility for different study designs.",
    "version": "3.0.0",
    "maintainer": "Juergen Niedballa <camtrapr@gmail.com>",
    "author": "Juergen Niedballa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9187-2116>),\n  Alexandre Courtiol [aut] (ORCID:\n    <https://orcid.org/0000-0003-0637-2959>),\n  Rahel Sollmann [aut] (ORCID: <https://orcid.org/0000-0002-1607-2039>),\n  John Mathai [ctb],\n  Seth Timothy Wong [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8083-9268>),\n  An The Truong Nguyen [ctb] (ORCID:\n    <https://orcid.org/0009-0000-2861-2672>),\n  Azlan bin Mohamed [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3788-4383>),\n  Andrew Tilker [ctb] (ORCID: <https://orcid.org/0000-0003-3630-8691>),\n  Roshan Guharajan [ctb] (ORCID: <https://orcid.org/0000-0001-8124-5461>),\n  Ioannis Alexiou [ctb] (ORCID: <https://orcid.org/0000-0001-5095-4767>),\n  Andreas Wilting [ctb, ths] (ORCID:\n    <https://orcid.org/0000-0001-5073-9186>)",
    "url": "https://github.com/jniedballa/camtrapR,\nhttps://jniedballa.github.io/camtrapR/,\nhttps://groups.google.com/forum/#!forum/camtrapr",
    "bug_reports": "https://groups.google.com/forum/#!forum/camtrapr",
    "repository": "https://cran.r-project.org/package=camtrapR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "camtrapR Camera Trap Data Management and Analysis Framework Management and analysis of camera trap wildlife data through an integrated workflow. Provides functions for image/video organization and metadata extraction, species/individual identification. Creates detection histories for occupancy and spatial capture-recapture analyses, with support for multi-season studies. Includes tools for fitting community occupancy models in JAGS and NIMBLE, and an interactive dashboard for survey data visualization and analysis. Features visualization of species distributions and activity patterns, plus export capabilities for GIS and reports. Emphasizes automation and reproducibility while maintaining flexibility for different study designs.  "
  },
  {
    "id": 9577,
    "package_name": "cancerR",
    "title": "Classification of Cancer Using Administrative Data",
    "description": "Classifies the type of cancer using routinely collected data\n    commonly found in cancer registries from pathology reports. The\n    package implements the International Classification of Diseases for\n    Oncology, 3rd Edition site (topography), histology (morphology), and behaviour codes of neoplasms to classify cancer type \n    <https://www.who.int/standards/classifications/other-classifications/international-classification-of-diseases-for-oncology>. \n    Classification in children utilize the International Classification of Childhood Cancer by Steliarova-Foucher et al. (2005) <doi:10.1002/cncr.20910>.\n    Adolescent and young adult cancer classification is based on Barr et al. (2020) <doi:10.1002/cncr.33041>.",
    "version": "0.1.0",
    "maintainer": "Giancarlo Di Giuseppe <digi.giancarlo@proton.me>",
    "author": "Giancarlo Di Giuseppe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5112-7560>)",
    "url": "https://github.com/giancarlodigi/cancerR",
    "bug_reports": "https://github.com/giancarlodigi/cancerR/issues",
    "repository": "https://cran.r-project.org/package=cancerR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cancerR Classification of Cancer Using Administrative Data Classifies the type of cancer using routinely collected data\n    commonly found in cancer registries from pathology reports. The\n    package implements the International Classification of Diseases for\n    Oncology, 3rd Edition site (topography), histology (morphology), and behaviour codes of neoplasms to classify cancer type \n    <https://www.who.int/standards/classifications/other-classifications/international-classification-of-diseases-for-oncology>. \n    Classification in children utilize the International Classification of Childhood Cancer by Steliarova-Foucher et al. (2005) <doi:10.1002/cncr.20910>.\n    Adolescent and young adult cancer classification is based on Barr et al. (2020) <doi:10.1002/cncr.33041>.  "
  },
  {
    "id": 9593,
    "package_name": "captr",
    "title": "Client for the Captricity API",
    "description": "Get text from images of text using Captricity Optical Character\n    Recognition (OCR) API. Captricity allows you to get text from handwritten\n    forms --- think surveys --- and other structured paper documents. And it can\n    output data in form a delimited file keeping field information intact. For more\n    information, read <https://shreddr.captricity.com/developer/overview/>.",
    "version": "0.3.0",
    "maintainer": "Gaurav Sood <gsood07@gmail.com>",
    "author": "Gaurav Sood [aut, cre]",
    "url": "http://github.com/soodoku/captR",
    "bug_reports": "http://github.com/soodoku/captR/issues",
    "repository": "https://cran.r-project.org/package=captr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "captr Client for the Captricity API Get text from images of text using Captricity Optical Character\n    Recognition (OCR) API. Captricity allows you to get text from handwritten\n    forms --- think surveys --- and other structured paper documents. And it can\n    output data in form a delimited file keeping field information intact. For more\n    information, read <https://shreddr.captricity.com/developer/overview/>.  "
  },
  {
    "id": 9601,
    "package_name": "carbonpredict",
    "title": "Predict Carbon Emissions for UK SMEs",
    "description": "Predict Scope 1, 2 and 3 carbon emissions for UK Small and Medium-sized Enterprises (SMEs), using Standard Industrial Classification (SIC) codes and annual turnover data, as well as Scope 1 carbon emissions for UK farms. The 'carbonpredict' package provides single and batch prediction, plotting, and workflow tools for carbon accounting and reporting. The package utilises pre-trained models, leveraging rich classified transaction data to accurately predict Scope 1, 2 and 3 carbon emissions for UK SMEs as well as identifying emissions hotspots. It also provides Scope 1 carbon emissions predictions for UK farms of types: Cereals ex. rice, Dairy, Mixed farming, Sheep and goats, Cattle & buffaloes, Poultry, Animal production and Support for crop production. The methodology used to produce the estimates in this package is fully detailed in the following peer-reviewed publication in the Journal of Industrial Ecology: Phillpotts, A., Owen. A., Norman, J., Trendl, A.,  Gathergood, J., Jobst, Norbert., Leake, D. (2025) <doi:10.1111/jiec.70106> \"Bridging the SME Reporting Gap: A New Model for Predicting Scope 1 and 2 Emissions\".",
    "version": "2.0.0",
    "maintainer": "Hamza Suleman <Hamza.Suleman@lloydsbanking.com>",
    "author": "Hamza Suleman [aut, cre, cph],\n  Alec Phillpotts [ctb, aut],\n  Jasmine Wells [ctb, aut],\n  David Leake [ctb, aut]",
    "url": "https://github.com/david-leake/carbonpredict",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=carbonpredict",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "carbonpredict Predict Carbon Emissions for UK SMEs Predict Scope 1, 2 and 3 carbon emissions for UK Small and Medium-sized Enterprises (SMEs), using Standard Industrial Classification (SIC) codes and annual turnover data, as well as Scope 1 carbon emissions for UK farms. The 'carbonpredict' package provides single and batch prediction, plotting, and workflow tools for carbon accounting and reporting. The package utilises pre-trained models, leveraging rich classified transaction data to accurately predict Scope 1, 2 and 3 carbon emissions for UK SMEs as well as identifying emissions hotspots. It also provides Scope 1 carbon emissions predictions for UK farms of types: Cereals ex. rice, Dairy, Mixed farming, Sheep and goats, Cattle & buffaloes, Poultry, Animal production and Support for crop production. The methodology used to produce the estimates in this package is fully detailed in the following peer-reviewed publication in the Journal of Industrial Ecology: Phillpotts, A., Owen. A., Norman, J., Trendl, A.,  Gathergood, J., Jobst, Norbert., Leake, D. (2025) <doi:10.1111/jiec.70106> \"Bridging the SME Reporting Gap: A New Model for Predicting Scope 1 and 2 Emissions\".  "
  },
  {
    "id": 9602,
    "package_name": "carbonr",
    "title": "Calculate Carbon-Equivalent Emissions",
    "description": "Provides a flexible tool for calculating carbon-equivalent emissions. Mostly using data from the UK Government's Greenhouse Gas Conversion Factors report <https://www.gov.uk/government/publications/greenhouse-gas-reporting-conversion-factors-2024>, it facilitates transparent emissions calculations for various sectors, including travel, accommodation, and clinical activities. The package is designed for easy integration into R workflows, with additional support for 'shiny' applications and community-driven extensions.",
    "version": "0.2.7",
    "maintainer": "Lily Clements <lily@idems.international>",
    "author": "Lily Clements [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8864-0552>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=carbonr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "carbonr Calculate Carbon-Equivalent Emissions Provides a flexible tool for calculating carbon-equivalent emissions. Mostly using data from the UK Government's Greenhouse Gas Conversion Factors report <https://www.gov.uk/government/publications/greenhouse-gas-reporting-conversion-factors-2024>, it facilitates transparent emissions calculations for various sectors, including travel, accommodation, and clinical activities. The package is designed for easy integration into R workflows, with additional support for 'shiny' applications and community-driven extensions.  "
  },
  {
    "id": 9606,
    "package_name": "cards",
    "title": "Analysis Results Data",
    "description": "Construct CDISC (Clinical Data Interchange Standards\n    Consortium) compliant Analysis Results Data objects. These objects are\n    used and re-used to construct summary tables, visualizations, and\n    written reports. The package also exports utilities for working with\n    these objects and creating new Analysis Results Data objects.",
    "version": "0.7.1",
    "maintainer": "Daniel D. Sjoberg <danield.sjoberg@gmail.com>",
    "author": "Daniel D. Sjoberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0862-2018>),\n  Becca Krouse [aut],\n  Emily de la Rua [aut] (ORCID: <https://orcid.org/0009-0000-8738-5561>),\n  Malan Bosman [aut] (ORCID: <https://orcid.org/0000-0002-3020-195X>),\n  F. Hoffmann-La Roche AG [cph, fnd],\n  GlaxoSmithKline Research & Development Limited [cph]",
    "url": "https://github.com/insightsengineering/cards,\nhttps://insightsengineering.github.io/cards/",
    "bug_reports": "https://github.com/insightsengineering/cards/issues",
    "repository": "https://cran.r-project.org/package=cards",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cards Analysis Results Data Construct CDISC (Clinical Data Interchange Standards\n    Consortium) compliant Analysis Results Data objects. These objects are\n    used and re-used to construct summary tables, visualizations, and\n    written reports. The package also exports utilities for working with\n    these objects and creating new Analysis Results Data objects.  "
  },
  {
    "id": 9607,
    "package_name": "cardx",
    "title": "Extra Analysis Results Data Utilities",
    "description": "Create extra Analysis Results Data (ARD) summary objects.\n    The package supplements the simple ARD functions from the 'cards'\n    package, exporting functions to put statistical results in the ARD\n    format. These objects are used and re-used to construct summary\n    tables, visualizations, and written reports.",
    "version": "0.3.1",
    "maintainer": "Daniel D. Sjoberg <danield.sjoberg@gmail.com>",
    "author": "Daniel D. Sjoberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0862-2018>),\n  Abinaya Yogasekaram [aut],\n  Emily de la Rua [aut],\n  Malcolm Barrett [ctb] (ORCID: <https://orcid.org/0000-0003-0299-5825>),\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/insightsengineering/cardx,\nhttps://insightsengineering.github.io/cardx/",
    "bug_reports": "https://github.com/insightsengineering/cardx/issues",
    "repository": "https://cran.r-project.org/package=cardx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cardx Extra Analysis Results Data Utilities Create extra Analysis Results Data (ARD) summary objects.\n    The package supplements the simple ARD functions from the 'cards'\n    package, exporting functions to put statistical results in the ARD\n    format. These objects are used and re-used to construct summary\n    tables, visualizations, and written reports.  "
  },
  {
    "id": 9620,
    "package_name": "carpenter",
    "title": "Build Common Tables of Summary Statistics for Reports",
    "description": "Mainly used to build tables that are commonly presented for\n    bio-medical/health research, such as basic characteristic tables or\n    descriptive statistics.",
    "version": "0.2.2",
    "maintainer": "Luke Johnston <lwjohnst@gmail.com>",
    "author": "Luke Johnston [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4169-2616>)",
    "url": "https://github.com/lwjohnst86/carpenter",
    "bug_reports": "https://github.com/lwjohnst86/carpenter/issues",
    "repository": "https://cran.r-project.org/package=carpenter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "carpenter Build Common Tables of Summary Statistics for Reports Mainly used to build tables that are commonly presented for\n    bio-medical/health research, such as basic characteristic tables or\n    descriptive statistics.  "
  },
  {
    "id": 9630,
    "package_name": "cascsim",
    "title": "Casualty Actuarial Society Individual Claim Simulator",
    "description": "It is an open source insurance claim simulation engine sponsored\n    by the Casualty Actuarial Society. It generates individual insurance claims \n\tincluding open claims, reopened claims, incurred but not reported claims \n\tand future claims. It also includes claim data fitting functions to help set \n\tsimulation assumptions. It is useful for claim level reserving analysis.\n\tParodi (2013) <https://www.actuaries.org.uk/documents/triangle-free-reserving-non-traditional-framework-estimating-reserves-and-reserve-uncertainty>.",
    "version": "0.4",
    "maintainer": "Kailan Shang <klshang81@gmail.com>",
    "author": "Robert Bear [aut],\n  Kailan Shang [aut, cre],\n  Hai You [aut],\n  Brian Fannin [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cascsim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cascsim Casualty Actuarial Society Individual Claim Simulator It is an open source insurance claim simulation engine sponsored\n    by the Casualty Actuarial Society. It generates individual insurance claims \n\tincluding open claims, reopened claims, incurred but not reported claims \n\tand future claims. It also includes claim data fitting functions to help set \n\tsimulation assumptions. It is useful for claim level reserving analysis.\n\tParodi (2013) <https://www.actuaries.org.uk/documents/triangle-free-reserving-non-traditional-framework-estimating-reserves-and-reserve-uncertainty>.  "
  },
  {
    "id": 9634,
    "package_name": "casimir",
    "title": "Comparing Automated Subject Indexing Methods in R",
    "description": "Perform evaluation of automatic subject \n  indexing methods. The main focus of the package is to enable efficient \n  computation of set retrieval and ranked retrieval metrics across multiple\n  dimensions of a dataset, e.g. document strata or subsets of the label set. \n  The package also provides the possibility of computing bootstrap confidence\n  intervals for all major metrics, with seamless integration of parallel\n  computation and propensity scored variants of standard metrics. ",
    "version": "0.3.3",
    "maintainer": "Maximilian K\u00e4hler <m.kaehler@dnb.de>",
    "author": "Maximilian K\u00e4hler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4695-0565>),\n  Markus Schumacher [aut],\n  Deutsche Nationalbibliothek [cph]",
    "url": "https://deutsche-nationalbibliothek.github.io/casimir/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=casimir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "casimir Comparing Automated Subject Indexing Methods in R Perform evaluation of automatic subject \n  indexing methods. The main focus of the package is to enable efficient \n  computation of set retrieval and ranked retrieval metrics across multiple\n  dimensions of a dataset, e.g. document strata or subsets of the label set. \n  The package also provides the possibility of computing bootstrap confidence\n  intervals for all major metrics, with seamless integration of parallel\n  computation and propensity scored variants of standard metrics.   "
  },
  {
    "id": 9652,
    "package_name": "catfun",
    "title": "Categorical Data Analysis",
    "description": "Includes wrapper functions around existing functions for the analysis of categorical data and introduces functions for calculating risk differences and matched odds ratios. R currently supports a wide variety of tools for the analysis of categorical data. However, many functions are spread across a variety of packages with differing syntax and poor compatibility with each another. prop_test() combines the functions binom.test(), prop.test() and BinomCI() into one output. prop_power() allows for power and sample size calculations for both balanced and unbalanced designs. riskdiff() is used for calculating risk differences and matched_or() is used for calculating matched odds ratios. For further information on methods used that are not documented in other packages see Nathan Mantel and William Haenszel (1959) <doi:10.1093/jnci/22.4.719> and Alan Agresti (2002) <ISBN:0-471-36093-7>. ",
    "version": "0.1.4",
    "maintainer": "Nick Williams <ntwilliams.personal@gmail.com>",
    "author": "Nick Williams",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=catfun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catfun Categorical Data Analysis Includes wrapper functions around existing functions for the analysis of categorical data and introduces functions for calculating risk differences and matched odds ratios. R currently supports a wide variety of tools for the analysis of categorical data. However, many functions are spread across a variety of packages with differing syntax and poor compatibility with each another. prop_test() combines the functions binom.test(), prop.test() and BinomCI() into one output. prop_power() allows for power and sample size calculations for both balanced and unbalanced designs. riskdiff() is used for calculating risk differences and matched_or() is used for calculating matched odds ratios. For further information on methods used that are not documented in other packages see Nathan Mantel and William Haenszel (1959) <doi:10.1093/jnci/22.4.719> and Alan Agresti (2002) <ISBN:0-471-36093-7>.   "
  },
  {
    "id": 9656,
    "package_name": "catmaply",
    "title": "Heatmap for Categorical Data using 'plotly'",
    "description": "Methods and plotting functions for displaying categorical data on an \n             interactive heatmap using 'plotly'. Provides functionality for strictly \n             categorical heatmaps, heatmaps illustrating categorized continuous data \n             and annotated heatmaps. Also, there are various options to interact with the x-axis\n             to prevent overlapping axis labels, e.g. via simple sliders or range sliders. \n             Besides the viewer pane, resulting plots can be saved as a standalone HTML file, \n             embedded in 'R Markdown' documents or in a 'Shiny' app.",
    "version": "0.9.5",
    "maintainer": "Verkehrsbetriebe Z\u00fcrich <github@vbz.ch>",
    "author": "Verkehrsbetriebe Z\u00fcrich [cre, cph],\n  Yves Mauron [aut],\n  Christoph Baur [aut]",
    "url": "https://github.com/VerkehrsbetriebeZuerich/catmaply,\nhttps://verkehrsbetriebezuerich.github.io/catmaply/",
    "bug_reports": "https://github.com/VerkehrsbetriebeZuerich/catmaply/issues",
    "repository": "https://cran.r-project.org/package=catmaply",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catmaply Heatmap for Categorical Data using 'plotly' Methods and plotting functions for displaying categorical data on an \n             interactive heatmap using 'plotly'. Provides functionality for strictly \n             categorical heatmaps, heatmaps illustrating categorized continuous data \n             and annotated heatmaps. Also, there are various options to interact with the x-axis\n             to prevent overlapping axis labels, e.g. via simple sliders or range sliders. \n             Besides the viewer pane, resulting plots can be saved as a standalone HTML file, \n             embedded in 'R Markdown' documents or in a 'Shiny' app.  "
  },
  {
    "id": 9657,
    "package_name": "catool",
    "title": "Compensation Analysis Tool for Instructor Overload Pay",
    "description": "Calculates equitable overload compensation for college instructors\n    based on institutional policies, enrollment thresholds, and regular teaching load limits.\n    Compensation is awarded only for credit hours that exceed the regular load and meet\n    minimum enrollment criteria. When enrollment is below a specified threshold, pay is\n    prorated accordingly. The package prioritizes compensation from high-enrollment courses,\n    or optionally from low-enrollment courses for fairness, depending on user-defined strategy.\n    Includes tools for flexible policy settings, instructor filtering, and produces\n    clean, audit-ready summary tables suitable for payroll and administrative reporting.",
    "version": "1.0.1",
    "maintainer": "Dawit Aberra <dawit3000@hotmail.com>",
    "author": "Dawit Aberra [aut, cre]",
    "url": "https://github.com/dawit3000/catool",
    "bug_reports": "https://github.com/dawit3000/catool/issues",
    "repository": "https://cran.r-project.org/package=catool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catool Compensation Analysis Tool for Instructor Overload Pay Calculates equitable overload compensation for college instructors\n    based on institutional policies, enrollment thresholds, and regular teaching load limits.\n    Compensation is awarded only for credit hours that exceed the regular load and meet\n    minimum enrollment criteria. When enrollment is below a specified threshold, pay is\n    prorated accordingly. The package prioritizes compensation from high-enrollment courses,\n    or optionally from low-enrollment courses for fairness, depending on user-defined strategy.\n    Includes tools for flexible policy settings, instructor filtering, and produces\n    clean, audit-ready summary tables suitable for payroll and administrative reporting.  "
  },
  {
    "id": 9743,
    "package_name": "cellKey",
    "title": "Consistent Perturbation of Statistical Frequency- And Magnitude\nTables",
    "description": "Data from statistical agencies and other institutions often\n    need to be protected before they can be published. This package \n    can be used to perturb statistical tables in a consistent way. The\n    main idea is to add - at the micro data level - a record key for each\n    unit. Based on these keys, for any cell in a statistical table a\n    cell key is computed as a function on the record keys contributing to\n    a specific cell. Values that are added to the cell in order to perturb\n    it are derived from a lookup-table that maps values of cell keys to \n    specific perturbation values. The theoretical basis for the methods \n    implemented can be found in Thompson, Broadfoot and Elazar \n    (2013) <https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_1_ABS.pdf>\n    which was extended and enhanced by Giessing and Tent \n    (2019) <https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2019/mtg1/SDC2019_S2_Germany_Giessing_Tent_AD.pdf>.",
    "version": "1.0.3",
    "maintainer": "Bernhard Meindl <bernhard.meindl@statistik.gv.at>",
    "author": "Bernhard Meindl [aut, cre]",
    "url": "https://github.com/sdcTools/cellKey",
    "bug_reports": "https://github.com/sdcTools/userSupport/issues",
    "repository": "https://cran.r-project.org/package=cellKey",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cellKey Consistent Perturbation of Statistical Frequency- And Magnitude\nTables Data from statistical agencies and other institutions often\n    need to be protected before they can be published. This package \n    can be used to perturb statistical tables in a consistent way. The\n    main idea is to add - at the micro data level - a record key for each\n    unit. Based on these keys, for any cell in a statistical table a\n    cell key is computed as a function on the record keys contributing to\n    a specific cell. Values that are added to the cell in order to perturb\n    it are derived from a lookup-table that maps values of cell keys to \n    specific perturbation values. The theoretical basis for the methods \n    implemented can be found in Thompson, Broadfoot and Elazar \n    (2013) <https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2013/Topic_1_ABS.pdf>\n    which was extended and enhanced by Giessing and Tent \n    (2019) <https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2019/mtg1/SDC2019_S2_Germany_Giessing_Tent_AD.pdf>.  "
  },
  {
    "id": 9756,
    "package_name": "censobr",
    "title": "Download Data from Brazil's Population Census",
    "description": "Easy access to data from Brazil's population censuses. The package\n             provides a simple and efficient way to download and read the data \n             sets and the documentation of all the population censuses taken in \n             and after 1960 in the country. The package is built on top of the \n             'Arrow' platform <https://arrow.apache.org/docs/r/>,  which allows \n             users to work with larger-than-memory census data using 'dplyr' \n             familiar functions. <https://arrow.apache.org/docs/r/articles/arrow.html#analyzing-arrow-data-with-dplyr>.",
    "version": "0.5.0",
    "maintainer": "Rafael H. M. Pereira <rafa.pereira.br@gmail.com>",
    "author": "Rafael H. M. Pereira [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2125-7465>),\n  Rog\u00e9rio J. Barbosa [aut] (ORCID:\n    <https://orcid.org/0000-0002-6796-4547>),\n  Diego Rabatone Oliveira [ctb],\n  Neal Richardson [ctb],\n  Ipea - Institute for Applied Economic Research [cph, fnd]",
    "url": "https://github.com/ipeaGIT/censobr,\nhttps://ipeagit.github.io/censobr/",
    "bug_reports": "https://github.com/ipeaGIT/censobr/issues",
    "repository": "https://cran.r-project.org/package=censobr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "censobr Download Data from Brazil's Population Census Easy access to data from Brazil's population censuses. The package\n             provides a simple and efficient way to download and read the data \n             sets and the documentation of all the population censuses taken in \n             and after 1960 in the country. The package is built on top of the \n             'Arrow' platform <https://arrow.apache.org/docs/r/>,  which allows \n             users to work with larger-than-memory census data using 'dplyr' \n             familiar functions. <https://arrow.apache.org/docs/r/articles/arrow.html#analyzing-arrow-data-with-dplyr>.  "
  },
  {
    "id": 9794,
    "package_name": "cgmanalysis",
    "title": "Clean and Analyze Continuous Glucose Monitor Data",
    "description": "This code provides several different functions for cleaning and analyzing continuous glucose monitor data. Currently it works with 'Dexcom', 'iPro 2', 'Diasend', 'Libre', or 'Carelink' data. The cleandata() function takes a directory of CGM data files and prepares them for analysis. cgmvariables() iterates through a directory of cleaned CGM data files and produces a single spreadsheet with data for each file in either rows or columns. The column format of this spreadsheet is compatible with REDCap data upload. cgmreport() also iterates through a directory of cleaned data, and produces PDFs of individual and aggregate AGP plots. Please visit <https://github.com/childhealthbiostatscore/R-Packages/> to download the new-user guide.",
    "version": "3.1.1",
    "maintainer": "Tim Vigers <timothy.vigers@cuanschutz.edu>",
    "author": "Tim Vigers [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cgmanalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cgmanalysis Clean and Analyze Continuous Glucose Monitor Data This code provides several different functions for cleaning and analyzing continuous glucose monitor data. Currently it works with 'Dexcom', 'iPro 2', 'Diasend', 'Libre', or 'Carelink' data. The cleandata() function takes a directory of CGM data files and prepares them for analysis. cgmvariables() iterates through a directory of cleaned CGM data files and produces a single spreadsheet with data for each file in either rows or columns. The column format of this spreadsheet is compatible with REDCap data upload. cgmreport() also iterates through a directory of cleaned data, and produces PDFs of individual and aggregate AGP plots. Please visit <https://github.com/childhealthbiostatscore/R-Packages/> to download the new-user guide.  "
  },
  {
    "id": 9828,
    "package_name": "checkdown",
    "title": "Check-Fields and Check-Boxes for 'rmarkdown'",
    "description": "Creates auto-grading check-fields and check-boxes for 'rmarkdown' \n    or 'quarto' HTML. It can be used in class, when teacher share materials \n    and tasks, so students can solve some problems and check their work. In \n    contrast to the 'learnr' package, the 'checkdown' package works serverlessly\n    without 'shiny'.",
    "version": "0.0.13",
    "maintainer": "George Moroz <agricolamz@gmail.com>",
    "author": "George Moroz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1990-6083>)",
    "url": "https://agricolamz.github.io/checkdown/",
    "bug_reports": "https://github.com/agricolamz/checkdown/issues",
    "repository": "https://cran.r-project.org/package=checkdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "checkdown Check-Fields and Check-Boxes for 'rmarkdown' Creates auto-grading check-fields and check-boxes for 'rmarkdown' \n    or 'quarto' HTML. It can be used in class, when teacher share materials \n    and tasks, so students can solve some problems and check their work. In \n    contrast to the 'learnr' package, the 'checkdown' package works serverlessly\n    without 'shiny'.  "
  },
  {
    "id": 9851,
    "package_name": "chevron",
    "title": "Standard TLGs for Clinical Trials Reporting",
    "description": "Provide standard tables, listings, and graphs (TLGs)\n    libraries used in clinical trials. This package implements a structure\n    to reformat the data with 'dunlin', create reporting tables using\n    'rtables' and 'tern' with standardized input arguments to enable quick\n    generation of standard outputs.  In addition, it also provides\n    comprehensive data checks and script generation functionality.",
    "version": "0.2.12",
    "maintainer": "Joe Zhu <joe.zhu@roche.com>",
    "author": "Liming Li [aut] (ORCID: <https://orcid.org/0009-0008-6870-0878>,\n    Original creator of the package),\n  Benoit Falquet [aut] (ORCID: <https://orcid.org/0000-0002-4434-3799>,\n    Original creator of the package),\n  Xiaoli Duan [aut],\n  Adrian Waddell [ctb],\n  Chenkai Lv [ctb],\n  Pawel Rucki [ctb],\n  Tim Barnett [ctb],\n  Tian Fang [ctb],\n  Joe Zhu [cre] (ORCID: <https://orcid.org/0000-0001-7566-2787>),\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://insightsengineering.github.io/chevron/,\nhttps://github.com/insightsengineering/chevron/",
    "bug_reports": "https://github.com/insightsengineering/chevron/issues",
    "repository": "https://cran.r-project.org/package=chevron",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chevron Standard TLGs for Clinical Trials Reporting Provide standard tables, listings, and graphs (TLGs)\n    libraries used in clinical trials. This package implements a structure\n    to reformat the data with 'dunlin', create reporting tables using\n    'rtables' and 'tern' with standardized input arguments to enable quick\n    generation of standard outputs.  In addition, it also provides\n    comprehensive data checks and script generation functionality.  "
  },
  {
    "id": 9858,
    "package_name": "childsds",
    "title": "Data and Methods Around Reference Values in Pediatrics",
    "description": "Calculation of standard deviation scores and percentiles adduced from different\n    standards (WHO, UK, Germany, Italy, China, etc). Also, references for laboratory values in\n    children and adults are available, e.g., serum lipids, iron-related blood parameters, IGF, liver enzymes. See package documentation for full list.",
    "version": "0.9.11",
    "maintainer": "Mandy Vogel <mandy.vogel@googlemail.com>",
    "author": "Mandy Vogel [aut, cre]",
    "url": "",
    "bug_reports": "https://git.sc.uni-leipzig.de/my221hepi/childsds/-/issues",
    "repository": "https://cran.r-project.org/package=childsds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "childsds Data and Methods Around Reference Values in Pediatrics Calculation of standard deviation scores and percentiles adduced from different\n    standards (WHO, UK, Germany, Italy, China, etc). Also, references for laboratory values in\n    children and adults are available, e.g., serum lipids, iron-related blood parameters, IGF, liver enzymes. See package documentation for full list.  "
  },
  {
    "id": 9859,
    "package_name": "chilemapas",
    "title": "Mapas de las Divisiones Politicas y Administrativas de Chile\n(Maps of the Political and Administrative Divisions of Chile)",
    "description": "Mapas terrestres con topologias simplificadas. Estos mapas no \n  tienen precision geodesica, por lo que aplica el DFL-83 de 1979 de la Republica\n  de Chile y se consideran referenciales sin validez legal.\n  No se incluyen los territorios antarticos y bajo ningun evento estos mapas\n  significan que exista una cesion u ocupacion de territorios soberanos en\n  contra del Derecho Internacional por parte de Chile. Esta paquete esta \n  documentado intencionalmente en castellano asciificado para que funcione sin \n  problema en diferentes plataformas.\n  (Terrestrial maps with simplified toplogies. These maps lack geodesic\n  precision, therefore DFL-83 1979 of the Republic of Chile applies and are\n  considered to have no legal validity.\n  Antartic territories are excluded and under no event these maps mean\n  there is a cession or occupation of sovereign territories against International\n  Laws from Chile. This package was intentionally documented in asciified\n  spanish to make it work without problem on different platforms.)",
    "version": "0.4.0",
    "maintainer": "Mauricio Vargas <mavargas11@uc.cl>",
    "author": "Mauricio Vargas [aut, cre],\n  Roberto Salas [ctb],\n  Joshua Kunst [ctb],\n  Juan Correa [dtc],\n  Ricardo Aravena [ths],\n  Pontificia Universidad Catolica de Chile [cph],\n  Instituto Nacional de Estadisticas (INE) [dtc],\n  Subsecretaria de Desarrollo Regional (SUBDERE) [dtc],\n  Biblioteca del Congreso Nacional de Chile (BCN) [dtc]",
    "url": "https://pacha.dev/chilemapas/",
    "bug_reports": "https://github.com/pachadotdev/chilemapas/issues/",
    "repository": "https://cran.r-project.org/package=chilemapas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chilemapas Mapas de las Divisiones Politicas y Administrativas de Chile\n(Maps of the Political and Administrative Divisions of Chile) Mapas terrestres con topologias simplificadas. Estos mapas no \n  tienen precision geodesica, por lo que aplica el DFL-83 de 1979 de la Republica\n  de Chile y se consideran referenciales sin validez legal.\n  No se incluyen los territorios antarticos y bajo ningun evento estos mapas\n  significan que exista una cesion u ocupacion de territorios soberanos en\n  contra del Derecho Internacional por parte de Chile. Esta paquete esta \n  documentado intencionalmente en castellano asciificado para que funcione sin \n  problema en diferentes plataformas.\n  (Terrestrial maps with simplified toplogies. These maps lack geodesic\n  precision, therefore DFL-83 1979 of the Republic of Chile applies and are\n  considered to have no legal validity.\n  Antartic territories are excluded and under no event these maps mean\n  there is a cession or occupation of sovereign territories against International\n  Laws from Chile. This package was intentionally documented in asciified\n  spanish to make it work without problem on different platforms.)  "
  },
  {
    "id": 9875,
    "package_name": "chores",
    "title": "A Collection of Large Language Model Assistants",
    "description": "Provides a collection of ergonomic large language model assistants \n    designed to help you complete repetitive, hard-to-automate tasks quickly. \n    After selecting some code, press the keyboard shortcut you've chosen to \n    trigger the package app, select an assistant, and watch your chore be \n    carried out. While the package ships with a number of chore helpers for R \n    package development, users can create custom helpers just by writing some\n    instructions in a markdown file.",
    "version": "0.3.0",
    "maintainer": "Simon Couch <simon.couch@posit.co>",
    "author": "Simon Couch [aut, cre] (ORCID: <https://orcid.org/0000-0001-5676-5107>),\n  Posit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/simonpcouch/chores,\nhttps://simonpcouch.github.io/chores/",
    "bug_reports": "https://github.com/simonpcouch/chores/issues",
    "repository": "https://cran.r-project.org/package=chores",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chores A Collection of Large Language Model Assistants Provides a collection of ergonomic large language model assistants \n    designed to help you complete repetitive, hard-to-automate tasks quickly. \n    After selecting some code, press the keyboard shortcut you've chosen to \n    trigger the package app, select an assistant, and watch your chore be \n    carried out. While the package ships with a number of chore helpers for R \n    package development, users can create custom helpers just by writing some\n    instructions in a markdown file.  "
  },
  {
    "id": 9882,
    "package_name": "chromoMap",
    "title": "Interactive Genomic Visualization of Biological Data",
    "description": "Provides interactive, configurable and elegant graphics visualization of the chromosomes or chromosome regions\n    of any living organism allowing users to map chromosome elements (like genes, SNPs etc.) on the chromosome plot. It introduces\n    a special plot viz. the \"chromosome heatmap\" that, in addition to mapping elements, can visualize the data\n    associated with chromosome elements (like gene expression) in the form of heat colors which can be highly\n    advantageous in the scientific interpretations and research work. Because of the large size of the chromosomes,\n    it is impractical to visualize each element on the same plot. However, the plot provides a magnified view for each\n    of chromosome locus to render additional information and visualization specific for that location. You can map\n    thousands of genes and can view all mappings easily. Users can investigate the detailed information about the mappings\n    (like gene names or total genes mapped on a location) or can view the magnified single or double stranded view of the\n    chromosome at a location showing each mapped element in sequential order. The package provide multiple features\n    like visualizing multiple sets, chromosome heat-maps, group annotations, adding hyperlinks, and labelling.\n    The plots can be saved as HTML documents that can be customized and shared easily. In addition, you can include them in R Markdown or in R 'Shiny' applications.",
    "version": "4.1.1",
    "maintainer": "Lakshay Anand <lakshayanand15@gmail.com>",
    "author": "Lakshay Anand [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=chromoMap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chromoMap Interactive Genomic Visualization of Biological Data Provides interactive, configurable and elegant graphics visualization of the chromosomes or chromosome regions\n    of any living organism allowing users to map chromosome elements (like genes, SNPs etc.) on the chromosome plot. It introduces\n    a special plot viz. the \"chromosome heatmap\" that, in addition to mapping elements, can visualize the data\n    associated with chromosome elements (like gene expression) in the form of heat colors which can be highly\n    advantageous in the scientific interpretations and research work. Because of the large size of the chromosomes,\n    it is impractical to visualize each element on the same plot. However, the plot provides a magnified view for each\n    of chromosome locus to render additional information and visualization specific for that location. You can map\n    thousands of genes and can view all mappings easily. Users can investigate the detailed information about the mappings\n    (like gene names or total genes mapped on a location) or can view the magnified single or double stranded view of the\n    chromosome at a location showing each mapped element in sequential order. The package provide multiple features\n    like visualizing multiple sets, chromosome heat-maps, group annotations, adding hyperlinks, and labelling.\n    The plots can be saved as HTML documents that can be customized and shared easily. In addition, you can include them in R Markdown or in R 'Shiny' applications.  "
  },
  {
    "id": 9885,
    "package_name": "chronicle",
    "title": "Grammar for Creating R Markdown Reports",
    "description": "A system for creating R Markdown reports with a sequential syntax.",
    "version": "0.3",
    "maintainer": "Philippe Heymans Smith <pheymanss@gmail.com>",
    "author": "Philippe Heymans Smith [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=chronicle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chronicle Grammar for Creating R Markdown Reports A system for creating R Markdown reports with a sequential syntax.  "
  },
  {
    "id": 9891,
    "package_name": "chunkhooks",
    "title": "Chunk Hooks for 'R Markdown'",
    "description": "\n    Set chunk hooks for 'R Markdown' documents <https://rmarkdown.rstudio.com/>,\n    and improve user experience.\n    For example, change units of figure sizes, benchmark chunks, and number\n    lines on code blocks.",
    "version": "0.0.1",
    "maintainer": "Atsushi Yasumoto <atusy.rpkg@gmail.com>",
    "author": "Atsushi Yasumoto [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-8335-495X>)",
    "url": "https://chunkhooks.atusy.net, https://github.com/atusy/chunkhooks",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=chunkhooks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chunkhooks Chunk Hooks for 'R Markdown' \n    Set chunk hooks for 'R Markdown' documents <https://rmarkdown.rstudio.com/>,\n    and improve user experience.\n    For example, change units of figure sizes, benchmark chunks, and number\n    lines on code blocks.  "
  },
  {
    "id": 9899,
    "package_name": "cif",
    "title": "Cointegrated ICU Forecasting",
    "description": "Set of forecasting tools to predict ICU beds using a Vector Error Correction model with a single cointegrating vector. Method described in  Berta, P. Lovaglio, P.G. Paruolo, P. Verzillo, S., 2020. \"Real Time Forecasting of Covid-19 Intensive Care Units demand\" Health, Econometrics and Data Group (HEDG) Working Papers 20/16, HEDG, Department of Economics, University of York, <https://www.york.ac.uk/media/economics/documents/hedg/workingpapers/2020/2016.pdf>. ",
    "version": "0.1.1",
    "maintainer": "Paolo Paruolo <Paolo.PARUOLO@ec.europa.eu>",
    "author": "Paolo Berta [aut],\n  Paolo Paruolo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3982-4889>),\n  Stefano Verzillo [ctb],\n  Pietro Giorgio Lovaglio [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cif",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cif Cointegrated ICU Forecasting Set of forecasting tools to predict ICU beds using a Vector Error Correction model with a single cointegrating vector. Method described in  Berta, P. Lovaglio, P.G. Paruolo, P. Verzillo, S., 2020. \"Real Time Forecasting of Covid-19 Intensive Care Units demand\" Health, Econometrics and Data Group (HEDG) Working Papers 20/16, HEDG, Department of Economics, University of York, <https://www.york.ac.uk/media/economics/documents/hedg/workingpapers/2020/2016.pdf>.   "
  },
  {
    "id": 9903,
    "package_name": "cimir",
    "title": "Interface to the CIMIS Web API",
    "description": "Connect to the California Irrigation Management \n    Information System (CIMIS) Web API. See the CIMIS main page \n    <https://cimis.water.ca.gov> and web API documentation\n    <https://et.water.ca.gov> for more information.",
    "version": "0.4-1",
    "maintainer": "Michael Koohafkan <michael.koohafkan@gmail.com>",
    "author": "Michael Koohafkan [aut, cre]",
    "url": "https://github.com/mkoohafkan/cimir",
    "bug_reports": "https://github.com/mkoohafkan/cimir/issues",
    "repository": "https://cran.r-project.org/package=cimir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cimir Interface to the CIMIS Web API Connect to the California Irrigation Management \n    Information System (CIMIS) Web API. See the CIMIS main page \n    <https://cimis.water.ca.gov> and web API documentation\n    <https://et.water.ca.gov> for more information.  "
  },
  {
    "id": 9909,
    "package_name": "circacompare",
    "title": "Analyses of Circadian Data",
    "description": "Uses non-linear regression to statistically compare two circadian rhythms.\n    Groups are only compared if both are rhythmic (amplitude is non-zero).\n    Performs analyses regarding mesor, phase, and amplitude, reporting on estimates and statistical differences, for each, between groups.\n    Details can be found in Parsons et al (2020) <doi:10.1093/bioinformatics/btz730>.",
    "version": "0.2.0",
    "maintainer": "Rex Parsons <Rex.Parsons94@gmail.com>",
    "author": "Rex Parsons [aut, cre] (ORCID: <https://orcid.org/0000-0002-6053-8174>),\n  Alexander Bender [ctb]",
    "url": "https://rwparsons.github.io/circacompare/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=circacompare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "circacompare Analyses of Circadian Data Uses non-linear regression to statistically compare two circadian rhythms.\n    Groups are only compared if both are rhythmic (amplitude is non-zero).\n    Performs analyses regarding mesor, phase, and amplitude, reporting on estimates and statistical differences, for each, between groups.\n    Details can be found in Parsons et al (2020) <doi:10.1093/bioinformatics/btz730>.  "
  },
  {
    "id": 9932,
    "package_name": "civis",
    "title": "R Client for the 'Civis Platform API'",
    "description": "A convenient interface for making\n  requests directly to the 'Civis Platform API' <https://www.civisanalytics.com/platform>.\n  Full documentation available 'here' <https://civisanalytics.github.io/civis-r/>.",
    "version": "3.1.3",
    "maintainer": "Dustin Leatherman <dleatherman@civisanalytics.com>",
    "author": "Dustin Leatherman [cre],\n  Peter Cooman [ctb],\n  Patrick Miller [aut],\n  Keith Ingersoll [aut],\n  Bill Lattner [ctb],\n  Anh Le [ctb],\n  Michelangelo D'Agostino [ctb],\n  Sam Weiss [ctb],\n  Stephen Hoover [ctb],\n  Danning Chen [ctb],\n  Elizabeth Sander [ctb],\n  Madison Hobbs [ctb],\n  Anna Bladey [ctb],\n  Sahil Shah [ctb],\n  Bryan Baird [ctb]",
    "url": "https://github.com/civisanalytics/civis-r",
    "bug_reports": "https://github.com/civisanalytics/civis-r/issues",
    "repository": "https://cran.r-project.org/package=civis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "civis R Client for the 'Civis Platform API' A convenient interface for making\n  requests directly to the 'Civis Platform API' <https://www.civisanalytics.com/platform>.\n  Full documentation available 'here' <https://civisanalytics.github.io/civis-r/>.  "
  },
  {
    "id": 9961,
    "package_name": "cleangeo",
    "title": "Cleaning Geometries from Spatial Objects",
    "description": "\n  Provides a set of utility tools to inspect spatial objects, facilitate\n  handling and reporting of topology errors and geometry validity issue with sp objects.\n  Finally, it provides a geometry cleaner that will fix all geometry problems,\n  and eliminate (at least reduce) the likelihood of having issues when doing\n  spatial data processing.",
    "version": "0.3-1",
    "maintainer": "Emmanuel Blondel <emmanuel.blondel1@gmail.com>",
    "author": "Emmanuel Blondel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5870-5762>)",
    "url": "https://github.com/eblondel/cleangeo",
    "bug_reports": "https://github.com/eblondel/cleangeo/issues",
    "repository": "https://cran.r-project.org/package=cleangeo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cleangeo Cleaning Geometries from Spatial Objects \n  Provides a set of utility tools to inspect spatial objects, facilitate\n  handling and reporting of topology errors and geometry validity issue with sp objects.\n  Finally, it provides a geometry cleaner that will fix all geometry problems,\n  and eliminate (at least reduce) the likelihood of having issues when doing\n  spatial data processing.  "
  },
  {
    "id": 9963,
    "package_name": "cleanrmd",
    "title": "Clean Class-Less 'R Markdown' HTML Documents",
    "description": "A collection of clean 'R Markdown' HTML document templates\n    using classy-looking classless CSS styles. These documents use a\n    minimal set of dependencies but still look great, making them suitable\n    for use a package vignettes or for sharing results via email.",
    "version": "0.1.1",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>),\n  Igor Adamenko [ctb, cph] (Author of awsm.css),\n  Alvaro Montoro [ctb, cph] (Author of almond.css),\n  Ruan Martinelli [ctb, cph] (Author of axist.css),\n  Tran Ngoc Tuan Anh [ctb, cph] (Author of bamboo.css),\n  Marco Pontili [ctb, cph] (Author of bullframe.css),\n  Evgeny Orekhov [ctb, cph] (Author of holiday.css),\n  Vincent Dorig [ctb, cph] (Author of latex.css),\n  Yegor Bugayenko [ctb, cph] (Author of tacit and kacit),\n  John Otander [ctb, cph] (Author of splendor, air, retro, modest),\n  Matthew Blode [ctb, cph] (Author of marx),\n  Angelos Chalaris [ctb, cph] (Author of mini.css),\n  Example [ctb, cph] (Author of new.css),\n  David Paulsson [ctb, cph] (Author of no-class),\n  Authors of Pico.css [ctb, cph],\n  Mitesh Shah [ctb, cph] (Author of sakura),\n  Dimitri Nicolas [ctb, cph] (Author of semantic.css),\n  Kev Quirk [ctb, cph] (Author of simple.css),\n  Nate Goldman [ctb, cph] (Author of style.css),\n  Jack Crawford [ctb, cph] (Author of stylize),\n  Caio Gondim [ctb, cph] (Author of superstylin),\n  Bradley Taunt [ctb, cph] (Author of vanilla),\n  Kognise [ctb, cph] (Author of water.css),\n  Curtis McEnroe [ctb, cph] (Author of writ)",
    "url": "https://pkg.garrickadenbuie.com/cleanrmd/,\nhttps://github.com/gadenbuie/cleanrmd",
    "bug_reports": "https://github.com/gadenbuie/cleanrmd/issues",
    "repository": "https://cran.r-project.org/package=cleanrmd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cleanrmd Clean Class-Less 'R Markdown' HTML Documents A collection of clean 'R Markdown' HTML document templates\n    using classy-looking classless CSS styles. These documents use a\n    minimal set of dependencies but still look great, making them suitable\n    for use a package vignettes or for sharing results via email.  "
  },
  {
    "id": 9985,
    "package_name": "clinDataReview",
    "title": "Clinical Data Review Tool",
    "description": "Creation of interactive tables, listings and figures ('TLFs') \n  and associated report for exploratory analysis of data in a clinical trial, \n  e.g. for clinical oversight activities.\n  Interactive figures include sunburst, treemap, scatterplot, line plot and\n  barplot of counts data. \n  Interactive tables include table of summary statistics\n  (as counts of adverse events, enrollment table) and listings.\n  Possibility to compare data (summary table or listing) across two data batches/sets.\n  A clinical data review report is created via study-specific configuration \n  files and template 'R Markdown' reports contained in the package.",
    "version": "1.6.2",
    "maintainer": "Laure Cougnaud <laure.cougnaud@openanalytics.eu>",
    "author": "Laure Cougnaud [aut, cre],\n  Michela Pasetto [aut],\n  Lennart Tuijnder [aut],\n  Adriaan Blommaert [aut],\n  Arne De Roeck [ctb, rev] (rev: tests),\n  Open Analytics [cph]",
    "url": "https://github.com/openanalytics/clinDataReview",
    "bug_reports": "https://github.com/openanalytics/clinDataReview/issues",
    "repository": "https://cran.r-project.org/package=clinDataReview",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinDataReview Clinical Data Review Tool Creation of interactive tables, listings and figures ('TLFs') \n  and associated report for exploratory analysis of data in a clinical trial, \n  e.g. for clinical oversight activities.\n  Interactive figures include sunburst, treemap, scatterplot, line plot and\n  barplot of counts data. \n  Interactive tables include table of summary statistics\n  (as counts of adverse events, enrollment table) and listings.\n  Possibility to compare data (summary table or listing) across two data batches/sets.\n  A clinical data review report is created via study-specific configuration \n  files and template 'R Markdown' reports contained in the package.  "
  },
  {
    "id": 9987,
    "package_name": "clinUtils",
    "title": "General Utility Functions for Analysis of Clinical Data",
    "description": "\n Utility functions to facilitate the import, \n the reporting and analysis of clinical data.\n Example datasets in 'SDTM' and 'ADaM' format, containing a subset of patients/domains\n from the 'CDISC Pilot 01 study' are also available as R datasets to demonstrate\n the package functionalities.",
    "version": "0.2.2",
    "maintainer": "Laure Cougnaud <laure.cougnaud@openanalytics.eu>",
    "author": "Laure Cougnaud [aut, cre],\n  Michela Pasetto [aut],\n  Arne De Roeck [rev] (tests),\n  Open Analytics [cph]",
    "url": "https://github.com/openanalytics/clinUtils",
    "bug_reports": "https://github.com/openanalytics/clinUtils/issues",
    "repository": "https://cran.r-project.org/package=clinUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinUtils General Utility Functions for Analysis of Clinical Data \n Utility functions to facilitate the import, \n the reporting and analysis of clinical data.\n Example datasets in 'SDTM' and 'ADaM' format, containing a subset of patients/domains\n from the 'CDISC Pilot 01 study' are also available as R datasets to demonstrate\n the package functionalities.  "
  },
  {
    "id": 9992,
    "package_name": "clinify",
    "title": "Clinical Table Styling Tools and Utilities",
    "description": "The primary motivation of this package is to take the things that are great about the R packages 'flextable' <https://davidgohel.github.io/flextable/> and 'officer' <https://davidgohel.github.io/officer/>, take the standard and complex pieces of formatting clinical tables for regulatory use, and simplify the tedious pieces. ",
    "version": "0.3.0",
    "maintainer": "Mike Stackhouse <mike.stackhouse@atorusresearch.com>",
    "author": "Mike Stackhouse [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6030-723X>),\n  Ross Didenko [aut],\n  Yevhenii Boiko [aut],\n  Marat Zakirov [ctb],\n  Roman Rogoza [ctb],\n  Atorus Research, Inc. [cph],\n  Incyte Corporation [cph]",
    "url": "https://atorus-research.github.io/clinify/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clinify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinify Clinical Table Styling Tools and Utilities The primary motivation of this package is to take the things that are great about the R packages 'flextable' <https://davidgohel.github.io/flextable/> and 'officer' <https://davidgohel.github.io/officer/>, take the standard and complex pieces of formatting clinical tables for regulatory use, and simplify the tedious pieces.   "
  },
  {
    "id": 9998,
    "package_name": "clintrialx",
    "title": "Connect and Work with Clinical Trials Data Sources",
    "description": "Are you spending too much time fetching and managing clinical trial data? Struggling with complex queries and bulk data extraction? What if you could simplify this process with just a few lines of code? Introducing 'clintrialx' - Fetch clinical trial data from sources like 'ClinicalTrials.gov' <https://clinicaltrials.gov/> and the 'Clinical Trials Transformation Initiative - Access to Aggregate Content of ClinicalTrials.gov' database <https://aact.ctti-clinicaltrials.org/>, supporting pagination and bulk downloads. Also, you can generate HTML reports based on the data obtained from the sources!",
    "version": "0.1.1",
    "maintainer": "Indraneel Chakraborty <hello.indraneel@gmail.com>",
    "author": "Indraneel Chakraborty [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6958-8269>)",
    "url": "http://www.indraneelchakraborty.com/clintrialx/,\nhttps://github.com/ineelhere/clintrialx",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clintrialx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clintrialx Connect and Work with Clinical Trials Data Sources Are you spending too much time fetching and managing clinical trial data? Struggling with complex queries and bulk data extraction? What if you could simplify this process with just a few lines of code? Introducing 'clintrialx' - Fetch clinical trial data from sources like 'ClinicalTrials.gov' <https://clinicaltrials.gov/> and the 'Clinical Trials Transformation Initiative - Access to Aggregate Content of ClinicalTrials.gov' database <https://aact.ctti-clinicaltrials.org/>, supporting pagination and bulk downloads. Also, you can generate HTML reports based on the data obtained from the sources!  "
  },
  {
    "id": 9999,
    "package_name": "cliot",
    "title": "Clinical Indices and Outcomes Tools",
    "description": "Collection of indices and tools relating to clinical research that aid epidemiological cohort or retrospective chart review with big data. All indices and tools take commonly used lab values, patient demographics, and clinical measurements to compute various risk and predictive values for survival or further classification/stratification. References to original literature and validation contained in each function documentation. Includes all commonly available calculators available online. ",
    "version": "1.0.0",
    "maintainer": "Neel Agarwal <neel.agarwal.216@gmail.com>",
    "author": "Neel Agarwal [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cliot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cliot Clinical Indices and Outcomes Tools Collection of indices and tools relating to clinical research that aid epidemiological cohort or retrospective chart review with big data. All indices and tools take commonly used lab values, patient demographics, and clinical measurements to compute various risk and predictive values for survival or further classification/stratification. References to original literature and validation contained in each function documentation. Includes all commonly available calculators available online.   "
  },
  {
    "id": 10026,
    "package_name": "clusrank",
    "title": "Wilcoxon Rank Tests for Clustered Data",
    "description": "Non-parametric tests (Wilcoxon rank sum test and Wilcoxon signed rank test)\n       for clustered data documented in\n       Jiang et. al (2020) <doi:10.18637/jss.v096.i06>.",
    "version": "1.0-4",
    "maintainer": "Wenjie Wang <wang@wwenjie.org>",
    "author": "Wenjie Wang [cre, ctb] (ORCID: <https://orcid.org/0000-0003-0363-3180>),\n  Yujing Jiang [aut],\n  Mei-Ling Ting Lee [ctb],\n  Jun Yan [ctb]",
    "url": "https://github.com/wenjie2wang/clusrank",
    "bug_reports": "https://github.com/wenjie2wang/clusrank/issues",
    "repository": "https://cran.r-project.org/package=clusrank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clusrank Wilcoxon Rank Tests for Clustered Data Non-parametric tests (Wilcoxon rank sum test and Wilcoxon signed rank test)\n       for clustered data documented in\n       Jiang et. al (2020) <doi:10.18637/jss.v096.i06>.  "
  },
  {
    "id": 10098,
    "package_name": "cnd",
    "title": "Create and Register Conditions",
    "description": "An interface for creating new condition generators objects.  \n    Generators are special functions that can be saved in registries and linked\n    to other functions.  Utilities for documenting your generators, and new\n    conditions is provided for package development.",
    "version": "0.1.1",
    "maintainer": "Jordan Mark Barbone <jmbarbone@gmail.com>",
    "author": "Jordan Mark Barbone [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-9788-3628>)",
    "url": "https://jmbarbone.github.io/cnd/, https://github.com/jmbarbone/cnd",
    "bug_reports": "https://github.com/jmbarbone/cnd/issues",
    "repository": "https://cran.r-project.org/package=cnd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cnd Create and Register Conditions An interface for creating new condition generators objects.  \n    Generators are special functions that can be saved in registries and linked\n    to other functions.  Utilities for documenting your generators, and new\n    conditions is provided for package development.  "
  },
  {
    "id": 10104,
    "package_name": "coarseDataTools",
    "title": "Analysis of Coarsely Observed Data",
    "description": "Functions to analyze coarse data.\n    Specifically, it contains functions to (1) fit parametric accelerated\n    failure time models to interval-censored survival time data, and (2)\n    estimate the case-fatality ratio in scenarios with under-reporting.\n    This package's development was motivated by applications to infectious\n    disease: in particular, problems with estimating the incubation period and\n    the case fatality ratio of a given disease.  Sample data files are included\n    in the package. See Reich et al. (2009) <doi:10.1002/sim.3659>, \n    Reich et al. (2012) <doi:10.1111/j.1541-0420.2011.01709.x>, and \n    Lessler et al. (2009) <doi:10.1016/S1473-3099(09)70069-6>.",
    "version": "0.7.2",
    "maintainer": "Nicholas G. Reich <nick@umass.edu>",
    "author": "Nicholas G. Reich [aut, cre],\n  Justin Lessler [aut],\n  Andrew Azman [aut],\n  Zhian N. Kamvar [ctb],\n  Hugo Gruson [ctb]",
    "url": "http://nickreich.github.io/coarseDataTools/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=coarseDataTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "coarseDataTools Analysis of Coarsely Observed Data Functions to analyze coarse data.\n    Specifically, it contains functions to (1) fit parametric accelerated\n    failure time models to interval-censored survival time data, and (2)\n    estimate the case-fatality ratio in scenarios with under-reporting.\n    This package's development was motivated by applications to infectious\n    disease: in particular, problems with estimating the incubation period and\n    the case fatality ratio of a given disease.  Sample data files are included\n    in the package. See Reich et al. (2009) <doi:10.1002/sim.3659>, \n    Reich et al. (2012) <doi:10.1111/j.1541-0420.2011.01709.x>, and \n    Lessler et al. (2009) <doi:10.1016/S1473-3099(09)70069-6>.  "
  },
  {
    "id": 10107,
    "package_name": "cobenrich",
    "title": "Using Multiple Continuous Biomarkers for Patient Enrichment in\nTwo-Stage Clinical Designs",
    "description": "Enrichment strategies play a critical role in modern clinical trial design, especially as precision medicine advances the focus on patient-specific efficacy. Recent developments in enrichment design have introduced biomarker randomness and accounted for the correlation structure between treatment effect and biomarker, resulting in a two-stage threshold enrichment design. We propose novel two-stage enrichment designs capable of handling two or more continuous biomarkers.\n    See Zhang, F. and Gou, J. (2025). Using multiple biomarkers for patient enrichment in two-stage clinical designs. Technical Report.",
    "version": "1.0.1",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing (Zoe) Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cobenrich",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cobenrich Using Multiple Continuous Biomarkers for Patient Enrichment in\nTwo-Stage Clinical Designs Enrichment strategies play a critical role in modern clinical trial design, especially as precision medicine advances the focus on patient-specific efficacy. Recent developments in enrichment design have introduced biomarker randomness and accounted for the correlation structure between treatment effect and biomarker, resulting in a two-stage threshold enrichment design. We propose novel two-stage enrichment designs capable of handling two or more continuous biomarkers.\n    See Zhang, F. and Gou, J. (2025). Using multiple biomarkers for patient enrichment in two-stage clinical designs. Technical Report.  "
  },
  {
    "id": 10114,
    "package_name": "cocoon",
    "title": "Extract, Format, and Print Statistical Output",
    "description": "Provides functions that format statistical output in a way that\n    can be inserted into R Markdown documents. This is analogous to the \n    apa_print() functions in the 'papaja' package but prints Markdown\n    or LaTeX syntax.",
    "version": "0.2.1",
    "maintainer": "Jeffrey R. Stevens <jeffrey.r.stevens@protonmail.com>",
    "author": "Jeffrey R. Stevens [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2375-1360>)",
    "url": "https://github.com/JeffreyRStevens/cocoon,\nhttps://jeffreyrstevens.github.io/cocoon/",
    "bug_reports": "https://github.com/JeffreyRStevens/cocoon/issues",
    "repository": "https://cran.r-project.org/package=cocoon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cocoon Extract, Format, and Print Statistical Output Provides functions that format statistical output in a way that\n    can be inserted into R Markdown documents. This is analogous to the \n    apa_print() functions in the 'papaja' package but prints Markdown\n    or LaTeX syntax.  "
  },
  {
    "id": 10118,
    "package_name": "cocotest",
    "title": "Dependence Condition Test Using Ranked Correlation Coefficients",
    "description": "A common misconception is that the Hochberg procedure comes up with adequate overall type I error control when test statistics are positively correlated. However, unless the test statistics follow some standard distributions, the Hochberg procedure requires a more stringent positive dependence assumption, beyond mere positive correlation, to ensure valid overall type I error control. To fill this gap, we formulate statistical tests grounded in rank correlation coefficients to validate fulfillment of the positive dependence through stochastic ordering (PDS) condition.\n    See Gou, J., Wu, K. and Chen, O. Y. (2024). Rank correlation coefficient based tests on positive dependence through stochastic ordering with application in cancer studies, Technical Report.",
    "version": "1.0.3",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing (Zoe) Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cocotest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cocotest Dependence Condition Test Using Ranked Correlation Coefficients A common misconception is that the Hochberg procedure comes up with adequate overall type I error control when test statistics are positively correlated. However, unless the test statistics follow some standard distributions, the Hochberg procedure requires a more stringent positive dependence assumption, beyond mere positive correlation, to ensure valid overall type I error control. To fill this gap, we formulate statistical tests grounded in rank correlation coefficients to validate fulfillment of the positive dependence through stochastic ordering (PDS) condition.\n    See Gou, J., Wu, K. and Chen, O. Y. (2024). Rank correlation coefficient based tests on positive dependence through stochastic ordering with application in cancer studies, Technical Report.  "
  },
  {
    "id": 10139,
    "package_name": "codified",
    "title": "Produce Standard/Formalized Demographics Tables",
    "description": "Augment clinical data with metadata to create\n    output used in conventional publications and reports.",
    "version": "0.3.0",
    "maintainer": "Will Beasley <wibeasley@hotmail.com>",
    "author": "Will Beasley [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5613-5006>),\n  Peter Higgins [ctb]",
    "url": "https://ouhscbbmc.github.io/codified/,\nhttps://github.com/OuhscBbmc/codified,\nhttps://github.com/higgi13425/nih_enrollment_table",
    "bug_reports": "https://github.com/OuhscBbmc/codified/issues",
    "repository": "https://cran.r-project.org/package=codified",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "codified Produce Standard/Formalized Demographics Tables Augment clinical data with metadata to create\n    output used in conventional publications and reports.  "
  },
  {
    "id": 10143,
    "package_name": "coefficientalpha",
    "title": "Robust Coefficient Alpha and Omega with Missing and Non-Normal\nData",
    "description": "Cronbach's alpha and McDonald's omega are widely used reliability or internal consistency measures in social, behavioral and education sciences. Alpha is reported in nearly every study that involves measuring a construct through multiple test items. The package 'coefficientalpha' calculates coefficient alpha and coefficient omega with missing data and non-normal data. Robust standard errors and confidence intervals are also provided. A test is also available to test the tau-equivalent and homogeneous assumptions. Since Version 0.5, the bootstrap confidence intervals were added.",
    "version": "0.7.2",
    "maintainer": "Zhiyong Zhang <johnnyzhz@gmail.com>",
    "author": "Zhiyong Zhang and Ke-Hai Yuan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=coefficientalpha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "coefficientalpha Robust Coefficient Alpha and Omega with Missing and Non-Normal\nData Cronbach's alpha and McDonald's omega are widely used reliability or internal consistency measures in social, behavioral and education sciences. Alpha is reported in nearly every study that involves measuring a construct through multiple test items. The package 'coefficientalpha' calculates coefficient alpha and coefficient omega with missing data and non-normal data. Robust standard errors and confidence intervals are also provided. A test is also available to test the tau-equivalent and homogeneous assumptions. Since Version 0.5, the bootstrap confidence intervals were added.  "
  },
  {
    "id": 10187,
    "package_name": "colorSpec",
    "title": "Color Calculations with Emphasis on Spectral Data",
    "description": "Calculate with spectral properties of light sources, materials, cameras, eyes, and scanners.\n    Build complex systems from simpler parts using a spectral product algebra. For light sources,\n    compute CCT, CRI, SSI, and IES TM-30 reports.  For object colors, compute optimal colors and Logvinenko coordinates.\n    Work with the standard CIE illuminants and color matching functions, and read spectra from \n    text files, including CGATS files.  Estimate a spectrum from its response. A user guide and 9 vignettes are included.",
    "version": "1.8-0",
    "maintainer": "Glenn Davis <gdavis@gluonics.com>",
    "author": "Glenn Davis [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=colorSpec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "colorSpec Color Calculations with Emphasis on Spectral Data Calculate with spectral properties of light sources, materials, cameras, eyes, and scanners.\n    Build complex systems from simpler parts using a spectral product algebra. For light sources,\n    compute CCT, CRI, SSI, and IES TM-30 reports.  For object colors, compute optimal colors and Logvinenko coordinates.\n    Work with the standard CIE illuminants and color matching functions, and read spectra from \n    text files, including CGATS files.  Estimate a spectrum from its response. A user guide and 9 vignettes are included.  "
  },
  {
    "id": 10192,
    "package_name": "colorfindr",
    "title": "Extract Colors from Windows BMP, JPEG, PNG, TIFF, and SVG Format\nImages",
    "description": "Extracts colors from various image types, returns customized reports and plots treemaps \n    and 3D scatterplots of image compositions. Color palettes can also be created. ",
    "version": "0.1.5",
    "maintainer": "David Zumbach <david.zumbach@gfzb.ch>",
    "author": "David Zumbach [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/zumbov2/colorfindr/issues",
    "repository": "https://cran.r-project.org/package=colorfindr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "colorfindr Extract Colors from Windows BMP, JPEG, PNG, TIFF, and SVG Format\nImages Extracts colors from various image types, returns customized reports and plots treemaps \n    and 3D scatterplots of image compositions. Color palettes can also be created.   "
  },
  {
    "id": 10196,
    "package_name": "colorize",
    "title": "Render Text in Color for Markdown/Quarto Documents",
    "description": "Provides some simple functions for printing text in color in 'markdown' or 'Quarto' documents, to be rendered as HTML or LaTeX. This is useful when writing about the use of colors in graphs or tables, where you want to print their names in their actual color to give a direct impression of the color, like \u201cred\u201d shown in red, or \u201cblue\u201d shown in blue. ",
    "version": "0.2.1",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>),\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>),\n  Richard Brath [ctb]",
    "url": "https://github.com/friendly/colorize",
    "bug_reports": "https://github.com/friendly/colorize/issues",
    "repository": "https://cran.r-project.org/package=colorize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "colorize Render Text in Color for Markdown/Quarto Documents Provides some simple functions for printing text in color in 'markdown' or 'Quarto' documents, to be rendered as HTML or LaTeX. This is useful when writing about the use of colors in graphs or tables, where you want to print their names in their actual color to give a direct impression of the color, like \u201cred\u201d shown in red, or \u201cblue\u201d shown in blue.   "
  },
  {
    "id": 10243,
    "package_name": "compareGroups",
    "title": "Descriptive Analysis by Groups",
    "description": "Create data summaries for quality control, extensive reports for exploring data, as well as publication-ready univariate or bivariate tables in several formats (plain text, HTML,LaTeX, PDF, Word or Excel. Create figures to quickly visualise the distribution of your data (boxplots, barplots, normality-plots, etc.). Display statistics (mean, median, frequencies, incidences, etc.). Perform the appropriate tests (t-test, Analysis of variance, Kruskal-Wallis, Fisher, log-rank, ...) depending on the nature of the described variable (normal, non-normal or qualitative). Summarize genetic data (Single Nucleotide Polymorphisms) data displaying Allele Frequencies and performing Hardy-Weinberg Equilibrium tests among other typical statistics and tests for these kind of data.",
    "version": "4.10.1",
    "maintainer": "Isaac Subirana <isubirana@imim.es>",
    "author": "Isaac Subirana [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1676-0197>),\n  Joan Salvador [ctb]",
    "url": "https://isubirana.github.io/compareGroups/index.html,\nhttps://isubirana.github.io/compareGroups/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=compareGroups",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "compareGroups Descriptive Analysis by Groups Create data summaries for quality control, extensive reports for exploring data, as well as publication-ready univariate or bivariate tables in several formats (plain text, HTML,LaTeX, PDF, Word or Excel. Create figures to quickly visualise the distribution of your data (boxplots, barplots, normality-plots, etc.). Display statistics (mean, median, frequencies, incidences, etc.). Perform the appropriate tests (t-test, Analysis of variance, Kruskal-Wallis, Fisher, log-rank, ...) depending on the nature of the described variable (normal, non-normal or qualitative). Summarize genetic data (Single Nucleotide Polymorphisms) data displaying Allele Frequencies and performing Hardy-Weinberg Equilibrium tests among other typical statistics and tests for these kind of data.  "
  },
  {
    "id": 10244,
    "package_name": "compareMCMCs",
    "title": "Compare MCMC Efficiency from 'nimble' and/or Other MCMC Engines",
    "description": "Manages comparison of MCMC performance metrics from multiple MCMC algorithms. These may come from different MCMC configurations using the 'nimble' package or from other packages. Plug-ins for JAGS via 'rjags' and Stan via 'rstan' are provided. It is possible to write plug-ins for other packages. Performance metrics are held in an MCMCresult class along with samples and timing data. It is easy to apply new performance metrics. Reports are generated as html pages with figures comparing sets of runs. It is possible to configure the html pages, including providing new figure components.",
    "version": "0.6.0",
    "maintainer": "Perry de Valpine <pdevalpine@berkeley.edu>",
    "author": "Perry de Valpine [aut, cre],\n  Sally Paganin [aut],\n  Daniel Turek [aut],\n  Christopher Paciorek [aut]",
    "url": "https://github.com/nimble-dev/compareMCMCs",
    "bug_reports": "https://github.com/nimble-dev/compareMCMCs/issues",
    "repository": "https://cran.r-project.org/package=compareMCMCs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "compareMCMCs Compare MCMC Efficiency from 'nimble' and/or Other MCMC Engines Manages comparison of MCMC performance metrics from multiple MCMC algorithms. These may come from different MCMC configurations using the 'nimble' package or from other packages. Plug-ins for JAGS via 'rjags' and Stan via 'rstan' are provided. It is possible to write plug-ins for other packages. Performance metrics are held in an MCMCresult class along with samples and timing data. It is easy to apply new performance metrics. Reports are generated as html pages with figures comparing sets of runs. It is possible to configure the html pages, including providing new figure components.  "
  },
  {
    "id": 10266,
    "package_name": "con2aqi",
    "title": "Calculate the AQI from Pollutant Concentration",
    "description": "To calculate the AQI (Air Quality Index) from pollutant concentration data.\n    O3, PM2.5, PM10, CO, SO2, and NO2 are available currently.\n    The method can be referenced at Environmental Protection Agency, United States as follows:\n    EPA (2016) <https://www3.epa.gov/airnow/aqi-technical-assistance-document-may2016.pdf>.",
    "version": "0.1.0",
    "maintainer": "Zhicheng Du <dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Ziqiang Lin, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=con2aqi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "con2aqi Calculate the AQI from Pollutant Concentration To calculate the AQI (Air Quality Index) from pollutant concentration data.\n    O3, PM2.5, PM10, CO, SO2, and NO2 are available currently.\n    The method can be referenced at Environmental Protection Agency, United States as follows:\n    EPA (2016) <https://www3.epa.gov/airnow/aqi-technical-assistance-document-may2016.pdf>.  "
  },
  {
    "id": 10270,
    "package_name": "conText",
    "title": "'a la Carte' on Text (ConText) Embedding Regression",
    "description": "A fast, flexible and transparent framework to estimate context-specific word and short document embeddings using the 'a la carte' \n    embeddings approach developed by Khodak et al. (2018) <doi:10.48550/arXiv.1805.05388> and evaluate hypotheses about covariate effects on embeddings using \n    the regression framework developed by Rodriguez et al. (2021)<doi:10.1017/S0003055422001228>. New version of the package applies a new estimator to measure the distance between word embeddings as described in Green et al. (2025) <doi:10.1017/pan.2024.22>.",
    "version": "3.0.0",
    "maintainer": "Sofia Avila <sofiaavila@princeton.edu>",
    "author": "Pedro L. Rodriguez [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4762-4550>),\n  Arthur Spirling [aut] (ORCID: <https://orcid.org/0000-0001-9959-1805>),\n  Brandon Stewart [aut] (ORCID: <https://orcid.org/0000-0002-7657-3089>),\n  Christopher Barrie [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9156-990X>),\n  Sofia Avila [cre, aut] (ORCID: <https://orcid.org/0009-0007-8746-6688>)",
    "url": "https://github.com/prodriguezsosa/conText",
    "bug_reports": "https://github.com/prodriguezsosa/ConText/issues",
    "repository": "https://cran.r-project.org/package=conText",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conText 'a la Carte' on Text (ConText) Embedding Regression A fast, flexible and transparent framework to estimate context-specific word and short document embeddings using the 'a la carte' \n    embeddings approach developed by Khodak et al. (2018) <doi:10.48550/arXiv.1805.05388> and evaluate hypotheses about covariate effects on embeddings using \n    the regression framework developed by Rodriguez et al. (2021)<doi:10.1017/S0003055422001228>. New version of the package applies a new estimator to measure the distance between word embeddings as described in Green et al. (2025) <doi:10.1017/pan.2024.22>.  "
  },
  {
    "id": 10283,
    "package_name": "condensr",
    "title": "Academic Group Website Generator",
    "description": "Helps automate 'Quarto' website creation for small academic groups.\n    Builds a database-like structure of people, projects and publications, linking\n    them together with a string-based ID system. Then, provides functions to automate\n    production of clean markdown for these structures, and in-built CSS formatting\n    using CSS flexbox.",
    "version": "1.0.0",
    "maintainer": "Michael Lydeamore <michael.lydeamore@monash.edu>",
    "author": "Michael Lydeamore [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6515-827X>)",
    "url": "http://www.michaellydeamore.com/condensr/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=condensr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "condensr Academic Group Website Generator Helps automate 'Quarto' website creation for small academic groups.\n    Builds a database-like structure of people, projects and publications, linking\n    them together with a string-based ID system. Then, provides functions to automate\n    production of clean markdown for these structures, and in-built CSS formatting\n    using CSS flexbox.  "
  },
  {
    "id": 10284,
    "package_name": "condformat",
    "title": "Conditional Formatting in Data Frames",
    "description": "Apply and visualize conditional formatting to data frames in R.\n    It renders a data frame with cells formatted according to\n    criteria defined by rules, using a tidy evaluation syntax. The table is\n    printed either opening a web browser or within the 'RStudio' viewer if\n    available. The conditional formatting rules allow to highlight cells\n    matching a condition or add a gradient background to a given column. This\n    package supports both 'HTML' and 'LaTeX' outputs in 'knitr' reports, and\n    exporting to an 'xlsx' file.",
    "version": "0.10.1",
    "maintainer": "Sergio Oller Moreno <sergioller@gmail.com>",
    "author": "Sergio Oller Moreno [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-8994-1549>)",
    "url": "https://zeehio.github.io/condformat/,\nhttps://github.com/zeehio/condformat",
    "bug_reports": "https://github.com/zeehio/condformat/issues",
    "repository": "https://cran.r-project.org/package=condformat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "condformat Conditional Formatting in Data Frames Apply and visualize conditional formatting to data frames in R.\n    It renders a data frame with cells formatted according to\n    criteria defined by rules, using a tidy evaluation syntax. The table is\n    printed either opening a web browser or within the 'RStudio' viewer if\n    available. The conditional formatting rules allow to highlight cells\n    matching a condition or add a gradient background to a given column. This\n    package supports both 'HTML' and 'LaTeX' outputs in 'knitr' reports, and\n    exporting to an 'xlsx' file.  "
  },
  {
    "id": 10298,
    "package_name": "confidence",
    "title": "Confidence Estimation of Environmental State Classifications",
    "description": "Functions for estimating and reporting multi-year averages and\n    corresponding confidence intervals and distributions. A potential use case\n    is reporting the chemical and ecological status of surface waters according\n    to the European Water Framework Directive.",
    "version": "1.1-3",
    "maintainer": "Dennis Walvoort <dennis.Walvoort@wur.nl>",
    "author": "Willem van Loon [aut, cph],\n  Dennis Walvoort [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=confidence",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "confidence Confidence Estimation of Environmental State Classifications Functions for estimating and reporting multi-year averages and\n    corresponding confidence intervals and distributions. A potential use case\n    is reporting the chemical and ecological status of surface waters according\n    to the European Water Framework Directive.  "
  },
  {
    "id": 10314,
    "package_name": "congress",
    "title": "Access the Congress.gov API",
    "description": "Download and read data on United States congressional proceedings. \n    Data is read from the Library of Congress's Congress.gov Application Programming \n    Interface (<https://github.com/LibraryOfCongress/api.congress.gov/>). Functions\n    exist for all version 3 endpoints, including for bills, amendments, congresses, \n    summaries, members, reports, communications, nominations, and treaties.",
    "version": "0.1.0",
    "maintainer": "Christopher T. Kenny <ctkenny@proton.me>",
    "author": "Christopher T. Kenny [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9386-6860>)",
    "url": "https://github.com/christopherkenny/congress,\nhttps://christophertkenny.com/congress/",
    "bug_reports": "https://github.com/christopherkenny/congress/issues",
    "repository": "https://cran.r-project.org/package=congress",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "congress Access the Congress.gov API Download and read data on United States congressional proceedings. \n    Data is read from the Library of Congress's Congress.gov Application Programming \n    Interface (<https://github.com/LibraryOfCongress/api.congress.gov/>). Functions\n    exist for all version 3 endpoints, including for bills, amendments, congresses, \n    summaries, members, reports, communications, nominations, and treaties.  "
  },
  {
    "id": 10318,
    "package_name": "conleyreg",
    "title": "Estimations using Conley Standard Errors",
    "description": "Functions calculating Conley (1999) <doi:10.1016/S0304-4076(98)00084-0> standard errors. The package started by merging and extending multiple packages and \n  other published scripts on this econometric technique. It strongly emphasizes computational optimization. Details are available in the function documentation and in \n  the vignette.",
    "version": "0.1.8",
    "maintainer": "Christian D\u00fcben <cdueben.ml+cran@proton.me>",
    "author": "Christian D\u00fcben [aut, cre],\n  Richard Bluhm [cph],\n  Luis Calderon [cph],\n  Darin Christensen [cph],\n  Timothy Conley [cph],\n  Thiemo Fetzer [cph],\n  Leander Heldring [cph]",
    "url": "https://github.com/cdueben/conleyreg",
    "bug_reports": "https://github.com/cdueben/conleyreg/issues",
    "repository": "https://cran.r-project.org/package=conleyreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conleyreg Estimations using Conley Standard Errors Functions calculating Conley (1999) <doi:10.1016/S0304-4076(98)00084-0> standard errors. The package started by merging and extending multiple packages and \n  other published scripts on this econometric technique. It strongly emphasizes computational optimization. Details are available in the function documentation and in \n  the vignette.  "
  },
  {
    "id": 10319,
    "package_name": "conmet",
    "title": "Construct Measurement Evaluation Tool",
    "description": "With this package you can run 'ConMET' locally in R. 'ConMET' is an R-shiny application that facilitates performing and evaluating confirmatory factor analyses (CFAs) and is useful for running and reporting typical measurement models in applied psychology and management journals. 'ConMET' automatically creates, compares and summarizes CFA models. Most common fit indices (E.g., CFI and SRMR) are put in an overview table. ConMET also allows to test for common method variance. The application is particularly useful for teaching and instruction of measurement issues in survey research. The application uses the 'lavaan' package (Rosseel, 2012) to run CFAs.",
    "version": "0.1.0",
    "maintainer": "Leander De Schutter <deschutter@rsm.nl>",
    "author": "Leander De Schutter [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9826-4896>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=conmet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conmet Construct Measurement Evaluation Tool With this package you can run 'ConMET' locally in R. 'ConMET' is an R-shiny application that facilitates performing and evaluating confirmatory factor analyses (CFAs) and is useful for running and reporting typical measurement models in applied psychology and management journals. 'ConMET' automatically creates, compares and summarizes CFA models. Most common fit indices (E.g., CFI and SRMR) are put in an overview table. ConMET also allows to test for common method variance. The application is particularly useful for teaching and instruction of measurement issues in survey research. The application uses the 'lavaan' package (Rosseel, 2012) to run CFAs.  "
  },
  {
    "id": 10320,
    "package_name": "connectapi",
    "title": "Utilities for Interacting with the 'Posit Connect' Server API",
    "description": "Provides a helpful 'R6' class and methods for interacting with\n    the 'Posit Connect' Server API along with some meaningful utility functions\n    for regular tasks. API documentation varies by 'Posit Connect' installation\n    and version, but the latest documentation is also hosted publicly at\n    <https://docs.posit.co/connect/api/>.",
    "version": "0.9.0",
    "maintainer": "Toph Allen <toph@posit.co>",
    "author": "Toph Allen [aut, cre],\n  Neal Richardson [aut],\n  Sean Lopp [aut],\n  Cole Arendt [aut],\n  Posit, PBC [cph, fnd]",
    "url": "https://posit-dev.github.io/connectapi/,\nhttps://github.com/posit-dev/connectapi",
    "bug_reports": "https://github.com/posit-dev/connectapi/issues",
    "repository": "https://cran.r-project.org/package=connectapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "connectapi Utilities for Interacting with the 'Posit Connect' Server API Provides a helpful 'R6' class and methods for interacting with\n    the 'Posit Connect' Server API along with some meaningful utility functions\n    for regular tasks. API documentation varies by 'Posit Connect' installation\n    and version, but the latest documentation is also hosted publicly at\n    <https://docs.posit.co/connect/api/>.  "
  },
  {
    "id": 10328,
    "package_name": "conover.test",
    "title": "Conover-Iman Test of Multiple Comparisons Using Rank Sums",
    "description": "Computes the Conover-Iman test (1979) for 0th-order stochastic dominance and reports the results among multiple pairwise comparisons after a Kruskal-Wallis omnibus test for i0th-order stochastic dominance among k groups (Kruskal and Wallis, 1952). conover.test makes k(k-1)/2 multiple pairwise comparisons based on Conover-Iman t-test-statistic of the rank differences. The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the rank-sum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Conover-Iman test may be understood as a test for median difference and for mean difference. conover.test accounts for tied ranks. The Conover-Iman test is strictly valid if and only if the corresponding Kruskal-Wallis null hypothesis is rejected.",
    "version": "1.1.6",
    "maintainer": "Alexis Dinno <alexis.dinno@pdx.edu>",
    "author": "Alexis Dinno",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=conover.test",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conover.test Conover-Iman Test of Multiple Comparisons Using Rank Sums Computes the Conover-Iman test (1979) for 0th-order stochastic dominance and reports the results among multiple pairwise comparisons after a Kruskal-Wallis omnibus test for i0th-order stochastic dominance among k groups (Kruskal and Wallis, 1952). conover.test makes k(k-1)/2 multiple pairwise comparisons based on Conover-Iman t-test-statistic of the rank differences. The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the rank-sum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Conover-Iman test may be understood as a test for median difference and for mean difference. conover.test accounts for tied ranks. The Conover-Iman test is strictly valid if and only if the corresponding Kruskal-Wallis null hypothesis is rejected.  "
  },
  {
    "id": 10335,
    "package_name": "consort",
    "title": "Create Consort Diagram",
    "description": "To make it easy to create CONSORT diagrams for the transparent reporting of participant allocation in randomized, controlled clinical trials. This is done by creating a standardized disposition data, and using this data as the source for the creation a standard CONSORT diagram. Human effort by supplying text labels on the node can also be achieved.",
    "version": "1.2.2",
    "maintainer": "Alim Dayim <ad938@cam.ac.uk>",
    "author": "Alim Dayim [aut, cre] (ORCID: <https://orcid.org/0000-0001-9998-7463>)",
    "url": "https://github.com/adayim/consort/",
    "bug_reports": "https://github.com/adayim/consort/issues",
    "repository": "https://cran.r-project.org/package=consort",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "consort Create Consort Diagram To make it easy to create CONSORT diagrams for the transparent reporting of participant allocation in randomized, controlled clinical trials. This is done by creating a standardized disposition data, and using this data as the source for the creation a standard CONSORT diagram. Human effort by supplying text labels on the node can also be achieved.  "
  },
  {
    "id": 10353,
    "package_name": "contoso",
    "title": "Dataset of the 'Contoso' Company",
    "description": "A collection of synthetic datasets simulating sales transactions from a fictional company. The dataset includes various related tables that contain essential business and operational data, useful for analyzing sales performance and other business insights. Key tables included in the package are:\n  - \"sales\": Contains data on individual sales transactions, including order details, pricing, quantities, and customer information.\n  - \"customer\": Stores customer-specific details such as demographics, geographic location, occupation, and birthday.\n  - \"store\": Provides information about stores, including location, size, status, and operational dates.\n  - \"orders\": Contains details about customer orders, including order and delivery dates, store, and customer data.\n  - \"product\": Contains data on products, including attributes such as product name, category, price, cost, and weight.\n  - \"date\": A time-based table that includes date-related attributes like year, month, quarter, day, and working day indicators.\n  This dataset is ideal for practicing data analysis, performing time-series analysis, creating reports, or simulating business intelligence scenarios.",
    "version": "1.2.1",
    "maintainer": "Alejandro Hagan <alejandro.hagan@outlook.com>",
    "author": "Alejandro Hagan [aut, cre]",
    "url": "https://usrbinr.github.io/contoso/,\nhttps://github.com/usrbinr/contoso",
    "bug_reports": "https://github.com/usrbinr/contoso/issues",
    "repository": "https://cran.r-project.org/package=contoso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "contoso Dataset of the 'Contoso' Company A collection of synthetic datasets simulating sales transactions from a fictional company. The dataset includes various related tables that contain essential business and operational data, useful for analyzing sales performance and other business insights. Key tables included in the package are:\n  - \"sales\": Contains data on individual sales transactions, including order details, pricing, quantities, and customer information.\n  - \"customer\": Stores customer-specific details such as demographics, geographic location, occupation, and birthday.\n  - \"store\": Provides information about stores, including location, size, status, and operational dates.\n  - \"orders\": Contains details about customer orders, including order and delivery dates, store, and customer data.\n  - \"product\": Contains data on products, including attributes such as product name, category, price, cost, and weight.\n  - \"date\": A time-based table that includes date-related attributes like year, month, quarter, day, and working day indicators.\n  This dataset is ideal for practicing data analysis, performing time-series analysis, creating reports, or simulating business intelligence scenarios.  "
  },
  {
    "id": 10357,
    "package_name": "contrastable",
    "title": "Consistent Contrast Coding for Factors",
    "description": "Quickly set and summarize contrasts for factors prior to regression  analyses. Intended comparisons, baseline conditions, and intercepts can be explicitly set and documented without the user needing to directly manipulate matrices. Reviews and introductions for contrast coding are available in Brehm and Alday (2022)<doi:10.1016/j.jml.2022.104334> and Schad et al. (2020)<doi:10.1016/j.jml.2019.104038>.",
    "version": "1.1.0",
    "maintainer": "Thomas Sostarics <tsostarics@gmail.com>",
    "author": "Thomas Sostarics [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1178-7967>)",
    "url": "https://github.com/tsostarics/contrastable,\nhttps://tsostarics.github.io/contrastable/",
    "bug_reports": "https://github.com/tsostarics/contrastable/issues",
    "repository": "https://cran.r-project.org/package=contrastable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "contrastable Consistent Contrast Coding for Factors Quickly set and summarize contrasts for factors prior to regression  analyses. Intended comparisons, baseline conditions, and intercepts can be explicitly set and documented without the user needing to directly manipulate matrices. Reviews and introductions for contrast coding are available in Brehm and Alday (2022)<doi:10.1016/j.jml.2022.104334> and Schad et al. (2020)<doi:10.1016/j.jml.2019.104038>.  "
  },
  {
    "id": 10363,
    "package_name": "convdistr",
    "title": "Convolute Probabilistic Distributions",
    "description": "Convolute probabilistic distributions using the random generator \n function of each distribution. A new random number generator function is created that \n perform the mathematical operation on the individual random samples from the \n random generator function of each distribution. See the documentation for examples.",
    "version": "1.6.3",
    "maintainer": "Aponte John <john.j.aponte@gmail.com>",
    "author": "Aponte John [aut, cre] (ORCID: <https://orcid.org/0000-0002-3014-3673>)",
    "url": "https://github.com/johnaponte/convdistr,\nhttps://johnaponte.github.io/convdistr/",
    "bug_reports": "https://github.com/johnaponte/convdistr/issues",
    "repository": "https://cran.r-project.org/package=convdistr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "convdistr Convolute Probabilistic Distributions Convolute probabilistic distributions using the random generator \n function of each distribution. A new random number generator function is created that \n perform the mathematical operation on the individual random samples from the \n random generator function of each distribution. See the documentation for examples.  "
  },
  {
    "id": 10364,
    "package_name": "convergEU",
    "title": "Monitoring Convergence of EU Countries",
    "description": "Indicators and measures by country and time describe\n    what happens at economic and social levels. This package provides\n    functions to calculate several measures of convergence after imputing\n    missing values. The automated downloading of Eurostat data,\n    followed by the production of country fiches and indicator fiches,\n    makes possible to produce automated reports.\n    The Eurofound report (<doi:10.2806/68012>)\n    \"Upward convergence in the EU: Concepts, measurements and indicators\", \n    2018, is a detailed  presentation of  convergence.",
    "version": "0.7.3.2",
    "maintainer": "Eleonora Peruffo <eleonora.peruffo@eurofound.europa.eu>",
    "author": "Federico M. Stefanini [arc, aut],\n  Massimiliano Mascherini [arc],\n  Eleonora Peruffo [cre],\n  Nedka Nikiforova [ctb],\n  Chiara Litardi [ctb],\n  Berta Mizsei [ctb],\n  Ricardo Simon-Carbajo [ctb],\n  Romila Ghosh [ctb],\n  Andres Suarez-Cetrulo [ctb]",
    "url": "https://www.eurofound.europa.eu/system/files/2022-04/introduction-to-the-convergeu-package-0.6.4-tutorial-v2-apr2022.pdf,\nhttps://www.eurofound.europa.eu/en/publications/eurofound-paper/2020/monitoring-upward-convergence-eu-r-convergeu-package,\nhttps://www.eurofound.europa.eu/en/publications/2018/upward-convergence-eu-concepts-measurements-and-indicators,\nhttps://www.ajs.or.at/index.php/ajs/article/view/1468",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=convergEU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "convergEU Monitoring Convergence of EU Countries Indicators and measures by country and time describe\n    what happens at economic and social levels. This package provides\n    functions to calculate several measures of convergence after imputing\n    missing values. The automated downloading of Eurostat data,\n    followed by the production of country fiches and indicator fiches,\n    makes possible to produce automated reports.\n    The Eurofound report (<doi:10.2806/68012>)\n    \"Upward convergence in the EU: Concepts, measurements and indicators\", \n    2018, is a detailed  presentation of  convergence.  "
  },
  {
    "id": 10393,
    "package_name": "corTest",
    "title": "Robust Tests for Equal Correlation",
    "description": "There are 6 novel robust tests for equal correlation. They are all based on logistic regressions. The score statistic U is proportion to difference of two correlations based on different types of correlation in  6 methods. The ST1() is based on Pearson correlation. ST2() improved ST1() by using median absolute deviation. ST3() utilized type M correlation and ST4() used Spearman correlation.  ST5() and ST6() used two different ways to combine ST3() and ST4().  We highly recommend ST5() according to the article titled ''New Statistical Methods for Constructing Robust Differential Correlation Networks to characterize the interactions among microRNAs'' published in Scientific Reports.  Please see the reference: Yu et al. (2019) <doi:10.1038/s41598-019-40167-8>. ",
    "version": "1.0.7",
    "maintainer": "Danyang Yu <dyu33@jhu.edu>",
    "author": "Danyang Yu, Weiliang Qiu, Zeyu Zhang, Kimberly Glass, Jessica Su, Dawn L. DeMeo, Kelan Tantisira, Scott T. Weiss",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corTest Robust Tests for Equal Correlation There are 6 novel robust tests for equal correlation. They are all based on logistic regressions. The score statistic U is proportion to difference of two correlations based on different types of correlation in  6 methods. The ST1() is based on Pearson correlation. ST2() improved ST1() by using median absolute deviation. ST3() utilized type M correlation and ST4() used Spearman correlation.  ST5() and ST6() used two different ways to combine ST3() and ST4().  We highly recommend ST5() according to the article titled ''New Statistical Methods for Constructing Robust Differential Correlation Networks to characterize the interactions among microRNAs'' published in Scientific Reports.  Please see the reference: Yu et al. (2019) <doi:10.1038/s41598-019-40167-8>.   "
  },
  {
    "id": 10403,
    "package_name": "corella",
    "title": "Prepare, Manipulate and Check Data to Comply with Darwin Core\nStandard",
    "description": "Helps users standardise data to the Darwin Core Standard, a global \n             data standard to store, document, and share biodiversity data like \n             species occurrence records. The package provides tools to \n             manipulate data to conform with, and check validity against, the \n             Darwin Core Standard. Using 'corella' allows users to verify that \n             their data can be used to build 'Darwin Core Archives' using the \n             'galaxias' package.",
    "version": "0.1.4",
    "maintainer": "Dax Kellie <dax.kellie@csiro.au>",
    "author": "Dax Kellie [aut, cre],\n  Shandiya Balasubramanium [aut],\n  Martin Westgate [aut]",
    "url": "https://corella.ala.org.au",
    "bug_reports": "https://github.com/AtlasOfLivingAustralia/corella/issues",
    "repository": "https://cran.r-project.org/package=corella",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corella Prepare, Manipulate and Check Data to Comply with Darwin Core\nStandard Helps users standardise data to the Darwin Core Standard, a global \n             data standard to store, document, and share biodiversity data like \n             species occurrence records. The package provides tools to \n             manipulate data to conform with, and check validity against, the \n             Darwin Core Standard. Using 'corella' allows users to verify that \n             their data can be used to build 'Darwin Core Archives' using the \n             'galaxias' package.  "
  },
  {
    "id": 10415,
    "package_name": "corpustools",
    "title": "Managing, Querying and Analyzing Tokenized Text",
    "description": "Provides text analysis in R, focusing on the use of a tokenized text format. In this format, the positions of tokens are maintained, and each token can be annotated (e.g., part-of-speech tags, dependency relations).\n    Prominent features include advanced Lucene-like querying for specific tokens or contexts (e.g., documents, sentences),\n    similarity statistics for words and documents, exporting to DTM for compatibility with many text analysis packages,\n    and the possibility to reconstruct original text from tokens to facilitate interpretation.",
    "version": "0.5.2",
    "maintainer": "Kasper Welbers <kasperwelbers@gmail.com>",
    "author": "Kasper Welbers [aut, cre],\n  Wouter van Atteveldt [aut]",
    "url": "https://github.com/kasperwelbers/corpustools",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corpustools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corpustools Managing, Querying and Analyzing Tokenized Text Provides text analysis in R, focusing on the use of a tokenized text format. In this format, the positions of tokens are maintained, and each token can be annotated (e.g., part-of-speech tags, dependency relations).\n    Prominent features include advanced Lucene-like querying for specific tokens or contexts (e.g., documents, sentences),\n    similarity statistics for words and documents, exporting to DTM for compatibility with many text analysis packages,\n    and the possibility to reconstruct original text from tokens to facilitate interpretation.  "
  },
  {
    "id": 10453,
    "package_name": "countdown",
    "title": "A Countdown Timer for HTML Presentations, Documents, and Web\nApps",
    "description": "A simple countdown timer for slides and HTML documents\n    written in 'R Markdown' or 'Quarto'. Integrates fully into 'Shiny'\n    apps. Countdown to something amazing.",
    "version": "0.6.0",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>)",
    "url": "https://pkg.garrickadenbuie.com/countdown/,\nhttps://github.com/gadenbuie/countdown",
    "bug_reports": "https://github.com/gadenbuie/countdown/issues",
    "repository": "https://cran.r-project.org/package=countdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "countdown A Countdown Timer for HTML Presentations, Documents, and Web\nApps A simple countdown timer for slides and HTML documents\n    written in 'R Markdown' or 'Quarto'. Integrates fully into 'Shiny'\n    apps. Countdown to something amazing.  "
  },
  {
    "id": 10476,
    "package_name": "covid19.analytics",
    "title": "Load and Analyze Live Data from the COVID-19 Pandemic",
    "description": "Load and analyze updated time series worldwide data of reported cases for the Novel Coronavirus Disease (COVID-19) from different sources, including the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) data repository <https://github.com/CSSEGISandData/COVID-19>, \"Our World in Data\" <https://github.com/owid/> among several others. The datasets reporting the COVID-19 cases are available in two main modalities, as a time series sequences and aggregated data for the last day with greater spatial resolution. Several analysis, visualization and modelling functions are available in the package that will allow the user to compute and visualize total number of cases, total number of changes and growth rate globally or for an specific geographical location, while at the same time generating models using these trends; generate interactive visualizations and generate Susceptible-Infected-Recovered (SIR) model for the disease spread.",
    "version": "2.1.3.3",
    "maintainer": "Marcelo Ponce <m.ponce@utoronto.ca>",
    "author": "Marcelo Ponce [aut, cre], Amit Sandhel [ctb]",
    "url": "https://mponce0.github.io/covid19.analytics/",
    "bug_reports": "https://github.com/mponce0/covid19.analytics/issues",
    "repository": "https://cran.r-project.org/package=covid19.analytics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covid19.analytics Load and Analyze Live Data from the COVID-19 Pandemic Load and analyze updated time series worldwide data of reported cases for the Novel Coronavirus Disease (COVID-19) from different sources, including the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) data repository <https://github.com/CSSEGISandData/COVID-19>, \"Our World in Data\" <https://github.com/owid/> among several others. The datasets reporting the COVID-19 cases are available in two main modalities, as a time series sequences and aggregated data for the last day with greater spatial resolution. Several analysis, visualization and modelling functions are available in the package that will allow the user to compute and visualize total number of cases, total number of changes and growth rate globally or for an specific geographical location, while at the same time generating models using these trends; generate interactive visualizations and generate Susceptible-Infected-Recovered (SIR) model for the disease spread.  "
  },
  {
    "id": 10486,
    "package_name": "covidcast",
    "title": "Client for Delphi's 'COVIDcast Epidata' API",
    "description": "Tools for Delphi's 'COVIDcast Epidata' API: data access, maps and\n    time series plotting, and basic signal processing. The API includes a\n    collection of numerous indicators relevant to the COVID-19 pandemic in the\n    United States, including official reports, de-identified aggregated medical\n    claims data, large-scale surveys of symptoms and public behavior, and\n    mobility data, typically updated daily and at the county level. All data\n    sources are documented at\n    <https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html>.",
    "version": "0.5.3",
    "maintainer": "Alex Reinhart <areinhar@stat.cmu.edu>",
    "author": "Taylor Arnold [aut],\n  Jacob Bien [aut],\n  Logan Brooks [aut],\n  Sarah Colquhoun [aut],\n  David Farrow [aut],\n  Jed Grabman [ctb],\n  Pedrito Maynard-Zhang [ctb],\n  Kathryn Mazaitis [aut],\n  Alex Reinhart [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6658-514X>),\n  Ryan Tibshirani [aut]",
    "url": "https://cmu-delphi.github.io/covidcast/covidcastR/,\nhttps://github.com/cmu-delphi/covidcast",
    "bug_reports": "https://github.com/cmu-delphi/covidcast/issues",
    "repository": "https://cran.r-project.org/package=covidcast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covidcast Client for Delphi's 'COVIDcast Epidata' API Tools for Delphi's 'COVIDcast Epidata' API: data access, maps and\n    time series plotting, and basic signal processing. The API includes a\n    collection of numerous indicators relevant to the COVID-19 pandemic in the\n    United States, including official reports, de-identified aggregated medical\n    claims data, large-scale surveys of symptoms and public behavior, and\n    mobility data, typically updated daily and at the county level. All data\n    sources are documented at\n    <https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html>.  "
  },
  {
    "id": 10487,
    "package_name": "covidmx",
    "title": "Descarga y analiza datos de COVID-19 en M\u00e9xico",
    "description": "Herramientas para el an\u00e1lisis de datos de COVID-19 en M\u00e9xico. Descarga y analiza \n  los datos para COVID-19 de la Direccion General de Epidemiolog\u00eda de M\u00e9xico (DGE) \n  <https://www.gob.mx/salud/documentos/datos-abiertos-152127>,\n  la Red de Infecciones Respiratorias Agudas Graves (Red IRAG)\n  <https://www.gits.igg.unam.mx/red-irag-dashboard/reviewHome> y la Iniciativa Global \n  para compartir todos los datos de influenza (GISAID)\n  <https://gisaid.org/>. \n  English: Downloads and analyzes data  of COVID-19 from the  Mexican General \n  Directorate of Epidemiology (DGE), the Network of \n  Severe Acute Respiratory  Infections (IRAG network),and the Global \n  Initiative on Sharing All Influenza Data GISAID.",
    "version": "0.7.7",
    "maintainer": "Rodrigo Zepeda-Tello <rzepeda17@gmail.com>",
    "author": "Rodrigo Zepeda-Tello [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4471-5270>),\n  Mauricio Hernandez-Avila [aut],\n  Alberto Almui\u00f1a [ctb] (Author of included zzz fragment),\n  Jonah Gabry [ctb] (Author of included cmdstanr fragment),\n  Rok \u010ce\u0161novar [ctb] (Author of included cmdstanr fragment),\n  Ben Bales [ctb] (Author of included cmdstanr fragment),\n  Mitzi Morris [ctb] (Author of included cmdstanr fragment),\n  Mikhail Popov [ctb] (Author of included cmdstanr fragment),\n  Mike Lawrence [ctb] (Author of included cmdstanr fragment),\n  William Michael Landau [ctb] (Author of included cmdstanr fragment),\n  Jacob Socolar [ctb] (Author of included cmdstanr fragment),\n  Andrew Johnson [ctb] (Author of included cmdstanr fragment),\n  Instituto Mexicano del Seguro Social [cph, fnd]",
    "url": "https://github.com/RodrigoZepeda/covidmx,\nhttps://rodrigozepeda.github.io/covidmx/",
    "bug_reports": "https://github.com/RodrigoZepeda/covidmx/issues",
    "repository": "https://cran.r-project.org/package=covidmx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covidmx Descarga y analiza datos de COVID-19 en M\u00e9xico Herramientas para el an\u00e1lisis de datos de COVID-19 en M\u00e9xico. Descarga y analiza \n  los datos para COVID-19 de la Direccion General de Epidemiolog\u00eda de M\u00e9xico (DGE) \n  <https://www.gob.mx/salud/documentos/datos-abiertos-152127>,\n  la Red de Infecciones Respiratorias Agudas Graves (Red IRAG)\n  <https://www.gits.igg.unam.mx/red-irag-dashboard/reviewHome> y la Iniciativa Global \n  para compartir todos los datos de influenza (GISAID)\n  <https://gisaid.org/>. \n  English: Downloads and analyzes data  of COVID-19 from the  Mexican General \n  Directorate of Epidemiology (DGE), the Network of \n  Severe Acute Respiratory  Infections (IRAG network),and the Global \n  Initiative on Sharing All Influenza Data GISAID.  "
  },
  {
    "id": 10493,
    "package_name": "covtracer",
    "title": "Contextualizing Tests",
    "description": "\n    Dissects a package environment or 'covr' coverage object in order to cross\n    reference tested code with the lines that are evaluated, as well as linking\n    those evaluated lines to the documentation that they are described within.\n    Connecting these three pieces of information provides a mechanism of \n\tlinking tests to documented behaviors.",
    "version": "0.0.1",
    "maintainer": "Doug Kelkhoff <doug.kelkhoff@gmail.com>",
    "author": "Doug Kelkhoff [cre, aut] (ORCID:\n    <https://orcid.org/0009-0003-7845-4061>),\n  Szymon Maksymiuk [aut] (ORCID: <https://orcid.org/0000-0002-3120-1601>),\n  Andrew McNeil [aut],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/genentech/covtracer",
    "bug_reports": "https://github.com/genentech/covtracer/issues",
    "repository": "https://cran.r-project.org/package=covtracer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covtracer Contextualizing Tests \n    Dissects a package environment or 'covr' coverage object in order to cross\n    reference tested code with the lines that are evaluated, as well as linking\n    those evaluated lines to the documentation that they are described within.\n    Connecting these three pieces of information provides a mechanism of \n\tlinking tests to documented behaviors.  "
  },
  {
    "id": 10495,
    "package_name": "cowfootR",
    "title": "Dairy Farm Carbon Footprint Assessment",
    "description": "Calculates the carbon footprint of dairy farms based on methodologies of the International Dairy Federation and the Intergovernmental Panel on Climate Change. Includes tools for single-farm and batch analysis, report generation, and visualization. Methods follow International Dairy Federation (2022) \"The IDF global Carbon Footprint standard for the dairy sector\" (Bulletin of the IDF n\u00b0 520/2022) <doi:10.56169/FKRK7166> and IPCC (2019) \"2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories, Chapter 10: Emissions from Livestock and Manure Management\" <https://www.ipcc-nggip.iges.or.jp/public/2019rf/pdf/4_Volume4/19R_V4_Ch10_Livestock.pdf> guidelines.",
    "version": "0.1.2",
    "maintainer": "Juan Moreno <juanmarcosmoreno@gmail.com>",
    "author": "Juan Moreno [aut, cre]",
    "url": "https://github.com/juanmarcosmoreno-arch/cowfootR,\nhttps://juanmarcosmoreno-arch.github.io/cowfootR/",
    "bug_reports": "https://github.com/juanmarcosmoreno-arch/cowfootR/issues",
    "repository": "https://cran.r-project.org/package=cowfootR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cowfootR Dairy Farm Carbon Footprint Assessment Calculates the carbon footprint of dairy farms based on methodologies of the International Dairy Federation and the Intergovernmental Panel on Climate Change. Includes tools for single-farm and batch analysis, report generation, and visualization. Methods follow International Dairy Federation (2022) \"The IDF global Carbon Footprint standard for the dairy sector\" (Bulletin of the IDF n\u00b0 520/2022) <doi:10.56169/FKRK7166> and IPCC (2019) \"2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories, Chapter 10: Emissions from Livestock and Manure Management\" <https://www.ipcc-nggip.iges.or.jp/public/2019rf/pdf/4_Volume4/19R_V4_Ch10_Livestock.pdf> guidelines.  "
  },
  {
    "id": 10523,
    "package_name": "cpp4r",
    "title": "Header-Only 'C++' and 'R' Interface",
    "description": "Provides a header only, 'C++' interface to 'R' with enhancements over 'cpp11'. Enforces copy-on-write\n    semantics consistent with 'R' behavior. Offers native support for ALTREP objects, 'UTF-8' string handling, modern \n    'C++11' features and idioms, and reduced memory requirements. Allows for vendoring, making it useful for restricted\n    environments. Compared to 'cpp11', it adds support for converting 'C++' maps to 'R' lists, 'Roxygen' documentation\n    directly in 'C++' code, proper handling of matrix attributes, support for nullable external pointers, bidirectional\n    copy of complex number types, flexibility in type conversions, use of nullable pointers, and various performance\n    optimizations.",
    "version": "0.4.0",
    "maintainer": "Mauricio Vargas Sepulveda <m.vargas.sepulveda@gmail.com>",
    "author": "Mauricio Vargas Sepulveda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1017-7574>),\n  Posit Software, PBC [aut] (Original cpp11 package)",
    "url": "https://cpp4r.org, https://github.com/pachadotdev/cpp4r",
    "bug_reports": "https://github.com/pachadotdev/cpp4r/issues",
    "repository": "https://cran.r-project.org/package=cpp4r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cpp4r Header-Only 'C++' and 'R' Interface Provides a header only, 'C++' interface to 'R' with enhancements over 'cpp11'. Enforces copy-on-write\n    semantics consistent with 'R' behavior. Offers native support for ALTREP objects, 'UTF-8' string handling, modern \n    'C++11' features and idioms, and reduced memory requirements. Allows for vendoring, making it useful for restricted\n    environments. Compared to 'cpp11', it adds support for converting 'C++' maps to 'R' lists, 'Roxygen' documentation\n    directly in 'C++' code, proper handling of matrix attributes, support for nullable external pointers, bidirectional\n    copy of complex number types, flexibility in type conversions, use of nullable pointers, and various performance\n    optimizations.  "
  },
  {
    "id": 10526,
    "package_name": "cppcheckR",
    "title": "Check 'C' and 'C++' Files using 'Cppcheck'",
    "description": "Allow to run 'Cppcheck' (<https://cppcheck.sourceforge.io/>)\n    on 'C' and 'C++' files with a 'R' command or a 'RStudio' addin. The report\n    appears in the 'RStudio' viewer pane as a formatted 'HTML' file. It is\n    also possible to get this report with a 'shiny' application. 'Cppcheck' \n    can spot many error types and it can also give some recommendations on the \n    code.",
    "version": "1.0.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Amit Gupta [cph] ('fast-xml-parser' library),\n  luyilin [cph] ('json-format-highlight' library)",
    "url": "https://github.com/stla/cppcheckR",
    "bug_reports": "https://github.com/stla/cppcheckR/issues",
    "repository": "https://cran.r-project.org/package=cppcheckR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cppcheckR Check 'C' and 'C++' Files using 'Cppcheck' Allow to run 'Cppcheck' (<https://cppcheck.sourceforge.io/>)\n    on 'C' and 'C++' files with a 'R' command or a 'RStudio' addin. The report\n    appears in the 'RStudio' viewer pane as a formatted 'HTML' file. It is\n    also possible to get this report with a 'shiny' application. 'Cppcheck' \n    can spot many error types and it can also give some recommendations on the \n    code.  "
  },
  {
    "id": 10533,
    "package_name": "cpss",
    "title": "Change-Point Detection by Sample-Splitting Methods",
    "description": "Implements multiple change searching algorithms for a variety of \n    frequently considered parametric change-point models. In particular, it \n    integrates a criterion proposed by Zou, Wang and Li (2020) \n    <doi:10.1214/19-AOS1814> to select the number of change-points in a \n    data-driven fashion. Moreover, it also provides interfaces for \n    user-customized change-point models with one's own cost function and \n    parameter estimation routine. It is easy to get started with the \n    cpss.* set of functions by accessing their documentation pages \n    (e.g., ?cpss).",
    "version": "0.0.3",
    "maintainer": "Guanghui Wang <ghwang.nk@gmail.com>",
    "author": "Guanghui Wang [aut, cre],\n  Changliang Zou [aut]",
    "url": "https://github.com/ghwang-nk/cpss",
    "bug_reports": "https://github.com/ghwang-nk/cpss/issues",
    "repository": "https://cran.r-project.org/package=cpss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cpss Change-Point Detection by Sample-Splitting Methods Implements multiple change searching algorithms for a variety of \n    frequently considered parametric change-point models. In particular, it \n    integrates a criterion proposed by Zou, Wang and Li (2020) \n    <doi:10.1214/19-AOS1814> to select the number of change-points in a \n    data-driven fashion. Moreover, it also provides interfaces for \n    user-customized change-point models with one's own cost function and \n    parameter estimation routine. It is easy to get started with the \n    cpss.* set of functions by accessing their documentation pages \n    (e.g., ?cpss).  "
  },
  {
    "id": 10534,
    "package_name": "cpsurvsim",
    "title": "Simulating Survival Data from Change-Point Hazard Distributions",
    "description": "Simulates time-to-event data\n    with type I right censoring using two methods: the inverse CDF\n    method and our proposed memoryless method. The latter method\n    takes advantage of the memoryless property of survival and\n    simulates a separate distribution between change-points. We\n    include two parametric distributions: exponential and Weibull.\n    Inverse CDF method draws on the work of Rainer Walke (2010), \n    <https://www.demogr.mpg.de/papers/technicalreports/tr-2010-003.pdf>.",
    "version": "1.2.2",
    "maintainer": "Camille Hochheimer <dochoch19@gmail.com>",
    "author": "Camille Hochheimer [aut, cre]",
    "url": "https://github.com/camillejo/cpsurvsim",
    "bug_reports": "https://github.com/camillejo/cpsurvsim/issues",
    "repository": "https://cran.r-project.org/package=cpsurvsim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cpsurvsim Simulating Survival Data from Change-Point Hazard Distributions Simulates time-to-event data\n    with type I right censoring using two methods: the inverse CDF\n    method and our proposed memoryless method. The latter method\n    takes advantage of the memoryless property of survival and\n    simulates a separate distribution between change-points. We\n    include two parametric distributions: exponential and Weibull.\n    Inverse CDF method draws on the work of Rainer Walke (2010), \n    <https://www.demogr.mpg.de/papers/technicalreports/tr-2010-003.pdf>.  "
  },
  {
    "id": 10535,
    "package_name": "cpsvote",
    "title": "A Toolbox for Using the CPS\u2019s Voting and Registration Supplement",
    "description": "Provides automated methods for downloading, recoding, and merging \n    selected years of the Current Population Survey's Voting and Registration \n    Supplement, a large N national survey about registration, voting, and \n    non-voting in United States federal elections. Provides documentation for \n    appropriate use of sample weights to generate statistical estimates, \n    drawing from Hur & Achen (2013) <doi:10.1093/poq/nft042> and McDonald (2018) \n    <http://www.electproject.org/home/voter-turnout/voter-turnout-data>.",
    "version": "0.1.0",
    "maintainer": "Jay Lee <jaylee@reed.edu>",
    "author": "Jay Lee [aut, cre],\n  Paul Gronke [aut],\n  Canyon Foot [ctb]",
    "url": "https://github.com/Reed-EVIC/cpsvote",
    "bug_reports": "https://github.com/Reed-EVIC/cpsvote/issues",
    "repository": "https://cran.r-project.org/package=cpsvote",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cpsvote A Toolbox for Using the CPS\u2019s Voting and Registration Supplement Provides automated methods for downloading, recoding, and merging \n    selected years of the Current Population Survey's Voting and Registration \n    Supplement, a large N national survey about registration, voting, and \n    non-voting in United States federal elections. Provides documentation for \n    appropriate use of sample weights to generate statistical estimates, \n    drawing from Hur & Achen (2013) <doi:10.1093/poq/nft042> and McDonald (2018) \n    <http://www.electproject.org/home/voter-turnout/voter-turnout-data>.  "
  },
  {
    "id": 10544,
    "package_name": "crane",
    "title": "Supplements the 'gtsummary' Package for Pharmaceutical Reporting",
    "description": "Tables summarizing clinical trial results are often complex\n    and require detailed tailoring prior to submission to a health\n    authority.  The 'crane' package supplements the functionality of the\n    'gtsummary' package for creating these often highly bespoke tables in\n    the pharmaceutical industry.",
    "version": "0.3.0",
    "maintainer": "Daniel D. Sjoberg <danield.sjoberg@gmail.com>",
    "author": "Daniel D. Sjoberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0862-2018>),\n  Emily de la Rua [aut] (ORCID: <https://orcid.org/0009-0000-8738-5561>),\n  Davide Garolini [aut],\n  Abinaya Yogasekaram [aut],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/insightsengineering/crane,\nhttps://insightsengineering.github.io/crane/",
    "bug_reports": "https://github.com/insightsengineering/crane/issues",
    "repository": "https://cran.r-project.org/package=crane",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crane Supplements the 'gtsummary' Package for Pharmaceutical Reporting Tables summarizing clinical trial results are often complex\n    and require detailed tailoring prior to submission to a health\n    authority.  The 'crane' package supplements the functionality of the\n    'gtsummary' package for creating these often highly bespoke tables in\n    the pharmaceutical industry.  "
  },
  {
    "id": 10554,
    "package_name": "creditr",
    "title": "Credit Default Swaps",
    "description": "Price credit default swaps using\n             'C' code from the International Swaps and Derivatives\n             Association CDS Standard Model. See\n             <https://www.cdsmodel.com/cdsmodel/documentation.html>\n             for more information about the model and \n             <https://www.cdsmodel.com/cdsmodel/cds-disclaimer.html>\n             for license details for the 'C' code.",
    "version": "0.6.2",
    "maintainer": "Yanrong Song <yrsong129@gmail.com>",
    "author": "Yanrong Song [aut, cre],\n  Zijie Zhu [aut],\n  David Kane [aut],\n  Heidi Chen [aut],\n  Yuanchu Dang [aut],\n  Yang Lu [aut],\n  Kanishka Malik [aut],\n  Skylar Smith [aut],\n  International Swaps and Derivatives Association [cph] (Copyright holder\n    of the free CDS standard model code used in this package)",
    "url": "https://github.com/yanrong-stacy-song/creditr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=creditr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "creditr Credit Default Swaps Price credit default swaps using\n             'C' code from the International Swaps and Derivatives\n             Association CDS Standard Model. See\n             <https://www.cdsmodel.com/cdsmodel/documentation.html>\n             for more information about the model and \n             <https://www.cdsmodel.com/cdsmodel/cds-disclaimer.html>\n             for license details for the 'C' code.  "
  },
  {
    "id": 10555,
    "package_name": "credsubs",
    "title": "Credible Subsets",
    "description": "Functions for constructing simultaneous credible bands and identifying subsets via the \"credible subsets\" (also called \"credible subgroups\") method. Package documentation includes the vignette included in this package, and the paper by Schnell, Fiecas, and Carlin (2020, <doi:10.18637/jss.v094.i07>).",
    "version": "1.1.1",
    "maintainer": "Patrick Schnell <schnell.31@osu.edu>",
    "author": "Patrick Schnell,\n  Brad Carlin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=credsubs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "credsubs Credible Subsets Functions for constructing simultaneous credible bands and identifying subsets via the \"credible subsets\" (also called \"credible subgroups\") method. Package documentation includes the vignette included in this package, and the paper by Schnell, Fiecas, and Carlin (2020, <doi:10.18637/jss.v094.i07>).  "
  },
  {
    "id": 10580,
    "package_name": "cronologia",
    "title": "Create an HTML Vertical Timeline from a Data Frame in\n'rmarkdown' and 'shiny'",
    "description": "Creates an HTML vertical timeline from a data frame as an input for\n    'rmarkdown' documents and 'shiny' applications.",
    "version": "0.2.0",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden",
    "url": "https://github.com/feddelegrand7/cronologia",
    "bug_reports": "https://github.com/feddelegrand7/cronologia/issues",
    "repository": "https://cran.r-project.org/package=cronologia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cronologia Create an HTML Vertical Timeline from a Data Frame in\n'rmarkdown' and 'shiny' Creates an HTML vertical timeline from a data frame as an input for\n    'rmarkdown' documents and 'shiny' applications.  "
  },
  {
    "id": 10596,
    "package_name": "crosstable",
    "title": "Crosstables for Descriptive Analyses",
    "description": "Create descriptive tables for continuous and categorical variables. \n    Apply summary statistics and counting function, with or without a grouping variable, and create beautiful reports using 'rmarkdown' or 'officer'.\n    You can also compute effect sizes and statistical tests if needed.",
    "version": "0.8.2",
    "maintainer": "Dan Chaltiel <dan.chaltiel@gmail.com>",
    "author": "Dan Chaltiel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3488-779X>),\n  David Hajage [ccp]",
    "url": "https://danchaltiel.github.io/crosstable/,\nhttps://github.com/DanChaltiel/crosstable/",
    "bug_reports": "https://github.com/DanChaltiel/crosstable/issues/",
    "repository": "https://cran.r-project.org/package=crosstable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crosstable Crosstables for Descriptive Analyses Create descriptive tables for continuous and categorical variables. \n    Apply summary statistics and counting function, with or without a grouping variable, and create beautiful reports using 'rmarkdown' or 'officer'.\n    You can also compute effect sizes and statistical tests if needed.  "
  },
  {
    "id": 10625,
    "package_name": "cryptowatchR",
    "title": "An API Wrapper for 'Cryptowatch'",
    "description": "An API wrapper for 'Cryptowatch' to get prices and other information (e.g., volume, trades, order books, bid and ask prices, live quotes, and more) about cryptocurrencies and crypto exchanges. See <https://docs.cryptowat.ch/rest-api> for a detailed documentation.",
    "version": "0.2.0",
    "maintainer": "Lorenz Brachtendorf <lorenz.brachtendorf@gmx.de>",
    "author": "Lorenz Brachtendorf [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7066-0838>)",
    "url": "https://github.com/lorenzbr/cryptowatchR",
    "bug_reports": "https://github.com/lorenzbr/cryptowatchR/issues",
    "repository": "https://cran.r-project.org/package=cryptowatchR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cryptowatchR An API Wrapper for 'Cryptowatch' An API wrapper for 'Cryptowatch' to get prices and other information (e.g., volume, trades, order books, bid and ask prices, live quotes, and more) about cryptocurrencies and crypto exchanges. See <https://docs.cryptowat.ch/rest-api> for a detailed documentation.  "
  },
  {
    "id": 10658,
    "package_name": "cthist",
    "title": "Clinical Trial Registry History",
    "description": "Retrieves historical versions of clinical trial registry\n    entries from <https://ClinicalTrials.gov>. Package functionality\n    and implementation for v 1.0.0 is documented in Carlisle (2022)\n    <DOI:10.1371/journal.pone.0270909>.",
    "version": "2.1.12",
    "maintainer": "Benjamin Gregory Carlisle <murph@bgcarlisle.com>",
    "author": "Benjamin Gregory Carlisle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8975-0649>)",
    "url": "https://github.com/bgcarlisle/cthist",
    "bug_reports": "https://github.com/bgcarlisle/cthist/issues",
    "repository": "https://cran.r-project.org/package=cthist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cthist Clinical Trial Registry History Retrieves historical versions of clinical trial registry\n    entries from <https://ClinicalTrials.gov>. Package functionality\n    and implementation for v 1.0.0 is documented in Carlisle (2022)\n    <DOI:10.1371/journal.pone.0270909>.  "
  },
  {
    "id": 10667,
    "package_name": "ctrdata",
    "title": "Retrieve and Analyze Clinical Trials Data from Public Registers",
    "description": "A system for querying, retrieving and analyzing\n        protocol- and results-related information on clinical trials from\n        four public registers, the 'European Union Clinical Trials Register'\n        ('EUCTR', <https://www.clinicaltrialsregister.eu/>), \n        'ClinicalTrials.gov' (<https://clinicaltrials.gov/> and also \n        translating queries the retired classic interface), the \n        'ISRCTN' (<http://www.isrctn.com/>) and the\n        'European Union Clinical Trials Information System'\n        ('CTIS', <https://euclinicaltrials.eu/>). \n        Trial information is downloaded, converted and stored in a database \n        ('PostgreSQL', 'SQLite', 'DuckDB' or 'MongoDB'; via package 'nodbi'). \n        Protocols, statistical analysis plans, informed consent sheets and other\n        documents in registers associated with trials can also be downloaded. \n        Other functions implement trial concepts canonically across registers,\n        identify deduplicated records, easily find and extract variables \n        (fields) of interest even from complex nested data as used by the \n        registers, merge variables and update queries. \n        The package can be used for monitoring, meta- and trend-analysis of\n        the design and conduct as well as of the results of clinical trials \n        across registers.\n        See overview in Herold, R. (2025) <doi:10.1017/rsm.2025.10061>.",
    "version": "1.25.1",
    "maintainer": "Ralf Herold <ralf.herold@mailbox.org>",
    "author": "Ralf Herold [aut, cre] (ORCID: <https://orcid.org/0000-0002-8148-6748>),\n  Marek Kubica [cph] (node-xml2js library),\n  Ivan Bozhanov [cph] (jstree library)",
    "url": "https://cran.r-project.org/package=ctrdata,\nhttps://rfhb.github.io/ctrdata/",
    "bug_reports": "https://github.com/rfhb/ctrdata/issues",
    "repository": "https://cran.r-project.org/package=ctrdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctrdata Retrieve and Analyze Clinical Trials Data from Public Registers A system for querying, retrieving and analyzing\n        protocol- and results-related information on clinical trials from\n        four public registers, the 'European Union Clinical Trials Register'\n        ('EUCTR', <https://www.clinicaltrialsregister.eu/>), \n        'ClinicalTrials.gov' (<https://clinicaltrials.gov/> and also \n        translating queries the retired classic interface), the \n        'ISRCTN' (<http://www.isrctn.com/>) and the\n        'European Union Clinical Trials Information System'\n        ('CTIS', <https://euclinicaltrials.eu/>). \n        Trial information is downloaded, converted and stored in a database \n        ('PostgreSQL', 'SQLite', 'DuckDB' or 'MongoDB'; via package 'nodbi'). \n        Protocols, statistical analysis plans, informed consent sheets and other\n        documents in registers associated with trials can also be downloaded. \n        Other functions implement trial concepts canonically across registers,\n        identify deduplicated records, easily find and extract variables \n        (fields) of interest even from complex nested data as used by the \n        registers, merge variables and update queries. \n        The package can be used for monitoring, meta- and trend-analysis of\n        the design and conduct as well as of the results of clinical trials \n        across registers.\n        See overview in Herold, R. (2025) <doi:10.1017/rsm.2025.10061>.  "
  },
  {
    "id": 10673,
    "package_name": "ctsmTMB",
    "title": "Continuous Time Stochastic Modelling using Template Model\nBuilder",
    "description": "Perform state and parameter inference, and forecasting, in\n    stochastic state-space systems using the 'ctsmTMB' class. This class,\n    built with the 'R6' package, provides a user-friendly interface for\n    defining and handling state-space models. Inference is based on\n    maximum likelihood estimation, with derivatives efficiently computed\n    through automatic differentiation enabled by the 'TMB'/'RTMB' packages\n    (Kristensen et al., 2016) <doi:10.18637/jss.v070.i05>. The available\n    inference methods include Kalman filters, in addition to a Laplace\n    approximation-based smoothing method. For further details of these\n    methods refer to the documentation of the 'CTSMR' package\n    <https://ctsm.info/ctsmr-reference.pdf> and Thygesen (2025)\n    <doi:10.48550/arXiv.2503.21358>. Forecasting capabilities include\n    moment predictions and stochastic path simulations, both implemented\n    in 'C++' using 'Rcpp' (Eddelbuettel et al., 2018)\n    <doi:10.1080/00031305.2017.1375990> for computational efficiency.",
    "version": "1.0.1",
    "maintainer": "Phillip Vetter <pbrve@dtu.dk>",
    "author": "Phillip Vetter [aut, cre, cph],\n  Jan M\u00f8ller [ctb],\n  Uffe Thygesen [ctb],\n  Peder Bacher [ctb],\n  Henrik Madsen [ctb]",
    "url": "https://github.com/phillipbvetter/ctsmTMB",
    "bug_reports": "https://github.com/phillipbvetter/ctsmTMB/issues",
    "repository": "https://cran.r-project.org/package=ctsmTMB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctsmTMB Continuous Time Stochastic Modelling using Template Model\nBuilder Perform state and parameter inference, and forecasting, in\n    stochastic state-space systems using the 'ctsmTMB' class. This class,\n    built with the 'R6' package, provides a user-friendly interface for\n    defining and handling state-space models. Inference is based on\n    maximum likelihood estimation, with derivatives efficiently computed\n    through automatic differentiation enabled by the 'TMB'/'RTMB' packages\n    (Kristensen et al., 2016) <doi:10.18637/jss.v070.i05>. The available\n    inference methods include Kalman filters, in addition to a Laplace\n    approximation-based smoothing method. For further details of these\n    methods refer to the documentation of the 'CTSMR' package\n    <https://ctsm.info/ctsmr-reference.pdf> and Thygesen (2025)\n    <doi:10.48550/arXiv.2503.21358>. Forecasting capabilities include\n    moment predictions and stochastic path simulations, both implemented\n    in 'C++' using 'Rcpp' (Eddelbuettel et al., 2018)\n    <doi:10.1080/00031305.2017.1375990> for computational efficiency.  "
  },
  {
    "id": 10692,
    "package_name": "cumulocityr",
    "title": "Client for the 'Cumulocity' API",
    "description": "Access the 'Cumulocity' API and retrieve data on devices, measurements, and events. Documentation for the API can be found at <https://www.cumulocity.com/guides/reference/rest-implementation/>.",
    "version": "0.1.0",
    "maintainer": "Dmitriy Bolotov <dmitriy.bolotov@softwareag.com>",
    "author": "Dmitriy Bolotov [aut, cre],\n  Software AG [cph]",
    "url": "https://softwareag.github.io/cumulocityr/,\nhttps://github.com/SoftwareAG/cumulocityr",
    "bug_reports": "https://github.com/SoftwareAG/cumulocityr/issues",
    "repository": "https://cran.r-project.org/package=cumulocityr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cumulocityr Client for the 'Cumulocity' API Access the 'Cumulocity' API and retrieve data on devices, measurements, and events. Documentation for the API can be found at <https://www.cumulocity.com/guides/reference/rest-implementation/>.  "
  },
  {
    "id": 10698,
    "package_name": "currencyapi",
    "title": "Client for the 'currencyapi.com' Currency Conversion API",
    "description": "An R client for the 'currencyapi.com' currency conversion API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://currencyapi.com/docs> .",
    "version": "0.1.0",
    "maintainer": "Dominik Kukacka <dominik@everapi.com>",
    "author": "Dominik Kukacka [aut, cre]",
    "url": "https://currencyapi.com, https://currencyapi.com/docs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=currencyapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "currencyapi Client for the 'currencyapi.com' Currency Conversion API An R client for the 'currencyapi.com' currency conversion API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://currencyapi.com/docs> .  "
  },
  {
    "id": 10711,
    "package_name": "customiser",
    "title": "Use R Markdown to Write your \"Rprofile\"",
    "description": "A simple way to write \".Rprofile\" code in an R Markdown file and \n    have it knit to the correct location for your operating system. ",
    "version": "0.1.1",
    "maintainer": "James Laird-Smith <jameslairdsmith@gmail.com>",
    "author": "James Laird-Smith [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1175-4046>)",
    "url": "https://github.com/jameslairdsmith/customiser",
    "bug_reports": "https://github.com/jameslairdsmith/customiser/issues",
    "repository": "https://cran.r-project.org/package=customiser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "customiser Use R Markdown to Write your \"Rprofile\" A simple way to write \".Rprofile\" code in an R Markdown file and \n    have it knit to the correct location for your operating system.   "
  },
  {
    "id": 10719,
    "package_name": "cvAUC",
    "title": "Cross-Validated Area Under the ROC Curve Confidence Intervals",
    "description": "Tools for working with and evaluating cross-validated area under the ROC curve (AUC) estimators.  The primary functions of the package are ci.cvAUC and ci.pooled.cvAUC, which report cross-validated AUC and compute confidence intervals for cross-validated AUC estimates based on influence curves for i.i.d. and pooled repeated measures data, respectively.  One benefit to using influence curve based confidence intervals is that they require much less computation time than bootstrapping methods.  The utility functions, AUC and cvAUC, are simple wrappers for functions from the ROCR package.",
    "version": "1.1.4",
    "maintainer": "Erin LeDell <oss@ledell.org>",
    "author": "Erin LeDell, Maya Petersen, Mark van der Laan",
    "url": "https://github.com/ledell/cvAUC",
    "bug_reports": "https://github.com/ledell/cvAUC/issues",
    "repository": "https://cran.r-project.org/package=cvAUC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvAUC Cross-Validated Area Under the ROC Curve Confidence Intervals Tools for working with and evaluating cross-validated area under the ROC curve (AUC) estimators.  The primary functions of the package are ci.cvAUC and ci.pooled.cvAUC, which report cross-validated AUC and compute confidence intervals for cross-validated AUC estimates based on influence curves for i.i.d. and pooled repeated measures data, respectively.  One benefit to using influence curve based confidence intervals is that they require much less computation time than bootstrapping methods.  The utility functions, AUC and cvAUC, are simple wrappers for functions from the ROCR package.  "
  },
  {
    "id": 10727,
    "package_name": "cvdprevent",
    "title": "Access and Analyse Data from the 'CVD Prevent' API",
    "description": "Provides an R interface to the 'CVD Prevent' application\n    programming interface (API), allowing users to retrieve and analyse\n    cardiovascular disease prevention data from primary care records\n    across England. The Cardiovascular Disease Prevention Audit\n    (CVDPREVENT) automatically extracts routinely held GP health data to\n    support national reporting and improvement initiatives.  See the API\n    documentation for details:\n    <https://bmchealthdocs.atlassian.net/wiki/spaces/CP/pages/317882369/CVDPREVENT+API+Documentation>.",
    "version": "0.2.4",
    "maintainer": "Craig Parylo <craig.parylo2@nhs.net>",
    "author": "Craig Parylo [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4297-7808>),\n  Cardiovascular Disease Prevention Audit (CVDPREVENT) [ant] (Data\n    provider and audit sponsor)",
    "url": "https://github.com/craig-parylo/cvdprevent\nhttps://craig-parylo.github.io/cvdprevent/",
    "bug_reports": "https://github.com/craig-parylo/cvdprevent/issues",
    "repository": "https://cran.r-project.org/package=cvdprevent",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvdprevent Access and Analyse Data from the 'CVD Prevent' API Provides an R interface to the 'CVD Prevent' application\n    programming interface (API), allowing users to retrieve and analyse\n    cardiovascular disease prevention data from primary care records\n    across England. The Cardiovascular Disease Prevention Audit\n    (CVDPREVENT) automatically extracts routinely held GP health data to\n    support national reporting and improvement initiatives.  See the API\n    documentation for details:\n    <https://bmchealthdocs.atlassian.net/wiki/spaces/CP/pages/317882369/CVDPREVENT+API+Documentation>.  "
  },
  {
    "id": 10737,
    "package_name": "cyclestreets",
    "title": "Cycle Routing and Data for Cycling Advocacy",
    "description": "An interface to the cycle routing/data services provided by\n    'CycleStreets', a not-for-profit social enterprise and advocacy\n    organisation.  The application programming interfaces (APIs) provided\n    by 'CycleStreets' are documented at\n    (<https://www.cyclestreets.net/api/>).  The focus of this package is\n    the journey planning API, which aims to emulate the routes taken by a\n    knowledgeable cyclist.  An innovative feature of the routing service\n    of its provision of fastest, quietest and balanced profiles.  These\n    represent routes taken to minimise time, avoid traffic and compromise\n    between the two, respectively.",
    "version": "1.0.3",
    "maintainer": "Robin Lovelace <rob00x@gmail.com>",
    "author": "Robin Lovelace [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5679-6536>),\n  Martin Lucas-Smith [aut],\n  Eric Krueger [ctb],\n  Joey Talbot [aut] (ORCID: <https://orcid.org/0000-0002-6520-4560>),\n  Malcolm Morgan [ctb] (ORCID: <https://orcid.org/0000-0002-9488-9183>),\n  Zhao Wang [ctb] (ORCID: <https://orcid.org/0000-0002-4054-0533>)",
    "url": "https://rpackage.cyclestreets.net/,\nhttps://github.com/cyclestreets/cyclestreets-r",
    "bug_reports": "https://github.com/cyclestreets/cyclestreets-r/issues",
    "repository": "https://cran.r-project.org/package=cyclestreets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cyclestreets Cycle Routing and Data for Cycling Advocacy An interface to the cycle routing/data services provided by\n    'CycleStreets', a not-for-profit social enterprise and advocacy\n    organisation.  The application programming interfaces (APIs) provided\n    by 'CycleStreets' are documented at\n    (<https://www.cyclestreets.net/api/>).  The focus of this package is\n    the journey planning API, which aims to emulate the routes taken by a\n    knowledgeable cyclist.  An innovative feature of the routing service\n    of its provision of fastest, quietest and balanced profiles.  These\n    represent routes taken to minimise time, avoid traffic and compromise\n    between the two, respectively.  "
  },
  {
    "id": 10746,
    "package_name": "cytofan",
    "title": "Plot Fan Plots for Cytometry Data using 'ggplot2'",
    "description": "An implementation of Fan plots for cytometry data in 'ggplot2'. \n    For reference see Britton, E.; Fisher, P. & J. Whitley (1998) The Inflation Report Projections: Understanding the Fan Chart \n    <https://www.bankofengland.co.uk/quarterly-bulletin/1998/q1/the-inflation-report-projections-understanding-the-fan-chart>).",
    "version": "0.1.1",
    "maintainer": "Yann Abraham <yann.abraham@gmail.com>",
    "author": "Yann Abraham [aut, cre]",
    "url": "https://github.com/yannabraham/cytofan",
    "bug_reports": "https://github.com/yannabraham/cytofan/issues",
    "repository": "https://cran.r-project.org/package=cytofan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cytofan Plot Fan Plots for Cytometry Data using 'ggplot2' An implementation of Fan plots for cytometry data in 'ggplot2'. \n    For reference see Britton, E.; Fisher, P. & J. Whitley (1998) The Inflation Report Projections: Understanding the Fan Chart \n    <https://www.bankofengland.co.uk/quarterly-bulletin/1998/q1/the-inflation-report-projections-understanding-the-fan-chart>).  "
  },
  {
    "id": 10752,
    "package_name": "d3Tree",
    "title": "Create Interactive Collapsible Trees with the JavaScript 'D3'\nLibrary",
    "description": "Create and customize interactive collapsible 'D3' trees using the 'D3'\n    JavaScript library and the 'htmlwidgets' package. These trees can be used\n    directly from the R console, from 'RStudio', in Shiny apps and R Markdown documents.\n    When in Shiny the tree layout is observed by the server and can be used as a reactive filter\n    of structured data.",
    "version": "0.3.0",
    "maintainer": "Jonathan Sidi <yonicd@gmail.com>",
    "author": "Jonathan Sidi [aut, cre],\n  Kenton Russell [ctb] (https://github.com/timelyportfolio)",
    "url": "https://github.com/yonicd/d3Tree",
    "bug_reports": "https://github.com/yonicd/d3Tree/issues",
    "repository": "https://cran.r-project.org/package=d3Tree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "d3Tree Create Interactive Collapsible Trees with the JavaScript 'D3'\nLibrary Create and customize interactive collapsible 'D3' trees using the 'D3'\n    JavaScript library and the 'htmlwidgets' package. These trees can be used\n    directly from the R console, from 'RStudio', in Shiny apps and R Markdown documents.\n    When in Shiny the tree layout is observed by the server and can be used as a reactive filter\n    of structured data.  "
  },
  {
    "id": 10754,
    "package_name": "d3po",
    "title": "Fast and Beautiful Interactive Visualization for 'Markdown' and\n'Shiny'",
    "description": "Apache licensed alternative to 'Highcharter' which provides \n  functions for both fast and beautiful interactive visualization for 'Markdown'\n  and 'Shiny'.",
    "version": "1.0.3",
    "maintainer": "Mauricio Vargas Sepulveda <m.vargas.sepulveda@gmail.com>",
    "author": "Mauricio Vargas Sepulveda [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1017-7574>),\n  John Coene [aut],\n  Ariel Alvarado [ctb],\n  Sylvain Lesage [ctb],\n  Curran Kelleher [ctb],\n  Fernando Becerra [ctb],\n  Natural Earth [dtc],\n  R Consortium [fnd] (Funded for the 2016-2017 ISC grants cycle)",
    "url": "https://pacha.dev/d3po/",
    "bug_reports": "https://github.com/pachadotdev/d3po/issues",
    "repository": "https://cran.r-project.org/package=d3po",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "d3po Fast and Beautiful Interactive Visualization for 'Markdown' and\n'Shiny' Apache licensed alternative to 'Highcharter' which provides \n  functions for both fast and beautiful interactive visualization for 'Markdown'\n  and 'Shiny'.  "
  },
  {
    "id": 10782,
    "package_name": "daff",
    "title": "Diff, Patch and Merge for Data.frames",
    "description": "Diff, patch and merge for data frames. Document changes in data\n    sets and use them to apply patches. Changes to data can be made visible by using\n    render_diff(). The 'V8' package is used to wrap the 'daff.js' 'JavaScript' library\n    which is included in the package.",
    "version": "1.1.1",
    "maintainer": "Edwin de Jonge <edwindjonge@gmail.com>",
    "author": "Paul Fitzpatrick [aut] (JavaScript original,\n    http://paulfitz.github.io/daff/),\n  Edwin de Jonge [aut, cre] (R wrapper, ORCID:\n    <https://orcid.org/0000-0002-6580-4718>),\n  Gregory R. Warnes [aut]",
    "url": "https://github.com/edwindj/daff",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=daff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "daff Diff, Patch and Merge for Data.frames Diff, patch and merge for data frames. Document changes in data\n    sets and use them to apply patches. Changes to data can be made visible by using\n    render_diff(). The 'V8' package is used to wrap the 'daff.js' 'JavaScript' library\n    which is included in the package.  "
  },
  {
    "id": 10789,
    "package_name": "daiR",
    "title": "Interface with Google Cloud Document AI API",
    "description": "R interface for the Google Cloud Services 'Document AI API'\n    <https://cloud.google.com/document-ai> with additional tools for\n    output file parsing and text reconstruction. 'Document AI' is a\n    powerful server-based OCR service that extracts text and tables from\n    images and PDF files with high accuracy. 'daiR' gives R users\n    programmatic access to this service and additional tools to handle\n    and visualize the output. See the package website <https://dair.info/>\n    for more information and examples.",
    "version": "1.2.0",
    "maintainer": "Thomas Hegghammer <hegghammer@gmail.com>",
    "author": "Thomas Hegghammer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6253-1518>)",
    "url": "https://github.com/Hegghammer/daiR, https://dair.info",
    "bug_reports": "https://github.com/Hegghammer/daiR/issues",
    "repository": "https://cran.r-project.org/package=daiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "daiR Interface with Google Cloud Document AI API R interface for the Google Cloud Services 'Document AI API'\n    <https://cloud.google.com/document-ai> with additional tools for\n    output file parsing and text reconstruction. 'Document AI' is a\n    powerful server-based OCR service that extracts text and tables from\n    images and PDF files with high accuracy. 'daiR' gives R users\n    programmatic access to this service and additional tools to handle\n    and visualize the output. See the package website <https://dair.info/>\n    for more information and examples.  "
  },
  {
    "id": 10803,
    "package_name": "dartR",
    "title": "Importing and Analysing 'SNP' and 'Silicodart' Data Generated by\nGenome-Wide Restriction Fragment Analysis",
    "description": "Functions are provided that facilitate the import and analysis of\n    'SNP' (single nucleotide polymorphism) and 'silicodart' (presence/absence) data. \n    The main focus is on data generated by 'DarT' (Diversity Arrays Technology), \n    however, data from other sequencing platforms can be used once 'SNP' or related \n    fragment presence/absence data from any source is imported. Genetic datasets \n    are stored in a derived 'genlight' format (package 'adegenet'), that allows for \n    a very compact storage of data and metadata. Functions are available for \n    importing and exporting of 'SNP' and 'silicodart' data, for reporting on and \n    filtering on various criteria (e.g. 'CallRate', heterozygosity, reproducibility, \n    maximum allele frequency). Additional functions are available for visualization \n    (e.g. Principle Coordinate Analysis) and creating a spatial representation \n    using maps. 'dartR' supports also the analysis of 3rd party software package \n    such as 'newhybrid', 'structure', 'NeEstimator' and 'blast'. Since version \n    2.0.3 we also implemented simulation functions, that allow to forward simulate\n    'SNP' dynamics under different population and evolutionary dynamics. \n    Comprehensive tutorials and support can be found at our 'github' repository: \n    github.com/green-striped-gecko/dartR/. If you want to cite 'dartR', you find \n    the information by typing citation('dartR') in the console.",
    "version": "2.9.9.5",
    "maintainer": "Bernd Gruber <bernd.gruber@canberra.edu.au>",
    "author": "Bernd Gruber [aut, cre],\n  Arthur Georges [aut],\n  Jose L. Mijangos [aut],\n  Carlo Pacioni [aut],\n  Diana Robledo-Ruiz [aut],\n  Peter J. Unmack [ctb],\n  Oliver Berry [ctb],\n  Lindsay V. Clark [ctb],\n  Floriaan Devloo-Delva [ctb],\n  Eric Archer [ctb]",
    "url": "https://green-striped-gecko.github.io/dartR/,\nhttps://github.com/green-striped-gecko/dartR",
    "bug_reports": "https://groups.google.com/g/dartr?pli=1",
    "repository": "https://cran.r-project.org/package=dartR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dartR Importing and Analysing 'SNP' and 'Silicodart' Data Generated by\nGenome-Wide Restriction Fragment Analysis Functions are provided that facilitate the import and analysis of\n    'SNP' (single nucleotide polymorphism) and 'silicodart' (presence/absence) data. \n    The main focus is on data generated by 'DarT' (Diversity Arrays Technology), \n    however, data from other sequencing platforms can be used once 'SNP' or related \n    fragment presence/absence data from any source is imported. Genetic datasets \n    are stored in a derived 'genlight' format (package 'adegenet'), that allows for \n    a very compact storage of data and metadata. Functions are available for \n    importing and exporting of 'SNP' and 'silicodart' data, for reporting on and \n    filtering on various criteria (e.g. 'CallRate', heterozygosity, reproducibility, \n    maximum allele frequency). Additional functions are available for visualization \n    (e.g. Principle Coordinate Analysis) and creating a spatial representation \n    using maps. 'dartR' supports also the analysis of 3rd party software package \n    such as 'newhybrid', 'structure', 'NeEstimator' and 'blast'. Since version \n    2.0.3 we also implemented simulation functions, that allow to forward simulate\n    'SNP' dynamics under different population and evolutionary dynamics. \n    Comprehensive tutorials and support can be found at our 'github' repository: \n    github.com/green-striped-gecko/dartR/. If you want to cite 'dartR', you find \n    the information by typing citation('dartR') in the console.  "
  },
  {
    "id": 10804,
    "package_name": "dartR.base",
    "title": "Analysing 'SNP' and 'Silicodart' Data - Basic Functions",
    "description": "Facilitates the import and analysis of 'SNP' (single nucleotide 'polymorphism') \n    and 'silicodart' (presence/absence) data. The main focus is on data generated by 'DarT' \n    (Diversity Arrays Technology), however, data from other sequencing platforms can be used \n    once 'SNP' or related fragment presence/absence data from any source is imported. Genetic \n    datasets are stored in a derived 'genlight' format (package 'adegenet'), that allows for \n    a very compact storage of data and metadata. Functions are available for \n    importing and exporting of 'SNP' and 'silicodart' data, for reporting on and \n    filtering on various criteria (e.g. 'callrate', 'heterozygosity', 'reproducibility', \n    maximum allele frequency). Additional functions are available for visualization \n    (e.g. Principle Coordinate Analysis) and creating a spatial representation \n    using maps. 'dartR.base' is the 'base' package of the 'dartRverse' suits of packages. \n    To install the other packages, we recommend to install the 'dartRverse' package, that \n    supports the installation of all packages in the 'dartRverse'.\n    If you want to cite 'dartR', you find the information by typing citation('dartR.base')\n    in the console.",
    "version": "1.0.7",
    "maintainer": "Bernd Gruber <bernd.gruber@canberra.edu.au>",
    "author": "Bernd Gruber [aut, cre],\n  Arthur Georges [aut],\n  Jose L. Mijangos [aut],\n  Carlo Pacioni [aut],\n  Diana Robledo-Ruiz [aut],\n  Peter J. Unmack [ctb],\n  Oliver Berry [ctb],\n  Lindsay V. Clark [ctb],\n  Floriaan Devloo-Delva [ctb],\n  Eric Archer [ctb],\n  Ching Ching Lau [ctb]",
    "url": "https://green-striped-gecko.github.io/dartR/",
    "bug_reports": "https://groups.google.com/g/dartr?pli=1",
    "repository": "https://cran.r-project.org/package=dartR.base",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dartR.base Analysing 'SNP' and 'Silicodart' Data - Basic Functions Facilitates the import and analysis of 'SNP' (single nucleotide 'polymorphism') \n    and 'silicodart' (presence/absence) data. The main focus is on data generated by 'DarT' \n    (Diversity Arrays Technology), however, data from other sequencing platforms can be used \n    once 'SNP' or related fragment presence/absence data from any source is imported. Genetic \n    datasets are stored in a derived 'genlight' format (package 'adegenet'), that allows for \n    a very compact storage of data and metadata. Functions are available for \n    importing and exporting of 'SNP' and 'silicodart' data, for reporting on and \n    filtering on various criteria (e.g. 'callrate', 'heterozygosity', 'reproducibility', \n    maximum allele frequency). Additional functions are available for visualization \n    (e.g. Principle Coordinate Analysis) and creating a spatial representation \n    using maps. 'dartR.base' is the 'base' package of the 'dartRverse' suits of packages. \n    To install the other packages, we recommend to install the 'dartRverse' package, that \n    supports the installation of all packages in the 'dartRverse'.\n    If you want to cite 'dartR', you find the information by typing citation('dartR.base')\n    in the console.  "
  },
  {
    "id": 10817,
    "package_name": "data.validator",
    "title": "Automatic Data Validation and Reporting",
    "description": "Validate dataset by columns and rows using convenient predicates inspired by 'assertr' package.\n             Generate good looking HTML report or print console output to display in logs of your data processing pipeline.",
    "version": "0.2.1",
    "maintainer": "Marcin Dubel <opensource+marcin@appsilon.com>",
    "author": "Marcin Dubel [aut, cre],\n  Pawe\u0142 Przytu\u0142a [aut],\n  Jakub Nowicki [aut],\n  Krystian Igras [aut],\n  Dominik Krzeminski [ctb],\n  Servet Ahmet \u00c7izmeli [ctb],\n  Appsilon Sp. z o.o. [cph]",
    "url": "https://appsilon.github.io/data.validator/,\nhttps://github.com/Appsilon/data.validator",
    "bug_reports": "https://github.com/Appsilon/data.validator/issues",
    "repository": "https://cran.r-project.org/package=data.validator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "data.validator Automatic Data Validation and Reporting Validate dataset by columns and rows using convenient predicates inspired by 'assertr' package.\n             Generate good looking HTML report or print console output to display in logs of your data processing pipeline.  "
  },
  {
    "id": 10821,
    "package_name": "dataMaid",
    "title": "A Suite of Checks for Identification of Potential Errors in a\nData Frame as Part of the Data Screening Process",
    "description": "Data screening is an important first step of any statistical\n    analysis. dataMaid auto generates a customizable data report with a thorough\n    summary of the checks and the results that a human can use to identify possible\n    errors. It provides an extendable suite of test for common potential\n    errors in a dataset. ",
    "version": "1.4.2",
    "maintainer": "Claus Thorn Ekstr\u00f8m <ekstrom@sund.ku.dk>",
    "author": "Anne Helby Petersen [aut],\n  Claus Thorn Ekstr\u00f8m [aut, cre]",
    "url": "https://github.com/ekstroem/dataMaid,\nhttps://doi.org/10.18637/jss.v090.i06",
    "bug_reports": "https://github.com/ekstroem/dataMaid/issues",
    "repository": "https://cran.r-project.org/package=dataMaid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataMaid A Suite of Checks for Identification of Potential Errors in a\nData Frame as Part of the Data Screening Process Data screening is an important first step of any statistical\n    analysis. dataMaid auto generates a customizable data report with a thorough\n    summary of the checks and the results that a human can use to identify possible\n    errors. It provides an extendable suite of test for common potential\n    errors in a dataset.   "
  },
  {
    "id": 10825,
    "package_name": "dataReporter",
    "title": "Reproducible Data Screening Checks and Report of Possible Errors",
    "description": "Data screening is an important first step of any statistical\n    analysis. 'dataReporter' auto generates a customizable data report with a thorough\n    summary of the checks and the results that a human can use to identify possible\n    errors. It provides an extendable suite of test for common potential\n    errors in a dataset. See Petersen AH, Ekstr\u00f8m CT (2019). \"dataMaid: Your Assistant for Documenting Supervised Data Quality Screening in R.\" _Journal of Statistical Software_, *90*(6), 1-38 <doi:10.18637/jss.v090.i06> for more information.",
    "version": "1.0.5",
    "maintainer": "Claus Thorn Ekstr\u00f8m <ekstrom@sund.ku.dk>",
    "author": "Anne Helby Petersen [aut],\n  Claus Thorn Ekstr\u00f8m [aut, cre]",
    "url": "https://github.com/ekstroem/dataReporter",
    "bug_reports": "https://github.com/ekstroem/dataReporter/issues",
    "repository": "https://cran.r-project.org/package=dataReporter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataReporter Reproducible Data Screening Checks and Report of Possible Errors Data screening is an important first step of any statistical\n    analysis. 'dataReporter' auto generates a customizable data report with a thorough\n    summary of the checks and the results that a human can use to identify possible\n    errors. It provides an extendable suite of test for common potential\n    errors in a dataset. See Petersen AH, Ekstr\u00f8m CT (2019). \"dataMaid: Your Assistant for Documenting Supervised Data Quality Screening in R.\" _Journal of Statistical Software_, *90*(6), 1-38 <doi:10.18637/jss.v090.i06> for more information.  "
  },
  {
    "id": 10833,
    "package_name": "datadriftR",
    "title": "Concept Drift Detection Methods for Stream Data",
    "description": "\n    A system designed for detecting concept drift in streaming datasets. It offers a comprehensive suite of statistical methods to detect concept drift, including methods for monitoring changes in data distributions\n    over time. The package supports several tests, such as Drift Detection Method (DDM), Early Drift Detection Method (EDDM), Hoeffding Drift Detection Methods (HDDM_A, HDDM_W), Kolmogorov-Smirnov test-based Windowing (KSWIN)\n    and Page Hinkley (PH) tests. The methods implemented in this package are based on established research and have been demonstrated to be effective in real-time data analysis. For more details on the methods, \n    please check to the following sources. Kobyli\u0144ska et al. (2023) <doi:10.48550/arXiv.2308.11446>, S. Kullback & R.A. Leibler (1951) <doi:10.1214/aoms/1177729694>, Gama et al. (2004) <doi:10.1007/978-3-540-28645-5_29>, Baena-Garcia et al. (2006) <https://www.researchgate.net/publication/245999704_Early_Drift_Detection_Method>,\n    Fr\u00edas-Blanco et al. (2014) <https://ieeexplore.ieee.org/document/6871418>, Raab et al. (2020) <doi:10.1016/j.neucom.2019.11.111>, Page (1954) <doi:10.1093/biomet/41.1-2.100>, Montiel et al. (2018) <https://jmlr.org/papers/volume19/18-251/18-251.pdf>.",
    "version": "1.0.0",
    "maintainer": "Ugur Dar <ugurdarr@gmail.com>",
    "author": "Ugur Dar [aut, cre],\n  Mustafa Cavus [ctb, ths]",
    "url": "https://github.com/ugurdar/datadriftR",
    "bug_reports": "https://github.com/ugurdar/datadriftR/issues",
    "repository": "https://cran.r-project.org/package=datadriftR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datadriftR Concept Drift Detection Methods for Stream Data \n    A system designed for detecting concept drift in streaming datasets. It offers a comprehensive suite of statistical methods to detect concept drift, including methods for monitoring changes in data distributions\n    over time. The package supports several tests, such as Drift Detection Method (DDM), Early Drift Detection Method (EDDM), Hoeffding Drift Detection Methods (HDDM_A, HDDM_W), Kolmogorov-Smirnov test-based Windowing (KSWIN)\n    and Page Hinkley (PH) tests. The methods implemented in this package are based on established research and have been demonstrated to be effective in real-time data analysis. For more details on the methods, \n    please check to the following sources. Kobyli\u0144ska et al. (2023) <doi:10.48550/arXiv.2308.11446>, S. Kullback & R.A. Leibler (1951) <doi:10.1214/aoms/1177729694>, Gama et al. (2004) <doi:10.1007/978-3-540-28645-5_29>, Baena-Garcia et al. (2006) <https://www.researchgate.net/publication/245999704_Early_Drift_Detection_Method>,\n    Fr\u00edas-Blanco et al. (2014) <https://ieeexplore.ieee.org/document/6871418>, Raab et al. (2020) <doi:10.1016/j.neucom.2019.11.111>, Page (1954) <doi:10.1093/biomet/41.1-2.100>, Montiel et al. (2018) <https://jmlr.org/papers/volume19/18-251/18-251.pdf>.  "
  },
  {
    "id": 10839,
    "package_name": "dataonderivatives",
    "title": "Easily Source Publicly Available Data on Derivatives",
    "description": "Post Global Financial Crisis derivatives reforms have lifted\n    the veil off over-the-counter (OTC) derivative markets. Swap Execution\n    Facilities (SEFs) and Swap Data Repositories (SDRs) now publish data\n    on swaps that are traded on or reported to those facilities\n    (respectively). This package provides you the ability to get this data\n    from supported sources.",
    "version": "0.4.0",
    "maintainer": "Imanuel Costigan <i.costigan@me.com>",
    "author": "Imanuel Costigan [aut, cre]",
    "url": "https://github.com/imanuelcostigan/dataonderivatives,\nhttp://imanuelcostigan.github.io/dataonderivatives/",
    "bug_reports": "https://github.com/imanuelcostigan/dataonderivatives/issues",
    "repository": "https://cran.r-project.org/package=dataonderivatives",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataonderivatives Easily Source Publicly Available Data on Derivatives Post Global Financial Crisis derivatives reforms have lifted\n    the veil off over-the-counter (OTC) derivative markets. Swap Execution\n    Facilities (SEFs) and Swap Data Repositories (SDRs) now publish data\n    on swaps that are traded on or reported to those facilities\n    (respectively). This package provides you the ability to get this data\n    from supported sources.  "
  },
  {
    "id": 10844,
    "package_name": "dataquieR",
    "title": "Data Quality in Epidemiological Research",
    "description": "Data quality assessments guided by a\n    'data quality framework introduced by Schmidt and colleagues, 2021' \n    <doi:10.1186/s12874-021-01252-7> target the\n    data quality dimensions integrity, completeness, consistency, and\n    accuracy. The scope of applicable functions rests on the\n    availability of extensive metadata which can be provided in\n    spreadsheet tables. Either standardized (e.g. as 'html5' reports) or\n    individually tailored reports can be generated. For an introduction\n    into the specification of corresponding metadata, please refer to the\n    'package website'\n    <https://dataquality.qihs.uni-greifswald.de/VIN_Annotation_of_Metadata.html>.",
    "version": "2.5.1",
    "maintainer": "Stephan Struckmann <stephan.struckmann@uni-greifswald.de>",
    "author": "University Medicine Greifswald [cph],\n  Elisa Kasbohm [aut] (ORCID: <https://orcid.org/0000-0001-5261-538X>),\n  Elena Salogni [aut] (ORCID: <https://orcid.org/0009-0007-3767-7145>),\n  Joany Marino [aut] (ORCID: <https://orcid.org/0000-0002-4657-3758>),\n  Adrian Richter [aut] (ORCID: <https://orcid.org/0000-0002-3372-2021>),\n  Carsten Oliver Schmidt [aut] (ORCID:\n    <https://orcid.org/0000-0001-5266-9396>),\n  Stephan Struckmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8565-7962>),\n  German Research Foundation (DFG SCHM 2744/3-1, SCHM 2744/9-1, SCHM\n    2744/3-4) [fnd],\n  National Research Data Infrastructure for Personal Health Data: (NFDI\n    13/1) [fnd],\n  European Union\u2019s Horizon 2020 programme (euCanSHare, grant agreement\n    No. 825903) [fnd]",
    "url": "https://dataquality.qihs.uni-greifswald.de/",
    "bug_reports": "https://gitlab.com/libreumg/dataquier/-/issues",
    "repository": "https://cran.r-project.org/package=dataquieR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataquieR Data Quality in Epidemiological Research Data quality assessments guided by a\n    'data quality framework introduced by Schmidt and colleagues, 2021' \n    <doi:10.1186/s12874-021-01252-7> target the\n    data quality dimensions integrity, completeness, consistency, and\n    accuracy. The scope of applicable functions rests on the\n    availability of extensive metadata which can be provided in\n    spreadsheet tables. Either standardized (e.g. as 'html5' reports) or\n    individually tailored reports can be generated. For an introduction\n    into the specification of corresponding metadata, please refer to the\n    'package website'\n    <https://dataquality.qihs.uni-greifswald.de/VIN_Annotation_of_Metadata.html>.  "
  },
  {
    "id": 10846,
    "package_name": "datareportR",
    "title": "Fast Data Summary Reports",
    "description": "Generates an RMarkdown data report with two components: \n   a summary of an input dataset and a diff of the dataset relative to an old version.",
    "version": "0.1.1",
    "maintainer": "Bryant Cong <bryant.bcp@gmail.com>",
    "author": "Bryant Cong [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=datareportR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datareportR Fast Data Summary Reports Generates an RMarkdown data report with two components: \n   a summary of an input dataset and a diff of the dataset relative to an old version.  "
  },
  {
    "id": 10847,
    "package_name": "dataresqc",
    "title": "C3S Quality Control Tools for Historical Climate Data",
    "description": "Quality control and formatting tools developed for the Copernicus Data Rescue Service. The package includes functions to handle the Station Exchange Format (SEF), various statistical tests for climate data at daily and sub-daily resolution, as well as functions to plot the data. For more information and documentation see <https://datarescue.climate.copernicus.eu/st_data-quality-control>.",
    "version": "1.1.1",
    "maintainer": "Yuri Brugnara <yuri.brugnara@gmail.com>",
    "author": "Yuri Brugnara [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8427-0064>),\n  Alba Gilabert [aut],\n  Clara Ventura [aut],\n  Stefan Hunziker [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dataresqc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataresqc C3S Quality Control Tools for Historical Climate Data Quality control and formatting tools developed for the Copernicus Data Rescue Service. The package includes functions to handle the Station Exchange Format (SEF), various statistical tests for climate data at daily and sub-daily resolution, as well as functions to plot the data. For more information and documentation see <https://datarescue.climate.copernicus.eu/st_data-quality-control>.  "
  },
  {
    "id": 10858,
    "package_name": "dataviewR",
    "title": "An Interactive and Feature-Rich Data Viewer",
    "description": "Provides an interactive viewer for 'data.frame' and 'tibble' objects using 'shiny' <https://shiny.posit.co/> and 'DT' <https://rstudio.github.io/DT/>. It supports complex filtering, column selection, and automatic generation of reproducible 'dplyr' <https://dplyr.tidyverse.org/> code for data manipulation. The package is designed for ease of use in data exploration and reporting workflows.",
    "version": "0.1.1",
    "maintainer": "Madhan Kumar N <madhanmanoj1999@gmail.com>",
    "author": "Madhan Kumar N [aut, cre]",
    "url": "https://github.com/madhankumarnagaraji/dataviewR",
    "bug_reports": "https://github.com/madhankumarnagaraji/dataviewR/issues",
    "repository": "https://cran.r-project.org/package=dataviewR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataviewR An Interactive and Feature-Rich Data Viewer Provides an interactive viewer for 'data.frame' and 'tibble' objects using 'shiny' <https://shiny.posit.co/> and 'DT' <https://rstudio.github.io/DT/>. It supports complex filtering, column selection, and automatic generation of reproducible 'dplyr' <https://dplyr.tidyverse.org/> code for data manipulation. The package is designed for ease of use in data exploration and reporting workflows.  "
  },
  {
    "id": 10909,
    "package_name": "dcmodify",
    "title": "Modify Data Using Externally Defined Modification Rules",
    "description": "Data cleaning scripts typically contain a lot of 'if this change that'\n    type of statements. Such statements are typically condensed expert knowledge.\n    With this package, such 'data modifying rules' are taken out of the code and\n    become in stead parameters to the work flow. This allows one to maintain, document,\n    and reason about data modification rules as separate entities.",
    "version": "0.9.0",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9807-4686>),\n  Edwin de Jonge [aut] (ORCID: <https://orcid.org/0000-0002-6580-4718>),\n  Sjabbo Schaveling [ctb],\n  Floris Ruijter [ctb]",
    "url": "https://github.com/data-cleaning/dcmodify",
    "bug_reports": "https://github.com/data-cleaning/dcmodify/issues",
    "repository": "https://cran.r-project.org/package=dcmodify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dcmodify Modify Data Using Externally Defined Modification Rules Data cleaning scripts typically contain a lot of 'if this change that'\n    type of statements. Such statements are typically condensed expert knowledge.\n    With this package, such 'data modifying rules' are taken out of the code and\n    become in stead parameters to the work flow. This allows one to maintain, document,\n    and reason about data modification rules as separate entities.  "
  },
  {
    "id": 10937,
    "package_name": "debar",
    "title": "A Post-Clustering Denoiser for COI-5P Barcode Data",
    "description": "The 'debar' sequence processing pipeline is designed for denoising high throughput \n    sequencing data for the animal DNA barcode marker cytochrome c oxidase I (COI). The package \n    is designed to detect and correct insertion and deletion errors within sequencer outputs. \n    This is accomplished through comparison of input sequences against a profile hidden Markov \n    model (PHMM) using the Viterbi algorithm (for algorithm details see Durbin et al. 1998, \n    ISBN: 9780521629713). Inserted base pairs are removed and deleted base pairs are accounted \n    for through the introduction of a placeholder character. Since the PHMM is a probabilistic \n    representation of the COI barcode, corrections are not always perfect. For this reason \n    'debar' censors base pairs adjacent to reported indel sites, turning them into placeholder \n    characters (default is 7 base pairs in either direction, this feature can be disabled).\n    Testing has shown that this censorship results in the correct sequence length being restored, \n    and erroneous base pairs being masked the vast majority of the time (>95%). ",
    "version": "0.1.1",
    "maintainer": "Cameron M. Nugent <camnugent@gmail.com>",
    "author": "Cameron M. Nugent",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=debar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "debar A Post-Clustering Denoiser for COI-5P Barcode Data The 'debar' sequence processing pipeline is designed for denoising high throughput \n    sequencing data for the animal DNA barcode marker cytochrome c oxidase I (COI). The package \n    is designed to detect and correct insertion and deletion errors within sequencer outputs. \n    This is accomplished through comparison of input sequences against a profile hidden Markov \n    model (PHMM) using the Viterbi algorithm (for algorithm details see Durbin et al. 1998, \n    ISBN: 9780521629713). Inserted base pairs are removed and deleted base pairs are accounted \n    for through the introduction of a placeholder character. Since the PHMM is a probabilistic \n    representation of the COI barcode, corrections are not always perfect. For this reason \n    'debar' censors base pairs adjacent to reported indel sites, turning them into placeholder \n    characters (default is 7 base pairs in either direction, this feature can be disabled).\n    Testing has shown that this censorship results in the correct sequence length being restored, \n    and erroneous base pairs being masked the vast majority of the time (>95%).   "
  },
  {
    "id": 10964,
    "package_name": "deepRstudio",
    "title": "Seamless Language Translation in 'RStudio' using 'DeepL' API and\n'Rstudioapi'",
    "description": "Enhancing cross-language compatibility within the 'RStudio' environment and supporting seamless language understanding, \n    the 'deepRstudio' package leverages the power of the 'DeepL' API (see <https://www.deepl.com/docs-api>) to enable seamless, fast, \n    accurate, and affordable translation of code comments, documents, and text. This package offers the ability to translate selected text \n    into English (EN), as well as from English into various languages, namely Japanese (JA), Chinese (ZH), Spanish (ES), \n    French (FR), Russian (RU), Portuguese (PT), and Indonesian (ID). With much of the text being written in English, the emphasis \n    is on compatibility from English. It is also designed for developers working on multilingual projects and data analysts collaborating \n    with international teams, simplifying the translation process and making code more accessible and comprehensible to people with \n    diverse language backgrounds. This package uses the 'rstudioapi' package and 'DeepL' API, and is simply implemented, executed \n    from addins or via shortcuts on 'RStudio'. With just a few steps, content can be translated between supported languages, promoting better \n    collaboration and expanding the global reach of work. The functionality of this package works only on 'RStudio' using 'rstudioapi'.",
    "version": "0.0.9",
    "maintainer": "Satoshi Kume <satoshi.kume.1984@gmail.com>",
    "author": "Satoshi Kume [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7481-2843>)",
    "url": "https://kumes.github.io/deepRstudio/,\nhttps://github.com/kumeS/deepRstudio",
    "bug_reports": "https://github.com/kumeS/deepRstudio/issues",
    "repository": "https://cran.r-project.org/package=deepRstudio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deepRstudio Seamless Language Translation in 'RStudio' using 'DeepL' API and\n'Rstudioapi' Enhancing cross-language compatibility within the 'RStudio' environment and supporting seamless language understanding, \n    the 'deepRstudio' package leverages the power of the 'DeepL' API (see <https://www.deepl.com/docs-api>) to enable seamless, fast, \n    accurate, and affordable translation of code comments, documents, and text. This package offers the ability to translate selected text \n    into English (EN), as well as from English into various languages, namely Japanese (JA), Chinese (ZH), Spanish (ES), \n    French (FR), Russian (RU), Portuguese (PT), and Indonesian (ID). With much of the text being written in English, the emphasis \n    is on compatibility from English. It is also designed for developers working on multilingual projects and data analysts collaborating \n    with international teams, simplifying the translation process and making code more accessible and comprehensible to people with \n    diverse language backgrounds. This package uses the 'rstudioapi' package and 'DeepL' API, and is simply implemented, executed \n    from addins or via shortcuts on 'RStudio'. With just a few steps, content can be translated between supported languages, promoting better \n    collaboration and expanding the global reach of work. The functionality of this package works only on 'RStudio' using 'rstudioapi'.  "
  },
  {
    "id": 10991,
    "package_name": "delma",
    "title": "Convert 'R Markdown' and 'Quarto' Documents to Ecological\nMetadata Language",
    "description": "Ecological Metadata Language or 'EML' is a long-established format\n  for describing ecological datasets to facilitate sharing and re-use. Because\n  'EML' is effectively a modified 'xml' schema, however, it is challenging to \n  write and manipulate for non-expert users. 'delma' supports users to write \n  metadata statements in 'R Markdown' or 'Quarto markdown' format, and parse \n  them to 'EML' and (optionally) back again.",
    "version": "0.1.1",
    "maintainer": "Martin Westgate <martin.westgate@csiro.au>",
    "author": "Martin Westgate [aut, cre],\n  Shandiya Balasubramaniam [aut],\n  Dax Kellie [aut]",
    "url": "https://delma.ala.org.au",
    "bug_reports": "https://github.com/AtlasOfLivingAustralia/delma/issues",
    "repository": "https://cran.r-project.org/package=delma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "delma Convert 'R Markdown' and 'Quarto' Documents to Ecological\nMetadata Language Ecological Metadata Language or 'EML' is a long-established format\n  for describing ecological datasets to facilitate sharing and re-use. Because\n  'EML' is effectively a modified 'xml' schema, however, it is challenging to \n  write and manipulate for non-expert users. 'delma' supports users to write \n  metadata statements in 'R Markdown' or 'Quarto markdown' format, and parse \n  them to 'EML' and (optionally) back again.  "
  },
  {
    "id": 10997,
    "package_name": "demoGraphic",
    "title": "Providing Demographic Table with the P-Value, Standardized Mean\nDifference Value",
    "description": "The Demographic Table in R combines contingency table for categorical variables, mean and standard deviation for continuous variables. t-test, chi-square test and Fisher's exact test calculated the p-value of two groups. The standardized mean difference were performed with 95 % confident interval, and writing table into document file.",
    "version": "0.1.0",
    "maintainer": "Loan Robinson <loankimrobinson@gmail.com>",
    "author": "Loan Robinson [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=demoGraphic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "demoGraphic Providing Demographic Table with the P-Value, Standardized Mean\nDifference Value The Demographic Table in R combines contingency table for categorical variables, mean and standard deviation for continuous variables. t-test, chi-square test and Fisher's exact test calculated the p-value of two groups. The standardized mean difference were performed with 95 % confident interval, and writing table into document file.  "
  },
  {
    "id": 11024,
    "package_name": "dentomedical",
    "title": "Publication-Ready Descriptive, Bivariate, Regression, and\nDiagnostic Accuracy Tools for Medical and Dental Data",
    "description": "The 'dentomedical' package provides a comprehensive suite of tools for\n medical and dental research. It includes automated descriptive statistics,\n bivariate analysis with intelligent test selection, logistic regression,\n and diagnostic accuracy assessment. All functions generate structured,\n publication-ready tables using 'flextable', ensuring reproducibility and\n clarity suitable for manuscripts, reports, and clinical research workflows.",
    "version": "0.1.3",
    "maintainer": "Umar Hussain <drumarhussain@gmail.com>",
    "author": "Umar Hussain [aut, cre]",
    "url": "https://github.com/umarhussain-git/dentomedical",
    "bug_reports": "https://github.com/umarhussain-git/dentomedical/issues",
    "repository": "https://cran.r-project.org/package=dentomedical",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dentomedical Publication-Ready Descriptive, Bivariate, Regression, and\nDiagnostic Accuracy Tools for Medical and Dental Data The 'dentomedical' package provides a comprehensive suite of tools for\n medical and dental research. It includes automated descriptive statistics,\n bivariate analysis with intelligent test selection, logistic regression,\n and diagnostic accuracy assessment. All functions generate structured,\n publication-ready tables using 'flextable', ensuring reproducibility and\n clarity suitable for manuscripts, reports, and clinical research workflows.  "
  },
  {
    "id": 11045,
    "package_name": "descstatsr",
    "title": "Descriptive Univariate Statistics",
    "description": "It generates summary statistics on the input dataset using different descriptive univariate \n    statistical measures on entire data or at a group level. Though there are other packages which does \n    similar job but each of these are deficient in one form or other, in the measures generated, in\n    treating numeric, character and date variables alike, no functionality to view these measures on a\n    group level or the way the output is represented. Given the foremost role of the descriptive statistics\n    in any of the exploratory data analysis or solution development, there is a need for a more constructive, \n    structured and refined version over these packages. This is the idea behind the package and it brings \n    together all the required descriptive measures to give an initial understanding of the data quality, \n    distribution in a faster,easier and elaborative way.The function brings an additional capability to be \n    able to generate these statistical measures on the entire dataset or at a group level. It calculates measures \n    of central tendency (mean, median), distribution (count, proportion), dispersion (min, max, quantile, \n    standard deviation, variance) and shape (skewness, kurtosis). Addition to these measures, it provides information on \n    the data type, count on no. of rows, unique entries and percentage of missing entries. More importantly the measures \n    are generated based on the data types as required by them,rather than applying numerical measures on character and \n    data variables and vice versa. Output as a dataframe object gives a very neat representation, which often is useful \n    when working with a large number of columns. It can easily be exported as csv and analyzed further or presented as a \n    summary report for the data.",
    "version": "0.1.0",
    "maintainer": "Harish Kumar <kadamatiharish@gmail.com>",
    "author": "Harish Kumar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=descstatsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "descstatsr Descriptive Univariate Statistics It generates summary statistics on the input dataset using different descriptive univariate \n    statistical measures on entire data or at a group level. Though there are other packages which does \n    similar job but each of these are deficient in one form or other, in the measures generated, in\n    treating numeric, character and date variables alike, no functionality to view these measures on a\n    group level or the way the output is represented. Given the foremost role of the descriptive statistics\n    in any of the exploratory data analysis or solution development, there is a need for a more constructive, \n    structured and refined version over these packages. This is the idea behind the package and it brings \n    together all the required descriptive measures to give an initial understanding of the data quality, \n    distribution in a faster,easier and elaborative way.The function brings an additional capability to be \n    able to generate these statistical measures on the entire dataset or at a group level. It calculates measures \n    of central tendency (mean, median), distribution (count, proportion), dispersion (min, max, quantile, \n    standard deviation, variance) and shape (skewness, kurtosis). Addition to these measures, it provides information on \n    the data type, count on no. of rows, unique entries and percentage of missing entries. More importantly the measures \n    are generated based on the data types as required by them,rather than applying numerical measures on character and \n    data variables and vice versa. Output as a dataframe object gives a very neat representation, which often is useful \n    when working with a large number of columns. It can easily be exported as csv and analyzed further or presented as a \n    summary report for the data.  "
  },
  {
    "id": 11050,
    "package_name": "deseats",
    "title": "Data-Driven Locally Weighted Regression for Trend and\nSeasonality in TS",
    "description": "Various methods for the identification of trend and seasonal\n  components in time series (TS) are provided. Among them is a data-driven locally\n  weighted regression approach with automatically selected bandwidth for\n  equidistant short-memory time series. The approach is a\n  combination / extension of the algorithms by\n  Feng (2013) <doi:10.1080/02664763.2012.740626> and Feng, Y., Gries, T.,\n  and Fritz, M. (2020) <doi:10.1080/10485252.2020.1759598> and a brief\n  description of this new method is provided in the package documentation.\n  Furthermore, the package allows its users to apply the base model of the\n  Berlin procedure, version 4.1, as described in Speth (2004) <https://www.destatis.de/DE/Methoden/Saisonbereinigung/BV41-methodenbericht-Heft3_2004.pdf?__blob=publicationFile>.\n  Permission to include this procedure was kindly provided by the Federal\n  Statistical Office of Germany.",
    "version": "1.1.1",
    "maintainer": "Dominik Schulz <dominik.schulz@uni-paderborn.de>",
    "author": "Yuanhua Feng [aut] (Paderborn University, Germany),\n  Dominik Schulz [aut, cre] (Paderborn University, Germany)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=deseats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deseats Data-Driven Locally Weighted Regression for Trend and\nSeasonality in TS Various methods for the identification of trend and seasonal\n  components in time series (TS) are provided. Among them is a data-driven locally\n  weighted regression approach with automatically selected bandwidth for\n  equidistant short-memory time series. The approach is a\n  combination / extension of the algorithms by\n  Feng (2013) <doi:10.1080/02664763.2012.740626> and Feng, Y., Gries, T.,\n  and Fritz, M. (2020) <doi:10.1080/10485252.2020.1759598> and a brief\n  description of this new method is provided in the package documentation.\n  Furthermore, the package allows its users to apply the base model of the\n  Berlin procedure, version 4.1, as described in Speth (2004) <https://www.destatis.de/DE/Methoden/Saisonbereinigung/BV41-methodenbericht-Heft3_2004.pdf?__blob=publicationFile>.\n  Permission to include this procedure was kindly provided by the Federal\n  Statistical Office of Germany.  "
  },
  {
    "id": 11061,
    "package_name": "details",
    "title": "Create Details HTML Tag for Markdown and Package Documentation",
    "description": "Create a details HTML tag around R objects to place\n    in a Markdown, 'Rmarkdown' and 'roxygen2' documentation.",
    "version": "0.4.0",
    "maintainer": "Jonathan Sidi <yonicd@gmail.com>",
    "author": "Jonathan Sidi [aut, cre]",
    "url": "https://github.com/yonicd/details",
    "bug_reports": "https://github.com/yonicd/details/issues",
    "repository": "https://cran.r-project.org/package=details",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "details Create Details HTML Tag for Markdown and Package Documentation Create a details HTML tag around R objects to place\n    in a Markdown, 'Rmarkdown' and 'roxygen2' documentation.  "
  },
  {
    "id": 11065,
    "package_name": "detectXOR",
    "title": "XOR Pattern Detection and Visualization",
    "description": "Provides tools for detecting XOR-like patterns in variable pairs in \n    two-class data sets. Includes visualizations for pattern exploration and reporting\n    capabilities with both text and HTML output formats.",
    "version": "0.1.0",
    "maintainer": "Jorn Lotsch <j.lotsch@em.uni-frankfurt.de>",
    "author": "Jorn Lotsch [aut, cre] (ORCID: <https://orcid.org/0000-0002-5818-6958>),\n  Alfred Ultsch [aut]",
    "url": "https://github.com/JornLotsch/detectXOR",
    "bug_reports": "https://github.com/JornLotsch/detectXOR/issues",
    "repository": "https://cran.r-project.org/package=detectXOR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "detectXOR XOR Pattern Detection and Visualization Provides tools for detecting XOR-like patterns in variable pairs in \n    two-class data sets. Includes visualizations for pattern exploration and reporting\n    capabilities with both text and HTML output formats.  "
  },
  {
    "id": 11070,
    "package_name": "detourr",
    "title": "Portable and Performant Tour Animations",
    "description": "Provides 2D and 3D tour animations as HTML widgets. The user can interact with the widgets using orbit controls, tooltips, brushing, and timeline controls. Linked brushing is supported using 'crosstalk', and widgets can be embedded in Shiny apps or HTML documents.",
    "version": "0.2.0",
    "maintainer": "Casper Hart <casperhart93@gmail.com>",
    "author": "Casper Hart [aut, cre],\n  Earo Wang [aut, ths] (ORCID: <https://orcid.org/0000-0001-6448-5260>)",
    "url": "https://casperhart.github.io/detourr/",
    "bug_reports": "https://github.com/casperhart/detourr/issues",
    "repository": "https://cran.r-project.org/package=detourr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "detourr Portable and Performant Tour Animations Provides 2D and 3D tour animations as HTML widgets. The user can interact with the widgets using orbit controls, tooltips, brushing, and timeline controls. Linked brushing is supported using 'crosstalk', and widgets can be embedded in Shiny apps or HTML documents.  "
  },
  {
    "id": 11099,
    "package_name": "dfvad",
    "title": "Diewert and Fox's Method of Value Added Growth Decomposition",
    "description": "Decomposing value added growth into explanatory factors.\n    A cost constrained value added function is defined to specify the \n    production frontier. Industry estimates can also be aggregated using \n    a weighted average approach.\n    Details about the methodology and data can be found in Diewert and Fox (2018)\n    <doi:10.1093/oxfordhb/9780190226718.013.19>\n    and Zeng, Parsons, Diewert and Fox (2018)\n    <https://www.business.unsw.edu.au/research-site/centreforappliedeconomicresearch-site/Documents/emg2018-6_SZeng_EMG-Slides.pdf>.",
    "version": "0.3.6",
    "maintainer": "Shipei Zeng <shipei.zeng@unswalumni.com>",
    "author": "Shipei Zeng",
    "url": "https://github.com/shipei-zeng/dfvad",
    "bug_reports": "https://github.com/shipei-zeng/dfvad/issues",
    "repository": "https://cran.r-project.org/package=dfvad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dfvad Diewert and Fox's Method of Value Added Growth Decomposition Decomposing value added growth into explanatory factors.\n    A cost constrained value added function is defined to specify the \n    production frontier. Industry estimates can also be aggregated using \n    a weighted average approach.\n    Details about the methodology and data can be found in Diewert and Fox (2018)\n    <doi:10.1093/oxfordhb/9780190226718.013.19>\n    and Zeng, Parsons, Diewert and Fox (2018)\n    <https://www.business.unsw.edu.au/research-site/centreforappliedeconomicresearch-site/Documents/emg2018-6_SZeng_EMG-Slides.pdf>.  "
  },
  {
    "id": 11109,
    "package_name": "dhsage",
    "title": "Reproductive Age Female Data of Various Demographic Health\nSurveys",
    "description": "We provide 70 data sets of females of reproductive age from 19 Asian countries, ranging in age from 15 to 49. The data sets are extracted from demographic and health surveys that were conducted over an extended period of time. Moreover, the functions also provide Whipple\u2019s index as well as age reporting quality such as very rough, rough, approximate, accurate, and highly accurate.",
    "version": "0.1.0",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Jamal Abdul Nasir [aut],\n  Andleeb Rani [aut],\n  Muhammad Imran [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dhsage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dhsage Reproductive Age Female Data of Various Demographic Health\nSurveys We provide 70 data sets of females of reproductive age from 19 Asian countries, ranging in age from 15 to 49. The data sets are extracted from demographic and health surveys that were conducted over an extended period of time. Moreover, the functions also provide Whipple\u2019s index as well as age reporting quality such as very rough, rough, approximate, accurate, and highly accurate.  "
  },
  {
    "id": 11119,
    "package_name": "diario",
    "title": "'R' Interface to the 'Diariodeobras' Application",
    "description": "Provides a set of functions for securely storing 'API' tokens and interacting with the <https://diariodeobras.net> system. Includes convenient wrappers around the 'httr2' package to perform authenticated requests, retrieve project details, tasks, reports, and more. ",
    "version": "0.1.0",
    "maintainer": "Andre Leite <leite@castlab.org>",
    "author": "Andre Leite [aut, cre],\n  Felipe Ferreira [aut],\n  Hugo Vaconcelos [aut],\n  Diogo Bezerra [aut],\n  Roger Azevedo [aut]",
    "url": "<https://github.com/StrategicProjects/diario>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=diario",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diario 'R' Interface to the 'Diariodeobras' Application Provides a set of functions for securely storing 'API' tokens and interacting with the <https://diariodeobras.net> system. Includes convenient wrappers around the 'httr2' package to perform authenticated requests, retrieve project details, tasks, reports, and more.   "
  },
  {
    "id": 11170,
    "package_name": "dipw",
    "title": "Debiased Inverse Propensity Score Weighting",
    "description": "Estimation of the average treatment effect when controlling for\n    high-dimensional confounders using debiased inverse propensity score\n    weighting (DIPW). DIPW relies on the propensity score following a sparse\n    logistic regression model, but the regression curves are not required to be\n    estimable. Despite this, our package also allows the users to estimate \n    the regression curves and take the estimated curves as input to our\n    methods. Details of the methodology can be found in Yuhao Wang and\n    Rajen D. Shah (2020) \"Debiased Inverse Propensity Score Weighting for\n    Estimation of Average Treatment Effects with High-Dimensional Confounders\"\n    <arXiv:2011.08661>. The package relies on the optimisation\n    software 'MOSEK' <https://www.mosek.com/> which must be installed separately;\n    see the documentation for 'Rmosek'. ",
    "version": "0.1.0",
    "maintainer": "Yuhao Wang <yuhaow.thu@gmail.com>",
    "author": "Yuhao Wang [cre, aut],\n  Rajen Shah [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dipw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dipw Debiased Inverse Propensity Score Weighting Estimation of the average treatment effect when controlling for\n    high-dimensional confounders using debiased inverse propensity score\n    weighting (DIPW). DIPW relies on the propensity score following a sparse\n    logistic regression model, but the regression curves are not required to be\n    estimable. Despite this, our package also allows the users to estimate \n    the regression curves and take the estimated curves as input to our\n    methods. Details of the methodology can be found in Yuhao Wang and\n    Rajen D. Shah (2020) \"Debiased Inverse Propensity Score Weighting for\n    Estimation of Average Treatment Effects with High-Dimensional Confounders\"\n    <arXiv:2011.08661>. The package relies on the optimisation\n    software 'MOSEK' <https://www.mosek.com/> which must be installed separately;\n    see the documentation for 'Rmosek'.   "
  },
  {
    "id": 11195,
    "package_name": "discoverableresearch",
    "title": "Checks Title, Abstract and Keywords to Optimise Discoverability",
    "description": "A suite of tools are provided here to support authors in making their research more discoverable. \n    check_keywords() - this function checks the keywords to assess whether they are already represented in the \n    title and abstract. check_fields() - this function compares terminology used across the title, abstract \n    and keywords to assess where terminological diversity (i.e. the use of synonyms) could increase the likelihood \n    of the record being identified in a search. The function looks for terms in the title and abstract that also \n    exist in other fields and highlights these as needing attention. suggest_keywords() - this function takes a \n    full text document and produces a list of unigrams, bigrams and trigrams (1-, 2- or 2-word phrases) \n    present in the full text after removing stop words (words with a low utility in natural language processing) \n    that do not occur in the title or abstract that may be suitable candidates for keywords. suggest_title() - \n    this function takes a full text document and produces a list of the most frequently used unigrams, bigrams \n    and trigrams after removing stop words that do not occur in the abstract or keywords that may be suitable \n    candidates for title words. check_title() - this function carries out a number of sub tasks:  1) it compares \n    the length (number of words) of the title with the mean length of titles in major bibliographic databases to \n    assess whether the title is likely to be too short; 2) it assesses the proportion of stop words in the title \n    to highlight titles with low utility in search engines that strip out stop words; 3) it compares the title \n    with a given sample of record titles from an .ris import and calculates a similarity score based on phrase \n    overlap. This highlights the level of uniqueness of the title. This version of the package also contains \n    functions currently in a non-CRAN package called 'litsearchr' <https://github.com/elizagrames/litsearchr>.",
    "version": "0.0.1",
    "maintainer": "Neal Haddaway <nealhaddaway@gmail.com>",
    "author": "Neal Haddaway [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3902-2234>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=discoverableresearch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "discoverableresearch Checks Title, Abstract and Keywords to Optimise Discoverability A suite of tools are provided here to support authors in making their research more discoverable. \n    check_keywords() - this function checks the keywords to assess whether they are already represented in the \n    title and abstract. check_fields() - this function compares terminology used across the title, abstract \n    and keywords to assess where terminological diversity (i.e. the use of synonyms) could increase the likelihood \n    of the record being identified in a search. The function looks for terms in the title and abstract that also \n    exist in other fields and highlights these as needing attention. suggest_keywords() - this function takes a \n    full text document and produces a list of unigrams, bigrams and trigrams (1-, 2- or 2-word phrases) \n    present in the full text after removing stop words (words with a low utility in natural language processing) \n    that do not occur in the title or abstract that may be suitable candidates for keywords. suggest_title() - \n    this function takes a full text document and produces a list of the most frequently used unigrams, bigrams \n    and trigrams after removing stop words that do not occur in the abstract or keywords that may be suitable \n    candidates for title words. check_title() - this function carries out a number of sub tasks:  1) it compares \n    the length (number of words) of the title with the mean length of titles in major bibliographic databases to \n    assess whether the title is likely to be too short; 2) it assesses the proportion of stop words in the title \n    to highlight titles with low utility in search engines that strip out stop words; 3) it compares the title \n    with a given sample of record titles from an .ris import and calculates a similarity score based on phrase \n    overlap. This highlights the level of uniqueness of the title. This version of the package also contains \n    functions currently in a non-CRAN package called 'litsearchr' <https://github.com/elizagrames/litsearchr>.  "
  },
  {
    "id": 11206,
    "package_name": "disparityfilter",
    "title": "Disparity Filter Algorithm for Weighted Networks",
    "description": "The disparity filter algorithm is a network reduction technique to\n    identify the 'backbone' structure of a weighted network without destroying\n    its multi-scale nature. The algorithm is documented in M. Angeles Serrano,\n    Marian Boguna and Alessandro Vespignani in \"Extracting the multiscale\n    backbone of complex weighted networks\", Proceedings of the National Academy\n    of Sciences 106 (16), 2009. This implementation of the algorithm supports\n    both directed and undirected networks.",
    "version": "2.2.3",
    "maintainer": "Alessandro Bessi <alessandro.bessi@iusspavia.it>",
    "author": "Alessandro Bessi [aut, cre],\n  Francois Briatte [aut]",
    "url": "https://github.com/alessandrobessi/disparityfilter",
    "bug_reports": "https://github.com/alessandrobessi/disparityfilter/issues",
    "repository": "https://cran.r-project.org/package=disparityfilter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "disparityfilter Disparity Filter Algorithm for Weighted Networks The disparity filter algorithm is a network reduction technique to\n    identify the 'backbone' structure of a weighted network without destroying\n    its multi-scale nature. The algorithm is documented in M. Angeles Serrano,\n    Marian Boguna and Alessandro Vespignani in \"Extracting the multiscale\n    backbone of complex weighted networks\", Proceedings of the National Academy\n    of Sciences 106 (16), 2009. This implementation of the algorithm supports\n    both directed and undirected networks.  "
  },
  {
    "id": 11235,
    "package_name": "distributions3",
    "title": "Probability Distributions as S3 Objects",
    "description": "Tools to create and manipulate probability distributions\n    using S3.  Generics pdf(), cdf(), quantile(), and random() provide\n    replacements for base R's d/p/q/r style functions.  Functions and\n    arguments have been named carefully to minimize confusion for students\n    in intro stats courses. The documentation for each distribution\n    contains detailed mathematical notes.",
    "version": "0.2.3",
    "maintainer": "Alex Hayes <alexpghayes@gmail.com>",
    "author": "Alex Hayes [aut, cre] (ORCID: <https://orcid.org/0000-0002-4985-5160>),\n  Ralph Moller-Trane [aut],\n  Emil Hvitfeldt [ctb] (ORCID: <https://orcid.org/0000-0002-0679-1945>),\n  Daniel Jordan [aut],\n  Paul Northrop [aut],\n  Moritz N. Lang [aut] (ORCID: <https://orcid.org/0000-0002-2533-9903>),\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>),\n  Bruna Wundervald [ctb],\n  Alessandro Gasparini [ctb]",
    "url": "https://github.com/alexpghayes/distributions3,\nhttps://alexpghayes.github.io/distributions3/",
    "bug_reports": "https://github.com/alexpghayes/distributions3/issues",
    "repository": "https://cran.r-project.org/package=distributions3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "distributions3 Probability Distributions as S3 Objects Tools to create and manipulate probability distributions\n    using S3.  Generics pdf(), cdf(), quantile(), and random() provide\n    replacements for base R's d/p/q/r style functions.  Functions and\n    arguments have been named carefully to minimize confusion for students\n    in intro stats courses. The documentation for each distribution\n    contains detailed mathematical notes.  "
  },
  {
    "id": 11242,
    "package_name": "div",
    "title": "Report on Diversity and Inclusion in a Corporate Setting",
    "description": "Facilitate the analysis of teams in a corporate setting:\n    assess the diversity per grade and job, present the results,\n    search for bias (in hiring and/or promoting processes).\n    It also provides methods to simulate the effect of bias, random team-data, etc.\n    White paper: 'Philippe J.S. De Brouwer' (2021) <http://www.de-brouwer.com/assets/div/div-white-paper.pdf>.\n    Book (chapter 36): 'Philippe J.S. De Brouwer' (2020, ISBN:978-1-119-63272-6) and 'Philippe J.S. De Brouwer' (2020) <doi:10.1002/9781119632757>.",
    "version": "0.3.1",
    "maintainer": "Philippe J.S. De Brouwer <philippe@de-brouwer.com>",
    "author": "Philippe J.S. De Brouwer [aut, cre]",
    "url": "http://www.de-brouwer.com/div/",
    "bug_reports": "https://github.com/DrPhilippeDB/div/issues/",
    "repository": "https://cran.r-project.org/package=div",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "div Report on Diversity and Inclusion in a Corporate Setting Facilitate the analysis of teams in a corporate setting:\n    assess the diversity per grade and job, present the results,\n    search for bias (in hiring and/or promoting processes).\n    It also provides methods to simulate the effect of bias, random team-data, etc.\n    White paper: 'Philippe J.S. De Brouwer' (2021) <http://www.de-brouwer.com/assets/div/div-white-paper.pdf>.\n    Book (chapter 36): 'Philippe J.S. De Brouwer' (2020, ISBN:978-1-119-63272-6) and 'Philippe J.S. De Brouwer' (2020) <doi:10.1002/9781119632757>.  "
  },
  {
    "id": 11267,
    "package_name": "dlookr",
    "title": "Tools for Data Diagnosis, Exploration, Transformation",
    "description": "A collection of tools that support data diagnosis, exploration, and transformation. \n    Data diagnostics provides information and visualization of missing values, outliers, and unique \n    and negative values to help you understand the distribution and quality of your data. \n    Data exploration provides information and visualization of the descriptive statistics of \n    univariate variables, normality tests and outliers, correlation of two variables, \n    and the relationship between the target variable and predictor. Data transformation supports binning \n    for categorizing continuous variables, imputes missing values and outliers, and resolves skewness. \n    And it creates automated reports that support these three tasks.",
    "version": "0.6.5",
    "maintainer": "Choonghyun Ryu <choonghyun.ryu@gmail.com>",
    "author": "Choonghyun Ryu [aut, cre]",
    "url": "https://github.com/choonghyunryu/dlookr/,\nhttps://choonghyunryu.github.io/dlookr/",
    "bug_reports": "https://github.com/choonghyunryu/dlookr/issues",
    "repository": "https://cran.r-project.org/package=dlookr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dlookr Tools for Data Diagnosis, Exploration, Transformation A collection of tools that support data diagnosis, exploration, and transformation. \n    Data diagnostics provides information and visualization of missing values, outliers, and unique \n    and negative values to help you understand the distribution and quality of your data. \n    Data exploration provides information and visualization of the descriptive statistics of \n    univariate variables, normality tests and outliers, correlation of two variables, \n    and the relationship between the target variable and predictor. Data transformation supports binning \n    for categorizing continuous variables, imputes missing values and outliers, and resolves skewness. \n    And it creates automated reports that support these three tasks.  "
  },
  {
    "id": 11278,
    "package_name": "dmm",
    "title": "Dyadic Mixed Model for Pedigree Data",
    "description": "Mixed model analysis for quantitative genetics\n  with multi-trait responses and pedigree-based partitioning \n  of individual variation into a range of environmental and \n  genetic variance components for individual and \n  maternal effects. Method documented in dmmOverview.pdf; dmm is an \n  implementation of dispersion mean model described by\n  Searle et al. (1992) \"Variance Components\", Wiley, NY.\n  Dmm() can do 'MINQUE', 'bias-corrected-ML', and 'REML'  variance and \n  covariance component estimates.",
    "version": "3.2-2",
    "maintainer": "Neville Jackson <nanddjackson@bigpond.com>",
    "author": "Neville Jackson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1820-9609>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dmm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dmm Dyadic Mixed Model for Pedigree Data Mixed model analysis for quantitative genetics\n  with multi-trait responses and pedigree-based partitioning \n  of individual variation into a range of environmental and \n  genetic variance components for individual and \n  maternal effects. Method documented in dmmOverview.pdf; dmm is an \n  implementation of dispersion mean model described by\n  Searle et al. (1992) \"Variance Components\", Wiley, NY.\n  Dmm() can do 'MINQUE', 'bias-corrected-ML', and 'REML'  variance and \n  covariance component estimates.  "
  },
  {
    "id": 11301,
    "package_name": "doc2vec",
    "title": "Distributed Representations of Sentences, Documents and Topics",
    "description": "Learn vector representations of sentences, paragraphs or documents by using the 'Paragraph Vector' algorithms,\n    namely the distributed bag of words ('PV-DBOW') and the distributed memory ('PV-DM') model. \n    The techniques in the package are detailed in the paper \"Distributed Representations of Sentences and Documents\" by Mikolov et al. (2014), available at <doi:10.48550/arXiv.1405.4053>.\n    The package also provides an implementation to cluster documents based on these embedding using a technique called top2vec. \n    Top2vec finds clusters in text documents by combining techniques to embed documents and words and density-based clustering.\n    It does this by embedding documents in the semantic space as defined by the 'doc2vec' algorithm. Next it maps\n    these document embeddings to a lower-dimensional space using the 'Uniform Manifold Approximation and Projection' (UMAP) clustering algorithm \n    and finds dense areas in that space using a 'Hierarchical Density-Based Clustering' technique (HDBSCAN). These dense\n    areas are the topic clusters which can be represented by the corresponding topic vector which is an aggregate of the \n    document embeddings of the documents which are part of that topic cluster. In the same semantic space similar words can \n    be found which are representative of the topic.\n    More details can be found in the paper 'Top2Vec: Distributed Representations of Topics' by D. Angelov available at <doi:10.48550/arXiv.2008.09470>. ",
    "version": "0.2.2",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  hiyijian [ctb, cph] (Code in src/doc2vec)",
    "url": "https://github.com/bnosac/doc2vec",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=doc2vec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doc2vec Distributed Representations of Sentences, Documents and Topics Learn vector representations of sentences, paragraphs or documents by using the 'Paragraph Vector' algorithms,\n    namely the distributed bag of words ('PV-DBOW') and the distributed memory ('PV-DM') model. \n    The techniques in the package are detailed in the paper \"Distributed Representations of Sentences and Documents\" by Mikolov et al. (2014), available at <doi:10.48550/arXiv.1405.4053>.\n    The package also provides an implementation to cluster documents based on these embedding using a technique called top2vec. \n    Top2vec finds clusters in text documents by combining techniques to embed documents and words and density-based clustering.\n    It does this by embedding documents in the semantic space as defined by the 'doc2vec' algorithm. Next it maps\n    these document embeddings to a lower-dimensional space using the 'Uniform Manifold Approximation and Projection' (UMAP) clustering algorithm \n    and finds dense areas in that space using a 'Hierarchical Density-Based Clustering' technique (HDBSCAN). These dense\n    areas are the topic clusters which can be represented by the corresponding topic vector which is an aggregate of the \n    document embeddings of the documents which are part of that topic cluster. In the same semantic space similar words can \n    be found which are representative of the topic.\n    More details can be found in the paper 'Top2Vec: Distributed Representations of Topics' by D. Angelov available at <doi:10.48550/arXiv.2008.09470>.   "
  },
  {
    "id": 11302,
    "package_name": "dockViewR",
    "title": "Layout Manager Widget for R and 'shiny' Apps",
    "description": "Provides R bindings to the 'dockview' 'JavaScript' library <https://dockview.dev/>.\n    Create fully customizable grid layouts (docks) in seconds to include in interactive R reports with R Markdown or 'Quarto' or \n    in 'shiny' apps <https://shiny.posit.co/>. In 'shiny' mode, modify docks by dynamically adding, removing or moving panels or \n    groups of panels from the server function. Choose among 8 stunning themes (dark and light),\n    serialise the state of a dock to restore it later.",
    "version": "0.3.0",
    "maintainer": "David Granjon <dgranjon@ymail.com>",
    "author": "David Granjon [aut, cre],\n  Nelson Stevens [aut],\n  Nicolas Bennett [aut],\n  mathuo [cph],\n  cynkra GmbH [fnd]",
    "url": "https://github.com/cynkra/dockViewR,\nhttps://cynkra.github.io/dockViewR/",
    "bug_reports": "https://github.com/cynkra/dockViewR/issues",
    "repository": "https://cran.r-project.org/package=dockViewR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dockViewR Layout Manager Widget for R and 'shiny' Apps Provides R bindings to the 'dockview' 'JavaScript' library <https://dockview.dev/>.\n    Create fully customizable grid layouts (docks) in seconds to include in interactive R reports with R Markdown or 'Quarto' or \n    in 'shiny' apps <https://shiny.posit.co/>. In 'shiny' mode, modify docks by dynamically adding, removing or moving panels or \n    groups of panels from the server function. Choose among 8 stunning themes (dark and light),\n    serialise the state of a dock to restore it later.  "
  },
  {
    "id": 11304,
    "package_name": "docket",
    "title": "Insert R Data into 'Word' Documents",
    "description": "Populate data from an R environment into '.doc' and '.docx' templates. Create a template document in a program such as 'Word', and add strings encased in guillemet characters to create flags (\u00abexample\u00bb). Use getDictionary() to create a dictionary of flags and replacement values, then call docket() to generate a populated document.",
    "version": "1.33",
    "maintainer": "Jonathan Conrad <JonathanGConrad@gmail.com>",
    "author": "Jonathan Conrad [aut, cre],\n  Ian Conrad [ctb]",
    "url": "https://github.com/JonathanConrad98/docket",
    "bug_reports": "https://github.com/JonathanConrad98/docket/issues",
    "repository": "https://cran.r-project.org/package=docket",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "docket Insert R Data into 'Word' Documents Populate data from an R environment into '.doc' and '.docx' templates. Create a template document in a program such as 'Word', and add strings encased in guillemet characters to create flags (\u00abexample\u00bb). Use getDictionary() to create a dictionary of flags and replacement values, then call docket() to generate a populated document.  "
  },
  {
    "id": 11305,
    "package_name": "docknitr",
    "title": "Use Docker Images to Process Rmarkdown Blocks",
    "description": "Gives you the ability to use arbitrary Docker images\n    (including custom ones) to process Rmarkdown code chunks.",
    "version": "1.0.1",
    "maintainer": "Ben Artin <ben@artins.org>",
    "author": "Ben Artin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=docknitr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "docknitr Use Docker Images to Process Rmarkdown Blocks Gives you the ability to use arbitrary Docker images\n    (including custom ones) to process Rmarkdown code chunks.  "
  },
  {
    "id": 11306,
    "package_name": "doconv",
    "title": "Document Conversion to 'PDF' or 'PNG'",
    "description": "It provides the ability to generate images from documents\n of different types. Three main features are provided: functions for\n generating document thumbnails, functions for performing visual tests\n of documents and a function for updating fields and table of contents\n of a 'Microsoft Word' or 'RTF' document. In order to work, 'LibreOffice' must be\n installed on the machine and or 'Microsoft Word'. If the latter is\n available, it can be used to produce PDF documents or images\n identical to the originals; otherwise, 'LibreOffice' is used and\n the rendering can be sometimes different from the original\n documents.",
    "version": "0.3.3",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  ArData [cph],\n  David Hajage [ctb] (initial powershell code)",
    "url": "",
    "bug_reports": "https://github.com/ardata-fr/doconv/issues",
    "repository": "https://cran.r-project.org/package=doconv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doconv Document Conversion to 'PDF' or 'PNG' It provides the ability to generate images from documents\n of different types. Three main features are provided: functions for\n generating document thumbnails, functions for performing visual tests\n of documents and a function for updating fields and table of contents\n of a 'Microsoft Word' or 'RTF' document. In order to work, 'LibreOffice' must be\n installed on the machine and or 'Microsoft Word'. If the latter is\n available, it can be used to produce PDF documents or images\n identical to the originals; otherwise, 'LibreOffice' is used and\n the rendering can be sometimes different from the original\n documents.  "
  },
  {
    "id": 11311,
    "package_name": "docstring",
    "title": "Provides Docstring Capabilities to R Functions",
    "description": "Provides the ability to display something analogous to\n    Python's docstrings within R.  By allowing the user to document\n    their functions as comments at the beginning of their function\n    without requiring putting the function into a package we allow\n    more users to easily provide documentation for their functions.\n    The documentation can be viewed just like any other help files\n    for functions provided by packages as well.",
    "version": "1.0.0",
    "maintainer": "Dason Kurkiewicz <dasonk@gmail.com>",
    "author": "Dason Kurkiewicz [aut, cre],\n  Neal Fultz [ctb]",
    "url": "https://github.com/dasonk/docstring",
    "bug_reports": "https://github.com/dasonk/docstring/issues?state=open",
    "repository": "https://cran.r-project.org/package=docstring",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "docstring Provides Docstring Capabilities to R Functions Provides the ability to display something analogous to\n    Python's docstrings within R.  By allowing the user to document\n    their functions as comments at the beginning of their function\n    without requiring putting the function into a package we allow\n    more users to easily provide documentation for their functions.\n    The documentation can be viewed just like any other help files\n    for functions provided by packages as well.  "
  },
  {
    "id": 11313,
    "package_name": "docuSignr",
    "title": "Connect to 'DocuSign' API",
    "description": "Connect to the 'DocuSign' Rest API <https://www.docusign.com/p/RESTAPIGuide/RESTAPIGuide.htm>, \n  which supports embedded signing, and sending of documents. ",
    "version": "0.0.3",
    "maintainer": "Carl Ganz <carl@cannadatasolutions.com>",
    "author": "Carl Ganz [aut, cre],\n  CannaData Solutions [cph]",
    "url": "https://github.com/CannaData/docuSignr",
    "bug_reports": "https://github.com/CannaData/docuSignr/issues",
    "repository": "https://cran.r-project.org/package=docuSignr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "docuSignr Connect to 'DocuSign' API Connect to the 'DocuSign' Rest API <https://www.docusign.com/p/RESTAPIGuide/RESTAPIGuide.htm>, \n  which supports embedded signing, and sending of documents.   "
  },
  {
    "id": 11314,
    "package_name": "document",
    "title": "Run 'roxygen2' on (Chunks of) Single Code Files",
    "description": "Have you ever been tempted to create 'roxygen2'-style documentation\n    comments for one of your functions that was not part of one of your\n    packages (yet)?\n    This is exactly what this package is about: running 'roxygen2' on\n    (chunks of) a single code file.",
    "version": "4.0.1",
    "maintainer": "Andreas Dominik Cullmann <fvafrcu@mailbox.org>",
    "author": "Andreas Dominik Cullmann [aut, cre]",
    "url": "https://gitlab.com/fvafrcu/document",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=document",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "document Run 'roxygen2' on (Chunks of) Single Code Files Have you ever been tempted to create 'roxygen2'-style documentation\n    comments for one of your functions that was not part of one of your\n    packages (yet)?\n    This is exactly what this package is about: running 'roxygen2' on\n    (chunks of) a single code file.  "
  },
  {
    "id": 11315,
    "package_name": "documenter",
    "title": "Documents Files",
    "description": "It is sometimes necessary to create documentation for all files in a directory. Doing so by hand can be very tedious. This task is made fast and reproducible using the functionality of 'documenter'. It aggregates all text files in a directory and its subdirectories into a single word document in a semi-automated fashion.",
    "version": "0.1.3",
    "maintainer": "Zachary Colburn <zcolburn@gmail.com>",
    "author": "Zachary Colburn [aut, cre],\n  Madigan Army Medical Center - Department of Clinical Investigation\n    [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=documenter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "documenter Documents Files It is sometimes necessary to create documentation for all files in a directory. Doing so by hand can be very tedious. This task is made fast and reproducible using the functionality of 'documenter'. It aggregates all text files in a directory and its subdirectories into a single word document in a semi-automated fashion.  "
  },
  {
    "id": 11316,
    "package_name": "docxtractr",
    "title": "Extract Data Tables and Comments from 'Microsoft' 'Word'\nDocuments",
    "description": "'Microsoft Word' 'docx' files provide an 'XML' structure that is fairly\n    straightforward to navigate, especially when it applies to 'Word' tables and\n    comments. Tools are provided to determine table count/structure, comment count\n    and also to extract/clean tables and comments from 'Microsoft Word' 'docx' documents.\n    There is also nascent support for '.doc' and '.pptx' files.",
    "version": "0.6.5",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis [aut, cre] (ORCID: <https://orcid.org/0000-0001-5670-2640>),\n  Mark Dulhunty [ctb],\n  Karlo Guidoni-Martins [ctb],\n  Chris Muir [aut, ctb],\n  John Muschelli [ctb]",
    "url": "http://gitlab.com/hrbrmstr/docxtractr",
    "bug_reports": "https://gitlab.com/hrbrmstr/docxtractr/issues",
    "repository": "https://cran.r-project.org/package=docxtractr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "docxtractr Extract Data Tables and Comments from 'Microsoft' 'Word'\nDocuments 'Microsoft Word' 'docx' files provide an 'XML' structure that is fairly\n    straightforward to navigate, especially when it applies to 'Word' tables and\n    comments. Tools are provided to determine table count/structure, comment count\n    and also to extract/clean tables and comments from 'Microsoft Word' 'docx' documents.\n    There is also nascent support for '.doc' and '.pptx' files.  "
  },
  {
    "id": 11327,
    "package_name": "door",
    "title": "Analysis of Clinical Trials with the Desirability of Outcome\nRanking Methodology",
    "description": "Statistical methods and related graphical representations for the Desirability of Outcome Ranking (DOOR) methodology. The DOOR is a paradigm for the design, analysis, interpretation of clinical trials and other research studies based on the patient centric benefit risk evaluation. The package provides functions for generating summary statistics from individual level/summary level datasets, conduct DOOR probability-based inference, and visualization of the results. For more details of DOOR methodology, see Hamasaki and Evans (2025) <doi:10.1201/9781003390855>. For more explanation of the statistical methods and the graphics, see the technical document and user manual of the DOOR 'Shiny' apps at <https://methods.bsc.gwu.edu>. ",
    "version": "0.0.3",
    "maintainer": "Yijie He <yih148@gwu.edu>",
    "author": "Yijie He [aut, cre],\n  Qihang Wu [ctb],\n  Toshimitsu Hamasaki [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=door",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "door Analysis of Clinical Trials with the Desirability of Outcome\nRanking Methodology Statistical methods and related graphical representations for the Desirability of Outcome Ranking (DOOR) methodology. The DOOR is a paradigm for the design, analysis, interpretation of clinical trials and other research studies based on the patient centric benefit risk evaluation. The package provides functions for generating summary statistics from individual level/summary level datasets, conduct DOOR probability-based inference, and visualization of the results. For more details of DOOR methodology, see Hamasaki and Evans (2025) <doi:10.1201/9781003390855>. For more explanation of the statistical methods and the graphics, see the technical document and user manual of the DOOR 'Shiny' apps at <https://methods.bsc.gwu.edu>.   "
  },
  {
    "id": 11349,
    "package_name": "downloadthis",
    "title": "Implement Download Buttons in 'rmarkdown'",
    "description": "Implement download buttons in HTML output from 'rmarkdown' without the need for 'runtime:shiny'.",
    "version": "0.5.0",
    "maintainer": "Felipe Mattioni Maturana <felipe.mattioni@med.uni-tuebingen.de>",
    "author": "Felipe Mattioni Maturana [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4221-6104>),\n  John Coene [ctb]",
    "url": "https://github.com/fmmattioni/downloadthis",
    "bug_reports": "https://github.com/fmmattioni/downloadthis/issues",
    "repository": "https://cran.r-project.org/package=downloadthis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "downloadthis Implement Download Buttons in 'rmarkdown' Implement download buttons in HTML output from 'rmarkdown' without the need for 'runtime:shiny'.  "
  },
  {
    "id": 11351,
    "package_name": "dowser",
    "title": "B Cell Receptor Phylogenetics Toolkit",
    "description": "Provides a set of functions for inferring, visualizing, and analyzing B cell phylogenetic trees.\n    Provides methods to 1) reconstruct unmutated ancestral sequences,\n    2) build B cell phylogenetic trees using multiple methods,\n    3) visualize trees with metadata at the tips,\n    4) reconstruct intermediate sequences,\n    5) detect biased ancestor-descendant relationships among metadata types\n    Workflow examples available at documentation site (see URL).\n    Citations:\n    Hoehn et al (2022) <doi:10.1371/journal.pcbi.1009885>,\n    Hoehn et al (2021) <doi:10.1101/2021.01.06.425648>.",
    "version": "2.4.0",
    "maintainer": "Kenneth Hoehn <kenneth.b.hoehn@dartmouth.edu>",
    "author": "Kenneth Hoehn [aut, cre],\n  Cole Jensen [aut],\n  Jessie Fielding [aut],\n  Hunter Melton [aut],\n  Susanna Marquez [ctb],\n  Jason Vander Heiden [ctb],\n  Steven Kleinstein [aut, cph]",
    "url": "https://dowser.readthedocs.io",
    "bug_reports": "https://github.com/immcantation/dowser/issues",
    "repository": "https://cran.r-project.org/package=dowser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dowser B Cell Receptor Phylogenetics Toolkit Provides a set of functions for inferring, visualizing, and analyzing B cell phylogenetic trees.\n    Provides methods to 1) reconstruct unmutated ancestral sequences,\n    2) build B cell phylogenetic trees using multiple methods,\n    3) visualize trees with metadata at the tips,\n    4) reconstruct intermediate sequences,\n    5) detect biased ancestor-descendant relationships among metadata types\n    Workflow examples available at documentation site (see URL).\n    Citations:\n    Hoehn et al (2022) <doi:10.1371/journal.pcbi.1009885>,\n    Hoehn et al (2021) <doi:10.1101/2021.01.06.425648>.  "
  },
  {
    "id": 11352,
    "package_name": "dparser",
    "title": "Port of 'Dparser' Package",
    "description": "A Scannerless GLR parser/parser generator.  Note that GLR standing for \"generalized LR\", where L stands for \"left-to-right\" and\n   R stands for \"rightmost (derivation)\".  For more information see <https://en.wikipedia.org/wiki/GLR_parser>. This parser is based on the Tomita\n   (1987) algorithm. (Paper can be found at <https://aclanthology.org/P84-1073.pdf>).\n   The original 'dparser' package documentation can be found at <https://dparser.sourceforge.net/>.  This allows you to add mini-languages to R (like\n   rxode2's ODE mini-language Wang, Hallow, and James 2015 <DOI:10.1002/psp4.12052>) or to parse other languages like 'NONMEM' to automatically translate\n   them to R code.   To use this in your code, add a LinkingTo dparser in your DESCRIPTION file and instead of using #include <dparse.h> use\n   #include <dparser.h>.  This also provides a R-based port of the make_dparser <https://dparser.sourceforge.net/d/make_dparser.cat> command called\n   mkdparser().  Additionally you can parse an arbitrary grammar within R using the dparse() function, which works on most OSes and is mainly for grammar\n   testing.  The fastest parsing, of course, occurs at the C level, and is suggested.",
    "version": "1.3.1-13",
    "maintainer": "Matthew Fidler <matthew.fidler@gmail.com>",
    "author": "Matthew Fidler [aut, cre],\n  John Plevyak [aut, cph]",
    "url": "https://nlmixr2.github.io/dparser-R/,\nhttps://github.com/nlmixr2/dparser-R/",
    "bug_reports": "https://github.com/nlmixr2/dparser-R/issues/",
    "repository": "https://cran.r-project.org/package=dparser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dparser Port of 'Dparser' Package A Scannerless GLR parser/parser generator.  Note that GLR standing for \"generalized LR\", where L stands for \"left-to-right\" and\n   R stands for \"rightmost (derivation)\".  For more information see <https://en.wikipedia.org/wiki/GLR_parser>. This parser is based on the Tomita\n   (1987) algorithm. (Paper can be found at <https://aclanthology.org/P84-1073.pdf>).\n   The original 'dparser' package documentation can be found at <https://dparser.sourceforge.net/>.  This allows you to add mini-languages to R (like\n   rxode2's ODE mini-language Wang, Hallow, and James 2015 <DOI:10.1002/psp4.12052>) or to parse other languages like 'NONMEM' to automatically translate\n   them to R code.   To use this in your code, add a LinkingTo dparser in your DESCRIPTION file and instead of using #include <dparse.h> use\n   #include <dparser.h>.  This also provides a R-based port of the make_dparser <https://dparser.sourceforge.net/d/make_dparser.cat> command called\n   mkdparser().  Additionally you can parse an arbitrary grammar within R using the dparse() function, which works on most OSes and is mainly for grammar\n   testing.  The fastest parsing, of course, occurs at the C level, and is suggested.  "
  },
  {
    "id": 11364,
    "package_name": "dr",
    "title": "Methods for Dimension Reduction for Regression",
    "description": "Functions, methods, and datasets for fitting dimension\n reduction regression, using slicing (methods SAVE and SIR), Principal\n Hessian Directions (phd, using residuals and the response), and an\n iterative IRE.  Partial methods, that condition on categorical\n predictors are also available.  A variety of tests, and stepwise\n deletion of predictors, is also included.  Also included is\n code for computing permutation tests of dimension.  Adding additional\n methods of estimating dimension is straightforward.\n For documentation, see the vignette in the package.   With version 3.0.4,\n the arguments for dr.step have been modified.",
    "version": "3.0.11",
    "maintainer": "\"Sanford Weisberg,\" <sandy@umn.edu>",
    "author": "Sanford Weisberg, [aut, cre]",
    "url": "https://CRAN.R-project.org/package=dr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dr Methods for Dimension Reduction for Regression Functions, methods, and datasets for fitting dimension\n reduction regression, using slicing (methods SAVE and SIR), Principal\n Hessian Directions (phd, using residuals and the response), and an\n iterative IRE.  Partial methods, that condition on categorical\n predictors are also available.  A variety of tests, and stepwise\n deletion of predictors, is also included.  Also included is\n code for computing permutation tests of dimension.  Adding additional\n methods of estimating dimension is straightforward.\n For documentation, see the vignette in the package.   With version 3.0.4,\n the arguments for dr.step have been modified.  "
  },
  {
    "id": 11373,
    "package_name": "drawer",
    "title": "An Interactive HTML Image Editing Tool",
    "description": "An interactive image editing tool that can be added as part of the HTML in Shiny,\n    R markdown or any type of HTML document. Often times, plots, photos are embedded\n    in the web application/file. 'drawer' can take screenshots of these image-like elements, or \n    any part of the HTML document and send to an image editing space called 'canvas' to allow users \n    immediately edit the screenshot(s) within the same document. Users can quickly \n    combine, compare different screenshots, upload their own images \n    and maybe make a scientific figure. ",
    "version": "0.2.0.1",
    "maintainer": "Le Zhang <lezhang100@gmail.com>",
    "author": "Le Zhang [aut, cre]",
    "url": "https://github.com/lz100/drawer",
    "bug_reports": "https://github.com/lz100/drawer/issues",
    "repository": "https://cran.r-project.org/package=drawer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "drawer An Interactive HTML Image Editing Tool An interactive image editing tool that can be added as part of the HTML in Shiny,\n    R markdown or any type of HTML document. Often times, plots, photos are embedded\n    in the web application/file. 'drawer' can take screenshots of these image-like elements, or \n    any part of the HTML document and send to an image editing space called 'canvas' to allow users \n    immediately edit the screenshot(s) within the same document. Users can quickly \n    combine, compare different screenshots, upload their own images \n    and maybe make a scientific figure.   "
  },
  {
    "id": 11376,
    "package_name": "drcSeedGerm",
    "title": "Utilities for Data Analyses in Seed Germination/Emergence Assays",
    "description": "Utility functions to be used to analyse datasets obtained from seed germination/emergence assays. Fits several types of seed germination/emergence models, including those reported in Onofri et al. (2018) \"Hydrothermal-time-to-event models for seed germination\", European Journal of Agronomy, 101, 129-139 <doi:10.1016/j.eja.2018.08.011>. Contains several datasets for practicing. ",
    "version": "1.0.1",
    "maintainer": "Andrea Onofri <andrea.onofri@unipg.it>",
    "author": "Andrea Onofri <andrea.onofri@unipg.it>",
    "url": "https://www.statforbiology.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=drcSeedGerm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "drcSeedGerm Utilities for Data Analyses in Seed Germination/Emergence Assays Utility functions to be used to analyse datasets obtained from seed germination/emergence assays. Fits several types of seed germination/emergence models, including those reported in Onofri et al. (2018) \"Hydrothermal-time-to-event models for seed germination\", European Journal of Agronomy, 101, 129-139 <doi:10.1016/j.eja.2018.08.011>. Contains several datasets for practicing.   "
  },
  {
    "id": 11431,
    "package_name": "dtComb",
    "title": "Statistical Combination of Diagnostic Tests",
    "description": "A system for combining two diagnostic tests using various approaches\n              that include statistical and machine-learning-based methodologies. \n              These approaches are divided into four groups: linear combination \n              methods, non-linear combination methods, mathematical operators, \n              and machine learning algorithms. See \n              the <https://biotools.erciyes.edu.tr/dtComb/> website \n              for more information, documentation, and examples.",
    "version": "1.0.7",
    "maintainer": "Gokmen Zararsiz <gokmen.zararsiz@gmail.com>",
    "author": "Serra Ilayda Yerlitas [aut, ctb],\n  Serra Bersan Gengec [aut, ctb],\n  Necla Kochan [aut, ctb],\n  Gozde Erturk Zararsiz [aut, ctb],\n  Selcuk Korkmaz [aut, ctb],\n  Gokmen Zararsiz [aut, ctb, cre]",
    "url": "https://github.com/gokmenzararsiz/dtComb",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dtComb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dtComb Statistical Combination of Diagnostic Tests A system for combining two diagnostic tests using various approaches\n              that include statistical and machine-learning-based methodologies. \n              These approaches are divided into four groups: linear combination \n              methods, non-linear combination methods, mathematical operators, \n              and machine learning algorithms. See \n              the <https://biotools.erciyes.edu.tr/dtComb/> website \n              for more information, documentation, and examples.  "
  },
  {
    "id": 11436,
    "package_name": "dtlg",
    "title": "A Performance-Focused Package for Clinical Trial Tables",
    "description": "Create high-performance clinical reporting tables (TLGs) from\n    ADaM-like inputs. The package provides a consistent, programmatic API\n    to generate common tables such as demographics, adverse event incidence,\n    and laboratory summaries, using 'data.table' for fast aggregation over\n    large populations. Functions support flexible target-variable selection,\n    stratification by treatment, and customizable summary statistics, and\n    return tidy, machine-readable results ready to render with downstream\n    table/formatting packages in analysis pipelines.",
    "version": "0.0.2",
    "maintainer": "Ramiro Magno <ramiro.morgado@ascent.io>",
    "author": "Max Ebenezer-Brown [aut],\n  Max Norman [aut],\n  Xinye Li [aut],\n  Anja Peebles-Brown [aut],\n  Ramiro Magno [aut, cre]",
    "url": "https://AscentSoftware.github.io/dtlg/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dtlg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dtlg A Performance-Focused Package for Clinical Trial Tables Create high-performance clinical reporting tables (TLGs) from\n    ADaM-like inputs. The package provides a consistent, programmatic API\n    to generate common tables such as demographics, adverse event incidence,\n    and laboratory summaries, using 'data.table' for fast aggregation over\n    large populations. Functions support flexible target-variable selection,\n    stratification by treatment, and customizable summary statistics, and\n    return tidy, machine-readable results ready to render with downstream\n    table/formatting packages in analysis pipelines.  "
  },
  {
    "id": 11441,
    "package_name": "dtrackr",
    "title": "Track your Data Pipelines",
    "description": "Track and\n    document 'dplyr' data pipelines. As you filter, mutate, and join your\n    way through a data set, 'dtrackr' seamlessly keeps track of your data\n    flow and makes publication ready documentation of a data pipeline simple.",
    "version": "0.5.0",
    "maintainer": "Robert Challen <rob.challen@bristol.ac.uk>",
    "author": "Robert Challen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5504-7768>)",
    "url": "https://terminological.github.io/dtrackr/index.html,\nhttps://github.com/terminological/dtrackr",
    "bug_reports": "https://github.com/terminological/dtrackr/issues",
    "repository": "https://cran.r-project.org/package=dtrackr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dtrackr Track your Data Pipelines Track and\n    document 'dplyr' data pipelines. As you filter, mutate, and join your\n    way through a data set, 'dtrackr' seamlessly keeps track of your data\n    flow and makes publication ready documentation of a data pipeline simple.  "
  },
  {
    "id": 11461,
    "package_name": "dunn.test",
    "title": "Dunn's Test of Multiple Comparisons Using Rank Sums",
    "description": "Computes Dunn's test (1964) for stochastic dominance and reports the results among multiple pairwise comparisons after a Kruskal-Wallis test for 0th-order stochastic dominance among k groups (Kruskal and Wallis, 1952). 'dunn.test' makes k(k-1)/2 multiple pairwise comparisons based on Dunn's z-test-statistic approximations to the actual rank statistics. The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the rank-sum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn's test may be understood as a test for median difference and for mean difference. 'dunn.test' accounts for tied ranks.",
    "version": "1.3.6",
    "maintainer": "Alexis Dinno <alexis.dinno@pdx.edu>",
    "author": "Alexis Dinno",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dunn.test",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dunn.test Dunn's Test of Multiple Comparisons Using Rank Sums Computes Dunn's test (1964) for stochastic dominance and reports the results among multiple pairwise comparisons after a Kruskal-Wallis test for 0th-order stochastic dominance among k groups (Kruskal and Wallis, 1952). 'dunn.test' makes k(k-1)/2 multiple pairwise comparisons based on Dunn's z-test-statistic approximations to the actual rank statistics. The null hypothesis for each pairwise comparison is that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group equals one half; this null hypothesis corresponds to that of the Wilcoxon-Mann-Whitney rank-sum test. Like the rank-sum test, if the data can be assumed to be continuous, and the distributions are assumed identical except for a difference in location, Dunn's test may be understood as a test for median difference and for mean difference. 'dunn.test' accounts for tied ranks.  "
  },
  {
    "id": 11465,
    "package_name": "dverse",
    "title": "Document a Universe of Packages",
    "description": "Creates a data frame containing the metadata associated with\n    the documentation of a collection of R packages. It allows for linking\n    topic names to their corresponding documentation online. If you\n    maintain a universe meta-package, it helps create a comprehensive\n    reference for its website.",
    "version": "0.2.0",
    "maintainer": "Mauro Lepore <maurolepore@gmail.com>",
    "author": "Mauro Lepore [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1986-7988>)",
    "url": "https://github.com/maurolepore/dverse,\nhttps://maurolepore.github.io/dverse/",
    "bug_reports": "https://github.com/maurolepore/dverse/issues",
    "repository": "https://cran.r-project.org/package=dverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dverse Document a Universe of Packages Creates a data frame containing the metadata associated with\n    the documentation of a collection of R packages. It allows for linking\n    topic names to their corresponding documentation online. If you\n    maintain a universe meta-package, it helps create a comprehensive\n    reference for its website.  "
  },
  {
    "id": 11513,
    "package_name": "eCerto",
    "title": "Statistical Tests for the Production of Reference Materials",
    "description": "The production of certified reference materials (CRMs) requires \n    various statistical tests depending on the task and recorded data to ensure \n    that reported values of CRMs are appropriate. \n    Often these tests are performed according to the procedures described in \n    'ISO GUIDE 35:2017'. The 'eCerto' package contains a 'Shiny' app which \n    provides functionality to load, process, report and backup data recorded \n    during CRM production and facilitates following the recommended procedures.\n    It is described in Lisec et al (2023) <doi:10.1007/s00216-023-05099-3> and\n    can also be accessed online <https://apps.bam.de/shn00/eCerto/> without \n    package installation.",
    "version": "0.8.5",
    "maintainer": "Jan Lisec <jan.lisec@bam.de>",
    "author": "Jan Lisec [cre, aut] (ORCID: <https://orcid.org/0000-0003-1220-2286>),\n  Frederik Kre\u00df [ctb]",
    "url": "https://github.com/janlisec/eCerto",
    "bug_reports": "https://github.com/janlisec/eCerto/issues",
    "repository": "https://cran.r-project.org/package=eCerto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eCerto Statistical Tests for the Production of Reference Materials The production of certified reference materials (CRMs) requires \n    various statistical tests depending on the task and recorded data to ensure \n    that reported values of CRMs are appropriate. \n    Often these tests are performed according to the procedures described in \n    'ISO GUIDE 35:2017'. The 'eCerto' package contains a 'Shiny' app which \n    provides functionality to load, process, report and backup data recorded \n    during CRM production and facilitates following the recommended procedures.\n    It is described in Lisec et al (2023) <doi:10.1007/s00216-023-05099-3> and\n    can also be accessed online <https://apps.bam.de/shn00/eCerto/> without \n    package installation.  "
  },
  {
    "id": 11544,
    "package_name": "easyORtables",
    "title": "Easy Odds Ratio Tables",
    "description": "Creates text, 'LaTeX', Markdown, or Bootstrap-styled HTML-formatted odds ratio tables with confidence intervals for multiple logistic regression models.",
    "version": "0.0.1",
    "maintainer": "Neil Mehta <neilmhta@gmail.com>",
    "author": "Neil Mehta [aut, cre] (ORCID: <https://orcid.org/0000-0002-1245-1103>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=easyORtables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easyORtables Easy Odds Ratio Tables Creates text, 'LaTeX', Markdown, or Bootstrap-styled HTML-formatted odds ratio tables with confidence intervals for multiple logistic regression models.  "
  },
  {
    "id": 11572,
    "package_name": "eatRep",
    "title": "Educational Assessment Tools for Replication Methods",
    "description": "Replication methods to compute some basic statistic operations (means, standard deviations,\n  frequency tables, percentiles, mean comparisons using weighted effect coding, generalized linear models,\n  and linear multilevel models) in complex survey designs comprising multiple imputed or nested imputed\n  variables and/or a clustered sampling structure which both deserve special procedures at least in\n  estimating standard errors. See the package documentation for a more detailed description along with references.",
    "version": "0.15.2",
    "maintainer": "Sebastian Weirich <sebastian.weirich@iqb.hu-berlin.de>",
    "author": "Sebastian Weirich [aut, cre],\n  Martin Hecht [aut],\n  Karoline Sachse [aut],\n  Benjamin Becker [aut],\n  Edna Grewers [ctb]",
    "url": "https://github.com/weirichs/eatRep,\nhttps://weirichs.github.io/eatRep/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eatRep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eatRep Educational Assessment Tools for Replication Methods Replication methods to compute some basic statistic operations (means, standard deviations,\n  frequency tables, percentiles, mean comparisons using weighted effect coding, generalized linear models,\n  and linear multilevel models) in complex survey designs comprising multiple imputed or nested imputed\n  variables and/or a clustered sampling structure which both deserve special procedures at least in\n  estimating standard errors. See the package documentation for a more detailed description along with references.  "
  },
  {
    "id": 11601,
    "package_name": "echor",
    "title": "Access EPA 'ECHO' Data",
    "description": "An R interface to United States Environmental \n    Protection Agency (EPA) Environmental Compliance \n    History Online ('ECHO') Application Program Interface\n    (API). 'ECHO' provides information about EPA permitted \n    facilities, discharges, and other reporting info \n    associated with permitted entities. Data are obtained \n    from <https://echo.epa.gov/>. ",
    "version": "0.1.9",
    "maintainer": "Michael Schramm <mpschramm@gmail.com>",
    "author": "Michael Schramm [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1876-6592>)",
    "url": "https://github.com/mps9506/echor, https://mps9506.github.io/echor/",
    "bug_reports": "https://github.com/mps9506/echor/issues",
    "repository": "https://cran.r-project.org/package=echor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "echor Access EPA 'ECHO' Data An R interface to United States Environmental \n    Protection Agency (EPA) Environmental Compliance \n    History Online ('ECHO') Application Program Interface\n    (API). 'ECHO' provides information about EPA permitted \n    facilities, discharges, and other reporting info \n    associated with permitted entities. Data are obtained \n    from <https://echo.epa.gov/>.   "
  },
  {
    "id": 11610,
    "package_name": "ecochange",
    "title": "Integrating Ecosystem Remote Sensing Products to Derive EBV\nIndicators",
    "description": "Essential Biodiversity Variables (EBV) are state variables with dimensions on time, space, and biological organization that document biodiversity change. Freely available ecosystem remote sensing products (ERSP) are downloaded and integrated with data for national or regional domains to derive indicators for EBV in the class ecosystem structure (Pereira et al., 2013) <doi:10.1126/science.1229931>, including horizontal ecosystem extents, fragmentation, and information-theory indices. To process ERSP, users must provide a polygon or geographic administrative data map. Downloadable ERSP include Global Surface Water (Peckel et al., 2016) <doi:10.1038/nature20584>, Forest Change (Hansen et al., 2013) <doi:10.1126/science.1244693>, and Continuous Tree Cover data (Sexton et al., 2013) <doi:10.1080/17538947.2013.786146>. ",
    "version": "2.9.3.3",
    "maintainer": "Wilson Lara Henao <wilarhen@gmail.com>",
    "author": "Wilson Lara Henao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3527-1380>),\n  Victor Gutierrez-Velez [aut] (ORCID:\n    <https://orcid.org/0000-0003-1338-2020>),\n  Ivan Gonzalez [ctb] (ORCID: <https://orcid.org/0000-0002-0313-398X>),\n  Maria C. Londono [ctb] (ORCID: <https://orcid.org/0000-0002-2317-5503>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ecochange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ecochange Integrating Ecosystem Remote Sensing Products to Derive EBV\nIndicators Essential Biodiversity Variables (EBV) are state variables with dimensions on time, space, and biological organization that document biodiversity change. Freely available ecosystem remote sensing products (ERSP) are downloaded and integrated with data for national or regional domains to derive indicators for EBV in the class ecosystem structure (Pereira et al., 2013) <doi:10.1126/science.1229931>, including horizontal ecosystem extents, fragmentation, and information-theory indices. To process ERSP, users must provide a polygon or geographic administrative data map. Downloadable ERSP include Global Surface Water (Peckel et al., 2016) <doi:10.1038/nature20584>, Forest Change (Hansen et al., 2013) <doi:10.1126/science.1244693>, and Continuous Tree Cover data (Sexton et al., 2013) <doi:10.1080/17538947.2013.786146>.   "
  },
  {
    "id": 11622,
    "package_name": "econid",
    "title": "Economic Entity Identifier Standardization",
    "description": "Provides utility functions for standardizing economic entity (economy, aggregate, institution, etc.) name and id in economic datasets such as those published by the International Monetary Fund and World Bank. Aims to facilitate consistent data analysis, reporting, and joining across datasets. Used as a foundational building block in the 'econdataverse' family of packages (<https://www.econdataverse.org>).",
    "version": "0.0.2",
    "maintainer": "L. Teal Emery <lte@tealinsights.com>",
    "author": "L. Teal Emery [cre],\n  Christopher C. Smith [aut],\n  Christoph Scheuch [ctb],\n  Teal Insights [cph]",
    "url": "https://teal-insights.github.io/r-econid/,\nhttps://github.com/Teal-Insights/r-econid",
    "bug_reports": "https://github.com/Teal-Insights/r-econid/issues",
    "repository": "https://cran.r-project.org/package=econid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "econid Economic Entity Identifier Standardization Provides utility functions for standardizing economic entity (economy, aggregate, institution, etc.) name and id in economic datasets such as those published by the International Monetary Fund and World Bank. Aims to facilitate consistent data analysis, reporting, and joining across datasets. Used as a foundational building block in the 'econdataverse' family of packages (<https://www.econdataverse.org>).  "
  },
  {
    "id": 11629,
    "package_name": "ecorest",
    "title": "Conducts Analyses Informing Ecosystem Restoration Decisions",
    "description": "Three sets of data and functions for informing ecosystem restoration\n    decisions, particularly in the context of the U.S. Army Corps of Engineers.\n    First, model parameters are compiled as a data set and associated metadata \n    for over 300 habitat suitability models developed by the U.S. Fish and \n    Wildlife Service (USFWS 1980, <https://www.fws.gov/policy-library/870fw1>).\n    Second, functions for conducting habitat suitability analyses both for the \n    models described above as well as generic user-specified model parameterizations. \n    Third, a suite of decision support tools for conducting cost-effectiveness and \n    incremental cost analyses (Robinson et al. 1995, IWR Report 95-R-1, U.S. \n    Army Corps of Engineers).",
    "version": "2.0.1",
    "maintainer": "S. Kyle McKay <skmckay@gmail.com>",
    "author": "S. Kyle McKay [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2703-3841>),\n  Darixa D. Hernandez-Abrams [aut],\n  Kiara C. Cushway [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ecorest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ecorest Conducts Analyses Informing Ecosystem Restoration Decisions Three sets of data and functions for informing ecosystem restoration\n    decisions, particularly in the context of the U.S. Army Corps of Engineers.\n    First, model parameters are compiled as a data set and associated metadata \n    for over 300 habitat suitability models developed by the U.S. Fish and \n    Wildlife Service (USFWS 1980, <https://www.fws.gov/policy-library/870fw1>).\n    Second, functions for conducting habitat suitability analyses both for the \n    models described above as well as generic user-specified model parameterizations. \n    Third, a suite of decision support tools for conducting cost-effectiveness and \n    incremental cost analyses (Robinson et al. 1995, IWR Report 95-R-1, U.S. \n    Army Corps of Engineers).  "
  },
  {
    "id": 11638,
    "package_name": "ecoteach",
    "title": "Educational Datasets for Ecology and Agriculture",
    "description": "A collection of curated educational datasets for teaching ecology and\n    agriculture concepts. Includes data on wildlife monitoring, plant treatments,\n    and ecological observations with documentation and examples for educational use.\n    All datasets are derived from published scientific studies and are available\n    under CC0 or compatible licenses.",
    "version": "0.1.0",
    "maintainer": "W. Edwin Harris <weh9000@gmail.com>",
    "author": "W. Edwin Harris [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ecoteach",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ecoteach Educational Datasets for Ecology and Agriculture A collection of curated educational datasets for teaching ecology and\n    agriculture concepts. Includes data on wildlife monitoring, plant treatments,\n    and ecological observations with documentation and examples for educational use.\n    All datasets are derived from published scientific studies and are available\n    under CC0 or compatible licenses.  "
  },
  {
    "id": 11652,
    "package_name": "edar",
    "title": "Convenient Functions for Exploratory Data Analysis",
    "description": "A collection of convenient functions to facilitate common tasks in exploratory data analysis. \n          Some common tasks include generating summary tables of variables, \n          displaying tables as a 'flextable' or a 'kable' and visualising variables using 'ggplot2'. \n          Labels stating the source file with run time can be easily generated\n          for annotation in tables and plots. ",
    "version": "0.0.6",
    "maintainer": "Tomas Sou <tomas.sou@carexer.com>",
    "author": "Tomas Sou [aut, cre] (ORCID: <https://orcid.org/0000-0002-7570-5545>)",
    "url": "https://soutomas.github.io/edar/,\nhttps://github.com/soutomas/edar/",
    "bug_reports": "https://github.com/soutomas/edar/issues",
    "repository": "https://cran.r-project.org/package=edar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "edar Convenient Functions for Exploratory Data Analysis A collection of convenient functions to facilitate common tasks in exploratory data analysis. \n          Some common tasks include generating summary tables of variables, \n          displaying tables as a 'flextable' or a 'kable' and visualising variables using 'ggplot2'. \n          Labels stating the source file with run time can be easily generated\n          for annotation in tables and plots.   "
  },
  {
    "id": 11723,
    "package_name": "eixport",
    "title": "Export Emissions to Atmospheric Models",
    "description": "Emissions are the mass of pollutants released into the atmosphere. Air quality models need emissions data, with spatial and temporal distribution, to represent air pollutant concentrations. This package, eixport, creates inputs for the air quality models 'WRF-Chem' Grell et al (2005) <doi:10.1016/j.atmosenv.2005.04.027>, 'MUNICH' Kim et al (2018) <doi:10.5194/gmd-11-611-2018> , 'BRAMS-SPM' Freitas et al (2005) <doi:10.1016/j.atmosenv.2005.07.017> and 'RLINE' Snyder et al (2013) <doi:10.1016/j.atmosenv.2013.05.074>. See the 'eixport' website (<https://atmoschem.github.io/eixport/>) for more information, documentations and examples. More details in Ibarra-Espinosa et al (2018) <doi:10.21105/joss.00607>.",
    "version": "0.6.2",
    "maintainer": "Sergio Ibarra-Espinosa <zergioibarra@gmail.com>",
    "author": "Sergio Ibarra-Espinosa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3162-1905>),\n  Daniel Schuch [aut] (ORCID: <https://orcid.org/0000-0001-5977-4519>),\n  Edmilson Freitas [ths] (ORCID: <https://orcid.org/0000-0001-8783-2747>)",
    "url": "https://atmoschem.github.io/eixport/",
    "bug_reports": "https://github.com/atmoschem/eixport/issues/",
    "repository": "https://cran.r-project.org/package=eixport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eixport Export Emissions to Atmospheric Models Emissions are the mass of pollutants released into the atmosphere. Air quality models need emissions data, with spatial and temporal distribution, to represent air pollutant concentrations. This package, eixport, creates inputs for the air quality models 'WRF-Chem' Grell et al (2005) <doi:10.1016/j.atmosenv.2005.04.027>, 'MUNICH' Kim et al (2018) <doi:10.5194/gmd-11-611-2018> , 'BRAMS-SPM' Freitas et al (2005) <doi:10.1016/j.atmosenv.2005.07.017> and 'RLINE' Snyder et al (2013) <doi:10.1016/j.atmosenv.2013.05.074>. See the 'eixport' website (<https://atmoschem.github.io/eixport/>) for more information, documentations and examples. More details in Ibarra-Espinosa et al (2018) <doi:10.21105/joss.00607>.  "
  },
  {
    "id": 11729,
    "package_name": "elastic",
    "title": "General Purpose Interface to 'Elasticsearch'",
    "description": "Connect to 'Elasticsearch', a 'NoSQL' database built on the 'Java'\n    Virtual Machine. Interacts with the 'Elasticsearch' 'HTTP' API\n    (<https://www.elastic.co/elasticsearch/>), including functions for\n    setting connection details to 'Elasticsearch' instances, loading bulk data,\n    searching for documents with both 'HTTP' query variables and 'JSON' based body\n    requests. In addition, 'elastic' provides functions for interacting with API's\n    for 'indices', documents, nodes, clusters, an interface to the cat API, and\n    more.",
    "version": "1.2.0",
    "maintainer": "Scott Chamberlain <myrmecocystus@gmail.com>",
    "author": "Scott Chamberlain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1444-9135>)",
    "url": "https://docs.ropensci.org/elastic/ (website),\nhttps://github.com/ropensci/elastic",
    "bug_reports": "https://github.com/ropensci/elastic/issues",
    "repository": "https://cran.r-project.org/package=elastic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "elastic General Purpose Interface to 'Elasticsearch' Connect to 'Elasticsearch', a 'NoSQL' database built on the 'Java'\n    Virtual Machine. Interacts with the 'Elasticsearch' 'HTTP' API\n    (<https://www.elastic.co/elasticsearch/>), including functions for\n    setting connection details to 'Elasticsearch' instances, loading bulk data,\n    searching for documents with both 'HTTP' query variables and 'JSON' based body\n    requests. In addition, 'elastic' provides functions for interacting with API's\n    for 'indices', documents, nodes, clusters, an interface to the cat API, and\n    more.  "
  },
  {
    "id": 11736,
    "package_name": "electoral",
    "title": "Allocating Seats Methods and Party System Scores",
    "description": "Highest averages & largest remainders allocating seats methods and\n    several party system scores.\n    Implemented highest averages allocating seats methods are D'Hondt, Webster,\n    Danish, Imperiali, Hill-Huntington, Dean, Modified Sainte-Lague,\n    equal proportions and Adams.\n    Implemented largest remainders allocating seats methods are Hare, Droop,\n    Hangenbach-Bischoff, Imperial, modified Imperial and quotas & remainders.\n    The main advantage of this package is that ties are always reported\n    and not incorrectly allocated.\n    Party system scores provided are competitiveness, concentration,\n    effective number of parties, party nationalization score,\n    party system nationalization score and volatility.\n    References:\n    Gallagher (1991) <doi:10.1016/0261-3794(91)90004-C>.\n    Norris (2004, ISBN:0-521-82977-1).\n    Laakso & Taagepera (1979) <https://escholarship.org/uc/item/703827nv>.\n    Jones & Mainwaring (2003) <https://kellogg.nd.edu/sites/default/files/old_files/documents/304_0.pdf>.\n    Pedersen (1979) <https://janda.org/c24/Readings/Pedersen/Pedersen.htm>.\n    Golosov (2010) <doi:10.1177/1354068809339538>.\n    Golosov (2014) <doi:10.1177/1354068814549342>.",
    "version": "0.1.4",
    "maintainer": "Jorge Albuja <albuja@yahoo.com>",
    "author": "Jorge Albuja [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=electoral",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "electoral Allocating Seats Methods and Party System Scores Highest averages & largest remainders allocating seats methods and\n    several party system scores.\n    Implemented highest averages allocating seats methods are D'Hondt, Webster,\n    Danish, Imperiali, Hill-Huntington, Dean, Modified Sainte-Lague,\n    equal proportions and Adams.\n    Implemented largest remainders allocating seats methods are Hare, Droop,\n    Hangenbach-Bischoff, Imperial, modified Imperial and quotas & remainders.\n    The main advantage of this package is that ties are always reported\n    and not incorrectly allocated.\n    Party system scores provided are competitiveness, concentration,\n    effective number of parties, party nationalization score,\n    party system nationalization score and volatility.\n    References:\n    Gallagher (1991) <doi:10.1016/0261-3794(91)90004-C>.\n    Norris (2004, ISBN:0-521-82977-1).\n    Laakso & Taagepera (1979) <https://escholarship.org/uc/item/703827nv>.\n    Jones & Mainwaring (2003) <https://kellogg.nd.edu/sites/default/files/old_files/documents/304_0.pdf>.\n    Pedersen (1979) <https://janda.org/c24/Readings/Pedersen/Pedersen.htm>.\n    Golosov (2010) <doi:10.1177/1354068809339538>.\n    Golosov (2014) <doi:10.1177/1354068814549342>.  "
  },
  {
    "id": 11759,
    "package_name": "emailvalidation",
    "title": "Client for the 'emailalvalidation.io' E-Mail Validation API",
    "description": "An R client for the 'emailvalidation.io' e-mail verification API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://emailvalidation.io/docs> .",
    "version": "0.1.0",
    "maintainer": "Dominik Kukacka <dominik@everapi.com>",
    "author": "Dominik Kukacka [aut, cre]",
    "url": "https://emailvalidation.io, https://emailvalidation.io/docs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=emailvalidation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "emailvalidation Client for the 'emailalvalidation.io' E-Mail Validation API An R client for the 'emailvalidation.io' e-mail verification API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://emailvalidation.io/docs> .  "
  },
  {
    "id": 11762,
    "package_name": "emcAdr",
    "title": "Evolutionary Version of the Metropolis-Hastings Algorithm",
    "description": "Provides computational methods for detecting adverse high-order drug interactions\n         from individual case safety reports using statistical techniques,\n         allowing the exploration of higher-order interactions among drug cocktails.",
    "version": "1.2",
    "maintainer": "Jules Bangard <jules.bangard@etu.unistra.fr>",
    "author": "Jules Bangard [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-4670-7860>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=emcAdr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "emcAdr Evolutionary Version of the Metropolis-Hastings Algorithm Provides computational methods for detecting adverse high-order drug interactions\n         from individual case safety reports using statistical techniques,\n         allowing the exploration of higher-order interactions among drug cocktails.  "
  },
  {
    "id": 11775,
    "package_name": "emphatic",
    "title": "Exploratory Analysis of Tabular Data using Colour Highlighting",
    "description": "Tools for exploratory analysis of tabular data using colour \n highlighting. Highlighting is displayed in any console supporting 'ANSI'\n colours, and can be converted to 'HTML', 'typst', 'latex' and 'SVG'. \n 'quarto' and 'rmarkdown' rendering are directly supported. It is \n also possible to add colour to regular expression matches and \n highlight differences between two arbitrary R objects.",
    "version": "0.1.8",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph]",
    "url": "https://coolbutuseless.github.io/package/emphatic/,\nhttps://github.com/coolbutuseless/emphatic",
    "bug_reports": "https://github.com/coolbutuseless/emphatic/issues",
    "repository": "https://cran.r-project.org/package=emphatic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "emphatic Exploratory Analysis of Tabular Data using Colour Highlighting Tools for exploratory analysis of tabular data using colour \n highlighting. Highlighting is displayed in any console supporting 'ANSI'\n colours, and can be converted to 'HTML', 'typst', 'latex' and 'SVG'. \n 'quarto' and 'rmarkdown' rendering are directly supported. It is \n also possible to add colour to regular expression matches and \n highlight differences between two arbitrary R objects.  "
  },
  {
    "id": 11785,
    "package_name": "encryptedRmd",
    "title": "Encrypt Html Reports Using 'Libsodium'",
    "description": "Create encrypted html files that are fully self contained and do\n  not require any additional software. Using the package you can encrypt\n  arbitrary html files and also directly create encrypted 'rmarkdown' html reports.",
    "version": "0.2.1",
    "maintainer": "Dirk Schumacher <mail@dirk-schumacher.net>",
    "author": "Dirk Schumacher [aut, cre, cph],\n  Jannis R. [aut, cph]",
    "url": "https://github.com/dirkschumacher/encryptedRmd",
    "bug_reports": "https://github.com/dirkschumacher/encryptedRmd/issues",
    "repository": "https://cran.r-project.org/package=encryptedRmd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "encryptedRmd Encrypt Html Reports Using 'Libsodium' Create encrypted html files that are fully self contained and do\n  not require any additional software. Using the package you can encrypt\n  arbitrary html files and also directly create encrypted 'rmarkdown' html reports.  "
  },
  {
    "id": 11805,
    "package_name": "ensembleBMA",
    "title": "Probabilistic Forecasting using Ensembles and Bayesian Model\nAveraging",
    "description": "Bayesian Model Averaging to create probabilistic forecasts\n        from ensemble forecasts and weather observations\n <https://stat.uw.edu/sites/default/files/files/reports/2007/tr516.pdf>.",
    "version": "5.1.8",
    "maintainer": "Chris Fraley <fraley@u.washington.edu>",
    "author": "Chris Fraley, Adrian E. Raftery, J. McLean Sloughter, Tilmann\n        Gneiting, University of Washington.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ensembleBMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ensembleBMA Probabilistic Forecasting using Ensembles and Bayesian Model\nAveraging Bayesian Model Averaging to create probabilistic forecasts\n        from ensemble forecasts and weather observations\n <https://stat.uw.edu/sites/default/files/files/reports/2007/tr516.pdf>.  "
  },
  {
    "id": 11814,
    "package_name": "envDocument",
    "title": "Document the R Working Environment",
    "description": "Prints out information about the R working environment\n    (system, R version,loaded and attached packages and versions) from a single\n    function \"env_doc()\".  Optionally adds information on git repository,\n    tags, commits and remotes (if available).",
    "version": "2.4.2",
    "maintainer": "Donald Jackson <djackson2641@gmail.com>",
    "author": "Donald Jackson [aut, cre]",
    "url": "https://github.com/dgJacks0n/envDocument",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=envDocument",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "envDocument Document the R Working Environment Prints out information about the R working environment\n    (system, R version,loaded and attached packages and versions) from a single\n    function \"env_doc()\".  Optionally adds information on git repository,\n    tags, commits and remotes (if available).  "
  },
  {
    "id": 11825,
    "package_name": "envstat",
    "title": "Configurable Reporting on your External Compute Environment",
    "description": "Runs a series of configurable tests against a user's compute \n  environment. This can be used for checking that things like a specific \n  directory or an environment variable is available before you start an analysis.\n  Alternatively, you can use the package's situation report when filing error\n  reports with your compute infrastructure.",
    "version": "0.0.3",
    "maintainer": "Mark Sellors <rstats@5vcc.com>",
    "author": "Mark Sellors [aut, cre]",
    "url": "https://envstat.sellorm.com",
    "bug_reports": "https://github.com/sellorm/envstat/issues",
    "repository": "https://cran.r-project.org/package=envstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "envstat Configurable Reporting on your External Compute Environment Runs a series of configurable tests against a user's compute \n  environment. This can be used for checking that things like a specific \n  directory or an environment variable is available before you start an analysis.\n  Alternatively, you can use the package's situation report when filing error\n  reports with your compute infrastructure.  "
  },
  {
    "id": 11859,
    "package_name": "epitraxr",
    "title": "Manipulate 'EpiTrax' Data and Generate Reports",
    "description": "A fast, flexible tool for generating disease surveillance\n    reports from data exported from 'EpiTrax', a central repository for\n    epidemiological data used by public health officials. It provides\n    functions to manipulate 'EpiTrax' datasets, tailor reports to internal\n    or public use, and export reports in CSV, Excel 'xlsx', or PDF formats.",
    "version": "0.5.0",
    "maintainer": "Andrew Pulsipher <pulsipher.a@gmail.com>",
    "author": "Andrew Pulsipher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0773-3210>),\n  Nate Lanza [aut, ctb],\n  Centers for Disease Control and Prevention's Center for Forecasting and\n    Outbreak Analytics [fnd] (Cooperative agreement CDC-RFA-FT-23-0069)",
    "url": "https://epiforesite.github.io/epitraxr/,\nhttps://github.com/EpiForeSITE/epitraxr",
    "bug_reports": "https://github.com/EpiForeSITE/epitraxr/issues",
    "repository": "https://cran.r-project.org/package=epitraxr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epitraxr Manipulate 'EpiTrax' Data and Generate Reports A fast, flexible tool for generating disease surveillance\n    reports from data exported from 'EpiTrax', a central repository for\n    epidemiological data used by public health officials. It provides\n    functions to manipulate 'EpiTrax' datasets, tailor reports to internal\n    or public use, and export reports in CSV, Excel 'xlsx', or PDF formats.  "
  },
  {
    "id": 11873,
    "package_name": "epoxy",
    "title": "String Interpolation for Documents, Reports and Apps",
    "description": "Extra strength 'glue' for data-driven templates. String\n    interpolation for 'Shiny' apps or 'R Markdown' and 'knitr'-powered\n    'Quarto' documents, built on the 'glue' and 'whisker' packages.",
    "version": "1.0.0",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>),\n  Kushagra Gour [ctb] (hint.css),\n  The mustache.js community [ctb] (mustache.js)",
    "url": "https://pkg.garrickadenbuie.com/epoxy/,\nhttps://github.com/gadenbuie/epoxy",
    "bug_reports": "https://github.com/gadenbuie/epoxy/issues",
    "repository": "https://cran.r-project.org/package=epoxy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epoxy String Interpolation for Documents, Reports and Apps Extra strength 'glue' for data-driven templates. String\n    interpolation for 'Shiny' apps or 'R Markdown' and 'knitr'-powered\n    'Quarto' documents, built on the 'glue' and 'whisker' packages.  "
  },
  {
    "id": 11878,
    "package_name": "eq5d",
    "title": "Methods for Analysing 'EQ-5D' Data and Calculating 'EQ-5D' Index\nScores",
    "description": "EQ-5D is a popular health related quality of life instrument used \n    in the clinical and economic evaluation of health care. Developed by the \n    EuroQol group <https://euroqol.org/>, the instrument consists of two \n    components: health state description and evaluation. For the description \n    component a subject self-rates their health in terms of five dimensions; \n    mobility, self-care, usual activities, pain/discomfort, and \n    anxiety/depression using either a three-level (EQ-5D-3L,\n    <https://euroqol.org/information-and-support/euroqol-instruments/eq-5d-3l/>) or a five-level\n    (EQ-5D-5L, <https://euroqol.org/information-and-support/euroqol-instruments/eq-5d-5l/>) \n    scale. Frequently the scores on these five dimensions are converted to a \n    single utility index using country specific value sets, which can be used\n    in the clinical and economic evaluation of health care as well as in \n    population health surveys. The eq5d package provides methods to calculate \n    index scores from a subject's dimension scores. 32 TTO and 11 VAS EQ-5D-3L\n    value sets including those for countries in Szende et al (2007) \n    <doi:10.1007/1-4020-5511-0> and Szende et al (2014) \n    <doi:10.1007/978-94-007-7596-1>, 48 EQ-5D-5L EQ-VT value sets, the \n    EQ-5D-5L crosswalk value sets developed by van Hout et al. (2012) \n    <doi:10.1016/j.jval.2012.02.008>, the crosswalk value sets for Bermuda, Jordan and \n    Russia and the van Hout (2021) reverse crosswalk value sets. 11 EQ-5D-Y3L \n    value sets are also included as are the NICE 'DSU' age-sex based EQ-5D-3L \n    to EQ-5D-5L and EQ-5D-5L to EQ-5D-3L mappings. Methods are also included \n    for the analysis of EQ-5D profiles, including those from the book \"Methods \n    for Analyzing and Reporting EQ-5D data\" by Devlin et al. (2020) \n    <doi:10.1007/978-3-030-47622-9>. Additionally a shiny web tool is included \n    to enable the calculation, visualisation and automated statistical analysis \n    of EQ-5D data via a web browser using EQ-5D dimension scores stored in CSV \n    or Excel files. ",
    "version": "0.16.1",
    "maintainer": "Fraser Morton <fraser.morton@glasgow.ac.uk>",
    "author": "Fraser Morton [aut, cre],\n  Jagtar Singh Nijjar [aut]",
    "url": "https://github.com/fragla/eq5d",
    "bug_reports": "https://github.com/fragla/eq5d/issues",
    "repository": "https://cran.r-project.org/package=eq5d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eq5d Methods for Analysing 'EQ-5D' Data and Calculating 'EQ-5D' Index\nScores EQ-5D is a popular health related quality of life instrument used \n    in the clinical and economic evaluation of health care. Developed by the \n    EuroQol group <https://euroqol.org/>, the instrument consists of two \n    components: health state description and evaluation. For the description \n    component a subject self-rates their health in terms of five dimensions; \n    mobility, self-care, usual activities, pain/discomfort, and \n    anxiety/depression using either a three-level (EQ-5D-3L,\n    <https://euroqol.org/information-and-support/euroqol-instruments/eq-5d-3l/>) or a five-level\n    (EQ-5D-5L, <https://euroqol.org/information-and-support/euroqol-instruments/eq-5d-5l/>) \n    scale. Frequently the scores on these five dimensions are converted to a \n    single utility index using country specific value sets, which can be used\n    in the clinical and economic evaluation of health care as well as in \n    population health surveys. The eq5d package provides methods to calculate \n    index scores from a subject's dimension scores. 32 TTO and 11 VAS EQ-5D-3L\n    value sets including those for countries in Szende et al (2007) \n    <doi:10.1007/1-4020-5511-0> and Szende et al (2014) \n    <doi:10.1007/978-94-007-7596-1>, 48 EQ-5D-5L EQ-VT value sets, the \n    EQ-5D-5L crosswalk value sets developed by van Hout et al. (2012) \n    <doi:10.1016/j.jval.2012.02.008>, the crosswalk value sets for Bermuda, Jordan and \n    Russia and the van Hout (2021) reverse crosswalk value sets. 11 EQ-5D-Y3L \n    value sets are also included as are the NICE 'DSU' age-sex based EQ-5D-3L \n    to EQ-5D-5L and EQ-5D-5L to EQ-5D-3L mappings. Methods are also included \n    for the analysis of EQ-5D profiles, including those from the book \"Methods \n    for Analyzing and Reporting EQ-5D data\" by Devlin et al. (2020) \n    <doi:10.1007/978-3-030-47622-9>. Additionally a shiny web tool is included \n    to enable the calculation, visualisation and automated statistical analysis \n    of EQ-5D data via a web browser using EQ-5D dimension scores stored in CSV \n    or Excel files.   "
  },
  {
    "id": 11879,
    "package_name": "eq5dsuite",
    "title": "Handling and Analysing EQ-5d Data",
    "description": "The EQ-5D is a widely-used standarized instrument for measuring Health Related Quality Of Life (HRQOL), \n    developed by the EuroQol group <https://euroqol.org/>. It assesses five dimensions; mobility, self-care, \n    usual activities, pain/discomfort, and anxiety/depression, using either a three-level (EQ-5D-3L) or five-level (EQ-5D-5L) scale.\n    Scores from these dimensions are commonly converted into a single utility index using country-specific value sets, \n    which are critical in clinical and economic evaluations of healthcare and in population health surveys. \n    The eq5dsuite package enables users to calculate utility index values for the EQ-5D instruments, \n    including crosswalk utilities using the original crosswalk developed by van Hout et al. (2012) <doi:10.1016/j.jval.2012.02.008> \n    (mapping EQ-5D-5L responses to EQ-5D-3L index values), or the recently developed reverse crosswalk \n    by van Hout et al. (2021) <doi:10.1016/j.jval.2021.03.009> (mapping EQ-5D-3L responses\n    to EQ-5D-5L index values). Users are allowed to add and/or remove user-defined value sets. \n    Additionally, the package provides tools to analyze EQ-5D data according to the recommended \n    guidelines outlined in \"Methods for Analyzing and Reporting EQ-5D data\" by Devlin et al. (2020) <doi:10.1007/978-3-030-47622-9>.",
    "version": "1.0.1",
    "maintainer": "Kim Rand <krand@mathsinhealth.com>",
    "author": "Kim Rand [aut, cre] (ORCID: <https://orcid.org/0000-0001-7692-4099>),\n  Iryna Schlackow [aut] (ORCID: <https://orcid.org/0000-0002-4154-1431>),\n  Anabel Est\u00e9vez-Carrillo [aut] (ORCID:\n    <https://orcid.org/0000-0001-8778-5055>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eq5dsuite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eq5dsuite Handling and Analysing EQ-5d Data The EQ-5D is a widely-used standarized instrument for measuring Health Related Quality Of Life (HRQOL), \n    developed by the EuroQol group <https://euroqol.org/>. It assesses five dimensions; mobility, self-care, \n    usual activities, pain/discomfort, and anxiety/depression, using either a three-level (EQ-5D-3L) or five-level (EQ-5D-5L) scale.\n    Scores from these dimensions are commonly converted into a single utility index using country-specific value sets, \n    which are critical in clinical and economic evaluations of healthcare and in population health surveys. \n    The eq5dsuite package enables users to calculate utility index values for the EQ-5D instruments, \n    including crosswalk utilities using the original crosswalk developed by van Hout et al. (2012) <doi:10.1016/j.jval.2012.02.008> \n    (mapping EQ-5D-5L responses to EQ-5D-3L index values), or the recently developed reverse crosswalk \n    by van Hout et al. (2021) <doi:10.1016/j.jval.2021.03.009> (mapping EQ-5D-3L responses\n    to EQ-5D-5L index values). Users are allowed to add and/or remove user-defined value sets. \n    Additionally, the package provides tools to analyze EQ-5D data according to the recommended \n    guidelines outlined in \"Methods for Analyzing and Reporting EQ-5D data\" by Devlin et al. (2020) <doi:10.1007/978-3-030-47622-9>.  "
  },
  {
    "id": 11883,
    "package_name": "equatags",
    "title": "Equations to 'XML'",
    "description": "Provides function to transform latex math expressions \n into format 'HTML' or 'Office Open XML Math'. The 'XML' \n result can then be included in 'HTML', 'Microsoft Word' \n documents or 'Microsoft PowerPoint' presentations by using \n a 'Markdown' document or the R package 'officer'. ",
    "version": "0.2.2",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  ArData [cph]",
    "url": "",
    "bug_reports": "https://github.com/ardata-fr/equatags/issues",
    "repository": "https://cran.r-project.org/package=equatags",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "equatags Equations to 'XML' Provides function to transform latex math expressions \n into format 'HTML' or 'Office Open XML Math'. The 'XML' \n result can then be included in 'HTML', 'Microsoft Word' \n documents or 'Microsoft PowerPoint' presentations by using \n a 'Markdown' document or the R package 'officer'.   "
  },
  {
    "id": 11895,
    "package_name": "eratosthenes",
    "title": "Archaeological Synchronism",
    "description": "Estimation of unknown historical or archaeological dates subject to relationships with other relative dates and absolute constraints, derived as marginal densities from the full joint conditional, using a two-stage Gibbs sampler with consistent batch means to assess convergence. Features reporting on Monte Carlo standard errors, as well as tools for rule-based estimation of dates of production and use of artifact types, aligning and checking relative sequences, and evaluating the impact of the omission of relative/absolute events upon one another.",
    "version": "0.0.9",
    "maintainer": "Stephen A. Collins-Elliott <sce@utk.edu>",
    "author": "Stephen A. Collins-Elliott [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5642-6903>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eratosthenes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eratosthenes Archaeological Synchronism Estimation of unknown historical or archaeological dates subject to relationships with other relative dates and absolute constraints, derived as marginal densities from the full joint conditional, using a two-stage Gibbs sampler with consistent batch means to assess convergence. Features reporting on Monte Carlo standard errors, as well as tools for rule-based estimation of dates of production and use of artifact types, aligning and checking relative sequences, and evaluating the impact of the omission of relative/absolute events upon one another.  "
  },
  {
    "id": 11899,
    "package_name": "ergMargins",
    "title": "Process Analysis for Exponential Random Graph Models",
    "description": "Calculates marginal effects and conducts process analysis in exponential family random graph models (ERGM).\n    Includes functions to conduct mediation and moderation analyses and to diagnose\n    multicollinearity.\n    URL: <https://github.com/sduxbury/ergMargins>.\n    BugReports: <https://github.com/sduxbury/ergMargins/issues>.\n    Duxbury, Scott W (2021) <doi:10.1177/0049124120986178>.\n    Long, J. Scott, and Sarah Mustillo (2018) <doi:10.1177/0049124118799374>.\n    Mize, Trenton D. (2019) <doi:10.15195/v6.a4>.\n    Karlson, Kristian Bernt, Anders Holm, and Richard Breen (2012) <doi:10.1177/0081175012444861>.\n    Duxbury, Scott W (2018) <doi:10.1177/0049124118782543>.\n    Duxbury, Scott W, Jenna Wertsching (2023) <doi:10.1016/j.socnet.2023.02.003>.\n    Huang, Peng, Carter Butts (2023) <doi:10.1016/j.socnet.2023.07.001>.",
    "version": "1.6.1",
    "maintainer": "Scott Duxbury <duxbury@email.unc.edu>",
    "author": "Scott Duxbury [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ergMargins",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ergMargins Process Analysis for Exponential Random Graph Models Calculates marginal effects and conducts process analysis in exponential family random graph models (ERGM).\n    Includes functions to conduct mediation and moderation analyses and to diagnose\n    multicollinearity.\n    URL: <https://github.com/sduxbury/ergMargins>.\n    BugReports: <https://github.com/sduxbury/ergMargins/issues>.\n    Duxbury, Scott W (2021) <doi:10.1177/0049124120986178>.\n    Long, J. Scott, and Sarah Mustillo (2018) <doi:10.1177/0049124118799374>.\n    Mize, Trenton D. (2019) <doi:10.15195/v6.a4>.\n    Karlson, Kristian Bernt, Anders Holm, and Richard Breen (2012) <doi:10.1177/0081175012444861>.\n    Duxbury, Scott W (2018) <doi:10.1177/0049124118782543>.\n    Duxbury, Scott W, Jenna Wertsching (2023) <doi:10.1016/j.socnet.2023.02.003>.\n    Huang, Peng, Carter Butts (2023) <doi:10.1016/j.socnet.2023.07.001>.  "
  },
  {
    "id": 11913,
    "package_name": "errors",
    "title": "Uncertainty Propagation for R Vectors",
    "description": "Support for measurement errors in R vectors, matrices and arrays:\n    automatic uncertainty propagation and reporting.\n    Documentation about 'errors' is provided in the paper by Ucar, Pebesma &\n    Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in this package as a\n    vignette; see 'citation(\"errors\")' for details.",
    "version": "0.4.4",
    "maintainer": "I\u00f1aki Ucar <iucar@fedoraproject.org>",
    "author": "I\u00f1aki Ucar [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-6403-5550>),\n  Lionel Henry [ctb],\n  RStudio [cph] (Copyright for code written by RStudio employees.)",
    "url": "https://r-quantities.github.io/errors/,\nhttps://github.com/r-quantities/errors",
    "bug_reports": "https://github.com/r-quantities/errors/issues",
    "repository": "https://cran.r-project.org/package=errors",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "errors Uncertainty Propagation for R Vectors Support for measurement errors in R vectors, matrices and arrays:\n    automatic uncertainty propagation and reporting.\n    Documentation about 'errors' is provided in the paper by Ucar, Pebesma &\n    Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in this package as a\n    vignette; see 'citation(\"errors\")' for details.  "
  },
  {
    "id": 11923,
    "package_name": "eseis",
    "title": "Environmental Seismology Toolbox",
    "description": "Environmental seismology is a scientific field that studies the  \n    seismic signals, emitted by Earth surface processes. This package \n    provides all relevant functions to read/write seismic data files, prepare, \n    analyse and visualise seismic data, and generate reports of the processing \n    history.",
    "version": "0.8.1",
    "maintainer": "Michael Dietze <michael.dietze@uni-goettingen.de>",
    "author": "Michael Dietze [cre, aut, trl],\n  Christoph Burow [ctb],\n  Sophie Lagarde [ctb, trl],\n  Clement Hibert [ctb, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eseis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eseis Environmental Seismology Toolbox Environmental seismology is a scientific field that studies the  \n    seismic signals, emitted by Earth surface processes. This package \n    provides all relevant functions to read/write seismic data files, prepare, \n    analyse and visualise seismic data, and generate reports of the processing \n    history.  "
  },
  {
    "id": 11930,
    "package_name": "esmtools",
    "title": "Preprocessing Experience Sampling Method (ESM) Data",
    "description": "Tailored explicitly for Experience Sampling Method (ESM)\n    data, it contains a suite of functions designed to simplify\n    preprocessing steps and create subsequent reporting. It empowers\n    users with capabilities to extract critical insights during\n    preprocessing, conducts thorough data quality assessments (e.g., design\n    and sampling scheme checks, compliance rate, careless responses), and\n    generates visualizations and concise summary tables tailored\n    specifically for ESM data. Additionally, it streamlines the creation\n    of informative and interactive preprocessing reports, enabling\n    researchers to transparently share their dataset preprocessing\n    methodologies. Finally, it is part of a larger ecosystem which\n    includes a framework and a web gallery\n    (<https://preprocess.esmtools.com/>).",
    "version": "1.0.1",
    "maintainer": "Jordan Revol <jordan.revol@kuleuven.be>",
    "author": "Jordan Revol [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5511-3617>),\n  Koen Niemeijer [ctb] (ORCID: <https://orcid.org/0000-0002-0816-534X>)",
    "url": "https://gitlab.kuleuven.be/ppw-okpiv/researchers/u0148925/esmtools/,\nhttps://package.esmtools.com/, https://preprocess.esmtools.com/",
    "bug_reports": "https://gitlab.kuleuven.be/ppw-okpiv/researchers/u0148925/esmtools/-/issues",
    "repository": "https://cran.r-project.org/package=esmtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "esmtools Preprocessing Experience Sampling Method (ESM) Data Tailored explicitly for Experience Sampling Method (ESM)\n    data, it contains a suite of functions designed to simplify\n    preprocessing steps and create subsequent reporting. It empowers\n    users with capabilities to extract critical insights during\n    preprocessing, conducts thorough data quality assessments (e.g., design\n    and sampling scheme checks, compliance rate, careless responses), and\n    generates visualizations and concise summary tables tailored\n    specifically for ESM data. Additionally, it streamlines the creation\n    of informative and interactive preprocessing reports, enabling\n    researchers to transparently share their dataset preprocessing\n    methodologies. Finally, it is part of a larger ecosystem which\n    includes a framework and a web gallery\n    (<https://preprocess.esmtools.com/>).  "
  },
  {
    "id": 11944,
    "package_name": "estmeansd",
    "title": "Estimating the Sample Mean and Standard Deviation from Commonly\nReported Quantiles in Meta-Analysis",
    "description": "Implements the methods of McGrath et al. (2020) \n    <doi:10.1177/0962280219889080> and Cai et al. (2021) \n    <doi:10.1177/09622802211047348> for estimating the sample mean and standard \n    deviation from commonly reported quantiles in meta-analysis. These methods \n    can be applied to studies that report the sample median, sample size, and \n    one or both of (i) the sample minimum and maximum values and (ii) the first \n    and third quartiles. The corresponding standard error estimators described \n    by McGrath et al. (2023) <doi:10.1177/09622802221139233> are also included.",
    "version": "1.0.1",
    "maintainer": "Sean McGrath <sean.mcgrath@mail.mcgill.ca>",
    "author": "Sean McGrath [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7281-3516>),\n  XiaoFei Zhao [aut],\n  Russell Steele [aut],\n  Andrea Benedetti [aut] (ORCID: <https://orcid.org/0000-0002-8314-9497>)",
    "url": "https://github.com/stmcg/estmeansd",
    "bug_reports": "https://github.com/stmcg/estmeansd/issues",
    "repository": "https://cran.r-project.org/package=estmeansd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "estmeansd Estimating the Sample Mean and Standard Deviation from Commonly\nReported Quantiles in Meta-Analysis Implements the methods of McGrath et al. (2020) \n    <doi:10.1177/0962280219889080> and Cai et al. (2021) \n    <doi:10.1177/09622802211047348> for estimating the sample mean and standard \n    deviation from commonly reported quantiles in meta-analysis. These methods \n    can be applied to studies that report the sample median, sample size, and \n    one or both of (i) the sample minimum and maximum values and (ii) the first \n    and third quartiles. The corresponding standard error estimators described \n    by McGrath et al. (2023) <doi:10.1177/09622802221139233> are also included.  "
  },
  {
    "id": 11961,
    "package_name": "eudract",
    "title": "Creates Safety Results Summary in XML to Upload to EudraCT, or\nClinicalTrials.gov",
    "description": "The remit of the European Clinical Trials Data Base (EudraCT <https://eudract.ema.europa.eu/> ), or ClinicalTrials.gov <https://clinicaltrials.gov/>, is to provide open access to summaries of all registered clinical trial results; thus aiming to prevent non-reporting of negative results and provide open-access to results to inform future research. The amount of information required and the format of the results, however, imposes a large extra workload at the end of studies on clinical trial units. In particular, the adverse-event-reporting component requires entering: each unique combination of treatment group and safety event; for every such event above, a further 4 pieces of information (body system, number of occurrences, number of subjects, number exposed) for non-serious events, plus an extra three pieces of data for serious adverse events (numbers of causally related events, deaths, causally related deaths). This package prepares the required statistics needed by EudraCT and formats them into the precise requirements to directly upload an XML file into the web portal, with no further data entry by hand.",
    "version": "1.0.5",
    "maintainer": "Simon Bond <simon.bond7@nhs.net>",
    "author": "Simon Bond [cre],\n  Beatrice Pantaleo [aut]",
    "url": "https://shug0131.github.io/eudraCT/",
    "bug_reports": "https://github.com/shug0131/eudraCT/issues",
    "repository": "https://cran.r-project.org/package=eudract",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eudract Creates Safety Results Summary in XML to Upload to EudraCT, or\nClinicalTrials.gov The remit of the European Clinical Trials Data Base (EudraCT <https://eudract.ema.europa.eu/> ), or ClinicalTrials.gov <https://clinicaltrials.gov/>, is to provide open access to summaries of all registered clinical trial results; thus aiming to prevent non-reporting of negative results and provide open-access to results to inform future research. The amount of information required and the format of the results, however, imposes a large extra workload at the end of studies on clinical trial units. In particular, the adverse-event-reporting component requires entering: each unique combination of treatment group and safety event; for every such event above, a further 4 pieces of information (body system, number of occurrences, number of subjects, number exposed) for non-serious events, plus an extra three pieces of data for serious adverse events (numbers of causally related events, deaths, causally related deaths). This package prepares the required statistics needed by EudraCT and formats them into the precise requirements to directly upload an XML file into the web portal, with no further data entry by hand.  "
  },
  {
    "id": 11981,
    "package_name": "evenBreak",
    "title": "A Posteriori Probs of Suits Breaking Evenly Across Four Hands",
    "description": "We quantitatively evaluated the assertion that says if one suit is found to be evenly distributed among the 4 players, the rest of the suits are more likely to be evenly distributed. Our mathematical analyses show that, if one suit is found to be evenly distributed, then a second suit has a slightly elevated probability (ranging between 10% to 15%) of being evenly distributed. If two suits are found to be evenly distributed, then a third suit has a substantially elevated probability (ranging between 30% to 50%) of being evenly distributed.This package refers to methods and authentic data from Ely Culbertson <https://www.bridgebum.com/law_of_symmetry.php>, Gregory Stoll <https://gregstoll.com/~gregstoll/bridge/math.html>, and details of performing the probability calculations from Jeremy L. Martin <https://jlmartin.ku.edu/~jlmartin/bridge/basics.pdf>, Emile Borel and Andre Cheron (1954) \"The Mathematical Theory of Bridge\",Antonio Vivaldi and Gianni Barracho (2001, ISBN:0 7134 8663 5)  \"Probabilities and Alternatives in Bridge\", Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>.",
    "version": "1.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=evenBreak",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evenBreak A Posteriori Probs of Suits Breaking Evenly Across Four Hands We quantitatively evaluated the assertion that says if one suit is found to be evenly distributed among the 4 players, the rest of the suits are more likely to be evenly distributed. Our mathematical analyses show that, if one suit is found to be evenly distributed, then a second suit has a slightly elevated probability (ranging between 10% to 15%) of being evenly distributed. If two suits are found to be evenly distributed, then a third suit has a substantially elevated probability (ranging between 30% to 50%) of being evenly distributed.This package refers to methods and authentic data from Ely Culbertson <https://www.bridgebum.com/law_of_symmetry.php>, Gregory Stoll <https://gregstoll.com/~gregstoll/bridge/math.html>, and details of performing the probability calculations from Jeremy L. Martin <https://jlmartin.ku.edu/~jlmartin/bridge/basics.pdf>, Emile Borel and Andre Cheron (1954) \"The Mathematical Theory of Bridge\",Antonio Vivaldi and Gianni Barracho (2001, ISBN:0 7134 8663 5)  \"Probabilities and Alternatives in Bridge\", Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>Ken Monzingo (2005) \"Hand and Suit Patterns\" <http://web2.acbl.org/documentlibrary/teachers/celebritylessons/handpatternsrevised.pdf>.  "
  },
  {
    "id": 11988,
    "package_name": "eventreport",
    "title": "Diagnose, Visualize, and Aggregate Event Report Level Data",
    "description": "Diagnose, visualize, and aggregate event report \n    level data to the event level. Users provide an event report level \n    dataset, specify their aggregation rules, and the package produces a\n    dataset aggregated at the event level. Also includes the Modes and \n    Agents of Election-Related Violence in C\u00f4te d'Ivoire and Kenya (MAVERICK) dataset,\n    an event report level dataset that records all documented instances of \n    electoral violence from the first multiparty election to 2022 in \n    C\u00f4te d'Ivoire (1995-2022) and Kenya (1992-2022).",
    "version": "0.1.1",
    "maintainer": "Sebastian van Baalen <sebastian.van-baalen@pcr.uu.se>",
    "author": "Sebastian van Baalen [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3098-5587>),\n  Kristine H\u00f6glund [aut] (ORCID: <https://orcid.org/0000-0001-7167-609X>)",
    "url": "https://github.com/sebastianvanbaalen/eventreport",
    "bug_reports": "https://github.com/sebastianvanbaalen/eventreport/issues",
    "repository": "https://cran.r-project.org/package=eventreport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eventreport Diagnose, Visualize, and Aggregate Event Report Level Data Diagnose, visualize, and aggregate event report \n    level data to the event level. Users provide an event report level \n    dataset, specify their aggregation rules, and the package produces a\n    dataset aggregated at the event level. Also includes the Modes and \n    Agents of Election-Related Violence in C\u00f4te d'Ivoire and Kenya (MAVERICK) dataset,\n    an event report level dataset that records all documented instances of \n    electoral violence from the first multiparty election to 2022 in \n    C\u00f4te d'Ivoire (1995-2022) and Kenya (1992-2022).  "
  },
  {
    "id": 11995,
    "package_name": "evidenceFactors",
    "title": "Reporting Tools for Sensitivity Analysis of Evidence Factors in\nObservational Studies",
    "description": "Provides tools for integrated sensitivity analysis of evidence factors in observational\n\tstudies. When an observational study allows for multiple independent or nearly\n\tindependent inferences which, if vulnerable, are vulnerable to different biases, we have \n\tmultiple evidence factors.  This package provides methods that respect type I error rate control. \n\tExamples are provided of integrated evidence factors analysis in a longitudinal study with\n\tcontinuous outcome and in a case-control study. \n\tKarmakar, B., French, B., and Small, D. S. (2019)<DOI:10.1093/biomet/asz003>.",
    "version": "1.8",
    "maintainer": "Bikram Karmakar <bkarmakar@ufl.edu>",
    "author": "Bikram Karmakar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=evidenceFactors",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evidenceFactors Reporting Tools for Sensitivity Analysis of Evidence Factors in\nObservational Studies Provides tools for integrated sensitivity analysis of evidence factors in observational\n\tstudies. When an observational study allows for multiple independent or nearly\n\tindependent inferences which, if vulnerable, are vulnerable to different biases, we have \n\tmultiple evidence factors.  This package provides methods that respect type I error rate control. \n\tExamples are provided of integrated evidence factors analysis in a longitudinal study with\n\tcontinuous outcome and in a case-control study. \n\tKarmakar, B., French, B., and Small, D. S. (2019)<DOI:10.1093/biomet/asz003>.  "
  },
  {
    "id": 12007,
    "package_name": "evolution",
    "title": "A Client for 'Evolution Cloud API'",
    "description": "Provides an 'R' interface to the 'Evolution API' <https://evoapicloud.com>, enabling sending and receiving 'WhatsApp' messages directly from 'R'. Functions include sending text, images, documents, stickers, geographic locations, and interactive messages (lists). Also includes 'webhook' parsing utilities and channel health checks.",
    "version": "0.0.1",
    "maintainer": "Andre Leite <leite@castlab.org>",
    "author": "Andre Leite [aut, cre],\n  Hugo Vaconcelos [aut],\n  Diogo Bezerra [aut]",
    "url": "https://github.com/StrategicProjects/evolution/",
    "bug_reports": "https://github.com/StrategicProjects/evolution/issues/",
    "repository": "https://cran.r-project.org/package=evolution",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evolution A Client for 'Evolution Cloud API' Provides an 'R' interface to the 'Evolution API' <https://evoapicloud.com>, enabling sending and receiving 'WhatsApp' messages directly from 'R'. Functions include sending text, images, documents, stickers, geographic locations, and interactive messages (lists). Also includes 'webhook' parsing utilities and channel health checks.  "
  },
  {
    "id": 12027,
    "package_name": "exampletestr",
    "title": "Help for Writing Unit Tests Based on Function Examples",
    "description": "Take the examples written in your documentation of functions\n    and use them to create shells (skeletons which must be manually\n    completed by the user) of test files to be tested with the 'testthat'\n    package. Sort of like python 'doctests' for R.",
    "version": "1.7.3",
    "maintainer": "Rory Nolan <rorynoolan@gmail.com>",
    "author": "Rory Nolan [aut, cre] (ORCID: <https://orcid.org/0000-0002-5239-4043>),\n  Sergi Padilla-Parra [ths] (ORCID:\n    <https://orcid.org/0000-0002-8010-9481>),\n  Thomas Quinn [rev] (ORCID: <https://orcid.org/0000-0003-0286-6329>),\n  Laurent Gatto [rev] (ORCID: <https://orcid.org/0000-0002-1520-2268>)",
    "url": "https://rorynolan.github.io/exampletestr/,\nhttps://github.com/rorynolan/exampletestr#readme",
    "bug_reports": "https://github.com/rorynolan/exampletestr/issues",
    "repository": "https://cran.r-project.org/package=exampletestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exampletestr Help for Writing Unit Tests Based on Function Examples Take the examples written in your documentation of functions\n    and use them to create shells (skeletons which must be manually\n    completed by the user) of test files to be tested with the 'testthat'\n    package. Sort of like python 'doctests' for R.  "
  },
  {
    "id": 12028,
    "package_name": "exams",
    "title": "Automatic Generation of Exams in R",
    "description": "Automatic generation of exams based on exercises in Markdown or LaTeX format,\n\tpossibly including R code for dynamic generation of exercise elements.\n\tExercise types include single-choice and multiple-choice questions, arithmetic problems,\n\tstring questions, and combinations thereof (cloze). Output formats include standalone\n\tfiles (PDF, HTML, Docx, ODT, ...), Moodle XML, QTI 1.2, QTI 2.1, Blackboard, Canvas, OpenOlat, ILIAS, TestVision,\n\tParticify, ARSnova, Kahoot!, Grasple, and TCExam. In addition to fully customizable PDF exams, a standardized PDF format\n\t(NOPS) is provided that can be printed, scanned, and automatically evaluated.",
    "version": "2.4-2",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Bettina Gruen [aut] (ORCID: <https://orcid.org/0000-0001-7265-4773>),\n  Friedrich Leisch [aut] (ORCID: <https://orcid.org/0000-0001-7278-1983>),\n  Nikolaus Umlauf [aut] (ORCID: <https://orcid.org/0000-0003-2160-9803>),\n  Mirko Birbaumer [ctb],\n  Dominik Ernst [ctb],\n  Patrik Keller [ctb],\n  Niels Smits [ctb] (ORCID: <https://orcid.org/0000-0003-3669-9266>),\n  Reto Stauffer [ctb] (ORCID: <https://orcid.org/0000-0002-3798-5507>),\n  Kenji Sato [ctb] (ORCID: <https://orcid.org/0009-0000-4520-2560>),\n  Florian Wickelmaier [ctb],\n  Sebastian Bachler [ctb]",
    "url": "https://www.R-exams.org/",
    "bug_reports": "https://www.R-exams.org/contact/",
    "repository": "https://cran.r-project.org/package=exams",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exams Automatic Generation of Exams in R Automatic generation of exams based on exercises in Markdown or LaTeX format,\n\tpossibly including R code for dynamic generation of exercise elements.\n\tExercise types include single-choice and multiple-choice questions, arithmetic problems,\n\tstring questions, and combinations thereof (cloze). Output formats include standalone\n\tfiles (PDF, HTML, Docx, ODT, ...), Moodle XML, QTI 1.2, QTI 2.1, Blackboard, Canvas, OpenOlat, ILIAS, TestVision,\n\tParticify, ARSnova, Kahoot!, Grasple, and TCExam. In addition to fully customizable PDF exams, a standardized PDF format\n\t(NOPS) is provided that can be printed, scanned, and automatically evaluated.  "
  },
  {
    "id": 12030,
    "package_name": "exams.forge.data",
    "title": "Sample and Precomputed Data for Use with 'exams.forge'",
    "description": "Provides a small collection of datasets supporting Pearson correlation \n    and linear regression analysis. It includes the precomputed dataset 'sos100', \n    with integer values summing to zero and squared sum equal to 100. For other \n    values of 'n' and user-defined parameters, the 'sos()' function from the \n    'exams.forge' package can be used to generate datasets on the fly. In addition, \n    the package contains around 500 R Markdown exercises that illustrate the usage \n    of 'exams.forge' commands.",
    "version": "0.1.2",
    "maintainer": "Sigbert Klinke <sigbert@hu-berlin.de>",
    "author": "Sigbert Klinke [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3337-1863>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=exams.forge.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exams.forge.data Sample and Precomputed Data for Use with 'exams.forge' Provides a small collection of datasets supporting Pearson correlation \n    and linear regression analysis. It includes the precomputed dataset 'sos100', \n    with integer values summing to zero and squared sum equal to 100. For other \n    values of 'n' and user-defined parameters, the 'sos()' function from the \n    'exams.forge' package can be used to generate datasets on the fly. In addition, \n    the package contains around 500 R Markdown exercises that illustrate the usage \n    of 'exams.forge' commands.  "
  },
  {
    "id": 12032,
    "package_name": "exams2forms",
    "title": "Embedding 'exams' Exercises as Forms in 'rmarkdown' or 'quarto'\nDocuments",
    "description": "Automatic generation of quizzes or individual questions as (interactive) forms within 'rmarkdown' or 'quarto' documents based on 'R/exams' exercises.",
    "version": "0.2-0",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Reto Stauffer [aut] (ORCID: <https://orcid.org/0000-0002-3798-5507>),\n  Dale Barr [ctb] (ORCID: <https://orcid.org/0000-0002-1121-4608>),\n  Lisa DeBruine [ctb] (ORCID: <https://orcid.org/0000-0002-7523-5539>),\n  Florian Stampfer [ctb] (ORCID: <https://orcid.org/0000-0003-4980-4272>),\n  Jonas Tscholl [ctb]",
    "url": "https://www.R-exams.org/tutorials/exams2forms/",
    "bug_reports": "https://www.R-exams.org/contact/",
    "repository": "https://cran.r-project.org/package=exams2forms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exams2forms Embedding 'exams' Exercises as Forms in 'rmarkdown' or 'quarto'\nDocuments Automatic generation of quizzes or individual questions as (interactive) forms within 'rmarkdown' or 'quarto' documents based on 'R/exams' exercises.  "
  },
  {
    "id": 12039,
    "package_name": "excerptr",
    "title": "Excerpt Structuring Comments from Your Code File and Set a Table\nof Contents",
    "description": "Ever read or wrote source files containing sectioning comments? \n    If these comments are markdown style section comments,\n    you can excerpt them and set a table of contents using the\n    'python' package 'excerpts' (<https://pypi.org/project/excerpts/>).",
    "version": "2.1.0",
    "maintainer": "Andreas Dominik Cullmann <fvafrcu@mailbox.org>",
    "author": "Andreas Dominik Cullmann [aut, cre]",
    "url": "https://gitlab.com/fvafrcu/excerptr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=excerptr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "excerptr Excerpt Structuring Comments from Your Code File and Set a Table\nof Contents Ever read or wrote source files containing sectioning comments? \n    If these comments are markdown style section comments,\n    you can excerpt them and set a table of contents using the\n    'python' package 'excerpts' (<https://pypi.org/project/excerpts/>).  "
  },
  {
    "id": 12061,
    "package_name": "explore",
    "title": "Simplifies Exploratory Data Analysis",
    "description": "Interactive data exploration with one line of code, automated\n    reporting or use an easy to remember set of tidy functions for low\n    code exploratory data analysis.",
    "version": "1.4.0",
    "maintainer": "Roland Krasser <roland.krasser@gmail.com>",
    "author": "Roland Krasser [aut, cre]",
    "url": "https://rolkra.github.io/explore/,\nhttps://github.com/rolkra/explore",
    "bug_reports": "https://github.com/rolkra/explore/issues",
    "repository": "https://cran.r-project.org/package=explore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "explore Simplifies Exploratory Data Analysis Interactive data exploration with one line of code, automated\n    reporting or use an easy to remember set of tidy functions for low\n    code exploratory data analysis.  "
  },
  {
    "id": 12063,
    "package_name": "export",
    "title": "Streamlined Export of Graphs and Data Tables",
    "description": "Easily export 'R' graphs and statistical output to 'Microsoft\n    Office' / 'LibreOffice', 'Latex' and 'HTML' Documents, using sensible defaults\n    that result in publication-quality output with simple, straightforward commands.\n    Output to 'Microsoft Office' is in editable 'DrawingML' vector format for\n    graphs, and can use corporate template documents for styling. This enables\n    the production of standardized reports and also allows for manual tidy-up\n    of the layout of 'R' graphs in 'Powerpoint' before final publication. Export\n    of graphs is flexible, and functions enable the currently showing R graph\n    or the currently showing 'R' stats object to be exported, but also allow the\n    graphical or tabular output to be passed as objects. The package relies on package\n    'officer' for export to 'Office' documents,and output files are also fully compatible\n    with 'LibreOffice'. Base 'R', 'ggplot2' and 'lattice' plots are supported, as\n    well as a wide variety of 'R' stats objects, via wrappers to xtable(), broom::tidy() \n    and stargazer(), including aov(), lm(), glm(), lme(), glmnet() and coxph() as\n    well as matrices and data frames and many more...",
    "version": "0.3.2",
    "maintainer": "Tom Wenseleers <tom.wenseleers@kuleuven.be>",
    "author": "Tom Wenseleers [aut, cre],\n  Christophe Vanderaa [aut]",
    "url": "",
    "bug_reports": "https://github.com/tomwenseleers/export/issues",
    "repository": "https://cran.r-project.org/package=export",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "export Streamlined Export of Graphs and Data Tables Easily export 'R' graphs and statistical output to 'Microsoft\n    Office' / 'LibreOffice', 'Latex' and 'HTML' Documents, using sensible defaults\n    that result in publication-quality output with simple, straightforward commands.\n    Output to 'Microsoft Office' is in editable 'DrawingML' vector format for\n    graphs, and can use corporate template documents for styling. This enables\n    the production of standardized reports and also allows for manual tidy-up\n    of the layout of 'R' graphs in 'Powerpoint' before final publication. Export\n    of graphs is flexible, and functions enable the currently showing R graph\n    or the currently showing 'R' stats object to be exported, but also allow the\n    graphical or tabular output to be passed as objects. The package relies on package\n    'officer' for export to 'Office' documents,and output files are also fully compatible\n    with 'LibreOffice'. Base 'R', 'ggplot2' and 'lattice' plots are supported, as\n    well as a wide variety of 'R' stats objects, via wrappers to xtable(), broom::tidy() \n    and stargazer(), including aov(), lm(), glm(), lme(), glmnet() and coxph() as\n    well as matrices and data frames and many more...  "
  },
  {
    "id": 12066,
    "package_name": "expss",
    "title": "Tables, Labels and Some Useful Functions from Spreadsheets and\n'SPSS' Statistics",
    "description": "Package computes and displays tables with support for 'SPSS'-style \n        labels, multiple and nested banners, weights, multiple-response variables \n        and significance testing. There are facilities for nice output of tables \n        in 'knitr', 'Shiny', '*.xlsx' files, R and 'Jupyter' notebooks. Methods \n        for labelled variables add value labels support to base R functions and to \n        some functions from other packages. Additionally, the package brings \n        popular data transformation functions from 'SPSS' Statistics and 'Excel': \n        'RECODE', 'COUNT', 'COUNTIF', 'VLOOKUP' and etc. \n        These functions are very useful for data processing in marketing research \n        surveys. Package intended to help people to move data \n        processing from 'Excel' and 'SPSS' to R.",
    "version": "0.11.7",
    "maintainer": "Gregory Demin <gdemin@gmail.com>",
    "author": "Gregory Demin [aut, cre],\n  Sebastian Jeworutzki [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2671-5253>),\n  Dan Chaltiel [ctb],\n  John Williams [ctb],\n  Tom Elliott [ctb]",
    "url": "https://gdemin.github.io/expss/",
    "bug_reports": "https://github.com/gdemin/expss/issues",
    "repository": "https://cran.r-project.org/package=expss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "expss Tables, Labels and Some Useful Functions from Spreadsheets and\n'SPSS' Statistics Package computes and displays tables with support for 'SPSS'-style \n        labels, multiple and nested banners, weights, multiple-response variables \n        and significance testing. There are facilities for nice output of tables \n        in 'knitr', 'Shiny', '*.xlsx' files, R and 'Jupyter' notebooks. Methods \n        for labelled variables add value labels support to base R functions and to \n        some functions from other packages. Additionally, the package brings \n        popular data transformation functions from 'SPSS' Statistics and 'Excel': \n        'RECODE', 'COUNT', 'COUNTIF', 'VLOOKUP' and etc. \n        These functions are very useful for data processing in marketing research \n        surveys. Package intended to help people to move data \n        processing from 'Excel' and 'SPSS' to R.  "
  },
  {
    "id": 12068,
    "package_name": "exreport",
    "title": "Fast, Reliable and Elegant Reproducible Research",
    "description": "Analysis of experimental results and automatic report generation in both interactive HTML and LaTeX. This package ships with a rich interface for data modeling and built in functions for the rapid application of statistical tests and generation of common plots and tables with publish-ready quality.",
    "version": "0.4.1",
    "maintainer": "Jacinto Arias <jacinto.arias@uclm.es>",
    "author": "Jacinto Arias [aut, cre],\n  Javier Cozar [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=exreport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exreport Fast, Reliable and Elegant Reproducible Research Analysis of experimental results and automatic report generation in both interactive HTML and LaTeX. This package ships with a rich interface for data modeling and built in functions for the rapid application of statistical tests and generation of common plots and tables with publish-ready quality.  "
  },
  {
    "id": 12075,
    "package_name": "extlasso",
    "title": "Maximum Penalized Likelihood Estimation with Extended Lasso\nPenalty",
    "description": "Estimates coefficients of extended LASSO penalized linear regression and generalized linear models. Currently lasso and elastic net penalized linear regression and generalized linear models are considered. This package currently utilizes an accurate approximation of L1 penalty and then a modified Jacobi algorithm to estimate the coefficients. There is provision for plotting of the solutions and predictions of coefficients at given values of lambda. This package also contains functions for cross validation to select a suitable lambda value given the data. Also provides a function for estimation in fused lasso penalized linear regression. For more details, see Mandal, B. N.(2014). Computational methods for L1 penalized GLM model fitting, unpublished report submitted to Macquarie University, NSW, Australia.",
    "version": "0.3",
    "maintainer": "B N Mandal <mandal.stat@gmail.com>",
    "author": "B N Mandal <mandal.stat@gmail.com> and Jun Ma <jun.ma@mq.edu.au>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=extlasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extlasso Maximum Penalized Likelihood Estimation with Extended Lasso\nPenalty Estimates coefficients of extended LASSO penalized linear regression and generalized linear models. Currently lasso and elastic net penalized linear regression and generalized linear models are considered. This package currently utilizes an accurate approximation of L1 penalty and then a modified Jacobi algorithm to estimate the coefficients. There is provision for plotting of the solutions and predictions of coefficients at given values of lambda. This package also contains functions for cross validation to select a suitable lambda value given the data. Also provides a function for estimation in fused lasso penalized linear regression. For more details, see Mandal, B. N.(2014). Computational methods for L1 penalized GLM model fitting, unpublished report submitted to Macquarie University, NSW, Australia.  "
  },
  {
    "id": 12078,
    "package_name": "extractFAERS",
    "title": "Extract Data from FAERS Database",
    "description": "Provides functions to extract and process data from the FDA Adverse Event Reporting System (FAERS). It facilitates the conversion of raw FAERS data published after 2014Q3 into structured formats for analysis. See Yang et al. (2022) <doi:10.3389/fphar.2021.772768> for related information. ",
    "version": "0.1.4",
    "maintainer": "Renjun Yang <rjyang@rcees.ac.cn>",
    "author": "Renjun Yang [ctb],\n  Renjun Yang [aut, cre] (ORCID: <https://orcid.org/0000-0002-9353-5041>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=extractFAERS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extractFAERS Extract Data from FAERS Database Provides functions to extract and process data from the FDA Adverse Event Reporting System (FAERS). It facilitates the conversion of raw FAERS data published after 2014Q3 into structured formats for analysis. See Yang et al. (2022) <doi:10.3389/fphar.2021.772768> for related information.   "
  },
  {
    "id": 12101,
    "package_name": "eyeris",
    "title": "Flexible, Extensible, & Reproducible Pupillometry Preprocessing",
    "description": "Pupillometry offers a non-invasive window into the mind and has been used extensively as a psychophysiological readout of arousal signals linked with cognitive processes like attention, stress, and emotional states [Clewett et al. (2020) <doi:10.1038/s41467-020-17851-9>; Kret & Sjak-Shie (2018) <doi:10.3758/s13428-018-1075-y>; Strauch (2024) <doi:10.1016/j.tins.2024.06.002>]. Yet, despite decades of pupillometry research, many established packages and workflows to date lack design patterns based on Findability, Accessibility, Interoperability, and Reusability (FAIR) principles [see Wilkinson et al. (2016) <doi:10.1038/sdata.2016.18>]. 'eyeris' provides a modular, performant, and extensible preprocessing framework for pupillometry data with BIDS-like organization and interactive output reports [Esteban et al. (2019) <doi:10.1038/s41592-018-0235-4>; Gorgolewski et al. (2016) <doi:10.1038/sdata.2016.44>]. Development was supported, in part, by the Stanford Wu Tsai Human Performance Alliance, Stanford Ric Weiland Graduate Fellowship, Stanford Center for Mind, Brain, Computation and Technology, NIH National Institute on Aging Grants (R01-AG065255, R01-AG079345), NSF GRFP (DGE-2146755), McKnight Brain Research Foundation Clinical Translational Research Scholarship in Cognitive Aging and Age-Related Memory Loss, American Brain Foundation, and the American Academy of Neurology.",
    "version": "3.0.1",
    "maintainer": "Shawn Schwartz <shawn.t.schwartz@gmail.com>",
    "author": "Shawn Schwartz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6444-8451>),\n  Mingjian He [ctb],\n  Haopei Yang [ctb],\n  Alice Xue [ctb],\n  Gustavo Santiago-Reyes [ctb]",
    "url": "https://shawnschwartz.com/eyeris/,\nhttps://github.com/shawntz/eyeris/",
    "bug_reports": "https://github.com/shawntz/eyeris/issues",
    "repository": "https://cran.r-project.org/package=eyeris",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eyeris Flexible, Extensible, & Reproducible Pupillometry Preprocessing Pupillometry offers a non-invasive window into the mind and has been used extensively as a psychophysiological readout of arousal signals linked with cognitive processes like attention, stress, and emotional states [Clewett et al. (2020) <doi:10.1038/s41467-020-17851-9>; Kret & Sjak-Shie (2018) <doi:10.3758/s13428-018-1075-y>; Strauch (2024) <doi:10.1016/j.tins.2024.06.002>]. Yet, despite decades of pupillometry research, many established packages and workflows to date lack design patterns based on Findability, Accessibility, Interoperability, and Reusability (FAIR) principles [see Wilkinson et al. (2016) <doi:10.1038/sdata.2016.18>]. 'eyeris' provides a modular, performant, and extensible preprocessing framework for pupillometry data with BIDS-like organization and interactive output reports [Esteban et al. (2019) <doi:10.1038/s41592-018-0235-4>; Gorgolewski et al. (2016) <doi:10.1038/sdata.2016.44>]. Development was supported, in part, by the Stanford Wu Tsai Human Performance Alliance, Stanford Ric Weiland Graduate Fellowship, Stanford Center for Mind, Brain, Computation and Technology, NIH National Institute on Aging Grants (R01-AG065255, R01-AG079345), NSF GRFP (DGE-2146755), McKnight Brain Research Foundation Clinical Translational Research Scholarship in Cognitive Aging and Age-Related Memory Loss, American Brain Foundation, and the American Academy of Neurology.  "
  },
  {
    "id": 12118,
    "package_name": "fChange",
    "title": "Functional Change Point Detection and Analysis",
    "description": "Analyze functional data and its change points. Includes \n    functionality to store and process data, summarize and validate assumptions, \n    characterize and perform inference of change points, and provide \n    visualizations. Data is stored as discretely collected observations without\n    requiring the selection of basis functions. For more details see chapter 8 \n    of Horvath and Rice (2024) <doi:10.1007/978-3-031-51609-2>. Additional papers\n    are forthcoming. Focused works are also included in the documentation of \n    corresponding functions.",
    "version": "2.1.0",
    "maintainer": "Jeremy VanderDoes <jeremy.vanderdoes@gmail.com>",
    "author": "Jeremy VanderDoes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0001-9885-3073>),\n  Gregory Rice [aut],\n  Martin Wendler [aut]",
    "url": "https://jrvanderdoes.github.io/fChange/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fChange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fChange Functional Change Point Detection and Analysis Analyze functional data and its change points. Includes \n    functionality to store and process data, summarize and validate assumptions, \n    characterize and perform inference of change points, and provide \n    visualizations. Data is stored as discretely collected observations without\n    requiring the selection of basis functions. For more details see chapter 8 \n    of Horvath and Rice (2024) <doi:10.1007/978-3-031-51609-2>. Additional papers\n    are forthcoming. Focused works are also included in the documentation of \n    corresponding functions.  "
  },
  {
    "id": 12144,
    "package_name": "fabricerin",
    "title": "Create Easily Canvas in 'shiny' and 'RMarkdown' Documents",
    "description": "Allows the user to implement easily canvas elements within a 'shiny' app or an 'RMarkdown' document. \n    The user can create shapes, images and text elements within the canvas which can also be used as a drawing tool for taking notes.\n    The package relies on the 'fabricjs' 'JavaScript' library. See <http://fabricjs.com/>.",
    "version": "0.1.2",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden [aut, cre],\n  Garrick Aden-Buie [ctb],\n  fabricjs contributors [ctb, cph] (fabricjs JavaScript library),\n  jQuery contributors [ctb, cph] (jQuery JavaScript library),\n  FileSaver.js contributors [ctb, cph] (FileSaver JavaScript library)",
    "url": "https://github.com/feddelegrand7/fabricerin",
    "bug_reports": "https://github.com/feddelegrand7/fabricerin/issues",
    "repository": "https://cran.r-project.org/package=fabricerin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fabricerin Create Easily Canvas in 'shiny' and 'RMarkdown' Documents Allows the user to implement easily canvas elements within a 'shiny' app or an 'RMarkdown' document. \n    The user can create shapes, images and text elements within the canvas which can also be used as a drawing tool for taking notes.\n    The package relies on the 'fabricjs' 'JavaScript' library. See <http://fabricjs.com/>.  "
  },
  {
    "id": 12151,
    "package_name": "facmodCS",
    "title": "Cross-Section Factor Models",
    "description": "Linear cross-section factor model fitting with least-squares \n    and robust fitting the 'lmrobdetMM()' function from 'RobStatTM'; related\n    volatility, Value at Risk and Expected Shortfall risk and performance \n    attribution (factor-contributed vs idiosyncratic returns);\n    tabular displays of risk and performance reports;\n    factor model Monte Carlo. The package authors would like to thank Chicago\n    Research on Security Prices,LLC for the cross-section of about 300\n    CRSP stocks data (in the data.table object 'stocksCRSP', and S&P GLOBAL MARKET\n    INTELLIGENCE for contributing 14 factor scores (a.k.a \"alpha factors\".and\n    \"factor exposures\") fundamental data on the 300 companies in the data.table\n    object 'factorSPGMI'.  The 'stocksCRSP' and 'factorsSPGMI' data are not covered by\n    the GPL-2 license, are not provided as open source of any kind, and they are\n    not to be redistributed in any form.",
    "version": "1.0",
    "maintainer": "Mido Shammaa <midoshammaa@yahoo.com>",
    "author": "Mido Shammaa [aut, cre],\n  Doug Martin [ctb, aut],\n  Kirk Li [aut, ctb],\n  Avinash Acharya [ctb],\n  Lingjie Yi [ctb]",
    "url": "https://github.com/robustport/facmodCS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=facmodCS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "facmodCS Cross-Section Factor Models Linear cross-section factor model fitting with least-squares \n    and robust fitting the 'lmrobdetMM()' function from 'RobStatTM'; related\n    volatility, Value at Risk and Expected Shortfall risk and performance \n    attribution (factor-contributed vs idiosyncratic returns);\n    tabular displays of risk and performance reports;\n    factor model Monte Carlo. The package authors would like to thank Chicago\n    Research on Security Prices,LLC for the cross-section of about 300\n    CRSP stocks data (in the data.table object 'stocksCRSP', and S&P GLOBAL MARKET\n    INTELLIGENCE for contributing 14 factor scores (a.k.a \"alpha factors\".and\n    \"factor exposures\") fundamental data on the 300 companies in the data.table\n    object 'factorSPGMI'.  The 'stocksCRSP' and 'factorsSPGMI' data are not covered by\n    the GPL-2 license, are not provided as open source of any kind, and they are\n    not to be redistributed in any form.  "
  },
  {
    "id": 12160,
    "package_name": "factorH",
    "title": "Multifactor Nonparametric Rank-Based ANOVA with Post Hoc Tests",
    "description": "Multifactor nonparametric analysis of variance based on ranks.\n    Builds on the Kruskal-Wallis H test and its 2x2 Scheirer-Ray-Hare\n    extension to handle any factorial designs. Provides effect sizes,\n    Dunn-Bonferroni pairwise-comparison matrices, and simple-effects\n    analyses. Tailored for psychology and the social sciences, with\n    beginner-friendly R syntax and outputs that can be dropped into\n    journal reports. Includes helpers to export tab-separated results and\n    compact tables of descriptive statistics (to APA-style reports).",
    "version": "0.5.0",
    "maintainer": "Tomasz Rak <tomasz.rak@upjp2.edu.pl>",
    "author": "Tomasz Rak [aut, cre],\n  Szymon Wrzesniowski [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=factorH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "factorH Multifactor Nonparametric Rank-Based ANOVA with Post Hoc Tests Multifactor nonparametric analysis of variance based on ranks.\n    Builds on the Kruskal-Wallis H test and its 2x2 Scheirer-Ray-Hare\n    extension to handle any factorial designs. Provides effect sizes,\n    Dunn-Bonferroni pairwise-comparison matrices, and simple-effects\n    analyses. Tailored for psychology and the social sciences, with\n    beginner-friendly R syntax and outputs that can be dropped into\n    journal reports. Includes helpers to export tab-separated results and\n    compact tables of descriptive statistics (to APA-style reports).  "
  },
  {
    "id": 12167,
    "package_name": "factset.protobuf.stach.v2",
    "title": "'FactSet' 'STACH V2' Library",
    "description": "Generates 'RProtobuf' classes for 'FactSet' 'STACH V2' tabular \n    format which represents complex multi-dimensional array of data. These \n    classes help in the 'serialization' and 'deserialization' of 'STACH V2' \n    formatted data. See 'GitHub' repository documentation for more \n    information.",
    "version": "1.0.6",
    "maintainer": "Charlie Mathis <chmathis@factset.com>",
    "author": "Charlie Mathis [aut, cre]",
    "url": "https://github.com/factset/stachschema-sdks",
    "bug_reports": "https://github.com/factset/stachschema-sdks/issues",
    "repository": "https://cran.r-project.org/package=factset.protobuf.stach.v2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "factset.protobuf.stach.v2 'FactSet' 'STACH V2' Library Generates 'RProtobuf' classes for 'FactSet' 'STACH V2' tabular \n    format which represents complex multi-dimensional array of data. These \n    classes help in the 'serialization' and 'deserialization' of 'STACH V2' \n    formatted data. See 'GitHub' repository documentation for more \n    information.  "
  },
  {
    "id": 12169,
    "package_name": "faersquarterlydata",
    "title": "FDA Adverse Event Reporting System Quarterly Data Extracting\nTool",
    "description": "An easy framework to read FDA Adverse Event Reporting System XML/ASCII files <https://www.fda.gov/drugs/questions-and-answers-fdas-adverse-event-reporting-system-faers/fda-adverse-event-reporting-system-faers-latest-quarterly-data-files>.",
    "version": "1.2.0",
    "maintainer": "Luis Garcez <luisgarcez1@gmail.com>",
    "author": "Luis Garcez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8637-7946>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=faersquarterlydata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "faersquarterlydata FDA Adverse Event Reporting System Quarterly Data Extracting\nTool An easy framework to read FDA Adverse Event Reporting System XML/ASCII files <https://www.fda.gov/drugs/questions-and-answers-fdas-adverse-event-reporting-system-faers/fda-adverse-event-reporting-system-faers-latest-quarterly-data-files>.  "
  },
  {
    "id": 12196,
    "package_name": "faraway",
    "title": "Datasets and Functions for Books by Julian Faraway",
    "description": "Books are \"Linear Models with R\" published 1st Ed. August 2004, 2nd Ed. July 2014, 3rd Ed. February 2025 by CRC press, ISBN 9781439887332, and \"Extending the Linear Model with R\" published by CRC press in 1st Ed. December 2005 and 2nd Ed. March 2016, ISBN 9781584884248 and \"Practical Regression and ANOVA in R\" contributed documentation on CRAN (now very dated).",
    "version": "1.0.9",
    "maintainer": "Julian Faraway <jjf23@bath.ac.uk>",
    "author": "Julian Faraway [aut, cre]",
    "url": "https://github.com/julianfaraway/faraway",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=faraway",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "faraway Datasets and Functions for Books by Julian Faraway Books are \"Linear Models with R\" published 1st Ed. August 2004, 2nd Ed. July 2014, 3rd Ed. February 2025 by CRC press, ISBN 9781439887332, and \"Extending the Linear Model with R\" published by CRC press in 1st Ed. December 2005 and 2nd Ed. March 2016, ISBN 9781584884248 and \"Practical Regression and ANOVA in R\" contributed documentation on CRAN (now very dated).  "
  },
  {
    "id": 12258,
    "package_name": "fastqcr",
    "title": "Quality Control of Sequencing Data",
    "description": "'FASTQC' is the most widely used tool for evaluating the quality of high throughput sequencing data.  \n    It produces, for each sample, an html report and a compressed file containing the raw data. \n    If you have hundreds of samples, you are not going to open up each 'HTML' page. \n    You need some way of looking at these data in aggregate. \n    'fastqcr' Provides helper functions to easily parse, aggregate and analyze \n    'FastQC' reports for large numbers of samples. It provides a convenient solution for building \n    a 'Multi-QC' report, as well as, a 'one-sample' report with result interpretations.",
    "version": "0.1.3",
    "maintainer": "Alboukadel Kassambara <alboukadel.kassambara@gmail.com>",
    "author": "Alboukadel Kassambara [aut, cre]",
    "url": "https://rpkgs.datanovia.com/fastqcr/index.html",
    "bug_reports": "https://github.com/kassambara/fastqcr/issues",
    "repository": "https://cran.r-project.org/package=fastqcr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastqcr Quality Control of Sequencing Data 'FASTQC' is the most widely used tool for evaluating the quality of high throughput sequencing data.  \n    It produces, for each sample, an html report and a compressed file containing the raw data. \n    If you have hundreds of samples, you are not going to open up each 'HTML' page. \n    You need some way of looking at these data in aggregate. \n    'fastqcr' Provides helper functions to easily parse, aggregate and analyze \n    'FastQC' reports for large numbers of samples. It provides a convenient solution for building \n    a 'Multi-QC' report, as well as, a 'one-sample' report with result interpretations.  "
  },
  {
    "id": 12261,
    "package_name": "fastrep",
    "title": "Time-Saving Package for Creating Reports",
    "description": "Provides  templates for reports in 'rmarkdown' and \n     functions to create tables and summaries of data.",
    "version": "0.7",
    "maintainer": "Alisson Rosa <alirpereira887@gmail.com>",
    "author": "Alisson Rosa [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/AlissonRP/fastrep/issues",
    "repository": "https://cran.r-project.org/package=fastrep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastrep Time-Saving Package for Creating Reports Provides  templates for reports in 'rmarkdown' and \n     functions to create tables and summaries of data.  "
  },
  {
    "id": 12271,
    "package_name": "faux",
    "title": "Simulation for Factorial Designs",
    "description": "Create datasets with factorial structure through simulation by specifying variable parameters. Extended documentation at <https://scienceverse.github.io/faux/>. Described in DeBruine (2020) <doi:10.5281/zenodo.2669586>.",
    "version": "1.2.3",
    "maintainer": "Lisa DeBruine <debruine@gmail.com>",
    "author": "Lisa DeBruine [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7523-5539>),\n  Anna Krystalli [ctb] (ORCID: <https://orcid.org/0000-0002-2378-4915>),\n  Andrew Heiss [ctb] (ORCID: <https://orcid.org/0000-0002-3948-3914>)",
    "url": "https://github.com/scienceverse/faux,\nhttps://scienceverse.github.io/faux/",
    "bug_reports": "https://github.com/scienceverse/faux/issues",
    "repository": "https://cran.r-project.org/package=faux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "faux Simulation for Factorial Designs Create datasets with factorial structure through simulation by specifying variable parameters. Extended documentation at <https://scienceverse.github.io/faux/>. Described in DeBruine (2020) <doi:10.5281/zenodo.2669586>.  "
  },
  {
    "id": 12275,
    "package_name": "faviconPlease",
    "title": "Find the URL to the 'Favicon' for a Website",
    "description": "Finds the URL to the 'favicon' for a website. This is useful if you\n  want to display the 'favicon' in an HTML document or web application,\n  especially if the website is behind a firewall.",
    "version": "0.1.4",
    "maintainer": "John Blischak <jdblischak@gmail.com>",
    "author": "John Blischak [aut, cre]",
    "url": "https://github.com/jdblischak/faviconPlease",
    "bug_reports": "https://github.com/jdblischak/faviconPlease/issues",
    "repository": "https://cran.r-project.org/package=faviconPlease",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "faviconPlease Find the URL to the 'Favicon' for a Website Finds the URL to the 'favicon' for a website. This is useful if you\n  want to display the 'favicon' in an HTML document or web application,\n  especially if the website is behind a firewall.  "
  },
  {
    "id": 12287,
    "package_name": "fcall",
    "title": "Parse Farm Credit Administration Call Report Data into Tidy Data\nFrames",
    "description": "Parses financial condition and performance data (Call Reports) for\n    institutions in the United States Farm Credit System. Contains functions for\n    downloading files from the Farm Credit Administration (FCA) Call Report\n    archive website and reading the files into tidy data frame format.\n    The archive website can be found at\n    <https://www.fca.gov/bank-oversight/call-report-data-for-download>.",
    "version": "0.1.6",
    "maintainer": "Michael Thomas <mthomas@ketchbrookanalytics.com>",
    "author": "Michael Thomas [aut, cre],\n  Ivan Millanes [aut],\n  Ketchbrook Analytics [cph, fnd]",
    "url": "https://ketchbrookanalytics.github.io/fcall/,\nhttps://github.com/ketchbrookanalytics/fcall",
    "bug_reports": "https://github.com/ketchbrookanalytics/fcall/issues",
    "repository": "https://cran.r-project.org/package=fcall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fcall Parse Farm Credit Administration Call Report Data into Tidy Data\nFrames Parses financial condition and performance data (Call Reports) for\n    institutions in the United States Farm Credit System. Contains functions for\n    downloading files from the Farm Credit Administration (FCA) Call Report\n    archive website and reading the files into tidy data frame format.\n    The archive website can be found at\n    <https://www.fca.gov/bank-oversight/call-report-data-for-download>.  "
  },
  {
    "id": 12335,
    "package_name": "fec16",
    "title": "Data Package for the 2016 United States Federal Elections",
    "description": "Easily analyze relational data from the United States 2016 federal \n    election cycle as reported by the Federal Election Commission.\n    This package contains data about candidates, committees, and a\n    variety of different financial expenditures. Data is from <https://www.fec.gov/data/browse-data/?tab=bulk-data>. ",
    "version": "0.1.6",
    "maintainer": "Marium Tapal <mariumtapal@gmail.com>",
    "author": "Marium Tapal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5093-6462>),\n  Rana Gahwagy [aut],\n  Irene Ryan [aut],\n  Benjamin S. Baumer [aut] (ORCID:\n    <https://orcid.org/0000-0002-3279-0516>)",
    "url": "https://github.com/baumer-lab/fec16",
    "bug_reports": "https://github.com/baumer-lab/fec16/issues",
    "repository": "https://cran.r-project.org/package=fec16",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fec16 Data Package for the 2016 United States Federal Elections Easily analyze relational data from the United States 2016 federal \n    election cycle as reported by the Federal Election Commission.\n    This package contains data about candidates, committees, and a\n    variety of different financial expenditures. Data is from <https://www.fec.gov/data/browse-data/?tab=bulk-data>.   "
  },
  {
    "id": 12345,
    "package_name": "felp",
    "title": "Functional Help for Functions, Objects, and Packages",
    "description": "\n    Enhance R help system by fuzzy search and preview interface, pseudo-postfix operators, and more.\n    The `?.` pseudo-postfix operator and the `?` prefix operator displays documents and contents (source or structure) of objects simultaneously to help understanding the objects.\n    The `?p` pseudo-postfix operator displays package documents, and is shorter than help(package = foo).",
    "version": "0.6.0",
    "maintainer": "Atsushi Yasumoto <atusy.rpkg@gmail.com>",
    "author": "Atsushi Yasumoto [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-8335-495X>)",
    "url": "https://felp.atusy.net/, https://github.com/atusy/felp",
    "bug_reports": "https://github.com/atusy/felp/issues",
    "repository": "https://cran.r-project.org/package=felp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "felp Functional Help for Functions, Objects, and Packages \n    Enhance R help system by fuzzy search and preview interface, pseudo-postfix operators, and more.\n    The `?.` pseudo-postfix operator and the `?` prefix operator displays documents and contents (source or structure) of objects simultaneously to help understanding the objects.\n    The `?p` pseudo-postfix operator displays package documents, and is shorter than help(package = foo).  "
  },
  {
    "id": 12356,
    "package_name": "ffiec",
    "title": "R Interface to 'FFIEC Central Data Repository REST API' Service",
    "description": "Provides a simplified interface to the Central Data Repository 'REST\n    API' service made available by the United States Federal Financial\n    Institutions Examination Council ('FFIEC'). Contains functions to retrieve\n    reports of Condition and Income (Call Reports) and Uniform Bank Performance\n    Reports ('UBPR') in list or tidy data frame format for most 'FDIC' insured\n    institutions. See\n    <https://cdr.ffiec.gov/public/Files/SIS611_-_Retrieve_Public_Data_via_Web_Service.pdf>\n    for the official 'REST API' documentation published by the 'FFIEC'.",
    "version": "0.1.3",
    "maintainer": "Michael Thomas <mthomas@ketchbrookanalytics.com>",
    "author": "Michael Thomas [aut, cre],\n  Ketchbrook Analytics [cph, fnd]",
    "url": "https://github.com/ketchbrookanalytics/ffiec,\nhttps://ketchbrookanalytics.github.io/ffiec/",
    "bug_reports": "https://github.com/ketchbrookanalytics/ffiec/issues",
    "repository": "https://cran.r-project.org/package=ffiec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ffiec R Interface to 'FFIEC Central Data Repository REST API' Service Provides a simplified interface to the Central Data Repository 'REST\n    API' service made available by the United States Federal Financial\n    Institutions Examination Council ('FFIEC'). Contains functions to retrieve\n    reports of Condition and Income (Call Reports) and Uniform Bank Performance\n    Reports ('UBPR') in list or tidy data frame format for most 'FDIC' insured\n    institutions. See\n    <https://cdr.ffiec.gov/public/Files/SIS611_-_Retrieve_Public_Data_via_Web_Service.pdf>\n    for the official 'REST API' documentation published by the 'FFIEC'.  "
  },
  {
    "id": 12364,
    "package_name": "fgeo",
    "title": "Analyze Forest Diversity and Dynamics",
    "description": "To help you access, transform, analyze, and\n    visualize ForestGEO data, we developed a collection of R packages\n    (<https://forestgeo.github.io/fgeo/>). This package, in particular,\n    helps you to install and load the entire package-collection with a\n    single R command, and provides convenient ways to find relevant\n    documentation. Most commonly, you should not worry about the\n    individual packages that make up the package-collection as you can\n    access all features via this package. To learn more about ForestGEO\n    visit <http://www.forestgeo.si.edu/>.",
    "version": "1.1.4",
    "maintainer": "Mauro Lepore <maurolepore@gmail.com>",
    "author": "Mauro Lepore [aut, ctr, cre] (ORCID:\n    <https://orcid.org/0000-0002-1986-7988>),\n  Gabriel Arellano [aut, rev],\n  Richard Condit [aut],\n  Stuart Davies [aut, rev],\n  Matteo Detto [aut],\n  Erika Gonzalez-Akre [aut, rev] (ORCID:\n    <https://orcid.org/0000-0001-8305-6672>),\n  Pamela Hall [aut],\n  Kyle Harms [aut],\n  Valentine Herrmann [aut, rev] (ORCID:\n    <https://orcid.org/0000-0002-4519-481X>),\n  Aaron Hogan [rev] (ORCID: <https://orcid.org/0000-0001-9806-3074>),\n  Bier Kraichak [rev],\n  David Kenfack [aut, rev],\n  Lauren Krizel [rev],\n  Suzanne Lao [aut, rev],\n  Sean McMahon [aut, rev],\n  Haley Overstreet [rev],\n  Sabrina Russo [aut, rev],\n  Cara Scalpone [rev] (ORCID: <https://orcid.org/0000-0001-8448-2147>),\n  Kristina Anderson-Teixeira [aut, rev],\n  Graham Zemunik [aut, rev],\n  Daniel Zuleta [aut, rev],\n  CTFS-ForestGEO [cph, fnd]",
    "url": "http://forestgeo.github.io/fgeo, https://github.com/forestgeo/fgeo",
    "bug_reports": "https://github.com/forestgeo/fgeo/issues",
    "repository": "https://cran.r-project.org/package=fgeo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fgeo Analyze Forest Diversity and Dynamics To help you access, transform, analyze, and\n    visualize ForestGEO data, we developed a collection of R packages\n    (<https://forestgeo.github.io/fgeo/>). This package, in particular,\n    helps you to install and load the entire package-collection with a\n    single R command, and provides convenient ways to find relevant\n    documentation. Most commonly, you should not worry about the\n    individual packages that make up the package-collection as you can\n    access all features via this package. To learn more about ForestGEO\n    visit <http://www.forestgeo.si.edu/>.  "
  },
  {
    "id": 12376,
    "package_name": "fidelius",
    "title": "Browser-Side Password-Protected HTML Documents",
    "description": "Create secure, encrypted, and password-protected static HTML \n  documents that include the machinery for secure in-browser authentication \n  and decryption.",
    "version": "0.0.2",
    "maintainer": "Matthew T. Warkentin <warkentin@lunenfeld.ca>",
    "author": "Matthew T. Warkentin [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8730-3511>)",
    "url": "https://mattwarkentin.github.io/fidelius/,\nhttps://github.com/mattwarkentin/fidelius",
    "bug_reports": "https://github.com/mattwarkentin/fidelius/issues",
    "repository": "https://cran.r-project.org/package=fidelius",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fidelius Browser-Side Password-Protected HTML Documents Create secure, encrypted, and password-protected static HTML \n  documents that include the machinery for secure in-browser authentication \n  and decryption.  "
  },
  {
    "id": 12400,
    "package_name": "finalfit",
    "title": "Quickly Create Elegant Regression Results Tables and Plots when\nModelling",
    "description": "Generate regression results tables and plots in final \n    format for publication. Explore models and export directly to PDF \n    and 'Word' using 'RMarkdown'. ",
    "version": "1.1.0",
    "maintainer": "Ewen Harrison <ewen.harrison@ed.ac.uk>",
    "author": "Ewen Harrison [aut, cre],\n  Tom Drake [aut],\n  Riinu Pius [aut]",
    "url": "https://github.com/ewenharrison/finalfit",
    "bug_reports": "https://github.com/ewenharrison/finalfit/issues",
    "repository": "https://cran.r-project.org/package=finalfit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "finalfit Quickly Create Elegant Regression Results Tables and Plots when\nModelling Generate regression results tables and plots in final \n    format for publication. Explore models and export directly to PDF \n    and 'Word' using 'RMarkdown'.   "
  },
  {
    "id": 12404,
    "package_name": "findInFiles",
    "title": "Find Pattern in Files",
    "description": "Creates a HTML widget which displays the results of searching\n    for a pattern in files in a given folder. The results can be viewed in \n    the 'RStudio' viewer pane, included in a 'R Markdown' document or in a \n    'Shiny' application. Also provides a 'Shiny' application allowing to run \n    this widget and to navigate in the files found by the search. Instead of\n    creating a HTML widget, it is also possible to get the results of the \n    search in a 'tibble'. The search is performed by the 'grep' command-line \n    utility.",
    "version": "0.5.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Rob Burns [ctb, cph] ('ansi-to-html' library)",
    "url": "https://github.com/stla/findInFiles",
    "bug_reports": "https://github.com/stla/findInFiles/issues",
    "repository": "https://cran.r-project.org/package=findInFiles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "findInFiles Find Pattern in Files Creates a HTML widget which displays the results of searching\n    for a pattern in files in a given folder. The results can be viewed in \n    the 'RStudio' viewer pane, included in a 'R Markdown' document or in a \n    'Shiny' application. Also provides a 'Shiny' application allowing to run \n    this widget and to navigate in the files found by the search. Instead of\n    creating a HTML widget, it is also possible to get the results of the \n    search in a 'tibble'. The search is performed by the 'grep' command-line \n    utility.  "
  },
  {
    "id": 12407,
    "package_name": "findR",
    "title": "Find Code Snippets, R Scripts, R Markdown, PDF and Text Files\nwith Pattern Matching",
    "description": "Scans all directories and subdirectories of a path for code snippets, R scripts,\n    R Markdown, PDF or text files containing a specific pattern.  Files found can be copied to a new folder.",
    "version": "0.2.1",
    "maintainer": "David Zumbach <david.zumbach@gfzb.ch>",
    "author": "David Zumbach [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/zumbov2/findR/issues",
    "repository": "https://cran.r-project.org/package=findR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "findR Find Code Snippets, R Scripts, R Markdown, PDF and Text Files\nwith Pattern Matching Scans all directories and subdirectories of a path for code snippets, R scripts,\n    R Markdown, PDF or text files containing a specific pattern.  Files found can be copied to a new folder.  "
  },
  {
    "id": 12408,
    "package_name": "findSVI",
    "title": "Calculate Social Vulnerability Index for Communities",
    "description": "Developed by CDC/ATSDR (Centers for Disease Control and Prevention/\n    Agency for Toxic Substances and Disease Registry), \n    Social Vulnerability Index (SVI) serves as a tool to assess the resilience \n    of communities by taking into account socioeconomic and demographic factors. \n    Provided with year(s), region(s) and a geographic level of interest, \n    'findSVI' retrieves required variables from US census data and calculates SVI \n    for communities in the specified area based on CDC/ATSDR SVI documentation. \n    Reference for the calculation methods: Flanagan BE, Gregory EW, Hallisey EJ, \n    Heitgerd JL, Lewis B (2011) <doi:10.2202/1547-7355.1792>.",
    "version": "0.2.0",
    "maintainer": "Heli Xu <xuheli91@gmail.com>",
    "author": "Heli Xu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9792-2727>),\n  Ran Li [aut] (ORCID: <https://orcid.org/0000-0002-4699-4755>),\n  Usama Bilal [aut] (ORCID: <https://orcid.org/0000-0002-9868-7773>)",
    "url": "https://github.com/heli-xu/findSVI,\nhttps://heli-xu.github.io/findSVI/",
    "bug_reports": "https://github.com/heli-xu/findSVI/issues",
    "repository": "https://cran.r-project.org/package=findSVI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "findSVI Calculate Social Vulnerability Index for Communities Developed by CDC/ATSDR (Centers for Disease Control and Prevention/\n    Agency for Toxic Substances and Disease Registry), \n    Social Vulnerability Index (SVI) serves as a tool to assess the resilience \n    of communities by taking into account socioeconomic and demographic factors. \n    Provided with year(s), region(s) and a geographic level of interest, \n    'findSVI' retrieves required variables from US census data and calculates SVI \n    for communities in the specified area based on CDC/ATSDR SVI documentation. \n    Reference for the calculation methods: Flanagan BE, Gregory EW, Hallisey EJ, \n    Heitgerd JL, Lewis B (2011) <doi:10.2202/1547-7355.1792>.  "
  },
  {
    "id": 12458,
    "package_name": "fitteR",
    "title": "Fit Hundreds of Theoretical Distributions to Empirical Data",
    "description": "Systematic fit of hundreds of theoretical univariate distributions to empirical data via maximum likelihood estimation. Fits are reported and summarized by a data.frame, a csv file or a 'shiny' app (here with additional features like visual representation of fits). All output formats provide assessment of goodness-of-fit by the following methods: Kolmogorov-Smirnov test, Shapiro-Wilks test, Anderson-Darling test.",
    "version": "0.2.0",
    "maintainer": "Markus Boenn <markus.boenn.sf@gmail.com>",
    "author": "Markus Boenn",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fitteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fitteR Fit Hundreds of Theoretical Distributions to Empirical Data Systematic fit of hundreds of theoretical univariate distributions to empirical data via maximum likelihood estimation. Fits are reported and summarized by a data.frame, a csv file or a 'shiny' app (here with additional features like visual representation of fits). All output formats provide assessment of goodness-of-fit by the following methods: Kolmogorov-Smirnov test, Shapiro-Wilks test, Anderson-Darling test.  "
  },
  {
    "id": 12463,
    "package_name": "fixerapi",
    "title": "An R Client for the \"Fixer.io\" Currency API",
    "description": "An R client for the \"fixer.io\" currency conversion and exchange \n  rate API. The API requires registration and some features are only available \n  on paid accounts. The full API documentation is available at \n  <https://fixer.io/documentation>.",
    "version": "0.1.6",
    "maintainer": "Evan Odell <evanodell91@gmail.com>",
    "author": "Evan Odell [aut, cre] (ORCID: <https://orcid.org/0000-0003-1845-808X>)",
    "url": "https://docs.evanodell.com/fixerapi",
    "bug_reports": "https://github.com/evanodell/fixerapi/issues",
    "repository": "https://cran.r-project.org/package=fixerapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fixerapi An R Client for the \"Fixer.io\" Currency API An R client for the \"fixer.io\" currency conversion and exchange \n  rate API. The API requires registration and some features are only available \n  on paid accounts. The full API documentation is available at \n  <https://fixer.io/documentation>.  "
  },
  {
    "id": 12471,
    "package_name": "flagr",
    "title": "Implementation of Flag Aggregation",
    "description": "Three methods are implemented in R to facilitate the aggregations of flags in official statistics.  \n            From the underlying flags the highest in the hierarchy, the most frequent, or with the highest total weight\n            is propagated to the flag(s) for EU or other aggregates. Below there are some reference documents for the topic: \n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_v2_1.docx>,\n            <https://sdmx.org/wp-content/uploads/CL_CONF_STATUS_1_2_2018.docx>,\n            <http://ec.europa.eu/eurostat/data/database/information>,\n            <http://www.oecd.org/sdd/33869551.pdf>,\n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_implementation_20-10-2014.pdf>.",
    "version": "0.3.2",
    "maintainer": "M\u00e1ty\u00e1s M\u00e9sz\u00e1ros <matyas.meszaros@ec.europa.eu>",
    "author": "M\u00e1ty\u00e1s M\u00e9sz\u00e1ros [aut, cre],\n  Matteo Salvati [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=flagr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flagr Implementation of Flag Aggregation Three methods are implemented in R to facilitate the aggregations of flags in official statistics.  \n            From the underlying flags the highest in the hierarchy, the most frequent, or with the highest total weight\n            is propagated to the flag(s) for EU or other aggregates. Below there are some reference documents for the topic: \n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_v2_1.docx>,\n            <https://sdmx.org/wp-content/uploads/CL_CONF_STATUS_1_2_2018.docx>,\n            <http://ec.europa.eu/eurostat/data/database/information>,\n            <http://www.oecd.org/sdd/33869551.pdf>,\n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_implementation_20-10-2014.pdf>.  "
  },
  {
    "id": 12486,
    "package_name": "flattabler",
    "title": "Obtaining a Flat Table from Pivot Tables",
    "description": "Transformations that allow obtaining a flat table from\n    reports in text or Excel format that contain data in the form of pivot\n    tables. They can be defined for a single report and applied to a set\n    of reports.",
    "version": "2.1.2",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/flattabler/,\nhttps://github.com/josesamos/flattabler",
    "bug_reports": "https://github.com/josesamos/flattabler/issues",
    "repository": "https://cran.r-project.org/package=flattabler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flattabler Obtaining a Flat Table from Pivot Tables Transformations that allow obtaining a flat table from\n    reports in text or Excel format that contain data in the form of pivot\n    tables. They can be defined for a single report and applied to a set\n    of reports.  "
  },
  {
    "id": 12497,
    "package_name": "flexlsx",
    "title": "Exporting 'flextable' to 'xlsx' Files",
    "description": "Exports 'flextable' objects to 'xlsx' files, \n    utilizing functionalities provided by 'flextable' and 'openxlsx2'.",
    "version": "0.3.5",
    "maintainer": "Tobias Heidler <flexlsx@heidler.ovh>",
    "author": "Tobias Heidler [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9193-0980>)",
    "url": "https://github.com/pteridin/flexlsx",
    "bug_reports": "https://github.com/pteridin/flexlsx/issues",
    "repository": "https://cran.r-project.org/package=flexlsx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flexlsx Exporting 'flextable' to 'xlsx' Files Exports 'flextable' objects to 'xlsx' files, \n    utilizing functionalities provided by 'flextable' and 'openxlsx2'.  "
  },
  {
    "id": 12505,
    "package_name": "flexsiteboard",
    "title": "Breaks Single Page Applications from 'flexdashboard' in Multiple\nFiles",
    "description": "A drop-in replacement for 'flexdashboard' 'Rmd' documents, which\n  implements an after-knit-hook to split the generated single page application\n  in one document per main section to reduce rendering load in the web browser\n  displaying the document. Put all 'JavaScript' stuff needed in all sections \n  before the first headline featuring navigation menu attributes. This package \n  is experimental and maybe replaced by a solution inside 'flexdashboard'.",
    "version": "0.0.7",
    "maintainer": "Stephan Struckmann <stephan.struckmann@uni-greifswald.de>",
    "author": "University Medicine Greifswald [cph],\n  Stephan Struckmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8565-7962>)",
    "url": "https://gitlab.com/libreumg/flexsiteboard/",
    "bug_reports": "https://gitlab.com/libreumg/flexsiteboard/-/issues",
    "repository": "https://cran.r-project.org/package=flexsiteboard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flexsiteboard Breaks Single Page Applications from 'flexdashboard' in Multiple\nFiles A drop-in replacement for 'flexdashboard' 'Rmd' documents, which\n  implements an after-knit-hook to split the generated single page application\n  in one document per main section to reduce rendering load in the web browser\n  displaying the document. Put all 'JavaScript' stuff needed in all sections \n  before the first headline featuring navigation menu attributes. This package \n  is experimental and maybe replaced by a solution inside 'flexdashboard'.  "
  },
  {
    "id": 12508,
    "package_name": "flextable",
    "title": "Functions for Tabular Reporting",
    "description": "Use a grammar for creating and customizing pretty tables.\n    The following formats are supported: 'HTML', 'PDF', 'RTF', 'Microsoft\n    Word', 'Microsoft PowerPoint' and R 'Grid Graphics'.  'R Markdown',\n    'Quarto' and the package 'officer' can be used to produce the result\n    files. The syntax is the same for the user regardless of the type of\n    output to be produced. A set of functions allows the creation,\n    definition of cell arrangement, addition of headers or footers,\n    formatting and definition of cell content with text and or images. The\n    package also offers a set of high-level functions that allow tabular\n    reporting of statistical models and the creation of complex cross\n    tabulations.",
    "version": "0.9.10",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  ArData [cph],\n  Clementine Jager [ctb],\n  Eli Daniels [ctb],\n  Panagiotis Skintzos [aut],\n  Quentin Fazilleau [ctb],\n  Maxim Nazarov [ctb],\n  Titouan Robert [ctb],\n  Michael Barrowman [ctb],\n  Atsushi Yasumoto [ctb],\n  Paul Julian [ctb],\n  Sean Browning [ctb],\n  R\u00e9mi Th\u00e9riault [ctb] (ORCID: <https://orcid.org/0000-0003-4315-6788>),\n  Samuel Jobert [ctb],\n  Keith Newman [ctb]",
    "url": "https://ardata-fr.github.io/flextable-book/,\nhttps://davidgohel.github.io/flextable/",
    "bug_reports": "https://github.com/davidgohel/flextable/issues",
    "repository": "https://cran.r-project.org/package=flextable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flextable Functions for Tabular Reporting Use a grammar for creating and customizing pretty tables.\n    The following formats are supported: 'HTML', 'PDF', 'RTF', 'Microsoft\n    Word', 'Microsoft PowerPoint' and R 'Grid Graphics'.  'R Markdown',\n    'Quarto' and the package 'officer' can be used to produce the result\n    files. The syntax is the same for the user regardless of the type of\n    output to be produced. A set of functions allows the creation,\n    definition of cell arrangement, addition of headers or footers,\n    formatting and definition of cell content with text and or images. The\n    package also offers a set of high-level functions that allow tabular\n    reporting of statistical models and the creation of complex cross\n    tabulations.  "
  },
  {
    "id": 12518,
    "package_name": "flipdownr",
    "title": "Implement a Countdown in 'RMarkdown' Documents and 'shiny'\nApplications",
    "description": "Allows the user to create a countdown in 'RMarkdown' documents and 'shiny' applications. \n    The package is a wrapper of the 'JavaScript' library 'flipdown.js'. See <https://pbutcher.uk/flipdown/> for more info.",
    "version": "0.1.1",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden [aut, cre],\n  Butcher Peter [ctb, cph] (flipdown.js library developer)",
    "url": "https://github.com/feddelegrand7/flipdownr",
    "bug_reports": "https://github.com/feddelegrand7/flipdownr/issues",
    "repository": "https://cran.r-project.org/package=flipdownr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flipdownr Implement a Countdown in 'RMarkdown' Documents and 'shiny'\nApplications Allows the user to create a countdown in 'RMarkdown' documents and 'shiny' applications. \n    The package is a wrapper of the 'JavaScript' library 'flipdown.js'. See <https://pbutcher.uk/flipdown/> for more info.  "
  },
  {
    "id": 12583,
    "package_name": "foreSIGHT",
    "title": "Systems Insights from Generation of Hydroclimatic Timeseries",
    "description": "A tool to create hydroclimate scenarios, stress test systems and visualize system performance in scenario-neutral climate change impact assessments. Scenario-neutral approaches 'stress-test' the performance of a modelled system by applying a wide range of plausible hydroclimate conditions (see Brown & Wilby (2012) <doi:10.1029/2012EO410001> and Prudhomme et al. (2010) <doi:10.1016/j.jhydrol.2010.06.043>). These approaches allow the identification of hydroclimatic variables that affect the vulnerability of a system to hydroclimate variation and change. This tool enables the generation of perturbed time series using a range of approaches including simple scaling of observed time series (e.g. Culley et al. (2016) <doi:10.1002/2015WR018253>) and stochastic simulation of perturbed time series via an inverse approach (see Guo et al. (2018) <doi:10.1016/j.jhydrol.2016.03.025>). It incorporates 'Richardson-type' weather generator model configurations documented in Richardson (1981) <doi:10.1029/WR017i001p00182>, Richardson and Wright (1984), as well as latent variable type model configurations documented in Bennett et al. (2018) <doi:10.1016/j.jhydrol.2016.12.043>, Rasmussen (2013) <doi:10.1002/wrcr.20164>, Bennett et al. (2019) <doi:10.5194/hess-23-4783-2019> to generate hydroclimate variables on a daily basis (e.g. precipitation, temperature, potential evapotranspiration) and allows a variety of different hydroclimate variable properties, herein called attributes, to be perturbed. Options are included for the easy integration of existing system models both internally in R and externally for seamless 'stress-testing'. A suite of visualization options for the results of a scenario-neutral analysis (e.g. plotting performance spaces and overlaying climate projection information) are also included. Version 1.0 of this package is described in Bennett et al. (2021) <doi:10.1016/j.envsoft.2021.104999>. As further developments in scenario-neutral approaches occur the tool will be updated to incorporate these advances.",
    "version": "2.0.0",
    "maintainer": "David McInerney <david.mcinerney@adelaide.edu.au>",
    "author": "Bree Bennett [aut] (ORCID: <https://orcid.org/0000-0002-2131-088X>),\n  David McInerney [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4876-8281>),\n  Sam Culley [aut] (ORCID: <https://orcid.org/0000-0003-4798-8522>),\n  Anjana Devanand [aut] (ORCID: <https://orcid.org/0000-0001-9422-3894>),\n  Seth Westra [aut] (ORCID: <https://orcid.org/0000-0003-4023-6061>),\n  Danlu Guo [ctb] (ORCID: <https://orcid.org/0000-0003-1083-1214>),\n  Holger Maier [ths] (ORCID: <https://orcid.org/0000-0002-0277-6887>)",
    "url": "",
    "bug_reports": "https://github.com/ClimateAnalytics/foreSIGHT/issues",
    "repository": "https://cran.r-project.org/package=foreSIGHT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "foreSIGHT Systems Insights from Generation of Hydroclimatic Timeseries A tool to create hydroclimate scenarios, stress test systems and visualize system performance in scenario-neutral climate change impact assessments. Scenario-neutral approaches 'stress-test' the performance of a modelled system by applying a wide range of plausible hydroclimate conditions (see Brown & Wilby (2012) <doi:10.1029/2012EO410001> and Prudhomme et al. (2010) <doi:10.1016/j.jhydrol.2010.06.043>). These approaches allow the identification of hydroclimatic variables that affect the vulnerability of a system to hydroclimate variation and change. This tool enables the generation of perturbed time series using a range of approaches including simple scaling of observed time series (e.g. Culley et al. (2016) <doi:10.1002/2015WR018253>) and stochastic simulation of perturbed time series via an inverse approach (see Guo et al. (2018) <doi:10.1016/j.jhydrol.2016.03.025>). It incorporates 'Richardson-type' weather generator model configurations documented in Richardson (1981) <doi:10.1029/WR017i001p00182>, Richardson and Wright (1984), as well as latent variable type model configurations documented in Bennett et al. (2018) <doi:10.1016/j.jhydrol.2016.12.043>, Rasmussen (2013) <doi:10.1002/wrcr.20164>, Bennett et al. (2019) <doi:10.5194/hess-23-4783-2019> to generate hydroclimate variables on a daily basis (e.g. precipitation, temperature, potential evapotranspiration) and allows a variety of different hydroclimate variable properties, herein called attributes, to be perturbed. Options are included for the easy integration of existing system models both internally in R and externally for seamless 'stress-testing'. A suite of visualization options for the results of a scenario-neutral analysis (e.g. plotting performance spaces and overlaying climate projection information) are also included. Version 1.0 of this package is described in Bennett et al. (2021) <doi:10.1016/j.envsoft.2021.104999>. As further developments in scenario-neutral approaches occur the tool will be updated to incorporate these advances.  "
  },
  {
    "id": 12618,
    "package_name": "formatdown",
    "title": "Formatting Numbers in 'rmarkdown' Documents",
    "description": "Provides a small set of tools for formatting numbers in R-markdown \n    documents. Convert a numerical vector to character strings in power-of-ten \n    form, decimal form, or measurement-units form; all are math-delimited for \n    rendering as inline equations. Can also convert text into math-delimited \n    text to match the font face and size of math-delimited numbers. Useful for \n    rendering single numbers in inline R code chunks and for rendering columns \n    in tables.   ",
    "version": "0.1.4",
    "maintainer": "Richard Layton <graphdoctor@gmail.com>",
    "author": "Richard Layton [aut, cre]",
    "url": "https://github.com/graphdr/formatdown/,\nhttps://graphdr.github.io/formatdown/,\nhttps://CRAN.R-project.org/package=formatdown",
    "bug_reports": "https://github.com/graphdr/formatdown/issues",
    "repository": "https://cran.r-project.org/package=formatdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "formatdown Formatting Numbers in 'rmarkdown' Documents Provides a small set of tools for formatting numbers in R-markdown \n    documents. Convert a numerical vector to character strings in power-of-ten \n    form, decimal form, or measurement-units form; all are math-delimited for \n    rendering as inline equations. Can also convert text into math-delimited \n    text to match the font face and size of math-delimited numbers. Useful for \n    rendering single numbers in inline R code chunks and for rendering columns \n    in tables.     "
  },
  {
    "id": 12623,
    "package_name": "formulaic",
    "title": "Dynamic Generation and Quality Checks of Formula Objects",
    "description": "Many statistical models and analyses in R are implemented through formula objects. The formulaic package creates a unified approach for programmatically and dynamically generating formula objects. Users may specify the outcome and inputs of a model directly, search for variables to include based upon naming patterns, incorporate interactions, and identify variables to exclude. A wide range of quality checks are implemented to identify issues such as misspecified variables, duplication, a lack of contrast in the inputs, and a large number of levels in categorical data.  Variables that do not meet these quality checks can be automatically excluded from the model.  These issues are documented and reported in a manner that provides greater accountability and useful information to guide an investigation of the data.",
    "version": "0.0.8",
    "maintainer": "Anderson Nelson <an2908@columbia.edu>",
    "author": "David Shilane [aut],\n  Anderson Nelson [aut, ctb, cre],\n  Caffrey Lee [aut, ctb],\n  Zichen Huang [aut, ctb]",
    "url": "https://dachosen1.github.io/formulaic/index.html",
    "bug_reports": "https://github.com/dachosen1/formulaic/issues",
    "repository": "https://cran.r-project.org/package=formulaic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "formulaic Dynamic Generation and Quality Checks of Formula Objects Many statistical models and analyses in R are implemented through formula objects. The formulaic package creates a unified approach for programmatically and dynamically generating formula objects. Users may specify the outcome and inputs of a model directly, search for variables to include based upon naming patterns, incorporate interactions, and identify variables to exclude. A wide range of quality checks are implemented to identify issues such as misspecified variables, duplication, a lack of contrast in the inputs, and a large number of levels in categorical data.  Variables that do not meet these quality checks can be automatically excluded from the model.  These issues are documented and reported in a manner that provides greater accountability and useful information to guide an investigation of the data.  "
  },
  {
    "id": 12650,
    "package_name": "fpow",
    "title": "Computing the Noncentrality Parameter of the Noncentral F\nDistribution",
    "description": "Returns the noncentrality parameter of the noncentral F\n        distribution if probability of type I and type II error, degrees\n        of freedom of the numerator and the denominator are given.  It\n        may be useful for computing minimal detectable differences for\n        general ANOVA models.  This program is documented in the paper\n        of A. Baharev, S. Kemeny, On the computation of the noncentral\n        F and noncentral beta distribution; Statistics and Computing,\n        2008, 18 (3), 333-340.",
    "version": "0.0-3",
    "maintainer": "Ali Baharev <ali.baharev@gmail.com>",
    "author": "Ali Baharev [aut, cre]",
    "url": "https://doi.org/10.1007/s11222-008-9061-3,\nhttps://baharev.info/publications/ncbeta.pdf",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fpow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fpow Computing the Noncentrality Parameter of the Noncentral F\nDistribution Returns the noncentrality parameter of the noncentral F\n        distribution if probability of type I and type II error, degrees\n        of freedom of the numerator and the denominator are given.  It\n        may be useful for computing minimal detectable differences for\n        general ANOVA models.  This program is documented in the paper\n        of A. Baharev, S. Kemeny, On the computation of the noncentral\n        F and noncentral beta distribution; Statistics and Computing,\n        2008, 18 (3), 333-340.  "
  },
  {
    "id": 12687,
    "package_name": "freecurrencyapi",
    "title": "Client for the 'freecurrencyapi.com' Currency Conversion API",
    "description": "An R client for the 'freecurrencyapi.com' currency conversion API. The API requires registration of an API key. You can find the full API documentation at <https://freecurrencyapi.com/docs> .",
    "version": "0.1.0",
    "maintainer": "Dominik Kukacka <dominik@everapi.com>",
    "author": "Dominik Kukacka [aut, cre]",
    "url": "https://freecurrencyapi.com, https://freecurrencyapi.com/docs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=freecurrencyapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "freecurrencyapi Client for the 'freecurrencyapi.com' Currency Conversion API An R client for the 'freecurrencyapi.com' currency conversion API. The API requires registration of an API key. You can find the full API documentation at <https://freecurrencyapi.com/docs> .  "
  },
  {
    "id": 12692,
    "package_name": "freewall",
    "title": "A Wrapper of the JavaScript Library 'Freewall'",
    "description": "Creates dynamic grid layouts of images that can be included in \n   'Shiny' applications and R markdown documents.",
    "version": "1.0.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Minh Nguyen [cph] (author of the JavaScript library 'Freewall')",
    "url": "https://github.com/stla/freewall",
    "bug_reports": "https://github.com/stla/freewall/issues",
    "repository": "https://cran.r-project.org/package=freewall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "freewall A Wrapper of the JavaScript Library 'Freewall' Creates dynamic grid layouts of images that can be included in \n   'Shiny' applications and R markdown documents.  "
  },
  {
    "id": 12711,
    "package_name": "froggeR",
    "title": "Enhance 'Quarto' Project Workflows and Standards",
    "description": "Streamlines 'Quarto' workflows by providing tools for consistent project setup and documentation. Enables portability through reusable metadata, automated project structure creation, and standardized templates. Features include enhanced project initialization, pre-formatted 'Quarto' documents, inclusion of 'Quarto' brand functionality, comprehensive data protection settings, custom styling, and structured documentation generation. Designed to improve efficiency and collaboration in R data science projects by reducing repetitive setup tasks while maintaining consistent formatting across multiple documents. ",
    "version": "0.6.0",
    "maintainer": "Kyle Grealis <kyleGrealis@icloud.com>",
    "author": "Kyle Grealis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9223-8854>)",
    "url": "https://www.kyleGrealis.com/froggeR/",
    "bug_reports": "https://github.com/kyleGrealis/froggeR/issues",
    "repository": "https://cran.r-project.org/package=froggeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "froggeR Enhance 'Quarto' Project Workflows and Standards Streamlines 'Quarto' workflows by providing tools for consistent project setup and documentation. Enables portability through reusable metadata, automated project structure creation, and standardized templates. Features include enhanced project initialization, pre-formatted 'Quarto' documents, inclusion of 'Quarto' brand functionality, comprehensive data protection settings, custom styling, and structured documentation generation. Designed to improve efficiency and collaboration in R data science projects by reducing repetitive setup tasks while maintaining consistent formatting across multiple documents.   "
  },
  {
    "id": 12723,
    "package_name": "fsia",
    "title": "Import and Analysis of OMR Data from FormScanner",
    "description": "Import data of tests and questionnaires from FormScanner. FormScanner is an open source software that converts scanned images to data using optical mark recognition (OMR) and it can be downloaded from <http://sourceforge.net/projects/formscanner/>. The spreadsheet file created by FormScanner is imported in a convenient format to perform the analyses provided by the package. These analyses include the conversion of multiple responses to binary (correct/incorrect) data, the computation of the number of corrected responses for each subject or item, scoring using weights,the computation and the graphical representation of the frequencies of the responses to each item and the report of the responses of a few subjects.",
    "version": "1.1.1",
    "maintainer": "Michela Battauz <michela.battauz@uniud.it>",
    "author": "Michela Battauz\t",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fsia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fsia Import and Analysis of OMR Data from FormScanner Import data of tests and questionnaires from FormScanner. FormScanner is an open source software that converts scanned images to data using optical mark recognition (OMR) and it can be downloaded from <http://sourceforge.net/projects/formscanner/>. The spreadsheet file created by FormScanner is imported in a convenient format to perform the analyses provided by the package. These analyses include the conversion of multiple responses to binary (correct/incorrect) data, the computation of the number of corrected responses for each subject or item, scoring using weights,the computation and the graphical representation of the frequencies of the responses to each item and the report of the responses of a few subjects.  "
  },
  {
    "id": 12735,
    "package_name": "ftExtra",
    "title": "Extensions for 'Flextable'",
    "description": "Build display tables easily by extending the functionality of the\n    'flextable' package. Features include spanning header, grouping rows,\n    parsing markdown and so on.",
    "version": "0.6.4",
    "maintainer": "Atsushi Yasumoto <atusy.rpkg@gmail.com>",
    "author": "Atsushi Yasumoto [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-8335-495X>),\n  David Gohel [ctb],\n  Romain Fran\u00e7ois [ctb] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Tatsuya Shima [ctb]",
    "url": "https://ftextra.atusy.net, https://github.com/atusy/ftExtra",
    "bug_reports": "https://github.com/atusy/ftExtra/issues",
    "repository": "https://cran.r-project.org/package=ftExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ftExtra Extensions for 'Flextable' Build display tables easily by extending the functionality of the\n    'flextable' package. Features include spanning header, grouping rows,\n    parsing markdown and so on.  "
  },
  {
    "id": 12742,
    "package_name": "fuel",
    "title": "Framework for Unified Estimation in Lognormal Models",
    "description": "Lognormal models have broad applications in various research areas such as economics, actuarial science, biology, environmental science and psychology. The estimation problem in lognormal models has been extensively studied. This R package 'fuel' implements thirty-nine existing and newly proposed estimators. See Zhang, F., and Gou, J. (2020), A unified framework for estimation in lognormal models, Technical report. ",
    "version": "1.2.0",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou and Fengqing (Zoe) Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fuel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fuel Framework for Unified Estimation in Lognormal Models Lognormal models have broad applications in various research areas such as economics, actuarial science, biology, environmental science and psychology. The estimation problem in lognormal models has been extensively studied. This R package 'fuel' implements thirty-nine existing and newly proposed estimators. See Zhang, F., and Gou, J. (2020), A unified framework for estimation in lognormal models, Technical report.   "
  },
  {
    "id": 12778,
    "package_name": "funviewR",
    "title": "Visualize Function Call Dependencies in R Source Code",
    "description": "Provides tools to analyze R source code and detect function definitions\n    and their internal dependencies across multiple files. Creates interactive network\n    visualizations using 'visNetwork' to display function call relationships, with\n    detailed tooltips showing function arguments, return values, and documentation.\n    Supports both individual files and directory-based analysis with automatic file\n    detection. Useful for understanding code structure, identifying dependencies,\n    and documenting R projects.",
    "version": "0.1.1",
    "maintainer": "Chathura Jayalath <acj.chathura@gmail.com>",
    "author": "Chathura Jayalath [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2056-0211>)",
    "url": "https://github.com/deamonpog/funviewR",
    "bug_reports": "https://github.com/deamonpog/funviewR/issues",
    "repository": "https://cran.r-project.org/package=funviewR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "funviewR Visualize Function Call Dependencies in R Source Code Provides tools to analyze R source code and detect function definitions\n    and their internal dependencies across multiple files. Creates interactive network\n    visualizations using 'visNetwork' to display function call relationships, with\n    detailed tooltips showing function arguments, return values, and documentation.\n    Supports both individual files and directory-based analysis with automatic file\n    detection. Useful for understanding code structure, identifying dependencies,\n    and documenting R projects.  "
  },
  {
    "id": 12784,
    "package_name": "fusen",
    "title": "Build a Package from Rmarkdown Files",
    "description": "Use Rmarkdown First method to build your package. Start your\n    package with documentation, functions, examples and tests in the same\n    unique file. Everything can be set from the Rmarkdown template file\n    provided in your project, then inflated as a package. Inflating the\n    template copies the relevant chunks and sections in the appropriate\n    files required for package development.",
    "version": "0.7.2",
    "maintainer": "Vincent Guyader <vincent@thinkr.fr>",
    "author": "Sebastien Rochette [aut] (ORCID:\n    <https://orcid.org/0000-0002-1565-9313>, creator),\n  Vincent Guyader [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0671-9270>),\n  Yohann Mansiaux [aut],\n  ThinkR [cph]",
    "url": "https://thinkr-open.github.io/fusen/,\nhttps://github.com/Thinkr-open/fusen",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fusen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fusen Build a Package from Rmarkdown Files Use Rmarkdown First method to build your package. Start your\n    package with documentation, functions, examples and tests in the same\n    unique file. Everything can be set from the Rmarkdown template file\n    provided in your project, then inflated as a package. Inflating the\n    template copies the relevant chunks and sections in the appropriate\n    files required for package development.  "
  },
  {
    "id": 12798,
    "package_name": "fuzzr",
    "title": "Fuzz-Test R Functions",
    "description": "Test function arguments with a wide array of inputs, and produce\n  reports summarizing messages, warnings, errors, and returned values.",
    "version": "0.2.2",
    "maintainer": "Matthew Lincoln <matthew.d.lincoln@gmail.com>",
    "author": "Matthew Lincoln [aut, cre]",
    "url": "https://github.com/mdlincoln/fuzzr",
    "bug_reports": "https://github.com/mdlincoln/fuzzr/issues",
    "repository": "https://cran.r-project.org/package=fuzzr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fuzzr Fuzz-Test R Functions Test function arguments with a wide array of inputs, and produce\n  reports summarizing messages, warnings, errors, and returned values.  "
  },
  {
    "id": 12826,
    "package_name": "gRain",
    "title": "Bayesian Networks",
    "description": "Probability propagation in Bayesian networks, also known as graphical independence networks. Documentation\n    of the package is provided in vignettes included in the package and in\n    the paper by H\u00f8jsgaard (2012, <doi:10.18637/jss.v046.i10>).\n    See 'citation(\"gRain\")' for details. ",
    "version": "1.4.5",
    "maintainer": "S\u00f8ren H\u00f8jsgaard <sorenh@math.aau.dk>",
    "author": "S\u00f8ren H\u00f8jsgaard [aut, cre]",
    "url": "https://people.math.aau.dk/~sorenh/software/gR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gRain",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gRain Bayesian Networks Probability propagation in Bayesian networks, also known as graphical independence networks. Documentation\n    of the package is provided in vignettes included in the package and in\n    the paper by H\u00f8jsgaard (2012, <doi:10.18637/jss.v046.i10>).\n    See 'citation(\"gRain\")' for details.   "
  },
  {
    "id": 12828,
    "package_name": "gRbase",
    "title": "A Package for Graphical Modelling in R",
    "description": "The 'gRbase' package provides graphical modelling features\n    used by e.g. the packages 'gRain', 'gRim' and 'gRc'. 'gRbase' implements\n    graph algorithms including (i) maximum cardinality search (for marked\n    and unmarked graphs).\n    (ii) moralization, (iii) triangulation, (iv) creation of junction tree.\n    'gRbase' facilitates array operations,\n    'gRbase' implements functions for testing for conditional independence.\n    'gRbase' illustrates how hierarchical log-linear models may be\n    implemented and describes concept of graphical meta\n    data. \n    The facilities of the package are documented in the book by H\u00f8jsgaard,\n    Edwards and Lauritzen (2012,\n    <doi:10.1007/978-1-4614-2299-0>) and in the paper by \n    Dethlefsen and H\u00f8jsgaard, (2005, <doi:10.18637/jss.v014.i17>).\n    Please see 'citation(\"gRbase\")' for citation details. ",
    "version": "2.0.3",
    "maintainer": "S\u00f8ren H\u00f8jsgaard <sorenh@math.aau.dk>",
    "author": "S\u00f8ren H\u00f8jsgaard [aut, cre]",
    "url": "https://people.math.aau.dk/~sorenh/software/gR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gRbase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gRbase A Package for Graphical Modelling in R The 'gRbase' package provides graphical modelling features\n    used by e.g. the packages 'gRain', 'gRim' and 'gRc'. 'gRbase' implements\n    graph algorithms including (i) maximum cardinality search (for marked\n    and unmarked graphs).\n    (ii) moralization, (iii) triangulation, (iv) creation of junction tree.\n    'gRbase' facilitates array operations,\n    'gRbase' implements functions for testing for conditional independence.\n    'gRbase' illustrates how hierarchical log-linear models may be\n    implemented and describes concept of graphical meta\n    data. \n    The facilities of the package are documented in the book by H\u00f8jsgaard,\n    Edwards and Lauritzen (2012,\n    <doi:10.1007/978-1-4614-2299-0>) and in the paper by \n    Dethlefsen and H\u00f8jsgaard, (2005, <doi:10.18637/jss.v014.i17>).\n    Please see 'citation(\"gRbase\")' for citation details.   "
  },
  {
    "id": 12829,
    "package_name": "gRc",
    "title": "Inference in Graphical Gaussian Models with Edge and Vertex\nSymmetries",
    "description": "Estimation, model selection and other aspects of\n    statistical inference in Graphical Gaussian models with edge and\n    vertex symmetries (Graphical Gaussian models with colours).\n    Documentation about 'gRc' is provided in the paper by Hojsgaard and\n    Lauritzen (2007, <doi:10.18637/jss.v023.i06>) and the paper by\n    Hojsgaard and Lauritzen (2008, <doi:10.1111/j.1467-9868.2008.00666.x>).",
    "version": "0.5.1",
    "maintainer": "S\u00f8ren H\u00f8jsgaard <sorenh@math.aau.dk>",
    "author": "Steffen Lauritzen [aut, cph],\n  S\u00f8ren H\u00f8jsgaard [aut, cre, cph]",
    "url": "https://people.math.aau.dk/~sorenh/software/gR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gRc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gRc Inference in Graphical Gaussian Models with Edge and Vertex\nSymmetries Estimation, model selection and other aspects of\n    statistical inference in Graphical Gaussian models with edge and\n    vertex symmetries (Graphical Gaussian models with colours).\n    Documentation about 'gRc' is provided in the paper by Hojsgaard and\n    Lauritzen (2007, <doi:10.18637/jss.v023.i06>) and the paper by\n    Hojsgaard and Lauritzen (2008, <doi:10.1111/j.1467-9868.2008.00666.x>).  "
  },
  {
    "id": 12830,
    "package_name": "gRim",
    "title": "Graphical Interaction Models",
    "description": "Provides the following types of models: Models for contingency\n    tables (i.e. log-linear models) Graphical Gaussian models for multivariate\n    normal data (i.e. covariance selection models) Mixed interaction models.\n    Documentation about 'gRim' is provided by vignettes included in this\n    package and the book by H\u00f8jsgaard, Edwards and Lauritzen (2012,\n    <doi:10.1007/978-1-4614-2299-0>); see 'citation(\"gRim\")' for details.",
    "version": "0.3.4",
    "maintainer": "S\u00f8ren H\u00f8jsgaard <sorenh@math.aau.dk>",
    "author": "S\u00f8ren H\u00f8jsgaard [aut, cre]",
    "url": "https://people.math.aau.dk/~sorenh/software/gR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gRim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gRim Graphical Interaction Models Provides the following types of models: Models for contingency\n    tables (i.e. log-linear models) Graphical Gaussian models for multivariate\n    normal data (i.e. covariance selection models) Mixed interaction models.\n    Documentation about 'gRim' is provided by vignettes included in this\n    package and the book by H\u00f8jsgaard, Edwards and Lauritzen (2012,\n    <doi:10.1007/978-1-4614-2299-0>); see 'citation(\"gRim\")' for details.  "
  },
  {
    "id": 12889,
    "package_name": "gander",
    "title": "High Performance, Low Friction Large Language Model Chat",
    "description": "Introduces a 'Copilot'-like completion experience, but it knows how\n    to talk to the objects in your R environment. 'ellmer' chats are integrated \n    directly into your 'RStudio' and 'Positron' sessions, automatically \n    incorporating relevant context from surrounding lines of code and your\n    global environment (like data frame columns and types). Open the package \n    dialog box with a keyboard shortcut, type your request, and the assistant \n    will stream its response directly into your documents.",
    "version": "0.1.0",
    "maintainer": "Simon Couch <simon.couch@posit.co>",
    "author": "Simon Couch [aut, cre] (ORCID: <https://orcid.org/0000-0001-5676-5107>),\n  Posit Software, PBC [cph, fnd]",
    "url": "https://github.com/simonpcouch/gander,\nhttps://simonpcouch.github.io/gander/",
    "bug_reports": "https://github.com/simonpcouch/gander/issues",
    "repository": "https://cran.r-project.org/package=gander",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gander High Performance, Low Friction Large Language Model Chat Introduces a 'Copilot'-like completion experience, but it knows how\n    to talk to the objects in your R environment. 'ellmer' chats are integrated \n    directly into your 'RStudio' and 'Positron' sessions, automatically \n    incorporating relevant context from surrounding lines of code and your\n    global environment (like data frame columns and types). Open the package \n    dialog box with a keyboard shortcut, type your request, and the assistant \n    will stream its response directly into your documents.  "
  },
  {
    "id": 12891,
    "package_name": "gap",
    "title": "Genetic Analysis Package",
    "description": "As first reported [Zhao, J. H. 2007. \"gap: Genetic Analysis Package\". J Stat Soft 23(8):1-18.\n        <doi:10.18637/jss.v023.i08>], it is designed as an integrated package for genetic data\n        analysis of both population and family data. Currently, it contains functions for\n        sample size calculations of both population-based and family-based designs, probability\n        of familial disease aggregation, kinship calculation, statistics in linkage analysis,\n        and association analysis involving genetic markers including haplotype analysis with or\n        without environmental covariates. Over years, the package has been developed in-between\n        many projects hence also in line with the name (gap).",
    "version": "1.6",
    "maintainer": "Jing Hua Zhao <jinghuazhao@hotmail.com>",
    "author": "Jing Hua Zhao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1463-5870>, 0000-0003-4930-3582),\n  Kurt Hornik [ctb],\n  Brian Ripley [ctb],\n  Uwe Ligges [ctb],\n  Achim Zeileis [ctb]",
    "url": "https://github.com/jinghuazhao/R",
    "bug_reports": "https://github.com/jinghuazhao/R/issues",
    "repository": "https://cran.r-project.org/package=gap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gap Genetic Analysis Package As first reported [Zhao, J. H. 2007. \"gap: Genetic Analysis Package\". J Stat Soft 23(8):1-18.\n        <doi:10.18637/jss.v023.i08>], it is designed as an integrated package for genetic data\n        analysis of both population and family data. Currently, it contains functions for\n        sample size calculations of both population-based and family-based designs, probability\n        of familial disease aggregation, kinship calculation, statistics in linkage analysis,\n        and association analysis involving genetic markers including haplotype analysis with or\n        without environmental covariates. Over years, the package has been developed in-between\n        many projects hence also in line with the name (gap).  "
  },
  {
    "id": 12909,
    "package_name": "gauseR",
    "title": "Lotka-Volterra Models for Gause's 'Struggle for Existence'",
    "description": "A collection of tools and data for analyzing the Gause microcosm experiments, and for fitting Lotka-Volterra models to time series data. Includes methods for fitting single-species logistic growth, and multi-species interaction models, e.g. of competition, predator/prey relationships, or mutualism. See documentation for individual functions for examples. In general, see the lv_optim() function for examples of how to fit parameter values in multi-species systems. Note that the general methods applied here, as well as the form of the differential equations that we use, are described in detail in the Quantitative Ecology textbook by Lehman et al., available at <http://hdl.handle.net/11299/204551>, and in Lina K. M\u00fchlbauer, Maximilienne Schulze, W. Stanley Harpole, and Adam T. Clark. 'gauseR': Simple methods for fitting Lotka-Volterra models describing Gause's 'Struggle for Existence' in the journal Ecology and Evolution.",
    "version": "1.3",
    "maintainer": "Adam Clark <adam.tclark@gmail.com>",
    "author": "Adam Clark [aut, cre] (ORCID: <https://orcid.org/0000-0002-8843-3278>),\n  Lina M\u00fchlbauer [aut],\n  Maximilienne Schulze [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gauseR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gauseR Lotka-Volterra Models for Gause's 'Struggle for Existence' A collection of tools and data for analyzing the Gause microcosm experiments, and for fitting Lotka-Volterra models to time series data. Includes methods for fitting single-species logistic growth, and multi-species interaction models, e.g. of competition, predator/prey relationships, or mutualism. See documentation for individual functions for examples. In general, see the lv_optim() function for examples of how to fit parameter values in multi-species systems. Note that the general methods applied here, as well as the form of the differential equations that we use, are described in detail in the Quantitative Ecology textbook by Lehman et al., available at <http://hdl.handle.net/11299/204551>, and in Lina K. M\u00fchlbauer, Maximilienne Schulze, W. Stanley Harpole, and Adam T. Clark. 'gauseR': Simple methods for fitting Lotka-Volterra models describing Gause's 'Struggle for Existence' in the journal Ecology and Evolution.  "
  },
  {
    "id": 12922,
    "package_name": "gbm.auto",
    "title": "Automated Boosted Regression Tree Modelling and Mapping Suite",
    "description": "Automates delta log-normal boosted regression tree abundance\n    prediction. Loops through parameters provided (LR (learning rate), TC\n    (tree complexity), BF (bag fraction)), chooses best, simplifies, &\n    generates line, dot & bar plots, & outputs these & predictions & a\n    report, makes predicted abundance maps, and Unrepresentativeness\n    surfaces.  Package core built around 'gbm' (gradient boosting machine)\n    functions in 'dismo' (Hijmans, Phillips, Leathwick & Jane Elith, 2020\n    & ongoing), itself built around 'gbm' (Greenwell, Boehmke, Cunningham\n    & Metcalfe, 2020 & ongoing, originally by Ridgeway). Indebted to\n    Elith/Leathwick/Hastie 2008 'Working Guide'\n    <doi:10.1111/j.1365-2656.2008.01390.x>; workflow follows Appendix S3.\n    See <https://www.simondedman.com/> for published guides and papers\n    using this package.",
    "version": "2024.10.01",
    "maintainer": "Simon Dedman <simondedman@gmail.com>",
    "author": "Simon Dedman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9108-972X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gbm.auto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gbm.auto Automated Boosted Regression Tree Modelling and Mapping Suite Automates delta log-normal boosted regression tree abundance\n    prediction. Loops through parameters provided (LR (learning rate), TC\n    (tree complexity), BF (bag fraction)), chooses best, simplifies, &\n    generates line, dot & bar plots, & outputs these & predictions & a\n    report, makes predicted abundance maps, and Unrepresentativeness\n    surfaces.  Package core built around 'gbm' (gradient boosting machine)\n    functions in 'dismo' (Hijmans, Phillips, Leathwick & Jane Elith, 2020\n    & ongoing), itself built around 'gbm' (Greenwell, Boehmke, Cunningham\n    & Metcalfe, 2020 & ongoing, originally by Ridgeway). Indebted to\n    Elith/Leathwick/Hastie 2008 'Working Guide'\n    <doi:10.1111/j.1365-2656.2008.01390.x>; workflow follows Appendix S3.\n    See <https://www.simondedman.com/> for published guides and papers\n    using this package.  "
  },
  {
    "id": 12940,
    "package_name": "gconsensus",
    "title": "Consensus Value Constructor",
    "description": "An implementation of the International Bureau of Weights and Measures (BIPM) generalized consensus estimators used to assign the reference value in a key comparison exercise. This can also be applied to any interlaboratory study. Given a set of different sources, primary laboratories or measurement methods this package provides an evaluation of the variance components according to the selected statistical method for consensus building. It also implements the comparison among different consensus builders and evaluates the participating method or sources against the consensus reference value. Based on a diverse set of references, DerSimonian-Laird (1986) <doi:10.1016/0197-2456(86)90046-2>, for a complete list of references look at the reference section in the package documentation.",
    "version": "0.3.2",
    "maintainer": "Hugo Gasca-Aragon <hugo_gasca_aragon@hotmail.com>",
    "author": "Hugo Gasca-Aragon",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gconsensus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gconsensus Consensus Value Constructor An implementation of the International Bureau of Weights and Measures (BIPM) generalized consensus estimators used to assign the reference value in a key comparison exercise. This can also be applied to any interlaboratory study. Given a set of different sources, primary laboratories or measurement methods this package provides an evaluation of the variance components according to the selected statistical method for consensus building. It also implements the comparison among different consensus builders and evaluates the participating method or sources against the consensus reference value. Based on a diverse set of references, DerSimonian-Laird (1986) <doi:10.1016/0197-2456(86)90046-2>, for a complete list of references look at the reference section in the package documentation.  "
  },
  {
    "id": 12957,
    "package_name": "gdtools",
    "title": "Utilities for Graphical Rendering and Fonts Management",
    "description": "Tools are provided to compute metrics of formatted strings\n    and to check the availability of a font.  Another set of functions is\n    provided to support the collection of fonts from 'Google Fonts' in a\n    cache. Their use is simple within 'R Markdown' documents and 'shiny'\n    applications but also with graphic productions generated with the\n    'ggiraph', 'ragg' and 'svglite' packages or with tabular productions\n    from the 'flextable' package.",
    "version": "0.4.4",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  Hadley Wickham [aut],\n  Lionel Henry [aut],\n  Jeroen Ooms [aut] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  Yixuan Qiu [ctb],\n  R Core Team [cph] (Cairo code from X11 device),\n  ArData [cph],\n  RStudio [cph]",
    "url": "https://davidgohel.github.io/gdtools/",
    "bug_reports": "https://github.com/davidgohel/gdtools/issues",
    "repository": "https://cran.r-project.org/package=gdtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdtools Utilities for Graphical Rendering and Fonts Management Tools are provided to compute metrics of formatted strings\n    and to check the availability of a font.  Another set of functions is\n    provided to support the collection of fonts from 'Google Fonts' in a\n    cache. Their use is simple within 'R Markdown' documents and 'shiny'\n    applications but also with graphic productions generated with the\n    'ggiraph', 'ragg' and 'svglite' packages or with tabular productions\n    from the 'flextable' package.  "
  },
  {
    "id": 12986,
    "package_name": "gen5helper",
    "title": "Processing 'Gen5' 2.06 Exported Data",
    "description": "A collection of functions for processing 'Gen5' 2.06 exported data.\n    'Gen5' is an essential data analysis software for BioTek plate readers <https://www.biotek.com/products/software-robotics-software/gen5-microplate-reader-and-imager-software/>. This package contains functions for data cleaning, \n    modeling and plotting using exported data from 'Gen5' version 2.06. It exports\n    technically correct data defined in (Edwin de Jonge and Mark van der Loo \n    (2013) <https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf>) for customized analysis. It \n    contains Boltzmann fitting for general kinetic analysis.  \n    See <https://www.github.com/yanxianUCSB/gen5helper> for more information,\n    documentation and examples.",
    "version": "1.0.1",
    "maintainer": "Yanxian Lin <yanxian00@gmail.com>",
    "author": "Yanxian Lin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gen5helper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gen5helper Processing 'Gen5' 2.06 Exported Data A collection of functions for processing 'Gen5' 2.06 exported data.\n    'Gen5' is an essential data analysis software for BioTek plate readers <https://www.biotek.com/products/software-robotics-software/gen5-microplate-reader-and-imager-software/>. This package contains functions for data cleaning, \n    modeling and plotting using exported data from 'Gen5' version 2.06. It exports\n    technically correct data defined in (Edwin de Jonge and Mark van der Loo \n    (2013) <https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf>) for customized analysis. It \n    contains Boltzmann fitting for general kinetic analysis.  \n    See <https://www.github.com/yanxianUCSB/gen5helper> for more information,\n    documentation and examples.  "
  },
  {
    "id": 12997,
    "package_name": "gender",
    "title": "Predict Gender from Names Using Historical Data",
    "description": "Infers state-recorded gender categories from first names and dates of birth using historical\n    datasets. By using these datasets instead of lists of male and female names,\n    this package is able to more accurately infer the gender of a name, and it\n    is able to report the probability that a name was male or female. GUIDELINES:\n    This method must be used cautiously and responsibly. Please be sure to see the\n    guidelines and warnings about usage in the 'README' or the package documentation.\n    See Blevins and Mullen (2015) <http://www.digitalhumanities.org/dhq/vol/9/3/000223/000223.html>.",
    "version": "0.6.0",
    "maintainer": "Lincoln Mullen <lincoln@lincolnmullen.com>",
    "author": "Lincoln Mullen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5103-6917>),\n  Cameron Blevins [ctb],\n  Ben Schmidt [ctb]",
    "url": "https://github.com/lmullen/gender",
    "bug_reports": "https://github.com/lmullen/gender/issues",
    "repository": "https://cran.r-project.org/package=gender",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gender Predict Gender from Names Using Historical Data Infers state-recorded gender categories from first names and dates of birth using historical\n    datasets. By using these datasets instead of lists of male and female names,\n    this package is able to more accurately infer the gender of a name, and it\n    is able to report the probability that a name was male or female. GUIDELINES:\n    This method must be used cautiously and responsibly. Please be sure to see the\n    guidelines and warnings about usage in the 'README' or the package documentation.\n    See Blevins and Mullen (2015) <http://www.digitalhumanities.org/dhq/vol/9/3/000223/000223.html>.  "
  },
  {
    "id": 12998,
    "package_name": "genderBR",
    "title": "Predict Gender from Brazilian First Names",
    "description": "A method to predict and report gender from Brazilian first names\n    using the Brazilian Institute of Geography and Statistics' Census data (<https://censo2010.ibge.gov.br/nomes/>).",
    "version": "1.1.2",
    "maintainer": "Fernando Meireles <fmeireles@ufmg.br>",
    "author": "Fernando Meireles [aut, cre]",
    "url": "https://github.com/meirelesff/genderBR",
    "bug_reports": "https://github.com/meirelesff/genderBR/issues",
    "repository": "https://cran.r-project.org/package=genderBR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genderBR Predict Gender from Brazilian First Names A method to predict and report gender from Brazilian first names\n    using the Brazilian Institute of Geography and Statistics' Census data (<https://censo2010.ibge.gov.br/nomes/>).  "
  },
  {
    "id": 13000,
    "package_name": "genderstat",
    "title": "Quantitative Analysis Tools for Gender Studies",
    "description": "Provides tools for quantitative analysis in gender studies, including functions to calculate various gender inequality metrics such as the Gender Pay Gap, Gender Inequality Index (GII), Gender Development Index (GDI), and Gender Empowerment Measure (GEM). Also includes extracted secondary example datasets for practice and learning purposes, which were obtained from the UNDP Human Development Reports Data Center and the World Bank Gender Data Portal by the author the dataset is available on <doi:10.34740/kaggle/dsv/6359326>. References: Miller, Kevin; Vagins, Deborah J. (2021) <https://eric.ed.gov/?id=ED596219>. Jacques Charmes & Saskia Wieringa (2003) <doi:10.1080/1464988032000125773>. Ga\u00eblle Ferrant (2010) <https://shs.hal.science/halshs-00462463/>.",
    "version": "0.1.5",
    "maintainer": "S M Mashrur Arafin Ayon <mashrur399@gmail.com>",
    "author": "S M Mashrur Arafin Ayon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3659-2891>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=genderstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genderstat Quantitative Analysis Tools for Gender Studies Provides tools for quantitative analysis in gender studies, including functions to calculate various gender inequality metrics such as the Gender Pay Gap, Gender Inequality Index (GII), Gender Development Index (GDI), and Gender Empowerment Measure (GEM). Also includes extracted secondary example datasets for practice and learning purposes, which were obtained from the UNDP Human Development Reports Data Center and the World Bank Gender Data Portal by the author the dataset is available on <doi:10.34740/kaggle/dsv/6359326>. References: Miller, Kevin; Vagins, Deborah J. (2021) <https://eric.ed.gov/?id=ED596219>. Jacques Charmes & Saskia Wieringa (2003) <doi:10.1080/1464988032000125773>. Ga\u00eblle Ferrant (2010) <https://shs.hal.science/halshs-00462463/>.  "
  },
  {
    "id": 13009,
    "package_name": "generalCorr",
    "title": "Generalized Correlations, Causal Paths and Portfolio Selection",
    "description": "Function gmcmtx0() computes a more reliable (general) \n    correlation matrix. Since causal paths from data are important for all sciences, the\n    package provides many sophisticated functions. causeSummBlk() and causeSum2Blk()\n    give easy-to-interpret causal paths.  Let Z denote control variables and compare \n    two flipped kernel regressions: X=f(Y, Z)+e1 and Y=g(X, Z)+e2. Our criterion Cr1 \n    says that if |e1*Y|>|e2*X| then variation in X is more \"exogenous or independent\"\n    than in Y, and the causal path is X to Y. Criterion Cr2 requires |e2|<|e1|. These\n    inequalities between many absolute values are quantified by four orders of \n    stochastic dominance. Our third criterion Cr3, for the causal path X to Y,\n    requires new generalized partial correlations to satisfy |r*(x|y,z)|< |r*(y|x,z)|.\n    The function parcorVec() reports generalized partials between the first\n    variable and all others.  The package provides several R functions including\n    get0outliers() for outlier detection, bigfp() for numerical integration by the\n    trapezoidal rule, stochdom2() for stochastic dominance, pillar3D() for 3D charts,\n    canonRho() for generalized canonical correlations, depMeas() measures nonlinear\n    dependence, and causeSummary(mtx) reports summary of causal paths among matrix \n    columns. Portfolio selection: decileVote(), momentVote(), dif4mtx(), exactSdMtx()\n    can rank several stocks. Functions whose names begin with 'boot' provide bootstrap\n    statistical inference, including a new bootGcRsq() test for \"Granger-causality\" \n    allowing nonlinear relations. A new tool for evaluation of out-of-sample\n    portfolio performance is outOFsamp(). Panel data implementation is now included.\n    See eight vignettes of the package for theory, examples, and\n    usage tips. See Vinod (2019) \\doi{10.1080/03610918.2015.1122048}.",
    "version": "1.2.6",
    "maintainer": "H. D. Vinod <vinod@fordham.edu>",
    "author": "Prof. H. D. Vinod, Fordham University, NY.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=generalCorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "generalCorr Generalized Correlations, Causal Paths and Portfolio Selection Function gmcmtx0() computes a more reliable (general) \n    correlation matrix. Since causal paths from data are important for all sciences, the\n    package provides many sophisticated functions. causeSummBlk() and causeSum2Blk()\n    give easy-to-interpret causal paths.  Let Z denote control variables and compare \n    two flipped kernel regressions: X=f(Y, Z)+e1 and Y=g(X, Z)+e2. Our criterion Cr1 \n    says that if |e1*Y|>|e2*X| then variation in X is more \"exogenous or independent\"\n    than in Y, and the causal path is X to Y. Criterion Cr2 requires |e2|<|e1|. These\n    inequalities between many absolute values are quantified by four orders of \n    stochastic dominance. Our third criterion Cr3, for the causal path X to Y,\n    requires new generalized partial correlations to satisfy |r*(x|y,z)|< |r*(y|x,z)|.\n    The function parcorVec() reports generalized partials between the first\n    variable and all others.  The package provides several R functions including\n    get0outliers() for outlier detection, bigfp() for numerical integration by the\n    trapezoidal rule, stochdom2() for stochastic dominance, pillar3D() for 3D charts,\n    canonRho() for generalized canonical correlations, depMeas() measures nonlinear\n    dependence, and causeSummary(mtx) reports summary of causal paths among matrix \n    columns. Portfolio selection: decileVote(), momentVote(), dif4mtx(), exactSdMtx()\n    can rank several stocks. Functions whose names begin with 'boot' provide bootstrap\n    statistical inference, including a new bootGcRsq() test for \"Granger-causality\" \n    allowing nonlinear relations. A new tool for evaluation of out-of-sample\n    portfolio performance is outOFsamp(). Panel data implementation is now included.\n    See eight vignettes of the package for theory, examples, and\n    usage tips. See Vinod (2019) \\doi{10.1080/03610918.2015.1122048}.  "
  },
  {
    "id": 13042,
    "package_name": "geoAr",
    "title": "Argentina's Spatial Data Toolbox",
    "description": "Collection of tools that facilitates data access and workflow for spatial analysis of Argentina. Includes historical information from censuses, administrative limits at different levels of aggregation, location of human settlements, among others. Since it is expected that the majority of users will be Spanish-speaking, the documentation of the package prioritizes this language, although an effort is made to also offer annotations in English. ",
    "version": "1.0.0",
    "maintainer": "Juan Pablo Ruiz Nicolini <juanpabloruiznicolini@gmail.com>",
    "author": "Juan Pablo Ruiz Nicolini [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3138-6343>),\n  Patricio Del Boca [aut],\n  Juan Gabriel Juara [aut]",
    "url": "https://github.com/PoliticaArgentina/geoAr",
    "bug_reports": "https://github.com/PoliticaArgentina/geoAr/issues",
    "repository": "https://cran.r-project.org/package=geoAr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoAr Argentina's Spatial Data Toolbox Collection of tools that facilitates data access and workflow for spatial analysis of Argentina. Includes historical information from censuses, administrative limits at different levels of aggregation, location of human settlements, among others. Since it is expected that the majority of users will be Spanish-speaking, the documentation of the package prioritizes this language, although an effort is made to also offer annotations in English.   "
  },
  {
    "id": 13105,
    "package_name": "geotopbricks",
    "title": "An R Plug-in for the Distributed Hydrological Model GEOtop",
    "description": "It analyzes raster maps and other information as input/output\n    files from the Hydrological Distributed Model GEOtop. It contains functions\n    and methods to import maps and other keywords from geotop.inpts file. Some\n    examples with simulation cases of GEOtop 2.x/3.x are presented in the package.\n    Any information about the GEOtop Distributed Hydrological Model can be found in the provided documentation.",
    "version": "1.5.9.1",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "author": "Emanuele Cordano [aut, cre, ctb] (ORCID:\n    <https://orcid.org/0000-0002-3508-5898>)",
    "url": "https://github.com/ecor/geotopbricks",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geotopbricks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geotopbricks An R Plug-in for the Distributed Hydrological Model GEOtop It analyzes raster maps and other information as input/output\n    files from the Hydrological Distributed Model GEOtop. It contains functions\n    and methods to import maps and other keywords from geotop.inpts file. Some\n    examples with simulation cases of GEOtop 2.x/3.x are presented in the package.\n    Any information about the GEOtop Distributed Hydrological Model can be found in the provided documentation.  "
  },
  {
    "id": 13108,
    "package_name": "gepaf",
    "title": "Google Encoded Polyline Algorithm Format",
    "description": "Encode and decode the Google Encoded Polyline Algorithm Format.\n    See <https://developers.google.com/maps/documentation/utilities/polylinealgorithm>\n    for more information.",
    "version": "0.2.0",
    "maintainer": "Timoth\u00e9e Giraud <timothee.giraud@cnrs.fr>",
    "author": "Matthieu Viry [aut] (ORCID: <https://orcid.org/0000-0002-0693-8556>),\n  Timoth\u00e9e Giraud [ctb, cre] (ORCID:\n    <https://orcid.org/0000-0002-1932-3323>)",
    "url": "https://github.com/riatelab/gepaf",
    "bug_reports": "https://github.com/riatelab/gepaf/issues",
    "repository": "https://cran.r-project.org/package=gepaf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gepaf Google Encoded Polyline Algorithm Format Encode and decode the Google Encoded Polyline Algorithm Format.\n    See <https://developers.google.com/maps/documentation/utilities/polylinealgorithm>\n    for more information.  "
  },
  {
    "id": 13120,
    "package_name": "getLattes",
    "title": "Import and Process Data from the 'Lattes' Curriculum Platform",
    "description": "Tool for import and process data from 'Lattes' curriculum platform (<http://lattes.cnpq.br/>). The Brazilian government keeps an extensive base of curricula for academics from all over the country, with over 5 million registrations. The academic life of the Brazilian researcher, or related to Brazilian universities, is documented in 'Lattes'. Some information that can be obtained: professional formation, research area, publications, academics advisories, projects, etc. 'getLattes' package allows work with 'Lattes' data exported to XML format.",
    "version": "1.0.0",
    "maintainer": "Roney Fraga Souza <roneyfraga@gmail.com>",
    "author": "Roney Fraga Souza [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5750-489X>),\n  Winicius Sabino [aut],\n  Luis Felipe de Souza Rodrigues [aut]",
    "url": "https://github.com/roneyfraga/getLattes",
    "bug_reports": "https://github.com/roneyfraga/getLattes/issues",
    "repository": "https://cran.r-project.org/package=getLattes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "getLattes Import and Process Data from the 'Lattes' Curriculum Platform Tool for import and process data from 'Lattes' curriculum platform (<http://lattes.cnpq.br/>). The Brazilian government keeps an extensive base of curricula for academics from all over the country, with over 5 million registrations. The academic life of the Brazilian researcher, or related to Brazilian universities, is documented in 'Lattes'. Some information that can be obtained: professional formation, research area, publications, academics advisories, projects, etc. 'getLattes' package allows work with 'Lattes' data exported to XML format.  "
  },
  {
    "id": 13124,
    "package_name": "getable",
    "title": "Fetching Tabular Data \"Onload\" in Compiled R Markdown HTML\nDocuments",
    "description": "Dynamically retrieve data from the web to render HTML tables\n    on inspection in R Markdown HTML documents.",
    "version": "1.0.3",
    "maintainer": "Yongfu Liao <liao961120@gmail.com>",
    "author": "Yongfu Liao [aut, cre] (ORCID: <https://orcid.org/0000-0002-1814-2993>)",
    "url": "https://yongfu.name/getable/,\nhttps://github.com/liao961120/getable/",
    "bug_reports": "https://github.com/liao961120/getable/issues/",
    "repository": "https://cran.r-project.org/package=getable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "getable Fetching Tabular Data \"Onload\" in Compiled R Markdown HTML\nDocuments Dynamically retrieve data from the web to render HTML tables\n    on inspection in R Markdown HTML documents.  "
  },
  {
    "id": 13126,
    "package_name": "getmstatistic",
    "title": "Quantifying Systematic Heterogeneity in Meta-Analysis",
    "description": "Quantifying systematic heterogeneity in meta-analysis using R.\n    The M statistic aggregates heterogeneity information across multiple\n    variants to, identify systematic heterogeneity patterns and their direction\n    of effect in meta-analysis. It's primary use is to identify outlier studies,\n    which either show \"null\" effects or consistently show stronger or weaker\n    genetic effects than average across, the panel of variants examined in a\n    GWAS meta-analysis. In contrast to conventional heterogeneity metrics\n    (Q-statistic, I-squared and tau-squared) which measure random heterogeneity\n    at individual variants, M measures systematic (non-random)\n    heterogeneity across multiple independently associated variants. Systematic\n    heterogeneity can arise in a meta-analysis due to differences in the study\n    characteristics of participating studies. Some of the differences may\n    include: ancestry, allele frequencies, phenotype definition, age-of-disease\n    onset, family-history, gender, linkage disequilibrium and quality control\n    thresholds. See <https://magosil86.github.io/getmstatistic/> for statistical\n    statistical theory, documentation and examples.",
    "version": "0.2.2",
    "maintainer": "Lerato E Magosi <magosil86@gmail.com>",
    "author": "Lerato E Magosi [aut],\n  Jemma C Hopewell [aut],\n  Martin Farrall [aut],\n  Lerato E Magosi [cre]",
    "url": "https://magosil86.github.io/getmstatistic/",
    "bug_reports": "https://github.com/magosil86/getmstatistic/issues",
    "repository": "https://cran.r-project.org/package=getmstatistic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "getmstatistic Quantifying Systematic Heterogeneity in Meta-Analysis Quantifying systematic heterogeneity in meta-analysis using R.\n    The M statistic aggregates heterogeneity information across multiple\n    variants to, identify systematic heterogeneity patterns and their direction\n    of effect in meta-analysis. It's primary use is to identify outlier studies,\n    which either show \"null\" effects or consistently show stronger or weaker\n    genetic effects than average across, the panel of variants examined in a\n    GWAS meta-analysis. In contrast to conventional heterogeneity metrics\n    (Q-statistic, I-squared and tau-squared) which measure random heterogeneity\n    at individual variants, M measures systematic (non-random)\n    heterogeneity across multiple independently associated variants. Systematic\n    heterogeneity can arise in a meta-analysis due to differences in the study\n    characteristics of participating studies. Some of the differences may\n    include: ancestry, allele frequencies, phenotype definition, age-of-disease\n    onset, family-history, gender, linkage disequilibrium and quality control\n    thresholds. See <https://magosil86.github.io/getmstatistic/> for statistical\n    statistical theory, documentation and examples.  "
  },
  {
    "id": 13130,
    "package_name": "getspres",
    "title": "SPRE Statistics for Exploring Heterogeneity in Meta-Analysis",
    "description": "An implementation of SPRE (standardised predicted random-effects)\n    statistics in R to explore heterogeneity in genetic association meta-\n    analyses, as described by Magosi et al. (2019) \n    <doi:10.1093/bioinformatics/btz590>. SPRE statistics are precision \n    weighted residuals that indicate the direction and extent with which \n    individual study-effects in a meta-analysis deviate from the average \n    genetic effect. Overly influential positive outliers have the potential \n    to inflate average genetic effects in a meta-analysis whilst negative \n    outliers might lower or change the direction of effect. See the 'getspres' \n    website for documentation and examples \n    <https://magosil86.github.io/getspres/>.",
    "version": "0.2.0",
    "maintainer": "Lerato E Magosi <magosil86@gmail.com>",
    "author": "Lerato E Magosi [aut],\n  Jemma C Hopewell [aut],\n  Martin Farrall [aut],\n  Lerato E Magosi [cre]",
    "url": "https://magosil86.github.io/getspres/",
    "bug_reports": "https://github.com/magosil86/getspres/issues",
    "repository": "https://cran.r-project.org/package=getspres",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "getspres SPRE Statistics for Exploring Heterogeneity in Meta-Analysis An implementation of SPRE (standardised predicted random-effects)\n    statistics in R to explore heterogeneity in genetic association meta-\n    analyses, as described by Magosi et al. (2019) \n    <doi:10.1093/bioinformatics/btz590>. SPRE statistics are precision \n    weighted residuals that indicate the direction and extent with which \n    individual study-effects in a meta-analysis deviate from the average \n    genetic effect. Overly influential positive outliers have the potential \n    to inflate average genetic effects in a meta-analysis whilst negative \n    outliers might lower or change the direction of effect. See the 'getspres' \n    website for documentation and examples \n    <https://magosil86.github.io/getspres/>.  "
  },
  {
    "id": 13207,
    "package_name": "ggfigdone",
    "title": "Manage & Modify 'ggplot' Figures using 'ggfigdone'",
    "description": "When you prepare a presentation or a report, you often need to manage a large number of 'ggplot' figures. You need to change the figure size, modify the title, label, themes, etc. It is inconvenient to go back to the original code to make these changes. This package provides a simple way to manage 'ggplot' figures. You can easily add the figure to the database and update them later using CLI (command line interface) or GUI (graphical user interface).",
    "version": "0.1.2",
    "maintainer": "Wenjie SUN <sunwjie@gmail.com>",
    "author": "Wenjie SUN [aut, cre] (ORCID: <https://orcid.org/0000-0002-3100-2346>)",
    "url": "",
    "bug_reports": "https://github.com/wenjie1991/ggfigdone/issues",
    "repository": "https://cran.r-project.org/package=ggfigdone",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggfigdone Manage & Modify 'ggplot' Figures using 'ggfigdone' When you prepare a presentation or a report, you often need to manage a large number of 'ggplot' figures. You need to change the figure size, modify the title, label, themes, etc. It is inconvenient to go back to the original code to make these changes. This package provides a simple way to manage 'ggplot' figures. You can easily add the figure to the database and update them later using CLI (command line interface) or GUI (graphical user interface).  "
  },
  {
    "id": 13332,
    "package_name": "ggsurveillance",
    "title": "Tools for Outbreak Investigation/Infectious Disease Surveillance",
    "description": "Create epicurves, epigantt charts, and diverging bar charts\n    using 'ggplot2'. Prepare data for visualisation or other reporting for\n    infectious disease surveillance and outbreak investigation (time\n    series data).  Includes tidy functions to solve date based\n    transformations for common reporting tasks, like (A) seasonal date\n    alignment for respiratory disease surveillance, (B) date-based case\n    binning based on specified time intervals like isoweek, epiweek, month\n    and more, (C) automated detection and marking of the new year based on\n    the date/datetime axis of the 'ggplot2', (D) labelling of the last\n    value of a time-series.  An introduction on how to use epicurves can\n    be found on the US CDC website (2012,\n    <https://www.cdc.gov/training/quicklearns/epimode/index.html>).",
    "version": "0.5.2",
    "maintainer": "Alexander Bartel <alexander.bartel@fu-berlin.de>",
    "author": "Alexander Bartel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1280-6138>)",
    "url": "https://ggsurveillance.biostats.dev,\nhttps://github.com/biostats-dev/ggsurveillance",
    "bug_reports": "https://github.com/biostats-dev/ggsurveillance/issues",
    "repository": "https://cran.r-project.org/package=ggsurveillance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggsurveillance Tools for Outbreak Investigation/Infectious Disease Surveillance Create epicurves, epigantt charts, and diverging bar charts\n    using 'ggplot2'. Prepare data for visualisation or other reporting for\n    infectious disease surveillance and outbreak investigation (time\n    series data).  Includes tidy functions to solve date based\n    transformations for common reporting tasks, like (A) seasonal date\n    alignment for respiratory disease surveillance, (B) date-based case\n    binning based on specified time intervals like isoweek, epiweek, month\n    and more, (C) automated detection and marking of the new year based on\n    the date/datetime axis of the 'ggplot2', (D) labelling of the last\n    value of a time-series.  An introduction on how to use epicurves can\n    be found on the US CDC website (2012,\n    <https://www.cdc.gov/training/quicklearns/epimode/index.html>).  "
  },
  {
    "id": 13343,
    "package_name": "ggtibble",
    "title": "Create Tibbles and Lists of 'ggplot' Figures for Reporting",
    "description": "\n  Create tibbles and lists of 'ggplot' figures that can be modified as easily as\n  regular 'ggplot' figures.  Typical use cases are for creating reports or web\n  pages where many figures are needed with different data and similar\n  formatting.",
    "version": "1.0.2",
    "maintainer": "Bill Denney <wdenney@humanpredictions.com>",
    "author": "Bill Denney [aut, cre] (ORCID: <https://orcid.org/0000-0002-5759-428X>)",
    "url": "https://humanpred.github.io/ggtibble/,\nhttps://github.com/humanpred/ggtibble",
    "bug_reports": "https://github.com/humanpred/ggtibble/issues",
    "repository": "https://cran.r-project.org/package=ggtibble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggtibble Create Tibbles and Lists of 'ggplot' Figures for Reporting \n  Create tibbles and lists of 'ggplot' figures that can be modified as easily as\n  regular 'ggplot' figures.  Typical use cases are for creating reports or web\n  pages where many figures are needed with different data and similar\n  formatting.  "
  },
  {
    "id": 13381,
    "package_name": "gips",
    "title": "Gaussian Model Invariant by Permutation Symmetry",
    "description": "Find the permutation symmetry group such that the covariance\n    matrix of the given data is approximately invariant under it.\n    Discovering such a permutation decreases the number of observations\n    needed to fit a Gaussian model, which is of great use when it is\n    smaller than the number of variables. Even if that is not the case,\n    the covariance matrix found with 'gips' approximates the actual\n    covariance with less statistical error. The methods implemented in\n    this package are described in Graczyk et al. (2022)\n    <doi:10.1214/22-AOS2174>. Documentation about 'gips' is\n    provided via its website at <https://przechoj.github.io/gips/> and\n    the paper by Chojecki, Morgen, Ko\u0142odziejek\n    (2025, <doi:10.18637/jss.v112.i07>).",
    "version": "1.2.3",
    "maintainer": "Adam Przemys\u0142aw Chojecki <adam.prze.choj@gmail.com>",
    "author": "Adam Przemys\u0142aw Chojecki [aut, cre],\n  Pawe\u0142 Morgen [aut],\n  Bartosz Ko\u0142odziejek [aut] (ORCID:\n    <https://orcid.org/0000-0002-5220-9012>)",
    "url": "https://github.com/PrzeChoj/gips, https://przechoj.github.io/gips/",
    "bug_reports": "https://github.com/PrzeChoj/gips/issues",
    "repository": "https://cran.r-project.org/package=gips",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gips Gaussian Model Invariant by Permutation Symmetry Find the permutation symmetry group such that the covariance\n    matrix of the given data is approximately invariant under it.\n    Discovering such a permutation decreases the number of observations\n    needed to fit a Gaussian model, which is of great use when it is\n    smaller than the number of variables. Even if that is not the case,\n    the covariance matrix found with 'gips' approximates the actual\n    covariance with less statistical error. The methods implemented in\n    this package are described in Graczyk et al. (2022)\n    <doi:10.1214/22-AOS2174>. Documentation about 'gips' is\n    provided via its website at <https://przechoj.github.io/gips/> and\n    the paper by Chojecki, Morgen, Ko\u0142odziejek\n    (2025, <doi:10.18637/jss.v112.i07>).  "
  },
  {
    "id": 13391,
    "package_name": "gitlink",
    "title": "Add 'Git' Links to Your Web Based Assets",
    "description": "Provides helpers to add 'Git' links to 'shiny'\n    applications, 'rmarkdown' documents, and other 'HTML' based resources.\n    This is most commonly used for 'GitHub' ribbons.",
    "version": "0.1.3",
    "maintainer": "Cole Arendt <cole@rstudio.com>",
    "author": "Cole Arendt [aut, cre],\n  RStudio [cph, fnd]",
    "url": "https://github.com/colearendt/gitlink",
    "bug_reports": "https://github.com/colearendt/gitlink/issues",
    "repository": "https://cran.r-project.org/package=gitlink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gitlink Add 'Git' Links to Your Web Based Assets Provides helpers to add 'Git' links to 'shiny'\n    applications, 'rmarkdown' documents, and other 'HTML' based resources.\n    This is most commonly used for 'GitHub' ribbons.  "
  },
  {
    "id": 13408,
    "package_name": "gld",
    "title": "Estimation and Use of the Generalised (Tukey) Lambda\nDistribution",
    "description": "The generalised lambda distribution, or Tukey lambda distribution, \n  provides a wide variety of shapes with one functional form.   \n  This package provides random numbers, quantiles, probabilities, \n  densities and density quantiles for four different types of the distribution,\n  the FKML (Freimer et al 1988), RS (Ramberg and Schmeiser 1974), GPD (van Staden\n  and Loots 2009) and FM5 - see documentation for details.\n  It provides the density function, distribution function, and Quantile-Quantile \n  plots.  \n  It implements a variety of estimation methods for the distribution, \n  including diagnostic plots. \n  Estimation methods include the starship (all 4 types), \n  method of L-Moments for the GPD and FKML types, and a \n  number of methods for only the FKML type.  \n  These include maximum likelihood, maximum product of spacings, \n  Titterington's method, Moments, Trimmed L-Moments and \n  Distributional Least Absolutes. ",
    "version": "2.6.8",
    "maintainer": "Robert King <Robert.King.Newcastle@gmail.com>",
    "author": "Robert King [aut, cre] (ORCID: <https://orcid.org/0000-0001-7495-6599>),\n  Benjamin Dean [aut],\n  Sigbert Klinke [aut],\n  Paul van Staden [aut] (ORCID: <https://orcid.org/0000-0002-5710-5984>)",
    "url": "https://github.com/newystats/gld/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gld",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gld Estimation and Use of the Generalised (Tukey) Lambda\nDistribution The generalised lambda distribution, or Tukey lambda distribution, \n  provides a wide variety of shapes with one functional form.   \n  This package provides random numbers, quantiles, probabilities, \n  densities and density quantiles for four different types of the distribution,\n  the FKML (Freimer et al 1988), RS (Ramberg and Schmeiser 1974), GPD (van Staden\n  and Loots 2009) and FM5 - see documentation for details.\n  It provides the density function, distribution function, and Quantile-Quantile \n  plots.  \n  It implements a variety of estimation methods for the distribution, \n  including diagnostic plots. \n  Estimation methods include the starship (all 4 types), \n  method of L-Moments for the GPD and FKML types, and a \n  number of methods for only the FKML type.  \n  These include maximum likelihood, maximum product of spacings, \n  Titterington's method, Moments, Trimmed L-Moments and \n  Distributional Least Absolutes.   "
  },
  {
    "id": 13412,
    "package_name": "glioblastomaEHRsData",
    "title": "Descriptive Analysis on 3 EHRs Datasets",
    "description": "Provides functions to load and analyze three open Electronic\n    Health Records (EHRs) datasets of patients diagnosed with\n    glioblastoma, previously released under the Creative Common\n    Attribution 4.0 International (CC BY 4.0) license.  Users can generate\n    basic descriptive statistics, frequency tables and save descriptive\n    summary tables, as well as create and export univariate or bivariate\n    plots.  The package is designed to work with the included datasets and\n    to facilitate quick exploratory data analysis and reporting.  More\n    information about these three datasets of EHRs of patients with\n    glioblastoma can be found in this article: Gabriel Cerono, Ombretta\n    Melaiu, and Davide Chicco, 'Clinical feature ranking based on ensemble\n    machine learning reveals top survival factors for glioblastoma\n    multiforme', Journal of Healthcare Informatics Research 8, 1-18 (March\n    2024).  <doi:10.1007/s41666-023-00138-1>.",
    "version": "0.1.0",
    "maintainer": "Samuele Marelli <samu2003.marelli@gmail.com>",
    "author": "Samuele Marelli [aut, cre],\n  Davide Chicco [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glioblastomaEHRsData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glioblastomaEHRsData Descriptive Analysis on 3 EHRs Datasets Provides functions to load and analyze three open Electronic\n    Health Records (EHRs) datasets of patients diagnosed with\n    glioblastoma, previously released under the Creative Common\n    Attribution 4.0 International (CC BY 4.0) license.  Users can generate\n    basic descriptive statistics, frequency tables and save descriptive\n    summary tables, as well as create and export univariate or bivariate\n    plots.  The package is designed to work with the included datasets and\n    to facilitate quick exploratory data analysis and reporting.  More\n    information about these three datasets of EHRs of patients with\n    glioblastoma can be found in this article: Gabriel Cerono, Ombretta\n    Melaiu, and Davide Chicco, 'Clinical feature ranking based on ensemble\n    machine learning reveals top survival factors for glioblastoma\n    multiforme', Journal of Healthcare Informatics Research 8, 1-18 (March\n    2024).  <doi:10.1007/s41666-023-00138-1>.  "
  },
  {
    "id": 13463,
    "package_name": "glossary",
    "title": "Glossaries for Markdown and Quarto Documents",
    "description": "Add glossaries to markdown and quarto documents by tagging individual words. Definitions can be provided inline or in a separate file.",
    "version": "1.0.0",
    "maintainer": "Lisa DeBruine <debruine@gmail.com>",
    "author": "Lisa DeBruine [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7523-5539>)",
    "url": "https://github.com/debruine/glossary,\nhttps://debruine.github.io/glossary/",
    "bug_reports": "https://github.com/debruine/glossary/issues",
    "repository": "https://cran.r-project.org/package=glossary",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glossary Glossaries for Markdown and Quarto Documents Add glossaries to markdown and quarto documents by tagging individual words. Definitions can be provided inline or in a separate file.  "
  },
  {
    "id": 13464,
    "package_name": "glossr",
    "title": "Use Interlinear Glosses in R Markdown",
    "description": "Read examples with interlinear glosses from files\n    or from text and print them in a way compatible with both\n    Latex and HTML outputs.",
    "version": "0.8.0",
    "maintainer": "Mariana Montes <montesmariana@gmail.com>",
    "author": "Mariana Montes [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3869-3207>),\n  Benjamin Chauvette [cph] (Author of included leipzig.js library)",
    "url": "https://montesmariana.github.io/glossr/,\nhttps://github.com/montesmariana/glossr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glossr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glossr Use Interlinear Glosses in R Markdown Read examples with interlinear glosses from files\n    or from text and print them in a way compatible with both\n    Latex and HTML outputs.  "
  },
  {
    "id": 13470,
    "package_name": "gluedown",
    "title": "Wrap Vectors in Markdown Formatting",
    "description": "Ease the transition between R vectors and markdown text. With\n    'gluedown' and 'rmarkdown', users can create traditional vectors in R,\n    glue those strings together with the markdown syntax, and print those\n    formatted vectors directly to the document. This package primarily\n    uses GitHub Flavored Markdown (GFM), an offshoot of the unambiguous\n    CommonMark specification by John MacFarlane (2019)\n    <https://spec.commonmark.org/>.",
    "version": "1.0.9",
    "maintainer": "Kiernan Nicholls <k5cents@gmail.com>",
    "author": "Kiernan Nicholls [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9229-7897>)",
    "url": "https://k5cents.github.io/gluedown/,\nhttps://github.com/k5cents/gluedown/",
    "bug_reports": "https://github.com/k5cents/gluedown/issues",
    "repository": "https://cran.r-project.org/package=gluedown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gluedown Wrap Vectors in Markdown Formatting Ease the transition between R vectors and markdown text. With\n    'gluedown' and 'rmarkdown', users can create traditional vectors in R,\n    glue those strings together with the markdown syntax, and print those\n    formatted vectors directly to the document. This package primarily\n    uses GitHub Flavored Markdown (GFM), an offshoot of the unambiguous\n    CommonMark specification by John MacFarlane (2019)\n    <https://spec.commonmark.org/>.  "
  },
  {
    "id": 13475,
    "package_name": "gm",
    "title": "Create Music with Ease",
    "description": "Provides a simple and intuitive high-level language for music\n    representation. Generates and embeds music scores and audio files in\n    'RStudio', 'R Markdown' documents, and R 'Jupyter Notebooks'.\n    Internally, uses 'MusicXML' <https://github.com/w3c/musicxml> to represent\n    music, and 'MuseScore' <https://musescore.org/> to convert 'MusicXML'.",
    "version": "2.0.0",
    "maintainer": "Renfei Mao <renfeimao@gmail.com>",
    "author": "Renfei Mao",
    "url": "https://github.com/flujoo/gm, https://flujoo.github.io/gm/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gm Create Music with Ease Provides a simple and intuitive high-level language for music\n    representation. Generates and embeds music scores and audio files in\n    'RStudio', 'R Markdown' documents, and R 'Jupyter Notebooks'.\n    Internally, uses 'MusicXML' <https://github.com/w3c/musicxml> to represent\n    music, and 'MuseScore' <https://musescore.org/> to convert 'MusicXML'.  "
  },
  {
    "id": 13518,
    "package_name": "gofigR",
    "title": "Client for 'GoFigr.io'",
    "description": "Integrates with your 'RMarkdown' documents to automatically publish\n  figures to the <https://GoFigr.io> service. Supports both 'knitr' and interactive \n  execution within 'RStudio'.",
    "version": "1.1.3",
    "maintainer": "Maciej Pacula <maciej@gofigr.io>",
    "author": "Maciej Pacula [cre, aut],\n  Flagstaff Solutions, LLC [cph]",
    "url": "https://github.com/GoFigr/gofigR",
    "bug_reports": "https://github.com/GoFigr/gofigR/issues",
    "repository": "https://cran.r-project.org/package=gofigR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gofigR Client for 'GoFigr.io' Integrates with your 'RMarkdown' documents to automatically publish\n  figures to the <https://GoFigr.io> service. Supports both 'knitr' and interactive \n  execution within 'RStudio'.  "
  },
  {
    "id": 13531,
    "package_name": "googleAnalyticsR",
    "title": "Google Analytics API into R",
    "description": "Interact with the Google Analytics \n  APIs <https://developers.google.com/analytics/>, including \n  the Core Reporting API (v3 and v4), Management API, User Activity API\n  GA4's Data API and Admin API and Multi-Channel Funnel API.",
    "version": "1.2.0",
    "maintainer": "Erik Gr\u00f6nroos <erik.gronroos@8-bit-sheep.com>",
    "author": "Mark Edmondson [aut] (ORCID: <https://orcid.org/0000-0002-8434-3881>),\n  Erik Gr\u00f6nroos [cre],\n  Artem Klevtsov [ctb],\n  Johann deBoer [ctb],\n  David Watkins [ctb],\n  Olivia Brode-Roger [ctb],\n  Jas Sohi [ctb],\n  Zoran Selinger [ctb],\n  Octavian Corlade [ctb],\n  Maegan Whytock [ctb],\n  Masaki Terashi [ctb]",
    "url": "https://github.com/8-bit-sheep/googleAnalyticsR/",
    "bug_reports": "https://github.com/8-bit-sheep/googleAnalyticsR/issues",
    "repository": "https://cran.r-project.org/package=googleAnalyticsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "googleAnalyticsR Google Analytics API into R Interact with the Google Analytics \n  APIs <https://developers.google.com/analytics/>, including \n  the Core Reporting API (v3 and v4), Management API, User Activity API\n  GA4's Data API and Admin API and Multi-Channel Funnel API.  "
  },
  {
    "id": 13535,
    "package_name": "googleErrorReportingR",
    "title": "Send Error Reports to the Google Error Reporting Service API",
    "description": "Send error reports to the Google Error Reporting service <https://cloud.google.com/error-reporting/> and view errors and assign error status in the Google Error Reporting user interface.",
    "version": "0.0.4",
    "maintainer": "Frans van Dunn\u00e9 <frans@ixpantia.com>",
    "author": "ixpantia, SRL [cph],\n  Frans van Dunn\u00e9 [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-7853-2811>),\n  Andrea Vargas [ctb]",
    "url": "https://github.com/ixpantia/googleErrorReportingR,\nhttps://ixpantia.github.io/googleErrorReportingR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=googleErrorReportingR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "googleErrorReportingR Send Error Reports to the Google Error Reporting Service API Send error reports to the Google Error Reporting service <https://cloud.google.com/error-reporting/> and view errors and assign error status in the Google Error Reporting user interface.  "
  },
  {
    "id": 13536,
    "package_name": "googlePolylines",
    "title": "Encoding Coordinates into 'Google' Polylines",
    "description": "Encodes simple feature ('sf') objects and coordinates, and decodes polylines \n    using the 'Google' polyline encoding algorithm (<https://developers.google.com/maps/documentation/utilities/polylinealgorithm>).",
    "version": "0.8.7",
    "maintainer": "David Cooley <dcooley@symbolix.com.au>",
    "author": "David Cooley [aut, cre],\n  Paulo Barcelos [ctb] (Author of c++ decode_polyline),\n  Chris Muir [ctb],\n  Michael Chirico [ctb]",
    "url": "https://github.com/SymbolixAU/googlePolylines",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=googlePolylines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "googlePolylines Encoding Coordinates into 'Google' Polylines Encodes simple feature ('sf') objects and coordinates, and decodes polylines \n    using the 'Google' polyline encoding algorithm (<https://developers.google.com/maps/documentation/utilities/polylinealgorithm>).  "
  },
  {
    "id": 13544,
    "package_name": "googletraffic",
    "title": "Google Traffic",
    "description": "Create geographically referenced traffic data from the Google Maps JavaScript API <https://developers.google.com/maps/documentation/javascript/examples/layer-traffic>.",
    "version": "0.1.7",
    "maintainer": "Robert Marty <rmarty@worldbank.org>",
    "author": "Robert Marty [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3164-3813>)",
    "url": "https://dime-worldbank.github.io/googletraffic/",
    "bug_reports": "https://github.com/dime-worldbank/googletraffic/issues",
    "repository": "https://cran.r-project.org/package=googletraffic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "googletraffic Google Traffic Create geographically referenced traffic data from the Google Maps JavaScript API <https://developers.google.com/maps/documentation/javascript/examples/layer-traffic>.  "
  },
  {
    "id": 13549,
    "package_name": "gotop",
    "title": "Scroll Back to Top Icon in Shiny and R Markdown",
    "description": "Add a scroll back to top 'Font Awesome' icon \n    <https://fontawesome.com/> in 'rmarkdown' documents and 'shiny'\n    apps thanks to 'jQuery GoTop' <https://scottdorman.blog/jquery-gotop/>.",
    "version": "0.1.4",
    "maintainer": "F\u00e9lix Luginbuhl <felix.luginbuhl@protonmail.ch>",
    "author": "F\u00e9lix Luginbuhl [aut, cre]",
    "url": "https://felixluginbuhl.com/gotop/,\nhttps://github.com/lgnbhl/gotop/",
    "bug_reports": "https://github.com/lgnbhl/gotop/issues/",
    "repository": "https://cran.r-project.org/package=gotop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gotop Scroll Back to Top Icon in Shiny and R Markdown Add a scroll back to top 'Font Awesome' icon \n    <https://fontawesome.com/> in 'rmarkdown' documents and 'shiny'\n    apps thanks to 'jQuery GoTop' <https://scottdorman.blog/jquery-gotop/>.  "
  },
  {
    "id": 13551,
    "package_name": "govdown",
    "title": "GOV.UK Style Templates for R Markdown",
    "description": "A suite of custom R Markdown formats and templates\n    for authoring web pages styled with the GOV.UK Design System.",
    "version": "0.10.1",
    "maintainer": "Duncan Garmonsway <duncan.garmonsway@digital.cabinet-office.gov.uk>",
    "author": "Crown Copyright 2019 [cph],\n  Duncan Garmonsway [aut, cre]",
    "url": "https://ukgovdatascience.github.io/govdown/",
    "bug_reports": "https://github.com/ukgovdatascience/govdown/issues",
    "repository": "https://cran.r-project.org/package=govdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "govdown GOV.UK Style Templates for R Markdown A suite of custom R Markdown formats and templates\n    for authoring web pages styled with the GOV.UK Design System.  "
  },
  {
    "id": 13556,
    "package_name": "goxygen",
    "title": "In-Code Documentation for 'GAMS'",
    "description": "A collection of tools which extract a model documentation from 'GAMS' code and comments. \n             In order to use the package you need to install 'pandoc' and 'pandoc-citeproc' \n             first (<https://pandoc.org/>).",
    "version": "1.4.5",
    "maintainer": "Jan Philipp Dietrich <dietrich@pik-potsdam.de>",
    "author": "Jan Philipp Dietrich [aut, cre],\n  Kristine Karstens [aut],\n  David Klein [aut],\n  Lavinia Baumstark [aut],\n  Falk Benke [aut]",
    "url": "https://github.com/pik-piam/goxygen,\nhttps://doi.org/10.5281/zenodo.1411404",
    "bug_reports": "https://github.com/pik-piam/goxygen/issues",
    "repository": "https://cran.r-project.org/package=goxygen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "goxygen In-Code Documentation for 'GAMS' A collection of tools which extract a model documentation from 'GAMS' code and comments. \n             In order to use the package you need to install 'pandoc' and 'pandoc-citeproc' \n             first (<https://pandoc.org/>).  "
  },
  {
    "id": 13576,
    "package_name": "gptzeror",
    "title": "Identify Text Written by Large Language Models using 'GPTZero'",
    "description": "An R interface to the 'GPTZero' API (<https://gptzero.me/docs>). Allows \n    users to classify text into human and computer written with probabilities. Formats\n    the data into data frames where each sentence is an observation. Paragraph-level\n    and document-level predictions are organized to align with the sentences.",
    "version": "0.0.2",
    "maintainer": "Christopher T. Kenny <ctkenny@proton.me>",
    "author": "Christopher T. Kenny [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9386-6860>)",
    "url": "https://github.com/christopherkenny/gptzeror,\nhttps://christophertkenny.com/gptzeror/",
    "bug_reports": "https://github.com/christopherkenny/gptzeror/issues",
    "repository": "https://cran.r-project.org/package=gptzeror",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gptzeror Identify Text Written by Large Language Models using 'GPTZero' An R interface to the 'GPTZero' API (<https://gptzero.me/docs>). Allows \n    users to classify text into human and computer written with probabilities. Formats\n    the data into data frames where each sentence is an observation. Paragraph-level\n    and document-level predictions are organized to align with the sentences.  "
  },
  {
    "id": 13587,
    "package_name": "grainscape",
    "title": "Landscape Connectivity, Habitat, and Protected Area Networks",
    "description": "Given a landscape resistance surface, creates minimum planar graph\n    (Fall et al. (2007) <doi:10.1007/s10021-007-9038-7>) and grains of connectivity\n    (Galpern et al. (2012) <doi:10.1111/j.1365-294X.2012.05677.x>) models that can be\n    used to calculate effective distances for landscape connectivity at multiple scales.\n    Documentation is provided by several vignettes, and a paper\n    (Chubaty, Galpern & Doctolero (2020) <doi:10.1111/2041-210X.13350>).",
    "version": "0.5.0",
    "maintainer": "Alex M Chubaty <achubaty@for-cast.ca>",
    "author": "Paul Galpern [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-0099-3981>),\n  Sam Doctolero [aut],\n  Alex M Chubaty [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7146-8135>)",
    "url": "https://www.alexchubaty.com/grainscape/,\nhttps://github.com/achubaty/grainscape",
    "bug_reports": "https://github.com/achubaty/grainscape/issues",
    "repository": "https://cran.r-project.org/package=grainscape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grainscape Landscape Connectivity, Habitat, and Protected Area Networks Given a landscape resistance surface, creates minimum planar graph\n    (Fall et al. (2007) <doi:10.1007/s10021-007-9038-7>) and grains of connectivity\n    (Galpern et al. (2012) <doi:10.1111/j.1365-294X.2012.05677.x>) models that can be\n    used to calculate effective distances for landscape connectivity at multiple scales.\n    Documentation is provided by several vignettes, and a paper\n    (Chubaty, Galpern & Doctolero (2020) <doi:10.1111/2041-210X.13350>).  "
  },
  {
    "id": 13589,
    "package_name": "grand",
    "title": "Guidelines for Reporting About Network Data",
    "description": "Interactively applies the Guidelines for Reporting About Network Data (GRAND) to an 'igraph' object, and generates a uniform narrative or tabular description of the object.",
    "version": "0.9.1",
    "maintainer": "Zachary Neal <zpneal@msu.edu>",
    "author": "Zachary Neal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3076-4995>)",
    "url": "https://github.com/zpneal/grand",
    "bug_reports": "https://github.com/zpneal/grand/issues",
    "repository": "https://cran.r-project.org/package=grand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grand Guidelines for Reporting About Network Data Interactively applies the Guidelines for Reporting About Network Data (GRAND) to an 'igraph' object, and generates a uniform narrative or tabular description of the object.  "
  },
  {
    "id": 13597,
    "package_name": "graph3d",
    "title": "A Wrapper of the JavaScript Library 'vis-graph3d'",
    "description": "Create interactive visualization charts to draw data in three dimensional graphs. The graphs can be included in Shiny apps and R markdown documents, or viewed from the R console and 'RStudio' Viewer. Based on the 'vis.js' Graph3d module and the 'htmlwidgets' R package.",
    "version": "0.2.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre] (R interface),\n  B. V. Almende [aut, cph] (vis.js library),\n  vis.js contributors [aut, cph]",
    "url": "https://github.com/stla/graph3d",
    "bug_reports": "https://github.com/stla/graph3d/issues",
    "repository": "https://cran.r-project.org/package=graph3d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "graph3d A Wrapper of the JavaScript Library 'vis-graph3d' Create interactive visualization charts to draw data in three dimensional graphs. The graphs can be included in Shiny apps and R markdown documents, or viewed from the R console and 'RStudio' Viewer. Based on the 'vis.js' Graph3d module and the 'htmlwidgets' R package.  "
  },
  {
    "id": 13612,
    "package_name": "graposas",
    "title": "Graphical Approach Optimal Sample Size",
    "description": "Graphical approach provides a useful framework for multiplicity adjustment in clinical trials with multiple endpoints. This package includes statistical methods to optimize sample size over initial weight and transition probability in a graphical approach under a common setting, which is to use marginal power for each endpoint in a trial design.\n    See Zhang, F. and Gou, J. (2023). Sample size optimization for clinical trials using graphical approaches for multiplicity adjustment, Technical Report.",
    "version": "1.0.0",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing (Zoe) Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=graposas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "graposas Graphical Approach Optimal Sample Size Graphical approach provides a useful framework for multiplicity adjustment in clinical trials with multiple endpoints. This package includes statistical methods to optimize sample size over initial weight and transition probability in a graphical approach under a common setting, which is to use marginal power for each endpoint in a trial design.\n    See Zhang, F. and Gou, J. (2023). Sample size optimization for clinical trials using graphical approaches for multiplicity adjustment, Technical Report.  "
  },
  {
    "id": 13614,
    "package_name": "grateful",
    "title": "Facilitate Citation of R Packages",
    "description": "Facilitates the citation of R packages used in analysis\n    projects. Scans project for packages used, gets their citations, and\n    produces a document with citations in the preferred bibliography\n    format, ready to be pasted into reports or manuscripts. Alternatively,\n    'grateful' can be used directly within an 'R Markdown' or 'Quarto' document.",
    "version": "0.3.0",
    "maintainer": "Francisco Rodriguez-Sanchez <f.rodriguez.sanc@gmail.com>",
    "author": "Francisco Rodriguez-Sanchez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7981-1599>),\n  Connor P. Jackson [aut] (ORCID:\n    <https://orcid.org/0000-0002-4220-5210>),\n  Shaurita D. Hutchins [ctb],\n  James M. Clawson [ctb]",
    "url": "https://pakillo.github.io/grateful/",
    "bug_reports": "https://github.com/Pakillo/grateful/issues",
    "repository": "https://cran.r-project.org/package=grateful",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grateful Facilitate Citation of R Packages Facilitates the citation of R packages used in analysis\n    projects. Scans project for packages used, gets their citations, and\n    produces a document with citations in the preferred bibliography\n    format, ready to be pasted into reports or manuscripts. Alternatively,\n    'grateful' can be used directly within an 'R Markdown' or 'Quarto' document.  "
  },
  {
    "id": 13632,
    "package_name": "greenfeedr",
    "title": "Process and Report 'GreenFeed' Data",
    "description": "Provides tools for downloading, processing, and reporting daily and finalized 'GreenFeed' data.",
    "version": "1.3.1",
    "maintainer": "Guillermo Martinez-Boggio <gmartinezboggio@ucdavis.edu>",
    "author": "Guillermo Martinez-Boggio [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-3597-9426>),\n  Meredith Harrison [ctb],\n  Patrick Lutz [ctb]",
    "url": "https://github.com/GMBog/greenfeedr",
    "bug_reports": "https://github.com/GMBog/greenfeedr/issues",
    "repository": "https://cran.r-project.org/package=greenfeedr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "greenfeedr Process and Report 'GreenFeed' Data Provides tools for downloading, processing, and reporting daily and finalized 'GreenFeed' data.  "
  },
  {
    "id": 13635,
    "package_name": "greport",
    "title": "Graphical Reporting for Clinical Trials",
    "description": "Contains many functions useful for\n    monitoring and reporting the results of clinical trials and other\n    experiments in which treatments are compared.  LaTeX is\n    used to typeset the resulting reports, recommended to be in the\n    context of 'knitr'. The 'Hmisc', 'ggplot2', and 'lattice' packages are used\n    by 'greport' for high-level graphics.",
    "version": "0.7-4",
    "maintainer": "Frank E Harrell Jr <fh@fharrell.com>",
    "author": "Frank E Harrell Jr <fh@fharrell.com>",
    "url": "http://hbiostat.org/R/greport/,\nhttps://github.com/harrelfe/greport/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=greport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "greport Graphical Reporting for Clinical Trials Contains many functions useful for\n    monitoring and reporting the results of clinical trials and other\n    experiments in which treatments are compared.  LaTeX is\n    used to typeset the resulting reports, recommended to be in the\n    context of 'knitr'. The 'Hmisc', 'ggplot2', and 'lattice' packages are used\n    by 'greport' for high-level graphics.  "
  },
  {
    "id": 13637,
    "package_name": "gretlR",
    "title": "A Seamless Integration of 'Gretl' and 'R'",
    "description": "It allows running 'gretl' (<http://gretl.sourceforge.net/index.html>) program from R, R Markdown and Quarto. 'gretl' ('Gnu' Regression, 'Econometrics', and Time-series Library) is a statistical software for Econometric analysis.  This package does not only integrate 'gretl' and 'R' but also serves  as a 'gretl' Knit-Engine for 'knitr' package. Write all your 'gretl' commands in 'R', R Markdown chunk.",
    "version": "0.1.4",
    "maintainer": "Sagiru Mati <smati@smati.com.ng>",
    "author": "Sagiru Mati [aut, cre] (ORCID: <https://orcid.org/0000-0003-1413-3974>)",
    "url": "https://CRAN.R-project.org/package=gretlR",
    "bug_reports": "https://github.com/sagirumati/gretlR/issues",
    "repository": "https://cran.r-project.org/package=gretlR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gretlR A Seamless Integration of 'Gretl' and 'R' It allows running 'gretl' (<http://gretl.sourceforge.net/index.html>) program from R, R Markdown and Quarto. 'gretl' ('Gnu' Regression, 'Econometrics', and Time-series Library) is a statistical software for Econometric analysis.  This package does not only integrate 'gretl' and 'R' but also serves  as a 'gretl' Knit-Engine for 'knitr' package. Write all your 'gretl' commands in 'R', R Markdown chunk.  "
  },
  {
    "id": 13649,
    "package_name": "gridify",
    "title": "Enrich Figures and Tables with Custom Headers and Footers and\nMore",
    "description": "A simple and flexible tool designed to create enriched figures and tables by providing a way to add text \n    around them through predefined or custom layouts. \n    Any input which is convertible to 'grob' is supported, like 'ggplot', 'gt' or 'flextable'. \n    Based on R 'grid' graphics, for more details see Paul Murrell (2018) <doi:10.1201/9780429422768>.",
    "version": "0.7.5",
    "maintainer": "Maciej Nasinski <Maciej.Nasinski@ucb.com>",
    "author": "Maciej Nasinski [aut, cre],\n  Alexandra Wall [aut],\n  Sarah Robson [aut],\n  Pritish Dash [aut],\n  Jennifer Winick-Ng [aut],\n  Lily Nan [ctb],\n  Alphonse Kwizera [ctb],\n  Agota Bodoni [ctb],\n  Eilis Meldrum-Dolan [ctb],\n  Gary Cao [ctb],\n  UCB S.A., Belgium [cph, fnd]",
    "url": "https://pharmaverse.github.io/gridify/",
    "bug_reports": "https://github.com/pharmaverse/gridify/issues",
    "repository": "https://cran.r-project.org/package=gridify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gridify Enrich Figures and Tables with Custom Headers and Footers and\nMore A simple and flexible tool designed to create enriched figures and tables by providing a way to add text \n    around them through predefined or custom layouts. \n    Any input which is convertible to 'grob' is supported, like 'ggplot', 'gt' or 'flextable'. \n    Based on R 'grid' graphics, for more details see Paul Murrell (2018) <doi:10.1201/9780429422768>.  "
  },
  {
    "id": 13653,
    "package_name": "gridtext",
    "title": "Improved Text Rendering Support for 'Grid' Graphics",
    "description": "Provides support for rendering of formatted text using 'grid' graphics. Text can be\n    formatted via a minimal subset of 'Markdown', 'HTML', and inline 'CSS' directives, and it can be\n    rendered both with and without word wrap.",
    "version": "0.1.5",
    "maintainer": "Brenton M. Wiernik <brenton@wiernik.org>",
    "author": "Claus O. Wilke [aut] (ORCID: <https://orcid.org/0000-0002-7470-9261>),\n  Brenton M. Wiernik [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9560-6336>, Twitter: @bmwiernik)",
    "url": "https://wilkelab.org/gridtext/",
    "bug_reports": "https://github.com/wilkelab/gridtext/issues",
    "repository": "https://cran.r-project.org/package=gridtext",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gridtext Improved Text Rendering Support for 'Grid' Graphics Provides support for rendering of formatted text using 'grid' graphics. Text can be\n    formatted via a minimal subset of 'Markdown', 'HTML', and inline 'CSS' directives, and it can be\n    rendered both with and without word wrap.  "
  },
  {
    "id": 13657,
    "package_name": "grobblR",
    "title": "Creating Flexible, Reproducible 'PDF' Reports",
    "description": "A tool which allows users the ability to intuitively create \n  flexible, reproducible portable document format reports comprised of \n  aesthetically pleasing tables, images, plots and/or text.",
    "version": "0.2.2",
    "maintainer": "Calvin Floyd <calvin.michael.floyd@gmail.com>",
    "author": "Calvin Floyd [aut, cre, cph]",
    "url": "https://github.com/calvinmfloyd/grobblR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=grobblR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grobblR Creating Flexible, Reproducible 'PDF' Reports A tool which allows users the ability to intuitively create \n  flexible, reproducible portable document format reports comprised of \n  aesthetically pleasing tables, images, plots and/or text.  "
  },
  {
    "id": 13660,
    "package_name": "groqR",
    "title": "A Coding Assistant using the Fast AI Inference 'Groq'",
    "description": "A comprehensive suite of functions and 'RStudio' Add-ins leveraging the capabilities of open-source Large Language Models (LLMs) to support R developers. These functions offer a range of utilities, including text rewriting, translation, and general query capabilities. Additionally, the programming-focused functions provide assistance with debugging, translating, commenting, documenting, and unit testing code, as well as suggesting variable and function names, thereby streamlining the development process.",
    "version": "0.0.3",
    "maintainer": "Gabriel Kaiser <quantresearch.gk@gmail.com>",
    "author": "Gabriel Kaiser [aut, cre]",
    "url": "https://github.com/GabrielKaiserQFin/groqR",
    "bug_reports": "https://github.com/GabrielKaiserQFin/groqR/issues",
    "repository": "https://cran.r-project.org/package=groqR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "groqR A Coding Assistant using the Fast AI Inference 'Groq' A comprehensive suite of functions and 'RStudio' Add-ins leveraging the capabilities of open-source Large Language Models (LLMs) to support R developers. These functions offer a range of utilities, including text rewriting, translation, and general query capabilities. Additionally, the programming-focused functions provide assistance with debugging, translating, commenting, documenting, and unit testing code, as well as suggesting variable and function names, thereby streamlining the development process.  "
  },
  {
    "id": 13732,
    "package_name": "gto",
    "title": "Insert 'gt' Tables into Word Documents",
    "description": "Insert tables created by the 'gt' R package into 'Microsoft Word' \n    documents. This gives users the ability to add to their existing word documents\n    the tables made in 'gt' using the familiar 'officer' package and syntax from\n    the 'officeverse'.",
    "version": "0.1.2",
    "maintainer": "Ellis Hughes <ellis.h.hughes@gsk.com>",
    "author": "Ellis Hughes [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0637-4436>),\n  GlaxoSmithKline Research & Development Limited [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gto Insert 'gt' Tables into Word Documents Insert tables created by the 'gt' R package into 'Microsoft Word' \n    documents. This gives users the ability to add to their existing word documents\n    the tables made in 'gt' using the familiar 'officer' package and syntax from\n    the 'officeverse'.  "
  },
  {
    "id": 13787,
    "package_name": "hahmmr",
    "title": "Haplotype-Aware Hidden Markov Model for RNA",
    "description": "Haplotype-aware Hidden Markov Model for RNA (HaHMMR) is a method for detecting copy number variations (CNVs) from bulk RNA-seq data. Additional examples, documentations, and details on the method are available at <https://github.com/kharchenkolab/hahmmr/>.",
    "version": "1.0.0",
    "maintainer": "Teng Gao <tgaoteng@gmail.com>",
    "author": "Teng Gao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0196-689X>),\n  Evan Biederstedt [aut],\n  Peter Kharchenko [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hahmmr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hahmmr Haplotype-Aware Hidden Markov Model for RNA Haplotype-aware Hidden Markov Model for RNA (HaHMMR) is a method for detecting copy number variations (CNVs) from bulk RNA-seq data. Additional examples, documentations, and details on the method are available at <https://github.com/kharchenkolab/hahmmr/>.  "
  },
  {
    "id": 13788,
    "package_name": "hakaiApi",
    "title": "Authenticated HTTP Request Client for the 'Hakai' API",
    "description": "Initializes a class that obtains API credentials and provides\n    a method to use those credentials to make GET requests to the 'Hakai'\n    API server. Usage instructions are documented at\n    <https://hakaiinstitute.github.io/hakai-api/>.",
    "version": "1.0.5",
    "maintainer": "Sam Albers <sam.albers@hakai.org>",
    "author": "Sam Albers [aut, cre],\n  Taylor Denouden [aut],\n  Brett Johnson [aut],\n  Nate Rosenstock [ctb],\n  Chris Davis [ctb],\n  Hakai Institute [cph]",
    "url": "https://github.com/HakaiInstitute/hakai-api-client-r",
    "bug_reports": "https://github.com/HakaiInstitute/hakai-api-client-r/issues",
    "repository": "https://cran.r-project.org/package=hakaiApi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hakaiApi Authenticated HTTP Request Client for the 'Hakai' API Initializes a class that obtains API credentials and provides\n    a method to use those credentials to make GET requests to the 'Hakai'\n    API server. Usage instructions are documented at\n    <https://hakaiinstitute.github.io/hakai-api/>.  "
  },
  {
    "id": 13797,
    "package_name": "handwriter",
    "title": "Handwriting Analysis in R",
    "description": "Perform statistical writership analysis of scanned handwritten documents.\n             Webpage provided at: <https://github.com/CSAFE-ISU/handwriter>.",
    "version": "3.2.4",
    "maintainer": "Stephanie Reinders <srein@iastate.edu>",
    "author": "Iowa State University of Science and Technology on behalf of its Center\n    for Statistics and Applications in Forensic Evidence [aut, cph,\n    fnd],\n  Nick Berry [aut],\n  Stephanie Reinders [aut, cre],\n  James Taylor [aut],\n  Felix Baez-Santiago [ctb],\n  Jon Gonz\u00e1lez [ctb]",
    "url": "https://github.com/CSAFE-ISU/handwriter",
    "bug_reports": "https://github.com/CSAFE-ISU/handwriter/issues",
    "repository": "https://cran.r-project.org/package=handwriter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "handwriter Handwriting Analysis in R Perform statistical writership analysis of scanned handwritten documents.\n             Webpage provided at: <https://github.com/CSAFE-ISU/handwriter>.  "
  },
  {
    "id": 13798,
    "package_name": "handwriterApp",
    "title": "A 'shiny' Application for Handwriting Analysis",
    "description": "Perform statistical writership analysis of scanned handwritten documents with a 'shiny' app for 'handwriter'.",
    "version": "2.0.0",
    "maintainer": "Stephanie Reinders <srein@iastate.edu>",
    "author": "Iowa State University of Science and Technology on behalf of its Center\n    for Statistics and Applications in Forensic Evidence [aut, cph,\n    fnd],\n  Stephanie Reinders [aut, cre]",
    "url": "https://github.com/CSAFE-ISU/handwriterApp",
    "bug_reports": "https://github.com/CSAFE-ISU/handwriterApp/issues",
    "repository": "https://cran.r-project.org/package=handwriterApp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "handwriterApp A 'shiny' Application for Handwriting Analysis Perform statistical writership analysis of scanned handwritten documents with a 'shiny' app for 'handwriter'.  "
  },
  {
    "id": 13799,
    "package_name": "handwriterRF",
    "title": "Handwriting Analysis with Random Forests",
    "description": "Perform forensic handwriting analysis of two scanned handwritten documents. This package implements the statistical method described by Madeline Johnson and Danica Ommen (2021) <doi:10.1002/sam.11566>. Similarity measures and a random forest produce a score-based likelihood ratio that quantifies the strength of the evidence in favor of the documents being written by the same writer or different writers.",
    "version": "1.1.1",
    "maintainer": "Stephanie Reinders <reinders.stephanie@gmail.com>",
    "author": "Iowa State University of Science and Technology on behalf of its Center\n    for Statistics and Applications in Forensic Evidence [aut, cph,\n    fnd],\n  Stephanie Reinders [aut, cre]",
    "url": "https://github.com/CSAFE-ISU/handwriterRF",
    "bug_reports": "https://github.com/CSAFE-ISU/handwriterRF/issues",
    "repository": "https://cran.r-project.org/package=handwriterRF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "handwriterRF Handwriting Analysis with Random Forests Perform forensic handwriting analysis of two scanned handwritten documents. This package implements the statistical method described by Madeline Johnson and Danica Ommen (2021) <doi:10.1002/sam.11566>. Similarity measures and a random forest produce a score-based likelihood ratio that quantifies the strength of the evidence in favor of the documents being written by the same writer or different writers.  "
  },
  {
    "id": 13814,
    "package_name": "harmonydata",
    "title": "R Library for 'Harmony'",
    "description": "'Harmony' is a tool using AI which allows you to compare items from questionnaires and identify similar content. You can try 'Harmony' at <https://harmonydata.ac.uk/app/> and you can read our blog at <https://harmonydata.ac.uk/blog/> or at <https://fastdatascience.com/how-does-harmony-work/>. Documentation at <https://harmonydata.ac.uk/harmony-r-released/>.",
    "version": "0.3.1",
    "maintainer": "Omar Hassoun <omtarful@gmail.com>",
    "author": "Omar Hassoun [aut, cre],\n  Thomas Wood [ctb],\n  Alex, Nikic [ctb],\n  Ulster University [cph]",
    "url": "<https://harmonydata.ac.uk>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=harmonydata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "harmonydata R Library for 'Harmony' 'Harmony' is a tool using AI which allows you to compare items from questionnaires and identify similar content. You can try 'Harmony' at <https://harmonydata.ac.uk/app/> and you can read our blog at <https://harmonydata.ac.uk/blog/> or at <https://fastdatascience.com/how-does-harmony-work/>. Documentation at <https://harmonydata.ac.uk/harmony-r-released/>.  "
  },
  {
    "id": 13853,
    "package_name": "hdf5r",
    "title": "Interface to the 'HDF5' Binary Data Format",
    "description": "'HDF5' is a data model, library and file format for storing\n    and managing large amounts of data. This package provides a nearly\n    feature complete, object oriented  wrapper for the 'HDF5' API\n    <https://support.hdfgroup.org/documentation/hdf5/latest/_r_m.html> using R6 classes.\n    Additionally, functionality is added so that 'HDF5' objects behave very\n    similar to their corresponding R counterparts.",
    "version": "1.3.12",
    "maintainer": "Holger Hoefling <hhoeflin@gmail.com>",
    "author": "Holger Hoefling [aut, cre],\n  Mario Annau [aut],\n  Novartis Institute for BioMedical Research (NIBR) [cph]",
    "url": "https://hhoeflin.github.io/hdf5r/,\nhttps://github.com/hhoeflin/hdf5r/",
    "bug_reports": "https://github.com/hhoeflin/hdf5r/issues",
    "repository": "https://cran.r-project.org/package=hdf5r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdf5r Interface to the 'HDF5' Binary Data Format 'HDF5' is a data model, library and file format for storing\n    and managing large amounts of data. This package provides a nearly\n    feature complete, object oriented  wrapper for the 'HDF5' API\n    <https://support.hdfgroup.org/documentation/hdf5/latest/_r_m.html> using R6 classes.\n    Additionally, functionality is added so that 'HDF5' objects behave very\n    similar to their corresponding R counterparts.  "
  },
  {
    "id": 13857,
    "package_name": "hdftsa",
    "title": "High-Dimensional Functional Time Series Analysis",
    "description": "Offers methods for visualizing, modelling, and forecasting high-dimensional functional time series, also known as functional panel data. Documentation about 'hdftsa' is provided via the paper by Cristian F. Jimenez-Varon, Ying Sun and Han Lin Shang (2024, <doi:10.1080/10618600.2024.2319166>).",
    "version": "1.0",
    "maintainer": "Han Lin Shang <hanlin.shang@mq.edu.au>",
    "author": "Han Lin Shang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1769-6430>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hdftsa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdftsa High-Dimensional Functional Time Series Analysis Offers methods for visualizing, modelling, and forecasting high-dimensional functional time series, also known as functional panel data. Documentation about 'hdftsa' is provided via the paper by Cristian F. Jimenez-Varon, Ying Sun and Han Lin Shang (2024, <doi:10.1080/10618600.2024.2319166>).  "
  },
  {
    "id": 13877,
    "package_name": "healthiar",
    "title": "Quantify and Monetize the Burden of Disease Attributable to\nExposure",
    "description": "This R package has been developed with a focus on air pollution and noise but can applied to other exposures. The initial development has been funded by the European Union project BEST-COST. Disclaimer: It is work in progress and the developers are not liable for any calculation errors or inaccuracies resulting from the use of this package.\n References (in chronological order): \n WHO (2003a) \"Assessing the environmental burden of disease at national and local levels\" <https://www.who.int/publications/i/item/9241546204> (accessed October 2025);\n WHO (2003b) \"Comparative quantification of health risks: Conceptual framework and methodological issues\" <doi:10.1186/1478-7954-1-1> (accessed October 2025);\n Miller & Hurley (2003) \"Life table methods for quantitative impact assessments in chronic mortality\" <doi:10.1136/jech.57.3.200> (accessed October 2025);\n Steenland & Armstrong (2006) \"An Overview of Methods for Calculating the Burden of Disease Due to Specific Risk Factors\" <doi:10.1097/01.ede.0000229155.05644.43> (accessed October 2025);\n Miller (2010) \"Report on estimation of mortality impacts of particulate air pollution in London\" <https://cleanair.london/app/uploads/CAL-098-Mayors-health-study-report-June-2010-1.pdf> (accessed October 2025);\n WHO (2011) \"Burden of disease from environmental noise\" <https://iris.who.int/items/723ab97c-5c33-4e3b-8df1-744aa5bc1c27> (accessed October 2025);\n Jerrett et al. (2013) \"Spatial Analysis of Air Pollution and Mortality in California\" <doi:10.1164/rccm.201303-0609OC> (accessed October 2025);\n GBD 2019 Risk Factors Collaborators (2020) \"Global burden of 87 risk factors in 204 countries and territories, 1990\u20132019\" <doi:10.1016/S0140-6736(20)30752-2> (accessed October 2025);\n VanderWeele (2019) \"Optimal Approximate Conversions of Odds Ratios and Hazard Ratios to Risk Ratios\" <doi: 10.1111/biom.13197> (accessed October 2025);\n WHO (2020) \"Health impact assessment of air pollution: AirQ+ life table manual\" <https://iris.who.int/bitstream/handle/10665/337683/WHO-EURO-2020-1559-41310-56212-eng.pdf?sequence=1> (accessed October 2025);\n ETC HE (2022) \"Health risk assessment of air pollution and the impact of the new WHO guidelines\" <https://www.eionet.europa.eu/etcs/all-etc-reports> (accessed October 2025);\n Kim et al. (2022) \"DALY Estimation Approaches: Understanding and Using the Incidence-based Approach and the Prevalence-based Approach\" <doi:10.3961/jpmph.21.597> (accessed October 2025);\n Pozzer et al. (2022) \"Mortality Attributable to Ambient Air Pollution: A Review of Global Estimates\" <doi:10.1029/2022GH000711> (accessed October 2025);\n Teaching group in EBM (2022) \"Evidence-based medicine research helper\" <https://ebm-helper.cn/en/Conv/HR_RR.html> (accessed October 2025).",
    "version": "0.2.1",
    "maintainer": "Alberto Castro <alberto.castrofernandez@swisstph.ch>",
    "author": "Alberto Castro [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-4665-3299>),\n  Axel Luyten [aut] (ORCID: <https://orcid.org/0000-0002-7005-5889>),\n  Arno Pauwels [ctb] (ORCID: <https://orcid.org/0000-0001-7519-8080>),\n  Liliana Vazquez Fernandez [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3778-9415>),\n  Vanessa Gorasso [ctb] (ORCID: <https://orcid.org/0000-0001-6884-9316>),\n  Carl Michael Baravelli [ctb] (ORCID:\n    <https://orcid.org/0000-0001-7772-5315>),\n  Susanne Breitner [ctb] (ORCID: <https://orcid.org/0000-0002-0956-6911>),\n  Maria Lepnurm [ctb] (ORCID: <https://orcid.org/0009-0009-4372-6227>),\n  Maria Jose Rueda Lopez [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2443-1038>),\n  Iracy Pimenta [ctb] (ORCID: <https://orcid.org/0000-0003-0032-1536>),\n  Andreia Novais [ctb] (ORCID: <https://orcid.org/0009-0007-7775-108X>),\n  Ana Barbosa [ctb] (ORCID: <https://orcid.org/0000-0002-9623-9002>),\n  Joao Vasco Santos [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4696-1002>),\n  Anette Kocbach Bolling [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4209-7448>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=healthiar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "healthiar Quantify and Monetize the Burden of Disease Attributable to\nExposure This R package has been developed with a focus on air pollution and noise but can applied to other exposures. The initial development has been funded by the European Union project BEST-COST. Disclaimer: It is work in progress and the developers are not liable for any calculation errors or inaccuracies resulting from the use of this package.\n References (in chronological order): \n WHO (2003a) \"Assessing the environmental burden of disease at national and local levels\" <https://www.who.int/publications/i/item/9241546204> (accessed October 2025);\n WHO (2003b) \"Comparative quantification of health risks: Conceptual framework and methodological issues\" <doi:10.1186/1478-7954-1-1> (accessed October 2025);\n Miller & Hurley (2003) \"Life table methods for quantitative impact assessments in chronic mortality\" <doi:10.1136/jech.57.3.200> (accessed October 2025);\n Steenland & Armstrong (2006) \"An Overview of Methods for Calculating the Burden of Disease Due to Specific Risk Factors\" <doi:10.1097/01.ede.0000229155.05644.43> (accessed October 2025);\n Miller (2010) \"Report on estimation of mortality impacts of particulate air pollution in London\" <https://cleanair.london/app/uploads/CAL-098-Mayors-health-study-report-June-2010-1.pdf> (accessed October 2025);\n WHO (2011) \"Burden of disease from environmental noise\" <https://iris.who.int/items/723ab97c-5c33-4e3b-8df1-744aa5bc1c27> (accessed October 2025);\n Jerrett et al. (2013) \"Spatial Analysis of Air Pollution and Mortality in California\" <doi:10.1164/rccm.201303-0609OC> (accessed October 2025);\n GBD 2019 Risk Factors Collaborators (2020) \"Global burden of 87 risk factors in 204 countries and territories, 1990\u20132019\" <doi:10.1016/S0140-6736(20)30752-2> (accessed October 2025);\n VanderWeele (2019) \"Optimal Approximate Conversions of Odds Ratios and Hazard Ratios to Risk Ratios\" <doi: 10.1111/biom.13197> (accessed October 2025);\n WHO (2020) \"Health impact assessment of air pollution: AirQ+ life table manual\" <https://iris.who.int/bitstream/handle/10665/337683/WHO-EURO-2020-1559-41310-56212-eng.pdf?sequence=1> (accessed October 2025);\n ETC HE (2022) \"Health risk assessment of air pollution and the impact of the new WHO guidelines\" <https://www.eionet.europa.eu/etcs/all-etc-reports> (accessed October 2025);\n Kim et al. (2022) \"DALY Estimation Approaches: Understanding and Using the Incidence-based Approach and the Prevalence-based Approach\" <doi:10.3961/jpmph.21.597> (accessed October 2025);\n Pozzer et al. (2022) \"Mortality Attributable to Ambient Air Pollution: A Review of Global Estimates\" <doi:10.1029/2022GH000711> (accessed October 2025);\n Teaching group in EBM (2022) \"Evidence-based medicine research helper\" <https://ebm-helper.cn/en/Conv/HR_RR.html> (accessed October 2025).  "
  },
  {
    "id": 13889,
    "package_name": "heatmaply",
    "title": "Interactive Cluster Heat Maps Using 'plotly' and 'ggplot2'",
    "description": "Create interactive cluster 'heatmaps' that can be saved as a stand-\n    alone HTML file, embedded in 'R Markdown' documents or in a 'Shiny' app, and\n    available in the 'RStudio' viewer pane. Hover the mouse pointer over a cell to\n    show details or drag a rectangle to zoom. A 'heatmap' is a popular graphical\n    method for visualizing high-dimensional data, in which a table of numbers\n    are encoded as a grid of colored cells. The rows and columns of the matrix\n    are ordered to highlight patterns and are often accompanied by 'dendrograms'.\n    'Heatmaps' are used in many fields for visualizing observations, correlations,\n    missing values patterns, and more. Interactive 'heatmaps' allow the inspection\n    of specific value by hovering the mouse over a cell, as well as zooming into\n    a region of the 'heatmap' by dragging a rectangle around the relevant area.\n    This work is based on the 'ggplot2' and 'plotly.js' engine. It produces\n    similar 'heatmaps' to 'heatmap.2' with the advantage of speed\n    ('plotly.js' is able to handle larger size matrix), the ability to zoom from\n    the 'dendrogram' panes, and the placing of factor variables in the sides of the\n    'heatmap'.",
    "version": "1.6.0",
    "maintainer": "Tal Galili <tal.galili@gmail.com>",
    "author": "Tal Galili [aut, cre, cph] (https://www.r-statistics.com),\n  Alan O'Callaghan [aut] (https://github.com/Alanocallaghan),\n  Jonathan Sidi [ctb] (https://github.com/yonicd),\n  Jaehyun Joo [ctb] (https://github.com/jaehyunjoo),\n  Yoav Benjamini [ths],\n  Mathew Simenc [ctb] (https://gitlab.com/mcsimenc,\n    https://github.com/mcsimenc)",
    "url": "https://talgalili.github.io/heatmaply/,\nhttps://cran.r-project.org/package=heatmaply,\nhttps://github.com/talgalili/heatmaply/,\nhttps://www.r-statistics.com/tag/heatmaply/",
    "bug_reports": "https://github.com/talgalili/heatmaply/issues",
    "repository": "https://cran.r-project.org/package=heatmaply",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "heatmaply Interactive Cluster Heat Maps Using 'plotly' and 'ggplot2' Create interactive cluster 'heatmaps' that can be saved as a stand-\n    alone HTML file, embedded in 'R Markdown' documents or in a 'Shiny' app, and\n    available in the 'RStudio' viewer pane. Hover the mouse pointer over a cell to\n    show details or drag a rectangle to zoom. A 'heatmap' is a popular graphical\n    method for visualizing high-dimensional data, in which a table of numbers\n    are encoded as a grid of colored cells. The rows and columns of the matrix\n    are ordered to highlight patterns and are often accompanied by 'dendrograms'.\n    'Heatmaps' are used in many fields for visualizing observations, correlations,\n    missing values patterns, and more. Interactive 'heatmaps' allow the inspection\n    of specific value by hovering the mouse over a cell, as well as zooming into\n    a region of the 'heatmap' by dragging a rectangle around the relevant area.\n    This work is based on the 'ggplot2' and 'plotly.js' engine. It produces\n    similar 'heatmaps' to 'heatmap.2' with the advantage of speed\n    ('plotly.js' is able to handle larger size matrix), the ability to zoom from\n    the 'dendrogram' panes, and the placing of factor variables in the sides of the\n    'heatmap'.  "
  },
  {
    "id": 13894,
    "package_name": "heddlr",
    "title": "Dynamic R Markdown Document Generation",
    "description": "Helper functions designed to make\n    dynamically generating R Markdown documents easier by providing a\n    simple and tidy way to create report pieces, shape them to your data,\n    and combine them for exporting into a single R Markdown document.",
    "version": "0.6.0",
    "maintainer": "Michael Mahoney <mike.mahoney.218@gmail.com>",
    "author": "Michael Mahoney [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2402-304X>)",
    "url": "https://github.com/mikemahoney218/heddlr,\nhttps://mikemahoney218.github.io/heddlr/",
    "bug_reports": "https://github.com/mikemahoney218/heddlr/issues",
    "repository": "https://cran.r-project.org/package=heddlr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "heddlr Dynamic R Markdown Document Generation Helper functions designed to make\n    dynamically generating R Markdown documents easier by providing a\n    simple and tidy way to create report pieces, shape them to your data,\n    and combine them for exporting into a single R Markdown document.  "
  },
  {
    "id": 13897,
    "package_name": "heemod",
    "title": "Markov Models for Health Economic Evaluations",
    "description": "An implementation of the modelling and reporting features described \n    in reference textbook and guidelines (Briggs, Andrew, et al. Decision \n    Modelling for Health Economic Evaluation. Oxford Univ. Press, 2011;\n    Siebert, U. et al. State-Transition Modeling. Medical Decision Making \n    32, 690-700 (2012).): deterministic and probabilistic sensitivity analysis, \n    heterogeneity analysis, time dependency on state-time and model-time \n    (semi-Markov and non-homogeneous Markov models), etc.",
    "version": "1.1.0",
    "maintainer": "Kevin Zarca <kevin.zarca@gmail.com>",
    "author": "Kevin Zarca [aut, cre],\n  Antoine Filipovic-Pierucci [aut],\n  Matthew Wiener [ctb],\n  Zdenek Kabat [ctb],\n  Vojtech Filipec [ctb],\n  Jordan Amdahl [ctb],\n  Yonatan Carranza Alarcon [ctb],\n  Vince Daniels [ctb]",
    "url": "https://aphp.github.io/heemod/",
    "bug_reports": "https://github.com/aphp/heemod/issues",
    "repository": "https://cran.r-project.org/package=heemod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "heemod Markov Models for Health Economic Evaluations An implementation of the modelling and reporting features described \n    in reference textbook and guidelines (Briggs, Andrew, et al. Decision \n    Modelling for Health Economic Evaluation. Oxford Univ. Press, 2011;\n    Siebert, U. et al. State-Transition Modeling. Medical Decision Making \n    32, 690-700 (2012).): deterministic and probabilistic sensitivity analysis, \n    heterogeneity analysis, time dependency on state-time and model-time \n    (semi-Markov and non-homogeneous Markov models), etc.  "
  },
  {
    "id": 13911,
    "package_name": "hereR",
    "title": "'sf'-Based Interface to the 'HERE' REST APIs",
    "description": "Interface to the 'HERE' REST APIs <https://developer.here.com/develop/rest-apis>:\n  (1) geocode and autosuggest addresses or reverse geocode POIs using the 'Geocoder' API;\n  (2) route directions, travel distance or time matrices and isolines using the 'Routing', 'Matrix Routing' and 'Isoline Routing' APIs;\n  (3) request real-time traffic flow and incident information from the 'Traffic' API;\n  (4) find request public transport connections and nearby stations from the 'Public Transit' API;\n  (5) request intermodal routes using the 'Intermodal Routing' API;\n  (6) get weather forecasts, reports on current weather conditions, astronomical\n  information and alerts at a specific location from the 'Destination Weather' API.\n  Locations, routes and isolines are returned as 'sf' objects.",
    "version": "1.1.0",
    "maintainer": "Merlin Unterfinger <info@munterfinger.ch>",
    "author": "Merlin Unterfinger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2020-2366>),\n  Daniel Possenriede [ctb] (ORCID:\n    <https://orcid.org/0000-0002-6738-9845>)",
    "url": "https://munterfi.github.io/hereR/,\nhttps://github.com/munterfi/hereR/",
    "bug_reports": "https://github.com/munterfi/hereR/issues/",
    "repository": "https://cran.r-project.org/package=hereR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hereR 'sf'-Based Interface to the 'HERE' REST APIs Interface to the 'HERE' REST APIs <https://developer.here.com/develop/rest-apis>:\n  (1) geocode and autosuggest addresses or reverse geocode POIs using the 'Geocoder' API;\n  (2) route directions, travel distance or time matrices and isolines using the 'Routing', 'Matrix Routing' and 'Isoline Routing' APIs;\n  (3) request real-time traffic flow and incident information from the 'Traffic' API;\n  (4) find request public transport connections and nearby stations from the 'Public Transit' API;\n  (5) request intermodal routes using the 'Intermodal Routing' API;\n  (6) get weather forecasts, reports on current weather conditions, astronomical\n  information and alerts at a specific location from the 'Destination Weather' API.\n  Locations, routes and isolines are returned as 'sf' objects.  "
  },
  {
    "id": 13927,
    "package_name": "hettreatreg",
    "title": "Heterogeneous Treatment Effects in Regression Analysis",
    "description": "Computes diagnostics for linear regression when treatment effects are heterogeneous.\n    The output of 'hettreatreg' represents ordinary least squares (OLS) \n    estimates of the effect of a binary treatment as a weighted average of the average treatment effect \n    on the treated (ATT) and the average treatment effect on the untreated (ATU). \n    The program estimates the OLS weights on these parameters, computes the associated model diagnostics, \n    and reports the implicit OLS estimate of the average treatment effect (ATE). \n    See Sloczynski (2019), <http://people.brandeis.edu/~tslocz/Sloczynski_paper_regression.pdf>.",
    "version": "0.1.0",
    "maintainer": "Mark McAvoy <mcavoy@brandeis.edu>",
    "author": "Tymon Sloczynski [aut],\n  Mark McAvoy [cre]",
    "url": "https://github.com/tslocz/hettreatreg",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hettreatreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hettreatreg Heterogeneous Treatment Effects in Regression Analysis Computes diagnostics for linear regression when treatment effects are heterogeneous.\n    The output of 'hettreatreg' represents ordinary least squares (OLS) \n    estimates of the effect of a binary treatment as a weighted average of the average treatment effect \n    on the treated (ATT) and the average treatment effect on the untreated (ATU). \n    The program estimates the OLS weights on these parameters, computes the associated model diagnostics, \n    and reports the implicit OLS estimate of the average treatment effect (ATE). \n    See Sloczynski (2019), <http://people.brandeis.edu/~tslocz/Sloczynski_paper_regression.pdf>.  "
  },
  {
    "id": 13954,
    "package_name": "hicp",
    "title": "Harmonised Index of Consumer Prices",
    "description": "The Harmonised Index of Consumer Prices (HICP) is the key economic figure to measure inflation in the euro area.\n              The methodology underlying the HICP is documented in the HICP Methodological Manual (<https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/w/ks-gq-24-003>).\n              Based on the manual, this package provides functions to access and work with HICP data from Eurostat's public database (<https://ec.europa.eu/eurostat/data/database>).",
    "version": "1.0.0",
    "maintainer": "Sebastian Weinand <sebastian.weinand@ec.europa.eu>",
    "author": "Sebastian Weinand [aut, cre]",
    "url": "https://github.com/eurostat/hicp",
    "bug_reports": "https://github.com/eurostat/hicp/issues",
    "repository": "https://cran.r-project.org/package=hicp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hicp Harmonised Index of Consumer Prices The Harmonised Index of Consumer Prices (HICP) is the key economic figure to measure inflation in the euro area.\n              The methodology underlying the HICP is documented in the HICP Methodological Manual (<https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/w/ks-gq-24-003>).\n              Based on the manual, this package provides functions to access and work with HICP data from Eurostat's public database (<https://ec.europa.eu/eurostat/data/database>).  "
  },
  {
    "id": 13972,
    "package_name": "highlightHTML",
    "title": "Highlight HTML Text and Tables",
    "description": "A tool to format R markdown with CSS ids for HTML output. \n    The tool may be most helpful for those using markdown to create reproducible\n    documents. The biggest limitations in formatting is the knowledge of CSS\n    by the document authors.",
    "version": "0.2.5",
    "maintainer": "Brandon LeBeau <lebebr01+highlightHTML@gmail.com>",
    "author": "Brandon LeBeau [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1265-8761>)",
    "url": "https://github.com/lebebr01/highlightHTML",
    "bug_reports": "https://github.com/lebebr01/highlightHTML/issues",
    "repository": "https://cran.r-project.org/package=highlightHTML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highlightHTML Highlight HTML Text and Tables A tool to format R markdown with CSS ids for HTML output. \n    The tool may be most helpful for those using markdown to create reproducible\n    documents. The biggest limitations in formatting is the knowledge of CSS\n    by the document authors.  "
  },
  {
    "id": 13973,
    "package_name": "highlighter",
    "title": "Code Syntax Highlighting using the 'Prism.js' Library",
    "description": "Code Syntax Highlighting made easy for code snippets or complete\n    files. Whether you're documenting your data analysis or creating interactive\n    'shiny' apps.",
    "version": "0.1",
    "maintainer": "Federico Rivadeneira <federivadeneira@gmail.com>",
    "author": "Federico Rivadeneira [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7818-1225>)",
    "url": "https://federiva.github.io/highlighter/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=highlighter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highlighter Code Syntax Highlighting using the 'Prism.js' Library Code Syntax Highlighting made easy for code snippets or complete\n    files. Whether you're documenting your data analysis or creating interactive\n    'shiny' apps.  "
  },
  {
    "id": 13974,
    "package_name": "highlightr",
    "title": "Highlight Conserved Edits Across Versions of a Document",
    "description": "Input multiple versions of a source document, \n    and receive HTML code for a highlighted version of the source document\n    indicating the frequency of occurrence of phrases in the different versions.\n    This method is described in Chapter 3 of \n    Rogers (2024) <https://digitalcommons.unl.edu/dissertations/AAI31240449/>.",
    "version": "1.2.0",
    "maintainer": "Rachel Rogers <rrogers.rpackages@gmail.com>",
    "author": "Center for Statistics and Applications in Forensic Evidence [aut, cph,\n    fnd],\n  Rachel Rogers [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4145-9630>),\n  Susan VanderPlas [aut] (ORCID: <https://orcid.org/0000-0002-3803-0972>)",
    "url": "https://rachelesrogers.github.io/highlightr/,\nhttps://github.com/rachelesrogers/highlightr",
    "bug_reports": "https://github.com/rachelesrogers/highlightr/issues",
    "repository": "https://cran.r-project.org/package=highlightr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highlightr Highlight Conserved Edits Across Versions of a Document Input multiple versions of a source document, \n    and receive HTML code for a highlighted version of the source document\n    indicating the frequency of occurrence of phrases in the different versions.\n    This method is described in Chapter 3 of \n    Rogers (2024) <https://digitalcommons.unl.edu/dissertations/AAI31240449/>.  "
  },
  {
    "id": 13989,
    "package_name": "hindexcalculator",
    "title": "H-Index Calculator using Data from a Web of Science (WoS)\nCitation Report",
    "description": "H(x) is the h-index for the past x years. Here, the h(x) of a scientist/department/etc. can be calculated using the exported excel file from a Web of Science citation report of a search. Also calculated is the year of first publication, total number of publications, and sum of times cited for the specified period. Therefore, for h-10: the date of first publication, total number of publications, and sum of times cited in the past 10 years are calculated. Note: the excel file has to first be saved in a .csv format.",
    "version": "1.0.0",
    "maintainer": "Sepand Alavifard <s.alavifard@gmail.com>",
    "author": "Sepand Alavifard [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hindexcalculator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hindexcalculator H-Index Calculator using Data from a Web of Science (WoS)\nCitation Report H(x) is the h-index for the past x years. Here, the h(x) of a scientist/department/etc. can be calculated using the exported excel file from a Web of Science citation report of a search. Also calculated is the year of first publication, total number of publications, and sum of times cited for the specified period. Therefore, for h-10: the date of first publication, total number of publications, and sum of times cited in the past 10 years are calculated. Note: the excel file has to first be saved in a .csv format.  "
  },
  {
    "id": 14002,
    "package_name": "hivdata",
    "title": "Six-Year Chronological Data of HIV and ART Cases in Pakistan",
    "description": "We provide the monthly number of HIV and antiretroviral therapy (ART) cases of male, female, children and transgender as well as for the whole of Pakistan reported at various treatment centers in Pakistan from January 2016 to December 2021. Related works include: \n  a) Imran, M., Nasir, J. A., & Riaz, S. (2018). Regional pattern of HIV cases in Pakistan. Journal of Postgraduate Medical Institute, 32(1), 9-13. \n  <https://jpmi.org.pk/index.php/jpmi/article/view/2108>.  ",
    "version": "0.1.0",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "author": "Muhammad Imran [aut, cre],\n  Saima Shakoor [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hivdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hivdata Six-Year Chronological Data of HIV and ART Cases in Pakistan We provide the monthly number of HIV and antiretroviral therapy (ART) cases of male, female, children and transgender as well as for the whole of Pakistan reported at various treatment centers in Pakistan from January 2016 to December 2021. Related works include: \n  a) Imran, M., Nasir, J. A., & Riaz, S. (2018). Regional pattern of HIV cases in Pakistan. Journal of Postgraduate Medical Institute, 32(1), 9-13. \n  <https://jpmi.org.pk/index.php/jpmi/article/view/2108>.    "
  },
  {
    "id": 14037,
    "package_name": "hopit",
    "title": "Hierarchical Ordered Probit Models with Application to Reporting\nHeterogeneity",
    "description": "Self-reported health, happiness, attitudes, and other statuses or perceptions are often the subject of biases that may come from different sources. For example, the evaluation of an individual\u2019s own health may depend on previous medical diagnoses, functional status, and symptoms and signs of illness; as on well as life-style behaviors, including contextual social, gender, age-specific, linguistic and other cultural factors (Jylha 2009 <doi:10.1016/j.socscimed.2009.05.013>; Oksuzyan et al. 2019 <doi:10.1016/j.socscimed.2019.03.002>). The hopit package offers versatile functions for analyzing different self-reported ordinal variables, and for helping to estimate their biases. Specifically, the package provides the function to fit a generalized ordered probit model that regresses original self-reported status measures on two sets of independent variables (King et al. 2004 <doi:10.1017/S0003055403000881>; Jurges 2007  <doi:10.1002/hec.1134>; Oksuzyan et al. 2019  <doi:10.1016/j.socscimed.2019.03.002>). The first set of variables (e.g., health variables) included in the regression are individual statuses and characteristics that are directly related to the self-reported variable. In the case of self-reported health, these could be chronic conditions, mobility level, difficulties with daily activities, performance on grip strength tests, anthropometric measures, and lifestyle behaviors. The second set of independent variables (threshold variables) is used to model cut-points between adjacent self-reported response categories as functions of individual characteristics, such as gender, age group, education, and country (Oksuzyan et al. 2019 <doi:10.1016/j.socscimed.2019.03.002>). The model helps to adjust for specific socio-demographic and cultural differences in how the continuous latent health is projected onto the ordinal self-rated measure. The fitted model can be used to calculate an individual predicted latent status variable, a latent index, and standardized latent coefficients; and makes it possible to reclassify a categorical status measure that has been adjusted for inter-individual differences in reporting behavior.",
    "version": "0.11.6",
    "maintainer": "Maciej J. Danko <Maciej.Danko@gmail.com>",
    "author": "Maciej J. Danko [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7924-9022>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hopit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hopit Hierarchical Ordered Probit Models with Application to Reporting\nHeterogeneity Self-reported health, happiness, attitudes, and other statuses or perceptions are often the subject of biases that may come from different sources. For example, the evaluation of an individual\u2019s own health may depend on previous medical diagnoses, functional status, and symptoms and signs of illness; as on well as life-style behaviors, including contextual social, gender, age-specific, linguistic and other cultural factors (Jylha 2009 <doi:10.1016/j.socscimed.2009.05.013>; Oksuzyan et al. 2019 <doi:10.1016/j.socscimed.2019.03.002>). The hopit package offers versatile functions for analyzing different self-reported ordinal variables, and for helping to estimate their biases. Specifically, the package provides the function to fit a generalized ordered probit model that regresses original self-reported status measures on two sets of independent variables (King et al. 2004 <doi:10.1017/S0003055403000881>; Jurges 2007  <doi:10.1002/hec.1134>; Oksuzyan et al. 2019  <doi:10.1016/j.socscimed.2019.03.002>). The first set of variables (e.g., health variables) included in the regression are individual statuses and characteristics that are directly related to the self-reported variable. In the case of self-reported health, these could be chronic conditions, mobility level, difficulties with daily activities, performance on grip strength tests, anthropometric measures, and lifestyle behaviors. The second set of independent variables (threshold variables) is used to model cut-points between adjacent self-reported response categories as functions of individual characteristics, such as gender, age group, education, and country (Oksuzyan et al. 2019 <doi:10.1016/j.socscimed.2019.03.002>). The model helps to adjust for specific socio-demographic and cultural differences in how the continuous latent health is projected onto the ordinal self-rated measure. The fitted model can be used to calculate an individual predicted latent status variable, a latent index, and standardized latent coefficients; and makes it possible to reclassify a categorical status measure that has been adjusted for inter-individual differences in reporting behavior.  "
  },
  {
    "id": 14076,
    "package_name": "htm2txt",
    "title": "Convert Html into Text",
    "description": "Convert a html document to plain texts by stripping off all html tags.",
    "version": "2.2.2",
    "maintainer": "Sangchul Park <mail@sangchul.com>",
    "author": "Sangchul Park [aut, cre]",
    "url": "https://github.com/replicable/htm2txt",
    "bug_reports": "https://github.com/replicable/htm2txt/issues",
    "repository": "https://cran.r-project.org/package=htm2txt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "htm2txt Convert Html into Text Convert a html document to plain texts by stripping off all html tags.  "
  },
  {
    "id": 14078,
    "package_name": "html5",
    "title": "Creates Valid HTML5 Strings",
    "description": "Generates valid HTML tag strings for HTML5 elements documented by Mozilla. \n    Attributes are passed as named lists, with names being the attribute name and values being the attribute value. \n    Attribute values are automatically double-quoted. To declare a DOCTYPE, wrap html() with function doctype().\n    Mozilla's documentation for HTML5 is available here: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element>.\n    Elements marked as obsolete are not included. ",
    "version": "1.0.2",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=html5",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "html5 Creates Valid HTML5 Strings Generates valid HTML tag strings for HTML5 elements documented by Mozilla. \n    Attributes are passed as named lists, with names being the attribute name and values being the attribute value. \n    Attribute values are automatically double-quoted. To declare a DOCTYPE, wrap html() with function doctype().\n    Mozilla's documentation for HTML5 is available here: <https://developer.mozilla.org/en-US/docs/Web/HTML/Element>.\n    Elements marked as obsolete are not included.   "
  },
  {
    "id": 14079,
    "package_name": "htmlTable",
    "title": "Advanced Tables for Markdown/HTML",
    "description": "Tables with state-of-the-art layout elements such as row spanners,\n    column spanners, table spanners, zebra striping, and more. While allowing\n    advanced layout, the underlying css-structure is simple in order to maximize\n    compatibility with common word processors. The package also contains a few\n    text formatting functions that help outputting text compatible with HTML/LaTeX.",
    "version": "2.4.3",
    "maintainer": "Max Gordon <max@gforge.se>",
    "author": "Max Gordon [aut, cre],\n  Stephen Gragg [aut],\n  Peter Konings [aut]",
    "url": "https://gforge.se/packages/",
    "bug_reports": "https://github.com/gforge/htmlTable/issues",
    "repository": "https://cran.r-project.org/package=htmlTable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "htmlTable Advanced Tables for Markdown/HTML Tables with state-of-the-art layout elements such as row spanners,\n    column spanners, table spanners, zebra striping, and more. While allowing\n    advanced layout, the underlying css-structure is simple in order to maximize\n    compatibility with common word processors. The package also contains a few\n    text formatting functions that help outputting text compatible with HTML/LaTeX.  "
  },
  {
    "id": 14080,
    "package_name": "htmlreportR",
    "title": "'HTML' Reporting Made Simple(R)",
    "description": "Create compressed, interactive 'HTML' (Hypertext Markup Language) reports with embedded 'Python' code, custom 'JS' ('JavaScript') and 'CSS' (Cascading Style Sheets), and wrappers for\n\t'CanvasXpress' plots, networks and more. Based on <https://pypi.org/project/py-report-html/>, its sister project.",
    "version": "1.0.0",
    "maintainer": "\u00c1lvaro Esteban Martos <alvaroesteban@uma.es>",
    "author": "\u00c1lvaro Esteban Martos [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-9791-7779>),\n  Jos\u00e9 C\u00f3rdoba Caballero [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-1821-5742>),\n  James Perkins [aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-4108-096X>),\n  Pedro Seoane Zonjic [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-3020-1415>),\n  Jes\u00fas P\u00e9rez Garc\u00eda [aut, cph] (ORCID:\n    <https://orcid.org/0009-0006-9388-7465>)",
    "url": "https://github.com/AEstebanMar/htmlreportR",
    "bug_reports": "https://github.com/AEstebanMar/htmlreportR/issues",
    "repository": "https://cran.r-project.org/package=htmlreportR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "htmlreportR 'HTML' Reporting Made Simple(R) Create compressed, interactive 'HTML' (Hypertext Markup Language) reports with embedded 'Python' code, custom 'JS' ('JavaScript') and 'CSS' (Cascading Style Sheets), and wrappers for\n\t'CanvasXpress' plots, networks and more. Based on <https://pypi.org/project/py-report-html/>, its sister project.  "
  },
  {
    "id": 14081,
    "package_name": "htmlwidgets",
    "title": "HTML Widgets for R",
    "description": "A framework for creating HTML widgets that render in various\n    contexts including the R console, 'R Markdown' documents, and 'Shiny'\n    web applications.",
    "version": "1.6.4",
    "maintainer": "Carson Sievert <carson@posit.co>",
    "author": "Ramnath Vaidyanathan [aut, cph],\n  Yihui Xie [aut],\n  JJ Allaire [aut],\n  Joe Cheng [aut],\n  Carson Sievert [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4958-2844>),\n  Kenton Russell [aut, cph],\n  Ellis Hughes [ctb],\n  Posit Software, PBC [cph, fnd]",
    "url": "https://github.com/ramnathv/htmlwidgets",
    "bug_reports": "https://github.com/ramnathv/htmlwidgets/issues",
    "repository": "https://cran.r-project.org/package=htmlwidgets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "htmlwidgets HTML Widgets for R A framework for creating HTML widgets that render in various\n    contexts including the R console, 'R Markdown' documents, and 'Shiny'\n    web applications.  "
  },
  {
    "id": 14092,
    "package_name": "httpproblems",
    "title": "Report Errors in Web Applications with 'Problem Details' (RFC\n7807)",
    "description": "Tools for emitting the 'Problem Details' structure defined in\n  'RFC' 7807 <https://tools.ietf.org/html/rfc7807> for reporting errors from\n  'HTTP' servers in a standard way.",
    "version": "1.0.1",
    "maintainer": "Aaron Jacobs <atheriel@gmail.com>",
    "author": "Aaron Jacobs [aut, cre]",
    "url": "https://github.com/atheriel/httpproblems",
    "bug_reports": "https://github.com/atheriel/httpproblems/issues",
    "repository": "https://cran.r-project.org/package=httpproblems",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "httpproblems Report Errors in Web Applications with 'Problem Details' (RFC\n7807) Tools for emitting the 'Problem Details' structure defined in\n  'RFC' 7807 <https://tools.ietf.org/html/rfc7807> for reporting errors from\n  'HTTP' servers in a standard way.  "
  },
  {
    "id": 14093,
    "package_name": "httptest",
    "title": "A Test Environment for HTTP Requests",
    "description": "Testing and documenting code that communicates with remote servers\n    can be painful. Dealing with authentication, server state,\n    and other complications can make testing seem too costly to\n    bother with. But it doesn't need to be that hard. This package enables one\n    to test all of the logic on the R sides of the API in your package without\n    requiring access to the remote service. Importantly, it provides three\n    contexts that mock the network connection in different ways, as well as\n    testing functions to assert that HTTP requests were---or were\n    not---made. It also allows one to safely record real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables one to write vignettes and other dynamic documents that can be\n    distributed without access to a live server.",
    "version": "4.2.3",
    "maintainer": "Neal Richardson <neal.p.richardson@gmail.com>",
    "author": "Neal Richardson [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-7992-3520>),\n  Jonathan Keane [ctb],\n  Ma\u00eblle Salmon [ctb] (ORCID: <https://orcid.org/0000-0002-2815-0399>)",
    "url": "https://enpiar.com/r/httptest/,\nhttps://github.com/nealrichardson/httptest",
    "bug_reports": "https://github.com/nealrichardson/httptest/issues",
    "repository": "https://cran.r-project.org/package=httptest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "httptest A Test Environment for HTTP Requests Testing and documenting code that communicates with remote servers\n    can be painful. Dealing with authentication, server state,\n    and other complications can make testing seem too costly to\n    bother with. But it doesn't need to be that hard. This package enables one\n    to test all of the logic on the R sides of the API in your package without\n    requiring access to the remote service. Importantly, it provides three\n    contexts that mock the network connection in different ways, as well as\n    testing functions to assert that HTTP requests were---or were\n    not---made. It also allows one to safely record real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables one to write vignettes and other dynamic documents that can be\n    distributed without access to a live server.  "
  },
  {
    "id": 14094,
    "package_name": "httptest2",
    "title": "Test Helpers for 'httr2'",
    "description": "Testing and documenting code that communicates with remote servers\n    can be painful. This package helps with writing tests for packages that\n    use 'httr2'. It enables testing all of the logic\n    on the R sides of the API without requiring access to the\n    remote service, and it also allows recording real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables writing vignettes and other dynamic documents that can be\n    distributed without access to a live server.",
    "version": "1.2.2",
    "maintainer": "Neal Richardson <neal.p.richardson@gmail.com>",
    "author": "Neal Richardson [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-7992-3520>),\n  Jonathan Keane [ctb],\n  Ma\u00eblle Salmon [ctb] (ORCID: <https://orcid.org/0000-0002-2815-0399>)",
    "url": "https://enpiar.com/httptest2/,\nhttps://github.com/nealrichardson/httptest2",
    "bug_reports": "https://github.com/nealrichardson/httptest2/issues",
    "repository": "https://cran.r-project.org/package=httptest2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "httptest2 Test Helpers for 'httr2' Testing and documenting code that communicates with remote servers\n    can be painful. This package helps with writing tests for packages that\n    use 'httr2'. It enables testing all of the logic\n    on the R sides of the API without requiring access to the\n    remote service, and it also allows recording real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables writing vignettes and other dynamic documents that can be\n    distributed without access to a live server.  "
  },
  {
    "id": 14110,
    "package_name": "huxtable",
    "title": "Easily Create and Style Tables for LaTeX, HTML and Other Formats",
    "description": "Creates styled tables for data presentation. Export to HTML, LaTeX,\n  RTF, 'Word', 'Excel', 'PowerPoint', 'typst', SVG and PNG. Simple, modern \n  interface to manipulate borders, size, position, captions, colours, \n  text styles and number formatting. Table cells can span multiple rows and/or columns.\n  Includes  a 'huxreg' function to create regression tables, and 'quick_*' \n  one-liners to print tables to a new document.",
    "version": "5.8.0",
    "maintainer": "David Hugh-Jones <davidhughjones@gmail.com>",
    "author": "David Hugh-Jones [aut, cre]",
    "url": "https://hughjonesd.github.io/huxtable/",
    "bug_reports": "https://github.com/hughjonesd/huxtable/issues",
    "repository": "https://cran.r-project.org/package=huxtable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "huxtable Easily Create and Style Tables for LaTeX, HTML and Other Formats Creates styled tables for data presentation. Export to HTML, LaTeX,\n  RTF, 'Word', 'Excel', 'PowerPoint', 'typst', SVG and PNG. Simple, modern \n  interface to manipulate borders, size, position, captions, colours, \n  text styles and number formatting. Table cells can span multiple rows and/or columns.\n  Includes  a 'huxreg' function to create regression tables, and 'quick_*' \n  one-liners to print tables to a new document.  "
  },
  {
    "id": 14114,
    "package_name": "hwordcloud",
    "title": "Rendering Word Clouds",
    "description": "Provides a way to display word clouds in R. The word cloud is a html widget, so you can use it in interactive documents and 'shiny' applications.",
    "version": "0.1.0",
    "maintainer": "Zhenxing Cheng <czxjnu@163.com>",
    "author": "Zhenxing Cheng [aut, cre]",
    "url": "https://github.com/czxa/hwordcloud",
    "bug_reports": "https://github.com/czxa/hwordcloud/issues",
    "repository": "https://cran.r-project.org/package=hwordcloud",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hwordcloud Rendering Word Clouds Provides a way to display word clouds in R. The word cloud is a html widget, so you can use it in interactive documents and 'shiny' applications.  "
  },
  {
    "id": 14135,
    "package_name": "hydroloom",
    "title": "Utilities to Weave Hydrologic Fabrics",
    "description": "A collection of utilities that support creation of network attributes for hydrologic networks. Methods and algorithms implemented are documented in Moore et al. (2019) <doi:10.3133/ofr20191096>), Cormen and Leiserson (2022) <ISBN:9780262046305> and Verdin and Verdin (1999) <doi:10.1016/S0022-1694(99)00011-6>.  ",
    "version": "1.1.1",
    "maintainer": "David Blodgett <dblodgett@usgs.gov>",
    "author": "David Blodgett [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9489-1710>)",
    "url": "https://github.com/DOI-USGS/hydroloom,\nhttps://doi-usgs.github.io/hydroloom/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hydroloom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hydroloom Utilities to Weave Hydrologic Fabrics A collection of utilities that support creation of network attributes for hydrologic networks. Methods and algorithms implemented are documented in Moore et al. (2019) <doi:10.3133/ofr20191096>), Cormen and Leiserson (2022) <ISBN:9780262046305> and Verdin and Verdin (1999) <doi:10.1016/S0022-1694(99)00011-6>.    "
  },
  {
    "id": 14153,
    "package_name": "hypothesis",
    "title": "Wrapper for 'hypothes.is'",
    "description": "Add, share and manage annotations for 'Shiny' applications and R Markdown documents via 'hypothes.is'.",
    "version": "1.1.0",
    "maintainer": "Krystian Igras <krystian8207@gmail.com>",
    "author": "Krystian Igras [cre, aut],\n  Adam Fory\u015b [ctb],\n  Hypothes.is Project and contributors [cph] (hi_embed.js)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hypothesis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hypothesis Wrapper for 'hypothes.is' Add, share and manage annotations for 'Shiny' applications and R Markdown documents via 'hypothes.is'.  "
  },
  {
    "id": 14211,
    "package_name": "ialiquor",
    "title": "Monthly Iowa Liquor Sales Summary",
    "description": "Provides a monthly summary of Iowa liquor (class E) sales from January 2015\n    to October 2020. See the package website for more information, \n    documentation and examples. Data source: \n    Iowa Data portal <https://data.iowa.gov/resource/m3tr-qhgy.csv>.",
    "version": "0.1.0",
    "maintainer": "Nikhil Agarwal <gitnik@niks.me>",
    "author": "Nikhil Agarwal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1470-7472>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ialiquor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ialiquor Monthly Iowa Liquor Sales Summary Provides a monthly summary of Iowa liquor (class E) sales from January 2015\n    to October 2020. See the package website for more information, \n    documentation and examples. Data source: \n    Iowa Data portal <https://data.iowa.gov/resource/m3tr-qhgy.csv>.  "
  },
  {
    "id": 14223,
    "package_name": "ibmAcousticR",
    "title": "Connect to Your 'IBM Acoustic' Data",
    "description": "Authentication can be the most difficult part about\n  working with a new API. 'ibmAcousticR' facilitates making a\n  connection to the 'IBM Acoustic' email campaign management API\n  and executing various queries. The 'IBM Acoustic' API \n  documentation is available at\n  <https://developer.ibm.com/customer-engagement/docs/>. This\n  package is not supported by 'IBM'.",
    "version": "0.2.1",
    "maintainer": "Chris Umphlett <christopher.umphlett@gmail.com>",
    "author": "Chris Umphlett [aut, cre],\n  Avinash Panigrahi [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ibmAcousticR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ibmAcousticR Connect to Your 'IBM Acoustic' Data Authentication can be the most difficult part about\n  working with a new API. 'ibmAcousticR' facilitates making a\n  connection to the 'IBM Acoustic' email campaign management API\n  and executing various queries. The 'IBM Acoustic' API \n  documentation is available at\n  <https://developer.ibm.com/customer-engagement/docs/>. This\n  package is not supported by 'IBM'.  "
  },
  {
    "id": 14230,
    "package_name": "icRSF",
    "title": "A Modified Random Survival Forest Algorithm",
    "description": "Implements a modification to the Random Survival Forests algorithm for obtaining variable importance in high dimensional datasets. The proposed algorithm is appropriate for settings in which a silent event is observed through sequentially administered, error-prone self-reports or laboratory based diagnostic tests.  The modified algorithm incorporates a formal likelihood framework that accommodates sequentially administered, error-prone self-reports or laboratory based diagnostic tests. The original Random Survival Forests algorithm is modified by the introduction of a new splitting criterion based on a likelihood ratio test statistic.",
    "version": "1.2",
    "maintainer": "Hui Xu <huix@schoolph.umass.edu>",
    "author": "Hui Xu and Raji Balasubramanian",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=icRSF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icRSF A Modified Random Survival Forest Algorithm Implements a modification to the Random Survival Forests algorithm for obtaining variable importance in high dimensional datasets. The proposed algorithm is appropriate for settings in which a silent event is observed through sequentially administered, error-prone self-reports or laboratory based diagnostic tests.  The modified algorithm incorporates a formal likelihood framework that accommodates sequentially administered, error-prone self-reports or laboratory based diagnostic tests. The original Random Survival Forests algorithm is modified by the introduction of a new splitting criterion based on a likelihood ratio test statistic.  "
  },
  {
    "id": 14248,
    "package_name": "icensmis",
    "title": "Study Design and Data Analysis in the Presence of Error-Prone\nDiagnostic Tests and Self-Reported Outcomes",
    "description": "We consider studies in which information from error-prone\n    diagnostic tests or self-reports are gathered sequentially to determine the\n    occurrence of a silent event. Using a likelihood-based approach\n    incorporating the proportional hazards assumption, we provide functions to\n    estimate the survival distribution and covariate effects. We also provide \n    functions for power and sample size calculations for this setting.\n    Please refer to Xiangdong Gu, Yunsheng Ma, and Raji Balasubramanian (2015) \n    <doi: 10.1214/15-AOAS810>, Xiangdong Gu and Raji Balasubramanian (2016) \n    <doi: 10.1002/sim.6962>, Xiangdong Gu, Mahlet G Tadesse, Andrea S Foulkes,\n    Yunsheng Ma, and Raji Balasubramanian (2020) <doi: 10.1186/s12911-020-01223-w>.",
    "version": "1.5.0",
    "maintainer": "Xiangdong Gu <ustcgxd@gmail.com>",
    "author": "Xiangdong Gu and Raji Balasubramanian",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=icensmis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icensmis Study Design and Data Analysis in the Presence of Error-Prone\nDiagnostic Tests and Self-Reported Outcomes We consider studies in which information from error-prone\n    diagnostic tests or self-reports are gathered sequentially to determine the\n    occurrence of a silent event. Using a likelihood-based approach\n    incorporating the proportional hazards assumption, we provide functions to\n    estimate the survival distribution and covariate effects. We also provide \n    functions for power and sample size calculations for this setting.\n    Please refer to Xiangdong Gu, Yunsheng Ma, and Raji Balasubramanian (2015) \n    <doi: 10.1214/15-AOAS810>, Xiangdong Gu and Raji Balasubramanian (2016) \n    <doi: 10.1002/sim.6962>, Xiangdong Gu, Mahlet G Tadesse, Andrea S Foulkes,\n    Yunsheng Ma, and Raji Balasubramanian (2020) <doi: 10.1186/s12911-020-01223-w>.  "
  },
  {
    "id": 14279,
    "package_name": "idmact",
    "title": "Interpreting Differences Between Mean ACT Scores",
    "description": "Interpreting the differences between mean scale scores across various\n    forms of an assessment can be challenging. This difficulty arises from different\n    mappings between raw scores and scale scores, complex mathematical relationships,\n    adjustments based on judgmental procedures, and diverse equating functions applied\n    to different assessment forms. An alternative method involves running simulations\n    to explore the effect of incrementing raw scores on mean scale scores. The\n    'idmact' package provides an implementation of this approach based on the\n    algorithm detailed in Schiel (1998)\n    <https://www.act.org/content/dam/act/unsecured/documents/ACT_RR98-01.pdf> which\n    was developed to help interpret differences between mean scale scores on the\n    American College Testing (ACT) assessment. The function idmact_subj() within\n    the package offers a framework for running simulations on subject-level scores.\n    In contrast, the idmact_comp() function provides a framework for conducting\n    simulations on composite scores.",
    "version": "1.0.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre, cph]",
    "url": "https://github.com/mncube/idmact",
    "bug_reports": "https://github.com/mncube/idmact/issues",
    "repository": "https://cran.r-project.org/package=idmact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "idmact Interpreting Differences Between Mean ACT Scores Interpreting the differences between mean scale scores across various\n    forms of an assessment can be challenging. This difficulty arises from different\n    mappings between raw scores and scale scores, complex mathematical relationships,\n    adjustments based on judgmental procedures, and diverse equating functions applied\n    to different assessment forms. An alternative method involves running simulations\n    to explore the effect of incrementing raw scores on mean scale scores. The\n    'idmact' package provides an implementation of this approach based on the\n    algorithm detailed in Schiel (1998)\n    <https://www.act.org/content/dam/act/unsecured/documents/ACT_RR98-01.pdf> which\n    was developed to help interpret differences between mean scale scores on the\n    American College Testing (ACT) assessment. The function idmact_subj() within\n    the package offers a framework for running simulations on subject-level scores.\n    In contrast, the idmact_comp() function provides a framework for conducting\n    simulations on composite scores.  "
  },
  {
    "id": 14301,
    "package_name": "ig.vancouver.2014.topcolour",
    "title": "Instagram 2014 Vancouver Top Colour Dataset",
    "description": "A dataset of the top colours of photos from Instagram \n taken in 2014 in the city of Vancouver, British Columbia, Canada.\n It consists of: top colour and counts data. This data was\n obtained using the Instagram API. Instagram is a web photo \n sharing service. It can be found at: <https://instagram.com>.\n The Instagram API is documented at: <https://instagram.com/developer/>. ",
    "version": "0.1.2.0",
    "maintainer": "Roland Tanglao <roland@rolandtanglao.com>",
    "author": "Roland Tanglao [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ig.vancouver.2014.topcolour",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ig.vancouver.2014.topcolour Instagram 2014 Vancouver Top Colour Dataset A dataset of the top colours of photos from Instagram \n taken in 2014 in the city of Vancouver, British Columbia, Canada.\n It consists of: top colour and counts data. This data was\n obtained using the Instagram API. Instagram is a web photo \n sharing service. It can be found at: <https://instagram.com>.\n The Instagram API is documented at: <https://instagram.com/developer/>.   "
  },
  {
    "id": 14302,
    "package_name": "igate",
    "title": "Guided Analytics for Testing Manufacturing Parameters",
    "description": "An implementation of the initial guided analytics for parameter testing and\n    controlband extraction framework. Functions are available for continuous and \n    categorical target variables as well as for generating standardized reports of the\n    conducted analysis. See <https://github.com/stefan-stein/igate> for more information\n    on the technology.",
    "version": "0.3.3",
    "maintainer": "Stefan Stein <s.stein@warwick.ac.uk>",
    "author": "Stefan Stein [aut, cre]",
    "url": "https://github.com/stefan-stein/igate",
    "bug_reports": "https://github.com/stefan-stein/igate/issues",
    "repository": "https://cran.r-project.org/package=igate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "igate Guided Analytics for Testing Manufacturing Parameters An implementation of the initial guided analytics for parameter testing and\n    controlband extraction framework. Functions are available for continuous and \n    categorical target variables as well as for generating standardized reports of the\n    conducted analysis. See <https://github.com/stefan-stein/igate> for more information\n    on the technology.  "
  },
  {
    "id": 14306,
    "package_name": "iglu",
    "title": "Interpreting Glucose Data from Continuous Glucose Monitors",
    "description": "Implements a wide range of metrics for measuring glucose control and glucose variability based on continuous glucose monitoring data. The list of implemented metrics is summarized in Rodbard (2009) <doi:10.1089/dia.2009.0015>. Additional visualization tools include time-series plots, lasagna plots and ambulatory glucose profile report. ",
    "version": "4.2.2",
    "maintainer": "Irina Gaynanova <irinagn@umich.edu>",
    "author": "Elizabeth Chun [aut],\n  Steve Broll [aut],\n  David Buchanan [aut],\n  John Muschelli [aut] (ORCID: <https://orcid.org/0000-0001-6469-1750>),\n  Nathaniel Fernandes [aut] (ORCID:\n    <https://orcid.org/0000-0003-0485-0726>),\n  Jung Hoon Seo [ctb],\n  Johnathan Shih [ctb],\n  Jacek Urbanek [ctb],\n  John Schwenck [ctb],\n  Marielle Hicban [ctb],\n  Mary Martin [ctb],\n  Pratik Patel [ctb],\n  Meyappan Ashok [ctb],\n  Nhan Nguyen [ctb],\n  Irina Gaynanova [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4116-0268>)",
    "url": "https://irinagain.github.io/iglu/",
    "bug_reports": "https://github.com/irinagain/iglu/issues",
    "repository": "https://cran.r-project.org/package=iglu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iglu Interpreting Glucose Data from Continuous Glucose Monitors Implements a wide range of metrics for measuring glucose control and glucose variability based on continuous glucose monitoring data. The list of implemented metrics is summarized in Rodbard (2009) <doi:10.1089/dia.2009.0015>. Additional visualization tools include time-series plots, lasagna plots and ambulatory glucose profile report.   "
  },
  {
    "id": 14316,
    "package_name": "iheiddown",
    "title": "For Writing Geneva Graduate Institute Documents",
    "description": "A set of tools for writing documents\n    according to Geneva Graduate Institute conventions and regulations.\n    The most common use is for writing and compiling theses or thesis\n    chapters, as drafts or for examination with correct preamble formatting. \n    However, the package also offers users to create HTML presentation\n    slides with 'xaringan', complete problem sets, format posters, and, \n    for course instructors, prepare a syllabus.\n    The package includes additional functions for institutional color palettes,\n    an institutional 'ggplot' theme, a function for counting manuscript words,\n    and a bibliographical analysis toolkit.",
    "version": "0.9.7",
    "maintainer": "James Hollway <james.hollway@graduateinstitute.ch>",
    "author": "James Hollway [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8361-9647>),\n  Bernhard Bieri [ctb] (ORCID: <https://orcid.org/0000-0001-5943-9059>),\n  Henrique Sposito [ctb] (ORCID: <https://orcid.org/0000-0003-3420-6085>)",
    "url": "https://github.com/jhollway/iheiddown",
    "bug_reports": "https://github.com/jhollway/iheiddown/issues",
    "repository": "https://cran.r-project.org/package=iheiddown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iheiddown For Writing Geneva Graduate Institute Documents A set of tools for writing documents\n    according to Geneva Graduate Institute conventions and regulations.\n    The most common use is for writing and compiling theses or thesis\n    chapters, as drafts or for examination with correct preamble formatting. \n    However, the package also offers users to create HTML presentation\n    slides with 'xaringan', complete problem sets, format posters, and, \n    for course instructors, prepare a syllabus.\n    The package includes additional functions for institutional color palettes,\n    an institutional 'ggplot' theme, a function for counting manuscript words,\n    and a bibliographical analysis toolkit.  "
  },
  {
    "id": 14340,
    "package_name": "imanr",
    "title": "Identify the Racial Complex of Native Corns from Mexico",
    "description": "A model that provides researchers with a powerful tool for the classification\n    and study of native corn by aiding in the identification of racial complexes\n    which are fundamental to Mexico's agriculture and culture. This package has been\n    developed based on data collected by \"Proyecto Global de Ma\u00edces Nativos M\u00e9xico\",\n    which has conducted exhaustive surveys across the country to document the\n    qualitative and quantitative characteristics of different types of native maize.\n    The trained model uses a robust and diverse dataset, enabling it to achieve an\n    80% accuracy in classifying maize racial complexes. The characteristics included\n    in the analysis comprise geographic location, grain and cob colors, as well as\n    various physical measurements, such as lengths and widths.",
    "version": "2.0.0",
    "maintainer": "Rafael Nieves-Alvarez <nievesalvarez1618@gmail.com>",
    "author": "Rafael Nieves-Alvarez [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-8016-1412>),\n  Arturo Sanchez-Porras [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-1691-286X>),\n  Aline Romero-Natale [aut] (ORCID:\n    <https://orcid.org/0000-0003-4267-1913>),\n  Otilio Arturo Acevedo-Sandoval [aut] (ORCID:\n    <https://orcid.org/0000-0003-0475-7003>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=imanr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imanr Identify the Racial Complex of Native Corns from Mexico A model that provides researchers with a powerful tool for the classification\n    and study of native corn by aiding in the identification of racial complexes\n    which are fundamental to Mexico's agriculture and culture. This package has been\n    developed based on data collected by \"Proyecto Global de Ma\u00edces Nativos M\u00e9xico\",\n    which has conducted exhaustive surveys across the country to document the\n    qualitative and quantitative characteristics of different types of native maize.\n    The trained model uses a robust and diverse dataset, enabling it to achieve an\n    80% accuracy in classifying maize racial complexes. The characteristics included\n    in the analysis comprise geographic location, grain and cob colors, as well as\n    various physical measurements, such as lengths and widths.  "
  },
  {
    "id": 14360,
    "package_name": "impimp",
    "title": "Imprecise Imputation for Statistical Matching",
    "description": "Imputing blockwise missing data by imprecise imputation,\n    featuring a domain-based, variable-wise, and case-wise strategy. \n    Furthermore, the estimation of lower and upper bounds for \n    unconditional and conditional probabilities based on the obtained\n    imprecise data is implemented.\n    Additionally, two utility functions are supplied: one to check \n    whether variables in a data set contain set-valued observations;\n    and another to merge two already imprecisely imputed data. \n    The method is described in a technical report by Endres, Fink and\n    Augustin (2018, <doi:10.5282/ubm/epub.42423>).",
    "version": "0.3.1",
    "maintainer": "Paul Fink <paul.fink@stat.uni-muenchen.de>",
    "author": "Paul Fink [aut, cre],\n  Eva Endres [aut],\n  Melissa Schmoll [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=impimp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "impimp Imprecise Imputation for Statistical Matching Imputing blockwise missing data by imprecise imputation,\n    featuring a domain-based, variable-wise, and case-wise strategy. \n    Furthermore, the estimation of lower and upper bounds for \n    unconditional and conditional probabilities based on the obtained\n    imprecise data is implemented.\n    Additionally, two utility functions are supplied: one to check \n    whether variables in a data set contain set-valued observations;\n    and another to merge two already imprecisely imputed data. \n    The method is described in a technical report by Endres, Fink and\n    Augustin (2018, <doi:10.5282/ubm/epub.42423>).  "
  },
  {
    "id": 14371,
    "package_name": "imputeFin",
    "title": "Imputation of Financial Time Series with Missing Values and/or\nOutliers",
    "description": "Missing values often occur in financial data due to a variety \n    of reasons (errors in the collection process or in the processing stage, \n    lack of asset liquidity, lack of reporting of funds, etc.). However, \n    most data analysis methods expect complete data and cannot be employed \n    with missing values. One convenient way to deal with this issue without \n    having to redesign the data analysis method is to impute the missing \n    values. This package provides an efficient way to impute the missing \n    values based on modeling the time series with a random walk or an \n    autoregressive (AR) model, convenient to model log-prices and log-volumes \n    in financial data. In the current version, the imputation is \n    univariate-based (so no asset correlation is used). In addition,\n    outliers can be detected and removed.\n    The package is based on the paper:\n    J. Liu, S. Kumar, and D. P. Palomar (2019). Parameter Estimation of \n    Heavy-Tailed AR Model With Missing Data Via Stochastic EM. IEEE Trans. on \n    Signal Processing, vol. 67, no. 8, pp. 2159-2172. <doi:10.1109/TSP.2019.2899816>.",
    "version": "0.1.2",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Daniel P. Palomar [cre, aut],\n  Junyan Liu [aut],\n  Rui Zhou [aut]",
    "url": "https://CRAN.R-project.org/package=imputeFin,\nhttps://github.com/dppalomar/imputeFin,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2019.2899816,\nhttps://doi.org/10.1109/TSP.2020.3033378",
    "bug_reports": "https://github.com/dppalomar/imputeFin/issues",
    "repository": "https://cran.r-project.org/package=imputeFin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imputeFin Imputation of Financial Time Series with Missing Values and/or\nOutliers Missing values often occur in financial data due to a variety \n    of reasons (errors in the collection process or in the processing stage, \n    lack of asset liquidity, lack of reporting of funds, etc.). However, \n    most data analysis methods expect complete data and cannot be employed \n    with missing values. One convenient way to deal with this issue without \n    having to redesign the data analysis method is to impute the missing \n    values. This package provides an efficient way to impute the missing \n    values based on modeling the time series with a random walk or an \n    autoregressive (AR) model, convenient to model log-prices and log-volumes \n    in financial data. In the current version, the imputation is \n    univariate-based (so no asset correlation is used). In addition,\n    outliers can be detected and removed.\n    The package is based on the paper:\n    J. Liu, S. Kumar, and D. P. Palomar (2019). Parameter Estimation of \n    Heavy-Tailed AR Model With Missing Data Via Stochastic EM. IEEE Trans. on \n    Signal Processing, vol. 67, no. 8, pp. 2159-2172. <doi:10.1109/TSP.2019.2899816>.  "
  },
  {
    "id": 14384,
    "package_name": "inTextSummaryTable",
    "title": "Creation of in-Text Summary Table",
    "description": "Creation of tables of summary statistics or counts for clinical data (for 'TLFs'). \n  These tables can be exported as in-text table (with the 'flextable' package) for a Clinical Study Report \n  (Word format) or a 'topline' presentation (PowerPoint format), \n  or as interactive table (with the 'DT' package) to an html document for clinical data review.",
    "version": "3.3.5",
    "maintainer": "Laure Cougnaud <laure.cougnaud@openanalytics.eu>",
    "author": "Laure Cougnaud [aut, cre],\n  Michela Pasetto [aut],\n  Margaux Faes [rev] (tests),\n  Open Analytics [cph]",
    "url": "https://github.com/openanalytics/inTextSummaryTable",
    "bug_reports": "https://github.com/openanalytics/inTextSummaryTable/issues",
    "repository": "https://cran.r-project.org/package=inTextSummaryTable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inTextSummaryTable Creation of in-Text Summary Table Creation of tables of summary statistics or counts for clinical data (for 'TLFs'). \n  These tables can be exported as in-text table (with the 'flextable' package) for a Clinical Study Report \n  (Word format) or a 'topline' presentation (PowerPoint format), \n  or as interactive table (with the 'DT' package) to an html document for clinical data review.  "
  },
  {
    "id": 14389,
    "package_name": "inca",
    "title": "Integer Calibration",
    "description": "Specific functions are provided for rounding real weights to integers and performing an integer programming algorithm for calibration problems. These functions are useful for census-weights adjustments, survey calibration, or for performing linear regression with integer parameters <https://www.nass.usda.gov/Education_and_Outreach/Reports,_Presentations_and_Conferences/reports/New_Integer_Calibration_%20Procedure_2016.pdf>. This research was supported in part by the U.S. Department of Agriculture, National Agriculture Statistics Service. The findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA, or US Government determination or policy.",
    "version": "0.1.0",
    "maintainer": "Luca Sartore <drwolf85@gmail.com>",
    "author": "Luca Sartore [aut] (ORCID = \"0000-0002-0446-1328\"),\n  Luca Sartore [cre] (ORCID = \"0000-0002-0446-1328\"),\n  Kelly Toppin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=inca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inca Integer Calibration Specific functions are provided for rounding real weights to integers and performing an integer programming algorithm for calibration problems. These functions are useful for census-weights adjustments, survey calibration, or for performing linear regression with integer parameters <https://www.nass.usda.gov/Education_and_Outreach/Reports,_Presentations_and_Conferences/reports/New_Integer_Calibration_%20Procedure_2016.pdf>. This research was supported in part by the U.S. Department of Agriculture, National Agriculture Statistics Service. The findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA, or US Government determination or policy.  "
  },
  {
    "id": 14393,
    "package_name": "incidental",
    "title": "Implements Empirical Bayes Incidence Curves",
    "description": "Make empirical Bayes incidence curves from reported case data using a specified delay distribution.",
    "version": "0.1",
    "maintainer": "Lauren Hannah <lauren_hannah@apple.com>",
    "author": "Andrew Miller [aut],\n  Lauren Hannah [aut, cre],\n  Nicholas Foti [aut],\n  Joseph Futoma [aut],\n  Apple, Inc. [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=incidental",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "incidental Implements Empirical Bayes Incidence Curves Make empirical Bayes incidence curves from reported case data using a specified delay distribution.  "
  },
  {
    "id": 14404,
    "package_name": "indiedown",
    "title": "Individual R Markdown Templates",
    "description": "Simplifies the generation of customized R Markdown PDF templates.\n    A template may include an individual logo, typography, geometry or color\n    scheme. The package provides a skeleton with detailed instructions for\n    customizations. The skeleton can be modified by changing defaults in the\n    'YAML' header, by adding additional 'LaTeX' commands or by applying dynamic\n    adjustments in R. Individual corporate design elements, such as a title page, can be added as R functions that produce 'LaTeX' code.",
    "version": "0.1.1",
    "maintainer": "Christoph Sax <christoph.sax@gmail.com>",
    "author": "Christoph Sax [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7192-7044>),\n  Kirill M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-1416-3412>),\n  Angelica Becerra [aut],\n  Frederik Aust [aut],\n  cynkra GmbH [fnd, cph]",
    "url": "https://cynkra.github.io/indiedown/,\nhttps://github.com/cynkra/indiedown",
    "bug_reports": "https://github.com/cynkra/indiedown/issues",
    "repository": "https://cran.r-project.org/package=indiedown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "indiedown Individual R Markdown Templates Simplifies the generation of customized R Markdown PDF templates.\n    A template may include an individual logo, typography, geometry or color\n    scheme. The package provides a skeleton with detailed instructions for\n    customizations. The skeleton can be modified by changing defaults in the\n    'YAML' header, by adding additional 'LaTeX' commands or by applying dynamic\n    adjustments in R. Individual corporate design elements, such as a title page, can be added as R functions that produce 'LaTeX' code.  "
  },
  {
    "id": 14428,
    "package_name": "influential",
    "title": "Identification and Classification of the Most Influential Nodes",
    "description": "Contains functions for the classification and ranking of top candidate features, reconstruction of networks from\n    adjacency matrices and data frames, analysis of the topology of the network \n    and calculation of centrality measures, and identification of the most\n    influential nodes. Also, a function is provided for running SIRIR model, which \n    is the combination of leave-one-out cross validation technique and the conventional SIR model, on a network to unsupervisedly rank the true influence of vertices. Additionally, some functions have been provided for the assessment \n    of dependence and correlation of two network centrality measures as well as \n    the conditional probability of deviation from their corresponding means in opposite direction.\n    Fred Viole and David Nawrocki (2013, ISBN:1490523995).\n    Csardi G, Nepusz T (2006). \"The igraph software package for complex network research.\" InterJournal, Complex Systems, 1695.\n    Adopted algorithms and sources are referenced in function document.",
    "version": "2.2.9",
    "maintainer": "Adrian Salavaty <abbas.salavaty@gmail.com>",
    "author": "Abbas (Adrian) Salavaty [aut, cre], Mirana Ramialison [ths], Peter D. Currie [ths]",
    "url": "https://github.com/asalavaty/influential,\nhttps://asalavaty.github.io/influential/",
    "bug_reports": "https://github.com/asalavaty/influential/issues",
    "repository": "https://cran.r-project.org/package=influential",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influential Identification and Classification of the Most Influential Nodes Contains functions for the classification and ranking of top candidate features, reconstruction of networks from\n    adjacency matrices and data frames, analysis of the topology of the network \n    and calculation of centrality measures, and identification of the most\n    influential nodes. Also, a function is provided for running SIRIR model, which \n    is the combination of leave-one-out cross validation technique and the conventional SIR model, on a network to unsupervisedly rank the true influence of vertices. Additionally, some functions have been provided for the assessment \n    of dependence and correlation of two network centrality measures as well as \n    the conditional probability of deviation from their corresponding means in opposite direction.\n    Fred Viole and David Nawrocki (2013, ISBN:1490523995).\n    Csardi G, Nepusz T (2006). \"The igraph software package for complex network research.\" InterJournal, Complex Systems, 1695.\n    Adopted algorithms and sources are referenced in function document.  "
  },
  {
    "id": 14433,
    "package_name": "informedSen",
    "title": "Sensitivity Analysis Informed by a Test for Bias",
    "description": "After testing for biased treatment assignment in an observational study using an unaffected outcome, the sensitivity analysis is constrained to be compatible with that test.  The package uses the optimization software gurobi obtainable from <https://www.gurobi.com/>, together with its associated R package, also called gurobi; see: <https://www.gurobi.com/documentation/7.0/refman/installing_the_r_package.html>.  The method is a substantial computational and practical enhancement of a concept introduced in Rosenbaum (1992) Detecting bias with confidence in observational studies Biometrika, 79(2), 367-374  <doi:10.1093/biomet/79.2.367>.",
    "version": "1.0.7",
    "maintainer": "Paul R Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=informedSen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "informedSen Sensitivity Analysis Informed by a Test for Bias After testing for biased treatment assignment in an observational study using an unaffected outcome, the sensitivity analysis is constrained to be compatible with that test.  The package uses the optimization software gurobi obtainable from <https://www.gurobi.com/>, together with its associated R package, also called gurobi; see: <https://www.gurobi.com/documentation/7.0/refman/installing_the_r_package.html>.  The method is a substantial computational and practical enhancement of a concept introduced in Rosenbaum (1992) Detecting bias with confidence in observational studies Biometrika, 79(2), 367-374  <doi:10.1093/biomet/79.2.367>.  "
  },
  {
    "id": 14445,
    "package_name": "inlinedocs",
    "title": "Convert Inline Comments to Documentation",
    "description": "Generates Rd files from R source code with comments.\n The main features of the default syntax are that\n (1) docs are defined in comments near the relevant code,\n (2) function argument names are not repeated in comments, and\n (3) examples are defined in R code, not comments.\n It is also easy to define a new syntax.",
    "version": "2023.9.4",
    "maintainer": "Toby Dylan Hocking <toby.hocking@r-project.org>",
    "author": "Toby Dylan Hocking [aut, cre],\n  Keith Ponting [aut],\n  Thomas Wutzler [aut],\n  Philippe Grosjean [aut],\n  Markus M\u00fcller [aut],\n  R Core Team [ctb, cph]",
    "url": "https://github.com/tdhock/inlinedocs",
    "bug_reports": "https://github.com/tdhock/inlinedocs/issues",
    "repository": "https://cran.r-project.org/package=inlinedocs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inlinedocs Convert Inline Comments to Documentation Generates Rd files from R source code with comments.\n The main features of the default syntax are that\n (1) docs are defined in comments near the relevant code,\n (2) function argument names are not repeated in comments, and\n (3) examples are defined in R code, not comments.\n It is also easy to define a new syntax.  "
  },
  {
    "id": 14450,
    "package_name": "inpdfr",
    "title": "Analyse Text Documents Using Ecological Tools",
    "description": "A set of functions to analyse and compare texts, using classical \n  text mining\tfunctions, as well as those from theoretical ecology.",
    "version": "0.1.12",
    "maintainer": "Rebaudo Francois <francois.rebaudo@ird.fr>",
    "author": "Rebaudo Francois (IRD, UMR EGCE, IRD, CNRS, Univ. ParisSaclay)",
    "url": "https://github.com/frareb/inpdfr/",
    "bug_reports": "https://github.com/frareb/inpdfr/issues",
    "repository": "https://cran.r-project.org/package=inpdfr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inpdfr Analyse Text Documents Using Ecological Tools A set of functions to analyse and compare texts, using classical \n  text mining\tfunctions, as well as those from theoretical ecology.  "
  },
  {
    "id": 14457,
    "package_name": "inspectdf",
    "title": "Inspection, Comparison and Visualisation of Data Frames",
    "description": "A collection of utilities for columnwise summary, comparison and visualisation of data frames.  Functions report missingness, categorical levels, numeric distribution, correlation, column types and memory usage.",
    "version": "0.0.12.1",
    "maintainer": "Alastair Rushworth <alastairmrushworth@gmail.com>",
    "author": "Alastair Rushworth [aut, cre],\n  David Wilkins [ctb]",
    "url": "https://alastairrushworth.github.io/inspectdf/",
    "bug_reports": "https://github.com/alastairrushworth/inspectdf/issues",
    "repository": "https://cran.r-project.org/package=inspectdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inspectdf Inspection, Comparison and Visualisation of Data Frames A collection of utilities for columnwise summary, comparison and visualisation of data frames.  Functions report missingness, categorical levels, numeric distribution, correlation, column types and memory usage.  "
  },
  {
    "id": 14477,
    "package_name": "intendo",
    "title": "A Group of Fun Datasets of Various Sizes and Differing Levels of\nQuality",
    "description": "Four datasets are provided here from the 'Intendo' game\n    'Super Jetroid'. It is data from the 2015 year of operation and it comprises\n    a revenue table ('all_revenue'), a daily users table ('users_daily'), a user\n    summary table ('user_summary'), and a table with data on all user sessions\n    ('all_sessions'). These core datasets come in different sizes, and, each of\n    them has a variant that was intentionally made faulty (totally riddled with\n    errors and inconsistencies). This suite of tables is useful for testing with\n    packages that focus on data validation and data documentation.",
    "version": "0.1.1",
    "maintainer": "Richard Iannone <riannone@me.com>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3925-190X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=intendo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intendo A Group of Fun Datasets of Various Sizes and Differing Levels of\nQuality Four datasets are provided here from the 'Intendo' game\n    'Super Jetroid'. It is data from the 2015 year of operation and it comprises\n    a revenue table ('all_revenue'), a daily users table ('users_daily'), a user\n    summary table ('user_summary'), and a table with data on all user sessions\n    ('all_sessions'). These core datasets come in different sizes, and, each of\n    them has a variant that was intentionally made faulty (totally riddled with\n    errors and inconsistencies). This suite of tables is useful for testing with\n    packages that focus on data validation and data documentation.  "
  },
  {
    "id": 14481,
    "package_name": "interactionR",
    "title": "Full Reporting of Interaction Analyses",
    "description": "Produces a publication-ready table that includes all effect estimates necessary for full reporting effect modification and interaction analysis as recommended by Knol and Vanderweele (2012) [<doi:10.1093/ije/dyr218>]. It also estimates confidence interval for the trio of additive interaction measures using the delta method (see Hosmer and Lemeshow (1992), [<doi:10.1097/00001648-199209000-00012>]), variance recovery method (see Zou (2008), [<doi:10.1093/aje/kwn104>]), or percentile bootstrapping (see Assmann et al. (1996), [<doi:10.1097/00001648-199605000-00012>]). ",
    "version": "0.1.7",
    "maintainer": "Babatunde Alli <babatunde.alli@mail.mcgill.ca>",
    "author": "Babatunde Alli",
    "url": "https://github.com/tunsmart/interactionR",
    "bug_reports": "https://github.com/tunsmart/interactionR/issues",
    "repository": "https://cran.r-project.org/package=interactionR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "interactionR Full Reporting of Interaction Analyses Produces a publication-ready table that includes all effect estimates necessary for full reporting effect modification and interaction analysis as recommended by Knol and Vanderweele (2012) [<doi:10.1093/ije/dyr218>]. It also estimates confidence interval for the trio of additive interaction measures using the delta method (see Hosmer and Lemeshow (1992), [<doi:10.1097/00001648-199209000-00012>]), variance recovery method (see Zou (2008), [<doi:10.1093/aje/kwn104>]), or percentile bootstrapping (see Assmann et al. (1996), [<doi:10.1097/00001648-199605000-00012>]).   "
  },
  {
    "id": 14498,
    "package_name": "interpretCI",
    "title": "Estimate the Confidence Interval and Interpret Step by Step",
    "description": "Estimate confidence intervals for mean, proportion, mean difference \n   for unpaired and paired samples and proportion difference. Plot the confidence \n   intervals. Generate documents explaining the statistical result step by step.",
    "version": "0.1.1",
    "maintainer": "Keon-Woong Moon <cardiomoon@gmail.com>",
    "author": "Keon-Woong Moon [aut, cre]",
    "url": "https://github.com/cardiomoon/interpretCI,\nhttps://cardiomoon.github.io/interpretCI/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=interpretCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "interpretCI Estimate the Confidence Interval and Interpret Step by Step Estimate confidence intervals for mean, proportion, mean difference \n   for unpaired and paired samples and proportion difference. Plot the confidence \n   intervals. Generate documents explaining the statistical result step by step.  "
  },
  {
    "id": 14522,
    "package_name": "inventorize",
    "title": "Inventory Analytics, Pricing and Markdowns",
    "description": "Simulate inventory policies with and without forecasting, facilitate inventory analysis calculations such as  stock levels and re-order points,pricing and promotions calculations. \n  The package includes calculations of inventory metrics, stock-out calculations and ABC analysis calculations.\n    The package includes revenue management techniques such as Multi-product optimization,logit and polynomial model optimization.\n  The functions are referenced from :\n  1-Harris, Ford W. (1913). \"How many parts to make at once\". Factory, The Magazine of Management. \n  2- Nahmias, S. Production and Operations Analysis. McGraw-Hill International Edition.\n  3-Silver, E.A., Pyke, D.F., Peterson, R. Inventory Management and Production Planning and Scheduling. \n  4-Ballou, R.H. Business Logistics Management. \n  5-MIT Micromasters Program. \n  6- Columbia University  course for supply and demand analysis.\n  8- Price Elasticity of Demand MATH 104,Mark Mac Lean (with assistance from Patrick Chan) 2011W\n  For further details or correspondence :<www.linkedin.com/in/haythamomar>, <www.rescaleanalytics.com>.",
    "version": "1.1.2",
    "maintainer": "Haytham Omar <haytham@rescaleanalytics.com>",
    "author": "Haytham Omar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=inventorize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inventorize Inventory Analytics, Pricing and Markdowns Simulate inventory policies with and without forecasting, facilitate inventory analysis calculations such as  stock levels and re-order points,pricing and promotions calculations. \n  The package includes calculations of inventory metrics, stock-out calculations and ABC analysis calculations.\n    The package includes revenue management techniques such as Multi-product optimization,logit and polynomial model optimization.\n  The functions are referenced from :\n  1-Harris, Ford W. (1913). \"How many parts to make at once\". Factory, The Magazine of Management. \n  2- Nahmias, S. Production and Operations Analysis. McGraw-Hill International Edition.\n  3-Silver, E.A., Pyke, D.F., Peterson, R. Inventory Management and Production Planning and Scheduling. \n  4-Ballou, R.H. Business Logistics Management. \n  5-MIT Micromasters Program. \n  6- Columbia University  course for supply and demand analysis.\n  8- Price Elasticity of Demand MATH 104,Mark Mac Lean (with assistance from Patrick Chan) 2011W\n  For further details or correspondence :<www.linkedin.com/in/haythamomar>, <www.rescaleanalytics.com>.  "
  },
  {
    "id": 14549,
    "package_name": "ipbase",
    "title": "Client for the 'ipbase.com' IP Geolocation API",
    "description": "An R client for the 'ipbase.com' IP Geolocation API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://ipbase.com/docs> .",
    "version": "0.1.1",
    "maintainer": "Dominik Kukacka <dominik@everapi.com>",
    "author": "Dominik Kukacka [aut, cre]",
    "url": "https://ipbase.com, https://ipbase.com/docs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ipbase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ipbase Client for the 'ipbase.com' IP Geolocation API An R client for the 'ipbase.com' IP Geolocation API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://ipbase.com/docs> .  "
  },
  {
    "id": 14561,
    "package_name": "iplookupapi",
    "title": "Client for the 'iplookupapi.com' IP Lookup API",
    "description": "An R client for the 'iplookupapi.com' IP Lookup API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://iplookupapi.com/docs> .",
    "version": "0.1.0",
    "maintainer": "Dominik Kukacka <dominik@everapi.com>",
    "author": "Dominik Kukacka [aut, cre]",
    "url": "https://iplookupapi.com, https://iplookupapi.com/docs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iplookupapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iplookupapi Client for the 'iplookupapi.com' IP Lookup API An R client for the 'iplookupapi.com' IP Lookup API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://iplookupapi.com/docs> .  "
  },
  {
    "id": 14636,
    "package_name": "itemanalysis",
    "title": "Classical Test Theory Item Analysis",
    "description": "Runs classical item analysis for multiple-choice test items and polytomous items (e.g., rating scales). The statistics reported in this package can be found in any measurement textbook such as Crocker and Algina (2006, ISBN:9780495395911).",
    "version": "1.1",
    "maintainer": "Cengiz Zopluoglu <cen.zop@gmail.com>",
    "author": "Cengiz Zopluoglu ",
    "url": "https://cengiz.me/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=itemanalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "itemanalysis Classical Test Theory Item Analysis Runs classical item analysis for multiple-choice test items and polytomous items (e.g., rating scales). The statistics reported in this package can be found in any measurement textbook such as Crocker and Algina (2006, ISBN:9780495395911).  "
  },
  {
    "id": 14648,
    "package_name": "itrimhoch",
    "title": "Improved Trimmed Weighted Hochberg Procedures and Sample Size\nOptimization",
    "description": "The improved trimmed weighted Hochberg procedure provides increased statistical power and relaxes the dependence assumptions for familywise error rate control compared to the original weighted Hochberg procedure. This package computes the boundaries required for implementing the proposed methodology and includes sample size optimization methods.\n    See Gou, J., Chang, Y., Li, T., and Zhang, F.(2025). Improved trimmed weighted Hochberg procedures with two endpoints and sample size optimization. Technical Report.",
    "version": "1.0.0",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing (Zoe) Zhang [aut],\n  Yizhuo Chang [ctb],\n  Tianqi Li [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=itrimhoch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "itrimhoch Improved Trimmed Weighted Hochberg Procedures and Sample Size\nOptimization The improved trimmed weighted Hochberg procedure provides increased statistical power and relaxes the dependence assumptions for familywise error rate control compared to the original weighted Hochberg procedure. This package computes the boundaries required for implementing the proposed methodology and includes sample size optimization methods.\n    See Gou, J., Chang, Y., Li, T., and Zhang, F.(2025). Improved trimmed weighted Hochberg procedures with two endpoints and sample size optimization. Technical Report.  "
  },
  {
    "id": 14658,
    "package_name": "ivdesign",
    "title": "Hypothesis Testing in Cluster-Randomized Encouragement Designs",
    "description": "An implementation of randomization-based hypothesis \n    testing for three different estimands in a cluster-randomized \n    encouragement experiment. The three estimands include (1) testing\n    a cluster-level constant proportional treatment effect (Fisher's\n    sharp null hypothesis), (2) pooled effect ratio, and (3) average \n    cluster effect ratio. To test the third estimand, user needs to install\n    'Gurobi' (>= 9.0.1) optimizer via its R API. Please refer to \n    <https://www.gurobi.com/documentation/9.0/refman/ins_the_r_package.html>.",
    "version": "0.1.0",
    "maintainer": "Bo Zhang <bozhan@wharton.upenn.edu>",
    "author": "Bo Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ivdesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ivdesign Hypothesis Testing in Cluster-Randomized Encouragement Designs An implementation of randomization-based hypothesis \n    testing for three different estimands in a cluster-randomized \n    encouragement experiment. The three estimands include (1) testing\n    a cluster-level constant proportional treatment effect (Fisher's\n    sharp null hypothesis), (2) pooled effect ratio, and (3) average \n    cluster effect ratio. To test the third estimand, user needs to install\n    'Gurobi' (>= 9.0.1) optimizer via its R API. Please refer to \n    <https://www.gurobi.com/documentation/9.0/refman/ins_the_r_package.html>.  "
  },
  {
    "id": 14664,
    "package_name": "ivo.table",
    "title": "Nicely Formatted Contingency Tables and Frequency Tables",
    "description": "Nicely formatted frequency tables and contingency tables (1-way, 2-way, 3-way and 4-way tables), that can easily be exported to HTML or 'Office' documents. Designed to work with pipes.",
    "version": "0.7.1",
    "maintainer": "M\u00e5ns Thulin <mans@statistikkonsult.com>",
    "author": "M\u00e5ns Thulin [aut, cre],\n  Kajsa Grind [aut],\n  Stefan Furne [aut]",
    "url": "https://github.com/mthulin/ivo.table,\nhttps://mthulin.github.io/ivo.table/",
    "bug_reports": "https://github.com/mthulin/ivo.table/issues",
    "repository": "https://cran.r-project.org/package=ivo.table",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ivo.table Nicely Formatted Contingency Tables and Frequency Tables Nicely formatted frequency tables and contingency tables (1-way, 2-way, 3-way and 4-way tables), that can easily be exported to HTML or 'Office' documents. Designed to work with pipes.  "
  },
  {
    "id": 14672,
    "package_name": "ixplorer",
    "title": "Easy DataOps for R Users",
    "description": "Create and view tickets in 'gitea', a self-hosted git service <https://gitea.io>, using an 'RStudio' addin, and use helper functions to publish documentation and use git.",
    "version": "0.2.2",
    "maintainer": "Frans van Dunne <frans@ixpantia.com>",
    "author": "ixpantia, SRL [cph],\n  Frans van Dunne [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-7853-2811>),\n  Ronny Hern\u00e1ndez Mora [aut] (ORCID:\n    <https://orcid.org/0000-0001-6225-7096>),\n  Daniel Granados Campos [ctb],\n  Nayib Vargas Zu\u00f1iga [ctb]",
    "url": "https://github.com/ixpantia/ixplorer",
    "bug_reports": "https://github.com/ixpantia/ixplorer/issues",
    "repository": "https://cran.r-project.org/package=ixplorer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ixplorer Easy DataOps for R Users Create and view tickets in 'gitea', a self-hosted git service <https://gitea.io>, using an 'RStudio' addin, and use helper functions to publish documentation and use git.  "
  },
  {
    "id": 14692,
    "package_name": "jamba",
    "title": "Just Analysis Methods Base",
    "description": "Just analysis methods ('jam') base functions\n    focused on bioinformatics.\n    Version- and gene-centric alphanumeric sort,\n    unique name and version assignment, colorized console and 'HTML' output,\n    color ramp and palette manipulation,\n    'Rmarkdown' cache import, styled 'Excel' worksheet import and export,\n    interpolated raster output from smooth scatter and image plots,\n    list to delimited vector, efficient list tools.",
    "version": "1.0.4",
    "maintainer": "James M. Ward <jmw86069@gmail.com>",
    "author": "James M. Ward [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9510-2848>)",
    "url": "https://jmw86069.github.io/jamba/",
    "bug_reports": "https://github.com/jmw86069/jamba/issues",
    "repository": "https://cran.r-project.org/package=jamba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jamba Just Analysis Methods Base Just analysis methods ('jam') base functions\n    focused on bioinformatics.\n    Version- and gene-centric alphanumeric sort,\n    unique name and version assignment, colorized console and 'HTML' output,\n    color ramp and palette manipulation,\n    'Rmarkdown' cache import, styled 'Excel' worksheet import and export,\n    interpolated raster output from smooth scatter and image plots,\n    list to delimited vector, efficient list tools.  "
  },
  {
    "id": 14694,
    "package_name": "janitor",
    "title": "Simple Tools for Examining and Cleaning Dirty Data",
    "description": "The main janitor functions can: perfectly format data.frame column\n    names; provide quick counts of variable combinations (i.e., frequency\n    tables and crosstabs); and explore duplicate records. Other janitor functions\n    nicely format the tabulation results. These tabulate-and-report functions\n    approximate popular features of SPSS and Microsoft Excel. This package\n    follows the principles of the \"tidyverse\" and works well with the pipe function\n    %>%. janitor was built with beginning-to-intermediate R users in mind and is\n    optimized for user-friendliness.",
    "version": "2.2.1",
    "maintainer": "Sam Firke <samuel.firke@gmail.com>",
    "author": "Sam Firke [aut, cre],\n  Bill Denney [ctb],\n  Chris Haid [ctb],\n  Ryan Knight [ctb],\n  Malte Grosser [ctb],\n  Jonathan Zadra [ctb]",
    "url": "https://github.com/sfirke/janitor,\nhttps://sfirke.github.io/janitor/",
    "bug_reports": "https://github.com/sfirke/janitor/issues",
    "repository": "https://cran.r-project.org/package=janitor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "janitor Simple Tools for Examining and Cleaning Dirty Data The main janitor functions can: perfectly format data.frame column\n    names; provide quick counts of variable combinations (i.e., frequency\n    tables and crosstabs); and explore duplicate records. Other janitor functions\n    nicely format the tabulation results. These tabulate-and-report functions\n    approximate popular features of SPSS and Microsoft Excel. This package\n    follows the principles of the \"tidyverse\" and works well with the pipe function\n    %>%. janitor was built with beginning-to-intermediate R users in mind and is\n    optimized for user-friendliness.  "
  },
  {
    "id": 14704,
    "package_name": "jds.rmd",
    "title": "R Markdown Templates for Journal of Data Science",
    "description": "\n    Customized R Markdown templates for authoring articles\n    for Journal of Data Science.",
    "version": "0.3.3",
    "maintainer": "Wenjie Wang <wang@wwenjie.org>",
    "author": "Wenjie Wang [aut, cre] (ORCID: <https://orcid.org/0000-0003-0363-3180>),\n  Jun Yan [aut] (ORCID: <https://orcid.org/0000-0003-4401-7296>)",
    "url": "https://github.com/wenjie2wang/jds.rmd",
    "bug_reports": "https://github.com/wenjie2wang/jds.rmd/issues",
    "repository": "https://cran.r-project.org/package=jds.rmd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jds.rmd R Markdown Templates for Journal of Data Science \n    Customized R Markdown templates for authoring articles\n    for Journal of Data Science.  "
  },
  {
    "id": 14714,
    "package_name": "jinjar",
    "title": "Template Engine Inspired by 'Jinja'",
    "description": "Template engine powered by the 'inja' C++ library. Users\n    write a template document, using syntax inspired by the 'Jinja' Python\n    package, and then render the final document by passing data from R.\n    The template syntax supports features such as variables, loops,\n    conditions and inheritance.",
    "version": "0.3.2",
    "maintainer": "David Hall <david.hall.physics@gmail.com>",
    "author": "David Hall [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2193-0480>),\n  Lars Berscheid [cph] (Author of bundled inja library),\n  Niels Lohmann [cph] (Author of bundled json library)",
    "url": "https://davidchall.github.io/jinjar/,\nhttps://github.com/davidchall/jinjar",
    "bug_reports": "https://github.com/davidchall/jinjar/issues",
    "repository": "https://cran.r-project.org/package=jinjar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jinjar Template Engine Inspired by 'Jinja' Template engine powered by the 'inja' C++ library. Users\n    write a template document, using syntax inspired by the 'Jinja' Python\n    package, and then render the final document by passing data from R.\n    The template syntax supports features such as variables, loops,\n    conditions and inheritance.  "
  },
  {
    "id": 14750,
    "package_name": "josaplay",
    "title": "Add Josa Based on Previous Letter in Korean",
    "description": "Josa in Korean is often determined by judging the previous word. \n            When writing reports using Rmd, a function that prints the \n            appropriate investigation for each case is helpful. \n            The 'josaplay' package then evaluates the previous word \n            to determine which josa is appropriate.",
    "version": "0.1.3",
    "maintainer": "Chanyub Park <mrchypark@gmail.com>",
    "author": "Chanyub Park [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6474-2570>)",
    "url": "https://github.com/mrchypark/josaplay",
    "bug_reports": "https://github.com/mrchypark/josaplay/issues",
    "repository": "https://cran.r-project.org/package=josaplay",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "josaplay Add Josa Based on Previous Letter in Korean Josa in Korean is often determined by judging the previous word. \n            When writing reports using Rmd, a function that prints the \n            appropriate investigation for each case is helpful. \n            The 'josaplay' package then evaluates the previous word \n            to determine which josa is appropriate.  "
  },
  {
    "id": 14751,
    "package_name": "jot",
    "title": "Jot Down Values for Later",
    "description": "Reproducible work requires a record of where every statistic originated.\n    When writing reports, some data is too big to load in the same environment and some statistics take a while to compute.\n    This package offers a way to keep notes on statistics, simple functions, and small objects.\n    Notepads can be locked to avoid accidental updates.\n    Notepads keep track of who added the notes and when the notes were added.\n    A simple text representation is used to allow for clear version histories.",
    "version": "0.0.5",
    "maintainer": "Christopher T. Kenny <ctkenny@proton.me>",
    "author": "Christopher T. Kenny [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9386-6860>)",
    "url": "http://christophertkenny.com/jot/,\nhttps://github.com/christopherkenny/jot",
    "bug_reports": "https://github.com/christopherkenny/jot/issues",
    "repository": "https://cran.r-project.org/package=jot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jot Jot Down Values for Later Reproducible work requires a record of where every statistic originated.\n    When writing reports, some data is too big to load in the same environment and some statistics take a while to compute.\n    This package offers a way to keep notes on statistics, simple functions, and small objects.\n    Notepads can be locked to avoid accidental updates.\n    Notepads keep track of who added the notes and when the notes were added.\n    A simple text representation is used to allow for clear version histories.  "
  },
  {
    "id": 14752,
    "package_name": "journalabbr",
    "title": "Journal Abbreviations for BibTeX Documents",
    "description": "Since the reference management software (such as 'Zotero', 'Mendeley') exports Bib file journal abbreviation is not detailed enough, the 'journalabbr' package only abbreviates the journal field of Bib file, and then outputs a new Bib file for generating reference format with journal abbreviation on other software (such as 'texstudio'). The abbreviation table is from 'JabRef'. At the same time, 'Shiny' application is provided to generate 'thebibliography', a reference format that can be directly used for latex paper writing based on 'Rmd' files.",
    "version": "0.4.3",
    "maintainer": "ShuCai Zou <zscmoyujian@163.com>",
    "author": "ShuCai Zou [aut, cre],\n  Yu Chen [aut]",
    "url": "https://github.com/zoushucai/journalabbr",
    "bug_reports": "https://github.com/zoushucai/journalabbr/issues",
    "repository": "https://cran.r-project.org/package=journalabbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "journalabbr Journal Abbreviations for BibTeX Documents Since the reference management software (such as 'Zotero', 'Mendeley') exports Bib file journal abbreviation is not detailed enough, the 'journalabbr' package only abbreviates the journal field of Bib file, and then outputs a new Bib file for generating reference format with journal abbreviation on other software (such as 'texstudio'). The abbreviation table is from 'JabRef'. At the same time, 'Shiny' application is provided to generate 'thebibliography', a reference format that can be directly used for latex paper writing based on 'Rmd' files.  "
  },
  {
    "id": 14761,
    "package_name": "jquerylib",
    "title": "Obtain 'jQuery' as an HTML Dependency Object",
    "description": "Obtain any major version of 'jQuery' (<https://code.jquery.com/>) and use it in any webpage generated by 'htmltools' (e.g. 'shiny', 'htmlwidgets', and 'rmarkdown').\n    Most R users don't need to use this package directly, but other R packages (e.g. 'shiny', 'rmarkdown', etc.) depend on this package to avoid bundling redundant copies of 'jQuery'.",
    "version": "0.1.4",
    "maintainer": "Carson Sievert <carson@rstudio.com>",
    "author": "Carson Sievert [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4958-2844>),\n  Joe Cheng [aut],\n  RStudio [cph],\n  jQuery Foundation [cph] (jQuery library and jQuery UI library),\n  jQuery contributors [ctb, cph] (jQuery library; authors listed in\n    inst/lib/jquery-AUTHORS.txt)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jquerylib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jquerylib Obtain 'jQuery' as an HTML Dependency Object Obtain any major version of 'jQuery' (<https://code.jquery.com/>) and use it in any webpage generated by 'htmltools' (e.g. 'shiny', 'htmlwidgets', and 'rmarkdown').\n    Most R users don't need to use this package directly, but other R packages (e.g. 'shiny', 'rmarkdown', etc.) depend on this package to avoid bundling redundant copies of 'jQuery'.  "
  },
  {
    "id": 14768,
    "package_name": "jsTree",
    "title": "Create Interactive Trees with the 'jQuery' 'jsTree' Plugin",
    "description": "Create and customize interactive trees using the\n       'jQuery' 'jsTree' <https://www.jstree.com/> plugin\n       library and the 'htmlwidgets' package. These trees can\n       be used directly from the R console, from 'RStudio', in\n       Shiny apps and R Markdown documents.",
    "version": "1.2",
    "maintainer": "Jonathan Sidi <yonicd@gmail.com>",
    "author": "Jonathan Sidi [aut, cre],\n  Kohleth Chia [ctb]",
    "url": "https://github.com/yonicd/jsTree",
    "bug_reports": "https://github.com/yonicd/jsTree/issues",
    "repository": "https://cran.r-project.org/package=jsTree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jsTree Create Interactive Trees with the 'jQuery' 'jsTree' Plugin Create and customize interactive trees using the\n       'jQuery' 'jsTree' <https://www.jstree.com/> plugin\n       library and the 'htmlwidgets' package. These trees can\n       be used directly from the R console, from 'RStudio', in\n       Shiny apps and R Markdown documents.  "
  },
  {
    "id": 14769,
    "package_name": "jsTreeR",
    "title": "A Wrapper of the JavaScript Library 'jsTree'",
    "description": "Creates interactive trees that can be included in 'Shiny'\n    apps and R markdown documents. A tree allows to represent hierarchical\n    data (e.g. the contents of a directory). Similar to the 'shinyTree'\n    package but offers more features and options, such as the grid\n    extension, restricting the drag-and-drop behavior, and settings for\n    the search functionality. It is possible to attach some data to the\n    nodes of a tree and then to get these data in 'Shiny' when a node is\n    selected. Also provides a 'Shiny' gadget allowing to manipulate one or\n    more folders, and a 'Shiny' module allowing to navigate in the server\n    side file system.",
    "version": "2.6.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  jQuery contributors [ctb, cph] (jQuery),\n  Ivan Bozhanov [ctb, cph] (jsTree),\n  Vedran Opacic [ctb, cph] (jsTree bootstrap theme),\n  Avi Deitcher [ctb, cph] (jsTreeGrid),\n  Philip Hutchison [ctb, cph] (PDFObject),\n  Terence Eden [ctb, cph] (SuperTinyIcons)",
    "url": "https://github.com/stla/jsTreeR",
    "bug_reports": "https://github.com/stla/jsTreeR/issues",
    "repository": "https://cran.r-project.org/package=jsTreeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jsTreeR A Wrapper of the JavaScript Library 'jsTree' Creates interactive trees that can be included in 'Shiny'\n    apps and R markdown documents. A tree allows to represent hierarchical\n    data (e.g. the contents of a directory). Similar to the 'shinyTree'\n    package but offers more features and options, such as the grid\n    extension, restricting the drag-and-drop behavior, and settings for\n    the search functionality. It is possible to attach some data to the\n    nodes of a tree and then to get these data in 'Shiny' when a node is\n    selected. Also provides a 'Shiny' gadget allowing to manipulate one or\n    more folders, and a 'Shiny' module allowing to navigate in the server\n    side file system.  "
  },
  {
    "id": 14770,
    "package_name": "jshintr",
    "title": "Lint 'JavaScript' Files",
    "description": "Allow to run 'jshint' on 'JavaScript' files with a 'R'\n    command or a 'RStudio' addin. The report appears in the 'RStudio'\n    viewer pane.",
    "version": "0.1.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Anton Kovalyov [cph] ('jshint' library)",
    "url": "https://github.com/stla/jshintr",
    "bug_reports": "https://github.com/stla/jshintr/issues",
    "repository": "https://cran.r-project.org/package=jshintr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jshintr Lint 'JavaScript' Files Allow to run 'jshint' on 'JavaScript' files with a 'R'\n    command or a 'RStudio' addin. The report appears in the 'RStudio'\n    viewer pane.  "
  },
  {
    "id": 14785,
    "package_name": "juicedown",
    "title": "'juice' + 'markdown': Convert 'R Markdown' into 'HTML' with\nInline Styles",
    "description": "A convenience tool to create 'HTML' with inline styles using \n  'juicyjuice' and 'markdown' packages. It is particularly useful when working \n  on a content management system (CMS) whose code editor eliminates style and \n  link tags. The main use case of the package is the learning management system, \n  'Moodle'. Additional helper functions for teaching purposes are provided. Learn \n  more about 'juicedown' at <https://kenjisato.github.io/juicedown/>.",
    "version": "0.1.2",
    "maintainer": "Kenji Sato <kenji@kenjisato.jp>",
    "author": "Kenji Sato [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0000-4520-2560>)",
    "url": "https://kenjisato.github.io/juicedown/,\nhttps://github.com/kenjisato/juicedown",
    "bug_reports": "https://github.com/kenjisato/juicedown/issues",
    "repository": "https://cran.r-project.org/package=juicedown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "juicedown 'juice' + 'markdown': Convert 'R Markdown' into 'HTML' with\nInline Styles A convenience tool to create 'HTML' with inline styles using \n  'juicyjuice' and 'markdown' packages. It is particularly useful when working \n  on a content management system (CMS) whose code editor eliminates style and \n  link tags. The main use case of the package is the learning management system, \n  'Moodle'. Additional helper functions for teaching purposes are provided. Learn \n  more about 'juicedown' at <https://kenjisato.github.io/juicedown/>.  "
  },
  {
    "id": 14786,
    "package_name": "juicr",
    "title": "Automated and Manual Extraction of Numerical Data from\nScientific Images",
    "description": "Provides a GUI interface for automating data extraction from \n  multiple images containing scatter and bar plots, semi-automated tools to tinker \n  with extraction attempts, and a fully-loaded point-and-click manual extractor \n  with image zoom, calibrator, and classifier. Also provides detailed and \n  R-independent extraction reports as fully-embedded .html records.",
    "version": "0.1",
    "maintainer": "Marc J. Lajeunesse <lajeunesse@usf.edu>",
    "author": "Marc J. Lajeunesse [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9678-2080>)",
    "url": "http://lajeunesse.myweb.usf.edu/ https://github.com/mjlajeunesse/\nhttps://www.youtube.com/c/LajeunesseLab/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=juicr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "juicr Automated and Manual Extraction of Numerical Data from\nScientific Images Provides a GUI interface for automating data extraction from \n  multiple images containing scatter and bar plots, semi-automated tools to tinker \n  with extraction attempts, and a fully-loaded point-and-click manual extractor \n  with image zoom, calibrator, and classifier. Also provides detailed and \n  R-independent extraction reports as fully-embedded .html records.  "
  },
  {
    "id": 14790,
    "package_name": "justifier",
    "title": "Human and Machine-Readable Justifications and Justified\nDecisions Based on 'YAML'",
    "description": "Leverages the 'yum' package to\n             implement a 'YAML' ('YAML Ain't Markup Language', a human\n             friendly standard for data serialization; see <https://yaml.org>)\n             standard for documenting justifications, such as for decisions\n             taken during the planning, execution and analysis of a study\n             or during the development of a behavior change intervention\n             as illustrated by Marques & Peters (2019)\n             <doi:10.17605/osf.io/ndxha>. These justifications are both\n             human- and machine-readable, facilitating efficient extraction\n             and organisation.",
    "version": "0.2.8",
    "maintainer": "Gjalt-Jorn Peters <justifier@opens.science>",
    "author": "Gjalt-Jorn Peters [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0336-9589>),\n  Szilvia Zorgo [ctb] (ORCID: <https://orcid.org/0000-0002-6916-2097>)",
    "url": "https://justifier.opens.science",
    "bug_reports": "https://codeberg.org/R-packages/justifier/issues",
    "repository": "https://cran.r-project.org/package=justifier",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "justifier Human and Machine-Readable Justifications and Justified\nDecisions Based on 'YAML' Leverages the 'yum' package to\n             implement a 'YAML' ('YAML Ain't Markup Language', a human\n             friendly standard for data serialization; see <https://yaml.org>)\n             standard for documenting justifications, such as for decisions\n             taken during the planning, execution and analysis of a study\n             or during the development of a behavior change intervention\n             as illustrated by Marques & Peters (2019)\n             <doi:10.17605/osf.io/ndxha>. These justifications are both\n             human- and machine-readable, facilitating efficient extraction\n             and organisation.  "
  },
  {
    "id": 14796,
    "package_name": "kableExtra",
    "title": "Construct Complex Table with 'kable' and Pipe Syntax",
    "description": "Build complex HTML or 'LaTeX' tables using 'kable()' from 'knitr' \n    and the piping syntax from 'magrittr'. Function 'kable()' is a light weight \n    table generator coming from 'knitr'. This package simplifies the way to \n    manipulate the HTML or 'LaTeX' codes generated by 'kable()' and allows \n    users to construct complex tables and customize styles using a readable \n    syntax. ",
    "version": "1.4.0",
    "maintainer": "Hao Zhu <haozhu233@gmail.com>",
    "author": "Hao Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0002-3386-6076>),\n  Thomas Travison [ctb],\n  Timothy Tsai [ctb],\n  Will Beasley [ctb],\n  Yihui Xie [ctb],\n  GuangChuang Yu [ctb],\n  St\u00e9phane Laurent [ctb],\n  Rob Shepherd [ctb],\n  Yoni Sidi [ctb],\n  Brian Salzer [ctb],\n  George Gui [ctb],\n  Yeliang Fan [ctb],\n  Duncan Murdoch [ctb],\n  Vincent Arel-Bundock [ctb],\n  Bill Evans [ctb]",
    "url": "http://haozhu233.github.io/kableExtra/,\nhttps://github.com/haozhu233/kableExtra",
    "bug_reports": "https://github.com/haozhu233/kableExtra/issues",
    "repository": "https://cran.r-project.org/package=kableExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kableExtra Construct Complex Table with 'kable' and Pipe Syntax Build complex HTML or 'LaTeX' tables using 'kable()' from 'knitr' \n    and the piping syntax from 'magrittr'. Function 'kable()' is a light weight \n    table generator coming from 'knitr'. This package simplifies the way to \n    manipulate the HTML or 'LaTeX' codes generated by 'kable()' and allows \n    users to construct complex tables and customize styles using a readable \n    syntax.   "
  },
  {
    "id": 14864,
    "package_name": "kfa",
    "title": "K-Fold Cross Validation for Factor Analysis",
    "description": "Provides functions to identify plausible and replicable factor\n  structures for a set of variables via k-fold cross validation. The process\n  combines the exploratory and confirmatory factor analytic approach to scale\n  development (Flora & Flake, 2017) <doi:10.1037/cbs0000069> with a cross validation\n  technique that maximizes the available data (Hastie, Tibshirani, & Friedman, 2009)\n  <isbn:978-0-387-21606-5>. Also available are functions to determine k by drawing \n  on power analytic techniques for covariance structures (MacCallum, Browne, &\n  Sugawara, 1996) <doi:10.1037/1082-989X.1.2.130>, generate model syntax, and\n  summarize results in a report.",
    "version": "0.2.2",
    "maintainer": "Kyle Nickodem <kyle.nickodem@gmail.com>",
    "author": "Kyle Nickodem [aut, cre] and Peter Halpin [aut]",
    "url": "https://github.com/knickodem/kfa",
    "bug_reports": "https://github.com/knickodem/kfa/issues",
    "repository": "https://cran.r-project.org/package=kfa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kfa K-Fold Cross Validation for Factor Analysis Provides functions to identify plausible and replicable factor\n  structures for a set of variables via k-fold cross validation. The process\n  combines the exploratory and confirmatory factor analytic approach to scale\n  development (Flora & Flake, 2017) <doi:10.1037/cbs0000069> with a cross validation\n  technique that maximizes the available data (Hastie, Tibshirani, & Friedman, 2009)\n  <isbn:978-0-387-21606-5>. Also available are functions to determine k by drawing \n  on power analytic techniques for covariance structures (MacCallum, Browne, &\n  Sugawara, 1996) <doi:10.1037/1082-989X.1.2.130>, generate model syntax, and\n  summarize results in a report.  "
  },
  {
    "id": 14866,
    "package_name": "kfigr",
    "title": "Integrated Code Chunk Anchoring and Referencing for R Markdown\nDocuments",
    "description": "A streamlined cross-referencing system for R Markdown documents\n    generated with 'knitr'. R Markdown is  an authoring format for generating\n    dynamic content from R. 'kfigr' provides a hook for anchoring code\n    chunks and a function to cross-reference document elements generated from\n    said chunks, e.g. figures and tables.",
    "version": "1.2.1",
    "maintainer": "Michael C Koohafkan <michael.koohafkan@gmail.com>",
    "author": "Michael C Koohafkan [aut, cre]",
    "url": "https://github.com/mkoohafkan/kfigr",
    "bug_reports": "https://github.com/mkoohafkan/kfigr/issues",
    "repository": "https://cran.r-project.org/package=kfigr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kfigr Integrated Code Chunk Anchoring and Referencing for R Markdown\nDocuments A streamlined cross-referencing system for R Markdown documents\n    generated with 'knitr'. R Markdown is  an authoring format for generating\n    dynamic content from R. 'kfigr' provides a hook for anchoring code\n    chunks and a function to cross-reference document elements generated from\n    said chunks, e.g. figures and tables.  "
  },
  {
    "id": 14878,
    "package_name": "kidsides",
    "title": "Download, Cache, and Connect to KidSIDES",
    "description": "Caches and then connects to a 'sqlite' database containing half a million pediatric drug safety signals. \n    The database is part of a family of resources catalogued at <https://nsides.io>. The database\n    contains 17 tables where the description table provides a map between the fields the field's details. \n    The database was created by Nicholas Giangreco during his PhD thesis which you can read in Giangreco (2022) <doi:10.7916/d8-5d9b-6738>. \n    The observations are from the Food and Drug Administration's Adverse Event Reporting System. Generalized additive models estimated\n    drug effects across child development stages for the occurrence of an adverse event when exposed to a drug compared to other drugs.\n    Read more at the methods detailed in Giangreco (2022) <doi:10.1016/j.medj.2022.06.001>. ",
    "version": "0.5.0",
    "maintainer": "Nicholas Giangreco <nick.giangreco@gmail.com>",
    "author": "Nicholas Giangreco [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-8138-4947>)",
    "url": "https://github.com/ngiangre/kidsides,\nhttps://ngiangre.github.io/kidsides/, https://nsides.io",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kidsides",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kidsides Download, Cache, and Connect to KidSIDES Caches and then connects to a 'sqlite' database containing half a million pediatric drug safety signals. \n    The database is part of a family of resources catalogued at <https://nsides.io>. The database\n    contains 17 tables where the description table provides a map between the fields the field's details. \n    The database was created by Nicholas Giangreco during his PhD thesis which you can read in Giangreco (2022) <doi:10.7916/d8-5d9b-6738>. \n    The observations are from the Food and Drug Administration's Adverse Event Reporting System. Generalized additive models estimated\n    drug effects across child development stages for the occurrence of an adverse event when exposed to a drug compared to other drugs.\n    Read more at the methods detailed in Giangreco (2022) <doi:10.1016/j.medj.2022.06.001>.   "
  },
  {
    "id": 14917,
    "package_name": "knitLatex",
    "title": "'Knitr' Helpers - Mostly Tables",
    "description": "Provides several helper functions for working with 'knitr' and 'LaTeX'.\n  It includes 'xTab' for creating traditional 'LaTeX' tables, 'lTab' for generating\n  'longtable' environments, and 'sTab' for generating a 'supertabular' environment.\n  Additionally, this package contains a knitr_setup() function which fixes a\n  well-known bug in 'knitr', which distorts the 'results=\"asis\"' command when used\n  in conjunction with user-defined commands; and a com command (<<com=TRUE>>=)\n  which renders the output from 'knitr' as a 'LaTeX' command.",
    "version": "0.9.0",
    "maintainer": "John Shea <coachshea@fastmail.com>",
    "author": "John Shea [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=knitLatex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knitLatex 'Knitr' Helpers - Mostly Tables Provides several helper functions for working with 'knitr' and 'LaTeX'.\n  It includes 'xTab' for creating traditional 'LaTeX' tables, 'lTab' for generating\n  'longtable' environments, and 'sTab' for generating a 'supertabular' environment.\n  Additionally, this package contains a knitr_setup() function which fixes a\n  well-known bug in 'knitr', which distorts the 'results=\"asis\"' command when used\n  in conjunction with user-defined commands; and a com command (<<com=TRUE>>=)\n  which renders the output from 'knitr' as a 'LaTeX' command.  "
  },
  {
    "id": 14918,
    "package_name": "knitcitations",
    "title": "Citations for 'Knitr' Markdown Files",
    "description": "Provides the ability to create dynamic citations\n    in which the bibliographic information is pulled from the web rather\n    than having to be entered into a local database such as 'bibtex' ahead of\n    time. The package is primarily aimed at authoring in the R 'markdown'\n    format, and can provide outputs for web-based authoring such as linked\n    text for inline citations.  Cite using a 'DOI', URL, or\n    'bibtex' file key.  See the package URL for details.",
    "version": "1.0.12",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>)",
    "url": "https://github.com/cboettig/knitcitations",
    "bug_reports": "https://github.com/cboettig/knitcitations/issues",
    "repository": "https://cran.r-project.org/package=knitcitations",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knitcitations Citations for 'Knitr' Markdown Files Provides the ability to create dynamic citations\n    in which the bibliographic information is pulled from the web rather\n    than having to be entered into a local database such as 'bibtex' ahead of\n    time. The package is primarily aimed at authoring in the R 'markdown'\n    format, and can provide outputs for web-based authoring such as linked\n    text for inline citations.  Cite using a 'DOI', URL, or\n    'bibtex' file key.  See the package URL for details.  "
  },
  {
    "id": 14919,
    "package_name": "knitr",
    "title": "A General-Purpose Package for Dynamic Report Generation in R",
    "description": "Provides a general-purpose tool for dynamic report generation in R\n    using Literate Programming techniques.",
    "version": "1.50",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>,\n    URL: https://yihui.org),\n  Abhraneel Sarma [ctb],\n  Adam Vogt [ctb],\n  Alastair Andrew [ctb],\n  Alex Zvoleff [ctb],\n  Amar Al-Zubaidi [ctb],\n  Andre Simon [ctb] (the CSS files under inst/themes/ were derived from\n    the Highlight package http://www.andre-simon.de),\n  Aron Atkins [ctb],\n  Aaron Wolen [ctb],\n  Ashley Manton [ctb],\n  Atsushi Yasumoto [ctb] (ORCID: <https://orcid.org/0000-0002-8335-495X>),\n  Ben Baumer [ctb],\n  Brian Diggs [ctb],\n  Brian Zhang [ctb],\n  Bulat Yapparov [ctb],\n  Cassio Pereira [ctb],\n  Christophe Dervieux [ctb],\n  David Hall [ctb],\n  David Hugh-Jones [ctb],\n  David Robinson [ctb],\n  Doug Hemken [ctb],\n  Duncan Murdoch [ctb],\n  Elio Campitelli [ctb],\n  Ellis Hughes [ctb],\n  Emily Riederer [ctb],\n  Fabian Hirschmann [ctb],\n  Fitch Simeon [ctb],\n  Forest Fang [ctb],\n  Frank E Harrell Jr [ctb] (the Sweavel package at inst/misc/Sweavel.sty),\n  Garrick Aden-Buie [ctb],\n  Gregoire Detrez [ctb],\n  Hadley Wickham [ctb],\n  Hao Zhu [ctb],\n  Heewon Jeon [ctb],\n  Henrik Bengtsson [ctb],\n  Hiroaki Yutani [ctb],\n  Ian Lyttle [ctb],\n  Hodges Daniel [ctb],\n  Jacob Bien [ctb],\n  Jake Burkhead [ctb],\n  James Manton [ctb],\n  Jared Lander [ctb],\n  Jason Punyon [ctb],\n  Javier Luraschi [ctb],\n  Jeff Arnold [ctb],\n  Jenny Bryan [ctb],\n  Jeremy Ashkenas [ctb, cph] (the CSS file at\n    inst/misc/docco-classic.css),\n  Jeremy Stephens [ctb],\n  Jim Hester [ctb],\n  Joe Cheng [ctb],\n  Johannes Ranke [ctb],\n  John Honaker [ctb],\n  John Muschelli [ctb],\n  Jonathan Keane [ctb],\n  JJ Allaire [ctb],\n  Johan Toloe [ctb],\n  Jonathan Sidi [ctb],\n  Joseph Larmarange [ctb],\n  Julien Barnier [ctb],\n  Kaiyin Zhong [ctb],\n  Kamil Slowikowski [ctb],\n  Karl Forner [ctb],\n  Kevin K. Smith [ctb],\n  Kirill Mueller [ctb],\n  Kohske Takahashi [ctb],\n  Lorenz Walthert [ctb],\n  Lucas Gallindo [ctb],\n  Marius Hofert [ctb],\n  Martin Modr\u00e1k [ctb],\n  Michael Chirico [ctb],\n  Michael Friendly [ctb],\n  Michal Bojanowski [ctb],\n  Michel Kuhlmann [ctb],\n  Miller Patrick [ctb],\n  Nacho Caballero [ctb],\n  Nick Salkowski [ctb],\n  Niels Richard Hansen [ctb],\n  Noam Ross [ctb],\n  Obada Mahdi [ctb],\n  Pavel N. Krivitsky [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9101-3362>),\n  Pedro Faria [ctb],\n  Qiang Li [ctb],\n  Ramnath Vaidyanathan [ctb],\n  Richard Cotton [ctb],\n  Robert Krzyzanowski [ctb],\n  Rodrigo Copetti [ctb],\n  Romain Francois [ctb],\n  Ruaridh Williamson [ctb],\n  Sagiru Mati [ctb] (ORCID: <https://orcid.org/0000-0003-1413-3974>),\n  Scott Kostyshak [ctb],\n  Sebastian Meyer [ctb],\n  Sietse Brouwer [ctb],\n  Simon de Bernard [ctb],\n  Sylvain Rousseau [ctb],\n  Taiyun Wei [ctb],\n  Thibaut Assus [ctb],\n  Thibaut Lamadon [ctb],\n  Thomas Leeper [ctb],\n  Tim Mastny [ctb],\n  Tom Torsney-Weir [ctb],\n  Trevor Davis [ctb],\n  Viktoras Veitas [ctb],\n  Weicheng Zhu [ctb],\n  Wush Wu [ctb],\n  Zachary Foster [ctb],\n  Zhian N. Kamvar [ctb] (ORCID: <https://orcid.org/0000-0003-1458-7108>),\n  Posit Software, PBC [cph, fnd]",
    "url": "https://yihui.org/knitr/",
    "bug_reports": "https://github.com/yihui/knitr/issues",
    "repository": "https://cran.r-project.org/package=knitr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knitr A General-Purpose Package for Dynamic Report Generation in R Provides a general-purpose tool for dynamic report generation in R\n    using Literate Programming techniques.  "
  },
  {
    "id": 14920,
    "package_name": "knitrBootstrap",
    "title": "'knitr' Bootstrap Framework",
    "description": "A framework to create Bootstrap <https://getbootstrap.com/> HTML reports from 'knitr'\n    'rmarkdown'.",
    "version": "1.0.4",
    "maintainer": "Jim Hester <james.f.hester@gmail.com>",
    "author": "Jim Hester [aut, cre]",
    "url": "https://github.com/jimhester/knitrBootstrap#readme,\nhttps://github.com/jimhester/knitrBootstrap",
    "bug_reports": "https://github.com/jimhester/knitrBootstrap/issues",
    "repository": "https://cran.r-project.org/package=knitrBootstrap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knitrBootstrap 'knitr' Bootstrap Framework A framework to create Bootstrap <https://getbootstrap.com/> HTML reports from 'knitr'\n    'rmarkdown'.  "
  },
  {
    "id": 14921,
    "package_name": "knitrProgressBar",
    "title": "Provides Progress Bars in 'knitr'",
    "description": "Provides a progress bar similar to 'dplyr' that can write progress out to a \n    variety of locations, including stdout(), stderr(), or from file(). Useful when using 'knitr' or 'rmarkdown',\n    and you still want to see progress of calculations in the terminal.",
    "version": "1.1.1",
    "maintainer": "Robert M Flight <rflight79@gmail.com>",
    "author": "Robert M Flight [aut, cre],\n  Hadley Wickham [ctb] (Author of included dplyr fragments),\n  Romain Francois [ctb] (Author of included dplyr fragments),\n  Lionel Henry [ctb] (Author of included dplyr fragments),\n  Kirill M\u00fcller [ctb] (Author of included dplyr fragments),\n  RStudio [cph] (Copyright holder of included dplyr fragments)",
    "url": "https://rmflight.github.io/knitrProgressBar/",
    "bug_reports": "https://github.com/rmflight/knitrProgressBar/issues",
    "repository": "https://cran.r-project.org/package=knitrProgressBar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knitrProgressBar Provides Progress Bars in 'knitr' Provides a progress bar similar to 'dplyr' that can write progress out to a \n    variety of locations, including stdout(), stderr(), or from file(). Useful when using 'knitr' or 'rmarkdown',\n    and you still want to see progress of calculations in the terminal.  "
  },
  {
    "id": 14922,
    "package_name": "knitxl",
    "title": "Generates a Spreadsheet Report from an 'rmarkdown' File",
    "description": "Convert an R Markdown documents into an '.xlsx' spreadsheet reports\n    with the knitxl() function, which works similarly to knit() from the \n    'knitr' package. The generated report can be opened in 'Excel' or similar\n    software for further analysis and presentation.",
    "version": "0.1.0",
    "maintainer": "Denis Dreano <denis.dreano@protonmail.ch>",
    "author": "Denis Dreano [cre, aut, cph]",
    "url": "https://github.com/dreanod/knitxl",
    "bug_reports": "https://github.com/dreanod/knitxl/issues",
    "repository": "https://cran.r-project.org/package=knitxl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knitxl Generates a Spreadsheet Report from an 'rmarkdown' File Convert an R Markdown documents into an '.xlsx' spreadsheet reports\n    with the knitxl() function, which works similarly to knit() from the \n    'knitr' package. The generated report can be opened in 'Excel' or similar\n    software for further analysis and presentation.  "
  },
  {
    "id": 14931,
    "package_name": "koRpus",
    "title": "Text Analysis with Emphasis on POS Tagging, Readability, and\nLexical Diversity",
    "description": "A set of tools to analyze texts. Includes, amongst others, functions for\n          automatic language detection, hyphenation, several indices of lexical diversity\n          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,\n          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also\n          provided, to enable frequency analyses (supports Celex and Leipzig Corpora\n          Collection file formats) and measures like tf-idf. Note: For full functionality\n          a local installation of TreeTagger is recommended. It is also recommended to\n          not load this package directly, but by loading one of the available language\n          support packages from the 'l10n' repository\n          <https://undocumeantit.github.io/repos/l10n/>. 'koRpus' also includes a plugin\n          for the R GUI and IDE RKWard, providing graphical dialogs for its basic\n          features. The respective R package 'rkward' cannot be installed directly from a\n          repository, as it is a part of RKWard. To make full use of this feature, please\n          install RKWard from <https://rkward.kde.org> (plugins are detected\n          automatically). Due to some restrictions on CRAN, the full package sources are\n          only available from the project homepage. To ask for help, report bugs, request\n          features, or discuss the development of the package, please subscribe to the\n          koRpus-dev mailing list (<https://korpusml.reaktanz.de>).",
    "version": "0.13-8",
    "maintainer": "Meik Michalke <meik.michalke@hhu.de>",
    "author": "Meik Michalke [aut, cre],\n  Earl Brown [ctb],\n  Alberto Mirisola [ctb],\n  Alexandre Brulet [ctb],\n  Laura Hauser [ctb]",
    "url": "https://reaktanz.de/?c=hacking&s=koRpus",
    "bug_reports": "https://github.com/unDocUMeantIt/koRpus/issues",
    "repository": "https://cran.r-project.org/package=koRpus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "koRpus Text Analysis with Emphasis on POS Tagging, Readability, and\nLexical Diversity A set of tools to analyze texts. Includes, amongst others, functions for\n          automatic language detection, hyphenation, several indices of lexical diversity\n          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,\n          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also\n          provided, to enable frequency analyses (supports Celex and Leipzig Corpora\n          Collection file formats) and measures like tf-idf. Note: For full functionality\n          a local installation of TreeTagger is recommended. It is also recommended to\n          not load this package directly, but by loading one of the available language\n          support packages from the 'l10n' repository\n          <https://undocumeantit.github.io/repos/l10n/>. 'koRpus' also includes a plugin\n          for the R GUI and IDE RKWard, providing graphical dialogs for its basic\n          features. The respective R package 'rkward' cannot be installed directly from a\n          repository, as it is a part of RKWard. To make full use of this feature, please\n          install RKWard from <https://rkward.kde.org> (plugins are detected\n          automatically). Due to some restrictions on CRAN, the full package sources are\n          only available from the project homepage. To ask for help, report bugs, request\n          features, or discuss the development of the package, please subscribe to the\n          koRpus-dev mailing list (<https://korpusml.reaktanz.de>).  "
  },
  {
    "id": 14932,
    "package_name": "koRpus.lang.en",
    "title": "Language Support for 'koRpus' Package: English",
    "description": "Adds support for the English language to the 'koRpus' package. To ask for help, report bugs, suggest feature improvements, or discuss the global development of the\n                    package, please consider subscribing to the koRpus-dev mailing list (<https://korpusml.reaktanz.de>).",
    "version": "0.1-4",
    "maintainer": "Meik Michalke <meik.michalke@hhu.de>",
    "author": "Meik Michalke [aut, cre],\n  Elen Le Foll [ctb] (BNC tagset)",
    "url": "https://reaktanz.de/?c=hacking&s=koRpus",
    "bug_reports": "https://github.com/unDocUMeantIt/koRpus.lang.en/issues",
    "repository": "https://cran.r-project.org/package=koRpus.lang.en",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "koRpus.lang.en Language Support for 'koRpus' Package: English Adds support for the English language to the 'koRpus' package. To ask for help, report bugs, suggest feature improvements, or discuss the global development of the\n                    package, please consider subscribing to the koRpus-dev mailing list (<https://korpusml.reaktanz.de>).  "
  },
  {
    "id": 14937,
    "package_name": "kollaR",
    "title": "Event Classification, Visualization and Analysis of Eye Tracking\nData",
    "description": "Functions for analysing eye tracking data, including event detection, visualizations and area of interest (AOI) based analyses. \n  The package includes implementations of the IV-T, I-DT, adaptive velocity threshold, and Identification by two means clustering (I2MC) algorithms.\n  See separate documentation for each function. The principles underlying I-VT and I-DT algorithms are described in   Salvucci & Goldberg (2000,\\doi{10.1145/355017.355028}). \n  Two-means clustering is described in Hessels et al. (2017, \\doi{10.3758/s13428-016-0822-1}). \n  The adaptive velocity threshold algorithm is described in Nystr\u00f6m & Holmqvist (2010,\\doi{10.3758/BRM.42.1.188}).\n  See a demonstration in the URL.",
    "version": "1.1.2",
    "maintainer": "Johan Lundin Kleberg <johan.lundin.kleberg@su.se>",
    "author": "Johan Lundin Kleberg [aut, cre]",
    "url": "https://drjohanlk.github.io/kollaR/demo.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kollaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kollaR Event Classification, Visualization and Analysis of Eye Tracking\nData Functions for analysing eye tracking data, including event detection, visualizations and area of interest (AOI) based analyses. \n  The package includes implementations of the IV-T, I-DT, adaptive velocity threshold, and Identification by two means clustering (I2MC) algorithms.\n  See separate documentation for each function. The principles underlying I-VT and I-DT algorithms are described in   Salvucci & Goldberg (2000,\\doi{10.1145/355017.355028}). \n  Two-means clustering is described in Hessels et al. (2017, \\doi{10.3758/s13428-016-0822-1}). \n  The adaptive velocity threshold algorithm is described in Nystr\u00f6m & Holmqvist (2010,\\doi{10.3758/BRM.42.1.188}).\n  See a demonstration in the URL.  "
  },
  {
    "id": 14938,
    "package_name": "komaletter",
    "title": "Simply Beautiful PDF Letters from Markdown",
    "description": "Write beautiful yet customizable letters in R Markdown and\n  directly obtain the finished PDF. Smooth generation of PDFs is realized by\n  'rmarkdown', the 'pandoc-letter' template and the 'KOMA-Script' letter class.\n  'KOMA-Script' provides enhanced replacements for the standard 'LaTeX' classes\n  with emphasis on typography and versatility. 'KOMA-Script' is particularly\n  useful for international writers as it handles various paper formats well, \n  provides layouts for many common window envelope types (e.g. German, US, \n  French, Japanese) and lets you define your own layouts. The package comes \n  with a default letter layout based on 'DIN 5008B'.",
    "version": "0.5.0",
    "maintainer": "Robert Nuske <robert.nuske@mailbox.org>",
    "author": "Robert Nuske [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9773-2061>),\n  Dirk Eddelbuettel [aut] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Aaron Wolen [aut] (ORCID: <https://orcid.org/0000-0003-2542-2202>)",
    "url": "https://rnuske.github.io/komaletter/,\nhttps://github.com/rnuske/komaletter",
    "bug_reports": "https://github.com/rnuske/komaletter/issues",
    "repository": "https://cran.r-project.org/package=komaletter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "komaletter Simply Beautiful PDF Letters from Markdown Write beautiful yet customizable letters in R Markdown and\n  directly obtain the finished PDF. Smooth generation of PDFs is realized by\n  'rmarkdown', the 'pandoc-letter' template and the 'KOMA-Script' letter class.\n  'KOMA-Script' provides enhanced replacements for the standard 'LaTeX' classes\n  with emphasis on typography and versatility. 'KOMA-Script' is particularly\n  useful for international writers as it handles various paper formats well, \n  provides layouts for many common window envelope types (e.g. German, US, \n  French, Japanese) and lets you define your own layouts. The package comes \n  with a default letter layout based on 'DIN 5008B'.  "
  },
  {
    "id": 14944,
    "package_name": "kpiwidget",
    "title": "KPI Widgets for Quarto Dashboards with Crosstalk",
    "description": "Provides an easy way to create interactive KPI (key performance indicator) widgets for 'Quarto' dashboards using 'Crosstalk'. The package enables visualization of key metrics in a structured format, supporting interactive filtering and linking with other 'Crosstalk'-enabled components. Designed for use in 'Quarto' Dashboards.",
    "version": "0.1.1",
    "maintainer": "Arnold Kakas <kakasarnold@gmail.com>",
    "author": "Arnold Kakas [aut, cre, cph]",
    "url": "https://arnold-kakas.github.io/kpiwidget/,\nhttps://github.com/Arnold-Kakas/kpiwidget",
    "bug_reports": "https://github.com/Arnold-Kakas/kpiwidget/issues",
    "repository": "https://cran.r-project.org/package=kpiwidget",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kpiwidget KPI Widgets for Quarto Dashboards with Crosstalk Provides an easy way to create interactive KPI (key performance indicator) widgets for 'Quarto' dashboards using 'Crosstalk'. The package enables visualization of key metrics in a structured format, supporting interactive filtering and linking with other 'Crosstalk'-enabled components. Designed for use in 'Quarto' Dashboards.  "
  },
  {
    "id": 14967,
    "package_name": "kuzuR",
    "title": "Interface to 'kuzu' Graph Database",
    "description": "Provides a high-performance 'R' interface to the 'kuzu' graph database.\n    It uses the 'reticulate' package to wrap the official 'Python' client\n    ('kuzu', 'pandas', and 'networkx'), allowing users to interact with 'kuzu'\n    seamlessly from within 'R'. Key features include managing database\n    connections, executing 'Cypher' queries, and efficiently loading data from\n    'R' data frames. It also provides seamless integration with the 'R' ecosystem\n    by converting query results directly into popular 'R' data structures,\n    including 'tibble', 'igraph', 'tidygraph', and 'g6R' objects, making\n    'kuzu's powerful graph computation capabilities readily available for data\n    analysis and visualization workflows in 'R'.\n    The 'kuzu' documentation can be found at <https://kuzudb.github.io/docs/>.",
    "version": "0.2.3",
    "maintainer": "Manuel Wick-Eckl <manuel.wick@gmail.com>",
    "author": "Manuel Wick-Eckl [aut, cre]",
    "url": "https://github.com/WickM/kuzuR, https://wickm.github.io/kuzuR/",
    "bug_reports": "https://github.com/WickM/kuzuR/issues",
    "repository": "https://cran.r-project.org/package=kuzuR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kuzuR Interface to 'kuzu' Graph Database Provides a high-performance 'R' interface to the 'kuzu' graph database.\n    It uses the 'reticulate' package to wrap the official 'Python' client\n    ('kuzu', 'pandas', and 'networkx'), allowing users to interact with 'kuzu'\n    seamlessly from within 'R'. Key features include managing database\n    connections, executing 'Cypher' queries, and efficiently loading data from\n    'R' data frames. It also provides seamless integration with the 'R' ecosystem\n    by converting query results directly into popular 'R' data structures,\n    including 'tibble', 'igraph', 'tidygraph', and 'g6R' objects, making\n    'kuzu's powerful graph computation capabilities readily available for data\n    analysis and visualization workflows in 'R'.\n    The 'kuzu' documentation can be found at <https://kuzudb.github.io/docs/>.  "
  },
  {
    "id": 14970,
    "package_name": "kyotil",
    "title": "Utility Functions for Statistical Analysis Report Generation and\nMonte Carlo Studies",
    "description": "Helper functions for creating formatted summary of regression models, writing publication-ready tables to latex files, and running Monte Carlo experiments.",
    "version": "2024.11-01",
    "maintainer": "Youyi Fong <youyifong@gmail.com>",
    "author": "Youyi Fong [cre],\n  Krisztian Sebestyen [aut],\n  Han Sunwoo [aut],\n  Jason Becker [ctb],\n  Bendix Carstensen [ctb],\n  Daryl Morris [ctb],\n  Josh Pasek [ctb],\n  Dennis Chao [ctb],\n  Andri Signorell [ctb],\n  Sue Li [ctb],\n  Jonathan Bartlett [ctb],\n  Christophe Dutang [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kyotil",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kyotil Utility Functions for Statistical Analysis Report Generation and\nMonte Carlo Studies Helper functions for creating formatted summary of regression models, writing publication-ready tables to latex files, and running Monte Carlo experiments.  "
  },
  {
    "id": 14985,
    "package_name": "labeleR",
    "title": "Automate the Production of Custom Labels, Badges, Certificates,\nand Other Documents",
    "description": "Create custom labels, badges, certificates\n  and other documents. Automate the production of potentially large \n  numbers of herbarium and collection labels, accreditation badges, \n  attendance and participation certificates, etc, and deliver them automatically.\n  Documents are generated in PDF format, which requires a working \n  installation of 'LaTeX', such as 'TinyTeX'.",
    "version": "0.4.0",
    "maintainer": "Ignacio Ramos-Gutierrez <ig.ramosgutierrez@gmail.com>",
    "author": "Ignacio Ramos-Gutierrez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8675-0114>),\n  Julia G. de Aledo [aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-9065-9316>),\n  Jimena Mateo-Mart\u00edn [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-9757-5497>),\n  Francisco Rodriguez-Sanchez [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-7981-1599>),\n  Giorgio Comai [ctb]",
    "url": "https://github.com/EcologyR/labeleR/,\nhttps://ecologyr.github.io/labeleR/",
    "bug_reports": "https://github.com/EcologyR/labeleR/issues/",
    "repository": "https://cran.r-project.org/package=labeleR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "labeleR Automate the Production of Custom Labels, Badges, Certificates,\nand Other Documents Create custom labels, badges, certificates\n  and other documents. Automate the production of potentially large \n  numbers of herbarium and collection labels, accreditation badges, \n  attendance and participation certificates, etc, and deliver them automatically.\n  Documents are generated in PDF format, which requires a working \n  installation of 'LaTeX', such as 'TinyTeX'.  "
  },
  {
    "id": 14999,
    "package_name": "ladder",
    "title": "Get on to the Slides",
    "description": "Create tables from within R directly on Google Slides\n    presentations.  Currently supports matrix, data.frame and 'flextable'\n    objects.",
    "version": "0.0.3",
    "maintainer": "Isaac Gravestock <isaac.gravestock@gmail.com>",
    "author": "Isaac Gravestock [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0283-2065>)",
    "url": "https://www.r-ladder.com",
    "bug_reports": "https://github.com/igrave/ladder/issues",
    "repository": "https://cran.r-project.org/package=ladder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ladder Get on to the Slides Create tables from within R directly on Google Slides\n    presentations.  Currently supports matrix, data.frame and 'flextable'\n    objects.  "
  },
  {
    "id": 15021,
    "package_name": "lang",
    "title": "Translates R Help Documentation using Large Language Models",
    "description": "Translates R help documentation on the fly by using a Large \n    Language model of your choice. If you are using 'RStudio' or 'Positron'\n    the translated help will appear in the help pane. ",
    "version": "0.1.0",
    "maintainer": "Edgar Ruiz <edgar@posit.co>",
    "author": "Edgar Ruiz [aut, cre]",
    "url": "https://mlverse.github.io/lang/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lang",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lang Translates R Help Documentation using Large Language Models Translates R help documentation on the fly by using a Large \n    Language model of your choice. If you are using 'RStudio' or 'Positron'\n    the translated help will appear in the help pane.   "
  },
  {
    "id": 15022,
    "package_name": "langevitour",
    "title": "Langevin Tour",
    "description": "\n    An HTML widget that randomly tours 2D projections of numerical data. A random walk through projections of the data is shown. The user can manipulate the plot to use specified axes, or turn on Guided Tour mode to find an informative projection of the data. Groups within the data can be hidden or shown, as can particular axes. Points can be brushed, and the selection can be linked to other widgets using crosstalk. The underlying method to produce the random walk and projection pursuit uses Langevin dynamics. The widget can be used from within R, or included in a self-contained R Markdown or Quarto document or presentation, or used in a Shiny app.",
    "version": "0.8.1",
    "maintainer": "Paul Harrison <pfh@logarithmic.net>",
    "author": "Paul Harrison [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3980-268X>)",
    "url": "https://logarithmic.net/langevitour/",
    "bug_reports": "https://github.com/pfh/langevitour/issues/",
    "repository": "https://cran.r-project.org/package=langevitour",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "langevitour Langevin Tour \n    An HTML widget that randomly tours 2D projections of numerical data. A random walk through projections of the data is shown. The user can manipulate the plot to use specified axes, or turn on Guided Tour mode to find an informative projection of the data. Groups within the data can be hidden or shown, as can particular axes. Points can be brushed, and the selection can be linked to other widgets using crosstalk. The underlying method to produce the random walk and projection pursuit uses Langevin dynamics. The widget can be used from within R, or included in a self-contained R Markdown or Quarto document or presentation, or used in a Shiny app.  "
  },
  {
    "id": 15032,
    "package_name": "latbias",
    "title": "Calculate the Latitudinal Bias Index",
    "description": "Studies that report shifts in species distributions may be biased\n  by the shape of the study area. The main functionality of this package is to\n  calculate the Latitudinal Bias Index (LBI) for any given shape. The LBI is\n  bounded between +1 (100% probability to exclusively record latitudinal\n  shifts, i.e., range shifts data sampled along a perfectly South-North\n  oriented straight line) and -1 (100% probability to exclusively record\n  longitudinal shifts, i.e., range shifts data sampled along a perfectly\n  East-West oriented straight line).",
    "version": "1.0.0",
    "maintainer": "Pierre Denelle <pierre.denelle@gmail.com>",
    "author": "Pierre Denelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5037-2281>),\n  Pieter Sanczuk [aut] (ORCID: <https://orcid.org/0000-0003-1107-4905>)",
    "url": "https://github.com/pierredenelle/latbias,",
    "bug_reports": "https://github.com/pierredenelle/latbias/issues",
    "repository": "https://cran.r-project.org/package=latbias",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "latbias Calculate the Latitudinal Bias Index Studies that report shifts in species distributions may be biased\n  by the shape of the study area. The main functionality of this package is to\n  calculate the Latitudinal Bias Index (LBI) for any given shape. The LBI is\n  bounded between +1 (100% probability to exclusively record latitudinal\n  shifts, i.e., range shifts data sampled along a perfectly South-North\n  oriented straight line) and -1 (100% probability to exclusively record\n  longitudinal shifts, i.e., range shifts data sampled along a perfectly\n  East-West oriented straight line).  "
  },
  {
    "id": 15043,
    "package_name": "latexdiffr",
    "title": "Diff TeX, 'rmarkdown' or 'quarto' Files Using the 'latexdiff'\nUtility",
    "description": "Produces a PDF diff of two 'rmarkdown', 'quarto', Sweave or TeX \n  files, using the external 'latexdiff' utility.",
    "version": "0.2.0",
    "maintainer": "David Hugh-Jones <davidhughjones@gmail.com>",
    "author": "David Hugh-Jones [aut, cre]",
    "url": "https://github.com/hughjonesd/latexdiffr",
    "bug_reports": "https://github.com/hughjonesd/latexdiffr/issues",
    "repository": "https://cran.r-project.org/package=latexdiffr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "latexdiffr Diff TeX, 'rmarkdown' or 'quarto' Files Using the 'latexdiff'\nUtility Produces a PDF diff of two 'rmarkdown', 'quarto', Sweave or TeX \n  files, using the external 'latexdiff' utility.  "
  },
  {
    "id": 15044,
    "package_name": "latexpdf",
    "title": "Convert Tables to PDF or PNG",
    "description": "Converts table-like objects to stand-alone PDF or PNG.\n Can be used to embed tables and arbitrary content in PDF or Word\n documents. Provides a low-level R interface for creating 'LaTeX'\n code, e.g. command() and a high-level interface for creating PDF\n documents, e.g. as.pdf.data.frame(). Extensive customization is\n available via mid-level functions, e.g. as.tabular(). See also \n 'package?latexpdf'. Support for PNG is experimental; see\n 'as.png.data.frame'. Adapted from 'metrumrg' \n <https://r-forge.r-project.org/R/?group_id=1215>.\n Requires a compatible installation of 'pdflatex',\n e.g. <https://miktex.org/>.",
    "version": "0.1.8",
    "maintainer": "Tim Bergsma <bergsmat@gmail.com>",
    "author": "Tim Bergsma",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=latexpdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "latexpdf Convert Tables to PDF or PNG Converts table-like objects to stand-alone PDF or PNG.\n Can be used to embed tables and arbitrary content in PDF or Word\n documents. Provides a low-level R interface for creating 'LaTeX'\n code, e.g. command() and a high-level interface for creating PDF\n documents, e.g. as.pdf.data.frame(). Extensive customization is\n available via mid-level functions, e.g. as.tabular(). See also \n 'package?latexpdf'. Support for PNG is experimental; see\n 'as.png.data.frame'. Adapted from 'metrumrg' \n <https://r-forge.r-project.org/R/?group_id=1215>.\n Requires a compatible installation of 'pdflatex',\n e.g. <https://miktex.org/>.  "
  },
  {
    "id": 15087,
    "package_name": "lcra",
    "title": "Bayesian Joint Latent Class and Regression Models",
    "description": "For fitting Bayesian joint latent class and regression models using\n    Gibbs sampling. See the documentation for the model.\n    The technical details of the model implemented here are described in Elliott,\n    Michael R., Zhao, Zhangchen, Mukherjee, Bhramar, Kanaya, Alka, Needham,\n    Belinda L., \"Methods to account for uncertainty in latent class assignments when\n    using latent classes as predictors in regression models, with application to\n    acculturation strategy measures\" (2020) In press at Epidemiology\n    <doi:10.1097/EDE.0000000000001139>.",
    "version": "1.1.5",
    "maintainer": "Michael Kleinsasser <biostat-cran-manager@umich.edu>",
    "author": "Michael Elliot [aut],\n  Zhangchen Zhao [aut],\n  Michael Kleinsasser [aut, cre]",
    "url": "https://github.com/umich-biostatistics/lcra",
    "bug_reports": "https://github.com/umich-biostatistics/lcra/issues",
    "repository": "https://cran.r-project.org/package=lcra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lcra Bayesian Joint Latent Class and Regression Models For fitting Bayesian joint latent class and regression models using\n    Gibbs sampling. See the documentation for the model.\n    The technical details of the model implemented here are described in Elliott,\n    Michael R., Zhao, Zhangchen, Mukherjee, Bhramar, Kanaya, Alka, Needham,\n    Belinda L., \"Methods to account for uncertainty in latent class assignments when\n    using latent classes as predictors in regression models, with application to\n    acculturation strategy measures\" (2020) In press at Epidemiology\n    <doi:10.1097/EDE.0000000000001139>.  "
  },
  {
    "id": 15097,
    "package_name": "ldmppr",
    "title": "Estimate and Simulate from Location Dependent Marked Point\nProcesses",
    "description": "A suite of tools for estimating, assessing model fit, simulating from, and visualizing location dependent marked point processes characterized by regularity in the pattern.\n    You provide a reference marked point process, a set of raster images containing location specific covariates, and select the estimation algorithm and type of mark model.\n    'ldmppr' estimates the process and mark models and allows you to check the appropriateness of the model using a variety of diagnostic tools.\n    Once a satisfactory model fit is obtained, you can simulate from the model and visualize the results.\n    Documentation for the package 'ldmppr' is available in the form of a vignette.",
    "version": "1.0.4",
    "maintainer": "Lane Drew <lanetdrew@gmail.com>",
    "author": "Lane Drew [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0006-5427-4092>),\n  Andee Kaplan [aut] (ORCID: <https://orcid.org/0000-0002-2940-889X>)",
    "url": "https://github.com/lanedrew/ldmppr",
    "bug_reports": "https://github.com/lanedrew/ldmppr/issues",
    "repository": "https://cran.r-project.org/package=ldmppr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ldmppr Estimate and Simulate from Location Dependent Marked Point\nProcesses A suite of tools for estimating, assessing model fit, simulating from, and visualizing location dependent marked point processes characterized by regularity in the pattern.\n    You provide a reference marked point process, a set of raster images containing location specific covariates, and select the estimation algorithm and type of mark model.\n    'ldmppr' estimates the process and mark models and allows you to check the appropriateness of the model using a variety of diagnostic tools.\n    Once a satisfactory model fit is obtained, you can simulate from the model and visualize the results.\n    Documentation for the package 'ldmppr' is available in the form of a vignette.  "
  },
  {
    "id": 15111,
    "package_name": "leakr",
    "title": "Data Leakage Detection Tools for Machine Learning",
    "description": "Provides utilities to detect common data leakage patterns including train/test\n           contamination, temporal leakage, and data duplication, enhancing model reliability and\n           reproducibility in machine learning workflows. Generates diagnostic reports and visual\n           summaries to support data validation. Methods based on best practices from Hastie,\n           Tibshirani, and Friedman (2009, ISBN:978-0387848570).",
    "version": "0.1.0",
    "maintainer": "Cheryl Isabella Lim <cheryl.academic@gmail.com>",
    "author": "Cheryl Isabella Lim [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=leakr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "leakr Data Leakage Detection Tools for Machine Learning Provides utilities to detect common data leakage patterns including train/test\n           contamination, temporal leakage, and data duplication, enhancing model reliability and\n           reproducibility in machine learning workflows. Generates diagnostic reports and visual\n           summaries to support data validation. Methods based on best practices from Hastie,\n           Tibshirani, and Friedman (2009, ISBN:978-0387848570).  "
  },
  {
    "id": 15136,
    "package_name": "lemon",
    "title": "Freshing Up your 'ggplot2' Plots",
    "description": "Functions for working with legends and axis lines of 'ggplot2',\n    facets that repeat axis lines on all panels, and some 'knitr' extensions.",
    "version": "0.5.2",
    "maintainer": "Stefan McKinnon Edwards <sme@iysik.com>",
    "author": "Stefan McKinnon Edwards [aut, ctb, cre] (ORCID:\n    <https://orcid.org/0000-0002-4628-8148>),\n  Baptiste Auguie [ctb] (For g_legend and grid_arrange_shared_legend),\n  Shaun Jackman [ctb] (For grid_arrange_shared_legend),\n  Hadley Wickham [ctb] (ggplot2 functions),\n  Winston Chang [ctb] (ggplot2 functions)",
    "url": "https://github.com/stefanedwards/lemon",
    "bug_reports": "https://github.com/stefanedwards/lemon/issues",
    "repository": "https://cran.r-project.org/package=lemon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lemon Freshing Up your 'ggplot2' Plots Functions for working with legends and axis lines of 'ggplot2',\n    facets that repeat axis lines on all panels, and some 'knitr' extensions.  "
  },
  {
    "id": 15188,
    "package_name": "lifeR",
    "title": "Identify Sites for Your Bird List",
    "description": "A suite of tools to use the 'eBird' database \n    (<https://ebird.org/home/>) and APIs to compare users' species lists to \n    recent observations and create a report of the top sites to visit to see \n    new species.",
    "version": "1.0.3",
    "maintainer": "Jeffrey Oliver <jcoliver@arizona.edu>",
    "author": "Jeffrey Oliver [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2160-1086>)",
    "url": "https://jcoliver.github.io/lifeR/,\nhttps://github.com/jcoliver/lifeR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lifeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lifeR Identify Sites for Your Bird List A suite of tools to use the 'eBird' database \n    (<https://ebird.org/home/>) and APIs to compare users' species lists to \n    recent observations and create a report of the top sites to visit to see \n    new species.  "
  },
  {
    "id": 15193,
    "package_name": "liftr",
    "title": "Containerize R Markdown Documents for Continuous Reproducibility",
    "description": "Persistent reproducible reporting by containerization of R Markdown documents.",
    "version": "0.9.2",
    "maintainer": "Nan Xiao <me@nanx.me>",
    "author": "Nan Xiao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0250-5673>),\n  Miao-Zhu Li [ctb],\n  Teng-Fei Yin [ctb]",
    "url": "https://nanx.me/liftr/, https://github.com/nanxstats/liftr",
    "bug_reports": "https://github.com/nanxstats/liftr/issues",
    "repository": "https://cran.r-project.org/package=liftr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "liftr Containerize R Markdown Documents for Continuous Reproducibility Persistent reproducible reporting by containerization of R Markdown documents.  "
  },
  {
    "id": 15197,
    "package_name": "lightparser",
    "title": "From 'Rmarkdown' and 'Quarto' Files to Tibble and Back",
    "description": "Split your 'rmarkdown' or 'quarto' files by sections into a\n    tibble: titles, text, chunks. Rebuild the file from the tibble.",
    "version": "0.1.0",
    "maintainer": "Sebastien Rochette <sebastien@thinkr.fr>",
    "author": "Sebastien Rochette [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1565-9313>),\n  ThinkR [cph]",
    "url": "https://github.com/ThinkR-open/lightparser,\nhttps://thinkr-open.github.io/lightparser/",
    "bug_reports": "https://github.com/ThinkR-open/lightparser/issues",
    "repository": "https://cran.r-project.org/package=lightparser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lightparser From 'Rmarkdown' and 'Quarto' Files to Tibble and Back Split your 'rmarkdown' or 'quarto' files by sections into a\n    tibble: titles, text, chunks. Rebuild the file from the tibble.  "
  },
  {
    "id": 15226,
    "package_name": "lingglosses",
    "title": "Interlinear Glossed Linguistic Examples and Abbreviation Lists\nGeneration",
    "description": "Helps to render interlinear glossed linguistic examples in html \n    'rmarkdown' documents and then semi-automatically compiles the list of\n    glosses at the end of the document. It also provides a database of linguistic\n    glosses.",
    "version": "0.0.11",
    "maintainer": "George Moroz <agricolamz@gmail.com>",
    "author": "George Moroz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1990-6083>)",
    "url": "https://CRAN.R-project.org/package=phonfieldwork,\nhttps://agricolamz.github.io/lingglosses/",
    "bug_reports": "https://github.com/agricolamz/lingglosses/issues",
    "repository": "https://cran.r-project.org/package=lingglosses",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lingglosses Interlinear Glossed Linguistic Examples and Abbreviation Lists\nGeneration Helps to render interlinear glossed linguistic examples in html \n    'rmarkdown' documents and then semi-automatically compiles the list of\n    glosses at the end of the document. It also provides a database of linguistic\n    glosses.  "
  },
  {
    "id": 15228,
    "package_name": "linguisticsdown",
    "title": "Easy Linguistics Document Writing with R Markdown",
    "description": "Provides 'Shiny gadgets' to search, type, and insert IPA symbols\n    into documents or scripts, requiring only knowledge about phonetics or \n    'X-SAMPA'. Also provides functions to facilitate the rendering of IPA\n    symbols in 'LaTeX' and PDF format, making IPA symbols properly rendered\n    in all output formats. A minimal R Markdown template for authoring \n    Linguistics related documents is also bundled with the package. Some\n    helper functions to facilitate authoring with R Markdown is also provided.",
    "version": "1.2.0",
    "maintainer": "Yongfu Liao <liao961120@gmail.com>",
    "author": "Yongfu Liao [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1814-2993>)",
    "url": "https://liao961120.github.io/linguisticsdown/,\nhttps://github.com/liao961120/linguisticsdown",
    "bug_reports": "https://github.com/liao961120/linguisticsdown/issues",
    "repository": "https://cran.r-project.org/package=linguisticsdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "linguisticsdown Easy Linguistics Document Writing with R Markdown Provides 'Shiny gadgets' to search, type, and insert IPA symbols\n    into documents or scripts, requiring only knowledge about phonetics or \n    'X-SAMPA'. Also provides functions to facilitate the rendering of IPA\n    symbols in 'LaTeX' and PDF format, making IPA symbols properly rendered\n    in all output formats. A minimal R Markdown template for authoring \n    Linguistics related documents is also bundled with the package. Some\n    helper functions to facilitate authoring with R Markdown is also provided.  "
  },
  {
    "id": 15229,
    "package_name": "link",
    "title": "Hyperlink Automatic Detection",
    "description": "Automatic detection of hyperlinks for packages and calls in the text\n    of 'rmarkdown' or 'quarto' documents.",
    "version": "2024.4.0",
    "maintainer": "Romain Fran\u00e7ois <romain@tada.science>",
    "author": "Romain Fran\u00e7ois [aut, cre],\n  tada.science [cph, fnd]",
    "url": "https://link.tada.science/, https://github.com/tadascience/link",
    "bug_reports": "https://github.com/tadascience/link/issues",
    "repository": "https://cran.r-project.org/package=link",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "link Hyperlink Automatic Detection Automatic detection of hyperlinks for packages and calls in the text\n    of 'rmarkdown' or 'quarto' documents.  "
  },
  {
    "id": 15234,
    "package_name": "linl",
    "title": "'linl' is not 'Letter'",
    "description": "A 'LaTeX' Letter class for 'rmarkdown', using the\n 'pandoc-letter' template adapted for use with 'markdown'.",
    "version": "0.0.5",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel and Aaron Wolen",
    "url": "https://github.com/eddelbuettel/linl,\nhttps://dirk.eddelbuettel.com/code/linl.html",
    "bug_reports": "https://github.com/eddelbuettel/linl/issues",
    "repository": "https://cran.r-project.org/package=linl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "linl 'linl' is not 'Letter' A 'LaTeX' Letter class for 'rmarkdown', using the\n 'pandoc-letter' template adapted for use with 'markdown'.  "
  },
  {
    "id": 15257,
    "package_name": "litedown",
    "title": "A Lightweight Version of R Markdown",
    "description": "Render R Markdown to Markdown (without using 'knitr'), and Markdown\n    to lightweight HTML or 'LaTeX' documents with the 'commonmark' package (instead\n    of 'Pandoc'). Some missing Markdown features in 'commonmark' are also\n    supported, such as raw HTML or 'LaTeX' blocks, 'LaTeX' math, superscripts,\n    subscripts, footnotes, element attributes, and appendices,\n    but not all 'Pandoc' Markdown features are (or will be) supported. With\n    additional JavaScript and CSS, you can also create HTML slides and articles.\n    This package can be viewed as a trimmed-down version of R Markdown and\n    'knitr'. It does not aim at rich Markdown features or a large variety of\n    output formats (the primary formats are HTML and 'LaTeX'). Book and website\n    projects of multiple input documents are also supported.",
    "version": "0.8",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>,\n    URL: https://yihui.org),\n  Tim Taylor [ctb] (ORCID: <https://orcid.org/0000-0002-8587-7113>)",
    "url": "https://github.com/yihui/litedown",
    "bug_reports": "https://github.com/yihui/litedown/issues",
    "repository": "https://cran.r-project.org/package=litedown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "litedown A Lightweight Version of R Markdown Render R Markdown to Markdown (without using 'knitr'), and Markdown\n    to lightweight HTML or 'LaTeX' documents with the 'commonmark' package (instead\n    of 'Pandoc'). Some missing Markdown features in 'commonmark' are also\n    supported, such as raw HTML or 'LaTeX' blocks, 'LaTeX' math, superscripts,\n    subscripts, footnotes, element attributes, and appendices,\n    but not all 'Pandoc' Markdown features are (or will be) supported. With\n    additional JavaScript and CSS, you can also create HTML slides and articles.\n    This package can be viewed as a trimmed-down version of R Markdown and\n    'knitr'. It does not aim at rich Markdown features or a large variety of\n    output formats (the primary formats are HTML and 'LaTeX'). Book and website\n    projects of multiple input documents are also supported.  "
  },
  {
    "id": 15369,
    "package_name": "logrxaddin",
    "title": "Addin for the 'logrx' Package",
    "description": "This is an extension package to 'logrx', which is a log creation \n  program focused on Clinical Reporting within the Pharma Industry.  This package \n  enables a simple 'shiny-based' Add-in that provides a point and click interface \n  to produce a log for a single program.",
    "version": "0.0.1",
    "maintainer": "Ben Straub <ben.x.straub@gsk.com>",
    "author": "Ben Straub [aut, cre]",
    "url": "https://github.com/pharmaverse/logrxaddin",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=logrxaddin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "logrxaddin Addin for the 'logrx' Package This is an extension package to 'logrx', which is a log creation \n  program focused on Clinical Reporting within the Pharma Industry.  This package \n  enables a simple 'shiny-based' Add-in that provides a point and click interface \n  to produce a log for a single program.  "
  },
  {
    "id": 15402,
    "package_name": "lorem",
    "title": "Generate Lorem Ipsum Text",
    "description": "Quickly generate lorem ipsum placeholder text. Easy to\n    integrate in RMarkdown documents. Includes an RStudio addin to insert\n    lorem ipsum into the current document.",
    "version": "1.0.0",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>)",
    "url": "https://github.com/gadenbuie/lorem,\nhttp://pkg.garrickadenbuie.com/lorem/",
    "bug_reports": "https://github.com/gadenbuie/lorem/issues",
    "repository": "https://cran.r-project.org/package=lorem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lorem Generate Lorem Ipsum Text Quickly generate lorem ipsum placeholder text. Easy to\n    integrate in RMarkdown documents. Includes an RStudio addin to insert\n    lorem ipsum into the current document.  "
  },
  {
    "id": 15417,
    "package_name": "lpdensity",
    "title": "Local Polynomial Density Estimation and Inference",
    "description": "Without imposing stringent distributional assumptions or shape restrictions, nonparametric estimation has been popular in economics and other social sciences for counterfactual analysis, program evaluation, and policy recommendations. This package implements a novel density (and derivatives) estimator based on local polynomial regressions, documented in Cattaneo, Jansson and Ma (2022) <doi:10.18637/jss.v101.i02>: lpdensity() to construct local polynomial based density (and derivatives) estimator, and lpbwdensity() to perform data-driven bandwidth selection. ",
    "version": "2.5",
    "maintainer": "Xinwei Ma <x1ma@ucsd.edu>",
    "author": "Matias D. Cattaneo [aut],\n  Michael Jansson [aut],\n  Xinwei Ma [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lpdensity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lpdensity Local Polynomial Density Estimation and Inference Without imposing stringent distributional assumptions or shape restrictions, nonparametric estimation has been popular in economics and other social sciences for counterfactual analysis, program evaluation, and policy recommendations. This package implements a novel density (and derivatives) estimator based on local polynomial regressions, documented in Cattaneo, Jansson and Ma (2022) <doi:10.18637/jss.v101.i02>: lpdensity() to construct local polynomial based density (and derivatives) estimator, and lpbwdensity() to perform data-driven bandwidth selection.   "
  },
  {
    "id": 15423,
    "package_name": "lqmm",
    "title": "Linear Quantile Mixed Models",
    "description": "Functions to fit quantile regression models for hierarchical\n    data (2-level nested designs) as described in Geraci and\n    Bottai (2014, Statistics and Computing) <doi:10.1007/s11222-013-9381-9>.\n    A vignette is given in Geraci (2014, Journal of Statistical Software)\n    <doi:10.18637/jss.v057.i13> and included in the package documents.\n    The packages also provides functions to fit quantile models for\n    independent\tdata and for count responses.",
    "version": "1.5.8",
    "maintainer": "Marco Geraci <marco.geraci@uniroma1.it>",
    "author": "Marco Geraci",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lqmm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lqmm Linear Quantile Mixed Models Functions to fit quantile regression models for hierarchical\n    data (2-level nested designs) as described in Geraci and\n    Bottai (2014, Statistics and Computing) <doi:10.1007/s11222-013-9381-9>.\n    A vignette is given in Geraci (2014, Journal of Statistical Software)\n    <doi:10.18637/jss.v057.i13> and included in the package documents.\n    The packages also provides functions to fit quantile models for\n    independent\tdata and for count responses.  "
  },
  {
    "id": 15429,
    "package_name": "lsa",
    "title": "Latent Semantic Analysis",
    "description": "The basic idea of latent semantic analysis (LSA) is, \n  that text do have a higher order (=latent semantic) structure which, \n  however, is obscured by word usage (e.g. through the use of synonyms \n  or polysemy). By using conceptual indices that are derived statistically \n  via a truncated singular value decomposition (a two-mode factor analysis) \n  over a given document-term matrix, this variability problem can be overcome. ",
    "version": "0.73.3",
    "maintainer": "Fridolin Wild <wild@brookes.ac.uk>",
    "author": "Fridolin Wild",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lsa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lsa Latent Semantic Analysis The basic idea of latent semantic analysis (LSA) is, \n  that text do have a higher order (=latent semantic) structure which, \n  however, is obscured by word usage (e.g. through the use of synonyms \n  or polysemy). By using conceptual indices that are derived statistically \n  via a truncated singular value decomposition (a two-mode factor analysis) \n  over a given document-term matrix, this variability problem can be overcome.   "
  },
  {
    "id": 15438,
    "package_name": "lsmeans",
    "title": "Least-Squares Means",
    "description": "Obtain least-squares means for linear, generalized linear, \n    and mixed models. Compute contrasts or linear functions of \n    least-squares means, and comparisons of slopes. \n    Plots and compact letter displays. Least-squares means were proposed in\n    Harvey, W (1960) \"Least-squares analysis of data with unequal subclass numbers\",\n    Tech Report ARS-20-8, USDA National Agricultural Library, and discussed\n    further in Searle, Speed, and Milliken (1980) \"Population marginal means \n    in the linear model: An alternative to least squares means\", \n    The American Statistician 34(4), 216-221 <doi:10.1080/00031305.1980.10483031>.\n    NOTE: lsmeans now relies primarily on code in the 'emmeans' package.\n    'lsmeans' will be archived in the near future.",
    "version": "2.30-2",
    "maintainer": "Russell Lenth <russell-lenth@uiowa.edu>",
    "author": "Russell Lenth [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lsmeans",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lsmeans Least-Squares Means Obtain least-squares means for linear, generalized linear, \n    and mixed models. Compute contrasts or linear functions of \n    least-squares means, and comparisons of slopes. \n    Plots and compact letter displays. Least-squares means were proposed in\n    Harvey, W (1960) \"Least-squares analysis of data with unequal subclass numbers\",\n    Tech Report ARS-20-8, USDA National Agricultural Library, and discussed\n    further in Searle, Speed, and Milliken (1980) \"Population marginal means \n    in the linear model: An alternative to least squares means\", \n    The American Statistician 34(4), 216-221 <doi:10.1080/00031305.1980.10483031>.\n    NOTE: lsmeans now relies primarily on code in the 'emmeans' package.\n    'lsmeans' will be archived in the near future.  "
  },
  {
    "id": 15445,
    "package_name": "ltable",
    "title": "Easy to Make (Lazy) Tables",
    "description": "Constructs tables of counts and proportions out of data sets with possibility to insert tables to Excel, Word, HTML, and PDF documents. Transforms tables to data suitable for modelling. Features Gibbs sampling based log-linear (NB2) and power analyses (original by Oleksandr Ocheredko <doi:10.35566/isdsa2019c5>) for tabulated data.  ",
    "version": "2.0.4",
    "maintainer": "Ocheredko Oleksandr <Ocheredko@yahoo.com>",
    "author": "Ocheredko Oleksandr",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ltable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ltable Easy to Make (Lazy) Tables Constructs tables of counts and proportions out of data sets with possibility to insert tables to Excel, Word, HTML, and PDF documents. Transforms tables to data suitable for modelling. Features Gibbs sampling based log-linear (NB2) and power analyses (original by Oleksandr Ocheredko <doi:10.35566/isdsa2019c5>) for tabulated data.    "
  },
  {
    "id": 15453,
    "package_name": "ltsk",
    "title": "Local Time Space Kriging",
    "description": "Implements local spatial and local spatiotemporal Kriging based on local spatial and local spatiotemporal variograms, respectively. The method is documented in Kumar et al (2013) <https://www.nature.com/articles/jes201352)>.",
    "version": "1.1.2",
    "maintainer": "Dong Liang <dliang@umces.edu>",
    "author": "Naresh Kumar, Dong Liang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ltsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ltsk Local Time Space Kriging Implements local spatial and local spatiotemporal Kriging based on local spatial and local spatiotemporal variograms, respectively. The method is documented in Kumar et al (2013) <https://www.nature.com/articles/jes201352)>.  "
  },
  {
    "id": 15455,
    "package_name": "ltxsparklines",
    "title": "Lightweight Sparklines for a LaTeX Document",
    "description": "Sparklines are small plots (about one line of text high),\n  made popular by Edward Tufte.  This package is the interface from R\n  to the LaTeX package sparklines by Andreas Loeffer and Dan Luecking\n  (<http://www.ctan.org/pkg/sparklines>).  It can work with Sweave or\n  knitr or other engines that produce TeX.  The package can be used to\n  plot vectors, matrices, data frames, time series (in ts or zoo format).",
    "version": "1.1.3",
    "maintainer": "Boris Veytsman <borisv@lk.net>",
    "author": "Boris Veytsman [aut, cre]",
    "url": "https://github.com/borisveytsman/ltxsparklines",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ltxsparklines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ltxsparklines Lightweight Sparklines for a LaTeX Document Sparklines are small plots (about one line of text high),\n  made popular by Edward Tufte.  This package is the interface from R\n  to the LaTeX package sparklines by Andreas Loeffer and Dan Luecking\n  (<http://www.ctan.org/pkg/sparklines>).  It can work with Sweave or\n  knitr or other engines that produce TeX.  The package can be used to\n  plot vectors, matrices, data frames, time series (in ts or zoo format).  "
  },
  {
    "id": 15484,
    "package_name": "mFD",
    "title": "Compute and Illustrate the Multiple Facets of Functional\nDiversity",
    "description": "Computing functional traits-based distances between pairs of \n    species for species gathered in assemblages allowing to build several\n    functional spaces. The package allows to compute functional diversity\n    indices assessing the distribution of species (and of their dominance) in a\n    given functional space for each assemblage and the overlap between\n    assemblages in a given functional space, see: Chao et al. (2018)\n    <doi:10.1002/ecm.1343>, Maire et al. (2015) <doi:10.1111/geb.12299>,\n    Mouillot et al. (2013) <doi:10.1016/j.tree.2012.10.004>, Mouillot et al.\n    (2014) <doi:10.1073/pnas.1317625111>, Ricotta and Szeidl (2009)\n    <doi:10.1016/j.tpb.2009.10.001>. Graphical outputs are included.\n    Visit the 'mFD' website for more information, documentation and examples.",
    "version": "1.0.7",
    "maintainer": "Camille Magneville <camille.magneville@gmail.com>",
    "author": "Camille Magneville [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0489-3822>),\n  Nicolas Loiseau [aut] (ORCID: <https://orcid.org/0000-0002-2469-1980>),\n  Camille Albouy [aut] (ORCID: <https://orcid.org/0000-0003-1629-2389>),\n  Nicolas Casajus [aut] (ORCID: <https://orcid.org/0000-0002-5537-5294>),\n  Thomas Claverie [aut] (ORCID: <https://orcid.org/0000-0002-6258-4991>),\n  Arthur Escalas [aut] (ORCID: <https://orcid.org/0000-0002-6450-5716>),\n  Fabien Leprieur [aut] (ORCID: <https://orcid.org/0000-0001-6869-342X>),\n  Eva Maire [aut] (ORCID: <https://orcid.org/0000-0002-1032-3394>),\n  David Mouillot [aut] (ORCID: <https://orcid.org/0000-0003-0402-2605>),\n  Sebastien Villeger [aut] (ORCID:\n    <https://orcid.org/0000-0002-2362-7178>)",
    "url": "https://cmlmagneville.github.io/mFD/,\nhttps://github.com/CmlMagneville/mFD",
    "bug_reports": "https://github.com/CmlMagneville/mFD/issues",
    "repository": "https://cran.r-project.org/package=mFD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mFD Compute and Illustrate the Multiple Facets of Functional\nDiversity Computing functional traits-based distances between pairs of \n    species for species gathered in assemblages allowing to build several\n    functional spaces. The package allows to compute functional diversity\n    indices assessing the distribution of species (and of their dominance) in a\n    given functional space for each assemblage and the overlap between\n    assemblages in a given functional space, see: Chao et al. (2018)\n    <doi:10.1002/ecm.1343>, Maire et al. (2015) <doi:10.1111/geb.12299>,\n    Mouillot et al. (2013) <doi:10.1016/j.tree.2012.10.004>, Mouillot et al.\n    (2014) <doi:10.1073/pnas.1317625111>, Ricotta and Szeidl (2009)\n    <doi:10.1016/j.tpb.2009.10.001>. Graphical outputs are included.\n    Visit the 'mFD' website for more information, documentation and examples.  "
  },
  {
    "id": 15486,
    "package_name": "mFLICA",
    "title": "Leadership-Inference Framework for Multivariate Time Series",
    "description": "A leadership-inference framework for multivariate time series. The framework for multiple-faction-leadership inference from coordinated activities or 'mFLICA' uses a notion of a leader as an individual who initiates collective patterns that everyone in a group follows. Given a set of time series of individual activities, our goal is to identify periods of coordinated activity, find factions of coordination if more than one exist, as well as identify leaders of each faction. For each time step, the framework infers following relations between individual time series, then identifying a leader of each faction whom many individuals follow but it follows no one. A faction is defined as a group of individuals that everyone follows the same leader. 'mFLICA' reports following relations, leaders of factions, and members of each faction for each time step. Please see Chainarong Amornbunchornvej and Tanya Berger-Wolf (2018) <doi:10.1137/1.9781611975321.62> for methodology and Chainarong Amornbunchornvej (2021) <doi:10.1016/j.softx.2021.100781> for software when referring to this package in publications.",
    "version": "0.1.7",
    "maintainer": "Chainarong Amornbunchornvej <grandca@gmail.com>",
    "author": "Chainarong Amornbunchornvej [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3131-0370>),\n  Namrata Banerji [ctb]",
    "url": "https://github.com/DarkEyes/mFLICA",
    "bug_reports": "https://github.com/DarkEyes/mFLICA/issues",
    "repository": "https://cran.r-project.org/package=mFLICA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mFLICA Leadership-Inference Framework for Multivariate Time Series A leadership-inference framework for multivariate time series. The framework for multiple-faction-leadership inference from coordinated activities or 'mFLICA' uses a notion of a leader as an individual who initiates collective patterns that everyone in a group follows. Given a set of time series of individual activities, our goal is to identify periods of coordinated activity, find factions of coordination if more than one exist, as well as identify leaders of each faction. For each time step, the framework infers following relations between individual time series, then identifying a leader of each faction whom many individuals follow but it follows no one. A faction is defined as a group of individuals that everyone follows the same leader. 'mFLICA' reports following relations, leaders of factions, and members of each faction for each time step. Please see Chainarong Amornbunchornvej and Tanya Berger-Wolf (2018) <doi:10.1137/1.9781611975321.62> for methodology and Chainarong Amornbunchornvej (2021) <doi:10.1016/j.softx.2021.100781> for software when referring to this package in publications.  "
  },
  {
    "id": 15492,
    "package_name": "mMARCH.AC",
    "title": "Processing of Accelerometry Data with 'GGIR' in mMARCH",
    "description": "Mobile Motor Activity Research Consortium for Health (mMARCH) is a collaborative network of studies of clinical and community samples that employ common clinical, biological, and digital mobile measures across involved studies. One of the main scientific goals of mMARCH sites is developing a better understanding of the inter-relationships between accelerometry-measured physical activity (PA), sleep (SL), and circadian rhythmicity (CR) and mental and physical health in children, adolescents, and adults. Currently, there is no consensus on a standard procedure for a data processing pipeline of raw accelerometry data, and few open-source tools to facilitate their development. The R package 'GGIR' is the most prominent open-source software package that offers great functionality and tremendous user flexibility to process raw accelerometry data. However, even with 'GGIR', processing done in a harmonized and reproducible fashion requires a non-trivial amount of expertise combined with a careful implementation. In addition, novel accelerometry-derived features of PA/SL/CR capturing multiscale, time-series, functional, distributional and other complimentary aspects of accelerometry data being constantly proposed and become available via non-GGIR R implementations.  To address these issues, mMARCH developed a streamlined harmonized and reproducible pipeline for loading and cleaning raw accelerometry data, extracting features available through 'GGIR' as well as through non-GGIR R packages, implementing several data and feature quality checks, merging all features of PA/SL/CR together, and performing multiple analyses including Joint Individual Variation Explained (JIVE), an unsupervised machine learning dimension reduction technique that identifies latent factors capturing joint across and individual to each of three domains of PA/SL/CR.  In detail, the pipeline generates all necessary R/Rmd/shell files for data processing after running 'GGIR' for accelerometer data. In module 1, all csv files in the 'GGIR' output directory were read, transformed and then merged. In module 2, the 'GGIR' output files were checked and summarized in one excel sheet. In module 3, the merged data was cleaned according to the number of valid hours on each night and the number of valid days for each subject. In module 4, the cleaned activity data was imputed by the average Euclidean norm minus one (ENMO) over all the valid days for each subject. Finally, a comprehensive report of data processing was created using Rmarkdown, and the report includes few exploratory plots and multiple commonly used features extracted from minute level actigraphy data.  Reference: Guo W, Leroux A, Shou S, Cui L, Kang S, Strippoli MP, Preisig M, Zipunnikov V, Merikangas K (2022) Processing of accelerometry data with GGIR in Motor Activity Research Consortium for Health (mMARCH) Journal for the Measurement of Physical Behaviour, 6(1): 37-44.",
    "version": "3.2.0.1",
    "maintainer": "Wei Guo <wei.guo3@nih.gov>",
    "author": "Wei Guo [aut, cre],\n  Andrew Leroux [aut],\n  Vadim Zipunnikov [aut],\n  Kathleen Merikangas [aut]",
    "url": "https://github.com/WeiGuoNIMH/mMARCH.AC",
    "bug_reports": "https://github.com/WeiGuoNIMH/mMARCH.AC/issues",
    "repository": "https://cran.r-project.org/package=mMARCH.AC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mMARCH.AC Processing of Accelerometry Data with 'GGIR' in mMARCH Mobile Motor Activity Research Consortium for Health (mMARCH) is a collaborative network of studies of clinical and community samples that employ common clinical, biological, and digital mobile measures across involved studies. One of the main scientific goals of mMARCH sites is developing a better understanding of the inter-relationships between accelerometry-measured physical activity (PA), sleep (SL), and circadian rhythmicity (CR) and mental and physical health in children, adolescents, and adults. Currently, there is no consensus on a standard procedure for a data processing pipeline of raw accelerometry data, and few open-source tools to facilitate their development. The R package 'GGIR' is the most prominent open-source software package that offers great functionality and tremendous user flexibility to process raw accelerometry data. However, even with 'GGIR', processing done in a harmonized and reproducible fashion requires a non-trivial amount of expertise combined with a careful implementation. In addition, novel accelerometry-derived features of PA/SL/CR capturing multiscale, time-series, functional, distributional and other complimentary aspects of accelerometry data being constantly proposed and become available via non-GGIR R implementations.  To address these issues, mMARCH developed a streamlined harmonized and reproducible pipeline for loading and cleaning raw accelerometry data, extracting features available through 'GGIR' as well as through non-GGIR R packages, implementing several data and feature quality checks, merging all features of PA/SL/CR together, and performing multiple analyses including Joint Individual Variation Explained (JIVE), an unsupervised machine learning dimension reduction technique that identifies latent factors capturing joint across and individual to each of three domains of PA/SL/CR.  In detail, the pipeline generates all necessary R/Rmd/shell files for data processing after running 'GGIR' for accelerometer data. In module 1, all csv files in the 'GGIR' output directory were read, transformed and then merged. In module 2, the 'GGIR' output files were checked and summarized in one excel sheet. In module 3, the merged data was cleaned according to the number of valid hours on each night and the number of valid days for each subject. In module 4, the cleaned activity data was imputed by the average Euclidean norm minus one (ENMO) over all the valid days for each subject. Finally, a comprehensive report of data processing was created using Rmarkdown, and the report includes few exploratory plots and multiple commonly used features extracted from minute level actigraphy data.  Reference: Guo W, Leroux A, Shou S, Cui L, Kang S, Strippoli MP, Preisig M, Zipunnikov V, Merikangas K (2022) Processing of accelerometry data with GGIR in Motor Activity Research Consortium for Health (mMARCH) Journal for the Measurement of Physical Behaviour, 6(1): 37-44.  "
  },
  {
    "id": 15537,
    "package_name": "mailmerge",
    "title": "Mail Merge Using R Markdown Documents and 'gmailr'",
    "description": "Perform a mail merge (mass email) using the message defined in \n    markdown, the recipients in a 'csv' file, and gmail as the mailing engine. \n    With this package you can parse markdown documents as the body of email, and \n    the 'yaml' header to specify the subject line of the email.  Any '{}' braces \n    in the email will be encoded with 'glue::glue()'. You can preview the email \n    in the RStudio viewer pane, and send (draft) email using 'gmailr'.",
    "version": "0.2.5",
    "maintainer": "Andrie de Vries <apdevries@gmail.com>",
    "author": "Andrie de Vries [aut, cre]",
    "url": "https://andrie.github.io/mailmerge/,\nhttps://github.com/andrie/mailmerge",
    "bug_reports": "https://github.com/andrie/mailmerge/issues",
    "repository": "https://cran.r-project.org/package=mailmerge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mailmerge Mail Merge Using R Markdown Documents and 'gmailr' Perform a mail merge (mass email) using the message defined in \n    markdown, the recipients in a 'csv' file, and gmail as the mailing engine. \n    With this package you can parse markdown documents as the body of email, and \n    the 'yaml' header to specify the subject line of the email.  Any '{}' braces \n    in the email will be encoded with 'glue::glue()'. You can preview the email \n    in the RStudio viewer pane, and send (draft) email using 'gmailr'.  "
  },
  {
    "id": 15544,
    "package_name": "makepipe",
    "title": "Pipeline Tools Inspired by 'GNU Make'",
    "description": "A suite of tools for transforming an existing workflow into a\n    self-documenting pipeline with very minimal upfront costs. Segments of\n    the pipeline are specified in much the same way a 'Make' rule is, by\n    declaring an executable recipe (which might be an R script), along\n    with the corresponding targets and dependencies. When the entire\n    pipeline is run through, only those recipes that need to be executed\n    will be. Meanwhile, execution metadata is captured behind the scenes\n    for later inspection.",
    "version": "0.2.2",
    "maintainer": "Kinto Behr <kinto.behr@gmail.com>",
    "author": "Kinto Behr [aut, cre, cph]",
    "url": "https://kinto-b.github.io/makepipe/,\nhttps://github.com/kinto-b/makepipe",
    "bug_reports": "https://github.com/kinto-b/makepipe/issues",
    "repository": "https://cran.r-project.org/package=makepipe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "makepipe Pipeline Tools Inspired by 'GNU Make' A suite of tools for transforming an existing workflow into a\n    self-documenting pipeline with very minimal upfront costs. Segments of\n    the pipeline are specified in much the same way a 'Make' rule is, by\n    declaring an executable recipe (which might be an R script), along\n    with the corresponding targets and dependencies. When the entire\n    pipeline is run through, only those recipes that need to be executed\n    will be. Meanwhile, execution metadata is captured behind the scenes\n    for later inspection.  "
  },
  {
    "id": 15552,
    "package_name": "maldipickr",
    "title": "Dereplicate and Cherry-Pick Mass Spectrometry Spectra",
    "description": "Convenient wrapper functions for the analysis of\n    matrix-assisted laser desorption/ionization-time-of-flight (MALDI-TOF)\n    spectra data in order to select only representative spectra (also\n    called cherry-pick). The package covers the preprocessing and\n    dereplication steps (based on Strejcek, Smrhova, Junkova and Uhlik\n    (2018) <doi:10.3389/fmicb.2018.01294>) needed to cluster MALDI-TOF\n    spectra before the final cherry-picking step. It enables the easy\n    exclusion of spectra and/or clusters to accommodate complex\n    cherry-picking strategies. Alternatively, cherry-picking using\n    taxonomic identification MALDI-TOF data is made easy with functions to\n    import inconsistently formatted reports.",
    "version": "1.3.1",
    "maintainer": "Charlie Pauvert <cpauvert@ukaachen.de>",
    "author": "Charlie Pauvert [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9832-2507>),\n  David Wylensek [ctb] (ORCID: <https://orcid.org/0000-0002-8424-5712>),\n  Selina N\u00fcchtern [ctb],\n  Thomas Clavel [ctb, fnd, cph] (ORCID:\n    <https://orcid.org/0000-0002-7229-5595>)",
    "url": "https://github.com/ClavelLab/maldipickr,\nhttps://clavellab.github.io/maldipickr/",
    "bug_reports": "https://github.com/ClavelLab/maldipickr/issues",
    "repository": "https://cran.r-project.org/package=maldipickr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maldipickr Dereplicate and Cherry-Pick Mass Spectrometry Spectra Convenient wrapper functions for the analysis of\n    matrix-assisted laser desorption/ionization-time-of-flight (MALDI-TOF)\n    spectra data in order to select only representative spectra (also\n    called cherry-pick). The package covers the preprocessing and\n    dereplication steps (based on Strejcek, Smrhova, Junkova and Uhlik\n    (2018) <doi:10.3389/fmicb.2018.01294>) needed to cluster MALDI-TOF\n    spectra before the final cherry-picking step. It enables the easy\n    exclusion of spectra and/or clusters to accommodate complex\n    cherry-picking strategies. Alternatively, cherry-picking using\n    taxonomic identification MALDI-TOF data is made easy with functions to\n    import inconsistently formatted reports.  "
  },
  {
    "id": 15562,
    "package_name": "manifestoR",
    "title": "Access and Process Data and Documents of the Manifesto Project",
    "description": "Provides access to coded election programmes from the Manifesto\n    Corpus and to the Manifesto Project's Main Dataset and routines to analyse this\n    data. The Manifesto Project <https://manifesto-project.wzb.eu> collects and\n    analyses election programmes across time and space to measure the political\n    preferences of parties. The Manifesto Corpus contains the collected and\n    annotated election programmes in the Corpus format of the package 'tm' to enable\n    easy use of text processing and text mining functionality. Specific functions\n    for scaling of coded political texts are included.",
    "version": "1.6.1",
    "maintainer": "Pola Lehmann <pola.lehmann@wzb.eu>",
    "author": "Jirka Lewandowski [aut],\n  Nicolas Merz [aut],\n  Sven Regel [aut],\n  Pola Lehmann [cre, ctb],\n  Paul Muscat [ctb]",
    "url": "https://manifesto-project.wzb.eu/manifestoR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=manifestoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "manifestoR Access and Process Data and Documents of the Manifesto Project Provides access to coded election programmes from the Manifesto\n    Corpus and to the Manifesto Project's Main Dataset and routines to analyse this\n    data. The Manifesto Project <https://manifesto-project.wzb.eu> collects and\n    analyses election programmes across time and space to measure the political\n    preferences of parties. The Manifesto Corpus contains the collected and\n    annotated election programmes in the Corpus format of the package 'tm' to enable\n    easy use of text processing and text mining functionality. Specific functions\n    for scaling of coded political texts are included.  "
  },
  {
    "id": 15580,
    "package_name": "mapboxer",
    "title": "An R Interface to 'Mapbox GL JS'",
    "description": "Makes 'Mapbox GL JS' <https://docs.mapbox.com/mapbox-gl-js/api/>,\n  an open source JavaScript library that uses WebGL to render interactive maps,\n  available within R via the 'htmlwidgets' package. Visualizations can be used from the R console,\n  in R Markdown documents and in Shiny apps.",
    "version": "0.4.0",
    "maintainer": "Stefan Kuethe <crazycapivara@gmail.com>",
    "author": "Stefan Kuethe [aut, cre]",
    "url": "https://github.com/crazycapivara/mapboxer",
    "bug_reports": "https://github.com/crazycapivara/mapboxer/issues",
    "repository": "https://cran.r-project.org/package=mapboxer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapboxer An R Interface to 'Mapbox GL JS' Makes 'Mapbox GL JS' <https://docs.mapbox.com/mapbox-gl-js/api/>,\n  an open source JavaScript library that uses WebGL to render interactive maps,\n  available within R via the 'htmlwidgets' package. Visualizations can be used from the R console,\n  in R Markdown documents and in Shiny apps.  "
  },
  {
    "id": 15592,
    "package_name": "mapme.biodiversity",
    "title": "Efficient Monitoring of Global Biodiversity Portfolios",
    "description": "Biodiversity areas, especially primary forest, serve a\n    multitude of functions for local economy, regional functionality of\n    the ecosystems as well as the global health of our planet. Recently,\n    adverse changes in human land use practices and climatic responses to\n    increased greenhouse gas emissions, put these biodiversity areas under\n    a variety of different threats. The present package helps to analyse a\n    number of biodiversity indicators based on freely available\n    geographical datasets. It supports computational efficient routines\n    that allow the analysis of potentially global biodiversity portfolios.\n    The primary use case of the package is to support evidence based\n    reporting of an organization's effort to protect biodiversity areas\n    under threat and to identify regions were intervention is most duly\n    needed.",
    "version": "0.9.5",
    "maintainer": "Sven Bergtold <sven.bergtold@gmail.com>",
    "author": "Darius A. G\u00f6rgen [aut] (ORCID: <https://orcid.org/0009-0008-5503-7704>),\n  Om Prakash Bhandari [aut],\n  Andreas Petutschnig [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5029-2425>),\n  Sven Bergtold [ctb, cre],\n  Zivan Karaman [ctb] (ORCID: <https://orcid.org/0000-0002-8933-4589>),\n  MAPME-Initiative [cph, fnd]",
    "url": "https://mapme-initiative.github.io/mapme.biodiversity/,\nhttps://github.com/mapme-initiative/mapme.biodiversity/",
    "bug_reports": "https://github.com/mapme-initiative/mapme.biodiversity/issues",
    "repository": "https://cran.r-project.org/package=mapme.biodiversity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapme.biodiversity Efficient Monitoring of Global Biodiversity Portfolios Biodiversity areas, especially primary forest, serve a\n    multitude of functions for local economy, regional functionality of\n    the ecosystems as well as the global health of our planet. Recently,\n    adverse changes in human land use practices and climatic responses to\n    increased greenhouse gas emissions, put these biodiversity areas under\n    a variety of different threats. The present package helps to analyse a\n    number of biodiversity indicators based on freely available\n    geographical datasets. It supports computational efficient routines\n    that allow the analysis of potentially global biodiversity portfolios.\n    The primary use case of the package is to support evidence based\n    reporting of an organization's effort to protect biodiversity areas\n    under threat and to identify regions were intervention is most duly\n    needed.  "
  },
  {
    "id": 15624,
    "package_name": "markdownInput",
    "title": "Shiny Module for a Markdown Input with Result Preview",
    "description": "An R-Shiny module containing a \"markdownInput\". This input allows\n    the user to write some markdown code and to preview the result. This input\n    has been inspired by the \"comment\" window of <https://github.com/>.",
    "version": "0.1.2",
    "maintainer": "Julien Diot <juliendiot@ut-biomet.org>",
    "author": "Julien Diot [aut, ctb, cre] (ORCID:\n    <https://orcid.org/0000-0002-8738-2034>)",
    "url": "https://github.com/juliendiot42/markdownInput",
    "bug_reports": "https://github.com/juliendiot42/markdownInput/issues",
    "repository": "https://cran.r-project.org/package=markdownInput",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "markdownInput Shiny Module for a Markdown Input with Result Preview An R-Shiny module containing a \"markdownInput\". This input allows\n    the user to write some markdown code and to preview the result. This input\n    has been inspired by the \"comment\" window of <https://github.com/>.  "
  },
  {
    "id": 15653,
    "package_name": "matchedcc",
    "title": "'Stata'-Like Matched Case-Control Analysis",
    "description": "Calculate multiple statistics with confidence intervals for matched\n    case-control data including risk difference, risk ratio, relative\n    difference, and the odds ratio. Results are equivalent to those from\n    'Stata', and you can choose how to format your input data. Methods used are\n    those described on page 56 the 'Stata' documentation for \"Epitab - Tables\n    for Epidemologists\" <https://www.stata.com/manuals/repitab.pdf>.",
    "version": "0.1.1",
    "maintainer": "Simon R Parker <simon.parker.24@ucl.ac.uk>",
    "author": "Simon R Parker [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0003-8214-4496>)",
    "url": "https://github.com/simpar1471/matchedcc/,\nhttps://simpar1471.github.io/matchedcc/",
    "bug_reports": "https://github.com/simpar1471/matchedcc/issues",
    "repository": "https://cran.r-project.org/package=matchedcc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matchedcc 'Stata'-Like Matched Case-Control Analysis Calculate multiple statistics with confidence intervals for matched\n    case-control data including risk difference, risk ratio, relative\n    difference, and the odds ratio. Results are equivalent to those from\n    'Stata', and you can choose how to format your input data. Methods used are\n    those described on page 56 the 'Stata' documentation for \"Epitab - Tables\n    for Epidemologists\" <https://www.stata.com/manuals/repitab.pdf>.  "
  },
  {
    "id": 15660,
    "package_name": "materialmodifier",
    "title": "Apply Photo Editing Effects",
    "description": "You can apply image processing effects that modifies the perceived material properties of objects\n    in photos, such as gloss, smoothness, and blemishes. This is an implementation of the algorithm proposed by\n    Boyadzhiev et al. (2015) \"Band-Sifting Decomposition for Image Based Material Editing\".\n    Documentation and practical tips of the package is available at <https://github.com/tsuda16k/materialmodifier>.",
    "version": "1.2.0",
    "maintainer": "Hiroyuki Tsuda <tsuda16k@gmail.com>",
    "author": "Hiroyuki Tsuda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9396-5327>)",
    "url": "https://github.com/tsuda16k/materialmodifier",
    "bug_reports": "https://github.com/tsuda16k/materialmodifier/issues/",
    "repository": "https://cran.r-project.org/package=materialmodifier",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "materialmodifier Apply Photo Editing Effects You can apply image processing effects that modifies the perceived material properties of objects\n    in photos, such as gloss, smoothness, and blemishes. This is an implementation of the algorithm proposed by\n    Boyadzhiev et al. (2015) \"Band-Sifting Decomposition for Image Based Material Editing\".\n    Documentation and practical tips of the package is available at <https://github.com/tsuda16k/materialmodifier>.  "
  },
  {
    "id": 15662,
    "package_name": "mathml",
    "title": "Translate R Expressions to 'MathML' and 'LaTeX'/'MathJax'",
    "description": "Translate R expressions to 'MathML' or 'MathJax'/'LaTeX' so that\n    they can be rendered in R markdown documents and shiny apps. This package\n    depends on R package 'rolog', which requires an installation of the\n    'SWI'-'Prolog' runtime either from 'swi-prolog.org' or from R\n    package 'rswipl'.",
    "version": "1.6",
    "maintainer": "Matthias Gondan <Matthias.Gondan-Rochon@uibk.ac.at>",
    "author": "Matthias Gondan [aut, cre, cph] (University of Innsbruck),\n  Irene Alfarone [aut] (University of Innsbruck),\n  European Commission [fnd] (Erasmus+ Programme,\n    2019-1-EE01-KA203-051708)",
    "url": "https://github.com/mgondan/mathml",
    "bug_reports": "https://github.com/mgondan/mathml/issues",
    "repository": "https://cran.r-project.org/package=mathml",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mathml Translate R Expressions to 'MathML' and 'LaTeX'/'MathJax' Translate R expressions to 'MathML' or 'MathJax'/'LaTeX' so that\n    they can be rendered in R markdown documents and shiny apps. This package\n    depends on R package 'rolog', which requires an installation of the\n    'SWI'-'Prolog' runtime either from 'swi-prolog.org' or from R\n    package 'rswipl'.  "
  },
  {
    "id": 15663,
    "package_name": "mathpix",
    "title": "Support for the 'Mathpix' API (Image to 'LaTeX')",
    "description": "Given an image of a formula (typeset or handwritten) this package\n    provides calls to the 'Mathpix' service to produce the 'LaTeX' code which should\n    generate that image, and pastes it into a (e.g. an 'rmarkdown') document. \n    See <https://docs.mathpix.com/> for full details. 'Mathpix' is an external service \n    and use of the API is subject to their terms and conditions.",
    "version": "0.6.0",
    "maintainer": "Jonathan Carroll <rpkg@jcarroll.com.au>",
    "author": "Jonathan Carroll [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1404-5264>)",
    "url": "https://github.com/jonocarroll/mathpix",
    "bug_reports": "https://github.com/jonocarroll/mathpix/issues",
    "repository": "https://cran.r-project.org/package=mathpix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mathpix Support for the 'Mathpix' API (Image to 'LaTeX') Given an image of a formula (typeset or handwritten) this package\n    provides calls to the 'Mathpix' service to produce the 'LaTeX' code which should\n    generate that image, and pastes it into a (e.g. an 'rmarkdown') document. \n    See <https://docs.mathpix.com/> for full details. 'Mathpix' is an external service \n    and use of the API is subject to their terms and conditions.  "
  },
  {
    "id": 15752,
    "package_name": "mcmapper",
    "title": "Mapping First Moment and C-Statistic to the Parameters of\nDistributions for Risk",
    "description": "Provides a series of numerical methods for extracting parameters of distributions for risks based on knowing the expected value and c-statistics (e.g., from a published report on the performance of a risk prediction model). This package implements the methodology described in Sadatsafavi et al (2024) <doi:10.48550/arXiv.2409.09178>. The core of the package is mcmap(), which takes a pair of (mean, c-statistic) and the distribution type requested. This function provides a generic interface to more customized functions (mcmap_beta(), mcmap_logitnorm(), mcmap_probitnorm()) for specific distributions. ",
    "version": "0.0.11",
    "maintainer": "Mohsen Sadatsafavi <mohsen.sadatsafavi@ubc.ca>",
    "author": "Mohsen Sadatsafavi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0419-7862>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcmapper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcmapper Mapping First Moment and C-Statistic to the Parameters of\nDistributions for Risk Provides a series of numerical methods for extracting parameters of distributions for risks based on knowing the expected value and c-statistics (e.g., from a published report on the performance of a risk prediction model). This package implements the methodology described in Sadatsafavi et al (2024) <doi:10.48550/arXiv.2409.09178>. The core of the package is mcmap(), which takes a pair of (mean, c-statistic) and the distribution type requested. This function provides a generic interface to more customized functions (mcmap_beta(), mcmap_logitnorm(), mcmap_probitnorm()) for specific distributions.   "
  },
  {
    "id": 15769,
    "package_name": "mcr",
    "title": "Method Comparison Regression",
    "description": "Regression methods to quantify the relation between two measurement\n    methods are provided by this package. In particular it addresses regression\n    problems with errors in both variables and without repeated measurements. It\n    implements the CLSI recommendations (see J. A. Budd et al. \n    (2018, <https://clsi.org/standards/products/method-evaluation/documents/ep09/>) \n    for analytical method comparison and bias estimation using patient samples.\n    Furthermore, algorithms for Theil-Sen and equivariant Passing-Bablok estimators \n    are implemented, see F. Dufey (2020, <doi:10.1515/ijb-2019-0157>) and \n    J. Raymaekers and F. Dufey (2022, <arXiv:2202:08060>).\n    A comprehensive overview over the implemented methods and references can be found \n    in the manual pages \"mcr-package\" and \"mcreg\".",
    "version": "1.3.3.1",
    "maintainer": "Sergej Potapov <sergej.potapov@roche.com>",
    "author": "Sergej Potapov [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-6251-0279>),\n  Fabian Model [aut],\n  Andre Schuetzenmeister [aut] (ORCID:\n    <https://orcid.org/0000-0002-2964-5502>),\n  Ekaterina Manuilova [aut],\n  Florian Dufey [aut] (ORCID: <https://orcid.org/0000-0001-6467-8556>),\n  Jakob Raymaekers [aut] (ORCID: <https://orcid.org/0000-0002-2093-3137>),\n  Venkatraman E. Seshan [ctb],\n  Roche [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcr Method Comparison Regression Regression methods to quantify the relation between two measurement\n    methods are provided by this package. In particular it addresses regression\n    problems with errors in both variables and without repeated measurements. It\n    implements the CLSI recommendations (see J. A. Budd et al. \n    (2018, <https://clsi.org/standards/products/method-evaluation/documents/ep09/>) \n    for analytical method comparison and bias estimation using patient samples.\n    Furthermore, algorithms for Theil-Sen and equivariant Passing-Bablok estimators \n    are implemented, see F. Dufey (2020, <doi:10.1515/ijb-2019-0157>) and \n    J. Raymaekers and F. Dufey (2022, <arXiv:2202:08060>).\n    A comprehensive overview over the implemented methods and references can be found \n    in the manual pages \"mcr-package\" and \"mcreg\".  "
  },
  {
    "id": 15770,
    "package_name": "mcrPioda",
    "title": "Method Comparison Regression - Mcr Fork for M- And MM-Deming\nRegression",
    "description": "Regression methods to quantify the relation between two measurement\n    methods are provided by this package. In particular it addresses regression\n    problems with errors in both variables and without repeated measurements. It\n    implements the Clinical Laboratory Standard International (CLSI) \n    recommendations (see J. A. Budd et al. \n    (2018, <https://clsi.org/standards/products/method-evaluation/documents/ep09/>) \n    for analytical method comparison and bias estimation using patient samples.\n    Furthermore, algorithms for Theil-Sen and equivariant Passing-Bablok estimators \n    are implemented, see F. Dufey (2020, <doi:10.1515/ijb-2019-0157>) and \n    J. Raymaekers and F. Dufey (2022, <arXiv:2202:08060>).\n    Further the robust M-Deming and MM-Deming (experimental) are available, see\n    G. Pioda (2021, <arXiv:2105:04628>).\n    A comprehensive overview over the implemented methods and references can be found \n    in the manual pages 'mcrPioda-package' and 'mcreg'.",
    "version": "1.3.4",
    "maintainer": "Giorgio Pioda <gfwp@ticino.com>",
    "author": "Giorgio Pioda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8714-8678>),\n  Sergej Potapov [aut] (ORCID: <https://orcid.org/0009-0002-6251-0279>),\n  Fabian Model [aut],\n  Andre Schuetzenmeister [aut] (ORCID:\n    <https://orcid.org/0000-0002-2964-5502>),\n  Ekaterina Manuilova [aut],\n  Florian Dufey [aut] (ORCID: <https://orcid.org/0000-0001-6467-8556>),\n  Jakob Raymaekers [aut] (ORCID: <https://orcid.org/0000-0002-2093-3137>),\n  Venkatraman E. Seshan [ctb],\n  Roche [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcrPioda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcrPioda Method Comparison Regression - Mcr Fork for M- And MM-Deming\nRegression Regression methods to quantify the relation between two measurement\n    methods are provided by this package. In particular it addresses regression\n    problems with errors in both variables and without repeated measurements. It\n    implements the Clinical Laboratory Standard International (CLSI) \n    recommendations (see J. A. Budd et al. \n    (2018, <https://clsi.org/standards/products/method-evaluation/documents/ep09/>) \n    for analytical method comparison and bias estimation using patient samples.\n    Furthermore, algorithms for Theil-Sen and equivariant Passing-Bablok estimators \n    are implemented, see F. Dufey (2020, <doi:10.1515/ijb-2019-0157>) and \n    J. Raymaekers and F. Dufey (2022, <arXiv:2202:08060>).\n    Further the robust M-Deming and MM-Deming (experimental) are available, see\n    G. Pioda (2021, <arXiv:2105:04628>).\n    A comprehensive overview over the implemented methods and references can be found \n    in the manual pages 'mcrPioda-package' and 'mcreg'.  "
  },
  {
    "id": 15779,
    "package_name": "md.log",
    "title": "Produces Markdown Log File with a Built-in Function Call",
    "description": "Produces clean and neat Markdown log file\n    and also provide an argument to include the function call inside the Markdown log.",
    "version": "0.2.0",
    "maintainer": "E. F. Haghish <haghish@uio.no>",
    "author": "E. F. Haghish",
    "url": "https://github.com/haghish/md.log",
    "bug_reports": "https://github.com/haghish/md.log/issues",
    "repository": "https://cran.r-project.org/package=md.log",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "md.log Produces Markdown Log File with a Built-in Function Call Produces clean and neat Markdown log file\n    and also provide an argument to include the function call inside the Markdown log.  "
  },
  {
    "id": 15780,
    "package_name": "md4r",
    "title": "Markdown Parser Implemented using the 'MD4C' Library",
    "description": "Provides an R wrapper for the 'MD4C' (Markdown for 'C') library. \n    Functions exist for parsing markdown ('CommonMark' compliant) along with support for other common\n    markdown extensions (e.g. GitHub flavored markdown, 'LaTeX' equation support, etc.). The \n    package also provides a number of higher level functions for exploring and manipulating markdown\n    abstract syntax trees as well as translating and displaying the documents.",
    "version": "0.5.2.0",
    "maintainer": "Colin Rundel <rundel@gmail.com>",
    "author": "Colin Rundel [aut, cre],\n  Martin Mit\u00e1\u0161 [cph] (md4c author: md4c.c, md4c.h, specs/md4c/),\n  RStudio, PBC [cph] (httpuv_url_tools.cpp),\n  John MacFarlane [cph] (specs/gfm/spec.txt, specs/md4c/spec.txt)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=md4r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "md4r Markdown Parser Implemented using the 'MD4C' Library Provides an R wrapper for the 'MD4C' (Markdown for 'C') library. \n    Functions exist for parsing markdown ('CommonMark' compliant) along with support for other common\n    markdown extensions (e.g. GitHub flavored markdown, 'LaTeX' equation support, etc.). The \n    package also provides a number of higher level functions for exploring and manipulating markdown\n    abstract syntax trees as well as translating and displaying the documents.  "
  },
  {
    "id": 15795,
    "package_name": "mds",
    "title": "Medical Devices Surveillance",
    "description": "A set of core functions for handling medical device event data in\n    the context of post-market surveillance, pharmacovigilance, signal detection\n    and trending, and regulatory reporting. Primary inputs are data on events by\n    device and data on exposures by device. Outputs include: standardized\n    device-event and exposure datasets, defined analyses, and time series.",
    "version": "0.3.2",
    "maintainer": "Gary Chung <gchung05@gmail.com>",
    "author": "Gary Chung [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mds Medical Devices Surveillance A set of core functions for handling medical device event data in\n    the context of post-market surveillance, pharmacovigilance, signal detection\n    and trending, and regulatory reporting. Primary inputs are data on events by\n    device and data on exposures by device. Outputs include: standardized\n    device-event and exposure datasets, defined analyses, and time series.  "
  },
  {
    "id": 15801,
    "package_name": "meanr",
    "title": "Sentiment Analysis Scorer",
    "description": "Sentiment analysis is a popular technique in text mining that\n    attempts to determine the emotional state of some text. We provide a new\n    implementation of a common method for computing sentiment, whereby words are\n    scored as positive or negative according to a dictionary lookup. Then the\n    sum of those scores is returned for the document. We use the 'Hu' and 'Liu'\n    sentiment dictionary ('Hu' and 'Liu', 2004) <doi:10.1145/1014052.1014073>\n    for determining sentiment. The scoring function is 'vectorized' by document,\n    and scores for multiple documents are computed in parallel via 'OpenMP'.",
    "version": "0.1-6",
    "maintainer": "Drew Schmidt <wrathematics@gmail.com>",
    "author": "Drew Schmidt [aut, cre]",
    "url": "https://github.com/wrathematics/meanr",
    "bug_reports": "https://github.com/wrathematics/meanr/issues",
    "repository": "https://cran.r-project.org/package=meanr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "meanr Sentiment Analysis Scorer Sentiment analysis is a popular technique in text mining that\n    attempts to determine the emotional state of some text. We provide a new\n    implementation of a common method for computing sentiment, whereby words are\n    scored as positive or negative according to a dictionary lookup. Then the\n    sum of those scores is returned for the document. We use the 'Hu' and 'Liu'\n    sentiment dictionary ('Hu' and 'Liu', 2004) <doi:10.1145/1014052.1014073>\n    for determining sentiment. The scoring function is 'vectorized' by document,\n    and scores for multiple documents are computed in parallel via 'OpenMP'.  "
  },
  {
    "id": 15824,
    "package_name": "medicaldata",
    "title": "Data Package for Medical Datasets",
    "description": "Provides access to well-documented medical datasets for teaching.\n    Featuring several from the Teaching of Statistics in the Health Sciences \n    website <https://www.causeweb.org/tshs/category/dataset/>, a few reconstructed datasets of historical significance in medical\n    research, some reformatted and extended from existing R packages, \n    and some data donations. ",
    "version": "0.2.0",
    "maintainer": "Peter Higgins <higgi13425@yahoo.com>",
    "author": "Peter Higgins [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>)",
    "url": "https://higgi13425.github.io/medicaldata/,\nhttps://github.com/higgi13425/medicaldata/",
    "bug_reports": "https://github.com/higgi13425/medicaldata/issues",
    "repository": "https://cran.r-project.org/package=medicaldata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "medicaldata Data Package for Medical Datasets Provides access to well-documented medical datasets for teaching.\n    Featuring several from the Teaching of Statistics in the Health Sciences \n    website <https://www.causeweb.org/tshs/category/dataset/>, a few reconstructed datasets of historical significance in medical\n    research, some reformatted and extended from existing R packages, \n    and some data donations.   "
  },
  {
    "id": 15829,
    "package_name": "mefa",
    "title": "Multivariate Data Handling in Ecology and Biogeography",
    "description": "A framework package aimed to provide standardized computational environment for specialist work via object classes to represent the data coded by samples, taxa and segments (i.e. subpopulations, repeated measures). It supports easy processing of the data along with cross tabulation and relational data tables for samples and taxa. An object of class `mefa' is a project specific compendium of the data and can be easily used in further analyses. Methods are provided for extraction, aggregation, conversion, plotting, summary and reporting of `mefa' objects. Reports can be generated in plain text or LaTeX format. Vignette contains worked examples.",
    "version": "3.2-10",
    "maintainer": "Peter Solymos <psolymos@gmail.com>",
    "author": "Peter Solymos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7337-1740>)",
    "url": "https://github.com/psolymos/mefa",
    "bug_reports": "https://github.com/psolymos/mefa/issues",
    "repository": "https://cran.r-project.org/package=mefa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mefa Multivariate Data Handling in Ecology and Biogeography A framework package aimed to provide standardized computational environment for specialist work via object classes to represent the data coded by samples, taxa and segments (i.e. subpopulations, repeated measures). It supports easy processing of the data along with cross tabulation and relational data tables for samples and taxa. An object of class `mefa' is a project specific compendium of the data and can be easily used in further analyses. Methods are provided for extraction, aggregation, conversion, plotting, summary and reporting of `mefa' objects. Reports can be generated in plain text or LaTeX format. Vignette contains worked examples.  "
  },
  {
    "id": 15843,
    "package_name": "memoiR",
    "title": "R Markdown and Bookdown Templates to Publish Documents",
    "description": "Producing high-quality documents suitable for publication directly from R is made possible by the R Markdown ecosystem.\n  'memoiR' makes it easy.\n  It provides templates to knit memoirs, articles and slideshows with helpers to publish the documents on GitHub Pages and activate continuous integration.",
    "version": "1.3-1",
    "maintainer": "Eric Marcon <eric.marcon@agroparistech.fr>",
    "author": "Eric Marcon [aut, cre] (ORCID: <https://orcid.org/0000-0002-5249-321X>)",
    "url": "https://ericmarcon.github.io/memoiR/,\nhttps://github.com/EricMarcon/memoiR/",
    "bug_reports": "https://github.com/EricMarcon/memoiR/issues/",
    "repository": "https://cran.r-project.org/package=memoiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "memoiR R Markdown and Bookdown Templates to Publish Documents Producing high-quality documents suitable for publication directly from R is made possible by the R Markdown ecosystem.\n  'memoiR' makes it easy.\n  It provides templates to knit memoirs, articles and slideshows with helpers to publish the documents on GitHub Pages and activate continuous integration.  "
  },
  {
    "id": 15844,
    "package_name": "memor",
    "title": "A 'rmarkdown' Template that Can be Highly Customized",
    "description": "A 'rmarkdown' template that supports company logo, contact info, \n    watermarks and more. Currently restricted to 'Latex'/'Markdown'; a similar \n    'HTML' theme will be added in the future. ",
    "version": "0.2.3",
    "maintainer": "Hao Zhu <haozhu233@gmail.com>",
    "author": "Hao Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0002-3386-6076>),\n  Timothy Tsai [aut] (ORCID: <https://orcid.org/0000-0002-0274-8042>),\n  Thomas Travison [aut],\n  Mikhail Popov [ctb]",
    "url": "https://github.com/hebrewseniorlife/memor",
    "bug_reports": "https://github.com/hebrewseniorlife/memor/issues",
    "repository": "https://cran.r-project.org/package=memor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "memor A 'rmarkdown' Template that Can be Highly Customized A 'rmarkdown' template that supports company logo, contact info, \n    watermarks and more. Currently restricted to 'Latex'/'Markdown'; a similar \n    'HTML' theme will be added in the future.   "
  },
  {
    "id": 15867,
    "package_name": "metaBLUE",
    "title": "BLUE for Combining Location and Scale Information in a\nMeta-Analysis",
    "description": "The sample mean and standard deviation are two commonly used statistics in meta-analyses, \n    but some trials use other summary statistics such as the median and quartiles to report the results. \n    Therefore, researchers need to transform those information back to the sample mean and \n    standard deviation. This package implemented sample mean estimators by Luo et al. (2016) <arXiv:1505.05687>, sample standard deviation estimators by Wan et al. (2014) <arXiv:1407.8038>, and the best linear unbiased estimators (BLUEs) of location and scale parameters by Yang et al. (2018, submitted) based on sample quantiles derived summaries in a meta-analysis.",
    "version": "1.0.0",
    "maintainer": "Xin Yang <xyang.krystal@gmail.com>",
    "author": "Xin Yang [cre, aut],\n  Alan Hutson [aut],\n  Dongliang Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metaBLUE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaBLUE BLUE for Combining Location and Scale Information in a\nMeta-Analysis The sample mean and standard deviation are two commonly used statistics in meta-analyses, \n    but some trials use other summary statistics such as the median and quartiles to report the results. \n    Therefore, researchers need to transform those information back to the sample mean and \n    standard deviation. This package implemented sample mean estimators by Luo et al. (2016) <arXiv:1505.05687>, sample standard deviation estimators by Wan et al. (2014) <arXiv:1407.8038>, and the best linear unbiased estimators (BLUEs) of location and scale parameters by Yang et al. (2018, submitted) based on sample quantiles derived summaries in a meta-analysis.  "
  },
  {
    "id": 15883,
    "package_name": "metabias",
    "title": "Meta-Analysis for Within-Study and/or Across-Study Biases",
    "description": "Provides common components (classes, methods, documentation) for\n  packages that conduct meta-analytic corrections and sensitivity analyses\n  for within-study and/or across-study biases in meta-analysis. See the packages\n  'PublicationBias', 'phacking', and 'multibiasmeta'. These package implement\n  methods described in, respectively: Mathur & VanderWeele (2020)\n  <doi:10.31219/osf.io/s9dp6>; Mathur (2022) <doi:10.31219/osf.io/ezjsx>;\n  Mathur (2022) <doi:10.31219/osf.io/u7vcb>.",
    "version": "0.1.1",
    "maintainer": "Peter Solymos <peter@analythium.io>",
    "author": "Mika Braginsky [aut],\n  Maya Mathur [aut],\n  Peter Solymos [cre, ctb] (ORCID:\n    <https://orcid.org/0000-0001-7337-1740>)",
    "url": "https://github.com/mathurlabstanford/metabias,\nhttps://mathurlabstanford.github.io/metabias/",
    "bug_reports": "https://github.com/mathurlabstanford/metabias/issues",
    "repository": "https://cran.r-project.org/package=metabias",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metabias Meta-Analysis for Within-Study and/or Across-Study Biases Provides common components (classes, methods, documentation) for\n  packages that conduct meta-analytic corrections and sensitivity analyses\n  for within-study and/or across-study biases in meta-analysis. See the packages\n  'PublicationBias', 'phacking', and 'multibiasmeta'. These package implement\n  methods described in, respectively: Mathur & VanderWeele (2020)\n  <doi:10.31219/osf.io/s9dp6>; Mathur (2022) <doi:10.31219/osf.io/ezjsx>;\n  Mathur (2022) <doi:10.31219/osf.io/u7vcb>.  "
  },
  {
    "id": 15891,
    "package_name": "metacor",
    "title": "Meta-Analytic Effect Size Calculation for Pre-Post Designs with\nCorrelation Imputation",
    "description": "Tools for the calculation of effect sizes (standardised mean difference) and mean difference in pre-post controlled studies, including robust imputation of missing variances (standard deviation of changes) and correlations (Pearson correlation coefficient). The main function 'metacor_dual()' implements several methods for imputing missing standard deviation of changes or Pearson correlation coefficient, and generates transparent imputation reports. Designed for meta-analyses with incomplete summary statistics. For details on the methods, see Higgins et al. (2023) and Fu et al. (2013).",
    "version": "1.2.1",
    "maintainer": "Iker J. Bautista <ikerugr@gmail.com>",
    "author": "Iker J. Bautista [aut, cre],\n  Saul M. Rodriguez [ctb]",
    "url": "https://github.com/ikerugr/metacor,\nhttps://ikerugr.r-universe.dev/metacor",
    "bug_reports": "https://github.com/ikerugr/metacor/issues",
    "repository": "https://cran.r-project.org/package=metacor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metacor Meta-Analytic Effect Size Calculation for Pre-Post Designs with\nCorrelation Imputation Tools for the calculation of effect sizes (standardised mean difference) and mean difference in pre-post controlled studies, including robust imputation of missing variances (standard deviation of changes) and correlations (Pearson correlation coefficient). The main function 'metacor_dual()' implements several methods for imputing missing standard deviation of changes or Pearson correlation coefficient, and generates transparent imputation reports. Designed for meta-analyses with incomplete summary statistics. For details on the methods, see Higgins et al. (2023) and Fu et al. (2013).  "
  },
  {
    "id": 15902,
    "package_name": "metagear",
    "title": "Comprehensive Research Synthesis Tools for Systematic Reviews\nand Meta-Analysis",
    "description": "Functionalities for facilitating systematic reviews, data\n    extractions, and meta-analyses. It includes a GUI (graphical user interface)\n    to help screen the abstracts and titles of bibliographic data; tools to assign\n    screening effort across multiple collaborators/reviewers and to assess inter-\n    reviewer reliability; tools to help automate the download and retrieval of\n    journal PDF articles from online databases; figure and image extractions \n    from PDFs; web scraping of citations; automated and manual data extraction \n    from scatter-plot and bar-plot images; PRISMA (Preferred Reporting Items for\n    Systematic Reviews and Meta-Analyses) flow diagrams; simple imputation tools\n    to fill gaps in incomplete or missing study parameters; generation of random\n    effects sizes for Hedges' d, log response ratio, odds ratio, and correlation\n    coefficients for Monte Carlo experiments; covariance equations for modelling\n    dependencies among multiple effect sizes (e.g., effect sizes with a common\n    control); and finally summaries that replicate analyses and outputs from \n    widely used but no longer updated meta-analysis software (i.e., metawin).\n\tFunding for this package was supported by National Science Foundation (NSF) \n\tgrants DBI-1262545 and DEB-1451031. CITE: Lajeunesse, M.J. (2016) \n\tFacilitating systematic reviews, data extraction and meta-analysis with the \n\tmetagear package for R. Methods in Ecology and Evolution 7, 323-330 \n\t<doi:10.1111/2041-210X.12472>.",
    "version": "0.7",
    "maintainer": "Marc J. Lajeunesse <lajeunesse@usf.edu>",
    "author": "Marc J. Lajeunesse [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9678-2080>)",
    "url": "http://lajeunesse.myweb.usf.edu/ https://github.com/mjlajeunesse/\nhttps://www.youtube.com/c/LajeunesseLab/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metagear",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metagear Comprehensive Research Synthesis Tools for Systematic Reviews\nand Meta-Analysis Functionalities for facilitating systematic reviews, data\n    extractions, and meta-analyses. It includes a GUI (graphical user interface)\n    to help screen the abstracts and titles of bibliographic data; tools to assign\n    screening effort across multiple collaborators/reviewers and to assess inter-\n    reviewer reliability; tools to help automate the download and retrieval of\n    journal PDF articles from online databases; figure and image extractions \n    from PDFs; web scraping of citations; automated and manual data extraction \n    from scatter-plot and bar-plot images; PRISMA (Preferred Reporting Items for\n    Systematic Reviews and Meta-Analyses) flow diagrams; simple imputation tools\n    to fill gaps in incomplete or missing study parameters; generation of random\n    effects sizes for Hedges' d, log response ratio, odds ratio, and correlation\n    coefficients for Monte Carlo experiments; covariance equations for modelling\n    dependencies among multiple effect sizes (e.g., effect sizes with a common\n    control); and finally summaries that replicate analyses and outputs from \n    widely used but no longer updated meta-analysis software (i.e., metawin).\n\tFunding for this package was supported by National Science Foundation (NSF) \n\tgrants DBI-1262545 and DEB-1451031. CITE: Lajeunesse, M.J. (2016) \n\tFacilitating systematic reviews, data extraction and meta-analysis with the \n\tmetagear package for R. Methods in Ecology and Evolution 7, 323-330 \n\t<doi:10.1111/2041-210X.12472>.  "
  },
  {
    "id": 15907,
    "package_name": "metalite",
    "title": "ADaM Metadata Structure",
    "description": "A metadata structure for clinical data analysis\n    and reporting based on Analysis Data Model (ADaM) datasets.\n    The package simplifies clinical analysis and reporting tool development\n    by defining standardized inputs, outputs, and workflow.\n    The package can be used to create analysis and reporting planning grid,\n    mock table, and validated analysis and reporting results based on\n    consistent inputs.",
    "version": "0.1.4",
    "maintainer": "Yujie Zhao <yujie.zhao@merck.com>",
    "author": "Yilong Zhang [aut],\n  Yujie Zhao [aut, cre],\n  Nan Xiao [aut],\n  Benjamin Wang [ctb],\n  Brian Lang [ctb],\n  Howard Baek [ctb],\n  Ruchitbhai Patel [ctb],\n  Madhusudhan Ginnaram [ctb],\n  Sarad Nepal [ctb],\n  Venkatesh Burla [ctb],\n  Merck Sharp & Dohme Corp [cph]",
    "url": "https://merck.github.io/metalite/,\nhttps://github.com/Merck/metalite",
    "bug_reports": "https://github.com/Merck/metalite/issues",
    "repository": "https://cran.r-project.org/package=metalite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metalite ADaM Metadata Structure A metadata structure for clinical data analysis\n    and reporting based on Analysis Data Model (ADaM) datasets.\n    The package simplifies clinical analysis and reporting tool development\n    by defining standardized inputs, outputs, and workflow.\n    The package can be used to create analysis and reporting planning grid,\n    mock table, and validated analysis and reporting results based on\n    consistent inputs.  "
  },
  {
    "id": 15908,
    "package_name": "metalite.ae",
    "title": "Adverse Events Analysis Using 'metalite'",
    "description": "Analyzes adverse events in clinical trials using the 'metalite'\n    data structure. The package simplifies the workflow to create\n    production-ready tables, listings, and figures discussed in the\n    adverse events analysis chapters of\n    \"R for Clinical Study Reports and Submission\"\n    by Zhang et al. (2022) <https://r4csr.org/>.",
    "version": "0.1.3",
    "maintainer": "Yujie Zhao <yujie.zhao@merck.com>",
    "author": "Yilong Zhang [aut],\n  Yujie Zhao [aut, cre],\n  Benjamin Wang [aut],\n  Nan Xiao [aut],\n  Sarad Nepal [aut],\n  Madhusudhan Ginnaram [aut],\n  Venkatesh Burla [ctb],\n  Ruchitbhai Patel [aut],\n  Brian Lang [aut],\n  Xuan Deng [aut],\n  Hiroaki Fukuda [aut],\n  Bing Liu [aut],\n  Jeetender Chauhan [aut],\n  Li Ma [ctb],\n  Merck Sharp & Dohme Corp [cph]",
    "url": "https://merck.github.io/metalite.ae/,\nhttps://github.com/Merck/metalite.ae",
    "bug_reports": "https://github.com/Merck/metalite.ae/issues",
    "repository": "https://cran.r-project.org/package=metalite.ae",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metalite.ae Adverse Events Analysis Using 'metalite' Analyzes adverse events in clinical trials using the 'metalite'\n    data structure. The package simplifies the workflow to create\n    production-ready tables, listings, and figures discussed in the\n    adverse events analysis chapters of\n    \"R for Clinical Study Reports and Submission\"\n    by Zhang et al. (2022) <https://r4csr.org/>.  "
  },
  {
    "id": 15909,
    "package_name": "metalite.sl",
    "title": "Subject-Level Analysis Using 'metalite'",
    "description": "Analyzes subject-level data in clinical trials using the 'metalite'\n    data structure. The package simplifies the workflow to create\n    production-ready tables, listings, and figures discussed in the\n    subject-level analysis chapters of\n    \"R for Clinical Study Reports and Submission\"\n    by Zhang et al. (2022) <https://r4csr.org/>.",
    "version": "0.1.1",
    "maintainer": "Benjamin Wang <benjamin.wang@merck.com>",
    "author": "Benjamin Wang [aut, cre],\n  Yujie Zhao [aut],\n  Hiroaki Fukuda [aut],\n  PoYao Niu [aut],\n  Nan Xiao [aut],\n  Jeetener Chauhan [ctb],\n  Li Ma [ctb],\n  Chen Wang [ctb],\n  Merck Sharp & Dohme Corp [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metalite.sl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metalite.sl Subject-Level Analysis Using 'metalite' Analyzes subject-level data in clinical trials using the 'metalite'\n    data structure. The package simplifies the workflow to create\n    production-ready tables, listings, and figures discussed in the\n    subject-level analysis chapters of\n    \"R for Clinical Study Reports and Submission\"\n    by Zhang et al. (2022) <https://r4csr.org/>.  "
  },
  {
    "id": 15911,
    "package_name": "metamedian",
    "title": "Meta-Analysis of Medians",
    "description": "Implements several methods to meta-analyze studies that report the \n    sample median of the outcome. The methods described by \n    McGrath et al. (2019) <doi:10.1002/sim.8013>, \n    Ozturk and Balakrishnan (2020) <doi:10.1002/sim.8738>, and \n    McGrath et al. (2020a) <doi:10.1002/bimj.201900036> can be \n    applied to directly meta-analyze the median or difference of medians between \n    groups. Additionally, a number of methods (e.g., McGrath et al. (2020b) \n    <doi:10.1177/0962280219889080>, Cai et al. (2021) \n    <doi:10.1177/09622802211047348>, and McGrath et al. (2023) \n    <doi:10.1177/09622802221139233>) are implemented to estimate \n    study-specific (difference of) means and their standard errors in order to \n    estimate the pooled (difference of) means. Methods for meta-analyzing median \n    survival times (McGrath et al. (2025) <doi:10.48550/arXiv.2503.03065>) are \n    also implemented. See McGrath et al. (2024) <doi:10.1002/jrsm.1686> for a \n    detailed guide on using the package.",
    "version": "1.2.1",
    "maintainer": "Sean McGrath <sean.mcgrath514@gmail.com>",
    "author": "Sean McGrath [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7281-3516>),\n  XiaoFei Zhao [aut],\n  Stephan Katzenschlager [aut],\n  Omer Ozturk [aut],\n  Renata Iskander [ctb],\n  Russell Steele [aut],\n  Andrea Benedetti [aut] (ORCID: <https://orcid.org/0000-0002-8314-9497>)",
    "url": "https://github.com/stmcg/metamedian,\nhttps://doi.org/10.1002/jrsm.1686",
    "bug_reports": "https://github.com/stmcg/metamedian/issues",
    "repository": "https://cran.r-project.org/package=metamedian",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metamedian Meta-Analysis of Medians Implements several methods to meta-analyze studies that report the \n    sample median of the outcome. The methods described by \n    McGrath et al. (2019) <doi:10.1002/sim.8013>, \n    Ozturk and Balakrishnan (2020) <doi:10.1002/sim.8738>, and \n    McGrath et al. (2020a) <doi:10.1002/bimj.201900036> can be \n    applied to directly meta-analyze the median or difference of medians between \n    groups. Additionally, a number of methods (e.g., McGrath et al. (2020b) \n    <doi:10.1177/0962280219889080>, Cai et al. (2021) \n    <doi:10.1177/09622802211047348>, and McGrath et al. (2023) \n    <doi:10.1177/09622802221139233>) are implemented to estimate \n    study-specific (difference of) means and their standard errors in order to \n    estimate the pooled (difference of) means. Methods for meta-analyzing median \n    survival times (McGrath et al. (2025) <doi:10.48550/arXiv.2503.03065>) are \n    also implemented. See McGrath et al. (2024) <doi:10.1002/jrsm.1686> for a \n    detailed guide on using the package.  "
  },
  {
    "id": 15918,
    "package_name": "metansue",
    "title": "Meta-Analysis of Studies with Non-Statistically Significant\nUnreported Effects",
    "description": "Novel method to unbiasedly include studies with Non-statistically Significant Unreported Effects (NSUEs) in a meta-analysis. First, the function calculates the interval where the unreported effects (e.g., t-values) should be according to the threshold of statistical significance used in each study. Afterward, the method uses maximum likelihood techniques to impute the expected effect size of each study with NSUEs, accounting for between-study heterogeneity and potential covariates. Multiple imputations of the NSUEs are then randomly created based on the expected value, variance, and statistical significance bounds. Finally, it conducts a restricted-maximum likelihood random-effects meta-analysis separately for each set of imputations, and it performs estimations from these meta-analyses. Please read the reference in 'metansue' for details of the procedure.",
    "version": "2.6",
    "maintainer": "Joaquim Radua <quimradua@gmail.com>",
    "author": "Joaquim Radua [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1240-5438>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metansue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metansue Meta-Analysis of Studies with Non-Statistically Significant\nUnreported Effects Novel method to unbiasedly include studies with Non-statistically Significant Unreported Effects (NSUEs) in a meta-analysis. First, the function calculates the interval where the unreported effects (e.g., t-values) should be according to the threshold of statistical significance used in each study. Afterward, the method uses maximum likelihood techniques to impute the expected effect size of each study with NSUEs, accounting for between-study heterogeneity and potential covariates. Multiple imputations of the NSUEs are then randomly created based on the expected value, variance, and statistical significance bounds. Finally, it conducts a restricted-maximum likelihood random-effects meta-analysis separately for each set of imputations, and it performs estimations from these meta-analyses. Please read the reference in 'metansue' for details of the procedure.  "
  },
  {
    "id": 15929,
    "package_name": "metasens",
    "title": "Statistical Methods for Sensitivity Analysis in Meta-Analysis",
    "description": "The following methods are implemented to evaluate how sensitive the results of a meta-analysis are to potential bias in meta-analysis and to support Schwarzer et al. (2015) <DOI:10.1007/978-3-319-21416-0>, Chapter 5 'Small-Study Effects in Meta-Analysis':\n - Copas selection model described in Copas & Shi (2001) <DOI:10.1177/096228020101000402>;\n - limit meta-analysis by R\u00fccker et al. (2011) <DOI:10.1093/biostatistics/kxq046>;\n - upper bound for outcome reporting bias by Copas & Jackson (2004) <DOI:10.1111/j.0006-341X.2004.00161.x>;\n - imputation methods for missing binary data by Gamble & Hollis (2005) <DOI:10.1016/j.jclinepi.2004.09.013> and Higgins et al. (2008) <DOI:10.1177/1740774508091600>;\n - LFK index test and Doi plot by Furuya-Kanamori et al. (2018) <DOI:10.1097/XEB.0000000000000141>.",
    "version": "1.5-3",
    "maintainer": "Guido Schwarzer <guido.schwarzer@uniklinik-freiburg.de>",
    "author": "Guido Schwarzer [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-6214-9087>),\n  James R. Carpenter [aut] (ORCID:\n    <https://orcid.org/0000-0003-3890-6206>),\n  Gerta R\u00fccker [aut] (ORCID: <https://orcid.org/0000-0002-2192-2560>)",
    "url": "https://github.com/guido-s/metasens,\nhttps://link.springer.com/book/10.1007/978-3-319-21416-0",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metasens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metasens Statistical Methods for Sensitivity Analysis in Meta-Analysis The following methods are implemented to evaluate how sensitive the results of a meta-analysis are to potential bias in meta-analysis and to support Schwarzer et al. (2015) <DOI:10.1007/978-3-319-21416-0>, Chapter 5 'Small-Study Effects in Meta-Analysis':\n - Copas selection model described in Copas & Shi (2001) <DOI:10.1177/096228020101000402>;\n - limit meta-analysis by R\u00fccker et al. (2011) <DOI:10.1093/biostatistics/kxq046>;\n - upper bound for outcome reporting bias by Copas & Jackson (2004) <DOI:10.1111/j.0006-341X.2004.00161.x>;\n - imputation methods for missing binary data by Gamble & Hollis (2005) <DOI:10.1016/j.jclinepi.2004.09.013> and Higgins et al. (2008) <DOI:10.1177/1740774508091600>;\n - LFK index test and Doi plot by Furuya-Kanamori et al. (2018) <DOI:10.1097/XEB.0000000000000141>.  "
  },
  {
    "id": 15931,
    "package_name": "metasplines",
    "title": "Pool Literature-Based and Individual Participant Data Based\nSpline Estimates",
    "description": "Pooling estimates reported in meta-analyses (literature-based, LB) and estimates based on individual participant data (IPD) is not straight-forward as the details of the LB nonlinear function estimate are not usually reported. This package pools the nonlinear IPD dose-response estimates based on a natural cubic spline from lm or glm with the pointwise LB estimates and their estimated variances. Details will be presented in H\u00e4rk\u00e4nen, Tapanainen, Sares-J\u00e4ske, M\u00e4nnist\u00f6, Kaartinen and Paalanen (2025) \"Novel pooling method for nonlinear cohort analysis and meta-analysis estimates: Predicting health outcomes based on climate-friendly diets\" (under revision) <https://journals.lww.com/epidem/pages/default.aspx>.",
    "version": "0.1.0",
    "maintainer": "Tommi H\u00e4rk\u00e4nen <tommi.harkanen@thl.fi>",
    "author": "Tommi H\u00e4rk\u00e4nen [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metasplines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metasplines Pool Literature-Based and Individual Participant Data Based\nSpline Estimates Pooling estimates reported in meta-analyses (literature-based, LB) and estimates based on individual participant data (IPD) is not straight-forward as the details of the LB nonlinear function estimate are not usually reported. This package pools the nonlinear IPD dose-response estimates based on a natural cubic spline from lm or glm with the pointwise LB estimates and their estimated variances. Details will be presented in H\u00e4rk\u00e4nen, Tapanainen, Sares-J\u00e4ske, M\u00e4nnist\u00f6, Kaartinen and Paalanen (2025) \"Novel pooling method for nonlinear cohort analysis and meta-analysis estimates: Predicting health outcomes based on climate-friendly diets\" (under revision) <https://journals.lww.com/epidem/pages/default.aspx>.  "
  },
  {
    "id": 15933,
    "package_name": "metathis",
    "title": "HTML Metadata Tags for 'R Markdown' and 'Shiny'",
    "description": "Create meta tags for 'R Markdown' HTML documents and 'Shiny'\n    apps for customized social media cards, for accessibility, and quality\n    search engine indexing. 'metathis' currently supports HTML documents\n    created with 'rmarkdown', 'shiny', 'xaringan', 'pagedown', 'bookdown',\n    and 'flexdashboard'.",
    "version": "1.1.4",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>)",
    "url": "https://pkg.garrickadenbuie.com/metathis/,\nhttps://github.com/gadenbuie/metathis",
    "bug_reports": "https://github.com/gadenbuie/metathis/issues",
    "repository": "https://cran.r-project.org/package=metathis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metathis HTML Metadata Tags for 'R Markdown' and 'Shiny' Create meta tags for 'R Markdown' HTML documents and 'Shiny'\n    apps for customized social media cards, for accessibility, and quality\n    search engine indexing. 'metathis' currently supports HTML documents\n    created with 'rmarkdown', 'shiny', 'xaringan', 'pagedown', 'bookdown',\n    and 'flexdashboard'.  "
  },
  {
    "id": 15947,
    "package_name": "metools",
    "title": "Macroeconomics Tools",
    "description": "Provides a number of functions to facilitate the handling and production of reports using time series data.\n    The package was developed to be understandable for beginners, so some functions aim to transform processes that would be\n    complex into functions with a few lines. The main advantage of using the 'metools' package is the ease of producing reports and\n    working with time series using a few lines of code, so the code is clean and easy to understand/maintain. \n    Learn more about the 'metools' at <https://metoolsr.wordpress.com>.",
    "version": "1.0.0",
    "maintainer": "Jo\u00e3o Victor Gomes de Araujo Santana <jvg.santana@gmail.com>",
    "author": "Jo\u00e3o Victor Gomes de Araujo Santana [aut, cre]",
    "url": "https://metoolsr.wordpress.com,https://github.com/jvg0mes/metools,https://jvg0mes.github.io/metoolsr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metools Macroeconomics Tools Provides a number of functions to facilitate the handling and production of reports using time series data.\n    The package was developed to be understandable for beginners, so some functions aim to transform processes that would be\n    complex into functions with a few lines. The main advantage of using the 'metools' package is the ease of producing reports and\n    working with time series using a few lines of code, so the code is clean and easy to understand/maintain. \n    Learn more about the 'metools' at <https://metoolsr.wordpress.com>.  "
  },
  {
    "id": 15972,
    "package_name": "mgi.report.reader",
    "title": "Read Mouse Genome Informatics Reports",
    "description": "Provides readers for easy and consistent importing of\n    Mouse Genome Informatics (MGI) report files:\n    <https://www.informatics.jax.org/downloads/reports/index.html>. These data\n    are provided by Baldarelli RM, Smith CL, Ringwald M, Richardson JE, Bult CJ,\n    Mouse Genome Informatics Group (2024) <doi:10.1093/genetics/iyae031>.",
    "version": "0.1.3",
    "maintainer": "Ramiro Magno <rmagno@pattern.institute>",
    "author": "Ramiro Magno [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5226-3441>),\n  David Shaw [aut] (ORCID: <https://orcid.org/0000-0002-9922-9934>),\n  Isabel Duarte [aut] (ORCID: <https://orcid.org/0000-0003-0060-2936>),\n  Ismail Gbadamosi [aut] (ORCID: <https://orcid.org/0000-0002-1932-7519>),\n  Ali Jawaid [aut] (ORCID: <https://orcid.org/0000-0002-5126-6744>),\n  Nencki Institute of Experimental Biology [fnd],\n  University of Algarve [fnd],\n  The Jackson Laboratory [fnd],\n  Pattern Institute [cph, fnd]",
    "url": "https://www.pattern.institute/mgi.report.reader/,\nhttps://github.com/patterninstitute/mgi.report.reader/",
    "bug_reports": "https://github.com/patterninstitute/mgi.report.reader/issues",
    "repository": "https://cran.r-project.org/package=mgi.report.reader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mgi.report.reader Read Mouse Genome Informatics Reports Provides readers for easy and consistent importing of\n    Mouse Genome Informatics (MGI) report files:\n    <https://www.informatics.jax.org/downloads/reports/index.html>. These data\n    are provided by Baldarelli RM, Smith CL, Ringwald M, Richardson JE, Bult CJ,\n    Mouse Genome Informatics Group (2024) <doi:10.1093/genetics/iyae031>.  "
  },
  {
    "id": 16000,
    "package_name": "micar",
    "title": "'Mica' Data Web Portal Client",
    "description": "'Mica' is a server application used to create data web portals for \n    large-scale epidemiological studies or multiple-study consortia. 'Mica' helps\n    studies to provide scientifically robust data visibility and web presence \n    without significant information technology effort. 'Mica' provides a \n    structured description of consortia, studies, annotated and searchable data\n    dictionaries, and data access request management. This 'Mica' client allows\n    to perform data extraction for reporting purposes.",
    "version": "1.1.2",
    "maintainer": "Yannick Marcon <yannick.marcon@obiba.org>",
    "author": "Yannick Marcon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0138-2023>),\n  OBiBa group [cph]",
    "url": "https://www.obiba.org/ https://www.obiba.org/pages/products/mica/\nhttps://doi.org/10.1093/ije/dyx180",
    "bug_reports": "https://github.com/obiba/micar",
    "repository": "https://cran.r-project.org/package=micar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "micar 'Mica' Data Web Portal Client 'Mica' is a server application used to create data web portals for \n    large-scale epidemiological studies or multiple-study consortia. 'Mica' helps\n    studies to provide scientifically robust data visibility and web presence \n    without significant information technology effort. 'Mica' provides a \n    structured description of consortia, studies, annotated and searchable data\n    dictionaries, and data access request management. This 'Mica' client allows\n    to perform data extraction for reporting purposes.  "
  },
  {
    "id": 16009,
    "package_name": "michelRodange",
    "title": "The Works (in Luxembourguish) of Michel Rodange",
    "description": "Michel Rodange was a Luxembourguish writer and poet who lived in the 19th century. \n    His most notable work is Rodange (1872, ISBN:1166177424), (\"Renert oder de Fuu\u00df am Frack an a Ma'nsgr\u00eb\u00dft\"),\n    but he also wrote many more works, including Rodange, Tockert (1928) <https://www.autorenlexikon.lu/page/document/361/3614/1/FRE/index.html> \n    (\"D'L\u00e9ierchen - Dem L\u00e9iweckerche s\u00e4i Lidd\") and Rodange, Welter (1929) <https://www.autorenlexikon.lu/page/document/361/3615/1/FRE/index.html>\n    (\"Dem Grow Sigfrid seng Goldkuommer\"). This package contains three datasets, each made from the\n    plain text versions of his works available on \n    <https://data.public.lu/fr/datasets/the-works-in-luxembourguish-of-michel-rodange/>.",
    "version": "1.0.0",
    "maintainer": "Bruno Andr\u00e9 Rodrigues coelho <bruno@brodrigues.co>",
    "author": "Bruno Andr\u00e9 Rodrigues coelho [aut, cre]",
    "url": "https://github.com/b-rodrigues/michelRodange",
    "bug_reports": "https://github.com/b-rodrigues/michelRodange/issues",
    "repository": "https://cran.r-project.org/package=michelRodange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "michelRodange The Works (in Luxembourguish) of Michel Rodange Michel Rodange was a Luxembourguish writer and poet who lived in the 19th century. \n    His most notable work is Rodange (1872, ISBN:1166177424), (\"Renert oder de Fuu\u00df am Frack an a Ma'nsgr\u00eb\u00dft\"),\n    but he also wrote many more works, including Rodange, Tockert (1928) <https://www.autorenlexikon.lu/page/document/361/3614/1/FRE/index.html> \n    (\"D'L\u00e9ierchen - Dem L\u00e9iweckerche s\u00e4i Lidd\") and Rodange, Welter (1929) <https://www.autorenlexikon.lu/page/document/361/3615/1/FRE/index.html>\n    (\"Dem Grow Sigfrid seng Goldkuommer\"). This package contains three datasets, each made from the\n    plain text versions of his works available on \n    <https://data.public.lu/fr/datasets/the-works-in-luxembourguish-of-michel-rodange/>.  "
  },
  {
    "id": 16017,
    "package_name": "microbiomeMQC",
    "title": "Calculate 4 Key Reporting Measures",
    "description": "Perform calculations for the WHO International Reference Reagents for the microbiome. Using strain, species or genera abundance tables generated through analysis of 16S ribosomal RNA sequencing or shotgun sequencing which included a reference reagent. This package will calculate measures of sensitivity, False positive relative abundance, diversity, and similarity based on mean average abundances with respect to the reference reagent.",
    "version": "1.0.2",
    "maintainer": "Jacob Dehinsilu <jacobdehinsilu@outlook.com>",
    "author": "Jacob Dehinsilu [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9041-970X>),\n  Ravneet Bhuller [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=microbiomeMQC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "microbiomeMQC Calculate 4 Key Reporting Measures Perform calculations for the WHO International Reference Reagents for the microbiome. Using strain, species or genera abundance tables generated through analysis of 16S ribosomal RNA sequencing or shotgun sequencing which included a reference reagent. This package will calculate measures of sensitivity, False positive relative abundance, diversity, and similarity based on mean average abundances with respect to the reference reagent.  "
  },
  {
    "id": 16027,
    "package_name": "microplot",
    "title": "Microplots (Sparklines) in 'LaTeX', 'Word', 'HTML', 'Excel'",
    "description": "The microplot function writes a set of R graphics files to be used as\n  microplots (sparklines) in tables in either 'LaTeX', 'HTML', 'Word',\n  or 'Excel' files.  For 'LaTeX', we provide methods for the\n  Hmisc::latex() generic function to construct 'latex' tabular\n  environments which include the graphs.  These can be used directly\n  with the operating system 'pdflatex' or 'latex' command, or by using\n  one of 'Sweave', 'knitr', 'rmarkdown', or 'Emacs org-mode' as an\n  intermediary.  For 'MS Word', the msWord() function uses the\n  'flextable' package to construct 'Word' tables which include the\n  graphs.  There are several distinct approaches for constructing HTML\n  files.  The simplest is to use the msWord() function with argument\n  filetype=\"html\".  Alternatively, use either 'Emacs org-mode' or the\n  htmlTable::htmlTable() function to construct an 'HTML' file\n  containing tables which include the graphs.  See the documentation\n  for our as.htmlimg() function.  For 'Excel' use on 'Windows', the\n  file examples/irisExcel.xls includes 'VBA' code which brings the\n  individual panels into individual cells in the spreadsheet.\n  Examples in the examples and demo subdirectories are shown with\n  'lattice' graphics, 'ggplot2' graphics, and 'base' graphics.\n  Examples for 'LaTeX' include 'Sweave' (both 'LaTeX'-style and\n  'Noweb'-style), 'knitr', 'emacs org-mode', and 'rmarkdown' input\n  files and their 'pdf' output files.  Examples for 'HTML' include\n  'org-mode' and 'Rmd' input files and their webarchive 'HTML' output\n  files.  In addition, the as.orgtable() function can display a\n  data.frame in an 'org-mode' document.  The examples for 'MS Word'\n  (with either filetype=\"docx\" or filetype=\"html\") work with all\n  operating systems.  The package does not require the installation of\n  'LaTeX' or 'MS Word' to be able to write '.tex' or '.docx' files.",
    "version": "1.0-47",
    "maintainer": "Richard M. Heiberger <rmh@temple.edu>",
    "author": "Richard M. Heiberger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9047-6931>),\n  Karen Byron [ctb],\n  Nooreen Dabbish [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=microplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "microplot Microplots (Sparklines) in 'LaTeX', 'Word', 'HTML', 'Excel' The microplot function writes a set of R graphics files to be used as\n  microplots (sparklines) in tables in either 'LaTeX', 'HTML', 'Word',\n  or 'Excel' files.  For 'LaTeX', we provide methods for the\n  Hmisc::latex() generic function to construct 'latex' tabular\n  environments which include the graphs.  These can be used directly\n  with the operating system 'pdflatex' or 'latex' command, or by using\n  one of 'Sweave', 'knitr', 'rmarkdown', or 'Emacs org-mode' as an\n  intermediary.  For 'MS Word', the msWord() function uses the\n  'flextable' package to construct 'Word' tables which include the\n  graphs.  There are several distinct approaches for constructing HTML\n  files.  The simplest is to use the msWord() function with argument\n  filetype=\"html\".  Alternatively, use either 'Emacs org-mode' or the\n  htmlTable::htmlTable() function to construct an 'HTML' file\n  containing tables which include the graphs.  See the documentation\n  for our as.htmlimg() function.  For 'Excel' use on 'Windows', the\n  file examples/irisExcel.xls includes 'VBA' code which brings the\n  individual panels into individual cells in the spreadsheet.\n  Examples in the examples and demo subdirectories are shown with\n  'lattice' graphics, 'ggplot2' graphics, and 'base' graphics.\n  Examples for 'LaTeX' include 'Sweave' (both 'LaTeX'-style and\n  'Noweb'-style), 'knitr', 'emacs org-mode', and 'rmarkdown' input\n  files and their 'pdf' output files.  Examples for 'HTML' include\n  'org-mode' and 'Rmd' input files and their webarchive 'HTML' output\n  files.  In addition, the as.orgtable() function can display a\n  data.frame in an 'org-mode' document.  The examples for 'MS Word'\n  (with either filetype=\"docx\" or filetype=\"html\") work with all\n  operating systems.  The package does not require the installation of\n  'LaTeX' or 'MS Word' to be able to write '.tex' or '.docx' files.  "
  },
  {
    "id": 16029,
    "package_name": "microsimulation",
    "title": "Discrete Event Simulation in R and C++, with Tools for\nCost-Effectiveness Analysis",
    "description": "Discrete event simulation using both R and C++ (Karlsson et al 2016; <doi:10.1109/eScience.2016.7870915>). The C++ code is adapted from the SSIM library <https://www.inf.usi.ch/carzaniga/ssim/>, allowing for event-oriented simulation. The code includes a SummaryReport class for reporting events and costs by age and other covariates. The C++ code is available as a static library for linking to other packages. A priority queue implementation is given in C++ together with an S3 closure and a reference class implementation. Finally, some tools are provided for cost-effectiveness analysis.",
    "version": "1.4.5",
    "maintainer": "Mark Clements <mark.clements@ki.se>",
    "author": "Mark Clements [aut, cre, cph],\n  Alexandra Jauhiainen [aut],\n  Andreas Karlsson [aut],\n  Antonio Carzaniga [cph],\n  University of Colorado [cph],\n  Pierre L'Ecuyer [cph]",
    "url": "https://github.com/mclements/microsimulation",
    "bug_reports": "https://github.com/mclements/microsimulation/issues",
    "repository": "https://cran.r-project.org/package=microsimulation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "microsimulation Discrete Event Simulation in R and C++, with Tools for\nCost-Effectiveness Analysis Discrete event simulation using both R and C++ (Karlsson et al 2016; <doi:10.1109/eScience.2016.7870915>). The C++ code is adapted from the SSIM library <https://www.inf.usi.ch/carzaniga/ssim/>, allowing for event-oriented simulation. The code includes a SummaryReport class for reporting events and costs by age and other covariates. The C++ code is available as a static library for linking to other packages. A priority queue implementation is given in C++ together with an S3 closure and a reference class implementation. Finally, some tools are provided for cost-effectiveness analysis.  "
  },
  {
    "id": 16041,
    "package_name": "midoc",
    "title": "A Decision-Making System for Multiple Imputation",
    "description": "A guidance system for analysis with missing data. It incorporates expert, up-to-date methodology to help researchers choose the most appropriate analysis approach when some data are missing. You provide the available data and the assumed causal structure, including the likely causes of missing data. 'midoc' will advise which analysis approaches can be used, and how best to perform them. 'midoc' follows the framework for the treatment and reporting of missing data in observational studies (TARMOS). Lee et al (2021). <doi:10.1016/j.jclinepi.2021.01.008>. ",
    "version": "1.0.0",
    "maintainer": "Elinor Curnow <elinor.curnow@bristol.ac.uk>",
    "author": "Elinor Curnow [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3109-3647>),\n  Jon Heron [aut],\n  Rosie Cornish [aut],\n  Kate Tilling [aut],\n  James Carpenter [aut]",
    "url": "https://elliecurnow.github.io/midoc/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=midoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "midoc A Decision-Making System for Multiple Imputation A guidance system for analysis with missing data. It incorporates expert, up-to-date methodology to help researchers choose the most appropriate analysis approach when some data are missing. You provide the available data and the assumed causal structure, including the likely causes of missing data. 'midoc' will advise which analysis approaches can be used, and how best to perform them. 'midoc' follows the framework for the treatment and reporting of missing data in observational studies (TARMOS). Lee et al (2021). <doi:10.1016/j.jclinepi.2021.01.008>.   "
  },
  {
    "id": 16044,
    "package_name": "miebl",
    "title": "Performance Criteria Modeler for Discrete Trial Training",
    "description": "Provides a tool for computing probabilities and other quantities that are relevant in selecting performance criteria for discrete trial training. The main function, miebl(), computes Bayesian and frequentist probabilities and bounds for each of n possible performance criterion choices when attempting to determine a student's true mastery level by counting their number of successful attempts at displaying learning among n trials. The reporting function miebl_re() takes output from miebl() and prepares it into a brief report for a specific criterion. miebl_cp() combines 2 to 5 distributions of true mastery level given performance criterion in one plot for comparison. Ramos (2025) <doi:10.1007/s40617-025-01058-9>.",
    "version": "0.1.0",
    "maintainer": "Mark Ramos <mlr6219@psu.edu>",
    "author": "Mark Ramos [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=miebl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "miebl Performance Criteria Modeler for Discrete Trial Training Provides a tool for computing probabilities and other quantities that are relevant in selecting performance criteria for discrete trial training. The main function, miebl(), computes Bayesian and frequentist probabilities and bounds for each of n possible performance criterion choices when attempting to determine a student's true mastery level by counting their number of successful attempts at displaying learning among n trials. The reporting function miebl_re() takes output from miebl() and prepares it into a brief report for a specific criterion. miebl_cp() combines 2 to 5 distributions of true mastery level given performance criterion in one plot for comparison. Ramos (2025) <doi:10.1007/s40617-025-01058-9>.  "
  },
  {
    "id": 16054,
    "package_name": "mikropml",
    "title": "User-Friendly R Package for Supervised Machine Learning\nPipelines",
    "description": "An interface to build machine learning models for\n    classification and regression problems. 'mikropml' implements the ML\n    pipeline described by Top\u00e7uo\u011flu et al. (2020)\n    <doi:10.1128/mBio.00434-20> with reasonable default options for data\n    preprocessing, hyperparameter tuning, cross-validation, testing, model\n    evaluation, and interpretation steps.  See the website\n    <https://www.schlosslab.org/mikropml/> for more information,\n    documentation, and examples.",
    "version": "1.7.0",
    "maintainer": "Kelly Sovacool <sovacool@umich.edu>",
    "author": "Beg\u00fcm Top\u00e7uo\u011flu [aut] (ORCID: <https://orcid.org/0000-0003-3140-537X>),\n  Zena Lapp [aut] (ORCID: <https://orcid.org/0000-0003-4674-2176>),\n  Kelly Sovacool [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3283-829X>),\n  Evan Snitkin [aut] (ORCID: <https://orcid.org/0000-0001-8409-278X>),\n  Jenna Wiens [aut] (ORCID: <https://orcid.org/0000-0002-1057-7722>),\n  Patrick Schloss [aut] (ORCID: <https://orcid.org/0000-0002-6935-4275>),\n  Nick Lesniak [ctb] (ORCID: <https://orcid.org/0000-0001-9359-5194>),\n  Courtney Armour [ctb] (ORCID: <https://orcid.org/0000-0002-5250-1224>),\n  Sarah Lucas [ctb] (ORCID: <https://orcid.org/0000-0003-1676-5801>),\n  Tuomas Borman [ctb] (ORCID: <https://orcid.org/0000-0002-8563-8884>)",
    "url": "https://www.schlosslab.org/mikropml/,\nhttps://github.com/SchlossLab/mikropml",
    "bug_reports": "https://github.com/SchlossLab/mikropml/issues",
    "repository": "https://cran.r-project.org/package=mikropml",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mikropml User-Friendly R Package for Supervised Machine Learning\nPipelines An interface to build machine learning models for\n    classification and regression problems. 'mikropml' implements the ML\n    pipeline described by Top\u00e7uo\u011flu et al. (2020)\n    <doi:10.1128/mBio.00434-20> with reasonable default options for data\n    preprocessing, hyperparameter tuning, cross-validation, testing, model\n    evaluation, and interpretation steps.  See the website\n    <https://www.schlosslab.org/mikropml/> for more information,\n    documentation, and examples.  "
  },
  {
    "id": 16081,
    "package_name": "minidown",
    "title": "Create Simple Yet Powerful HTML Documents with Light Weight CSS\nFrameworks",
    "description": "Create minimal, responsive, and style-agnostic HTML documents with\n    the lightweight CSS frameworks such as 'sakura', 'Water.css', and 'spcss'.\n    Powerful features include table of contents floating as a sidebar,\n    folding codes and results, and more.",
    "version": "0.4.0",
    "maintainer": "Atsushi Yasumoto <atusy.rpkg@gmail.com>",
    "author": "Atsushi Yasumoto [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-8335-495X>),\n  Angelos Chalaris [aut] (mini.css),\n  Susam Pal [aut] (spcss),\n  Mitesh Shah [aut] (sakura),\n  Kognise [aut] (Water.css)",
    "url": "https://minidown.atusy.net, https://github.com/atusy/minidown",
    "bug_reports": "https://github.com/atusy/minidown/issues",
    "repository": "https://cran.r-project.org/package=minidown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minidown Create Simple Yet Powerful HTML Documents with Light Weight CSS\nFrameworks Create minimal, responsive, and style-agnostic HTML documents with\n    the lightweight CSS frameworks such as 'sakura', 'Water.css', and 'spcss'.\n    Powerful features include table of contents floating as a sidebar,\n    folding codes and results, and more.  "
  },
  {
    "id": 16088,
    "package_name": "minioclient",
    "title": "Interface to the 'MinIO' Client",
    "description": "An R interface to the 'MinIO' Client. The 'MinIO' Client ('mc')\n  provides a modern alternative to UNIX commands like 'ls', 'cat', 'cp',\n  'mirror', 'diff', 'find' etc. It supports 'filesystems'  and Amazon \"S3\"\n  compatible cloud storage service (\"AWS\" Signature v2 and v4).\n  This package provides convenience functions for installing the 'MinIO'\n  client and running any operations, as described in the official \n  documentation, <https://min.io/docs/minio/linux/reference/minio-mc.html?ref=docs-redirect>.\n  This package provides a flexible and high-performance alternative to 'aws.s3'.",
    "version": "0.0.6",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>),\n  Markus Skyttner [ctb]",
    "url": "https://github.com/cboettig/minioclient,\nhttps://cboettig.github.io/minioclient/",
    "bug_reports": "https://github.com/cboettig/minioclient/issues",
    "repository": "https://cran.r-project.org/package=minioclient",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minioclient Interface to the 'MinIO' Client An R interface to the 'MinIO' Client. The 'MinIO' Client ('mc')\n  provides a modern alternative to UNIX commands like 'ls', 'cat', 'cp',\n  'mirror', 'diff', 'find' etc. It supports 'filesystems'  and Amazon \"S3\"\n  compatible cloud storage service (\"AWS\" Signature v2 and v4).\n  This package provides convenience functions for installing the 'MinIO'\n  client and running any operations, as described in the official \n  documentation, <https://min.io/docs/minio/linux/reference/minio-mc.html?ref=docs-redirect>.\n  This package provides a flexible and high-performance alternative to 'aws.s3'.  "
  },
  {
    "id": 16089,
    "package_name": "minipdf",
    "title": "PDF Document Creator",
    "description": "PDF is a standard file format for laying out\n    text and images in documents.  At its core, these documents are sequences of \n    objects defined in plain text.  This package allows for the creation\n    of PDF documents at a very low level without any library or graphics\n    device dependencies.",
    "version": "0.2.7",
    "maintainer": "Mike Cheng <mikefc@coolbutuseless.com>",
    "author": "Mike Cheng [aut, cre, cph]",
    "url": "https://github.com/coolbutuseless/minipdf",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minipdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minipdf PDF Document Creator PDF is a standard file format for laying out\n    text and images in documents.  At its core, these documents are sequences of \n    objects defined in plain text.  This package allows for the creation\n    of PDF documents at a very low level without any library or graphics\n    device dependencies.  "
  },
  {
    "id": 16136,
    "package_name": "misuvi",
    "title": "Access the Michigan Substance Use Vulnerability Index (MI-SUVI)",
    "description": "Easily import the MI-SUVI data sets. The user can import data sets with full metrics, percentiles, Z-scores, or rankings.\n    Data is available at both the County and Zip Code Tabulation Area (ZCTA) levels. This package also includes a function to import \n    shape files for easy mapping and a function to access the full technical documentation. All data is sourced from the Michigan Department of Health and Human Services.",
    "version": "0.1.1",
    "maintainer": "Brenden Smith <brendensmithmi@gmail.com>",
    "author": "Brenden Smith [aut, cre, cph]",
    "url": "https://github.com/brendensm/misuvi",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=misuvi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "misuvi Access the Michigan Substance Use Vulnerability Index (MI-SUVI) Easily import the MI-SUVI data sets. The user can import data sets with full metrics, percentiles, Z-scores, or rankings.\n    Data is available at both the County and Zip Code Tabulation Area (ZCTA) levels. This package also includes a function to import \n    shape files for easy mapping and a function to access the full technical documentation. All data is sourced from the Michigan Department of Health and Human Services.  "
  },
  {
    "id": 16160,
    "package_name": "mixedbiastest",
    "title": "Bias Diagnostic for Linear Mixed Models",
    "description": "Provides a function to perform bias diagnostics on linear mixed models fitted with lmer() from the 'lme4' package. Implements permutation tests for assessing the bias of fixed effects, as described in Karl and Zimmerman (2021) <doi:10.1016/j.jspi.2020.06.004>. Karl and Zimmerman (2020) <doi:10.17632/tmynggddfm.1> provide R code for implementing the test using 'mvglmmRank' output. Development of this package was assisted by 'GPT o1-preview' for code structure and documentation.",
    "version": "1.0.2",
    "maintainer": "Andrew T. Karl <akarl@asu.edu>",
    "author": "Andrew T. Karl [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-5933-8706>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mixedbiastest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixedbiastest Bias Diagnostic for Linear Mixed Models Provides a function to perform bias diagnostics on linear mixed models fitted with lmer() from the 'lme4' package. Implements permutation tests for assessing the bias of fixed effects, as described in Karl and Zimmerman (2021) <doi:10.1016/j.jspi.2020.06.004>. Karl and Zimmerman (2020) <doi:10.17632/tmynggddfm.1> provide R code for implementing the test using 'mvglmmRank' output. Development of this package was assisted by 'GPT o1-preview' for code structure and documentation.  "
  },
  {
    "id": 16176,
    "package_name": "mixtur",
    "title": "Modelling Continuous Report Visual Short-Term Memory Studies",
    "description": "A set of utility functions for analysing and modelling data from \n    continuous report short-term memory experiments using either the 2-component\n    mixture model of Zhang and Luck (2008) <doi:10.1038/nature06860> or the \n    3-component mixture model of Bays et al. (2009) <doi:10.1167/9.10.7>. Users \n    are also able to simulate from these models.",
    "version": "1.2.2",
    "maintainer": "Jim Grange <grange.jim@gmail.com>",
    "author": "Jim Grange [aut, cre] (ORCID: <https://orcid.org/0000-0002-8352-8390>),\n  Stuart B. Moore [aut] (ORCID: <https://orcid.org/0000-0002-0747-9304>),\n  Ed D. J. Berry [ctb],\n  Vencislav Popov [ctb] (ORCID: <https://orcid.org/0000-0002-8073-4199>)",
    "url": "https://github.com/JimGrange/mixtur",
    "bug_reports": "https://github.com/JimGrange/mixtur/issues",
    "repository": "https://cran.r-project.org/package=mixtur",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixtur Modelling Continuous Report Visual Short-Term Memory Studies A set of utility functions for analysing and modelling data from \n    continuous report short-term memory experiments using either the 2-component\n    mixture model of Zhang and Luck (2008) <doi:10.1038/nature06860> or the \n    3-component mixture model of Bays et al. (2009) <doi:10.1167/9.10.7>. Users \n    are also able to simulate from these models.  "
  },
  {
    "id": 16182,
    "package_name": "mkin",
    "title": "Kinetic Evaluation of Chemical Degradation Data",
    "description": "Calculation routines based on the FOCUS Kinetics Report (2006,\n  2014).  Includes a function for conveniently defining differential equation\n  models, model solution based on eigenvalues if possible or using numerical\n  solvers.  If a C compiler (on windows: 'Rtools') is installed, differential\n  equation models are solved using automatically generated C functions.\n  Non-constant errors can be taken into account using variance by variable or\n  two-component error models <doi:10.3390/environments6120124>.  Hierarchical\n  degradation models can be fitted using nonlinear mixed-effects model packages\n  as a back end <doi:10.3390/environments8080071>.  Please\n  note that no warranty is implied for correctness of results or fitness for a\n  particular purpose.",
    "version": "1.2.10",
    "maintainer": "Johannes Ranke <johannes.ranke@jrwb.de>",
    "author": "Johannes Ranke [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4371-6538>),\n  Katrin Lindenberger [ctb] (contributed to mkinresplot()),\n  Ren\u00e9 Lehmann [ctb] (ilr() and invilr()),\n  Eurofins Regulatory AG [cph] (copyright for some of the contributions\n    of JR 2012-2014)",
    "url": "https://pkgdown.jrwb.de/mkin/",
    "bug_reports": "https://github.com/jranke/mkin/issues/",
    "repository": "https://cran.r-project.org/package=mkin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mkin Kinetic Evaluation of Chemical Degradation Data Calculation routines based on the FOCUS Kinetics Report (2006,\n  2014).  Includes a function for conveniently defining differential equation\n  models, model solution based on eigenvalues if possible or using numerical\n  solvers.  If a C compiler (on windows: 'Rtools') is installed, differential\n  equation models are solved using automatically generated C functions.\n  Non-constant errors can be taken into account using variance by variable or\n  two-component error models <doi:10.3390/environments6120124>.  Hierarchical\n  degradation models can be fitted using nonlinear mixed-effects model packages\n  as a back end <doi:10.3390/environments8080071>.  Please\n  note that no warranty is implied for correctness of results or fitness for a\n  particular purpose.  "
  },
  {
    "id": 16207,
    "package_name": "mlmhelpr",
    "title": "Multilevel/Mixed Model Helper Functions",
    "description": "A collection of miscellaneous helper function for running multilevel/mixed models in 'lme4'. This package aims to provide functions to compute common tasks when estimating multilevel models such as computing the intraclass correlation and design effect, centering variables, estimating the proportion of variance explained at each level, pseudo-R squared, random intercept and slope reliabilities, tests for homogeneity of variance at level-1, and cluster robust and bootstrap standard errors. The tests and statistics reported in the package are from Raudenbush & Bryk (2002, ISBN:9780761919049), Hox et al. (2018, ISBN:9781138121362), and Snijders & Bosker (2012, ISBN:9781849202015). ",
    "version": "0.1.1",
    "maintainer": "Louis Rocconi <lrocconi@utk.edu>",
    "author": "Louis Rocconi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0693-0957>),\n  Anthony Schmidt [aut] (ORCID: <https://orcid.org/0000-0003-4478-0638>)",
    "url": "https://github.com/lrocconi/mlmhelpr",
    "bug_reports": "https://github.com/lrocconi/mlmhelpr/issues",
    "repository": "https://cran.r-project.org/package=mlmhelpr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlmhelpr Multilevel/Mixed Model Helper Functions A collection of miscellaneous helper function for running multilevel/mixed models in 'lme4'. This package aims to provide functions to compute common tasks when estimating multilevel models such as computing the intraclass correlation and design effect, centering variables, estimating the proportion of variance explained at each level, pseudo-R squared, random intercept and slope reliabilities, tests for homogeneity of variance at level-1, and cluster robust and bootstrap standard errors. The tests and statistics reported in the package are from Raudenbush & Bryk (2002, ISBN:9780761919049), Hox et al. (2018, ISBN:9781138121362), and Snijders & Bosker (2012, ISBN:9781849202015).   "
  },
  {
    "id": 16225,
    "package_name": "mlr3fairness",
    "title": "Fairness Auditing and Debiasing for 'mlr3'",
    "description": "Integrates fairness auditing and bias mitigation methods for\n    the 'mlr3' ecosystem.  This includes fairness metrics, reporting\n    tools, visualizations and bias mitigation techniques such as\n    \"Reweighing\" described in 'Kamiran, Calders' (2012)\n    <doi:10.1007/s10115-011-0463-8> and \"Equalized Odds\" described in\n    'Hardt et al.' (2016)\n    <https://papers.nips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf>.\n    Integration with 'mlr3' allows for auditing of ML models as well as\n    convenient joint tuning of machine learning algorithms and debiasing\n    methods.",
    "version": "0.4.0",
    "maintainer": "Florian Pfisterer <pfistererf@googlemail.com>",
    "author": "Florian Pfisterer [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-8867-762X>),\n  Wei Siyi [aut],\n  Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>)",
    "url": "https://mlr3fairness.mlr-org.com,\nhttps://github.com/mlr-org/mlr3fairness",
    "bug_reports": "https://github.com/mlr-org/mlr3fairness/issues",
    "repository": "https://cran.r-project.org/package=mlr3fairness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3fairness Fairness Auditing and Debiasing for 'mlr3' Integrates fairness auditing and bias mitigation methods for\n    the 'mlr3' ecosystem.  This includes fairness metrics, reporting\n    tools, visualizations and bias mitigation techniques such as\n    \"Reweighing\" described in 'Kamiran, Calders' (2012)\n    <doi:10.1007/s10115-011-0463-8> and \"Equalized Odds\" described in\n    'Hardt et al.' (2016)\n    <https://papers.nips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf>.\n    Integration with 'mlr3' allows for auditing of ML models as well as\n    convenient joint tuning of machine learning algorithms and debiasing\n    methods.  "
  },
  {
    "id": 16254,
    "package_name": "mlstrOpalr",
    "title": "Support Compatibility Between 'Maelstrom' R Packages and 'Opal'\nEnvironment",
    "description": "Functions to support compatibility between 'Maelstrom' R packages \n     and 'Opal' environment. 'Opal' is the 'OBiBa' core database application for \n     biobanks. It is used to build data repositories that integrates data \n     collected from multiple sources. 'Opal Maelstrom' is a specific \n     implementation of this software. This 'Opal' client is specifically \n     designed to interact with 'Opal Maelstrom' distributions to perform \n     operations on the R server side. The user must have adequate credentials.\n     Please see <https://opaldoc.obiba.org/> for complete documentation.",
    "version": "1.0.3",
    "maintainer": "Guillaume Fabre <guijoseph.fabre@gmail.com>",
    "author": "Guillaume Fabre [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0124-9970>),\n  Maelstrom-research group [fnd, cph],\n  OBiBa group [ctb]",
    "url": "https://github.com/maelstrom-research/mlstrOpalr",
    "bug_reports": "https://github.com/maelstrom-research/mlstrOpalr/issues",
    "repository": "https://cran.r-project.org/package=mlstrOpalr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlstrOpalr Support Compatibility Between 'Maelstrom' R Packages and 'Opal'\nEnvironment Functions to support compatibility between 'Maelstrom' R packages \n     and 'Opal' environment. 'Opal' is the 'OBiBa' core database application for \n     biobanks. It is used to build data repositories that integrates data \n     collected from multiple sources. 'Opal Maelstrom' is a specific \n     implementation of this software. This 'Opal' client is specifically \n     designed to interact with 'Opal Maelstrom' distributions to perform \n     operations on the R server side. The user must have adequate credentials.\n     Please see <https://opaldoc.obiba.org/> for complete documentation.  "
  },
  {
    "id": 16258,
    "package_name": "mlts",
    "title": "Multilevel Latent Time Series Models with 'R' and 'Stan'",
    "description": "Fit multilevel manifest or latent time-series models, including popular Dynamic Structural Equation Models (DSEM).\n  The models can be set up and modified with user-friendly functions and are fit to the data using 'Stan' for Bayesian inference.\n  Path models and formulas for user-defined models can be easily created with functions using 'knitr'. \n  Asparouhov, Hamaker, & Muthen (2018) <doi:10.1080/10705511.2017.1406803>.",
    "version": "2.0.1",
    "maintainer": "Kenneth Koslowski <kenneth.koslowski@uni-leipzig.de>",
    "author": "Kenneth Koslowski [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5296-5267>),\n  Fabian M\u00fcnch [aut] (ORCID: <https://orcid.org/0000-0001-5591-9901>),\n  Tobias Koch [aut] (ORCID: <https://orcid.org/0000-0002-8143-3566>),\n  Jana Holtmann [aut] (ORCID: <https://orcid.org/0000-0002-7949-0772>)",
    "url": "https://github.com/munchfab/mlts",
    "bug_reports": "https://github.com/munchfab/mlts/issues",
    "repository": "https://cran.r-project.org/package=mlts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlts Multilevel Latent Time Series Models with 'R' and 'Stan' Fit multilevel manifest or latent time-series models, including popular Dynamic Structural Equation Models (DSEM).\n  The models can be set up and modified with user-friendly functions and are fit to the data using 'Stan' for Bayesian inference.\n  Path models and formulas for user-defined models can be easily created with functions using 'knitr'. \n  Asparouhov, Hamaker, & Muthen (2018) <doi:10.1080/10705511.2017.1406803>.  "
  },
  {
    "id": 16282,
    "package_name": "mmodely",
    "title": "Modeling Multivariate Origins Determinants - Evolutionary\nLineages in Ecology",
    "description": "\n Perform multivariate modeling of evolved traits, with special attention to\n understanding the interplay of the multi-factorial determinants of their origins\n in complex ecological settings (Stephens, 2007 <doi:10.1016/j.tree.2006.12.003>).\n This software primarily concentrates on phylogenetic regression analysis, enabling\n implementation of tree transformation averaging and visualization functionality.\n Functions additionally support information theoretic approaches\n (Grueber, 2011 <doi:10.1111/j.1420-9101.2010.02210.x>;\n Garamszegi, 2011 <doi:10.1007/s00265-010-1028-7>)\n such as  model averaging and selection of phylogenetic models.\n Accessory functions are also implemented for coef standardization (Cade 2015), \n selection uncertainty, and variable importance (Burnham & Anderson 2000).\n There are other numerous functions for visualizing confounded variables,\n plotting phylogenetic trees, as well as reporting and exporting modeling results.\n Lastly, as challenges to ecology are inherently multifarious, and therefore often\n multi-dataset, this package features several functions to support the identification,\n interpolation, merging, and updating of missing data and outdated nomenclature.",
    "version": "0.2.5",
    "maintainer": "David M Schruth <dschruth@anthropoidea.org>",
    "author": "David M Schruth",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mmodely",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmodely Modeling Multivariate Origins Determinants - Evolutionary\nLineages in Ecology \n Perform multivariate modeling of evolved traits, with special attention to\n understanding the interplay of the multi-factorial determinants of their origins\n in complex ecological settings (Stephens, 2007 <doi:10.1016/j.tree.2006.12.003>).\n This software primarily concentrates on phylogenetic regression analysis, enabling\n implementation of tree transformation averaging and visualization functionality.\n Functions additionally support information theoretic approaches\n (Grueber, 2011 <doi:10.1111/j.1420-9101.2010.02210.x>;\n Garamszegi, 2011 <doi:10.1007/s00265-010-1028-7>)\n such as  model averaging and selection of phylogenetic models.\n Accessory functions are also implemented for coef standardization (Cade 2015), \n selection uncertainty, and variable importance (Burnham & Anderson 2000).\n There are other numerous functions for visualizing confounded variables,\n plotting phylogenetic trees, as well as reporting and exporting modeling results.\n Lastly, as challenges to ecology are inherently multifarious, and therefore often\n multi-dataset, this package features several functions to support the identification,\n interpolation, merging, and updating of missing data and outdated nomenclature.  "
  },
  {
    "id": 16288,
    "package_name": "mmstat4",
    "title": "Access to Teaching Materials from a ZIP File or GitHub",
    "description": "Provides access to teaching materials for various statistics courses, including R and Python programs, \n    Shiny apps, data, and PDF/HTML documents. These materials are stored on the Internet as a ZIP file \n    (e.g., in a GitHub repository) and can be downloaded and displayed or run locally. The content of the ZIP file \n    is temporarily or permanently stored. By default, the package uses the GitHub repository \n    'sigbertklinke/mmstat4.data.' Additionally, the package includes 'association_measures.R' \n    from the archived package 'ryouready' by Mark Heckman and some auxiliary functions.",
    "version": "0.2.1",
    "maintainer": "Sigbert Klinke <sigbert@hu-berlin.de>",
    "author": "Sigbert Klinke [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3337-1863>),\n  Jekaterina Zukovska [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7753-9210>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mmstat4",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmstat4 Access to Teaching Materials from a ZIP File or GitHub Provides access to teaching materials for various statistics courses, including R and Python programs, \n    Shiny apps, data, and PDF/HTML documents. These materials are stored on the Internet as a ZIP file \n    (e.g., in a GitHub repository) and can be downloaded and displayed or run locally. The content of the ZIP file \n    is temporarily or permanently stored. By default, the package uses the GitHub repository \n    'sigbertklinke/mmstat4.data.' Additionally, the package includes 'association_measures.R' \n    from the archived package 'ryouready' by Mark Heckman and some auxiliary functions.  "
  },
  {
    "id": 16325,
    "package_name": "modeltools",
    "title": "Tools and Classes for Statistical Models",
    "description": "A collection of tools to deal with statistical models. \n  The functionality is experimental and the user interface is likely to\n  change in the future. The documentation is rather terse, but packages `coin'\n  and `party' have some working examples. However, if you find the\n  implemented ideas interesting we would be very interested in a discussion\n  of this proposal. Contributions are more than welcome!",
    "version": "0.2-24",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "author": "Torsten Hothorn [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8301-0471>),\n  Friedrich Leisch [aut] (ORCID: <https://orcid.org/0000-0001-7278-1983>),\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=modeltools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modeltools Tools and Classes for Statistical Models A collection of tools to deal with statistical models. \n  The functionality is experimental and the user interface is likely to\n  change in the future. The documentation is rather terse, but packages `coin'\n  and `party' have some working examples. However, if you find the\n  implemented ideas interesting we would be very interested in a discussion\n  of this proposal. Contributions are more than welcome!  "
  },
  {
    "id": 16355,
    "package_name": "monaco",
    "title": "The 'Monaco' Editor as a HTML Widget",
    "description": "A HTML widget rendering the 'Monaco' editor. The 'Monaco' editor is the code editor which powers 'VS Code'. It is particularly well developed for 'JavaScript'. In addition to the built-in features of the 'Monaco' editor, the widget allows to prettify multiple languages, to view the 'HTML' rendering of 'Markdown' code, and to view and resize 'SVG' images.",
    "version": "0.2.2",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Microsoft Corporation [ctb, cph] ('Monaco Editor' library),\n  James Long and contributors [ctb, cph] ('Prettier' library),\n  Rich Harris [ctb, cph] ('svg-parser' library),\n  Lionel Tzatzkin [ctb, cph] ('scale-that-svg' library),\n  Andrei Kashcha [ctb, cph] ('panzoom' library),\n  Vitaly Puzrin [ctb, cph] ('markdown-it' library),\n  Alex Kocharin [ctb, cph] ('markdown-it' library),\n  John Schlinkert [ctb, cph] ('word-wrap' library),\n  jQuery contributors [ctb, cph] ('jQuery' library),\n  Kyle Fox [ctb, cph] ('jQuery Modal' library),\n  Tristan Edwards [ctb, cph] ('sweetalert2' library),\n  Limon Monte [ctb, cph] ('sweetalert2' library)",
    "url": "https://github.com/stla/monaco",
    "bug_reports": "https://github.com/stla/monaco/issues",
    "repository": "https://cran.r-project.org/package=monaco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monaco The 'Monaco' Editor as a HTML Widget A HTML widget rendering the 'Monaco' editor. The 'Monaco' editor is the code editor which powers 'VS Code'. It is particularly well developed for 'JavaScript'. In addition to the built-in features of the 'Monaco' editor, the widget allows to prettify multiple languages, to view the 'HTML' rendering of 'Markdown' code, and to view and resize 'SVG' images.  "
  },
  {
    "id": 16378,
    "package_name": "moodlequiz",
    "title": "R Markdown format for 'Moodle' XML cloze quizzes",
    "description": "Enables the creation of 'Moodle' quiz questions using literate\n  programming with R Markdown. This makes it easy to quickly create a quiz that\n  can be randomly replicated with new datasets, questions, and options for \n  answers.",
    "version": "0.2.0",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "author": "Mitchell O'Hara-Wild [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3813-7155>),\n  Emi Tanaka [aut] (ORCID: <https://orcid.org/0000-0002-1455-259X>)",
    "url": "https://github.com/numbats/moodlequiz,\nhttps://numbats.github.io/moodlequiz/",
    "bug_reports": "https://github.com/numbats/moodlequiz/issues",
    "repository": "https://cran.r-project.org/package=moodlequiz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "moodlequiz R Markdown format for 'Moodle' XML cloze quizzes Enables the creation of 'Moodle' quiz questions using literate\n  programming with R Markdown. This makes it easy to quickly create a quiz that\n  can be randomly replicated with new datasets, questions, and options for \n  answers.  "
  },
  {
    "id": 16388,
    "package_name": "morph",
    "title": "3D Segmentation of Voxels into Morphologic Classes",
    "description": "Automatically segments a 3D array of voxels into mutually exclusive morphological elements. This package extends existing work for segmenting 2D binary raster data. A paper documenting this approach has been accepted for publication in the journal Landscape Ecology. Detailed references will be updated here once those are known.",
    "version": "1.1.0",
    "maintainer": "Tarmo K. Remmel <remmelt@yorku.ca>",
    "author": "Tarmo K. Remmel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6251-876X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=morph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morph 3D Segmentation of Voxels into Morphologic Classes Automatically segments a 3D array of voxels into mutually exclusive morphological elements. This package extends existing work for segmenting 2D binary raster data. A paper documenting this approach has been accepted for publication in the journal Landscape Ecology. Detailed references will be updated here once those are known.  "
  },
  {
    "id": 16474,
    "package_name": "mschart",
    "title": "Chart Generation for 'Microsoft Word' and 'Microsoft PowerPoint'\nDocuments",
    "description": "Create native charts for 'Microsoft PowerPoint' and 'Microsoft Word' documents. \n These can then be edited and annotated. Functions are provided to let users create charts, modify \n and format their content. The chart's underlying data is automatically saved within the \n 'Word' document or 'PowerPoint' presentation. It extends package 'officer' that does \n not contain any feature for 'Microsoft' native charts production. ",
    "version": "0.4.1",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  ArData [cph],\n  YouGov [fnd],\n  Jan Marvin Garbuszus [ctb] (support for openxls2),\n  Stefan Moog [ctb] (support to set chart and plot area color and border),\n  Eli Daniels [ctb],\n  Marlon Molina [ctb] (added table feature),\n  Rokas Klydzia [ctb] (custom labels),\n  David Camposeco [ctb] (chart_data_smooth function),\n  Dan Joplin [ctb] (fix scatter plot data structure)",
    "url": "https://ardata-fr.github.io/officeverse/,\nhttps://ardata-fr.github.io/mschart/",
    "bug_reports": "https://github.com/ardata-fr/mschart/issues",
    "repository": "https://cran.r-project.org/package=mschart",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mschart Chart Generation for 'Microsoft Word' and 'Microsoft PowerPoint'\nDocuments Create native charts for 'Microsoft PowerPoint' and 'Microsoft Word' documents. \n These can then be edited and annotated. Functions are provided to let users create charts, modify \n and format their content. The chart's underlying data is automatically saved within the \n 'Word' document or 'PowerPoint' presentation. It extends package 'officer' that does \n not contain any feature for 'Microsoft' native charts production.   "
  },
  {
    "id": 16507,
    "package_name": "mtb",
    "title": "My Toolbox for Assisting Document Editing and Data Presenting",
    "description": "\n    The purpose of this package is to share a collection of functions the author wrote during weekends for managing\n    kitchen and garden tasks, e.g. making plant growth charts or Thanksgiving kitchen schedule charts, etc. \n    Functions might include but not limited to:\n    (1) aiding summarizing time related data; \n    (2) generating axis transformation from data; and\n    (3) aiding Markdown (with html output) and Shiny file editing.",
    "version": "0.1.9",
    "maintainer": "Y Hsu <yh202109@gmail.com>",
    "author": "Y Hsu [aut, cre]",
    "url": "https://github.com/yh202109/mtb",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mtb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mtb My Toolbox for Assisting Document Editing and Data Presenting \n    The purpose of this package is to share a collection of functions the author wrote during weekends for managing\n    kitchen and garden tasks, e.g. making plant growth charts or Thanksgiving kitchen schedule charts, etc. \n    Functions might include but not limited to:\n    (1) aiding summarizing time related data; \n    (2) generating axis transformation from data; and\n    (3) aiding Markdown (with html output) and Shiny file editing.  "
  },
  {
    "id": 16522,
    "package_name": "muir",
    "title": "Exploring Data with Tree Data Structures",
    "description": "A simple tool allowing users to easily and dynamically explore or document a data set using a tree structure.",
    "version": "0.1.0",
    "maintainer": "Justin Alford <justin.alford@gmail.com>",
    "author": "Justin Alford [aut, cre]",
    "url": "https://github.com/alforj/muir",
    "bug_reports": "https://github.com/alforj/muir/issues",
    "repository": "https://cran.r-project.org/package=muir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "muir Exploring Data with Tree Data Structures A simple tool allowing users to easily and dynamically explore or document a data set using a tree structure.  "
  },
  {
    "id": 16569,
    "package_name": "multilandr",
    "title": "Landscape Analysis at Multiple Spatial Scales",
    "description": "Provides a tidy workflow for landscape-scale analysis. 'multilandr' offers tools to generate landscapes at multiple spatial scales and compute landscape metrics, primarily using the 'landscapemetrics' package. It also features utility functions for plotting and analyzing multi-scale landscapes, exploring correlations between metrics, filtering landscapes based on specific conditions, generating landscape gradients for a given metric, and preparing datasets for further statistical analysis. Documentation about 'multilandr' is provided in an introductory vignette included in this package and in the paper by Huais (2024) <doi:10.1007/s10980-024-01930-z>; see citation(\"multilandr\") for details.",
    "version": "1.0.0",
    "maintainer": "Pablo Yair Huais <pablo.huais@unc.edu.ar>",
    "author": "Pablo Yair Huais [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4062-0779>)",
    "url": "https://github.com/phuais/multilandr",
    "bug_reports": "https://github.com/phuais/multilandr/issues",
    "repository": "https://cran.r-project.org/package=multilandr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multilandr Landscape Analysis at Multiple Spatial Scales Provides a tidy workflow for landscape-scale analysis. 'multilandr' offers tools to generate landscapes at multiple spatial scales and compute landscape metrics, primarily using the 'landscapemetrics' package. It also features utility functions for plotting and analyzing multi-scale landscapes, exploring correlations between metrics, filtering landscapes based on specific conditions, generating landscape gradients for a given metric, and preparing datasets for further statistical analysis. Documentation about 'multilandr' is provided in an introductory vignette included in this package and in the paper by Huais (2024) <doi:10.1007/s10980-024-01930-z>; see citation(\"multilandr\") for details.  "
  },
  {
    "id": 16613,
    "package_name": "multiverse",
    "title": "Create 'multiverse analysis' in R",
    "description": "Implement 'multiverse' style analyses (Steegen S., Tuerlinckx F, Gelman A., Vanpaemal, W., 2016)\n    <doi:10.1177/1745691616658637> to show the robustness of statistical inference. \n    'Multiverse analysis' is a philosophy of statistical reporting where paper authors \n    report the outcomes of many different statistical analyses in order to show how \n    fragile or robust their findings are. The 'multiverse' package \n    (Sarma A., Kale A., Moon M., Taback N., Chevalier F., Hullman J., Kay M., 2021) \n    <doi:10.31219/osf.io/yfbwm> allows users to concisely and flexibly implement 'multiverse-style' \n    analysis, which involve declaring alternate ways of performing an analysis step, in R and R Notebooks.",
    "version": "0.6.2",
    "maintainer": "Abhraneel Sarma <abhraneel@u.northwestern.edu>",
    "author": "Abhraneel Sarma [aut, cre],\n  Matthew Kay [aut],\n  Michael Moon [ctb],\n  Mark Miller [ctb],\n  Kyle Hwang [ctb],\n  Hadley Wickham [ctb],\n  Alex Kale [ctb],\n  Nathan Taback [ctb],\n  Fanny Chevalier [ctb],\n  Jessica Hullman [ctb],\n  Pierre Dragicevic [ctb],\n  Yvonne Jansen [ctb]",
    "url": "https://mucollective.github.io/multiverse/,\nhttps://github.com/mucollective/multiverse/",
    "bug_reports": "https://github.com/MUCollective/multiverse/issues/",
    "repository": "https://cran.r-project.org/package=multiverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multiverse Create 'multiverse analysis' in R Implement 'multiverse' style analyses (Steegen S., Tuerlinckx F, Gelman A., Vanpaemal, W., 2016)\n    <doi:10.1177/1745691616658637> to show the robustness of statistical inference. \n    'Multiverse analysis' is a philosophy of statistical reporting where paper authors \n    report the outcomes of many different statistical analyses in order to show how \n    fragile or robust their findings are. The 'multiverse' package \n    (Sarma A., Kale A., Moon M., Taback N., Chevalier F., Hullman J., Kay M., 2021) \n    <doi:10.31219/osf.io/yfbwm> allows users to concisely and flexibly implement 'multiverse-style' \n    analysis, which involve declaring alternate ways of performing an analysis step, in R and R Notebooks.  "
  },
  {
    "id": 16651,
    "package_name": "mvbutils",
    "title": "Workspace Organization, Code and Documentation Editing, Package\nPrep and Editing, Etc",
    "description": "Hierarchical workspace tree, code editing and backup, easy package prep, editing of packages while loaded, per-object lazy-loading, easy documentation, macro functions, and miscellaneous utilities. Needed by debug package.",
    "version": "2.8.232",
    "maintainer": "Mark V. Bravington <mark.bravington@csiro.au>",
    "author": "Mark V. Bravington <mark.bravington@csiro.au>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mvbutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvbutils Workspace Organization, Code and Documentation Editing, Package\nPrep and Editing, Etc Hierarchical workspace tree, code editing and backup, easy package prep, editing of packages while loaded, per-object lazy-loading, easy documentation, macro functions, and miscellaneous utilities. Needed by debug package.  "
  },
  {
    "id": 16659,
    "package_name": "mvglmmRank",
    "title": "Multivariate Generalized Linear Mixed Models for Ranking Sports\nTeams",
    "description": "Maximum likelihood estimates are obtained via an EM algorithm with either a first-order or a fully exponential Laplace approximation as documented by Broatch and Karl (2018) <doi:10.48550/arXiv.1710.05284>,\n    Karl, Yang, and Lohr (2014) <doi:10.1016/j.csda.2013.11.019>, and by \n\tKarl (2012) <doi:10.1515/1559-0410.1471>. Karl and Zimmerman <doi:10.1016/j.jspi.2020.06.004> use this package to illustrate how the home field effect estimator from a mixed model can be biased under nonrandom scheduling. ",
    "version": "1.2-4",
    "maintainer": "Andrew T. Karl <akarl@asu.edu>",
    "author": "Andrew T. Karl [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-5933-8706>),\n  Jennifer Broatch [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mvglmmRank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mvglmmRank Multivariate Generalized Linear Mixed Models for Ranking Sports\nTeams Maximum likelihood estimates are obtained via an EM algorithm with either a first-order or a fully exponential Laplace approximation as documented by Broatch and Karl (2018) <doi:10.48550/arXiv.1710.05284>,\n    Karl, Yang, and Lohr (2014) <doi:10.1016/j.csda.2013.11.019>, and by \n\tKarl (2012) <doi:10.1515/1559-0410.1471>. Karl and Zimmerman <doi:10.1016/j.jspi.2020.06.004> use this package to illustrate how the home field effect estimator from a mixed model can be biased under nonrandom scheduling.   "
  },
  {
    "id": 16705,
    "package_name": "n1qn1",
    "title": "Port of the 'Scilab' 'n1qn1' Module for Unconstrained BFGS\nOptimization",
    "description": "Provides 'Scilab' 'n1qn1'. This takes more memory than traditional L-BFGS.  The n1qn1 routine is useful since it allows prespecification of a Hessian.\n       If the Hessian is near enough the truth in optimization it can speed up the optimization problem. The algorithm is described in the\n       'Scilab' optimization documentation located at\n       <https://www.scilab.org/sites/default/files/optimization_in_scilab.pdf>. This version uses manually modified code from 'f2c' to make this a C only binary.",
    "version": "6.0.1-12",
    "maintainer": "Matthew Fidler <matthew.fidler@gmail.com>",
    "author": "Matthew Fidler [aut, cre],\n  Wenping Wang [aut],\n  Claude Lemarechal [aut, ctb],\n  Joseph Bonnans [ctb],\n  Jean-Charles Gilbert [ctb],\n  Claudia Sagastizabal [ctb],\n  Stephen L. Campbell, [ctb],\n  Jean-Philippe Chancelier [ctb],\n  Ramine Nikoukhah [ctb],\n  Dirk Eddelbuettel [ctb],\n  Bruno Jofret [ctb],\n  INRIA [cph]",
    "url": "https://github.com/nlmixr2/n1qn1c",
    "bug_reports": "https://github.com/nlmixr2/n1qn1c/issues",
    "repository": "https://cran.r-project.org/package=n1qn1",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "n1qn1 Port of the 'Scilab' 'n1qn1' Module for Unconstrained BFGS\nOptimization Provides 'Scilab' 'n1qn1'. This takes more memory than traditional L-BFGS.  The n1qn1 routine is useful since it allows prespecification of a Hessian.\n       If the Hessian is near enough the truth in optimization it can speed up the optimization problem. The algorithm is described in the\n       'Scilab' optimization documentation located at\n       <https://www.scilab.org/sites/default/files/optimization_in_scilab.pdf>. This version uses manually modified code from 'f2c' to make this a C only binary.  "
  },
  {
    "id": 16721,
    "package_name": "namer",
    "title": "Names Your 'R Markdown' Chunks",
    "description": "It names the 'R Markdown' chunks of files based on the\n    filename.",
    "version": "0.1.9",
    "maintainer": "Colin Gillespie <colin@jumpingrivers.com>",
    "author": "Colin Gillespie [aut, cre],\n  Steph Locke [aut],\n  Ma\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\n  Ellis Valentiner [ctb],\n  Charlie Hadley [ctb] (ORCID: <https://orcid.org/0000-0002-3039-6849>),\n  Jumping Rivers [fnd] (https://jumpingrivers.com),\n  Han Oostdijk [ctb] (ORCID: <https://orcid.org/0000-0001-6710-4566>),\n  Patrick Schratz [ctb] (ORCID: <https://orcid.org/0000-0003-0748-6624>)",
    "url": "https://github.com/jumpingrivers/namer,\nhttps://jumpingrivers.github.io/namer/",
    "bug_reports": "https://github.com/jumpingrivers/namer/issues",
    "repository": "https://cran.r-project.org/package=namer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "namer Names Your 'R Markdown' Chunks It names the 'R Markdown' chunks of files based on the\n    filename.  "
  },
  {
    "id": 16726,
    "package_name": "nanostringr",
    "title": "Performs Quality Control, Data Normalization, and Batch Effect\nCorrection for 'NanoString nCounter' Data",
    "description": "Provides quality control (QC), normalization, and batch\n    effect correction operations for 'NanoString nCounter' data, Talhouk\n    et al. (2016) <doi:10.1371/journal.pone.0153844>.  Various metrics are\n    used to determine which samples passed or failed QC.  Gene expression\n    should first be normalized to housekeeping genes, before a\n    reference-based approach is used to adjust for batch effects.  Raw\n    NanoString data can be imported in the form of Reporter Code Count\n    (RCC) files.",
    "version": "0.6.1",
    "maintainer": "Derek Chiu <dchiu@bccrc.ca>",
    "author": "Derek Chiu [aut, cre] (ORCID: <https://orcid.org/0000-0002-7591-4881>),\n  Aline Talhouk [aut] (ORCID: <https://orcid.org/0000-0001-7760-410X>),\n  Samuel Leung [aut] (ORCID: <https://orcid.org/0000-0003-0138-7254>)",
    "url": "https://github.com/TalhoukLab/nanostringr/,\nhttps://talhouklab.github.io/nanostringr/",
    "bug_reports": "https://github.com/TalhoukLab/nanostringr/issues",
    "repository": "https://cran.r-project.org/package=nanostringr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nanostringr Performs Quality Control, Data Normalization, and Batch Effect\nCorrection for 'NanoString nCounter' Data Provides quality control (QC), normalization, and batch\n    effect correction operations for 'NanoString nCounter' data, Talhouk\n    et al. (2016) <doi:10.1371/journal.pone.0153844>.  Various metrics are\n    used to determine which samples passed or failed QC.  Gene expression\n    should first be normalized to housekeeping genes, before a\n    reference-based approach is used to adjust for batch effects.  Raw\n    NanoString data can be imported in the form of Reporter Code Count\n    (RCC) files.  "
  },
  {
    "id": 16764,
    "package_name": "ncar",
    "title": "Noncompartmental Analysis for Pharmacokinetic Report",
    "description": "Conduct a noncompartmental analysis with industrial strength.\n             Some features are\n             1) CDISC SDTM terms\n             2) Automatic or manual slope selection\n             3) Supporting both 'linear-up linear-down' and 'linear-up log-down' method\n             4) Interval(partial) AUCs with 'linear' or 'log' interpolation method\n             5) Produce pdf, rtf, text report files.\n             * Reference: Gabrielsson J, Weiner D. Pharmacokinetic and Pharmacodynamic Data Analysis - Concepts and Applications. 5th ed. 2016. (ISBN:9198299107).",
    "version": "0.5.0",
    "maintainer": "Kyun-Seop Bae <k@acr.kr>",
    "author": "Kyun-Seop Bae [aut]",
    "url": "https://cran.r-project.org/package=ncar",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ncar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ncar Noncompartmental Analysis for Pharmacokinetic Report Conduct a noncompartmental analysis with industrial strength.\n             Some features are\n             1) CDISC SDTM terms\n             2) Automatic or manual slope selection\n             3) Supporting both 'linear-up linear-down' and 'linear-up log-down' method\n             4) Interval(partial) AUCs with 'linear' or 'log' interpolation method\n             5) Produce pdf, rtf, text report files.\n             * Reference: Gabrielsson J, Weiner D. Pharmacokinetic and Pharmacodynamic Data Analysis - Concepts and Applications. 5th ed. 2016. (ISBN:9198299107).  "
  },
  {
    "id": 16772,
    "package_name": "ncodeR",
    "title": "Techniques for Automated Classifiers",
    "description": "A set of techniques that can be used to develop, validate, and implement automated classifiers. A powerful tool for transforming raw data into meaningful information, 'ncodeR' (Shaffer, D. W. (2017) Quantitative Ethnography. ISBN: 0578191687) is designed specifically for working with big data: large document collections, logfiles, and other text data.",
    "version": "0.2.0.1",
    "maintainer": "Cody L Marquart <cody.marquart@wisc.edu>",
    "author": "Cody L Marquart [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3387-6792>),\n  Zachari Swiecki [aut],\n  Brendan Eagan [aut],\n  David Williamson Shaffer [aut]",
    "url": "",
    "bug_reports": "https://gitlab.com/epistemic-analytics/qe-packages/ncoder/issues",
    "repository": "https://cran.r-project.org/package=ncodeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ncodeR Techniques for Automated Classifiers A set of techniques that can be used to develop, validate, and implement automated classifiers. A powerful tool for transforming raw data into meaningful information, 'ncodeR' (Shaffer, D. W. (2017) Quantitative Ethnography. ISBN: 0578191687) is designed specifically for working with big data: large document collections, logfiles, and other text data.  "
  },
  {
    "id": 16782,
    "package_name": "neatR",
    "title": "Neat Data for Presentation",
    "description": "Utilities for unambiguous, neat and legible\n    representation of data (date, time stamp, numbers, percentages and strings) \n    for presentation of analysis , aiming for elegance and consistency.\n    The purpose of this package is to format data, that is better \n    for presentation and any automation jobs that reports numbers.",
    "version": "0.2.1",
    "maintainer": "Shivaprakash Suresh <dswithai@gmail.com>",
    "author": "Shivaprakash Suresh [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=neatR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neatR Neat Data for Presentation Utilities for unambiguous, neat and legible\n    representation of data (date, time stamp, numbers, percentages and strings) \n    for presentation of analysis , aiming for elegance and consistency.\n    The purpose of this package is to format data, that is better \n    for presentation and any automation jobs that reports numbers.  "
  },
  {
    "id": 16784,
    "package_name": "neatStats",
    "title": "Neat and Painless Statistical Reporting",
    "description": "User-friendly, clear and simple statistics, primarily for\n  publication in psychological science. The main functions are wrappers for\n  other packages, but there are various additions as well. Every relevant step\n  from data aggregation to reportable printed statistics is covered for basic\n  experimental designs.",
    "version": "1.13.5",
    "maintainer": "G\u00e1sp\u00e1r Luk\u00e1cs <lkcsgaspar@gmail.com>",
    "author": "G\u00e1sp\u00e1r Luk\u00e1cs [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9401-4830>),\n  Bennett Kleinberg [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1658-9086>),\n  Johnny van Doorn [ctb] (ORCID: <https://orcid.org/0000-0003-0270-096X>)",
    "url": "https://github.com/gasparl/neatstats",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=neatStats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neatStats Neat and Painless Statistical Reporting User-friendly, clear and simple statistics, primarily for\n  publication in psychological science. The main functions are wrappers for\n  other packages, but there are various additions as well. Every relevant step\n  from data aggregation to reportable printed statistics is covered for basic\n  experimental designs.  "
  },
  {
    "id": 16794,
    "package_name": "nemsqar",
    "title": "National Emergency Medical Service Quality Alliance Measure\nCalculations",
    "description": "Designed to automate the calculation of Emergency Medical\n    Service (EMS) quality metrics, 'nemsqar' implements measures defined\n    by the National EMS Quality Alliance (NEMSQA). By providing reliable,\n    evidence-based quality assessments, the package supports EMS agencies,\n    healthcare providers, and researchers in evaluating and improving\n    patient outcomes.  Users can find details on all approved NEMSQA\n    measures at <https://www.nemsqa.org/measures>.  Full technical\n    specifications, including documentation and pseudocode used to develop\n    'nemsqar', are available on the NEMSQA website after creating a user\n    profile at <https://www.nemsqa.org>.",
    "version": "1.1.2",
    "maintainer": "Nicolas Foss <nicolas.foss@hhs.iowa.gov>",
    "author": "Nicolas Foss [aut, cre],\n  Samuel Kordik [aut] (ORCID: <https://orcid.org/0000-0003-4230-1154>),\n  Alyssa Green [ctb],\n  Iowa Department of Health and Human Services [cph]",
    "url": "https://github.com/bemts-hhs/nemsqar,\nhttps://bemts-hhs.github.io/nemsqar/",
    "bug_reports": "https://github.com/bemts-hhs/nemsqar/issues",
    "repository": "https://cran.r-project.org/package=nemsqar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nemsqar National Emergency Medical Service Quality Alliance Measure\nCalculations Designed to automate the calculation of Emergency Medical\n    Service (EMS) quality metrics, 'nemsqar' implements measures defined\n    by the National EMS Quality Alliance (NEMSQA). By providing reliable,\n    evidence-based quality assessments, the package supports EMS agencies,\n    healthcare providers, and researchers in evaluating and improving\n    patient outcomes.  Users can find details on all approved NEMSQA\n    measures at <https://www.nemsqa.org/measures>.  Full technical\n    specifications, including documentation and pseudocode used to develop\n    'nemsqar', are available on the NEMSQA website after creating a user\n    profile at <https://www.nemsqa.org>.  "
  },
  {
    "id": 16804,
    "package_name": "neonUtilities",
    "title": "Utilities for Working with NEON Data",
    "description": "NEON data packages can be accessed through the NEON Data Portal <https://www.neonscience.org>\n    or through the NEON Data API (see <https://data.neonscience.org/data-api> for documentation). Data delivered from\n    the Data Portal are provided as monthly zip files packaged within a parent zip file, while individual files\n    can be accessed from the API. This package provides tools that aid in discovering, downloading, and reformatting \n    data prior to use in analyses. This includes downloading data via the API, merging data tables by type, and \n    converting formats. For more information, see the readme file at <https://github.com/NEONScience/NEON-utilities>.",
    "version": "3.0.2",
    "maintainer": "Claire Lunch <clunch@battelleecology.org>",
    "author": "Claire Lunch [aut, cre, ctb],\n  Christine Laney [aut, ctb],\n  Nathan Mietkiewicz [aut, ctb],\n  Eric Sokol [aut, ctb],\n  Kaelin Cawley [aut, ctb],\n  NEON (National Ecological Observatory Network) [aut]",
    "url": "https://github.com/NEONScience/NEON-utilities",
    "bug_reports": "https://github.com/NEONScience/NEON-utilities/issues",
    "repository": "https://cran.r-project.org/package=neonUtilities",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neonUtilities Utilities for Working with NEON Data NEON data packages can be accessed through the NEON Data Portal <https://www.neonscience.org>\n    or through the NEON Data API (see <https://data.neonscience.org/data-api> for documentation). Data delivered from\n    the Data Portal are provided as monthly zip files packaged within a parent zip file, while individual files\n    can be accessed from the API. This package provides tools that aid in discovering, downloading, and reformatting \n    data prior to use in analyses. This includes downloading data via the API, merging data tables by type, and \n    converting formats. For more information, see the readme file at <https://github.com/NEONScience/NEON-utilities>.  "
  },
  {
    "id": 16808,
    "package_name": "nestcolor",
    "title": "Colors for NEST Graphs",
    "description": "Clinical reporting figures require to use consistent colors\n    and configurations. As a part of the Roche open-source clinical\n    reporting project, namely the NEST project, the 'nestcolor' package\n    specifies the color code and default theme with specifying 'ggplot2'\n    theme parameters. Users can easily customize color and theme settings\n    before using the reset of NEST packages to ensure consistent settings\n    in both static and interactive output at the downstream.",
    "version": "0.1.3",
    "maintainer": "Joe Zhu <joe.zhu@roche.com>",
    "author": "Joe Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0001-7566-2787>),\n  Emily de la Rua [aut],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://insightsengineering.github.io/nestcolor/,\nhttps://github.com/insightsengineering/nestcolor/",
    "bug_reports": "https://github.com/insightsengineering/nestcolor/issues",
    "repository": "https://cran.r-project.org/package=nestcolor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nestcolor Colors for NEST Graphs Clinical reporting figures require to use consistent colors\n    and configurations. As a part of the Roche open-source clinical\n    reporting project, namely the NEST project, the 'nestcolor' package\n    specifies the color code and default theme with specifying 'ggplot2'\n    theme parameters. Users can easily customize color and theme settings\n    before using the reset of NEST packages to ensure consistent settings\n    in both static and interactive output at the downstream.  "
  },
  {
    "id": 16817,
    "package_name": "netShiny",
    "title": "Tool for Comparison and Visualization of Multiple Networks",
    "description": "We developed a comprehensive tool that helps with visualization and analysis of networks with the same variables across multiple factor levels. The 'netShiny' contains most of the popular network features such as centrality measures, modularity, and other summary statistics (e.g. clustering coefficient). It also contains known tools to look at the (dis)similarities between two networks, such as pairwise distance measures between networks, set operations on the nodes of the networks, distribution of the weights of the edges and a network representing the difference between two correlation matrices. The package 'netShiny' also contains tools to perform bootstrapping and find clusters in networks. See the 'netShiny' manual for more information, documentation and examples.",
    "version": "1.0",
    "maintainer": "Pariya Behrouzi <pariya.behrouzi@gmail.com>",
    "author": "Rocherno de Jongh [aut],\n  Pariya Behrouzi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6762-5433>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=netShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "netShiny Tool for Comparison and Visualization of Multiple Networks We developed a comprehensive tool that helps with visualization and analysis of networks with the same variables across multiple factor levels. The 'netShiny' contains most of the popular network features such as centrality measures, modularity, and other summary statistics (e.g. clustering coefficient). It also contains known tools to look at the (dis)similarities between two networks, such as pairwise distance measures between networks, set operations on the nodes of the networks, distribution of the weights of the edges and a network representing the difference between two correlation matrices. The package 'netShiny' also contains tools to perform bootstrapping and find clusters in networks. See the 'netShiny' manual for more information, documentation and examples.  "
  },
  {
    "id": 16827,
    "package_name": "netknitr",
    "title": "Knit Network Map for any Dataset",
    "description": "Designed to create interactive and visually compelling network maps using R Shiny. It allows users to quickly analyze CSV files and visualize complex relationships, structures, and connections within data by leveraging powerful network analysis libraries and dynamic web interfaces.",
    "version": "0.2.1",
    "maintainer": "Jayachandra N <itsjay510@gmail.com>",
    "author": "Jayachandra N [aut, cre],\n  Pushker Ravindra [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=netknitr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "netknitr Knit Network Map for any Dataset Designed to create interactive and visually compelling network maps using R Shiny. It allows users to quickly analyze CSV files and visualize complex relationships, structures, and connections within data by leveraging powerful network analysis libraries and dynamic web interfaces.  "
  },
  {
    "id": 16829,
    "package_name": "netmediate",
    "title": "Micro-Macro Analysis for Social Networks",
    "description": "Estimates micro effects on macro structures (MEMS) and average micro mediated effects (AMME).\n    URL: <https://github.com/sduxbury/netmediate>.\n    BugReports: <https://github.com/sduxbury/netmediate/issues>.\n    Robins, Garry, Phillipa Pattison, and Jodie Woolcock (2005) <doi:10.1086/427322>.\n    Snijders, Tom A. B., and Christian E. G. Steglich (2015) <doi:10.1177/0049124113494573>.\n    Imai, Kosuke, Luke Keele, and Dustin Tingley (2010) <doi:10.1037/a0020761>.\n    Duxbury, Scott (2023) <doi:10.1177/00811750231209040>.\n    Duxbury, Scott (2024) <doi:10.1177/00811750231220950>.",
    "version": "1.1.0",
    "maintainer": "Scott Duxbury <duxbury@email.unc.edu>",
    "author": "Scott Duxbury [aut, cre, cph],\n  Xin Zhao [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=netmediate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "netmediate Micro-Macro Analysis for Social Networks Estimates micro effects on macro structures (MEMS) and average micro mediated effects (AMME).\n    URL: <https://github.com/sduxbury/netmediate>.\n    BugReports: <https://github.com/sduxbury/netmediate/issues>.\n    Robins, Garry, Phillipa Pattison, and Jodie Woolcock (2005) <doi:10.1086/427322>.\n    Snijders, Tom A. B., and Christian E. G. Steglich (2015) <doi:10.1177/0049124113494573>.\n    Imai, Kosuke, Luke Keele, and Dustin Tingley (2010) <doi:10.1037/a0020761>.\n    Duxbury, Scott (2023) <doi:10.1177/00811750231209040>.\n    Duxbury, Scott (2024) <doi:10.1177/00811750231220950>.  "
  },
  {
    "id": 16866,
    "package_name": "neutralitytestr",
    "title": "Test for a Neutral Evolutionary Model in Cancer Sequencing Data",
    "description": "Package takes frequencies of mutations as reported by high throughput sequencing data from cancer and fits a theoretical neutral model of tumour evolution. Package outputs summary statistics and contains code for plotting the data and model fits. See Williams et al 2016 <doi:10.1038/ng.3489> and Williams et al 2017 <doi:10.1101/096305> for further details of the method.",
    "version": "0.0.3",
    "maintainer": "Marc Williams <marcjwilliams1@gmail.com>",
    "author": "Marc Williams [aut, cre]",
    "url": "https://github.com/marcjwilliams1/neutralitytestr",
    "bug_reports": "https://github.com/marcjwilliams1/neutralitytestr/issues",
    "repository": "https://cran.r-project.org/package=neutralitytestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neutralitytestr Test for a Neutral Evolutionary Model in Cancer Sequencing Data Package takes frequencies of mutations as reported by high throughput sequencing data from cancer and fits a theoretical neutral model of tumour evolution. Package outputs summary statistics and contains code for plotting the data and model fits. See Williams et al 2016 <doi:10.1038/ng.3489> and Williams et al 2017 <doi:10.1101/096305> for further details of the method.  "
  },
  {
    "id": 16878,
    "package_name": "newsmap",
    "title": "Semi-Supervised Model for Geographical Document Classification",
    "description": "Semissupervised model for geographical document classification (Watanabe 2018) <doi:10.1080/21670811.2017.1293487>. \n    This package currently contains seed dictionaries in English, German, French, Spanish, Italian, Russian, Hebrew, Arabic, Turkish, Japanese and Chinese (Simplified and Traditional).",
    "version": "0.9.2",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph],\n  Stefan M\u00fcller [aut],\n  Dani Madrid-Morales [aut],\n  Katerina Tertytchnaya [aut],\n  Ke Cheng [aut],\n  Chung-hong Chan [aut],\n  Claude Grasland [aut],\n  Giuseppe Carteny [aut],\n  Elad Segev [aut],\n  Dai Yamao [aut],\n  Barbara Ellynes Zucchi Nobre Silva [aut],\n  Lanabi la Lova [aut],\n  Lungta Seki [aut]",
    "url": "https://github.com/koheiw/newsmap",
    "bug_reports": "https://github.com/koheiw/newsmap/issues",
    "repository": "https://cran.r-project.org/package=newsmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "newsmap Semi-Supervised Model for Geographical Document Classification Semissupervised model for geographical document classification (Watanabe 2018) <doi:10.1080/21670811.2017.1293487>. \n    This package currently contains seed dictionaries in English, German, French, Spanish, Italian, Russian, Hebrew, Arabic, Turkish, Japanese and Chinese (Simplified and Traditional).  "
  },
  {
    "id": 16897,
    "package_name": "nhdplusTools",
    "title": "NHDPlus Tools",
    "description": "Tools for traversing and working with National Hydrography Dataset Plus (NHDPlus) data. All methods implemented in 'nhdplusTools' are available in the NHDPlus documentation available from the US Environmental Protection Agency <https://www.epa.gov/waterdata/basic-information>.",
    "version": "1.4.1",
    "maintainer": "David Blodgett <dblodgett@usgs.gov>",
    "author": "David Blodgett [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9489-1710>),\n  Mike Johnson [aut] (ORCID: <https://orcid.org/0000-0002-5288-8350>),\n  Marc Weber [ctb] (ORCID: <https://orcid.org/0000-0002-9742-4744>),\n  Josh Erickson [ctb],\n  Lauren Koenig [ctb] (ORCID: <https://orcid.org/0000-0002-7790-330X>)",
    "url": "https://doi-usgs.github.io/nhdplusTools/\nhttps://github.com/doi-usgs/nhdplusTools/",
    "bug_reports": "https://github.com/doi-usgs/nhdplusTools/issues/",
    "repository": "https://cran.r-project.org/package=nhdplusTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nhdplusTools NHDPlus Tools Tools for traversing and working with National Hydrography Dataset Plus (NHDPlus) data. All methods implemented in 'nhdplusTools' are available in the NHDPlus documentation available from the US Environmental Protection Agency <https://www.epa.gov/waterdata/basic-information>.  "
  },
  {
    "id": 16935,
    "package_name": "nlgm",
    "title": "Non Linear Growth Models",
    "description": "Six growth models are fitted using non-linear least squares. These are the Richards, the 3, 4 and 5 parameter logistic, the Gompetz and the Weibull growth models. Reference: Reddy T., Shkedy Z., van Rensburg C. J., Mwambi H., Debba P., Zuma K. and Manda, S. (2021). \"Short-term real-time prediction of total number of reported COVID-19 cases and deaths in South Africa: a data driven approach\". BMC medical research methodology, 21(1), 1-11. <doi:10.1186/s12874-020-01165-x>.",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Nikolaos Pandis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nlgm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlgm Non Linear Growth Models Six growth models are fitted using non-linear least squares. These are the Richards, the 3, 4 and 5 parameter logistic, the Gompetz and the Weibull growth models. Reference: Reddy T., Shkedy Z., van Rensburg C. J., Mwambi H., Debba P., Zuma K. and Manda, S. (2021). \"Short-term real-time prediction of total number of reported COVID-19 cases and deaths in South Africa: a data driven approach\". BMC medical research methodology, 21(1), 1-11. <doi:10.1186/s12874-020-01165-x>.  "
  },
  {
    "id": 16948,
    "package_name": "nlmixr2rpt",
    "title": "Templated Word and PowerPoint Reporting of 'nlmixr2' Fitting\nResults",
    "description": "This allows you to generate reporting workflows around 'nlmixr2' analyses with outputs in Word and PowerPoint. You can specify figures, tables and report structure in a user-definable 'YAML' file. Also you can use the internal functions to access the figures and tables to allow their including in other outputs (e.g. R Markdown).",
    "version": "0.2.2",
    "maintainer": "John Harrold <john.m.harrold@gmail.com>",
    "author": "John Harrold [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2052-4373>)",
    "url": "https://nlmixr2.github.io/nlmixr2rpt/",
    "bug_reports": "https://github.com/nlmixr2/nlmixr2rpt/issues",
    "repository": "https://cran.r-project.org/package=nlmixr2rpt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlmixr2rpt Templated Word and PowerPoint Reporting of 'nlmixr2' Fitting\nResults This allows you to generate reporting workflows around 'nlmixr2' analyses with outputs in Word and PowerPoint. You can specify figures, tables and report structure in a user-definable 'YAML' file. Also you can use the internal functions to access the figures and tables to allow their including in other outputs (e.g. R Markdown).  "
  },
  {
    "id": 17005,
    "package_name": "noise",
    "title": "Estimation of Intrinsic and Extrinsic Noise from Single-Cell\nData",
    "description": "Functions to calculate estimates of intrinsic and extrinsic noise from the two-reporter single-cell experiment, as in Elowitz, M. B., A. J. Levine, E. D. Siggia, and P. S. Swain (2002) Stochastic gene expression in a single cell. Science, 297, 1183-1186.  Functions implement multiple estimators developed for unbiasedness or min Mean Squared Error (MSE) in Fu, A. Q. and Pachter, L. (2016). Estimating intrinsic and extrinsic noise from single-cell gene expression measurements. Statistical Applications in Genetics and Molecular Biology, 15(6), 447-471.",
    "version": "1.0.2",
    "maintainer": "Audrey Q. Fu <audreyqyfu@gmail.com>",
    "author": "Audrey Qiuyan Fu and Lior Pachter",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=noise",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "noise Estimation of Intrinsic and Extrinsic Noise from Single-Cell\nData Functions to calculate estimates of intrinsic and extrinsic noise from the two-reporter single-cell experiment, as in Elowitz, M. B., A. J. Levine, E. D. Siggia, and P. S. Swain (2002) Stochastic gene expression in a single cell. Science, 297, 1183-1186.  Functions implement multiple estimators developed for unbiasedness or min Mean Squared Error (MSE) in Fu, A. Q. and Pachter, L. (2016). Estimating intrinsic and extrinsic noise from single-cell gene expression measurements. Statistical Applications in Genetics and Molecular Biology, 15(6), 447-471.  "
  },
  {
    "id": 17069,
    "package_name": "npIntFactRep",
    "title": "Nonparametric Interaction Tests for Factorial Designs with\nRepeated Measures",
    "description": "Returns nonparametric aligned rank tests for the interaction in two-way factorial designs, on R data sets with repeated measures in 'wide' format. Five ANOVAs tables are reported. A PARAMETRIC one on the original data, one for a CHECK upon the interaction alignments, and three aligned rank tests: one on the aligned REGULAR, one on the FRIEDMAN, and one on the KOCH ranks. In these rank tests, only the resulting values for the interaction are relevant.",
    "version": "1.5",
    "maintainer": "Jos Feys <jos.feys@faber.kuleuven.be>",
    "author": "Jos Feys",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=npIntFactRep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "npIntFactRep Nonparametric Interaction Tests for Factorial Designs with\nRepeated Measures Returns nonparametric aligned rank tests for the interaction in two-way factorial designs, on R data sets with repeated measures in 'wide' format. Five ANOVAs tables are reported. A PARAMETRIC one on the original data, one for a CHECK upon the interaction alignments, and three aligned rank tests: one on the aligned REGULAR, one on the FRIEDMAN, and one on the KOCH ranks. In these rank tests, only the resulting values for the interaction are relevant.  "
  },
  {
    "id": 17078,
    "package_name": "npclust",
    "title": "Nonparametric Tests for Incomplete Clustered Data",
    "description": "Nonparametric tests for clustered data in pre-post intervention design documented in Cui and Harrar (2021) <doi:10.1002/bimj.201900310> and Harrar and Cui (2022) <doi:10.1016/j.jspi.2022.05.009>. Other than the main test results mentioned in the reference paper, this package also provides a function to calculate the sample size allocations for the input long format data set, and also a function for adjusted/unadjusted confidence intervals calculations. There are also functions to visualize the distribution of data across different intervention groups over time, and also the adjusted/unadjusted confidence intervals.",
    "version": "0.1.1",
    "maintainer": "Yue Cui <YueCui@MissouriState.edu>",
    "author": "Yue Cui [aut, cre] (ORCID: <https://orcid.org/0000-0002-7304-9409>),\n  Solomon W. Harrar [aut] (ORCID:\n    <https://orcid.org/0000-0001-6802-340X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=npclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "npclust Nonparametric Tests for Incomplete Clustered Data Nonparametric tests for clustered data in pre-post intervention design documented in Cui and Harrar (2021) <doi:10.1002/bimj.201900310> and Harrar and Cui (2022) <doi:10.1016/j.jspi.2022.05.009>. Other than the main test results mentioned in the reference paper, this package also provides a function to calculate the sample size allocations for the input long format data set, and also a function for adjusted/unadjusted confidence intervals calculations. There are also functions to visualize the distribution of data across different intervention groups over time, and also the adjusted/unadjusted confidence intervals.  "
  },
  {
    "id": 17112,
    "package_name": "nrba",
    "title": "Methods for Conducting Nonresponse Bias Analysis (NRBA)",
    "description": "Facilitates nonresponse bias analysis (NRBA) \n    for survey data.  Such data may arise from a complex\n    sampling design with features such as stratification, clustering, or\n    unequal probabilities of selection. Multiple types of analyses may be\n    conducted: comparisons of response rates across subgroups; comparisons\n    of estimates before and after weighting adjustments; comparisons of\n    sample-based estimates to external population totals; tests of\n    systematic differences in covariate means between respondents\n    and full samples; tests of independence between response status\n    and covariates; and modeling of outcomes and response status as a \n    function of covariates. Extensive documentation and references are\n    provided for each type of analysis. Krenzke, Van de Kerckhove,\n    and Mohadjer (2005) <http://www.asasrms.org/Proceedings/y2005/files/JSM2005-000572.pdf>\n    and Lohr and Riddles (2016) <https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2016002/article/14677-eng.pdf?st=q7PyNsGR>\n    provide an overview of the methods implemented in this package.",
    "version": "0.3.1",
    "maintainer": "Ben Schneider <BenjaminSchneider@westat.com>",
    "author": "Ben Schneider [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0406-8470>),\n  Jim Green [aut],\n  Shelley Brock [aut] (Author of original SAS macro, WesNRBA),\n  Tom Krenzke [aut] (Author of original SAS macro, WesNRBA),\n  Michael Jones [aut] (Author of original SAS macro, WesNRBA),\n  Wendy Van de Kerckhove [aut] (Author of original SAS macro, WesNRBA),\n  David Ferraro [aut] (Author of original SAS macro, WesNRBA),\n  Laura Alvarez-Rojas [aut] (Author of original SAS macro, WesNRBA),\n  Katie Hubbell [aut] (Author of original SAS macro, WesNRBA),\n  Westat [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nrba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nrba Methods for Conducting Nonresponse Bias Analysis (NRBA) Facilitates nonresponse bias analysis (NRBA) \n    for survey data.  Such data may arise from a complex\n    sampling design with features such as stratification, clustering, or\n    unequal probabilities of selection. Multiple types of analyses may be\n    conducted: comparisons of response rates across subgroups; comparisons\n    of estimates before and after weighting adjustments; comparisons of\n    sample-based estimates to external population totals; tests of\n    systematic differences in covariate means between respondents\n    and full samples; tests of independence between response status\n    and covariates; and modeling of outcomes and response status as a \n    function of covariates. Extensive documentation and references are\n    provided for each type of analysis. Krenzke, Van de Kerckhove,\n    and Mohadjer (2005) <http://www.asasrms.org/Proceedings/y2005/files/JSM2005-000572.pdf>\n    and Lohr and Riddles (2016) <https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2016002/article/14677-eng.pdf?st=q7PyNsGR>\n    provide an overview of the methods implemented in this package.  "
  },
  {
    "id": 17138,
    "package_name": "numbat",
    "title": "Haplotype-Aware CNV Analysis from scRNA-Seq",
    "description": "A computational method that infers copy number variations (CNVs) in cancer scRNA-seq data and reconstructs the tumor phylogeny. 'numbat' integrates signals from gene expression, allelic ratio, and population haplotype structures to accurately infer allele-specific CNVs in single cells and reconstruct their lineage relationship. 'numbat' can be used to: 1. detect allele-specific copy number variations from single-cells; 2. differentiate tumor versus normal cells in the tumor microenvironment; 3. infer the clonal architecture and evolutionary history of profiled tumors. 'numbat' does not require tumor/normal-paired DNA or genotype data, but operates solely on the donor scRNA-data data (for example, 10x Cell Ranger output). Additional examples and documentations are available at <https://kharchenkolab.github.io/numbat/>. For details on the method please see Gao et al. Nature Biotechnology (2022) <doi:10.1038/s41587-022-01468-y>.",
    "version": "1.5.1",
    "maintainer": "Teng Gao <tgaoteng@gmail.com>",
    "author": "Teng Gao [cre, aut],\n  Ruslan Soldatov [aut],\n  Hirak Sarkar [aut],\n  Evan Biederstedt [aut],\n  Peter Kharchenko [aut]",
    "url": "https://github.com/kharchenkolab/numbat/,\nhttps://kharchenkolab.github.io/numbat/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=numbat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "numbat Haplotype-Aware CNV Analysis from scRNA-Seq A computational method that infers copy number variations (CNVs) in cancer scRNA-seq data and reconstructs the tumor phylogeny. 'numbat' integrates signals from gene expression, allelic ratio, and population haplotype structures to accurately infer allele-specific CNVs in single cells and reconstruct their lineage relationship. 'numbat' can be used to: 1. detect allele-specific copy number variations from single-cells; 2. differentiate tumor versus normal cells in the tumor microenvironment; 3. infer the clonal architecture and evolutionary history of profiled tumors. 'numbat' does not require tumor/normal-paired DNA or genotype data, but operates solely on the donor scRNA-data data (for example, 10x Cell Ranger output). Additional examples and documentations are available at <https://kharchenkolab.github.io/numbat/>. For details on the method please see Gao et al. Nature Biotechnology (2022) <doi:10.1038/s41587-022-01468-y>.  "
  },
  {
    "id": 17151,
    "package_name": "o2geosocial",
    "title": "Reconstruction of Transmission Chains from Surveillance Data",
    "description": "Bayesian reconstruction of who infected whom during past outbreaks using routinely-collected surveillance data. Inference of transmission trees using genotype, age specific social contacts, distance between cases and onset dates of the reported cases. (Robert A, Kucharski AJ, Gastanaduy PA, Paul P, Funk S. (2020) <doi:10.1098/rsif.2020.0084>).",
    "version": "1.1.3",
    "maintainer": "Alexis Robert <alexis.robert@lshtm.ac.uk>",
    "author": "Alexis Robert [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4516-2965>),\n  Sebastian Funk [aut] (ORCID: <https://orcid.org/0000-0002-2842-3406>),\n  Adam J Kucharski [aut] (ORCID: <https://orcid.org/0000-0001-8814-9421>),\n  Thibaut Jombart [ctb]",
    "url": "https://github.com/alxsrobert/o2geosocial",
    "bug_reports": "https://github.com/alxsrobert/o2geosocial/issues",
    "repository": "https://cran.r-project.org/package=o2geosocial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "o2geosocial Reconstruction of Transmission Chains from Surveillance Data Bayesian reconstruction of who infected whom during past outbreaks using routinely-collected surveillance data. Inference of transmission trees using genotype, age specific social contacts, distance between cases and onset dates of the reported cases. (Robert A, Kucharski AJ, Gastanaduy PA, Paul P, Funk S. (2020) <doi:10.1098/rsif.2020.0084>).  "
  },
  {
    "id": 17166,
    "package_name": "objectSignals",
    "title": "Observer Pattern for S4",
    "description": "A mutable Signal object can report changes to its state,\n    clients could register functions so that they are called whenever\n    the signal is emitted. The signal could be emitted, disconnected,\n    blocked, unblocked, and buffered.",
    "version": "0.10.3",
    "maintainer": "Michael Lawrence <michafla@gene.com>",
    "author": "Michael Lawrence, Tengfei Yin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=objectSignals",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "objectSignals Observer Pattern for S4 A mutable Signal object can report changes to its state,\n    clients could register functions so that they are called whenever\n    the signal is emitted. The signal could be emitted, disconnected,\n    blocked, unblocked, and buffered.  "
  },
  {
    "id": 17208,
    "package_name": "oecdoda",
    "title": "Seamless Access to OECD Official Development Assistance (ODA)\nData",
    "description": "Access and Analyze Official Development Assistance (ODA) data using \n    the OECD API <https://gitlab.algobank.oecd.org/public-documentation/dotstat-migration/-/raw/main/OECD_Data_API_documentation.pdf>.\n    ODA data includes sovereign-level aid data such as key aggregates (DAC1), geographical\n    distributions (DAC2A), project-level data (CRS), and multilateral contributions (Multisystem).",
    "version": "0.1.0",
    "maintainer": "Christoph Scheuch <christoph@tidy-intelligence.com>",
    "author": "Christoph Scheuch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>)",
    "url": "https://github.com/tidy-intelligence/r-oecdoda,\nhttps://tidy-intelligence.github.io/r-oecdoda/",
    "bug_reports": "https://github.com/tidy-intelligence/r-oecdoda/issues",
    "repository": "https://cran.r-project.org/package=oecdoda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oecdoda Seamless Access to OECD Official Development Assistance (ODA)\nData Access and Analyze Official Development Assistance (ODA) data using \n    the OECD API <https://gitlab.algobank.oecd.org/public-documentation/dotstat-migration/-/raw/main/OECD_Data_API_documentation.pdf>.\n    ODA data includes sovereign-level aid data such as key aggregates (DAC1), geographical\n    distributions (DAC2A), project-level data (CRS), and multilateral contributions (Multisystem).  "
  },
  {
    "id": 17215,
    "package_name": "officedown",
    "title": "Enhanced 'R Markdown' Format for 'Word' and 'PowerPoint'",
    "description": "Allows production of 'Microsoft' corporate documents from 'R\n    Markdown' by reusing formatting defined in 'Microsoft Word' documents.\n    You can reuse table styles, list styles but also add column sections,\n    landscape oriented pages. Table and image captions as well as\n    cross-references are transformed into 'Microsoft Word' fields,\n    allowing documents edition and merging without issue with references;\n    the syntax conforms to the 'bookdown' cross-reference definition.\n    Objects generated by the 'officer' package are also supported in the\n    'knitr' chunks.  'Microsoft PowerPoint' presentations also benefit\n    from this as well as the ability to produce editable vector graphics\n    in 'PowerPoint' and also to define placeholder where content is to be\n    added.",
    "version": "0.4.1",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre, cph],\n  ArData [cph],\n  Institut f\u00fcr Qualit\u00e4tssicherung und Transparenz im Gesundheitswesen\n    [fnd],\n  Noam Ross [aut] (rmarkdown implementation for rvg),\n  ArData [cph],\n  Martin Camitz [ctb]",
    "url": "https://ardata-fr.github.io/officeverse/,\nhttps://davidgohel.github.io/officedown/",
    "bug_reports": "https://github.com/davidgohel/officedown/issues",
    "repository": "https://cran.r-project.org/package=officedown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "officedown Enhanced 'R Markdown' Format for 'Word' and 'PowerPoint' Allows production of 'Microsoft' corporate documents from 'R\n    Markdown' by reusing formatting defined in 'Microsoft Word' documents.\n    You can reuse table styles, list styles but also add column sections,\n    landscape oriented pages. Table and image captions as well as\n    cross-references are transformed into 'Microsoft Word' fields,\n    allowing documents edition and merging without issue with references;\n    the syntax conforms to the 'bookdown' cross-reference definition.\n    Objects generated by the 'officer' package are also supported in the\n    'knitr' chunks.  'Microsoft PowerPoint' presentations also benefit\n    from this as well as the ability to produce editable vector graphics\n    in 'PowerPoint' and also to define placeholder where content is to be\n    added.  "
  },
  {
    "id": 17216,
    "package_name": "officer",
    "title": "Manipulation of Microsoft Word and PowerPoint Documents",
    "description": "Access and manipulate 'Microsoft Word', 'RTF' and 'Microsoft\n    PowerPoint' documents from R.  The package focuses on tabular and\n    graphical reporting from R; it also provides two functions that let\n    users get document content into data objects. A set of functions lets\n    add and remove images, tables and paragraphs of text in new or\n    existing documents.  The package does not require any installation of\n    Microsoft products to be able to write Microsoft files.",
    "version": "0.7.2",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  Stefan Moog [aut],\n  Mark Heckmann [aut] (ORCID: <https://orcid.org/0000-0002-0736-7417>),\n  ArData [cph],\n  Frank Hangler [ctb] (function body_replace_all_text),\n  Liz Sander [ctb] (several documentation fixes),\n  Anton Victorson [ctb] (fixes xml structures),\n  Jon Calder [ctb] (update vignettes),\n  John Harrold [ctb] (function annotate_base),\n  John Muschelli [ctb] (google doc compatibility),\n  Bill Denney [ctb] (ORCID: <https://orcid.org/0000-0002-5759-428X>,\n    function as.matrix.rpptx),\n  Nikolai Beck [ctb] (set speaker notes for .pptx documents),\n  Greg Leleu [ctb] (fields functionality in ppt),\n  Majid Eismann [ctb],\n  Hongyuan Jia [ctb] (ORCID: <https://orcid.org/0000-0002-0075-8183>),\n  Michael Stackhouse [ctb]",
    "url": "https://ardata-fr.github.io/officeverse/,\nhttps://davidgohel.github.io/officer/",
    "bug_reports": "https://github.com/davidgohel/officer/issues",
    "repository": "https://cran.r-project.org/package=officer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "officer Manipulation of Microsoft Word and PowerPoint Documents Access and manipulate 'Microsoft Word', 'RTF' and 'Microsoft\n    PowerPoint' documents from R.  The package focuses on tabular and\n    graphical reporting from R; it also provides two functions that let\n    users get document content into data objects. A set of functions lets\n    add and remove images, tables and paragraphs of text in new or\n    existing documents.  The package does not require any installation of\n    Microsoft products to be able to write Microsoft files.  "
  },
  {
    "id": 17219,
    "package_name": "ofpetrial",
    "title": "Design on-Farm Precision Field Agronomic Trials",
    "description": "A comprehensive system for designing and implementing on-farm precision field agronomic trials. You provide field data, tell 'ofpetrial' how to design a trial, and get readily-usable trial design files and a report checks the validity and reliability of the trial design.",
    "version": "0.1.3",
    "maintainer": "Taro Mieno <tmieno2@unl.edu>",
    "author": "Taro Mieno [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0614-0771>),\n  Brittani Edge [aut, ctb] (ORCID:\n    <https://orcid.org/0009-0005-2051-1048>)",
    "url": "https://difm-brain.github.io/ofpetrial/",
    "bug_reports": "https://github.com/DIFM-Brain/ofpetrial/issues",
    "repository": "https://cran.r-project.org/package=ofpetrial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ofpetrial Design on-Farm Precision Field Agronomic Trials A comprehensive system for designing and implementing on-farm precision field agronomic trials. You provide field data, tell 'ofpetrial' how to design a trial, and get readily-usable trial design files and a report checks the validity and reliability of the trial design.  "
  },
  {
    "id": 17226,
    "package_name": "ojsr",
    "title": "Crawler and Data Scraper for Open Journal System ('OJS')",
    "description": "Crawler for 'OJS' pages and scraper for meta-data from articles. \n    You can crawl 'OJS' archives, issues, articles, galleys, and search results. \n    You can scrape articles metadata from their head tag in html, \n    or from Open Archives Initiative ('OAI') records.\n    Most of these functions rely on 'OJS' routing conventions \n    (<https://docs.pkp.sfu.ca/dev/documentation/en/architecture-routes>).",
    "version": "0.1.5",
    "maintainer": "Gaston Becerra <gaston.becerra@gmail.com>",
    "author": "Gaston Becerra [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9432-8848>)",
    "url": "https://github.com/gastonbecerra/ojsr",
    "bug_reports": "https://github.com/gastonbecerra/ojsr/issues",
    "repository": "https://cran.r-project.org/package=ojsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ojsr Crawler and Data Scraper for Open Journal System ('OJS') Crawler for 'OJS' pages and scraper for meta-data from articles. \n    You can crawl 'OJS' archives, issues, articles, galleys, and search results. \n    You can scrape articles metadata from their head tag in html, \n    or from Open Archives Initiative ('OAI') records.\n    Most of these functions rely on 'OJS' routing conventions \n    (<https://docs.pkp.sfu.ca/dev/documentation/en/architecture-routes>).  "
  },
  {
    "id": 17238,
    "package_name": "omnibus",
    "title": "Helper Tools for Managing Data, Dates, Missing Values, and Text",
    "description": "An assortment of helper functions for managing data (e.g.,\n\trotating values in matrices by a user-defined angle, switching from\n\trow- to column-indexing), dates (e.g., intuiting year from messy date\n\tstrings), handling missing values (e.g., removing elements/rows across\n\tmultiple vectors or matrices if any have an NA), text (e.g.,\n\tflushing reports to the console in real-time); and combining data frames\n\twith different schema (copying, filling, or concatenating columns or\n\tapplying functions before combining).",
    "version": "1.2.15",
    "maintainer": "Adam B. Smith <adam.smith@mobot.org>",
    "author": "Adam B. Smith [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-6420-1659>)",
    "url": "https://github.com/adamlilith/omnibus,\nhttps://adamlilith.github.io/omnibus/",
    "bug_reports": "https://github.com/adamlilith/omnibus/issues",
    "repository": "https://cran.r-project.org/package=omnibus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "omnibus Helper Tools for Managing Data, Dates, Missing Values, and Text An assortment of helper functions for managing data (e.g.,\n\trotating values in matrices by a user-defined angle, switching from\n\trow- to column-indexing), dates (e.g., intuiting year from messy date\n\tstrings), handling missing values (e.g., removing elements/rows across\n\tmultiple vectors or matrices if any have an NA), text (e.g.,\n\tflushing reports to the console in real-time); and combining data frames\n\twith different schema (copying, filling, or concatenating columns or\n\tapplying functions before combining).  "
  },
  {
    "id": 17241,
    "package_name": "ompr",
    "title": "Model and Solve Mixed Integer Linear Programs",
    "description": "Model mixed integer linear programs in an algebraic way directly in R.\n             The model is solver-independent and thus offers the possibility\n             to solve a model with different solvers. It currently only supports\n             linear constraints and objective functions. See the 'ompr'\n             website <https://dirkschumacher.github.io/ompr/> for more information,\n             documentation and examples.",
    "version": "1.0.4",
    "maintainer": "Dirk Schumacher <mail@dirk-schumacher.net>",
    "author": "Dirk Schumacher [aut, cre]",
    "url": "https://github.com/dirkschumacher/ompr",
    "bug_reports": "https://github.com/dirkschumacher/ompr/issues",
    "repository": "https://cran.r-project.org/package=ompr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ompr Model and Solve Mixed Integer Linear Programs Model mixed integer linear programs in an algebraic way directly in R.\n             The model is solver-independent and thus offers the possibility\n             to solve a model with different solvers. It currently only supports\n             linear constraints and objective functions. See the 'ompr'\n             website <https://dirkschumacher.github.io/ompr/> for more information,\n             documentation and examples.  "
  },
  {
    "id": 17247,
    "package_name": "onbrand",
    "title": "Templated Reporting Workflows in Word and PowerPoint",
    "description": "Automated reporting in Word and PowerPoint can require customization for each organizational template. This package works around this by adding standard reporting functions and an abstraction layer to facilitate automated reporting workflows that can be replicated across different organizational templates.",
    "version": "1.0.7",
    "maintainer": "John Harrold <john.m.harrold@gmail.com>",
    "author": "John Harrold [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2052-4373>),\n  Bryan Smith [aut]",
    "url": "https://onbrand.ubiquity.tools",
    "bug_reports": "https://github.com/john-harrold/onbrand/issues",
    "repository": "https://cran.r-project.org/package=onbrand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "onbrand Templated Reporting Workflows in Word and PowerPoint Automated reporting in Word and PowerPoint can require customization for each organizational template. This package works around this by adding standard reporting functions and an abstraction layer to facilitate automated reporting workflows that can be replicated across different organizational templates.  "
  },
  {
    "id": 17286,
    "package_name": "opalr",
    "title": "'Opal' Data Repository Client and 'DataSHIELD' Utils",
    "description": "Data integration Web application for biobanks by 'OBiBa'. 'Opal' is\n    the core database application for biobanks. Participant data, once\n    collected from any data source, must be integrated and stored in a central\n    data repository under a uniform model. 'Opal' is such a central repository.\n    It can import, process, validate, query, analyze, report, and export data.\n    'Opal' is typically used in a research center to analyze the data acquired at\n    assessment centres. Its ultimate purpose is to achieve seamless\n    data-sharing among biobanks. This 'Opal' client allows to interact with 'Opal'\n    web services and to perform operations on the R server side. 'DataSHIELD'\n    administration tools are also provided.",
    "version": "3.5.2",
    "maintainer": "Yannick Marcon <yannick.marcon@obiba.org>",
    "author": "Yannick Marcon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0138-2023>),\n  Amadou Gaye [ctb] (ORCID: <https://orcid.org/0000-0002-1180-2792>),\n  OBiBa group [cph]",
    "url": "https://github.com/obiba/opalr/, https://www.obiba.org/opalr/,\nhttps://www.obiba.org/pages/products/opal/,\nhttps://academic.oup.com/ije/article/46/5/1372/4102813,\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008880,\nhttps://datashield.org/",
    "bug_reports": "https://github.com/obiba/opalr/issues",
    "repository": "https://cran.r-project.org/package=opalr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "opalr 'Opal' Data Repository Client and 'DataSHIELD' Utils Data integration Web application for biobanks by 'OBiBa'. 'Opal' is\n    the core database application for biobanks. Participant data, once\n    collected from any data source, must be integrated and stored in a central\n    data repository under a uniform model. 'Opal' is such a central repository.\n    It can import, process, validate, query, analyze, report, and export data.\n    'Opal' is typically used in a research center to analyze the data acquired at\n    assessment centres. Its ultimate purpose is to achieve seamless\n    data-sharing among biobanks. This 'Opal' client allows to interact with 'Opal'\n    web services and to perform operations on the R server side. 'DataSHIELD'\n    administration tools are also provided.  "
  },
  {
    "id": 17289,
    "package_name": "openEBGM",
    "title": "EBGM Disproportionality Scores for Adverse Event Data Mining",
    "description": "An implementation of DuMouchel's (1999) <doi:10.1080/00031305.1999.10474456>\n  Bayesian data mining method for the market basket problem.\n  Calculates Empirical Bayes Geometric Mean (EBGM) and posterior quantile scores\n  using the Gamma-Poisson Shrinker (GPS) model to find unusually large cell\n  counts in large, sparse contingency tables. Can be used to find unusually high\n  reporting rates of adverse events associated with products. In general, can be\n  used to mine any database where the co-occurrence of two variables or items is\n  of interest. Also calculates relative and proportional reporting ratios.\n  Builds on the work of the 'PhViD' package, from which much of the code is\n  derived. Some of the added features include stratification to adjust for\n  confounding variables and data squashing to improve computational efficiency.\n  Includes an implementation of the EM algorithm for hyperparameter estimation\n  loosely derived from the 'mederrRank' package.",
    "version": "0.9.1",
    "maintainer": "John Ihrie <John.Ihrie@fda.hhs.gov>",
    "author": "John Ihrie [cre, aut],\n  Travis Canida [aut],\n  Isma\u00efl Ahmed [ctb] (author of 'PhViD' package (derived code)),\n  Antoine Poncet [ctb] (author of 'PhViD'),\n  Sergio Venturini [ctb] (author of 'mederrRank' package (derived code)),\n  Jessica Myers [ctb] (author of 'mederrRank')",
    "url": "https://journal.r-project.org/archive/2017/RJ-2017-063/index.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=openEBGM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openEBGM EBGM Disproportionality Scores for Adverse Event Data Mining An implementation of DuMouchel's (1999) <doi:10.1080/00031305.1999.10474456>\n  Bayesian data mining method for the market basket problem.\n  Calculates Empirical Bayes Geometric Mean (EBGM) and posterior quantile scores\n  using the Gamma-Poisson Shrinker (GPS) model to find unusually large cell\n  counts in large, sparse contingency tables. Can be used to find unusually high\n  reporting rates of adverse events associated with products. In general, can be\n  used to mine any database where the co-occurrence of two variables or items is\n  of interest. Also calculates relative and proportional reporting ratios.\n  Builds on the work of the 'PhViD' package, from which much of the code is\n  derived. Some of the added features include stratification to adjust for\n  confounding variables and data squashing to improve computational efficiency.\n  Includes an implementation of the EM algorithm for hyperparameter estimation\n  loosely derived from the 'mederrRank' package.  "
  },
  {
    "id": 17302,
    "package_name": "opencpu",
    "title": "Producing and Reproducing Results",
    "description": "A system for embedded scientific computing and reproducible research with R.\n    The OpenCPU server exposes a simple but powerful HTTP api for RPC and data interchange\n    with R. This provides a reliable and scalable foundation for statistical services or \n    building R web applications. The OpenCPU server runs either as a single-user development\n    server within the interactive R session, or as a multi-user Linux stack based on Apache2. \n    The entire system is fully open source and permissively licensed. The OpenCPU website\n    has detailed documentation and example apps.",
    "version": "2.2.14",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://www.opencpu.org https://opencpu.r-universe.dev/opencpu",
    "bug_reports": "https://github.com/opencpu/opencpu/issues",
    "repository": "https://cran.r-project.org/package=opencpu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "opencpu Producing and Reproducing Results A system for embedded scientific computing and reproducible research with R.\n    The OpenCPU server exposes a simple but powerful HTTP api for RPC and data interchange\n    with R. This provides a reliable and scalable foundation for statistical services or \n    building R web applications. The OpenCPU server runs either as a single-user development\n    server within the interactive R session, or as a multi-user Linux stack based on Apache2. \n    The entire system is fully open source and permissively licensed. The OpenCPU website\n    has detailed documentation and example apps.  "
  },
  {
    "id": 17303,
    "package_name": "opendataformat",
    "title": "Reading and Writing Open Data Format Files",
    "description": "The Open Data Format (ODF) is a new, non-proprietary, multilingual, metadata enriched, and zip-compressed data format with metadata structured in the Data Documentation Initiative (DDI) Codebook standard. This package allows reading and writing of data files in the Open Data Format (ODF) in R, and displaying metadata in different languages. For further information on the Open Data Format, see <https://opendataformat.github.io/>.",
    "version": "2.2.2",
    "maintainer": "Tom Hartl <tom.hartl96@gmail.com>",
    "author": "Tom Hartl [aut, cre] (ORCID: <https://orcid.org/0009-0006-4226-9835>),\n  Claudia Saalbach [ctb]",
    "url": "https://github.com/opendataformat/r-package-opendataformat",
    "bug_reports": "https://github.com/opendataformat/r-package-opendataformat/issues",
    "repository": "https://cran.r-project.org/package=opendataformat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "opendataformat Reading and Writing Open Data Format Files The Open Data Format (ODF) is a new, non-proprietary, multilingual, metadata enriched, and zip-compressed data format with metadata structured in the Data Documentation Initiative (DDI) Codebook standard. This package allows reading and writing of data files in the Open Data Format (ODF) in R, and displaying metadata in different languages. For further information on the Open Data Format, see <https://opendataformat.github.io/>.  "
  },
  {
    "id": 17321,
    "package_name": "opinAr",
    "title": "Argentina's Public Opinion Toolbox",
    "description": "A toolbox for working with public opinion data from Argentina. It facilitates access to microdata and the calculation of indicators of the Trust in Government Index (ICG), prepared by the Torcuato Di Tella University. Although we will try to document everything possible in English, by its very nature Spanish will be the main language. El paquete fue pensado como una caja de herramientas para el trabajo con datos de opini\u00f3n p\u00fablica de Argentina. El mismo  facilita el acceso a los microdatos y el c\u00e1lculos de indicadores del \u00cdndice de Confianza en el Gobierno (ICG), elaborado por la Universidad Torcuato Di Tella.",
    "version": "1.0.0",
    "maintainer": "Juan Pablo Ruiz Nicolini <juanpabloruiznicolini@gmail.com>",
    "author": "Camila Higa [aut, cph],\n  Juan Pablo Ruiz Nicolini [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-3138-6343>)",
    "url": "https://github.com/PoliticaArgentina/opinAr",
    "bug_reports": "https://github.com/PoliticaArgentina/opinAr/issues",
    "repository": "https://cran.r-project.org/package=opinAr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "opinAr Argentina's Public Opinion Toolbox A toolbox for working with public opinion data from Argentina. It facilitates access to microdata and the calculation of indicators of the Trust in Government Index (ICG), prepared by the Torcuato Di Tella University. Although we will try to document everything possible in English, by its very nature Spanish will be the main language. El paquete fue pensado como una caja de herramientas para el trabajo con datos de opini\u00f3n p\u00fablica de Argentina. El mismo  facilita el acceso a los microdatos y el c\u00e1lculos de indicadores del \u00cdndice de Confianza en el Gobierno (ICG), elaborado por la Universidad Torcuato Di Tella.  "
  },
  {
    "id": 17322,
    "package_name": "opitools",
    "title": "Analyzing the Opinions in a Big Text Document",
    "description": "Designed for performing impact analysis of\n  opinions in a digital text document (DTD). The \n  package allows a user to assess the extent to which a theme\n  or subject within a document impacts the overall opinion \n  expressed in the document. The package can be applied to a wide \n  range of opinion-based DTD, including commentaries on social media\n  platforms (such as 'Facebook', 'Twitter' and 'Youtube'), \n  online products reviews, and so on. \n  The utility of 'opitools' was originally demonstrated \n  in Adepeju and Jimoh (2021) <doi:10.31235/osf.io/c32qh> in the \n  assessment of COVID-19 impacts on neighbourhood policing using \n  Twitter data. Further examples can be found in the vignette of \n  the package.",
    "version": "1.8.0",
    "maintainer": "Monsuru Adepeju <monsuur2010@yahoo.com>",
    "author": "Monsuru Adepeju [cre, aut],",
    "url": "https://github.com/MAnalytics/opitools",
    "bug_reports": "https://github.com/MAnalytics/opitools/issues/1",
    "repository": "https://cran.r-project.org/package=opitools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "opitools Analyzing the Opinions in a Big Text Document Designed for performing impact analysis of\n  opinions in a digital text document (DTD). The \n  package allows a user to assess the extent to which a theme\n  or subject within a document impacts the overall opinion \n  expressed in the document. The package can be applied to a wide \n  range of opinion-based DTD, including commentaries on social media\n  platforms (such as 'Facebook', 'Twitter' and 'Youtube'), \n  online products reviews, and so on. \n  The utility of 'opitools' was originally demonstrated \n  in Adepeju and Jimoh (2021) <doi:10.31235/osf.io/c32qh> in the \n  assessment of COVID-19 impacts on neighbourhood policing using \n  Twitter data. Further examples can be found in the vignette of \n  the package.  "
  },
  {
    "id": 17365,
    "package_name": "optrefine",
    "title": "Optimally Refine Strata",
    "description": "Splits initial strata into refined strata that optimize covariate balance. For more information, please email the author for a copy of the accompanying manuscript. To solve the linear program, the 'Gurobi' commercial optimization software is recommended, but not required. The 'gurobi' R package can be installed following the instructions at <https://www.gurobi.com/documentation/9.1/refman/ins_the_r_package.html>.",
    "version": "1.1.0",
    "maintainer": "Katherine Brumberg <kbrum@wharton.upenn.edu>",
    "author": "Katherine Brumberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5193-6250>)",
    "url": "https://github.com/kkbrum/optrefine,\nhttps://kkbrum.github.io/optrefine/,\nhttps://www.gurobi.com/documentation/9.1/refman/ins_the_r_package.html",
    "bug_reports": "https://github.com/kkbrum/optrefine/issues",
    "repository": "https://cran.r-project.org/package=optrefine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "optrefine Optimally Refine Strata Splits initial strata into refined strata that optimize covariate balance. For more information, please email the author for a copy of the accompanying manuscript. To solve the linear program, the 'Gurobi' commercial optimization software is recommended, but not required. The 'gurobi' R package can be installed following the instructions at <https://www.gurobi.com/documentation/9.1/refman/ins_the_r_package.html>.  "
  },
  {
    "id": 17376,
    "package_name": "orderanalyzer",
    "title": "Extracting Order Position Tables from PDF-Based Order Documents",
    "description": "Functions for extracting text and tables from \n  PDF-based order documents. It provides an n-gram-based approach for identifying \n  the language of an order document. It furthermore uses R-package 'pdftools' to \n  extract the text from an order document. In the case that the PDF document is \n  only including an image (because it is scanned document), R package 'tesseract' \n  is used for OCR. Furthermore, the package provides functionality for identifying \n  and extracting order position tables in order documents based on a clustering approach.",
    "version": "1.0.0",
    "maintainer": "Michael Scholz <michael.scholz@th-deg.de>",
    "author": "Michael Scholz [cre, aut],\n  Joerg Bauer [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=orderanalyzer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "orderanalyzer Extracting Order Position Tables from PDF-Based Order Documents Functions for extracting text and tables from \n  PDF-based order documents. It provides an n-gram-based approach for identifying \n  the language of an order document. It furthermore uses R-package 'pdftools' to \n  extract the text from an order document. In the case that the PDF document is \n  only including an image (because it is scanned document), R package 'tesseract' \n  is used for OCR. Furthermore, the package provides functionality for identifying \n  and extracting order position tables in order documents based on a clustering approach.  "
  },
  {
    "id": 17397,
    "package_name": "org",
    "title": "Organising Projects",
    "description": "A framework for organizing R projects with a standardized structure. \n    Most analyses consist of three main components: code, results, and data, \n    each with different requirements such as version control, sharing, and encryption. \n    This package provides tools to set up and manage project directories, handle \n    file paths consistently across operating systems, organize results using \n    date-based structures, source code from specified directories, create and \n    manage Quarto documents, and perform file operations safely. \n    It ensures consistency across projects while accommodating different \n    requirements for various types of content.",
    "version": "2025.11.24",
    "maintainer": "Richard Aubrey White <hello@rwhite.no>",
    "author": "Richard Aubrey White [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6747-1726>)",
    "url": "https://www.rwhite.no/org/, https://github.com/raubreywhite/org",
    "bug_reports": "https://github.com/raubreywhite/org/issues",
    "repository": "https://cran.r-project.org/package=org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "org Organising Projects A framework for organizing R projects with a standardized structure. \n    Most analyses consist of three main components: code, results, and data, \n    each with different requirements such as version control, sharing, and encryption. \n    This package provides tools to set up and manage project directories, handle \n    file paths consistently across operating systems, organize results using \n    date-based structures, source code from specified directories, create and \n    manage Quarto documents, and perform file operations safely. \n    It ensures consistency across projects while accommodating different \n    requirements for various types of content.  "
  },
  {
    "id": 17400,
    "package_name": "organizr",
    "title": "Shortcuts for File Creation with Informative Prefixes",
    "description": "Provides functions for quickly creating R and Python scripts, as \n    well as 'Rmarkdown' or Quarto documents with automatically assigned name \n    prefixes. Prefixes are either file counts (e.g. \"001\") or dates \n    (e.g. \"2022-09-26\").",
    "version": "0.1.0",
    "maintainer": "Johannes Brachem <jbrachem@posteo.de>",
    "author": "Johannes Brachem [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7884-4631>)",
    "url": "https://github.com/jobrachem/organizr",
    "bug_reports": "https://github.com/jobrachem/organizr/issues",
    "repository": "https://cran.r-project.org/package=organizr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "organizr Shortcuts for File Creation with Informative Prefixes Provides functions for quickly creating R and Python scripts, as \n    well as 'Rmarkdown' or Quarto documents with automatically assigned name \n    prefixes. Prefixes are either file counts (e.g. \"001\") or dates \n    (e.g. \"2022-09-26\").  "
  },
  {
    "id": 17436,
    "package_name": "ottr",
    "title": "An R Autograding Extension for Otter-Grader",
    "description": "\n    An R autograding extension for Otter-Grader (<https://otter-grader.readthedocs.io>). It supports \n    grading R scripts, R Markdown documents, and R Jupyter Notebooks.",
    "version": "1.5.3",
    "maintainer": "Christopher Pyles <cpyles@berkeley.edu>",
    "author": "Christopher Pyles [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8520-7593>),\n  UC Berkeley Data Science Education Program [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ottr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ottr An R Autograding Extension for Otter-Grader \n    An R autograding extension for Otter-Grader (<https://otter-grader.readthedocs.io>). It supports \n    grading R scripts, R Markdown documents, and R Jupyter Notebooks.  "
  },
  {
    "id": 17463,
    "package_name": "oysteR",
    "title": "Scans R Projects for Vulnerable Third Party Dependencies",
    "description": "Collects a list of your third party R packages, and scans\n    them with the 'OSS' Index provided by 'Sonatype', reporting back on\n    any vulnerabilities that are found in the third party packages you\n    use.",
    "version": "0.1.4",
    "maintainer": "Colin Gillespie <csgillespie@gmail.com>",
    "author": "Jeffry Hesse [aut],\n  Brittany Belle [aut],\n  Colin Gillespie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1787-0275>),\n  Dan Rollo [aut],\n  Josiah Parry [aut] (ORCID: <https://orcid.org/0000-0001-9910-865X>),\n  Sonatype [cph]",
    "url": "https://github.com/sonatype-nexus-community/oysteR",
    "bug_reports": "https://github.com/sonatype-nexus-community/oysteR/issues",
    "repository": "https://cran.r-project.org/package=oysteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oysteR Scans R Projects for Vulnerable Third Party Dependencies Collects a list of your third party R packages, and scans\n    them with the 'OSS' Index provided by 'Sonatype', reporting back on\n    any vulnerabilities that are found in the third party packages you\n    use.  "
  },
  {
    "id": 17503,
    "package_name": "pacta.loanbook",
    "title": "Easily Install and Load PACTA for Banks Packages",
    "description": "PACTA (Paris Agreement Capital Transition Assessment) for Banks is\n  a tool that allows banks to calculate the climate alignment of their corporate\n  lending portfolios. This package is designed to make it easy to install and\n  load multiple PACTA for Banks packages in a single step. It also provides\n  thorough documentation - the PACTA for Banks cookbook at\n  <https://rmi-pacta.github.io/pacta.loanbook/articles/cookbook_overview.html> -\n  on how to run a PACTA for Banks analysis. This covers prerequisites for the\n  analysis, the separate steps of running the analysis, the interpretation of\n  PACTA for Banks results, and advanced use cases.",
    "version": "0.1.1",
    "maintainer": "Jacob Kastl <jacob.kastl@gmail.com>",
    "author": "Jacob Kastl [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0000-8281-8129>),\n  Jackson Hoffart [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-8600-5042>),\n  CJ Yetman [aut, ctr] (ORCID: <https://orcid.org/0000-0001-5099-9500>),\n  RMI [cph, fnd] (ROR: <https://ror.org/03anfar33>)",
    "url": "https://rmi-pacta.github.io/pacta.loanbook/,\nhttps://github.com/RMI-PACTA/pacta.loanbook",
    "bug_reports": "https://github.com/RMI-PACTA/pacta.loanbook/issues",
    "repository": "https://cran.r-project.org/package=pacta.loanbook",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pacta.loanbook Easily Install and Load PACTA for Banks Packages PACTA (Paris Agreement Capital Transition Assessment) for Banks is\n  a tool that allows banks to calculate the climate alignment of their corporate\n  lending portfolios. This package is designed to make it easy to install and\n  load multiple PACTA for Banks packages in a single step. It also provides\n  thorough documentation - the PACTA for Banks cookbook at\n  <https://rmi-pacta.github.io/pacta.loanbook/articles/cookbook_overview.html> -\n  on how to run a PACTA for Banks analysis. This covers prerequisites for the\n  analysis, the separate steps of running the analysis, the interpretation of\n  PACTA for Banks results, and advanced use cases.  "
  },
  {
    "id": 17505,
    "package_name": "pacu",
    "title": "Precision Agriculture Computational Utilities",
    "description": "Support for a variety of commonly used precision agriculture operations. Includes functions to download and process raw satellite images from Sentinel-2 <https://documentation.dataspace.copernicus.eu/APIs/OData.html>. Includes functions that download vegetation index statistics for a given period of time, without the need to download the raw images <https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Statistical.html>. There are also functions to download and visualize weather data in a historical context. Lastly, the package also contains functions to process yield monitor data. These functions can build polygons around recorded data points, evaluate the overlap between polygons, clean yield data, and smooth yield maps.",
    "version": "0.1.74",
    "maintainer": "dos Santos Caio <clsantos@iastate.edu>",
    "author": "dos Santos Caio [aut, cre],\n  Miguez Fernando [aut]",
    "url": "",
    "bug_reports": "https://github.com/cldossantos/pacu/issues",
    "repository": "https://cran.r-project.org/package=pacu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pacu Precision Agriculture Computational Utilities Support for a variety of commonly used precision agriculture operations. Includes functions to download and process raw satellite images from Sentinel-2 <https://documentation.dataspace.copernicus.eu/APIs/OData.html>. Includes functions that download vegetation index statistics for a given period of time, without the need to download the raw images <https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Statistical.html>. There are also functions to download and visualize weather data in a historical context. Lastly, the package also contains functions to process yield monitor data. These functions can build polygons around recorded data points, evaluate the overlap between polygons, clean yield data, and smooth yield maps.  "
  },
  {
    "id": 17506,
    "package_name": "pacviz",
    "title": "Pac-Man Visualization Package",
    "description": "Provides a broad-view perspective on data via\n    linear mapping of data onto a radial coordinate system. The package\n    contains functions to visualize the residual values of linear\n    regression and Cartesian data in the defined radial scheme. See the\n    'pacviz' documentation page for more information:\n    <https://pacviz.sriley.dev/>.",
    "version": "1.0.4",
    "maintainer": "Sarah Riley <academic@sriley.dev>",
    "author": "Sarah Riley [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pacviz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pacviz Pac-Man Visualization Package Provides a broad-view perspective on data via\n    linear mapping of data onto a radial coordinate system. The package\n    contains functions to visualize the residual values of linear\n    regression and Cartesian data in the defined radial scheme. See the\n    'pacviz' documentation page for more information:\n    <https://pacviz.sriley.dev/>.  "
  },
  {
    "id": 17510,
    "package_name": "pagemap",
    "title": "Create Mini Map for Web Pages",
    "description": "Quickly and easily add a mini map to your 'rmarkdown' html documents.",
    "version": "0.1.3",
    "maintainer": "Wei Su <swsoyee@gmail.com>",
    "author": "Wei Su [aut, cre] (ORCID: <https://orcid.org/0000-0002-9302-5332>),\n  Lars Jung [aut, cph] (pagemap library in htmlwidgets/lib,\n    https://github.com/lrsjng/pagemap)",
    "url": "https://github.com/swsoyee/pagemapR",
    "bug_reports": "https://github.com/swsoyee/pagemapR/issues",
    "repository": "https://cran.r-project.org/package=pagemap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pagemap Create Mini Map for Web Pages Quickly and easily add a mini map to your 'rmarkdown' html documents.  "
  },
  {
    "id": 17548,
    "package_name": "pan",
    "title": "Multiple Imputation for Multivariate Panel or Clustered Data",
    "description": "It provides functions and examples for maximum likelihood estimation for\n              generalized linear mixed models and Gibbs sampler for multivariate linear\n              mixed models with incomplete data, as described in Schafer JL (1997)\n              \"Imputation of missing covariates under a multivariate linear mixed model\".\n              Technical report 97-04, Dept. of Statistics, The Pennsylvania State University.",
    "version": "1.9",
    "maintainer": "Jing hua Zhao <jinghuazhao@hotmail.com>",
    "author": "Original by Joseph L. Schafer ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pan Multiple Imputation for Multivariate Panel or Clustered Data It provides functions and examples for maximum likelihood estimation for\n              generalized linear mixed models and Gibbs sampler for multivariate linear\n              mixed models with incomplete data, as described in Schafer JL (1997)\n              \"Imputation of missing covariates under a multivariate linear mixed model\".\n              Technical report 97-04, Dept. of Statistics, The Pennsylvania State University.  "
  },
  {
    "id": 17551,
    "package_name": "pander",
    "title": "An R 'Pandoc' Writer",
    "description": "Contains some functions catching all messages, 'stdout' and other\n    useful information while evaluating R code and other helpers to return user\n    specified text elements (like: header, paragraph, table, image, lists etc.)\n    in 'pandoc' markdown or several type of R objects similarly automatically\n    transformed to markdown format. Also capable of exporting/converting (the\n    resulting) complex 'pandoc' documents to e.g. HTML, 'PDF', 'docx' or 'odt'. This\n    latter reporting feature is supported in brew syntax or with a custom reference\n    class with a smarty caching 'backend'.",
    "version": "0.6.6",
    "maintainer": "Gergely Dar\u00f3czi <daroczig@rapporter.net>",
    "author": "Gergely Dar\u00f3czi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3149-8537>),\n  Roman Tsegelskyi [aut]",
    "url": "https://rapporter.github.io/pander/",
    "bug_reports": "https://github.com/rapporter/pander/issues",
    "repository": "https://cran.r-project.org/package=pander",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pander An R 'Pandoc' Writer Contains some functions catching all messages, 'stdout' and other\n    useful information while evaluating R code and other helpers to return user\n    specified text elements (like: header, paragraph, table, image, lists etc.)\n    in 'pandoc' markdown or several type of R objects similarly automatically\n    transformed to markdown format. Also capable of exporting/converting (the\n    resulting) complex 'pandoc' documents to e.g. HTML, 'PDF', 'docx' or 'odt'. This\n    latter reporting feature is supported in brew syntax or with a custom reference\n    class with a smarty caching 'backend'.  "
  },
  {
    "id": 17553,
    "package_name": "pandocfilters",
    "title": "Pandoc Filters for R",
    "description": "The document converter 'pandoc' <https://pandoc.org/> is widely used\n    in the R community. One feature of 'pandoc' is that it can produce and consume\n    JSON-formatted abstract syntax trees (AST). This allows to transform a given\n    source document into JSON-formatted AST, alter it by so called filters and pass\n    the altered JSON-formatted AST back to 'pandoc'. This package provides functions\n    which allow to write such filters in native R code. \n    Although this package is inspired by the Python package 'pandocfilters' \n    <https://github.com/jgm/pandocfilters/>, it provides additional convenience functions which make it simple to use the 'pandocfilters' package as a \n    report generator. Since 'pandocfilters' inherits most of it's functionality\n    from 'pandoc' it can create documents in many formats \n    (for more information see <https://pandoc.org/>) but is also bound to the same\n    limitations as 'pandoc'.",
    "version": "0.1-6",
    "maintainer": "Florian Schwendinger <FlorianSchwendinger@gmx.at>",
    "author": "Florian Schwendinger [aut, cre],\n  Kurt Hornik [aut],\n  Andrie de Vries [ctb]",
    "url": "https://pandoc.org/, https://github.com/jgm/pandocfilters/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pandocfilters",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pandocfilters Pandoc Filters for R The document converter 'pandoc' <https://pandoc.org/> is widely used\n    in the R community. One feature of 'pandoc' is that it can produce and consume\n    JSON-formatted abstract syntax trees (AST). This allows to transform a given\n    source document into JSON-formatted AST, alter it by so called filters and pass\n    the altered JSON-formatted AST back to 'pandoc'. This package provides functions\n    which allow to write such filters in native R code. \n    Although this package is inspired by the Python package 'pandocfilters' \n    <https://github.com/jgm/pandocfilters/>, it provides additional convenience functions which make it simple to use the 'pandocfilters' package as a \n    report generator. Since 'pandocfilters' inherits most of it's functionality\n    from 'pandoc' it can create documents in many formats \n    (for more information see <https://pandoc.org/>) but is also bound to the same\n    limitations as 'pandoc'.  "
  },
  {
    "id": 17564,
    "package_name": "papaja",
    "title": "Prepare American Psychological Association Journal Articles with\nR Markdown",
    "description": "Tools to create dynamic, submission-ready manuscripts, which\n  conform to American Psychological Association manuscript guidelines. We\n  provide R Markdown document formats for manuscripts (PDF and Word) and\n  revision letters (PDF). Helper functions facilitate reporting statistical\n  analyses or create publication-ready tables and plots.",
    "version": "0.1.4",
    "maintainer": "Frederik Aust <frederik.aust@uni-koeln.de>",
    "author": "Frederik Aust [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4900-788X>),\n  Marius Barth [aut] (ORCID: <https://orcid.org/0000-0002-3421-6665>),\n  Birk Diedenhofen [ctb],\n  Christoph Stahl [ctb],\n  Joseph V. Casillas [ctb],\n  Rudolf Siegel [ctb]",
    "url": "https://github.com/crsh/papaja",
    "bug_reports": "https://github.com/crsh/papaja/issues",
    "repository": "https://cran.r-project.org/package=papaja",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "papaja Prepare American Psychological Association Journal Articles with\nR Markdown Tools to create dynamic, submission-ready manuscripts, which\n  conform to American Psychological Association manuscript guidelines. We\n  provide R Markdown document formats for manuscripts (PDF and Word) and\n  revision letters (PDF). Helper functions facilitate reporting statistical\n  analyses or create publication-ready tables and plots.  "
  },
  {
    "id": 17565,
    "package_name": "papeR",
    "title": "A Toolbox for Writing Pretty Papers and Reports",
    "description": "A toolbox for writing 'knitr', 'Sweave' or other 'LaTeX'- or 'markdown'-based\n\t     reports and to prettify the output of various estimated models.",
    "version": "1.0-6",
    "maintainer": "Benjamin Hofner <benjamin.hofner@pei.de>",
    "author": "Benjamin Hofner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2810-3186>),\n  Romain Francois [ctb],\n  Kurt Hornik [ctb],\n  Martin Maechler [ctb],\n  David Dahl [ctb] (see inst/CONTRIBUTIONS for details)",
    "url": "https://github.com/hofnerb/papeR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=papeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "papeR A Toolbox for Writing Pretty Papers and Reports A toolbox for writing 'knitr', 'Sweave' or other 'LaTeX'- or 'markdown'-based\n\t     reports and to prettify the output of various estimated models.  "
  },
  {
    "id": 17586,
    "package_name": "paramtest",
    "title": "Run a Function Iteratively While Varying Parameters",
    "description": "Run simulations or other functions while easily varying parameters\n    from one iteration to the next. Some common use cases would be grid search\n    for machine learning algorithms, running sets of simulations (e.g.,\n    estimating statistical power for complex models), or bootstrapping under\n    various conditions. See the 'paramtest' documentation for more information\n    and examples.",
    "version": "0.1.1",
    "maintainer": "Jeffrey Hughes <jeff.hughes@gmail.com>",
    "author": "Jeffrey Hughes [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=paramtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paramtest Run a Function Iteratively While Varying Parameters Run simulations or other functions while easily varying parameters\n    from one iteration to the next. Some common use cases would be grid search\n    for machine learning algorithms, running sets of simulations (e.g.,\n    estimating statistical power for complex models), or bootstrapping under\n    various conditions. See the 'paramtest' documentation for more information\n    and examples.  "
  },
  {
    "id": 17591,
    "package_name": "pargasite",
    "title": "Pollution-Associated Risk Geospatial Analysis Site",
    "description": "Offers tools to estimate and visualize levels of major pollutants\n             (CO, NO2, SO2, Ozone, PM2.5 and PM10) across the conterminous\n             United States for user-defined time ranges. Provides functions to\n             retrieve pollutant data from the U.S. Environmental Protection\n             Agency\u2019s 'Air Quality System' (AQS) API service\n             <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             interactive visualization through a 'shiny' application, allowing\n             users to explore pollutant levels for a given location over time\n             relative to the National Ambient Air Quality Standards (NAAQS).",
    "version": "2.1.1",
    "maintainer": "Jaehyun Joo <jaehyunjoo@outlook.com>",
    "author": "Jaehyun Joo [aut, cre],\n  Rebecca Greenblatt [aut],\n  Avantika Diwadkar [aut],\n  Nisha Narayanan [aut],\n  Blanca Himes [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pargasite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pargasite Pollution-Associated Risk Geospatial Analysis Site Offers tools to estimate and visualize levels of major pollutants\n             (CO, NO2, SO2, Ozone, PM2.5 and PM10) across the conterminous\n             United States for user-defined time ranges. Provides functions to\n             retrieve pollutant data from the U.S. Environmental Protection\n             Agency\u2019s 'Air Quality System' (AQS) API service\n             <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             interactive visualization through a 'shiny' application, allowing\n             users to explore pollutant levels for a given location over time\n             relative to the National Ambient Air Quality Standards (NAAQS).  "
  },
  {
    "id": 17603,
    "package_name": "parsermd",
    "title": "Formal Parser and Related Tools for R Markdown Documents",
    "description": "An implementation of a formal grammar and parser for R Markdown documents\n    using the Boost Spirit X3 library. It also includes a collection of high level\n    functions for working with the resulting abstract syntax tree.",
    "version": "0.2.0",
    "maintainer": "Colin Rundel <rundel@gmail.com>",
    "author": "Colin Rundel [aut, cre]",
    "url": "https://rundel.github.io/parsermd/,\nhttps://github.com/rundel/parsermd",
    "bug_reports": "https://github.com/rundel/parsermd/issues",
    "repository": "https://cran.r-project.org/package=parsermd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parsermd Formal Parser and Related Tools for R Markdown Documents An implementation of a formal grammar and parser for R Markdown documents\n    using the Boost Spirit X3 library. It also includes a collection of high level\n    functions for working with the resulting abstract syntax tree.  "
  },
  {
    "id": 17628,
    "package_name": "patchDVI",
    "title": "Package to Patch '.dvi' or '.synctex' Files",
    "description": "Functions to patch specials in '.dvi' files,\n        or entries in '.synctex' files.  Works with concordance=TRUE \n        in Sweave, knitr or R Markdown to link sources to previews.",
    "version": "1.11.3",
    "maintainer": "Duncan Murdoch <murdoch.duncan@gmail.com>",
    "author": "Duncan Murdoch [aut, cre]",
    "url": "https://github.com/dmurdoch/patchDVI",
    "bug_reports": "https://github.com/dmurdoch/patchDVI/issues",
    "repository": "https://cran.r-project.org/package=patchDVI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "patchDVI Package to Patch '.dvi' or '.synctex' Files Functions to patch specials in '.dvi' files,\n        or entries in '.synctex' files.  Works with concordance=TRUE \n        in Sweave, knitr or R Markdown to link sources to previews.  "
  },
  {
    "id": 17629,
    "package_name": "patchSynctex",
    "title": "Communication Between Editor and Viewer for Literate Programs",
    "description": "This utility eases the debugging of literate documents\n\t     ('noweb' files) by patching the synchronization information\n\t     (the '.synctex(.gz)' file) produced by 'pdflatex' with\n\t     concordance information produced by 'Sweave' or 'knitr' and\n\t     'Sweave' or 'knitr' ; this allows for bilateral communication\n\t     between a text editor (visualizing the 'noweb' source) and\n\t     a viewer (visualizing the resultant 'PDF'), thus bypassing\n\t     the intermediate 'TeX' file.",
    "version": "0.1-4",
    "maintainer": "Emmanuel Charpentier <emm.charpentier@free.fr>",
    "author": "Jan Gleixner [aut],\n  Daniel Hicks [ctb],\n  Kyle J. Harms [ctb],\n  Emmanuel Charpentier [aut, cre]",
    "url": "https://github.com/EmmanuelCharpentier/patchSynctex",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=patchSynctex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "patchSynctex Communication Between Editor and Viewer for Literate Programs This utility eases the debugging of literate documents\n\t     ('noweb' files) by patching the synchronization information\n\t     (the '.synctex(.gz)' file) produced by 'pdflatex' with\n\t     concordance information produced by 'Sweave' or 'knitr' and\n\t     'Sweave' or 'knitr' ; this allows for bilateral communication\n\t     between a text editor (visualizing the 'noweb' source) and\n\t     a viewer (visualizing the resultant 'PDF'), thus bypassing\n\t     the intermediate 'TeX' file.  "
  },
  {
    "id": 17640,
    "package_name": "patientProfilesVis",
    "title": "Visualization of Patient Profiles",
    "description": "Creation of patient profile visualizations for\n  exploration, diagnostic or monitoring purposes during a clinical trial.\n  These static visualizations display a patient-specific overview\n  of the evolution during the trial time frame of \n  parameters of interest (as laboratory, ECG, vital signs),\n  presence of adverse events, exposure to a treatment; \n  associated with metadata patient information, \n  as demography, concomitant medication.\n  The visualizations can be tailored for specific domain(s) or endpoint(s) of interest.\n  Visualizations are exported into patient profile report(s)\n  or can be embedded in custom report(s).",
    "version": "2.0.10",
    "maintainer": "Laure Cougnaud <laure.cougnaud@openanalytics.eu>",
    "author": "Laure Cougnaud [aut, cre],\n  Margaux Faes [rev] (tests),\n  Open Analytics [cph]",
    "url": "https://github.com/openanalytics/patientProfilesVis",
    "bug_reports": "https://github.com/openanalytics/patientProfilesVis/issues",
    "repository": "https://cran.r-project.org/package=patientProfilesVis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "patientProfilesVis Visualization of Patient Profiles Creation of patient profile visualizations for\n  exploration, diagnostic or monitoring purposes during a clinical trial.\n  These static visualizations display a patient-specific overview\n  of the evolution during the trial time frame of \n  parameters of interest (as laboratory, ECG, vital signs),\n  presence of adverse events, exposure to a treatment; \n  associated with metadata patient information, \n  as demography, concomitant medication.\n  The visualizations can be tailored for specific domain(s) or endpoint(s) of interest.\n  Visualizations are exported into patient profile report(s)\n  or can be embedded in custom report(s).  "
  },
  {
    "id": 17654,
    "package_name": "paws.cost.management",
    "title": "'Amazon Web Services' Cost Management Services",
    "description": "Interface to 'Amazon Web Services' cost management services,\n    including cost and usage reports, budgets, pricing, and more\n    <https://aws.amazon.com/>.",
    "version": "0.9.0",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "David Kretch [aut],\n  Adam Banker [aut],\n  Dyfan Jones [cre],\n  Amazon.com, Inc. [cph]",
    "url": "https://github.com/paws-r/paws,\nhttps://paws-r.r-universe.dev/paws.cost.management",
    "bug_reports": "https://github.com/paws-r/paws/issues",
    "repository": "https://cran.r-project.org/package=paws.cost.management",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paws.cost.management 'Amazon Web Services' Cost Management Services Interface to 'Amazon Web Services' cost management services,\n    including cost and usage reports, budgets, pricing, and more\n    <https://aws.amazon.com/>.  "
  },
  {
    "id": 17658,
    "package_name": "paws.end.user.computing",
    "title": "'Amazon Web Services' End User Computing Services",
    "description": "Interface to 'Amazon Web Services' end user computing\n    services, including collaborative document editing, mobile intranet,\n    and more <https://aws.amazon.com/>.",
    "version": "0.9.0",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "David Kretch [aut],\n  Adam Banker [aut],\n  Dyfan Jones [cre],\n  Amazon.com, Inc. [cph]",
    "url": "https://github.com/paws-r/paws,\nhttps://paws-r.r-universe.dev/paws.end.user.computing",
    "bug_reports": "https://github.com/paws-r/paws/issues",
    "repository": "https://cran.r-project.org/package=paws.end.user.computing",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paws.end.user.computing 'Amazon Web Services' End User Computing Services Interface to 'Amazon Web Services' end user computing\n    services, including collaborative document editing, mobile intranet,\n    and more <https://aws.amazon.com/>.  "
  },
  {
    "id": 17674,
    "package_name": "pbkrtest",
    "title": "Parametric Bootstrap, Kenward-Roger and Satterthwaite Based\nMethods for Test in Mixed Models",
    "description": "Computes p-values based on (a) Satterthwaite or\n    Kenward-Rogers degree of freedom methods and (b) parametric bootstrap\n    for mixed effects models as implemented in the 'lme4'\n    package. Implements parametric bootstrap test for generalized linear\n    mixed models as implemented in 'lme4' and generalized linear\n    models. The package is documented in the paper by Halekoh and\n    H\u00f8jsgaard, (2012, <doi:10.18637/jss.v059.i09>).  Please see\n    'citation(\"pbkrtest\")' for citation details.",
    "version": "0.5.5",
    "maintainer": "S\u00f8ren H\u00f8jsgaard <sorenh@math.aau.dk>",
    "author": "Ulrich Halekoh [aut, cph],\n  S\u00f8ren H\u00f8jsgaard [aut, cre, cph]",
    "url": "https://people.math.aau.dk/~sorenh/software/pbkrtest/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pbkrtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pbkrtest Parametric Bootstrap, Kenward-Roger and Satterthwaite Based\nMethods for Test in Mixed Models Computes p-values based on (a) Satterthwaite or\n    Kenward-Rogers degree of freedom methods and (b) parametric bootstrap\n    for mixed effects models as implemented in the 'lme4'\n    package. Implements parametric bootstrap test for generalized linear\n    mixed models as implemented in 'lme4' and generalized linear\n    models. The package is documented in the paper by Halekoh and\n    H\u00f8jsgaard, (2012, <doi:10.18637/jss.v059.i09>).  Please see\n    'citation(\"pbkrtest\")' for citation details.  "
  },
  {
    "id": 17713,
    "package_name": "pcvr",
    "title": "Plant Phenotyping and Bayesian Statistics",
    "description": "Analyse common types of plant phenotyping data, provide a simplified interface\n\tto longitudinal growth modeling and select Bayesian statistics,\n\tand streamline use of 'PlantCV' output.\n\tSeveral Bayesian methods and reporting guidelines for Bayesian methods are described in\n\tKruschke (2018) <doi:10.1177/2515245918771304>,\n\tKruschke (2013) <doi:10.1037/a0029146>, and Kruschke (2021) <doi:10.1038/s41562-021-01177-7>.",
    "version": "1.3.1",
    "maintainer": "Josh Sumner <jsumner@danforthcenter.org>",
    "author": "Josh Sumner [aut, cre] (ORCID: <https://orcid.org/0000-0002-3399-9063>),\n  Jeffrey Berry [aut] (ORCID: <https://orcid.org/0000-0002-8064-9787>),\n  Noah Fahlgren [rev] (ORCID: <https://orcid.org/0000-0002-5597-4537>),\n  Donald Danforth Plant Science Center [cph]",
    "url": "https://github.com/danforthcenter/pcvr, https://plantcv.org/,\nhttps://danforthcenter.github.io/pcvr/",
    "bug_reports": "https://github.com/danforthcenter/pcvr/issues",
    "repository": "https://cran.r-project.org/package=pcvr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcvr Plant Phenotyping and Bayesian Statistics Analyse common types of plant phenotyping data, provide a simplified interface\n\tto longitudinal growth modeling and select Bayesian statistics,\n\tand streamline use of 'PlantCV' output.\n\tSeveral Bayesian methods and reporting guidelines for Bayesian methods are described in\n\tKruschke (2018) <doi:10.1177/2515245918771304>,\n\tKruschke (2013) <doi:10.1037/a0029146>, and Kruschke (2021) <doi:10.1038/s41562-021-01177-7>.  "
  },
  {
    "id": 17723,
    "package_name": "pdfminer",
    "title": "Read Portable Document Format (PDF) Files",
    "description": "Provides an interface to 'PDFMiner' <https://github.com/pdfminer/pdfminer.six> \n    a 'Python' package for extracting information from 'PDF'-files. \n    'PDFMiner' has the goal to get all information available in a 'PDF'-file, \n    position of the characters, font type, font size and informations about lines. \n    Which makes it the perfect starting point for extracting tables from 'PDF'-files.\n    More information can be found in the package 'README'-file.",
    "version": "1.0",
    "maintainer": "Florian Schwendinger <FlorianSchwendinger@gmx.at>",
    "author": "Florian Schwendinger [aut, cre, cph],\n  Benjamin Schwendinger [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pdfminer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdfminer Read Portable Document Format (PDF) Files Provides an interface to 'PDFMiner' <https://github.com/pdfminer/pdfminer.six> \n    a 'Python' package for extracting information from 'PDF'-files. \n    'PDFMiner' has the goal to get all information available in a 'PDF'-file, \n    position of the characters, font type, font size and informations about lines. \n    Which makes it the perfect starting point for extracting tables from 'PDF'-files.\n    More information can be found in the package 'README'-file.  "
  },
  {
    "id": 17741,
    "package_name": "pedMermaid",
    "title": "Pedigree Mermaid Syntax",
    "description": "Generate Mermaid syntax for a pedigree flowchart from a pedigree data frame.\n    Mermaid syntax is commonly used to generate plots, charts, diagrams, and flowcharts. It is a textual syntax for creating reproducible illustrations. This package generates Mermaid syntax from a pedigree data frame to visualize a pedigree flowchart. The Mermaid syntax can be embedded in a Markdown or R Markdown file, or viewed on Mermaid editors and renderers. Links' shape, style, and orientation can be customized via function arguments, and nodes' shapes and styles can be customized via optional columns in the pedigree data frame.",
    "version": "1.0.2",
    "maintainer": "Mohammad Ali Nilforooshan <m.a.nilforooshan@gmail.com>",
    "author": "Mohammad Ali Nilforooshan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0339-5442>)",
    "url": "https://github.com/nilforooshan/pedMermaid",
    "bug_reports": "https://github.com/nilforooshan/pedMermaid/issues",
    "repository": "https://cran.r-project.org/package=pedMermaid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pedMermaid Pedigree Mermaid Syntax Generate Mermaid syntax for a pedigree flowchart from a pedigree data frame.\n    Mermaid syntax is commonly used to generate plots, charts, diagrams, and flowcharts. It is a textual syntax for creating reproducible illustrations. This package generates Mermaid syntax from a pedigree data frame to visualize a pedigree flowchart. The Mermaid syntax can be embedded in a Markdown or R Markdown file, or viewed on Mermaid editors and renderers. Links' shape, style, and orientation can be customized via function arguments, and nodes' shapes and styles can be customized via optional columns in the pedigree data frame.  "
  },
  {
    "id": 17743,
    "package_name": "pedalfast.data",
    "title": "PEDALFAST Data",
    "description": "Data files and documentation for PEDiatric vALidation oF vAriableS\n    in TBI (PEDALFAST).  The data was used in \"Functional Status Scale in\n    Children With Traumatic Brain Injury: A Prospective Cohort Study\" by Bennett,\n    Dixon, et al  (2016) <doi:10.1097/PCC.0000000000000934>.",
    "version": "1.0.2",
    "maintainer": "Peter DeWitt <peter.dewitt@cuanschutz.edu>",
    "author": "Peter DeWitt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6391-0795>),\n  Tell Bennett [aut] (ORCID: <https://orcid.org/0000-0003-1483-4236>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pedalfast.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pedalfast.data PEDALFAST Data Data files and documentation for PEDiatric vALidation oF vAriableS\n    in TBI (PEDALFAST).  The data was used in \"Functional Status Scale in\n    Children With Traumatic Brain Injury: A Prospective Cohort Study\" by Bennett,\n    Dixon, et al  (2016) <doi:10.1097/PCC.0000000000000934>.  "
  },
  {
    "id": 17802,
    "package_name": "permutes",
    "title": "Permutation Tests for Time Series Data",
    "description": "Helps you determine the analysis window to use when analyzing densely-sampled\n    time-series data, such as EEG data, using permutation testing (Maris & Oostenveld, 2007)\n    <doi:10.1016/j.jneumeth.2007.03.024>. These permutation tests can help identify the timepoints\n    where significance of an effect begins and ends, and the results can be plotted in various\n    types of heatmap for reporting. Mixed-effects models are supported using an implementation of\n    the approach by Lee & Braun (2012) <doi:10.1111/j.1541-0420.2011.01675.x>.",
    "version": "2.8",
    "maintainer": "Cesko C. Voeten <cvoeten@gmail.com>",
    "author": "Cesko C. Voeten [aut, cre]",
    "url": "",
    "bug_reports": "https://gitlab.com/cvoeten/permutes/-/issues",
    "repository": "https://cran.r-project.org/package=permutes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "permutes Permutation Tests for Time Series Data Helps you determine the analysis window to use when analyzing densely-sampled\n    time-series data, such as EEG data, using permutation testing (Maris & Oostenveld, 2007)\n    <doi:10.1016/j.jneumeth.2007.03.024>. These permutation tests can help identify the timepoints\n    where significance of an effect begins and ends, and the results can be plotted in various\n    types of heatmap for reporting. Mixed-effects models are supported using an implementation of\n    the approach by Lee & Braun (2012) <doi:10.1111/j.1541-0420.2011.01675.x>.  "
  },
  {
    "id": 17812,
    "package_name": "personr",
    "title": "Test Your Personality",
    "description": "An R-package-version of an open online science-based personality\n    test from <https://openpsychometrics.org/tests/IPIP-BFFM/>,\n    providing a better-designed interface and a more detailed report.\n    The core command launch_test() opens a personality test in your browser,\n    and generates a report after you click \"Submit\". In this report, your results\n    are compared with other people's, to show what these results mean.\n    Other people's data is from <https://openpsychometrics.org/_rawdata/BIG5.zip>.",
    "version": "1.0.0",
    "maintainer": "Renfei Mao <renfeimao@gmail.com>",
    "author": "Renfei Mao",
    "url": "https://github.com/flujoo/personr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=personr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "personr Test Your Personality An R-package-version of an open online science-based personality\n    test from <https://openpsychometrics.org/tests/IPIP-BFFM/>,\n    providing a better-designed interface and a more detailed report.\n    The core command launch_test() opens a personality test in your browser,\n    and generates a report after you click \"Submit\". In this report, your results\n    are compared with other people's, to show what these results mean.\n    Other people's data is from <https://openpsychometrics.org/_rawdata/BIG5.zip>.  "
  },
  {
    "id": 17831,
    "package_name": "pgTools",
    "title": "Functions for Generating PostgreSQL Statements/Scripts",
    "description": "Create PostgreSQL statements/scripts from R, optionally executing the SQL statements.\n    Common SQL operations are included, although not every configurable option is available at this time. \n    SQL output is intended to be compliant with PostgreSQL syntax specifications. PostgreSQL documentation is available here\n    <https://www.postgresql.org/docs/current/index.html>.",
    "version": "1.0.2",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell",
    "url": "https://github.com/tconwell/pgTools",
    "bug_reports": "https://github.com/tconwell/pgTools/issues",
    "repository": "https://cran.r-project.org/package=pgTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pgTools Functions for Generating PostgreSQL Statements/Scripts Create PostgreSQL statements/scripts from R, optionally executing the SQL statements.\n    Common SQL operations are included, although not every configurable option is available at this time. \n    SQL output is intended to be compliant with PostgreSQL syntax specifications. PostgreSQL documentation is available here\n    <https://www.postgresql.org/docs/current/index.html>.  "
  },
  {
    "id": 17848,
    "package_name": "pharmaRTF",
    "title": "Enhanced RTF Wrapper for Use with Existing Table Packages",
    "description": "Enhanced RTF wrapper written in R for use with existing R tables\n    packages such as 'Huxtable' or 'GT'. This package fills a gap where tables in\n    certain packages can be written out to RTF, but cannot add certain metadata\n    or features to the document that are required/expected in a report for a\n    regulatory submission, such as multiple levels of titles and footnotes,\n    making the document landscape, and controlling properties such as margins.",
    "version": "0.1.4",
    "maintainer": "Michael Stackhouse <mike.stackhouse@atorusresearch.com>",
    "author": "Eli Miller [aut] (ORCID: <https://orcid.org/0000-0002-2127-9456>),\n  Ashley Tarasiewicz [aut],\n  Michael Stackhouse [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6030-723X>),\n  Atorus Research LLC [cph]",
    "url": "",
    "bug_reports": "https://github.com/atorus-research/pharmaRTF/issues",
    "repository": "https://cran.r-project.org/package=pharmaRTF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pharmaRTF Enhanced RTF Wrapper for Use with Existing Table Packages Enhanced RTF wrapper written in R for use with existing R tables\n    packages such as 'Huxtable' or 'GT'. This package fills a gap where tables in\n    certain packages can be written out to RTF, but cannot add certain metadata\n    or features to the document that are required/expected in a report for a\n    regulatory submission, such as multiple levels of titles and footnotes,\n    making the document landscape, and controlling properties such as margins.  "
  },
  {
    "id": 17849,
    "package_name": "pharmaverse",
    "title": "Navigate 'Pharmaverse'",
    "description": "The 'pharmaverse' is a set of packages that compose multiple pathways \n    through clinical data generation and reporting in the pharmaceutical industry.\n    This package is designed to guide users to our work-spaces on 'GitHub', 'Slack' \n    and 'LinkedIn' as well as our website and examples.\n    Learn more about the 'pharmaverse' at <https://pharmaverse.org>.",
    "version": "0.0.2",
    "maintainer": "Ari Siggaard Knoph <aikp@novonordisk.com>",
    "author": "Ari Siggaard Knoph [aut, cre],\n  pharmaverse [cph]",
    "url": "https://github.com/pharmaverse/pharmaverse-pkg,\nhttps://pharmaverse.org",
    "bug_reports": "https://github.com/pharmaverse/pharmaverse-pkg/issues",
    "repository": "https://cran.r-project.org/package=pharmaverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pharmaverse Navigate 'Pharmaverse' The 'pharmaverse' is a set of packages that compose multiple pathways \n    through clinical data generation and reporting in the pharmaceutical industry.\n    This package is designed to guide users to our work-spaces on 'GitHub', 'Slack' \n    and 'LinkedIn' as well as our website and examples.\n    Learn more about the 'pharmaverse' at <https://pharmaverse.org>.  "
  },
  {
    "id": 17862,
    "package_name": "phdcocktail",
    "title": "Enhance the Ease of R Experience as an Emerging Researcher",
    "description": "A toolkit of functions to help: i) effortlessly transform collected data into a \n  publication ready format, ii) generate insightful visualizations from clinical data, iii) report\n  summary statistics in a publication-ready format, iv) efficiently export, save and reload R objects\n  within the framework of R projects.",
    "version": "0.1.0",
    "maintainer": "Dahham Alsoud <dahhamalsoud@gmail.com>",
    "author": "Dahham Alsoud [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6795-6328>)",
    "url": "https://dahhamalsoud.github.io/phdcocktail/,\nhttps://github.com/DahhamAlsoud/phdcocktail",
    "bug_reports": "https://github.com/DahhamAlsoud/phdcocktail/issues",
    "repository": "https://cran.r-project.org/package=phdcocktail",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phdcocktail Enhance the Ease of R Experience as an Emerging Researcher A toolkit of functions to help: i) effortlessly transform collected data into a \n  publication ready format, ii) generate insightful visualizations from clinical data, iii) report\n  summary statistics in a publication-ready format, iv) efficiently export, save and reload R objects\n  within the framework of R projects.  "
  },
  {
    "id": 17880,
    "package_name": "phm",
    "title": "Phrase Mining",
    "description": "Functions to extract and handle commonly occurring principal phrases\n    obtained from collections of texts. Major speed improvements - core functions \n    rewritten in C++ for faster phrase-document parsing, clustering, and text \n    distance computations. Based on, Small, E., & Cabrera, J. (2025). Principal \n    phrase mining, an automated method for extracting meaningful phrases from \n    text. International Journal of Computers and Applications, 47(1), 84\u201392.",
    "version": "2.1.2",
    "maintainer": "Ellie Small <ellie_small@yahoo.com>",
    "author": "Ellie Small [aut, cre] (ORCID: <https://orcid.org/0000-0003-1313-115X>)",
    "url": "https://doi.org/10.1080/1206212X.2024.2448494",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phm Phrase Mining Functions to extract and handle commonly occurring principal phrases\n    obtained from collections of texts. Major speed improvements - core functions \n    rewritten in C++ for faster phrase-document parsing, clustering, and text \n    distance computations. Based on, Small, E., & Cabrera, J. (2025). Principal \n    phrase mining, an automated method for extracting meaningful phrases from \n    text. International Journal of Computers and Applications, 47(1), 84\u201392.  "
  },
  {
    "id": 17887,
    "package_name": "phonics",
    "title": "Phonetic Spelling Algorithms",
    "description": "Provides a collection of phonetic algorithms including\n    Soundex, Metaphone, NYSIIS, Caverphone, and others.  The package is\n    documented in <doi:10.18637/jss.v095.i08>.",
    "version": "1.3.10",
    "maintainer": "James Howard <jh@jameshoward.us>",
    "author": "James Howard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4530-1547>),\n  Kyle Haynes [ctb],\n  Amanda Hood [ctb],\n  Os Keyes [ctb]",
    "url": "https://jameshoward.us/phonics-in-r/",
    "bug_reports": "https://github.com/k3jph/phonics-in-r/issues",
    "repository": "https://cran.r-project.org/package=phonics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phonics Phonetic Spelling Algorithms Provides a collection of phonetic algorithms including\n    Soundex, Metaphone, NYSIIS, Caverphone, and others.  The package is\n    documented in <doi:10.18637/jss.v095.i08>.  "
  },
  {
    "id": 17906,
    "package_name": "phyclust",
    "title": "Phylogenetic Clustering (Phyloclustering)",
    "description": "Phylogenetic clustering (phyloclustering) is an evolutionary\n        Continuous Time Markov Chain model-based approach to identify\n        population structure from molecular data without assuming\n        linkage equilibrium. The package phyclust (Chen 2011) provides a\n        convenient implementation of phyloclustering for DNA and SNP data,\n        capable of clustering individuals into subpopulations and identifying\n        molecular sequences representative of those subpopulations. It is\n        designed in C for performance, interfaced with R for visualization,\n        and incorporates other popular open source programs including\n        ms (Hudson 2002) <doi:10.1093/bioinformatics/18.2.337>,\n        seq-gen (Rambaut and Grassly 1997)\n        <doi:10.1093/bioinformatics/13.3.235>,\n        Hap-Clustering (Tzeng 2005) <doi:10.1002/gepi.20063> and\n        PAML baseml (Yang 1997, 2007) <doi:10.1093/bioinformatics/13.5.555>,\n        <doi:10.1093/molbev/msm088>,\n        for simulating data, additional analyses, and searching the best tree.\n        See the phyclust website for more information, documentations and\n        examples.",
    "version": "0.1-34",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "author": "Wei-Chen Chen [aut, cre],\n  Karin Dorman [aut],\n  Yan-Han Chen [ctb]",
    "url": "https://snoweye.github.io/phyclust/",
    "bug_reports": "https://github.com/snoweye/phyclust/issues",
    "repository": "https://cran.r-project.org/package=phyclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phyclust Phylogenetic Clustering (Phyloclustering) Phylogenetic clustering (phyloclustering) is an evolutionary\n        Continuous Time Markov Chain model-based approach to identify\n        population structure from molecular data without assuming\n        linkage equilibrium. The package phyclust (Chen 2011) provides a\n        convenient implementation of phyloclustering for DNA and SNP data,\n        capable of clustering individuals into subpopulations and identifying\n        molecular sequences representative of those subpopulations. It is\n        designed in C for performance, interfaced with R for visualization,\n        and incorporates other popular open source programs including\n        ms (Hudson 2002) <doi:10.1093/bioinformatics/18.2.337>,\n        seq-gen (Rambaut and Grassly 1997)\n        <doi:10.1093/bioinformatics/13.3.235>,\n        Hap-Clustering (Tzeng 2005) <doi:10.1002/gepi.20063> and\n        PAML baseml (Yang 1997, 2007) <doi:10.1093/bioinformatics/13.5.555>,\n        <doi:10.1093/molbev/msm088>,\n        for simulating data, additional analyses, and searching the best tree.\n        See the phyclust website for more information, documentations and\n        examples.  "
  },
  {
    "id": 17912,
    "package_name": "phylocanvas",
    "title": "Interactive Phylogenetic Trees Using the 'Phylocanvas'\nJavaScript Library",
    "description": "Create and customize interactive phylogenetic trees using the 'phylocanvas' JavaScript library and the 'htmlwidgets' package. These trees can be used directly from the R console, from 'RStudio', in Shiny apps, and in R Markdown documents.  See <http://phylocanvas.org/>  for more information on the 'phylocanvas' library.",
    "version": "0.1.3",
    "maintainer": "zachary charlop-powers <zach.charlop.powers@gmail.com>",
    "author": "zachary charlop-powers [aut, cre]",
    "url": "https://github.com/zachcp/phylocanvas, http://phylocanvas.org/",
    "bug_reports": "https://github.com/zachcp/phylocanvas/issues",
    "repository": "https://cran.r-project.org/package=phylocanvas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylocanvas Interactive Phylogenetic Trees Using the 'Phylocanvas'\nJavaScript Library Create and customize interactive phylogenetic trees using the 'phylocanvas' JavaScript library and the 'htmlwidgets' package. These trees can be used directly from the R console, from 'RStudio', in Shiny apps, and in R Markdown documents.  See <http://phylocanvas.org/>  for more information on the 'phylocanvas' library.  "
  },
  {
    "id": 17920,
    "package_name": "phylosem",
    "title": "Phylogenetic Structural Equation Model",
    "description": "Applies phylogenetic comparative methods (PCM) and phylogenetic trait imputation using \n    structural equation models (SEM), extending methods from Thorson et al. (2023) <doi:10.1111/2041-210X.14076>.  \n    This implementation includes a minimal set of features, to \n    allow users to easily read all of the documentation and source code.  PCM using SEM \n    includes phylogenetic linear models and structural equation models as nested submodels, \n    but also allows imputation of missing values.  Features and comparison with other packages\n    are described in Thorson and van der Bijl (2023) <doi:10.1111/jeb.14234>. ",
    "version": "1.1.4",
    "maintainer": "James Thorson <James.Thorson@noaa.gov>",
    "author": "James Thorson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7415-1010>),\n  Wouter van der Bijl [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7366-1868>)",
    "url": "https://james-thorson-noaa.github.io/phylosem/",
    "bug_reports": "https://github.com/James-Thorson-NOAA/phylosem/issues",
    "repository": "https://cran.r-project.org/package=phylosem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylosem Phylogenetic Structural Equation Model Applies phylogenetic comparative methods (PCM) and phylogenetic trait imputation using \n    structural equation models (SEM), extending methods from Thorson et al. (2023) <doi:10.1111/2041-210X.14076>.  \n    This implementation includes a minimal set of features, to \n    allow users to easily read all of the documentation and source code.  PCM using SEM \n    includes phylogenetic linear models and structural equation models as nested submodels, \n    but also allows imputation of missing values.  Features and comparison with other packages\n    are described in Thorson and van der Bijl (2023) <doi:10.1111/jeb.14234>.   "
  },
  {
    "id": 17946,
    "package_name": "pikchr",
    "title": "R Wrapper for 'pikchr' (PIC) Diagram Language",
    "description": "An 'R' interface to 'pikchr' (<https://pikchr.org>, pronounced \u201cpicture\u201d), a 'PIC'-like markup language for creating diagrams within technical documentation. Originally developed by Brian Kernighan, 'PIC' has been adapted into 'pikchr' by D. Richard Hipp, the creator of 'SQLite'. 'pikchr' is designed to be embedded in fenced code blocks of Markdown or other documentation markup languages, making it ideal for generating diagrams in text-based formats. This package allows R users to seamlessly integrate the descriptive syntax of 'pikchr' for diagram creation directly within the 'R' environment.",
    "version": "1.0.3",
    "maintainer": "Andre Leite <leite@castlab.org>",
    "author": "Andre Leite [aut, cre],\n  Hugo Vaconcelos [aut],\n  Richard Hipp [ctb],\n  Brian Kernighan [ctb]",
    "url": "<https://github.com/StrategicProjects/pikchr>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pikchr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pikchr R Wrapper for 'pikchr' (PIC) Diagram Language An 'R' interface to 'pikchr' (<https://pikchr.org>, pronounced \u201cpicture\u201d), a 'PIC'-like markup language for creating diagrams within technical documentation. Originally developed by Brian Kernighan, 'PIC' has been adapted into 'pikchr' by D. Richard Hipp, the creator of 'SQLite'. 'pikchr' is designed to be embedded in fenced code blocks of Markdown or other documentation markup languages, making it ideal for generating diagrams in text-based formats. This package allows R users to seamlessly integrate the descriptive syntax of 'pikchr' for diagram creation directly within the 'R' environment.  "
  },
  {
    "id": 17952,
    "package_name": "pinochet",
    "title": "Data About the Victims of the Pinochet Regime, 1973-1990",
    "description": "Packages data about the victims of the Pinochet regime as compiled by the Chilean National Commission for Truth and Reconciliation Report (1991, ISBN:9780268016463).",
    "version": "0.1.0",
    "maintainer": "Danilo Freire <danilofreire@gmail.com>",
    "author": "Danilo Freire [aut, cre],\n  Lucas Mingardi [aut],\n  Robert McDonnell [aut]",
    "url": "http://github.com/danilofreire/pinochet",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pinochet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pinochet Data About the Victims of the Pinochet Regime, 1973-1990 Packages data about the victims of the Pinochet regime as compiled by the Chilean National Commission for Truth and Reconciliation Report (1991, ISBN:9780268016463).  "
  },
  {
    "id": 17953,
    "package_name": "pinp",
    "title": "'pinp' is not 'PNAS'",
    "description": "A 'PNAS'-alike style for 'rmarkdown', derived from the 'Proceedings of the\n National Academy of Sciences of the United States of America' ('PNAS') 'LaTeX' style,\n and adapted for use with 'markdown' and 'pandoc'.",
    "version": "0.0.11",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  James Joseph Balamuta [aut] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>)",
    "url": "https://github.com/eddelbuettel/pinp,\nhttp://dirk.eddelbuettel.com/code/pinp.html",
    "bug_reports": "https://github.com/eddelbuettel/pinp/issues",
    "repository": "https://cran.r-project.org/package=pinp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pinp 'pinp' is not 'PNAS' A 'PNAS'-alike style for 'rmarkdown', derived from the 'Proceedings of the\n National Academy of Sciences of the United States of America' ('PNAS') 'LaTeX' style,\n and adapted for use with 'markdown' and 'pandoc'.  "
  },
  {
    "id": 17973,
    "package_name": "piwikproR",
    "title": "Access 'Piwik Pro' Website Statistics",
    "description": "Run Queries against the API of 'Piwik Pro' <https://developers.piwik.pro/en/latest/custom_reports/http_api/http_api.html>. The result is a tibble.",
    "version": "0.4.0",
    "maintainer": "Martin Stingl <martin.stingl@dfv.de>",
    "author": "Martin Stingl <martin.stingl@dfv.de>",
    "url": "https://piwikpror.rstats-tips.net",
    "bug_reports": "https://github.com/dfv-ms/piwikproR/issues",
    "repository": "https://cran.r-project.org/package=piwikproR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "piwikproR Access 'Piwik Pro' Website Statistics Run Queries against the API of 'Piwik Pro' <https://developers.piwik.pro/en/latest/custom_reports/http_api/http_api.html>. The result is a tibble.  "
  },
  {
    "id": 17977,
    "package_name": "pixiedust",
    "title": "Tables so Beautifully Fine-Tuned You Will Believe It's Magic",
    "description": "The introduction of the 'broom' package has made converting model\n    objects into data frames as simple as a single function. While the 'broom'\n    package focuses on providing tidy data frames that can be used in advanced\n    analysis, it deliberately stops short of providing functionality for reporting\n    models in publication-ready tables. 'pixiedust' provides this functionality with\n    a programming interface intended to be similar to 'ggplot2's system of layers\n    with fine tuned control over each cell of the table. Options for output include\n    printing to the console and to the common markdown formats (markdown, HTML, and\n    LaTeX). With a little 'pixiedust' (and happy thoughts) tables can really fly.",
    "version": "0.9.4",
    "maintainer": "Benjamin Nutter <benjamin.nutter@gmail.com>",
    "author": "Benjamin Nutter [aut, cre],\n  David Kretch [ctb]",
    "url": "https://github.com/nutterb/pixiedust",
    "bug_reports": "https://github.com/nutterb/pixiedust/issues",
    "repository": "https://cran.r-project.org/package=pixiedust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pixiedust Tables so Beautifully Fine-Tuned You Will Believe It's Magic The introduction of the 'broom' package has made converting model\n    objects into data frames as simple as a single function. While the 'broom'\n    package focuses on providing tidy data frames that can be used in advanced\n    analysis, it deliberately stops short of providing functionality for reporting\n    models in publication-ready tables. 'pixiedust' provides this functionality with\n    a programming interface intended to be similar to 'ggplot2's system of layers\n    with fine tuned control over each cell of the table. Options for output include\n    printing to the console and to the common markdown formats (markdown, HTML, and\n    LaTeX). With a little 'pixiedust' (and happy thoughts) tables can really fly.  "
  },
  {
    "id": 17983,
    "package_name": "pkgKitten",
    "title": "Create Simple Packages Which Do not Upset R Package Checks",
    "description": "Provides a function kitten() which creates cute little \n packages which pass R package checks. This sets it apart from \n package.skeleton() which it calls, and which leaves imperfect files \n behind. As this is not exactly helpful for beginners, kitten() offers \n an alternative. Unit test support can be added via the 'tinytest'\n package (if present), and documentation-creation support can be\n added via 'roxygen2' (if present).",
    "version": "0.2.4",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>)",
    "url": "https://github.com/eddelbuettel/pkgkitten,\nhttps://eddelbuettel.github.io/pkgkitten/",
    "bug_reports": "https://github.com/eddelbuettel/pkgkitten/issues",
    "repository": "https://cran.r-project.org/package=pkgKitten",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pkgKitten Create Simple Packages Which Do not Upset R Package Checks Provides a function kitten() which creates cute little \n packages which pass R package checks. This sets it apart from \n package.skeleton() which it calls, and which leaves imperfect files \n behind. As this is not exactly helpful for beginners, kitten() offers \n an alternative. Unit test support can be added via the 'tinytest'\n package (if present), and documentation-creation support can be\n added via 'roxygen2' (if present).  "
  },
  {
    "id": 17986,
    "package_name": "pkgdown.offline",
    "title": "Build 'pkgdown' Websites Offline",
    "description": "Provides support for building 'pkgdown' websites without an\n    internet connection. Works by bundling cached dependencies and\n    implementing drop-in replacements for key 'pkgdown' functions.\n    Enables package documentation websites to be built in environments\n    where internet access is unavailable or restricted.\n    For more details on generating 'pkgdown' websites, see\n    Wickham et al. (2025) <doi:10.32614/CRAN.package.pkgdown>.",
    "version": "0.1.2",
    "maintainer": "Nan Xiao <me@nanx.me>",
    "author": "Nan Xiao [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0250-5673>),\n  John Blischak [aut] (ORCID: <https://orcid.org/0000-0003-2634-9879>),\n  Algolia, Inc. and other contributors [ctb, cph] (autocomplete.js\n    library),\n  Aidan Feldman [ctb, cph] (bootstrap-toc library),\n  Zeno Rocha [ctb, cph] (clipboard.js library),\n  Nick Williams [ctb, cph] (headroom.js library),\n  Julian K\u00fchnel [ctb, cph] (mark.js library),\n  Kiro Risk [ctb, cph] (Fuse.js library),\n  Khan Academy and other contributors [ctb, cph] (KaTeX library),\n  The MathJax Consortium [ctb, cph] (MathJax library)",
    "url": "https://nanx.me/pkgdown.offline/,\nhttps://github.com/nanxstats/pkgdown.offline",
    "bug_reports": "https://github.com/nanxstats/pkgdown.offline/issues",
    "repository": "https://cran.r-project.org/package=pkgdown.offline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pkgdown.offline Build 'pkgdown' Websites Offline Provides support for building 'pkgdown' websites without an\n    internet connection. Works by bundling cached dependencies and\n    implementing drop-in replacements for key 'pkgdown' functions.\n    Enables package documentation websites to be built in environments\n    where internet access is unavailable or restricted.\n    For more details on generating 'pkgdown' websites, see\n    Wickham et al. (2025) <doi:10.32614/CRAN.package.pkgdown>.  "
  },
  {
    "id": 18025,
    "package_name": "plinkQC",
    "title": "Genotype Quality Control with 'PLINK'",
    "description": "Genotyping arrays enable the direct measurement of an individuals\n    genotype at thousands of markers. 'plinkQC' facilitates genotype quality\n    control for genetic association studies as described by Anderson and\n    colleagues (2010) <doi:10.1038/nprot.2010.116>. It makes 'PLINK' basic\n    statistics (e.g. missing genotyping rates per individual, allele frequencies\n    per genetic marker) and relationship functions accessible from 'R' and\n    generates a per-individual and per-marker quality control report.\n    Individuals and markers that fail the quality control can subsequently be\n    removed to generate a new, clean dataset. Removal of individuals based on\n    relationship status is optimised to retain as many individuals as possible\n    in the study. Additionally, there is a trained classifier to predict \n    genomic ancestry of human samples.",
    "version": "1.0.0",
    "maintainer": "Hannah Meyer <hannah.v.meyer@gmail.com>",
    "author": "Hannah Meyer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4564-0899>),\n  Caroline Walter [ctb],\n  Maha Syed [ctb]",
    "url": "https://meyer-lab-cshl.github.io/plinkQC/",
    "bug_reports": "https://github.com/meyer-lab-cshl/plinkQC/issues",
    "repository": "https://cran.r-project.org/package=plinkQC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plinkQC Genotype Quality Control with 'PLINK' Genotyping arrays enable the direct measurement of an individuals\n    genotype at thousands of markers. 'plinkQC' facilitates genotype quality\n    control for genetic association studies as described by Anderson and\n    colleagues (2010) <doi:10.1038/nprot.2010.116>. It makes 'PLINK' basic\n    statistics (e.g. missing genotyping rates per individual, allele frequencies\n    per genetic marker) and relationship functions accessible from 'R' and\n    generates a per-individual and per-marker quality control report.\n    Individuals and markers that fail the quality control can subsequently be\n    removed to generate a new, clean dataset. Removal of individuals based on\n    relationship status is optimised to retain as many individuals as possible\n    in the study. Additionally, there is a trained classifier to predict \n    genomic ancestry of human samples.  "
  },
  {
    "id": 18029,
    "package_name": "plnr",
    "title": "A Framework for Planning and Executing Analyses",
    "description": "A comprehensive framework for planning and executing analyses in R. \n    It provides a structured approach to running the same function multiple times \n    with different arguments, executing multiple functions on the same datasets, \n    and creating systematic analyses across multiple strata or variables. \n    The framework is particularly useful for applying the same analysis across \n    multiple strata (e.g., locations, age groups), running statistical methods \n    on multiple variables (e.g., exposures, outcomes), generating multiple tables \n    or graphs for reports, and creating systematic surveillance analyses. \n    Key features include efficient data management, structured analysis planning, \n    flexible execution options, built-in debugging tools, and hash-based caching.",
    "version": "2025.11.22",
    "maintainer": "Richard Aubrey White <hello@rwhite.no>",
    "author": "Richard Aubrey White [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6747-1726>)",
    "url": "https://www.rwhite.no/plnr/, https://github.com/raubreywhite/plnr",
    "bug_reports": "https://github.com/raubreywhite/plnr/issues",
    "repository": "https://cran.r-project.org/package=plnr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plnr A Framework for Planning and Executing Analyses A comprehensive framework for planning and executing analyses in R. \n    It provides a structured approach to running the same function multiple times \n    with different arguments, executing multiple functions on the same datasets, \n    and creating systematic analyses across multiple strata or variables. \n    The framework is particularly useful for applying the same analysis across \n    multiple strata (e.g., locations, age groups), running statistical methods \n    on multiple variables (e.g., exposures, outcomes), generating multiple tables \n    or graphs for reports, and creating systematic surveillance analyses. \n    Key features include efficient data management, structured analysis planning, \n    flexible execution options, built-in debugging tools, and hash-based caching.  "
  },
  {
    "id": 18053,
    "package_name": "plotor",
    "title": "Odds Ratio Tools for Logistic Regression",
    "description": "\n    Produces odds ratio analyses with comprehensive reporting tools. Generates \n    plots, summary tables, and diagnostic checks for logistic regression models \n    fitted with 'glm()' using binomial family. Provides visualisation methods, \n    formatted reporting tables via 'gt', and tools to assess logistic regression \n    model assumptions.",
    "version": "0.8.0",
    "maintainer": "Craig Parylo <craig.parylo2@nhs.net>",
    "author": "Craig Parylo [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4297-7808>)",
    "url": "https://github.com/craig-parylo/plotor,\nhttps://craig-parylo.github.io/plotor/",
    "bug_reports": "https://github.com/craig-parylo/plotor/issues",
    "repository": "https://cran.r-project.org/package=plotor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plotor Odds Ratio Tools for Logistic Regression \n    Produces odds ratio analyses with comprehensive reporting tools. Generates \n    plots, summary tables, and diagnostic checks for logistic regression models \n    fitted with 'glm()' using binomial family. Provides visualisation methods, \n    formatted reporting tables via 'gt', and tools to assess logistic regression \n    model assumptions.  "
  },
  {
    "id": 18074,
    "package_name": "plumber2",
    "title": "Easy and Powerful Webservers",
    "description": "Automatically create a webserver from annotated 'R' files or by\n    building it up programmatically. Provides automatic 'OpenAPI' documentation,\n    input handling, async support, and middleware support.",
    "version": "0.1.1",
    "maintainer": "Thomas Lin Pedersen <thomas.pedersen@posit.co>",
    "author": "Thomas Lin Pedersen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5147-4711>),\n  Posit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://plumber2.posit.co/, https://github.com/posit-dev/plumber2",
    "bug_reports": "https://github.com/posit-dev/plumber2/issues",
    "repository": "https://cran.r-project.org/package=plumber2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plumber2 Easy and Powerful Webservers Automatically create a webserver from annotated 'R' files or by\n    building it up programmatically. Provides automatic 'OpenAPI' documentation,\n    input handling, async support, and middleware support.  "
  },
  {
    "id": 18077,
    "package_name": "plume",
    "title": "A Simple Author Handler for Scientific Writing",
    "description": "Handles and formats author information in scientific writing\n    in 'R Markdown' and 'Quarto'. 'plume' provides easy-to-use and\n    flexible tools for inserting author data in 'YAML' as well as\n    generating author and contribution lists (among others) as strings\n    from tabular data.",
    "version": "0.3.0",
    "maintainer": "Arnaud Gallou <arangacas@gmail.com>",
    "author": "Arnaud Gallou [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1002-4247>)",
    "url": "https://arnaudgallou.github.io/plume/,\nhttps://github.com/arnaudgallou/plume",
    "bug_reports": "https://github.com/arnaudgallou/plume/issues",
    "repository": "https://cran.r-project.org/package=plume",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plume A Simple Author Handler for Scientific Writing Handles and formats author information in scientific writing\n    in 'R Markdown' and 'Quarto'. 'plume' provides easy-to-use and\n    flexible tools for inserting author data in 'YAML' as well as\n    generating author and contribution lists (among others) as strings\n    from tabular data.  "
  },
  {
    "id": 18086,
    "package_name": "pmclust",
    "title": "Parallel Model-Based Clustering using\nExpectation-Gathering-Maximization Algorithm for Finite Mixture\nGaussian Model",
    "description": "Aims to utilize model-based clustering (unsupervised)\n        for high dimensional and ultra large data, especially in a distributed\n        manner. The code employs 'pbdMPI' to perform a\n        expectation-gathering-maximization algorithm\n        for finite mixture Gaussian\n        models. The unstructured dispersion matrices are assumed in the\n        Gaussian models. The implementation is default in the single program\n        multiple data programming model. The code can be executed\n        through 'pbdMPI' and MPI' implementations such as 'OpenMPI'\n        and 'MPICH'.\n        See the High Performance Statistical Computing website\n\t<https://snoweye.github.io/hpsc/>\n\tfor more information, documents and examples.",
    "version": "0.2-1",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "author": "Wei-Chen Chen [aut, cre],\n  George Ostrouchov [aut]",
    "url": "https://pbdr.org/",
    "bug_reports": "https://github.com/snoweye/pmclust/issues",
    "repository": "https://cran.r-project.org/package=pmclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmclust Parallel Model-Based Clustering using\nExpectation-Gathering-Maximization Algorithm for Finite Mixture\nGaussian Model Aims to utilize model-based clustering (unsupervised)\n        for high dimensional and ultra large data, especially in a distributed\n        manner. The code employs 'pbdMPI' to perform a\n        expectation-gathering-maximization algorithm\n        for finite mixture Gaussian\n        models. The unstructured dispersion matrices are assumed in the\n        Gaussian models. The implementation is default in the single program\n        multiple data programming model. The code can be executed\n        through 'pbdMPI' and MPI' implementations such as 'OpenMPI'\n        and 'MPICH'.\n        See the High Performance Statistical Computing website\n\t<https://snoweye.github.io/hpsc/>\n\tfor more information, documents and examples.  "
  },
  {
    "id": 18088,
    "package_name": "pmetar",
    "title": "Processing METAR Weather Reports",
    "description": "Allows to download current and historical METAR weather reports\n  extract and parse basic parameters and present main weather information. \n  Current reports are downloaded from Aviation Weather Center \n  <https://aviationweather.gov/data/metar/> and historical reports from\n  Iowa Environmental Mesonet web page of Iowa State University\n  ASOS-AWOS-METAR <http://mesonet.agron.iastate.edu/AWOS/>.",
    "version": "0.6.0",
    "maintainer": "Pawel Cwiek <prc.altodato@gmail.com>",
    "author": "Pawel Cwiek [aut, cre],\n  David Megginson [ctb] (Author of data set with airports list\n    https://ourairports.com/data/),\n  Greg Thompson [ctb] (Author of data set with airports list\n    https://weather.ral.ucar.edu/surface/stations.txt)",
    "url": "https://github.com/prcwiek/pmetar",
    "bug_reports": "https://github.com/prcwiek/pmetar/issues",
    "repository": "https://cran.r-project.org/package=pmetar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmetar Processing METAR Weather Reports Allows to download current and historical METAR weather reports\n  extract and parse basic parameters and present main weather information. \n  Current reports are downloaded from Aviation Weather Center \n  <https://aviationweather.gov/data/metar/> and historical reports from\n  Iowa Environmental Mesonet web page of Iowa State University\n  ASOS-AWOS-METAR <http://mesonet.agron.iastate.edu/AWOS/>.  "
  },
  {
    "id": 18101,
    "package_name": "pmwg",
    "title": "Particle Metropolis Within Gibbs",
    "description": "Provides an R implementation of the Particle Metropolis within\n    Gibbs sampler for model parameter, covariance matrix and random effect\n    estimation. A more general implementation of the sampler based on the paper\n    by Gunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., & Brown, S. D.\n    (2020) <doi:10.1016/j.jmp.2020.102368>. An HTML tutorial document describing\n    the package is available at\n    <https://university-of-newcastle-research.github.io/samplerDoc/> and\n    includes several detailed examples, some background and troubleshooting\n    steps.",
    "version": "0.2.7",
    "maintainer": "Gavin Cooper <gavin@gavincooper.net>",
    "author": "Gavin Cooper [aut, cre, trl] (Package creator and maintainer),\n  Reilly Innes [aut],\n  Caroline Kuhne [aut],\n  Jon-Paul Cavallaro [aut],\n  David Gunawan [aut] (Author of original MATLAB code),\n  Guy Hawkins [aut],\n  Scott Brown [aut, trl] (Original translation from MATLAB to R),\n  Niek Stevenson [aut]",
    "url": "https://github.com/university-of-newcastle-research/pmwg",
    "bug_reports": "https://github.com/university-of-newcastle-research/pmwg/issues",
    "repository": "https://cran.r-project.org/package=pmwg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmwg Particle Metropolis Within Gibbs Provides an R implementation of the Particle Metropolis within\n    Gibbs sampler for model parameter, covariance matrix and random effect\n    estimation. A more general implementation of the sampler based on the paper\n    by Gunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., & Brown, S. D.\n    (2020) <doi:10.1016/j.jmp.2020.102368>. An HTML tutorial document describing\n    the package is available at\n    <https://university-of-newcastle-research.github.io/samplerDoc/> and\n    includes several detailed examples, some background and troubleshooting\n    steps.  "
  },
  {
    "id": 18114,
    "package_name": "pogit",
    "title": "Bayesian Variable Selection for a Poisson-Logistic Model",
    "description": "Bayesian variable selection for regression models of under-reported\n    count data as well as for (overdispersed) Poisson, negative binomal and\n    binomial logit regression models using spike and slab priors.",
    "version": "1.3.0",
    "maintainer": "Michaela Dvorzak <m.dvorzak@gmx.at>",
    "author": "Michaela Dvorzak [aut, cre],\n  Helga Wagner [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pogit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pogit Bayesian Variable Selection for a Poisson-Logistic Model Bayesian variable selection for regression models of under-reported\n    count data as well as for (overdispersed) Poisson, negative binomal and\n    binomial logit regression models using spike and slab priors.  "
  },
  {
    "id": 18136,
    "package_name": "polished",
    "title": "Authentication and Hosting for 'shiny' Apps",
    "description": "Authentication, user administration, hosting, and additional infrastructure for 'shiny' apps. See\n  <https://polished.tech> for additional documentation and examples.",
    "version": "0.8.1",
    "maintainer": "Andy Merlino <andy.merlino@tychobra.com>",
    "author": "Andy Merlino [aut, cre],\n  Patrick Howard [aut],\n  Tychobra LLC [cph, fnd]",
    "url": "https://github.com/tychobra/polished, https://polished.tech",
    "bug_reports": "https://github.com/tychobra/polished/issues",
    "repository": "https://cran.r-project.org/package=polished",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "polished Authentication and Hosting for 'shiny' Apps Authentication, user administration, hosting, and additional infrastructure for 'shiny' apps. See\n  <https://polished.tech> for additional documentation and examples.  "
  },
  {
    "id": 18138,
    "package_name": "politeness",
    "title": "Detecting Politeness Features in Text",
    "description": "Detecting markers of politeness in English natural language. This package allows researchers to easily visualize and quantify politeness between groups of documents. This package combines prior research on the linguistic markers of politeness. We thank the Spencer Foundation, the Hewlett Foundation, and Harvard's Institute for Quantitative Social Science for support.",
    "version": "0.9.4",
    "maintainer": "Mike Yeomans <mk.yeomans@gmail.com>",
    "author": "Mike Yeomans [aut, cre],\n  Alejandro Kantor [aut],\n  Dustin Tingley [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=politeness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "politeness Detecting Politeness Features in Text Detecting markers of politeness in English natural language. This package allows researchers to easily visualize and quantify politeness between groups of documents. This package combines prior research on the linguistic markers of politeness. We thank the Spencer Foundation, the Hewlett Foundation, and Harvard's Institute for Quantitative Social Science for support.  "
  },
  {
    "id": 18140,
    "package_name": "polle",
    "title": "Policy Learning",
    "description": "Package for learning and evaluating (subgroup) policies via doubly robust loss functions. Policy learning methods include doubly robust blip/conditional average treatment effect learning and sequential policy tree learning. Methods for (subgroup) policy evaluation include doubly robust cross-fitting and online estimation/sequential validation. See Nordland and Holst (2022) <doi:10.48550/arXiv.2212.02335> for documentation and references.",
    "version": "1.6.2",
    "maintainer": "Andreas Nordland <andreasnordland@gmail.com>",
    "author": "Andreas Nordland [aut, cre],\n  Klaus Holst [aut] (ORCID: <https://orcid.org/0000-0002-1364-6789>)",
    "url": "",
    "bug_reports": "https://github.com/AndreasNordland/polle/issues",
    "repository": "https://cran.r-project.org/package=polle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "polle Policy Learning Package for learning and evaluating (subgroup) policies via doubly robust loss functions. Policy learning methods include doubly robust blip/conditional average treatment effect learning and sequential policy tree learning. Methods for (subgroup) policy evaluation include doubly robust cross-fitting and online estimation/sequential validation. See Nordland and Holst (2022) <doi:10.48550/arXiv.2212.02335> for documentation and references.  "
  },
  {
    "id": 18142,
    "package_name": "polmineR",
    "title": "Verbs and Nouns for Corpus Analysis",
    "description": "Package for corpus analysis using the Corpus Workbench \n    ('CWB', <https://cwb.sourceforge.io>) as an efficient back end for indexing\n    and querying large corpora. The package offers functionality to flexibly create\n    subcorpora and to carry out basic statistical operations (count, co-occurrences\n    etc.). The original full text of documents can be reconstructed and inspected at\n    any time. Beyond that, the package is intended to serve as an interface to \n    packages implementing advanced statistical procedures. Respective data structures\n    (document-term matrices, term-co-occurrence matrices etc.) can be created based \n    on the indexed corpora.",
    "version": "0.8.9",
    "maintainer": "Andreas Blaette <andreas.blaette@uni-due.de>",
    "author": "Andreas Blaette [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8970-8010>),\n  Christoph Leonhardt [ctb],\n  Marius Bertram [ctb]",
    "url": "https://github.com/PolMine/polmineR",
    "bug_reports": "https://github.com/PolMine/polmineR/issues",
    "repository": "https://cran.r-project.org/package=polmineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "polmineR Verbs and Nouns for Corpus Analysis Package for corpus analysis using the Corpus Workbench \n    ('CWB', <https://cwb.sourceforge.io>) as an efficient back end for indexing\n    and querying large corpora. The package offers functionality to flexibly create\n    subcorpora and to carry out basic statistical operations (count, co-occurrences\n    etc.). The original full text of documents can be reconstructed and inspected at\n    any time. Beyond that, the package is intended to serve as an interface to \n    packages implementing advanced statistical procedures. Respective data structures\n    (document-term matrices, term-co-occurrence matrices etc.) can be created based \n    on the indexed corpora.  "
  },
  {
    "id": 18211,
    "package_name": "positron.tutorials",
    "title": "Tutorials for Learning 'Positron' and Related Tools",
    "description": "Collection of tutorials for working with 'Positron'. Covers scripts, \n    'Quarto' documents, 'Git', 'GitHub', and 'Quarto' websites. Makes extensive use of \n    the tools in the 'tutorial.helpers' package. ",
    "version": "0.2.0",
    "maintainer": "David Kane <dave.kane@gmail.com>",
    "author": "David Kane [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6660-3934>)",
    "url": "https://ppbds.github.io/positron.tutorials/,\nhttps://github.com/PPBDS/positron.tutorials",
    "bug_reports": "https://github.com/PPBDS/positron.tutorials/issues",
    "repository": "https://cran.r-project.org/package=positron.tutorials",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "positron.tutorials Tutorials for Learning 'Positron' and Related Tools Collection of tutorials for working with 'Positron'. Covers scripts, \n    'Quarto' documents, 'Git', 'GitHub', and 'Quarto' websites. Makes extensive use of \n    the tools in the 'tutorial.helpers' package.   "
  },
  {
    "id": 18213,
    "package_name": "postGGIR",
    "title": "Data Processing after Running 'GGIR' for Accelerometer Data",
    "description": "Generate all necessary R/Rmd/shell files for data processing after running 'GGIR' (v2.4.0) for accelerometer data. In part 1, all csv files in the GGIR output directory were read, transformed and then merged. In part 2, the GGIR output files were checked and summarized in one excel sheet. In part 3, the merged data was cleaned according to the number of valid hours on each night and the number of valid days for each subject. In part 4, the cleaned activity data was imputed by the average Euclidean norm minus one (ENMO) over all the valid days for each subject. Finally, a comprehensive report of data processing was created using Rmarkdown, and the report includes few exploratory plots and multiple commonly used features extracted from minute level actigraphy data. ",
    "version": "2.4.0.2",
    "maintainer": "Wei Guo <wei.guo3@nih.gov>",
    "author": "Wei Guo [aut, cre],\n  Andrew Leroux [aut],\n  Vadim Zipunnikov [aut],\n  Kathleen Merikangas [aut]",
    "url": "https://github.com/dora201888/postGGIR",
    "bug_reports": "https://github.com/dora201888/postGGIR/issues",
    "repository": "https://cran.r-project.org/package=postGGIR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "postGGIR Data Processing after Running 'GGIR' for Accelerometer Data Generate all necessary R/Rmd/shell files for data processing after running 'GGIR' (v2.4.0) for accelerometer data. In part 1, all csv files in the GGIR output directory were read, transformed and then merged. In part 2, the GGIR output files were checked and summarized in one excel sheet. In part 3, the merged data was cleaned according to the number of valid hours on each night and the number of valid days for each subject. In part 4, the cleaned activity data was imputed by the average Euclidean norm minus one (ENMO) over all the valid days for each subject. Finally, a comprehensive report of data processing was created using Rmarkdown, and the report includes few exploratory plots and multiple commonly used features extracted from minute level actigraphy data.   "
  },
  {
    "id": 18216,
    "package_name": "postcards",
    "title": "Create Beautiful, Simple Personal Websites",
    "description": "A collection of R Markdown templates for creating simple and easy \n  to personalize single page websites.",
    "version": "0.2.3",
    "maintainer": "Sean Kross <sean@seankross.com>",
    "author": "Sean Kross [aut, cre] (ORCID: <https://orcid.org/0000-0001-5215-0316>)",
    "url": "https://github.com/seankross/postcards",
    "bug_reports": "https://github.com/seankross/postcards/issues",
    "repository": "https://cran.r-project.org/package=postcards",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "postcards Create Beautiful, Simple Personal Websites A collection of R Markdown templates for creating simple and easy \n  to personalize single page websites.  "
  },
  {
    "id": 18245,
    "package_name": "powerbydesign",
    "title": "Power Estimates for ANOVA Designs",
    "description": "Functions for bootstrapping the power of ANOVA designs\n    based on estimated means and standard deviations of the conditions.\n    Please refer to the documentation of the boot.power.anova() function\n    for further details.",
    "version": "1.0.5",
    "maintainer": "Frank Papenmeier <frank.papenmeier@uni-tuebingen.de>",
    "author": "Frank Papenmeier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerbydesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerbydesign Power Estimates for ANOVA Designs Functions for bootstrapping the power of ANOVA designs\n    based on estimated means and standard deviations of the conditions.\n    Please refer to the documentation of the boot.power.anova() function\n    for further details.  "
  },
  {
    "id": 18253,
    "package_name": "ppRank",
    "title": "Classification of Algorithms",
    "description": "Implements the Bi-objective Lexicographical Classification method and Performance Assessment Ratio at 10% metric for algorithm classification. Constructs matrices representing algorithm performance under multiple criteria, facilitating decision-making in algorithm selection and evaluation. Analyzes and compares algorithm performance based on various metrics to identify the most suitable algorithms for specific tasks. This package includes methods for algorithm classification and evaluation, with examples provided in the documentation. Carvalho (2019) presents a statistical evaluation of algorithmic computational experimentation with infeasible solutions <doi:10.48550/arXiv.1902.00101>. Moreira and Carvalho (2023) analyze power in preprocessing methodologies for datasets with missing values <doi:10.1080/03610918.2023.2234683>.",
    "version": "0.1.1",
    "maintainer": "Iago Augusto de Carvalho <iago.carvalho@unifal-mg.edu.br>",
    "author": "Tiago Costa Soares [aut],\n  Iago Augusto de Carvalho [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ppRank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ppRank Classification of Algorithms Implements the Bi-objective Lexicographical Classification method and Performance Assessment Ratio at 10% metric for algorithm classification. Constructs matrices representing algorithm performance under multiple criteria, facilitating decision-making in algorithm selection and evaluation. Analyzes and compares algorithm performance based on various metrics to identify the most suitable algorithms for specific tasks. This package includes methods for algorithm classification and evaluation, with examples provided in the documentation. Carvalho (2019) presents a statistical evaluation of algorithmic computational experimentation with infeasible solutions <doi:10.48550/arXiv.1902.00101>. Moreira and Carvalho (2023) analyze power in preprocessing methodologies for datasets with missing values <doi:10.1080/03610918.2023.2234683>.  "
  },
  {
    "id": 18264,
    "package_name": "ppitables",
    "title": "Lookup Tables to Generate Poverty Likelihoods and Rates using\nthe Poverty Probability Index (PPI)",
    "description": "The Poverty Probability Index (PPI) is a poverty measurement tool \n    for organizations and businesses with a mission to serve the poor. The PPI \n    is statistically-sound, yet simple to use: the answers to 10 questions about \n    a household's characteristics and asset ownership are scored to compute the \n    likelihood that the household is living below the poverty line - or above by \n    only a narrow margin. This package contains country-specific lookup data\n    tables used as reference to determine the poverty likelihood of a household\n    based on their score from the country-specific PPI questionnaire. These\n    lookup tables have been extracted from documentation of the PPI found at \n    <https://www.povertyindex.org> and managed by Innovations for Poverty Action \n    <https://poverty-action.org/>.",
    "version": "0.6.0",
    "maintainer": "Ernest Guevarra <ernestgmd@gmail.com>",
    "author": "Ernest Guevarra [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4887-4415>)",
    "url": "https://github.com/katilingban/ppitables,\nhttps://katilingban.io/ppitables/",
    "bug_reports": "https://github.com/katilingban/ppitables/issues",
    "repository": "https://cran.r-project.org/package=ppitables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ppitables Lookup Tables to Generate Poverty Likelihoods and Rates using\nthe Poverty Probability Index (PPI) The Poverty Probability Index (PPI) is a poverty measurement tool \n    for organizations and businesses with a mission to serve the poor. The PPI \n    is statistically-sound, yet simple to use: the answers to 10 questions about \n    a household's characteristics and asset ownership are scored to compute the \n    likelihood that the household is living below the poverty line - or above by \n    only a narrow margin. This package contains country-specific lookup data\n    tables used as reference to determine the poverty likelihood of a household\n    based on their score from the country-specific PPI questionnaire. These\n    lookup tables have been extracted from documentation of the PPI found at \n    <https://www.povertyindex.org> and managed by Innovations for Poverty Action \n    <https://poverty-action.org/>.  "
  },
  {
    "id": 18302,
    "package_name": "predfairness",
    "title": "Discrimination Mitigation for Machine Learning Models",
    "description": "Based on different statistical definitions of discrimination, several methods have been proposed to detect and mitigate social inequality in machine learning models. \n  This package aims to provide an alternative to fairness treatment in predictive models. The ROC method implemented in this package\n  is described by Kamiran, Karim and Zhang (2012) <https://ieeexplore.ieee.org/document/6413831/>.",
    "version": "0.1.0",
    "maintainer": "Tha\u00eds de Bessa Gontijo de Oliveira <thais.bgo@gmail.com>",
    "author": "Tha\u00eds de Bessa Gontijo de Oliveira [aut, cre],\n  Leonardo Paes Vieira [aut],\n  Gustavo Rodrigues Lacerda Silva [ctb],\n  Barbara Bianca Alves Cardoso [ctb],\n  Douglas Alexandre Gomes Vieira [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=predfairness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "predfairness Discrimination Mitigation for Machine Learning Models Based on different statistical definitions of discrimination, several methods have been proposed to detect and mitigate social inequality in machine learning models. \n  This package aims to provide an alternative to fairness treatment in predictive models. The ROC method implemented in this package\n  is described by Kamiran, Karim and Zhang (2012) <https://ieeexplore.ieee.org/document/6413831/>.  "
  },
  {
    "id": 18307,
    "package_name": "prediction",
    "title": "Tidy, Type-Safe 'prediction()' Methods",
    "description": "A one-function package containing prediction(), a type-safe alternative to predict() that always returns a data frame. The summary() method provides a data frame with average predictions, possibly over counterfactual versions of the data (\u00e0 la the margins command in 'Stata'). Marginal effect estimation is provided by the related package, 'margins' <https://cran.r-project.org/package=margins>. The package currently supports common model types (e.g., lm, glm) from the 'stats' package, as well as numerous other model classes from other add-on packages. See the README file or main package documentation page for a complete listing.",
    "version": "0.3.18",
    "maintainer": "Ben Bolker <bolker@mcmaster.ca>",
    "author": "Thomas J. Leeper [aut] (ORCID: <https://orcid.org/0000-0003-4097-6326>),\n  Carl Ganz [ctb],\n  Vincent Arel-Bundock [ctb] (ORCID:\n    <https://orcid.org/0000-0003-2042-7063>),\n  Ben Bolker [ctb, cre] (ORCID: <https://orcid.org/0000-0002-2127-0443>)",
    "url": "https://github.com/bbolker/prediction",
    "bug_reports": "https://github.com/bbolker/prediction/issues",
    "repository": "https://cran.r-project.org/package=prediction",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prediction Tidy, Type-Safe 'prediction()' Methods A one-function package containing prediction(), a type-safe alternative to predict() that always returns a data frame. The summary() method provides a data frame with average predictions, possibly over counterfactual versions of the data (\u00e0 la the margins command in 'Stata'). Marginal effect estimation is provided by the related package, 'margins' <https://cran.r-project.org/package=margins>. The package currently supports common model types (e.g., lm, glm) from the 'stats' package, as well as numerous other model classes from other add-on packages. See the README file or main package documentation page for a complete listing.  "
  },
  {
    "id": 18319,
    "package_name": "preference",
    "title": "2-Stage Preference Trial Design and Analysis",
    "description": "Design and analyze two-stage randomized trials with a continuous\n    outcome measure. The package contains functions to compute the required \n    sample size needed to detect a given preference, treatment, and selection \n    effect; alternatively, the package contains functions that can report the \n    study power given a fixed sample size. Finally, analysis functions are \n    provided to test each effect using either summary data (i.e. means, \n    variances) or raw study data <doi:10.18637/jss.v094.c02>.",
    "version": "1.1.6",
    "maintainer": "Michael Kane <michael.kane@yale.edu>",
    "author": "Briana Cameron [aut, cph],\n  Denise Esserman [ctb],\n  Michael Kane [cre, ctb] (ORCID:\n    <https://orcid.org/0000-0003-1899-6662>)",
    "url": "https://github.com/kaneplusplus/preference",
    "bug_reports": "https://github.com/kaneplusplus/preference/issues",
    "repository": "https://cran.r-project.org/package=preference",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "preference 2-Stage Preference Trial Design and Analysis Design and analyze two-stage randomized trials with a continuous\n    outcome measure. The package contains functions to compute the required \n    sample size needed to detect a given preference, treatment, and selection \n    effect; alternatively, the package contains functions that can report the \n    study power given a fixed sample size. Finally, analysis functions are \n    provided to test each effect using either summary data (i.e. means, \n    variances) or raw study data <doi:10.18637/jss.v094.c02>.  "
  },
  {
    "id": 18328,
    "package_name": "prereg",
    "title": "R Markdown Templates to Preregister Scientific Studies",
    "description": "Provides a collection of templates to author preregistration documents for scientific studies in PDF format.",
    "version": "0.6.0",
    "maintainer": "Frederik Aust <frederik.aust@uni-koeln.de>",
    "author": "Frederik Aust [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4900-788X>),\n  Lisa Spitzer [aut],\n  Jeffrey R. Stevens [ctb] (ORCID:\n    <https://orcid.org/0000-0003-2375-1360>),\n  Masataka Ogawa [ctb]",
    "url": "https://github.com/crsh/prereg",
    "bug_reports": "https://github.com/crsh/prereg/issues",
    "repository": "https://cran.r-project.org/package=prereg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prereg R Markdown Templates to Preregister Scientific Studies Provides a collection of templates to author preregistration documents for scientific studies in PDF format.  "
  },
  {
    "id": 18329,
    "package_name": "preregr",
    "title": "Specify (Pre)Registrations and Export Them Human- And\nMachine-Readably",
    "description": "Preregistrations, or more generally, registrations, enable\n    explicit timestamped and (often but not necessarily publicly) frozen\n    documentation of plans and expectations as well as decisions and\n    justifications. In research, preregistrations are commonly used to\n    clearly document plans and facilitate justifications of deviations from\n    those plans, as well as decreasing the effects of publication bias by\n    enabling identification of research that was conducted but not published.\n    Like reporting guidelines, (pre)registration forms often have specific\n    structures that facilitate systematic reporting of important items. The\n    'preregr' package facilitates specifying (pre)registrations in R and\n    exporting them to a human-readable format (using R Markdown partials or\n    exporting to an 'HTML' file) as well as human-readable embedded data\n    (using 'JSON'), as well as importing such exported (pre)registration\n    specifications from such embedded 'JSON'.",
    "version": "0.2.9",
    "maintainer": "Gjalt-Jorn Peters <preregr@opens.science>",
    "author": "Gjalt-Jorn Peters [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0336-9589>),\n  Szilvia Z\u00f6rg\u0151 [ctb] (ORCID: <https://orcid.org/0000-0002-6916-2097>),\n  Olmo den Akker [ctb] (ORCID: <https://orcid.org/0000-0002-0712-3746>),\n  Aleksandra Lazi\u0107 [ctb] (ORCID: <https://orcid.org/0000-0002-0433-0483>),\n  Thomas G\u00fcltzow [ctb] (ORCID: <https://orcid.org/0000-0002-9268-1880>)",
    "url": "https://preregr.opens.science",
    "bug_reports": "https://gitlab.com/r-packages/preregr/-/issues",
    "repository": "https://cran.r-project.org/package=preregr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "preregr Specify (Pre)Registrations and Export Them Human- And\nMachine-Readably Preregistrations, or more generally, registrations, enable\n    explicit timestamped and (often but not necessarily publicly) frozen\n    documentation of plans and expectations as well as decisions and\n    justifications. In research, preregistrations are commonly used to\n    clearly document plans and facilitate justifications of deviations from\n    those plans, as well as decreasing the effects of publication bias by\n    enabling identification of research that was conducted but not published.\n    Like reporting guidelines, (pre)registration forms often have specific\n    structures that facilitate systematic reporting of important items. The\n    'preregr' package facilitates specifying (pre)registrations in R and\n    exporting them to a human-readable format (using R Markdown partials or\n    exporting to an 'HTML' file) as well as human-readable embedded data\n    (using 'JSON'), as well as importing such exported (pre)registration\n    specifications from such embedded 'JSON'.  "
  },
  {
    "id": 18331,
    "package_name": "presenter",
    "title": "Present Data with Style",
    "description": "Consists of custom wrapper functions using packages\n    'openxlsx', 'flextable', and 'officer' to create highly formatted MS office friendly output of your data frames.\n    These viewer friendly outputs are intended to match expectations of professional looking presentations\n    in business and consulting scenarios. The functions are opinionated in the sense that they expect the input data\n    frame to have certain properties in order to take advantage of the automated formatting.",
    "version": "0.1.2",
    "maintainer": "Harrison Tietze <Harrison4192@gmail.com>",
    "author": "Harrison Tietze [aut, cre]",
    "url": "https://github.com/Harrison4192/presenter",
    "bug_reports": "https://github.com/Harrison4192/presenter/issues",
    "repository": "https://cran.r-project.org/package=presenter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "presenter Present Data with Style Consists of custom wrapper functions using packages\n    'openxlsx', 'flextable', and 'officer' to create highly formatted MS office friendly output of your data frames.\n    These viewer friendly outputs are intended to match expectations of professional looking presentations\n    in business and consulting scenarios. The functions are opinionated in the sense that they expect the input data\n    frame to have certain properties in order to take advantage of the automated formatting.  "
  },
  {
    "id": 18343,
    "package_name": "prettydoc",
    "title": "Creating Pretty Documents from R Markdown",
    "description": "Creating tiny yet beautiful documents and vignettes from R\n    Markdown. The package provides the 'html_pretty' output format as an\n    alternative to the 'html_document' and 'html_vignette' engines that\n    convert R Markdown into HTML pages. Various themes and syntax highlight\n    styles are supported.",
    "version": "0.4.1",
    "maintainer": "Yixuan Qiu <yixuan.qiu@cos.name>",
    "author": "Yixuan Qiu [aut, cre],\n  Jason Long [ctb] (the Cayman, Tactile and Architect themes),\n  Renyuan Zou [ctb] (the Leonids theme),\n  Michael Rose [ctb] (the HPSTR theme),\n  JJ Allaire [ctb] (pandoc template for rmarkdown),\n  Hadley Wickham [ctb] (the html_vignette() function),\n  Yihui Xie [ctb] (the html_vignette() function),\n  Steve Matteson [ctb] (the Open Sans fonts),\n  Emily Eisenberg [ctb] (the KaTeX library),\n  Sophie Alpert [ctb] (the KaTeX library)",
    "url": "https://github.com/yixuan/prettydoc",
    "bug_reports": "https://github.com/yixuan/prettydoc/issues",
    "repository": "https://cran.r-project.org/package=prettydoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prettydoc Creating Pretty Documents from R Markdown Creating tiny yet beautiful documents and vignettes from R\n    Markdown. The package provides the 'html_pretty' output format as an\n    alternative to the 'html_document' and 'html_vignette' engines that\n    convert R Markdown into HTML pages. Various themes and syntax highlight\n    styles are supported.  "
  },
  {
    "id": 18347,
    "package_name": "prevederer",
    "title": "Wrapper for the 'Prevedere' API",
    "description": "Easy and efficient access to the API provided by 'Prevedere', \n  an industry insights and predictive analytics company. Query and \n  download indicators, models and workbenches built with 'Prevedere' for further \n  analysis and reporting <https://www.prevedere.com/>.",
    "version": "0.0.1",
    "maintainer": "Wil Davis <davis.3243@osu.edu>",
    "author": "Wil Davis [aut, cre] (ORCID: <https://orcid.org/0000-0001-9780-2576>),\n  Hiram Foster [ctb],\n  Ted Dickinson [ctb]",
    "url": "https://github.com/wkdavis/prevederer,\nhttps://api.prevedere.com/index.html,\nhttps://www.prevedere.com/",
    "bug_reports": "https://github.com/wkdavis/prevederer/issues",
    "repository": "https://cran.r-project.org/package=prevederer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prevederer Wrapper for the 'Prevedere' API Easy and efficient access to the API provided by 'Prevedere', \n  an industry insights and predictive analytics company. Query and \n  download indicators, models and workbenches built with 'Prevedere' for further \n  analysis and reporting <https://www.prevedere.com/>.  "
  },
  {
    "id": 18356,
    "package_name": "primarycensored",
    "title": "Primary Event Censored Distributions",
    "description": "Provides functions for working with primary\n    event censored distributions and 'Stan' implementations for use in Bayesian\n    modeling. Primary event censored distributions are useful for modeling\n    delayed reporting scenarios in epidemiology and other fields (Charniga et\n    al. (2024) <doi:10.48550/arXiv.2405.08841>). It also provides support for\n    arbitrary delay distributions, a range of common primary distributions, and\n    allows for truncation and secondary event censoring to be accounted for\n    (Park et al. (2024) <doi:10.1101/2024.01.12.24301247>). A subset of\n    common distributions also have analytical solutions implemented, allowing\n    for faster computation. In addition, it provides multiple methods for\n    fitting primary event censored distributions to data via optional\n    dependencies.",
    "version": "1.3.0",
    "maintainer": "Sam Abbott <contact@samabbott.co.uk>",
    "author": "Sam Abbott [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8057-8037>),\n  Sam Brand [aut] (ORCID: <https://orcid.org/0000-0003-0645-5367>),\n  Adam Howes [ctb] (ORCID: <https://orcid.org/0000-0003-2386-4031>),\n  James Mba Azam [aut] (ORCID: <https://orcid.org/0000-0001-5782-7330>),\n  Carl Pearson [aut] (ORCID: <https://orcid.org/0000-0003-0701-7860>),\n  Sebastian Funk [aut] (ORCID: <https://orcid.org/0000-0002-2842-3406>),\n  Kelly Charniga [aut] (ORCID: <https://orcid.org/0000-0002-7648-7041>)",
    "url": "https://primarycensored.epinowcast.org,\nhttps://github.com/epinowcast/primarycensored",
    "bug_reports": "https://github.com/epinowcast/primarycensored/issues",
    "repository": "https://cran.r-project.org/package=primarycensored",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "primarycensored Primary Event Censored Distributions Provides functions for working with primary\n    event censored distributions and 'Stan' implementations for use in Bayesian\n    modeling. Primary event censored distributions are useful for modeling\n    delayed reporting scenarios in epidemiology and other fields (Charniga et\n    al. (2024) <doi:10.48550/arXiv.2405.08841>). It also provides support for\n    arbitrary delay distributions, a range of common primary distributions, and\n    allows for truncation and secondary event censoring to be accounted for\n    (Park et al. (2024) <doi:10.1101/2024.01.12.24301247>). A subset of\n    common distributions also have analytical solutions implemented, allowing\n    for faster computation. In addition, it provides multiple methods for\n    fitting primary event censored distributions to data via optional\n    dependencies.  "
  },
  {
    "id": 18365,
    "package_name": "printr",
    "title": "Automatically Print R Objects to Appropriate Formats According\nto the 'knitr' Output Format",
    "description": "Extends the S3 generic function knit_print() in 'knitr'\n    to automatically print some objects using an appropriate format such as\n    Markdown or LaTeX. For example, data frames are automatically printed as\n    tables, and the help() pages can also be rendered in 'knitr' documents.",
    "version": "0.3",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>)",
    "url": "https://yihui.org/printr/",
    "bug_reports": "https://github.com/yihui/printr/issues",
    "repository": "https://cran.r-project.org/package=printr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "printr Automatically Print R Objects to Appropriate Formats According\nto the 'knitr' Output Format Extends the S3 generic function knit_print() in 'knitr'\n    to automatically print some objects using an appropriate format such as\n    Markdown or LaTeX. For example, data frames are automatically printed as\n    tables, and the help() pages can also be rendered in 'knitr' documents.  "
  },
  {
    "id": 18370,
    "package_name": "prioriactions",
    "title": "Multi-Action Conservation Planning",
    "description": "This uses a mixed integer mathematical programming (MIP)\n        approach for building and solving multi-action planning problems, \n        where the goal is to find an optimal combination of management actions that\n        abate threats, in an efficient way while accounting for spatial aspects. \n        Thus, optimizing the connectivity and conservation effectiveness of \n        the prioritized units and of the deployed actions. The package is capable of \n        handling different commercial (gurobi, CPLEX) and non-commercial (symphony, CBC) MIP solvers. \n        Gurobi optimization solver can be installed using comprehensive instructions in \n        the 'gurobi' installation vignette of the prioritizr package (available in \n        <https://prioritizr.net/articles/gurobi_installation_guide.html>). Instead, 'CPLEX'\n        optimization solver can be obtain from IBM CPLEX web page (available here \n        <https://www.ibm.com/es-es/products/ilog-cplex-optimization-studio>). Additionally, \n        the 'rcbc' R package (available at\n    <https://github.com/dirkschumacher/rcbc>) can be used to obtain solutions\n    using the CBC optimization software (<https://github.com/coin-or/Cbc>). Methods used in the \n        package refers to Salgado-Rojas et al. (2020) <doi:10.1016/j.ecolmodel.2019.108901>,\n        Beyer et al. (2016) <doi:10.1016/j.ecolmodel.2016.02.005>, Cattarino et al. (2015)\n        <doi:10.1371/journal.pone.0128027> and Watts et al. (2009) <doi:10.1016/j.envsoft.2009.06.005>. \n        See the prioriactions website for more information, documentations and examples.",
    "version": "0.5.0",
    "maintainer": "Jose Salgado-Rojas <jose.salgroj@gmail.com>",
    "author": "Jose Salgado-Rojas [aut, cre],\n  Irlanda Ceballos-Fuentealba [aut],\n  Virgilio Hermoso [aut],\n  Eduardo Alvarez-Miranda [aut],\n  Jordi Garcia-Gonzalo [aut]",
    "url": "https://prioriactions.github.io/prioriactions/,\nhttps://github.com/prioriactions/prioriactions",
    "bug_reports": "https://github.com/prioriactions/prioriactions/issues",
    "repository": "https://cran.r-project.org/package=prioriactions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prioriactions Multi-Action Conservation Planning This uses a mixed integer mathematical programming (MIP)\n        approach for building and solving multi-action planning problems, \n        where the goal is to find an optimal combination of management actions that\n        abate threats, in an efficient way while accounting for spatial aspects. \n        Thus, optimizing the connectivity and conservation effectiveness of \n        the prioritized units and of the deployed actions. The package is capable of \n        handling different commercial (gurobi, CPLEX) and non-commercial (symphony, CBC) MIP solvers. \n        Gurobi optimization solver can be installed using comprehensive instructions in \n        the 'gurobi' installation vignette of the prioritizr package (available in \n        <https://prioritizr.net/articles/gurobi_installation_guide.html>). Instead, 'CPLEX'\n        optimization solver can be obtain from IBM CPLEX web page (available here \n        <https://www.ibm.com/es-es/products/ilog-cplex-optimization-studio>). Additionally, \n        the 'rcbc' R package (available at\n    <https://github.com/dirkschumacher/rcbc>) can be used to obtain solutions\n    using the CBC optimization software (<https://github.com/coin-or/Cbc>). Methods used in the \n        package refers to Salgado-Rojas et al. (2020) <doi:10.1016/j.ecolmodel.2019.108901>,\n        Beyer et al. (2016) <doi:10.1016/j.ecolmodel.2016.02.005>, Cattarino et al. (2015)\n        <doi:10.1371/journal.pone.0128027> and Watts et al. (2009) <doi:10.1016/j.envsoft.2009.06.005>. \n        See the prioriactions website for more information, documentations and examples.  "
  },
  {
    "id": 18381,
    "package_name": "prmisc",
    "title": "Miscellaneous Printing of Numeric and Statistical Output in R\nMarkdown and Quarto Documents",
    "description": "Miscellaneous printing of numeric or statistical results in R Markdown or Quarto documents according to guidelines of the \"Publication Manual\" of the American Psychological Association (2020, ISBN: 978-1-4338-3215-4). These guidelines are usually referred to as APA style (<https://apastyle.apa.org/>) and include specific rules on the formatting of numbers and statistical test results. APA style has to be implemented when submitting scientific reports in a wide range of research fields, especially in the social sciences. The default output of numbers in the R console or R Markdown and Quarto documents does not meet the APA style requirements, and reformatting results manually can be cumbersome and error-prone. This package covers the automatic conversion of R objects to textual representations that meet the APA style requirements, which can be included in R Markdown or Quarto documents. It covers some basic statistical tests (t-test, ANOVA, correlation, chi-squared test, Wilcoxon test) as well as some basic number printing manipulations (formatting p-values, removing leading zeros for numbers that cannot be greater than one, and others). Other packages exist for formatting numbers and tests according to the APA style guidelines, such as 'papaja' (<https://cran.r-project.org/package=papaja>) and 'apa' (<https://cran.r-project.org/package=apa>), but they do not offer all convenience functionality included in 'prmisc'. The vignette has an overview of most of the functions included in the package.",
    "version": "0.0.3",
    "maintainer": "Martin Papenberg <martin.papenberg@hhu.de>",
    "author": "Martin Papenberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9900-4268>),\n  Juliane V. Nagel [aut] (ORCID: <https://orcid.org/0000-0002-5310-8088>)",
    "url": "https://github.com/m-Py/prmisc",
    "bug_reports": "https://github.com/m-Py/prmisc/issues",
    "repository": "https://cran.r-project.org/package=prmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prmisc Miscellaneous Printing of Numeric and Statistical Output in R\nMarkdown and Quarto Documents Miscellaneous printing of numeric or statistical results in R Markdown or Quarto documents according to guidelines of the \"Publication Manual\" of the American Psychological Association (2020, ISBN: 978-1-4338-3215-4). These guidelines are usually referred to as APA style (<https://apastyle.apa.org/>) and include specific rules on the formatting of numbers and statistical test results. APA style has to be implemented when submitting scientific reports in a wide range of research fields, especially in the social sciences. The default output of numbers in the R console or R Markdown and Quarto documents does not meet the APA style requirements, and reformatting results manually can be cumbersome and error-prone. This package covers the automatic conversion of R objects to textual representations that meet the APA style requirements, which can be included in R Markdown or Quarto documents. It covers some basic statistical tests (t-test, ANOVA, correlation, chi-squared test, Wilcoxon test) as well as some basic number printing manipulations (formatting p-values, removing leading zeros for numbers that cannot be greater than one, and others). Other packages exist for formatting numbers and tests according to the APA style guidelines, such as 'papaja' (<https://cran.r-project.org/package=papaja>) and 'apa' (<https://cran.r-project.org/package=apa>), but they do not offer all convenience functionality included in 'prmisc'. The vignette has an overview of most of the functions included in the package.  "
  },
  {
    "id": 18404,
    "package_name": "proffer",
    "title": "Profile R Code and Visualize with 'Pprof'",
    "description": "Like similar profiling tools,\n  the 'proffer' package automatically detects\n  sources of slowness in R code.\n  The distinguishing feature of 'proffer' is its utilization of\n  'pprof', which supplies interactive visualizations\n  that are efficient and easy to interpret.\n  Behind the scenes, the 'profile' package converts\n  native Rprof() data to a protocol buffer\n  that 'pprof' understands.\n  For the documentation of 'proffer',\n  visit <https://r-prof.github.io/proffer/>.\n  To learn about the implementations and methodologies of\n  'pprof', 'profile', and protocol buffers,\n  visit <https://github.com/google/pprof>.\n  <https://protobuf.dev>,\n  and <https://github.com/r-prof/profile>, respectively.",
    "version": "0.2.2",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1878-3253>),\n  Eli Lilly and Company [cph]",
    "url": "https://github.com/r-prof/proffer,\nhttps://r-prof.github.io/proffer/",
    "bug_reports": "https://github.com/r-prof/proffer/issues",
    "repository": "https://cran.r-project.org/package=proffer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "proffer Profile R Code and Visualize with 'Pprof' Like similar profiling tools,\n  the 'proffer' package automatically detects\n  sources of slowness in R code.\n  The distinguishing feature of 'proffer' is its utilization of\n  'pprof', which supplies interactive visualizations\n  that are efficient and easy to interpret.\n  Behind the scenes, the 'profile' package converts\n  native Rprof() data to a protocol buffer\n  that 'pprof' understands.\n  For the documentation of 'proffer',\n  visit <https://r-prof.github.io/proffer/>.\n  To learn about the implementations and methodologies of\n  'pprof', 'profile', and protocol buffers,\n  visit <https://github.com/google/pprof>.\n  <https://protobuf.dev>,\n  and <https://github.com/r-prof/profile>, respectively.  "
  },
  {
    "id": 18414,
    "package_name": "progressr",
    "title": "An Inclusive, Unifying API for Progress Updates",
    "description": "A minimal, unifying API for scripts and packages to report progress updates from anywhere including when using parallel processing.  The package is designed such that the developer can to focus on what progress should be reported on without having to worry about how to present it.  The end user has full control of how, where, and when to render these progress updates, e.g. in the terminal using utils::txtProgressBar(), cli::cli_progress_bar(), in a graphical user interface using utils::winProgressBar(), tcltk::tkProgressBar() or shiny::withProgress(), via the speakers using beepr::beep(), or on a file system via the size of a file. Anyone can add additional, customized, progression handlers. The 'progressr' package uses R's condition framework for signaling progress updated. Because of this, progress can be reported from almost anywhere in R, e.g. from classical for and while loops, from map-reduce API:s like the lapply() family of functions, 'purrr', 'plyr', and 'foreach'. It will also work with parallel processing via the 'future' framework, e.g. future.apply::future_lapply(), furrr::future_map(), and 'foreach' with 'doFuture'. The package is compatible with Shiny applications.",
    "version": "0.18.0",
    "maintainer": "Henrik Bengtsson <henrikb@braju.com>",
    "author": "Henrik Bengtsson [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7579-5165>)",
    "url": "https://progressr.futureverse.org,\nhttps://github.com/futureverse/progressr",
    "bug_reports": "https://github.com/futureverse/progressr/issues",
    "repository": "https://cran.r-project.org/package=progressr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "progressr An Inclusive, Unifying API for Progress Updates A minimal, unifying API for scripts and packages to report progress updates from anywhere including when using parallel processing.  The package is designed such that the developer can to focus on what progress should be reported on without having to worry about how to present it.  The end user has full control of how, where, and when to render these progress updates, e.g. in the terminal using utils::txtProgressBar(), cli::cli_progress_bar(), in a graphical user interface using utils::winProgressBar(), tcltk::tkProgressBar() or shiny::withProgress(), via the speakers using beepr::beep(), or on a file system via the size of a file. Anyone can add additional, customized, progression handlers. The 'progressr' package uses R's condition framework for signaling progress updated. Because of this, progress can be reported from almost anywhere in R, e.g. from classical for and while loops, from map-reduce API:s like the lapply() family of functions, 'purrr', 'plyr', and 'foreach'. It will also work with parallel processing via the 'future' framework, e.g. future.apply::future_lapply(), furrr::future_map(), and 'foreach' with 'doFuture'. The package is compatible with Shiny applications.  "
  },
  {
    "id": 18419,
    "package_name": "projmgr",
    "title": "Task Tracking and Project Management with GitHub",
    "description": "Provides programmatic access to 'GitHub' API with a\n    focus on project management.  Key functionality includes\n    setting up issues and milestones from R objects or 'YAML' configurations,\n    querying outstanding or completed tasks, and generating progress updates\n    in tables, charts, and RMarkdown reports. Useful for those using 'GitHub' in personal,\n    professional, or academic settings with an emphasis on streamlining\n    the workflow of data analysis projects.",
    "version": "0.1.2",
    "maintainer": "Emily Riederer <emilyriederer@gmail.com>",
    "author": "Emily Riederer [cre, aut]",
    "url": "https://github.com/emilyriederer/projmgr,\nhttps://emilyriederer.github.io/projmgr/",
    "bug_reports": "https://github.com/emilyriederer/projmgr/issues",
    "repository": "https://cran.r-project.org/package=projmgr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "projmgr Task Tracking and Project Management with GitHub Provides programmatic access to 'GitHub' API with a\n    focus on project management.  Key functionality includes\n    setting up issues and milestones from R objects or 'YAML' configurations,\n    querying outstanding or completed tasks, and generating progress updates\n    in tables, charts, and RMarkdown reports. Useful for those using 'GitHub' in personal,\n    professional, or academic settings with an emphasis on streamlining\n    the workflow of data analysis projects.  "
  },
  {
    "id": 18423,
    "package_name": "prome",
    "title": "Patient-Reported Outcome Data Analysis with Stan",
    "description": "Algorithms and subroutines for patient-reported outcome data analysis.",
    "version": "3.0.1.5",
    "maintainer": "Bin Wang <bwang831@gmail.com>",
    "author": "Bin Wang [aut, cre] (ORCID: <https://orcid.org/0000-0002-3689-6932>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=prome",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prome Patient-Reported Outcome Data Analysis with Stan Algorithms and subroutines for patient-reported outcome data analysis.  "
  },
  {
    "id": 18443,
    "package_name": "protein8k",
    "title": "Perform Analysis and Create Visualizations of Proteins",
    "description": "Read Protein Data Bank (PDB) files, performs its analysis, and \n    presents the result using different visualization types including 3D. The \n    package also has additional capability for handling Virus Report data from \n    the National Center for Biotechnology Information (NCBI) database.\n    Nature Structural Biology 10, 980 (2003) <doi:10.1038/nsb1203-980>.\n    US National Library of Medicine (2021) <https://www.ncbi.nlm.nih.gov/datasets/docs/reference-docs/data-reports/virus/>.",
    "version": "0.0.1",
    "maintainer": "Simon Liles <simon@quantknot.com>",
    "author": "Simon Liles",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=protein8k",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "protein8k Perform Analysis and Create Visualizations of Proteins Read Protein Data Bank (PDB) files, performs its analysis, and \n    presents the result using different visualization types including 3D. The \n    package also has additional capability for handling Virus Report data from \n    the National Center for Biotechnology Information (NCBI) database.\n    Nature Structural Biology 10, 980 (2003) <doi:10.1038/nsb1203-980>.\n    US National Library of Medicine (2021) <https://www.ncbi.nlm.nih.gov/datasets/docs/reference-docs/data-reports/virus/>.  "
  },
  {
    "id": 18472,
    "package_name": "psHarmonize",
    "title": "Creates a Harmonized Dataset Based on a Set of Instructions",
    "description": "Functions which facilitate harmonization of data from multiple\n    different datasets. Data harmonization involves taking data sources with\n    differing values, creating coding instructions to create a harmonized\n    set of values, then making those data modifications. 'psHarmonize' will\n    assist with data modification once the harmonization instructions are\n    written. Coding instructions are written by the user to create a\n    \"harmonization sheet\". This sheet catalogs variable names, domains\n    (e.g. clinical, behavioral, outcomes), provides R code instructions for\n    mapping or conversion of data, specifies the variable name in the\n    harmonized data set, and tracks notes. The package will then harmonize\n    the source datasets according to the harmonization sheet to create a\n    harmonized dataset. Once harmonization is finished, the package also has\n    functions that will create descriptive statistics using 'RMarkdown'. Data\n    Harmonization guidelines have been described by Fortier I, Raina P,\n    Van den Heuvel ER, et al. (2017) <doi:10.1093/ije/dyw075>. Additional\n    details of our R package have been described by Stephen JJ, Carolan P,\n    Krefman AE, et al. (2024) <doi:10.1016/j.patter.2024.101003>.",
    "version": "0.3.6",
    "maintainer": "John Stephen <John.Stephen@northwestern.edu>",
    "author": "John Stephen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7309-9193>),\n  Maxwell Mansolf [ctb] (ORCID: <https://orcid.org/0000-0001-6861-8657>)",
    "url": "https://github.com/NUDACC/psHarmonize",
    "bug_reports": "https://github.com/NUDACC/psHarmonize/issues",
    "repository": "https://cran.r-project.org/package=psHarmonize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psHarmonize Creates a Harmonized Dataset Based on a Set of Instructions Functions which facilitate harmonization of data from multiple\n    different datasets. Data harmonization involves taking data sources with\n    differing values, creating coding instructions to create a harmonized\n    set of values, then making those data modifications. 'psHarmonize' will\n    assist with data modification once the harmonization instructions are\n    written. Coding instructions are written by the user to create a\n    \"harmonization sheet\". This sheet catalogs variable names, domains\n    (e.g. clinical, behavioral, outcomes), provides R code instructions for\n    mapping or conversion of data, specifies the variable name in the\n    harmonized data set, and tracks notes. The package will then harmonize\n    the source datasets according to the harmonization sheet to create a\n    harmonized dataset. Once harmonization is finished, the package also has\n    functions that will create descriptive statistics using 'RMarkdown'. Data\n    Harmonization guidelines have been described by Fortier I, Raina P,\n    Van den Heuvel ER, et al. (2017) <doi:10.1093/ije/dyw075>. Additional\n    details of our R package have been described by Stephen JJ, Carolan P,\n    Krefman AE, et al. (2024) <doi:10.1016/j.patter.2024.101003>.  "
  },
  {
    "id": 18512,
    "package_name": "psychReport",
    "title": "Reproducible Reports in Psychology",
    "description": "Helper functions for producing reports in Psychology (Reproducible Research). Provides required formatted strings (APA style) for use in 'Knitr'/'Latex' integration within *.Rnw files.",
    "version": "3.0.2",
    "maintainer": "Ian G Mackenzie <ian.mackenzie@uni-tuebingen.de>",
    "author": "Ian G Mackenzie [cre, aut],\n  Carolin Dudschig [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychReport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychReport Reproducible Reports in Psychology Helper functions for producing reports in Psychology (Reproducible Research). Provides required formatted strings (APA style) for use in 'Knitr'/'Latex' integration within *.Rnw files.  "
  },
  {
    "id": 18514,
    "package_name": "psychmeta",
    "title": "Psychometric Meta-Analysis Toolkit",
    "description": "Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.",
    "version": "2.7.0",
    "maintainer": "Jeffrey A. Dahlke <jeff.dahlke.phd@gmail.com>",
    "author": "Jeffrey A. Dahlke [aut, cre],\n  Brenton M. Wiernik [aut],\n  Wesley Gardiner [ctb] (Unit tests),\n  Michael T. Brannick [ctb] (Testing),\n  Jack Kostal [ctb] (Code for reshape_mat2dat function),\n  Sean Potter [ctb] (Testing; Code for cumulative and leave1out plots),\n  John Sakaluk [ctb] (Code for funnel and forest plots),\n  Yuejia (Mandy) Teng [ctb] (Testing)",
    "url": "",
    "bug_reports": "https://github.com/psychmeta/psychmeta/issues",
    "repository": "https://cran.r-project.org/package=psychmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychmeta Psychometric Meta-Analysis Toolkit Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.  "
  },
  {
    "id": 18515,
    "package_name": "psycho",
    "title": "Efficient and Publishing-Oriented Workflow for Psychological\nScience",
    "description": "The main goal of the psycho package is to provide tools for psychologists, neuropsychologists and neuroscientists, \n   to facilitate and speed up the time spent on data analysis. It aims at supporting best practices and tools to format the output \n   of statistical methods to directly paste them into a manuscript, ensuring statistical reporting standardization and conformity.",
    "version": "0.6.1",
    "maintainer": "Dominique Makowski <dom.makowski@gmail.com>",
    "author": "Dominique Makowski [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5375-9967>),\n  Hugo Najberg [ctb],\n  Viliam Simko [ctb],\n  Sasha Epskamp [rev] (Sasha reviewed the package for JOSS, see\n    https://github.com/openjournals/joss-reviews/issues/470)",
    "url": "https://github.com/neuropsychology/psycho.R",
    "bug_reports": "https://github.com/neuropsychology/psycho.R/issues",
    "repository": "https://cran.r-project.org/package=psycho",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psycho Efficient and Publishing-Oriented Workflow for Psychological\nScience The main goal of the psycho package is to provide tools for psychologists, neuropsychologists and neuroscientists, \n   to facilitate and speed up the time spent on data analysis. It aims at supporting best practices and tools to format the output \n   of statistical methods to directly paste them into a manuscript, ensuring statistical reporting standardization and conformity.  "
  },
  {
    "id": 18527,
    "package_name": "ptetools",
    "title": "Panel Treatment Effects Tools",
    "description": "Generic code for estimating treatment effects with panel data.  The idea is to break into separate steps organizing the data, looping over groups and time periods, computing group-time average treatment effects, and aggregating group-time average treatment effects.  Often, one is able to implement a new identification/estimation procedure by simply replacing the step on estimating group-time average treatment effects.  See several different examples of this approach in the package documentation.",
    "version": "1.0.0",
    "maintainer": "Brantly Callaway <brantly.callaway@uga.edu>",
    "author": "Brantly Callaway [aut, cre]",
    "url": "https://github.com/bcallaway11/ptetools",
    "bug_reports": "https://github.com/bcallaway11/ptetools/issues",
    "repository": "https://cran.r-project.org/package=ptetools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ptetools Panel Treatment Effects Tools Generic code for estimating treatment effects with panel data.  The idea is to break into separate steps organizing the data, looping over groups and time periods, computing group-time average treatment effects, and aggregating group-time average treatment effects.  Often, one is able to implement a new identification/estimation procedure by simply replacing the step on estimating group-time average treatment effects.  See several different examples of this approach in the package documentation.  "
  },
  {
    "id": 18535,
    "package_name": "ptvapi",
    "title": "Access the 'Public Transport Victoria' Timetable API",
    "description": "Access the 'Public Transport Victoria' Timetable API \n    <https://www.ptv.vic.gov.au/footer/data-and-reporting/datasets/ptv-timetable-api/>,\n    with results returned as familiar R data structures. Retrieve information on\n    stops, routes, disruptions, departures, and more.",
    "version": "2.0.5",
    "maintainer": "David Neuzerling <david@neuzerling.com>",
    "author": "David Neuzerling [aut, cre, cph]",
    "url": "https://github.com/mdneuzerling/ptvapi",
    "bug_reports": "https://github.com/mdneuzerling/ptvapi/issues",
    "repository": "https://cran.r-project.org/package=ptvapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ptvapi Access the 'Public Transport Victoria' Timetable API Access the 'Public Transport Victoria' Timetable API \n    <https://www.ptv.vic.gov.au/footer/data-and-reporting/datasets/ptv-timetable-api/>,\n    with results returned as familiar R data structures. Retrieve information on\n    stops, routes, disruptions, departures, and more.  "
  },
  {
    "id": 18539,
    "package_name": "pubh",
    "title": "A Toolbox for Public Health and Epidemiology",
    "description": "A toolbox for making R functions and capabilities more\n    accessible to students and professionals from Epidemiology and\n    Public Health related disciplines. Includes a function to report \n    coefficients and confidence intervals from models using robust\n    standard errors (when available), functions that expand 'ggplot2'\n    plots and functions relevant for introductory papers in Epidemiology \n    or Public Health. Please note that use of the \n    provided data sets is for educational purposes only.",
    "version": "3.0.0",
    "maintainer": "Josie Athens <josie.athens@gmail.com>",
    "author": "Josie Athens [aut, cre],\n  Frank Harell [ctb],\n  John Fox [ctb],\n  R-Core [ctb]",
    "url": "",
    "bug_reports": "https://github.com/josie-athens/pubh/issues",
    "repository": "https://cran.r-project.org/package=pubh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pubh A Toolbox for Public Health and Epidemiology A toolbox for making R functions and capabilities more\n    accessible to students and professionals from Epidemiology and\n    Public Health related disciplines. Includes a function to report \n    coefficients and confidence intervals from models using robust\n    standard errors (when available), functions that expand 'ggplot2'\n    plots and functions relevant for introductory papers in Epidemiology \n    or Public Health. Please note that use of the \n    provided data sets is for educational purposes only.  "
  },
  {
    "id": 18553,
    "package_name": "puniform",
    "title": "Meta-Analysis Methods Correcting for Publication Bias",
    "description": "Provides meta-analysis methods that correct for\n    publication bias and outcome reporting bias. Four methods and a visual tool \n    are currently included in the package. The p-uniform method as described in \n    van Assen, van Aert, and Wicherts (2015) <doi:10.1037/met0000025> \n    can be used for estimating the average effect size, testing the null hypothesis \n    of no effect, and testing for publication bias using only the statistically \n    significant effect sizes of primary studies. The second method in the package \n    is the p-uniform* method as described in van Aert and van Assen (2023) \n    <doi:10.31222/osf.io/zqjr9>. This method is an extension of the p-uniform \n    method that allows for estimation of the average effect size and the \n    between-study variance in a meta-analysis, and uses both the statistically \n    significant and nonsignificant effect sizes. The third method in the package \n    is the hybrid method as described in van Aert and van Assen (2018) \n    <doi:10.3758/s13428-017-0967-6>. The hybrid method is a meta-analysis method \n    for combining a conventional study and replication/preregistered study while \n    taking into account statistical significance of the conventional study. This\n    method was extended in van Aert (2025) <doi:10.1037/met0000719> \n    such that it allows for the inclusion of multiple conventional and \n    replication/preregistered studies. The p-uniform and hybrid method are based \n    on the statistical theory that the distribution of p-values is uniform \n    conditional on the population effect size. The fourth method in the package \n    is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert \n    and van Assen (2018) <doi:10.1371/journal.pone.0175302>. This method computes \n    posterior probabilities for four true effect sizes (no, small, medium, and \n    large) based on an original study and replication while taking into account \n    publication bias in the original study. The method can also be used for \n    computing the required sample size of the replication akin to power analysis \n    in null-hypothesis significance testing. The meta-plot is a visual tool for \n    meta-analysis that provides information on the primary studies in the \n    meta-analysis, the results of the meta-analysis, and characteristics of the \n    research on the effect under study (van Assen et al., 2023). Helper functions \n    to apply the Correcting for Outcome Reporting Bias (CORB) method to correct \n    for outcome reporting bias in a meta-analysis (van Aert & Wicherts, 2023).",
    "version": "0.2.8",
    "maintainer": "Robbie C.M. van Aert <R.C.M.vanAert@tilburguniversity.edu>",
    "author": "Robbie C.M. van Aert [aut, cre]",
    "url": "https://github.com/RobbievanAert/puniform",
    "bug_reports": "https://github.com/RobbievanAert/puniform/issues",
    "repository": "https://cran.r-project.org/package=puniform",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "puniform Meta-Analysis Methods Correcting for Publication Bias Provides meta-analysis methods that correct for\n    publication bias and outcome reporting bias. Four methods and a visual tool \n    are currently included in the package. The p-uniform method as described in \n    van Assen, van Aert, and Wicherts (2015) <doi:10.1037/met0000025> \n    can be used for estimating the average effect size, testing the null hypothesis \n    of no effect, and testing for publication bias using only the statistically \n    significant effect sizes of primary studies. The second method in the package \n    is the p-uniform* method as described in van Aert and van Assen (2023) \n    <doi:10.31222/osf.io/zqjr9>. This method is an extension of the p-uniform \n    method that allows for estimation of the average effect size and the \n    between-study variance in a meta-analysis, and uses both the statistically \n    significant and nonsignificant effect sizes. The third method in the package \n    is the hybrid method as described in van Aert and van Assen (2018) \n    <doi:10.3758/s13428-017-0967-6>. The hybrid method is a meta-analysis method \n    for combining a conventional study and replication/preregistered study while \n    taking into account statistical significance of the conventional study. This\n    method was extended in van Aert (2025) <doi:10.1037/met0000719> \n    such that it allows for the inclusion of multiple conventional and \n    replication/preregistered studies. The p-uniform and hybrid method are based \n    on the statistical theory that the distribution of p-values is uniform \n    conditional on the population effect size. The fourth method in the package \n    is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert \n    and van Assen (2018) <doi:10.1371/journal.pone.0175302>. This method computes \n    posterior probabilities for four true effect sizes (no, small, medium, and \n    large) based on an original study and replication while taking into account \n    publication bias in the original study. The method can also be used for \n    computing the required sample size of the replication akin to power analysis \n    in null-hypothesis significance testing. The meta-plot is a visual tool for \n    meta-analysis that provides information on the primary studies in the \n    meta-analysis, the results of the meta-analysis, and characteristics of the \n    research on the effect under study (van Assen et al., 2023). Helper functions \n    to apply the Correcting for Outcome Reporting Bias (CORB) method to correct \n    for outcome reporting bias in a meta-analysis (van Aert & Wicherts, 2023).  "
  },
  {
    "id": 18571,
    "package_name": "pvda",
    "title": "Disproportionality Functions for Pharmacovigilance",
    "description": "Tools for performing disproportionality analysis using the information component, proportional reporting rate and the reporting odds ratio. The anticipated use is passing data to the da() function, which executes the disproportionality analysis. See Nor\u00e9n et al (2011) <doi:10.1177/0962280211403604> and Montastruc et al (2011) <doi:10.1111/j.1365-2125.2011.04037.x> for further details.",
    "version": "0.0.4",
    "maintainer": "Michele Fusaroli <michele.fusaroli@who-umc.org>",
    "author": "Oskar Gauffin [aut] (ORCID: <https://orcid.org/0000-0003-1593-356X>),\n  Michele Fusaroli [cre] (ORCID: <https://orcid.org/0000-0002-0254-2212>)",
    "url": "https://oskargauffin.github.io/pvda/",
    "bug_reports": "https://github.com/OskarGauffin/pvda/issues",
    "repository": "https://cran.r-project.org/package=pvda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pvda Disproportionality Functions for Pharmacovigilance Tools for performing disproportionality analysis using the information component, proportional reporting rate and the reporting odds ratio. The anticipated use is passing data to the da() function, which executes the disproportionality analysis. See Nor\u00e9n et al (2011) <doi:10.1177/0962280211403604> and Montastruc et al (2011) <doi:10.1111/j.1365-2125.2011.04037.x> for further details.  "
  },
  {
    "id": 18594,
    "package_name": "pylintR",
    "title": "Lint 'Python' Files with a R Command or a 'RStudio' Addin",
    "description": "Allow to run 'pylint' on Python files with a R command or a 'RStudio' addin. The report appears in the RStudio viewer pane as a formatted HTML file.",
    "version": "0.1.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent",
    "url": "https://github.com/stla/pylintR",
    "bug_reports": "https://github.com/stla/pylintR/issues",
    "repository": "https://cran.r-project.org/package=pylintR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pylintR Lint 'Python' Files with a R Command or a 'RStudio' Addin Allow to run 'pylint' on Python files with a R command or a 'RStudio' addin. The report appears in the RStudio viewer pane as a formatted HTML file.  "
  },
  {
    "id": 18606,
    "package_name": "qad",
    "title": "Quantification of Asymmetric Dependence",
    "description": "A copula-based measure for quantifying asymmetry in dependence and associations. Documentation and theory about 'qad' is provided\n    by the paper by Junker, Griessenberger & Trutschnig (2021, <doi:10.1016/j.csda.2020.107058>), and the paper by Trutschnig (2011, <doi:10.1016/j.jmaa.2011.06.013>).",
    "version": "1.0.6",
    "maintainer": "Lea Maislinger <lea.maislinger@plus.ac.at>",
    "author": "Thimo Kasper [aut],\n  Florian Griessenberger [aut],\n  Robert R. Junker [aut],\n  Valentin Petzel [aut],\n  Wolfgang Trutschnig [aut],\n  Lea Maislinger [cre]",
    "url": "https://github.com/griefl/qad",
    "bug_reports": "https://github.com/griefl/qad/issues",
    "repository": "https://cran.r-project.org/package=qad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qad Quantification of Asymmetric Dependence A copula-based measure for quantifying asymmetry in dependence and associations. Documentation and theory about 'qad' is provided\n    by the paper by Junker, Griessenberger & Trutschnig (2021, <doi:10.1016/j.csda.2020.107058>), and the paper by Trutschnig (2011, <doi:10.1016/j.jmaa.2011.06.013>).  "
  },
  {
    "id": 18652,
    "package_name": "qmethod",
    "title": "Analysis of Subjective Perspectives Using Q Methodology",
    "description": "Analysis of Q methodology, used to identify distinct perspectives existing within a group.\n  This methodology is used across social, health and environmental sciences to understand diversity of attitudes, discourses, or decision-making styles (for more information, see <https://qmethod.org/>).\n  A single function runs the full analysis. Each step can be run separately using the corresponding functions: for automatic flagging of Q-sorts (manual flagging is optional), for statement scores, for distinguishing and consensus statements, and for general characteristics of the factors.\n  The package allows to choose either principal components or centroid factor extraction, manual or automatic flagging, a number of mathematical methods for rotation (or none), and a number of correlation coefficients for the initial correlation matrix, among many other options.\n  Additional functions are available to import and export data (from raw *.CSV, 'HTMLQ' and 'FlashQ' *.CSV, 'PQMethod' *.DAT and 'easy-htmlq' *.JSON files), to print and plot, to import raw data from individual *.CSV files, and to make printable cards.\n  The package also offers functions to print Q cards and to generate Q distributions for study administration.\n  See further details in the package documentation, and in the web pages below, which include a cookbook, guidelines for more advanced analysis (how to perform manual flagging or change the sign of factors), data management, and a graphical user interface (GUI) for online and offline use.",
    "version": "1.8.4",
    "maintainer": "Aiora Zabala <aiora.zabala@gmail.com>",
    "author": "Aiora Zabala [aut, cre] (Main author, ORCID:\n    <https://orcid.org/0000-0001-8534-3325>),\n  Maximilian Held [aut] (Author of additional data management functions),\n  Frans Hermans [aut] (Author of centroid extraction function)",
    "url": "https://github.com/aiorazabala/qmethod,\nhttp://aiorazabala.github.io/qmethod/",
    "bug_reports": "https://github.com/aiorazabala/qmethod/issues",
    "repository": "https://cran.r-project.org/package=qmethod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qmethod Analysis of Subjective Perspectives Using Q Methodology Analysis of Q methodology, used to identify distinct perspectives existing within a group.\n  This methodology is used across social, health and environmental sciences to understand diversity of attitudes, discourses, or decision-making styles (for more information, see <https://qmethod.org/>).\n  A single function runs the full analysis. Each step can be run separately using the corresponding functions: for automatic flagging of Q-sorts (manual flagging is optional), for statement scores, for distinguishing and consensus statements, and for general characteristics of the factors.\n  The package allows to choose either principal components or centroid factor extraction, manual or automatic flagging, a number of mathematical methods for rotation (or none), and a number of correlation coefficients for the initial correlation matrix, among many other options.\n  Additional functions are available to import and export data (from raw *.CSV, 'HTMLQ' and 'FlashQ' *.CSV, 'PQMethod' *.DAT and 'easy-htmlq' *.JSON files), to print and plot, to import raw data from individual *.CSV files, and to make printable cards.\n  The package also offers functions to print Q cards and to generate Q distributions for study administration.\n  See further details in the package documentation, and in the web pages below, which include a cookbook, guidelines for more advanced analysis (how to perform manual flagging or change the sign of factors), data management, and a graphical user interface (GUI) for online and offline use.  "
  },
  {
    "id": 18673,
    "package_name": "qreport",
    "title": "Statistical Reporting with 'Quarto'",
    "description": "Provides statistical components, tables, and graphs\n    that are useful in 'Quarto' and 'RMarkdown' reports and that produce 'Quarto'\n    elements for special formatting such as tabs and marginal notes and graphs.\n    Some of the functions produce entire report sections with tabs, e.g.,\n    the missing data report created by missChk().  Functions for inserting\n\t\tvariables and tables inside 'graphviz' and 'mermaid' diagrams are included,\n\t\tand so are special clinical trial graphics for adverse event reporting.",
    "version": "1.1-0",
    "maintainer": "Frank E Harrell Jr <fh@fharrell.com>",
    "author": "Frank E Harrell Jr [aut, cre]",
    "url": "https://hbiostat.org/R/qreport/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qreport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qreport Statistical Reporting with 'Quarto' Provides statistical components, tables, and graphs\n    that are useful in 'Quarto' and 'RMarkdown' reports and that produce 'Quarto'\n    elements for special formatting such as tabs and marginal notes and graphs.\n    Some of the functions produce entire report sections with tabs, e.g.,\n    the missing data report created by missChk().  Functions for inserting\n\t\tvariables and tables inside 'graphviz' and 'mermaid' diagrams are included,\n\t\tand so are special clinical trial graphics for adverse event reporting.  "
  },
  {
    "id": 18692,
    "package_name": "qtkit",
    "title": "Quantitative Text Kit",
    "description": "Support package for the textbook \"An Introduction to\n    Quantitative Text Analysis for Linguists: Reproducible Research Using\n    R\" (Francom, 2024) <doi:10.4324/9781003393764>. Includes functions to\n    acquire, clean, and analyze text data as well as functions to document\n    and share the results of text analysis. The package is designed to be\n    used in conjunction with the book, but can also be used as a standalone\n    package for text analysis.",
    "version": "1.1.1",
    "maintainer": "Jerid Francom <francojc@wfu.edu>",
    "author": "Jerid Francom [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5972-6330>)",
    "url": "https://cran.r-project.org/package=qtkit",
    "bug_reports": "https://github.com/qtalr/qtkit/issues",
    "repository": "https://cran.r-project.org/package=qtkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qtkit Quantitative Text Kit Support package for the textbook \"An Introduction to\n    Quantitative Text Analysis for Linguists: Reproducible Research Using\n    R\" (Francom, 2024) <doi:10.4324/9781003393764>. Includes functions to\n    acquire, clean, and analyze text data as well as functions to document\n    and share the results of text analysis. The package is designed to be\n    used in conjunction with the book, but can also be used as a standalone\n    package for text analysis.  "
  },
  {
    "id": 18706,
    "package_name": "qtwAcademic",
    "title": "'Quarto' Website Templates for Academics",
    "description": "Provides three 'Quarto' website templates as an R project, which are commonly used by academics.\n    Templates for personal websites and course/workshop websites are included, as well as a template with minimal content for customization.",
    "version": "2022.12.13",
    "maintainer": "Chi Zhang <andreachizhang@yahoo.com>",
    "author": "Chi Zhang [aut, cre] (ORCID: <https://orcid.org/0000-0003-0501-5909>)",
    "url": "https://andreaczhang.github.io/qtwAcademic/,\nhttps://github.com/andreaczhang/qtwAcademic",
    "bug_reports": "https://github.com/andreaczhang/qtwAcademic/issues",
    "repository": "https://cran.r-project.org/package=qtwAcademic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qtwAcademic 'Quarto' Website Templates for Academics Provides three 'Quarto' website templates as an R project, which are commonly used by academics.\n    Templates for personal websites and course/workshop websites are included, as well as a template with minimal content for customization.  "
  },
  {
    "id": 18713,
    "package_name": "quadprog",
    "title": "Functions to Solve Quadratic Programming Problems",
    "description": "This package contains routines and documentation for\n        solving quadratic programming problems.",
    "version": "1.5-8",
    "maintainer": "Berwin A. Turlach <Berwin.Turlach@gmail.com>",
    "author": "S original by Berwin A. Turlach <Berwin.Turlach@gmail.com> \n        R port by Andreas Weingessel <Andreas.Weingessel@ci.tuwien.ac.at>\n        Fortran contributions from Cleve Moler (dposl/LINPACK and\n        (a modified version of) dpodi/LINPACK) ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quadprog",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quadprog Functions to Solve Quadratic Programming Problems This package contains routines and documentation for\n        solving quadratic programming problems.  "
  },
  {
    "id": 18725,
    "package_name": "quanteda",
    "title": "Quantitative Analysis of Textual Data",
    "description": "A fast, flexible, and comprehensive framework for \n    quantitative text analysis in R.  Provides functionality for corpus management,\n    creating and manipulating tokens and n-grams, exploring keywords in context, \n    forming and manipulating sparse matrices\n    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and\n    distances, applying content dictionaries, applying supervised and unsupervised machine learning, \n    visually representing text and text analyses, and more. ",
    "version": "4.3.1",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Kohei Watanabe [aut] (ORCID: <https://orcid.org/0000-0001-6519-5265>),\n  Haiyan Wang [aut] (ORCID: <https://orcid.org/0000-0003-4992-4311>),\n  Paul Nulty [aut] (ORCID: <https://orcid.org/0000-0002-7214-4666>),\n  Adam Obeng [aut] (ORCID: <https://orcid.org/0000-0002-2906-4775>),\n  Stefan M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-6315-4125>),\n  Akitaka Matsuo [aut] (ORCID: <https://orcid.org/0000-0002-3323-6330>),\n  William Lowe [aut] (ORCID: <https://orcid.org/0000-0002-1549-6163>),\n  Christian M\u00fcller [ctb],\n  Olivier Delmarcelle [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4347-070X>),\n  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)",
    "url": "https://quanteda.io",
    "bug_reports": "https://github.com/quanteda/quanteda/issues",
    "repository": "https://cran.r-project.org/package=quanteda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda Quantitative Analysis of Textual Data A fast, flexible, and comprehensive framework for \n    quantitative text analysis in R.  Provides functionality for corpus management,\n    creating and manipulating tokens and n-grams, exploring keywords in context, \n    forming and manipulating sparse matrices\n    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and\n    distances, applying content dictionaries, applying supervised and unsupervised machine learning, \n    visually representing text and text analyses, and more.   "
  },
  {
    "id": 18726,
    "package_name": "quanteda.textmodels",
    "title": "Scaling Models and Classifiers for Textual Data",
    "description": "Scaling models and classifiers for sparse matrix objects representing \n    textual data in the form of a document-feature matrix.  Includes original \n    implementations of 'Laver', 'Benoit', and Garry's (2003) <doi:10.1017/S0003055403000698>,\n    'Wordscores' model, the Perry and 'Benoit' (2017) <doi:10.48550/arXiv.1710.08963> class affinity scaling model, \n    and the 'Slapin' and 'Proksch' (2008) <doi:10.1111/j.1540-5907.2008.00338.x> 'wordfish'\n    model, as well as methods for correspondence analysis, latent semantic analysis,\n    and fast Naive Bayes and linear 'SVMs' specially designed for sparse textual data.",
    "version": "0.9.10",
    "maintainer": "Kenneth Benoit <kbenoit@quanteda.org>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Kohei Watanabe [aut] (ORCID: <https://orcid.org/0000-0001-6519-5265>),\n  Haiyan Wang [aut] (ORCID: <https://orcid.org/0000-0003-4992-4311>),\n  Patrick O. Perry [aut] (ORCID: <https://orcid.org/0000-0001-7460-127X>),\n  Benjamin Lauderdale [aut] (ORCID:\n    <https://orcid.org/0000-0003-3090-0969>),\n  Johannes Gruber [aut] (ORCID: <https://orcid.org/0000-0001-9177-1772>),\n  William Lowe [aut] (ORCID: <https://orcid.org/0000-0002-1549-6163>),\n  Vikas Sindhwani [cph] (authored svmlin C++ source code),\n  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)",
    "url": "https://github.com/quanteda/quanteda.textmodels",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quanteda.textmodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda.textmodels Scaling Models and Classifiers for Textual Data Scaling models and classifiers for sparse matrix objects representing \n    textual data in the form of a document-feature matrix.  Includes original \n    implementations of 'Laver', 'Benoit', and Garry's (2003) <doi:10.1017/S0003055403000698>,\n    'Wordscores' model, the Perry and 'Benoit' (2017) <doi:10.48550/arXiv.1710.08963> class affinity scaling model, \n    and the 'Slapin' and 'Proksch' (2008) <doi:10.1111/j.1540-5907.2008.00338.x> 'wordfish'\n    model, as well as methods for correspondence analysis, latent semantic analysis,\n    and fast Naive Bayes and linear 'SVMs' specially designed for sparse textual data.  "
  },
  {
    "id": 18728,
    "package_name": "quanteda.textstats",
    "title": "Textual Statistics for the Quantitative Analysis of Textual Data",
    "description": "Textual statistics functions formerly in the 'quanteda' package.\n    Textual statistics for characterizing and comparing textual data. Includes \n    functions for measuring term and document frequency, the co-occurrence of \n    words, similarity and distance between features and documents, feature entropy, \n    keyword occurrence, readability, and lexical diversity.  These functions \n    extend the 'quanteda' package and are specially designed for sparse textual data.",
    "version": "0.97.2",
    "maintainer": "Kenneth Benoit <kbenoit@lse.ac.uk>",
    "author": "Kenneth Benoit [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0797-564X>),\n  Kohei Watanabe [aut] (ORCID: <https://orcid.org/0000-0001-6519-5265>),\n  Haiyan Wang [aut] (ORCID: <https://orcid.org/0000-0003-4992-4311>),\n  Jiong Wei Lua [aut],\n  Jouni Kuha [aut] (ORCID: <https://orcid.org/0000-0002-1156-8465>),\n  European Research Council [fnd] (ERC-2011-StG 283794-QUANTESS)",
    "url": "https://quanteda.io",
    "bug_reports": "https://github.com/quanteda/quanteda.textstats/issues",
    "repository": "https://cran.r-project.org/package=quanteda.textstats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quanteda.textstats Textual Statistics for the Quantitative Analysis of Textual Data Textual statistics functions formerly in the 'quanteda' package.\n    Textual statistics for characterizing and comparing textual data. Includes \n    functions for measuring term and document frequency, the co-occurrence of \n    words, similarity and distance between features and documents, feature entropy, \n    keyword occurrence, readability, and lexical diversity.  These functions \n    extend the 'quanteda' package and are specially designed for sparse textual data.  "
  },
  {
    "id": 18732,
    "package_name": "quantities",
    "title": "Quantity Calculus for R Vectors",
    "description": "Integration of the 'units' and 'errors' packages for a complete\n    quantity calculus system for R vectors, matrices and arrays, with automatic\n    propagation, conversion, derivation and simplification of magnitudes and\n    uncertainties. Documentation about 'units' and 'errors' is provided in the\n    papers by Pebesma, Mailund & Hiebert (2016, <doi:10.32614/RJ-2016-061>) and\n    by Ucar, Pebesma & Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in\n    those packages as vignettes; see 'citation(\"quantities\")' for details.",
    "version": "0.2.3",
    "maintainer": "I\u00f1aki Ucar <iucar@fedoraproject.org>",
    "author": "I\u00f1aki Ucar [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-6403-5550>)",
    "url": "https://r-quantities.github.io/quantities/,\nhttps://github.com/r-quantities/quantities",
    "bug_reports": "https://github.com/r-quantities/quantities/issues",
    "repository": "https://cran.r-project.org/package=quantities",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quantities Quantity Calculus for R Vectors Integration of the 'units' and 'errors' packages for a complete\n    quantity calculus system for R vectors, matrices and arrays, with automatic\n    propagation, conversion, derivation and simplification of magnitudes and\n    uncertainties. Documentation about 'units' and 'errors' is provided in the\n    papers by Pebesma, Mailund & Hiebert (2016, <doi:10.32614/RJ-2016-061>) and\n    by Ucar, Pebesma & Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in\n    those packages as vignettes; see 'citation(\"quantities\")' for details.  "
  },
  {
    "id": 18745,
    "package_name": "quartabs",
    "title": "Dynamically Generate Tabset Panels in 'Quarto' HTML Documents",
    "description": "Dynamically generate tabset panels\n    <https://quarto.org/docs/output-formats/html-basics.html#tabsets> in\n    'Quarto' HTML documents using a data frame as input.",
    "version": "0.1.1",
    "maintainer": "Yusuke Sasaki <sayuks.dev@gmail.com>",
    "author": "Yusuke Sasaki [aut, cre]",
    "url": "https://sayuks.github.io/quartabs/,\nhttps://github.com/sayuks/quartabs",
    "bug_reports": "https://github.com/sayuks/quartabs/issues",
    "repository": "https://cran.r-project.org/package=quartabs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quartabs Dynamically Generate Tabset Panels in 'Quarto' HTML Documents Dynamically generate tabset panels\n    <https://quarto.org/docs/output-formats/html-basics.html#tabsets> in\n    'Quarto' HTML documents using a data frame as input.  "
  },
  {
    "id": 18747,
    "package_name": "quarto",
    "title": "R Interface to 'Quarto' Markdown Publishing System",
    "description": "Convert R Markdown documents and 'Jupyter' notebooks to a\n    variety of output formats using 'Quarto'.",
    "version": "1.5.1",
    "maintainer": "Christophe Dervieux <cderv@posit.co>",
    "author": "JJ Allaire [aut] (ORCID: <https://orcid.org/0000-0003-0174-9868>),\n  Christophe Dervieux [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-4474-2498>),\n  Posit Software, PBC [cph, fnd],\n  Gordon Woodhull [ctb]",
    "url": "https://github.com/quarto-dev/quarto-r,\nhttps://quarto-dev.github.io/quarto-r/",
    "bug_reports": "https://github.com/quarto-dev/quarto-r/issues",
    "repository": "https://cran.r-project.org/package=quarto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quarto R Interface to 'Quarto' Markdown Publishing System Convert R Markdown documents and 'Jupyter' notebooks to a\n    variety of output formats using 'Quarto'.  "
  },
  {
    "id": 18748,
    "package_name": "quartose",
    "title": "Dynamically Generate Quarto Syntax",
    "description": "\n    Provides helper functions to work programmatically within a quarto document. It allows \n    the user to create section headers, tabsets, divs, and spans, and formats these objects\n    into quarto syntax when printed into a document. ",
    "version": "0.1.0",
    "maintainer": "Danielle Navarro <djnavarro@protonmail.com>",
    "author": "Danielle Navarro [aut, cre, cph]",
    "url": "https://github.com/djnavarro/quartose,\nhttps://quartose.djnavarro.net/",
    "bug_reports": "https://github.com/djnavarro/quartose/issues",
    "repository": "https://cran.r-project.org/package=quartose",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quartose Dynamically Generate Quarto Syntax \n    Provides helper functions to work programmatically within a quarto document. It allows \n    the user to create section headers, tabsets, divs, and spans, and formats these objects\n    into quarto syntax when printed into a document.   "
  },
  {
    "id": 18785,
    "package_name": "r2dictionary",
    "title": "A Mini-Dictionary for 'R', 'shiny' and 'rmarkdown' Documents",
    "description": "Despite the predominant use of R for data manipulation and various robust statistical calculations, in recent years, more people from various disciplines are beginning to use R for other purposes. In doing this seemlessly, further tools are needed users to easily and freely write in R for all kinds of purposes. The r2dictionary introduces a means for users to directly search for definitions of terms within the R environment.",
    "version": "0.3",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://r2dictionary.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/r2dictionary/issues",
    "repository": "https://cran.r-project.org/package=r2dictionary",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2dictionary A Mini-Dictionary for 'R', 'shiny' and 'rmarkdown' Documents Despite the predominant use of R for data manipulation and various robust statistical calculations, in recent years, more people from various disciplines are beginning to use R for other purposes. In doing this seemlessly, further tools are needed users to easily and freely write in R for all kinds of purposes. The r2dictionary introduces a means for users to directly search for definitions of terms within the R environment.  "
  },
  {
    "id": 18790,
    "package_name": "r2fireworks",
    "title": "Enhance Your 'Rmarkdown' and 'shiny' Apps with Dazzling\nFireworks Celebrations",
    "description": "Implementation of 'JQuery' <https://jquery.com> and 'CSS' styles to allow the display of fireworks on a document. Toolkit to easily incorporate celebratory splashes in 'Rmarkdown' and 'shiny' apps.",
    "version": "0.1.0",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://r2fireworks.obi.obianom.com/",
    "bug_reports": "https://github.com/oobianom/r2fireworks/issues",
    "repository": "https://cran.r-project.org/package=r2fireworks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2fireworks Enhance Your 'Rmarkdown' and 'shiny' Apps with Dazzling\nFireworks Celebrations Implementation of 'JQuery' <https://jquery.com> and 'CSS' styles to allow the display of fireworks on a document. Toolkit to easily incorporate celebratory splashes in 'Rmarkdown' and 'shiny' apps.  "
  },
  {
    "id": 18797,
    "package_name": "r2resize",
    "title": "In-Text Resize for Images, Tables and Fancy Resize Containers in\n'shiny', 'rmarkdown' and 'quarto' Documents",
    "description": "Offers a suite of tools designed to enhance the responsiveness and interactivity of web-based documents and applications created with R. It provides an automatic, configurable resizing toolbar that can be seamlessly integrated with HTML elements such as containers, images, and tables, allowing end-users to dynamically adjust their dimensions. Beyond the toolbar, the package includes a rich collection of flexible, expandable, and interactive container functionalities, such as highly customizable split-screen layouts (splitCard), versatile sizeable cards (sizeableCard), dynamic window-like elements (windowCard), visually engaging emphasis cards (empahsisCard), and sophisticated flexible and elastic card layouts (flexCard, elastiCard). Furthermore, it offers an elegant image viewer and resizer (shinyExpandImage) perfect for interactive galleries. r2resize is particularly well-suited for developers and data scientists looking to create modern, responsive, and user-friendly 'shiny' applications, 'markdown' reports, and 'quarto' documents that adapt gracefully to different screen sizes and user preferences, significantly improving the user experience.",
    "version": "2.0",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://r2resize.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/r2resize/issues",
    "repository": "https://cran.r-project.org/package=r2resize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2resize In-Text Resize for Images, Tables and Fancy Resize Containers in\n'shiny', 'rmarkdown' and 'quarto' Documents Offers a suite of tools designed to enhance the responsiveness and interactivity of web-based documents and applications created with R. It provides an automatic, configurable resizing toolbar that can be seamlessly integrated with HTML elements such as containers, images, and tables, allowing end-users to dynamically adjust their dimensions. Beyond the toolbar, the package includes a rich collection of flexible, expandable, and interactive container functionalities, such as highly customizable split-screen layouts (splitCard), versatile sizeable cards (sizeableCard), dynamic window-like elements (windowCard), visually engaging emphasis cards (empahsisCard), and sophisticated flexible and elastic card layouts (flexCard, elastiCard). Furthermore, it offers an elegant image viewer and resizer (shinyExpandImage) perfect for interactive galleries. r2resize is particularly well-suited for developers and data scientists looking to create modern, responsive, and user-friendly 'shiny' applications, 'markdown' reports, and 'quarto' documents that adapt gracefully to different screen sizes and user preferences, significantly improving the user experience.  "
  },
  {
    "id": 18800,
    "package_name": "r2social",
    "title": "Seamless Integration of Sharing and Connect Buttons in Markdown\nand Apps",
    "description": "Implementation of 'JQuery' <https://jquery.com> and 'CSS' styles to allow easy incorporation of various social media elements on a page. The elements include addition of share buttons or connect with us buttons or hyperlink buttons to 'Shiny' applications or dashboards and 'Rmarkdown' documents.Sharing capability on social media platforms including 'Facebook' <https://www.facebook.com>, 'Linkedin' <https://www.linkedin.com>, 'X/Twitter' <https://x.com>, 'Tumblr' <https://www.tumblr.com>, 'Pinterest' <https://www.pinterest.com>, 'Whatsapp' <https://www.whatsapp.com>, 'Reddit' <https://www.reddit.com>, 'Baidu' <https://www.baidu.com>, 'Blogger' <https://www.blogger.com>, 'Weibo' <https://www.weibo.com>, 'Instagram' <https://www.instagram.com>, 'Telegram' <https://www.telegram.me>, 'Youtube' <https://www.youtube.com>.  ",
    "version": "1.2.1",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://r2social.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/r2social/issues",
    "repository": "https://cran.r-project.org/package=r2social",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2social Seamless Integration of Sharing and Connect Buttons in Markdown\nand Apps Implementation of 'JQuery' <https://jquery.com> and 'CSS' styles to allow easy incorporation of various social media elements on a page. The elements include addition of share buttons or connect with us buttons or hyperlink buttons to 'Shiny' applications or dashboards and 'Rmarkdown' documents.Sharing capability on social media platforms including 'Facebook' <https://www.facebook.com>, 'Linkedin' <https://www.linkedin.com>, 'X/Twitter' <https://x.com>, 'Tumblr' <https://www.tumblr.com>, 'Pinterest' <https://www.pinterest.com>, 'Whatsapp' <https://www.whatsapp.com>, 'Reddit' <https://www.reddit.com>, 'Baidu' <https://www.baidu.com>, 'Blogger' <https://www.blogger.com>, 'Weibo' <https://www.weibo.com>, 'Instagram' <https://www.instagram.com>, 'Telegram' <https://www.telegram.me>, 'Youtube' <https://www.youtube.com>.    "
  },
  {
    "id": 18801,
    "package_name": "r2spss",
    "title": "Format R Output to Look Like SPSS",
    "description": "Create plots and LaTeX tables that look like SPSS output for use in teaching materials.  Rather than copying-and-pasting SPSS output into documents, R code that mocks up SPSS output can be integrated directly into dynamic LaTeX documents with tools such as knitr.  Functionality includes statistical techniques that are typically covered in introductory statistics classes: descriptive statistics, common hypothesis tests, ANOVA, and linear regression, as well as box plots, histograms, scatter plots, and line plots (including profile plots).",
    "version": "0.3.2",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "author": "Andreas Alfons [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2513-3788>)",
    "url": "https://github.com/aalfons/r2spss",
    "bug_reports": "https://github.com/aalfons/r2spss/issues",
    "repository": "https://cran.r-project.org/package=r2spss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2spss Format R Output to Look Like SPSS Create plots and LaTeX tables that look like SPSS output for use in teaching materials.  Rather than copying-and-pasting SPSS output into documents, R code that mocks up SPSS output can be integrated directly into dynamic LaTeX documents with tools such as knitr.  Functionality includes statistical techniques that are typically covered in introductory statistics classes: descriptive statistics, common hypothesis tests, ANOVA, and linear regression, as well as box plots, histograms, scatter plots, and line plots (including profile plots).  "
  },
  {
    "id": 18804,
    "package_name": "r2symbols",
    "title": "Symbols for 'Markdown' and 'Shiny' Application",
    "description": "Direct insertion of over 1000 symbols (e.g. currencies, letters, emojis, arrows, mathematical symbols and so on) into 'Rmarkdown' documents and 'Shiny' applications by incorporating 'HTML' hex codes.",
    "version": "1.4",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://r2symbols.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/r2symbols/issues",
    "repository": "https://cran.r-project.org/package=r2symbols",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2symbols Symbols for 'Markdown' and 'Shiny' Application Direct insertion of over 1000 symbols (e.g. currencies, letters, emojis, arrows, mathematical symbols and so on) into 'Rmarkdown' documents and 'Shiny' applications by incorporating 'HTML' hex codes.  "
  },
  {
    "id": 18805,
    "package_name": "r311",
    "title": "Interface to the 'open311' Standard",
    "description": "Access and handle APIs that use the international 'open311'\n    'GeoReport v2' standard for civic issue tracking\n    <https://wiki.open311.org/GeoReport_v2/>. Retrieve civic service types\n    and request data. Select and add available 'open311' endpoints and\n    jurisdictions. Implicitly supports custom queries and 'open311'\n    extensions.  Requires a minimal number of hard dependencies while\n    still allowing the integration in common R formats ('xml2', 'tibble',\n    'sf').",
    "version": "0.4.3",
    "maintainer": "Jonas Lieth <jonas.lieth@gesis.org>",
    "author": "Jonas Lieth [aut, cre, cph]",
    "url": "https://ropengov.github.io/r311/, https://github.com/rOpenGov/r311",
    "bug_reports": "https://github.com/rOpenGov/r311/issues",
    "repository": "https://cran.r-project.org/package=r311",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r311 Interface to the 'open311' Standard Access and handle APIs that use the international 'open311'\n    'GeoReport v2' standard for civic issue tracking\n    <https://wiki.open311.org/GeoReport_v2/>. Retrieve civic service types\n    and request data. Select and add available 'open311' endpoints and\n    jurisdictions. Implicitly supports custom queries and 'open311'\n    extensions.  Requires a minimal number of hard dependencies while\n    still allowing the integration in common R formats ('xml2', 'tibble',\n    'sf').  "
  },
  {
    "id": 18807,
    "package_name": "r3dmol",
    "title": "Create Interactive 3D Visualizations of Molecular Data",
    "description": "Create rich and fully interactive 3D visualizations of molecular data.\n    Visualizations can be included in Shiny apps and R markdown documents, or viewed\n    from the R console and 'RStudio' Viewer. 'r3dmol' includes an extensive API\n    to manipulate the visualization after creation, and supports getting data out of\n    the visualization into R. Based on the '3dmol.js' and the 'htmlwidgets' R package.",
    "version": "0.1.2",
    "maintainer": "Wei Su <swsoyee@gmail.com>",
    "author": "Wei Su [aut, cre] (ORCID: <https://orcid.org/0000-0002-9302-5332>),\n  Brady Johnston [aut] (ORCID: <https://orcid.org/0000-0001-6301-2269>)",
    "url": "https://github.com/swsoyee/r3dmol",
    "bug_reports": "https://github.com/swsoyee/r3dmol/issues",
    "repository": "https://cran.r-project.org/package=r3dmol",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r3dmol Create Interactive 3D Visualizations of Molecular Data Create rich and fully interactive 3D visualizations of molecular data.\n    Visualizations can be included in Shiny apps and R markdown documents, or viewed\n    from the R console and 'RStudio' Viewer. 'r3dmol' includes an extensive API\n    to manipulate the visualization after creation, and supports getting data out of\n    the visualization into R. Based on the '3dmol.js' and the 'htmlwidgets' R package.  "
  },
  {
    "id": 18814,
    "package_name": "r5r",
    "title": "Rapid Realistic Routing with 'R5'",
    "description": "Rapid realistic routing on multimodal transport networks\n    (walk, bike, public transport and car) using 'R5', the Rapid Realistic\n    Routing on Real-world and Reimagined networks engine\n    <https://github.com/conveyal/r5>. The package allows users to generate\n    detailed routing analysis or calculate travel time and monetary cost matrices \n    using seamless parallel computing on top of the R5 Java machine.  While R5\n    is developed by Conveyal, the package r5r is independently developed\n    by a team at the Institute for Applied Economic Research (Ipea) with\n    contributions from collaborators. Apart from the documentation in this\n    package, users will find additional information on R5 documentation at\n    <https://docs.conveyal.com/>. Although we try to keep new releases of\n    r5r in synchrony with R5, the development of R5 follows Conveyal's\n    independent update process. Hence, users should confirm the R5 version\n    implied by the Conveyal user manual (see\n    <https://docs.conveyal.com/changelog>) corresponds with the R5 version\n    that r5r depends on. This version of r5r depends on R5 v7.1.",
    "version": "2.3.0",
    "maintainer": "Rafael H. M. Pereira <rafa.pereira.br@gmail.com>",
    "author": "Marcus Saraiva [aut] (ORCID: <https://orcid.org/0000-0001-6218-2338>),\n  Rafael H. M. Pereira [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2125-7465>),\n  Daniel Herszenhut [aut] (ORCID:\n    <https://orcid.org/0000-0001-8066-1105>),\n  Alex Magnus [aut],\n  Matthew Wigginton Bhagat-Conway [aut] (ORCID:\n    <https://orcid.org/0000-0002-1210-2982>),\n  Carlos Kaue Vieira Braga [ctb] (ORCID:\n    <https://orcid.org/0000-0002-6104-7297>),\n  Luyu Liu [ctb] (ORCID: <https://orcid.org/0000-0002-6684-5570>),\n  Daniel Snow [ctb],\n  Ipea - Institute for Applied Economic Research [cph, fnd],\n  Department of Geography & Planning, University of Toronto [fnd]",
    "url": "https://github.com/ipeaGIT/r5r, https://ipeagit.github.io/r5r/",
    "bug_reports": "https://github.com/ipeaGIT/r5r/issues",
    "repository": "https://cran.r-project.org/package=r5r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r5r Rapid Realistic Routing with 'R5' Rapid realistic routing on multimodal transport networks\n    (walk, bike, public transport and car) using 'R5', the Rapid Realistic\n    Routing on Real-world and Reimagined networks engine\n    <https://github.com/conveyal/r5>. The package allows users to generate\n    detailed routing analysis or calculate travel time and monetary cost matrices \n    using seamless parallel computing on top of the R5 Java machine.  While R5\n    is developed by Conveyal, the package r5r is independently developed\n    by a team at the Institute for Applied Economic Research (Ipea) with\n    contributions from collaborators. Apart from the documentation in this\n    package, users will find additional information on R5 documentation at\n    <https://docs.conveyal.com/>. Although we try to keep new releases of\n    r5r in synchrony with R5, the development of R5 follows Conveyal's\n    independent update process. Hence, users should confirm the R5 version\n    implied by the Conveyal user manual (see\n    <https://docs.conveyal.com/changelog>) corresponds with the R5 version\n    that r5r depends on. This version of r5r depends on R5 v7.1.  "
  },
  {
    "id": 18818,
    "package_name": "rADA",
    "title": "Statistical Analysis and Cut-Point Determination of Immunoassays",
    "description": "Systematically transform immunoassay data, evaluate if the data is normally distributed, and pick the right method for cut point determination based on that evaluation. This package can also produce plots that are needed for reports, so data analysis and visualization can be done easily.",
    "version": "1.1.9",
    "maintainer": "Emma Gail <emmahelengail@gmail.com>",
    "author": "Emma Gail [cre, aut],\n  Lidija Turkovic [aut],\n  Anil Dolgun [ctb],\n  Monther Alhamdoosh [ctb],\n  Milica Ng [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rADA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rADA Statistical Analysis and Cut-Point Determination of Immunoassays Systematically transform immunoassay data, evaluate if the data is normally distributed, and pick the right method for cut point determination based on that evaluation. This package can also produce plots that are needed for reports, so data analysis and visualization can be done easily.  "
  },
  {
    "id": 18823,
    "package_name": "rAmCharts4",
    "title": "Interface to the JavaScript Library 'amCharts 4'",
    "description": "Creates JavaScript charts. The charts can be included in 'Shiny' apps and R markdown documents, or viewed from the R console and 'RStudio' viewer. Based on the JavaScript library 'amCharts 4' and the R packages 'htmlwidgets' and 'reactR'. Currently available types of chart are: vertical and horizontal bar chart, radial bar chart, stacked bar chart, vertical and horizontal Dumbbell chart, line chart, scatter chart, range area chart, gauge chart, boxplot chart, pie chart, and 100% stacked bar chart.",
    "version": "1.6.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Antanas Marcelionis [ctb, cph] ('amCharts' library\n    (https://www.amcharts.com/)),\n  Terence Eden [ctb, cph] ('SuperTinyIcons' library\n    (https://github.com/edent/SuperTinyIcons/)),\n  Tom Alexander [ctb, cph] ('regression-js' library\n    (https://github.com/Tom-Alexander/regression-js))",
    "url": "https://github.com/stla/rAmCharts4",
    "bug_reports": "https://github.com/stla/rAmCharts4/issues",
    "repository": "https://cran.r-project.org/package=rAmCharts4",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rAmCharts4 Interface to the JavaScript Library 'amCharts 4' Creates JavaScript charts. The charts can be included in 'Shiny' apps and R markdown documents, or viewed from the R console and 'RStudio' viewer. Based on the JavaScript library 'amCharts 4' and the R packages 'htmlwidgets' and 'reactR'. Currently available types of chart are: vertical and horizontal bar chart, radial bar chart, stacked bar chart, vertical and horizontal Dumbbell chart, line chart, scatter chart, range area chart, gauge chart, boxplot chart, pie chart, and 100% stacked bar chart.  "
  },
  {
    "id": 18851,
    "package_name": "rGhanaCensus",
    "title": "2021 Ghana Population and Housing Census Results as Data Frames",
    "description": "Datasets from the 2021 Ghana Population and Housing Census Results. Users can access results as 'tidyverse' and 'sf'-Ready Data Frames.    The data in this package is scraped from pdf reports released by the Ghana Statistical Service website <https://census2021.statsghana.gov.gh/> . The package currently only contains datasets from the literacy and education reports. Namely, school attendance data for respondents aged 3 years and above.",
    "version": "0.1.0",
    "maintainer": "Ama Owusu-Darko <aowusuda@asu.edu>",
    "author": "Ama Owusu-Darko [cre, aut]",
    "url": "https://github.com/ktemadarko/rGhanaCensus",
    "bug_reports": "https://github.com/ktemadarko/rGhanaCensus/issues",
    "repository": "https://cran.r-project.org/package=rGhanaCensus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rGhanaCensus 2021 Ghana Population and Housing Census Results as Data Frames Datasets from the 2021 Ghana Population and Housing Census Results. Users can access results as 'tidyverse' and 'sf'-Ready Data Frames.    The data in this package is scraped from pdf reports released by the Ghana Statistical Service website <https://census2021.statsghana.gov.gh/> . The package currently only contains datasets from the literacy and education reports. Namely, school attendance data for respondents aged 3 years and above.  "
  },
  {
    "id": 18857,
    "package_name": "rJavaEnv",
    "title": "'Java' Environments for R Projects",
    "description": "Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.",
    "version": "0.3.0",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6690-5345>),\n  Chung-hong Chan [aut] (ORCID: <https://orcid.org/0000-0002-6232-7530>),\n  Mauricio Vargas [ctb] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\n  Hadley Wickham [ctb] (use_java feature suggestion and PR review),\n  Enrique Mondragon-Estrada [ctb] (ORCID:\n    <https://orcid.org/0009-0004-5592-1728>),\n  Jonas Lieth [ctb] (ORCID: <https://orcid.org/0000-0002-3451-3176>)",
    "url": "https://github.com/e-kotov/rJavaEnv,\nhttps://www.ekotov.pro/rJavaEnv/",
    "bug_reports": "https://github.com/e-kotov/rJavaEnv/issues",
    "repository": "https://cran.r-project.org/package=rJavaEnv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rJavaEnv 'Java' Environments for R Projects Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.  "
  },
  {
    "id": 18862,
    "package_name": "rLDCP",
    "title": "Text Generation from Data",
    "description": "Linguistic Descriptions of Complex Phenomena (LDCP) is an architecture and methodology that allows us to model complex phenomena, interpreting input data, and generating automatic text reports customized to the user needs (see <doi:10.1016/j.ins.2016.11.002> and <doi:10.1007/s00500-016-2430-5>). The proposed package contains a set of methods that facilitates the development of LDCP systems. It main goal is increasing the visibility and practical use of this research line.",
    "version": "1.0.2",
    "maintainer": "Patricia Conde-Clemente <patricia.condeclemente@gmail.com>",
    "author": "Patricia Conde-Clemente [aut, cre], Jose M. Alonso [aut], Gracian Trivino [aut]",
    "url": "http://phedes.com/rLDCP",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rLDCP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rLDCP Text Generation from Data Linguistic Descriptions of Complex Phenomena (LDCP) is an architecture and methodology that allows us to model complex phenomena, interpreting input data, and generating automatic text reports customized to the user needs (see <doi:10.1016/j.ins.2016.11.002> and <doi:10.1007/s00500-016-2430-5>). The proposed package contains a set of methods that facilitates the development of LDCP systems. It main goal is increasing the visibility and practical use of this research line.  "
  },
  {
    "id": 18904,
    "package_name": "rTRNG",
    "title": "Advanced and Parallel Random Number Generation via 'TRNG'",
    "description": "Embeds sources and headers from Tina's Random\n    Number Generator ('TRNG') C++ library. Exposes some functionality for\n    easier access, testing and benchmarking into R. Provides examples of\n    how to use parallel RNG with 'RcppParallel'. The methods and\n    techniques behind 'TRNG' are illustrated in the package vignettes and\n    examples. Full documentation is available in Bauke (2021)\n    <https://github.com/rabauke/trng4/blob/v4.23.1/doc/trng.pdf>.",
    "version": "4.23.1-4",
    "maintainer": "Riccardo Porreca <riccardo.porreca@mirai-solutions.com>",
    "author": "Riccardo Porreca [aut, cre],\n  Roland Schmid [aut],\n  Mirai Solutions GmbH [cph],\n  Heiko Bauke [ctb, cph] (TRNG sources and headers)",
    "url": "https://github.com/miraisolutions/rTRNG#readme,\nhttps://mirai-solutions.ch",
    "bug_reports": "https://github.com/miraisolutions/rTRNG/issues",
    "repository": "https://cran.r-project.org/package=rTRNG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rTRNG Advanced and Parallel Random Number Generation via 'TRNG' Embeds sources and headers from Tina's Random\n    Number Generator ('TRNG') C++ library. Exposes some functionality for\n    easier access, testing and benchmarking into R. Provides examples of\n    how to use parallel RNG with 'RcppParallel'. The methods and\n    techniques behind 'TRNG' are illustrated in the package vignettes and\n    examples. Full documentation is available in Bauke (2021)\n    <https://github.com/rabauke/trng4/blob/v4.23.1/doc/trng.pdf>.  "
  },
  {
    "id": 18909,
    "package_name": "rUM",
    "title": "R Templates from the University of Miami",
    "description": "This holds r markdown and quarto templates for academic papers and \n    slide decks. It also has templates to create research projects which \n    contain academic papers as vignettes.",
    "version": "2.2.0",
    "maintainer": "Raymond Balise <balise@miami.edu>",
    "author": "Raymond Balise [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9856-5901>),\n  Gabriel Odom [aut] (ORCID: <https://orcid.org/0000-0003-1341-4555>),\n  Kyle Grealis [aut] (ORCID: <https://orcid.org/0000-0002-9223-8854>),\n  Francisco Cardozo [aut] (ORCID:\n    <https://orcid.org/0000-0002-1925-4954>),\n  Frank Gutierrez [ctb] (ORCID: <https://orcid.org/0009-0005-9742-6992>)",
    "url": "https://raymondbalise.github.io/rUM/,\nhttps://github.com/RaymondBalise/rUM",
    "bug_reports": "https://github.com/RaymondBalise/rUM/issues",
    "repository": "https://cran.r-project.org/package=rUM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rUM R Templates from the University of Miami This holds r markdown and quarto templates for academic papers and \n    slide decks. It also has templates to create research projects which \n    contain academic papers as vignettes.  "
  },
  {
    "id": 18923,
    "package_name": "radiant.data",
    "title": "Data Menu for Radiant: Business Analytics using R and Shiny",
    "description": "The Radiant Data menu includes interfaces for loading, saving,\n    viewing, visualizing, summarizing, transforming, and combining data. It also\n    contains functionality to generate reproducible reports of the analyses\n    conducted in the application.",
    "version": "1.6.8",
    "maintainer": "Vincent Nijs <radiant@rady.ucsd.edu>",
    "author": "Vincent Nijs [aut, cre],\n  Niklas von Hertzen [aut] (html2canvas library)",
    "url": "https://github.com/radiant-rstats/radiant.data/,\nhttps://radiant-rstats.github.io/radiant.data/,\nhttps://radiant-rstats.github.io/docs/",
    "bug_reports": "https://github.com/radiant-rstats/radiant.data/issues/",
    "repository": "https://cran.r-project.org/package=radiant.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "radiant.data Data Menu for Radiant: Business Analytics using R and Shiny The Radiant Data menu includes interfaces for loading, saving,\n    viewing, visualizing, summarizing, transforming, and combining data. It also\n    contains functionality to generate reproducible reports of the analyses\n    conducted in the application.  "
  },
  {
    "id": 18937,
    "package_name": "raincin",
    "title": "Ranking with Incomplete Information",
    "description": "Various statistical and mathematical ranking and rating methods with incomplete information are included. This package is initially designed for the scoring system in a high school project showcase to rank student research projects, where each judge can only evaluate a set of projects in a limited time period. See Langville, A. N. and Meyer, C. D. (2012), Who is Number 1: The Science of Rating and Ranking, Princeton University Press <doi:10.1515/9781400841677>, and Gou, J. and Wu, S. (2020), A Judging System for Project Showcase: Rating and Ranking with Incomplete Information, Technical Report.",
    "version": "1.0.3",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing Zhang [aut],\n  Shuyi Wu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=raincin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raincin Ranking with Incomplete Information Various statistical and mathematical ranking and rating methods with incomplete information are included. This package is initially designed for the scoring system in a high school project showcase to rank student research projects, where each judge can only evaluate a set of projects in a limited time period. See Langville, A. N. and Meyer, C. D. (2012), Who is Number 1: The Science of Rating and Ranking, Princeton University Press <doi:10.1515/9781400841677>, and Gou, J. and Wu, S. (2020), A Judging System for Project Showcase: Rating and Ranking with Incomplete Information, Technical Report.  "
  },
  {
    "id": 18943,
    "package_name": "ramchoice",
    "title": "Revealed Preference and Attention Analysis in Random Limited\nAttention Models",
    "description": "It is widely documented in psychology, economics and other disciplines that socio-economic agent may not pay full attention to all available alternatives, rendering standard revealed preference theory invalid. This package implements the estimation and inference procedures of Cattaneo, Ma, Masatlioglu and Suleymanov (2020) <arXiv:1712.03448> and Cattaneo, Cheung, Ma, and Masatlioglu (2022) <arXiv:2110.10650>, which utilizes standard choice data to partially identify and estimate a decision maker's preference and attention. For inference, several simulation-based critical values are provided. ",
    "version": "2.2",
    "maintainer": "Xinwei Ma <x1ma@ucsd.edu>",
    "author": "Matias D. Cattaneo, Paul Cheung, Xinwei Ma, Yusufcan Masatlioglu, Elchin Suleymanov",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ramchoice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ramchoice Revealed Preference and Attention Analysis in Random Limited\nAttention Models It is widely documented in psychology, economics and other disciplines that socio-economic agent may not pay full attention to all available alternatives, rendering standard revealed preference theory invalid. This package implements the estimation and inference procedures of Cattaneo, Ma, Masatlioglu and Suleymanov (2020) <arXiv:1712.03448> and Cattaneo, Cheung, Ma, and Masatlioglu (2022) <arXiv:2110.10650>, which utilizes standard choice data to partially identify and estimate a decision maker's preference and attention. For inference, several simulation-based critical values are provided.   "
  },
  {
    "id": 18995,
    "package_name": "rapidoc",
    "title": "Generates 'RapiDoc' Documentation from an 'OpenAPI'\nSpecification",
    "description": "A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'RapiDoc' documentation from an 'OpenAPI' Specification:\n   <https://mrin9.github.io/RapiDoc/>.",
    "version": "9.3.4",
    "maintainer": "Bruno Tremblay <openr@neoxone.com>",
    "author": "Bruno Tremblay [aut, cre],\n  Barret Schloerke [ctb] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\n  Mrinmoy Majumdar [cph]",
    "url": "https://github.com/meztez/rapidoc",
    "bug_reports": "https://github.com/meztez/rapidoc/issues",
    "repository": "https://cran.r-project.org/package=rapidoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rapidoc Generates 'RapiDoc' Documentation from an 'OpenAPI'\nSpecification A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'RapiDoc' documentation from an 'OpenAPI' Specification:\n   <https://mrin9.github.io/RapiDoc/>.  "
  },
  {
    "id": 18997,
    "package_name": "rapidraker",
    "title": "Rapid Automatic Keyword Extraction (RAKE) Algorithm",
    "description": "A 'Java' implementation of the RAKE algorithm ('Rose', S., 'Engel', D., \n  'Cramer', N. and 'Cowley', W. (2010) <doi:10.1002/9780470689646.ch1>), which can \n  be used to extract keywords from documents without any training data.",
    "version": "0.1.3",
    "maintainer": "Christopher Baker <chriscrewbaker@gmail.com>",
    "author": "Christopher Baker [aut, cre]",
    "url": "https://crew102.github.io/slowraker/articles/rapidraker.html",
    "bug_reports": "https://github.com/crew102/rapidraker/issues",
    "repository": "https://cran.r-project.org/package=rapidraker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rapidraker Rapid Automatic Keyword Extraction (RAKE) Algorithm A 'Java' implementation of the RAKE algorithm ('Rose', S., 'Engel', D., \n  'Cramer', N. and 'Cowley', W. (2010) <doi:10.1002/9780470689646.ch1>), which can \n  be used to extract keywords from documents without any training data.  "
  },
  {
    "id": 19000,
    "package_name": "rappleads",
    "title": "Get Data from 'Apple Ads Campaign Management API'",
    "description": "Provides functions to load and manage data from Apple Ads \n    accounts using the 'Apple Ads Campaign Management API' \n    <https://developer.apple.com/documentation/apple_ads>.",
    "version": "0.1.3",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0410-7385>),\n  Netpeak [cph]",
    "url": "https://github.com/selesnow/rappleads",
    "bug_reports": "https://github.com/selesnow/rappleads/issues",
    "repository": "https://cran.r-project.org/package=rappleads",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rappleads Get Data from 'Apple Ads Campaign Management API' Provides functions to load and manage data from Apple Ads \n    accounts using the 'Apple Ads Campaign Management API' \n    <https://developer.apple.com/documentation/apple_ads>.  "
  },
  {
    "id": 19001,
    "package_name": "rapport",
    "title": "A Report Templating System",
    "description": "Facilitating the creation of reproducible statistical\n    report templates. Once created, rapport templates can be exported to\n    various external formats (HTML, LaTeX, PDF, ODT etc.) with pandoc as the\n    converter backend.",
    "version": "1.2",
    "maintainer": "Gergely Dar\u00f3czi <daroczig@rapporter.net>",
    "author": "Aleksandar Blagoti\u0107 [aut],\n  Gergely Dar\u00f3czi [aut, cre]",
    "url": "https://rapporter.github.io/rapport/",
    "bug_reports": "https://github.com/rapporter/rapport/issues",
    "repository": "https://cran.r-project.org/package=rapport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rapport A Report Templating System Facilitating the creation of reproducible statistical\n    report templates. Once created, rapport templates can be exported to\n    various external formats (HTML, LaTeX, PDF, ODT etc.) with pandoc as the\n    converter backend.  "
  },
  {
    "id": 19002,
    "package_name": "rapportools",
    "title": "Miscellaneous (Stats) Helper Functions with Sane Defaults for\nReporting",
    "description": "Helper functions that act as wrappers to more advanced statistical\n    methods with the advantage of having sane defaults for quick reporting.",
    "version": "1.2",
    "maintainer": "Gergely Dar\u00f3czi <daroczig@rapporter.net>",
    "author": "Gergely Dar\u00f3czi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3149-8537>),\n  Aleksandar Blagoti\u0107 [aut]",
    "url": "",
    "bug_reports": "https://github.com/rapporter/rapportools/issues",
    "repository": "https://cran.r-project.org/package=rapportools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rapportools Miscellaneous (Stats) Helper Functions with Sane Defaults for\nReporting Helper functions that act as wrappers to more advanced statistical\n    methods with the advantage of having sane defaults for quick reporting.  "
  },
  {
    "id": 19008,
    "package_name": "raqs",
    "title": "Interface to the US EPA Air Quality System (AQS) API",
    "description": "Offers functions for fetching JSON data from the US EPA Air Quality\n             System (AQS) API with options to comply with the API rate limits.\n             See <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             details of the AQS API.",
    "version": "1.0.2",
    "maintainer": "Jaehyun Joo <jaehyunjoo@outlook.com>",
    "author": "Jaehyun Joo [aut, cre],\n  Blanca Himes [aut]",
    "url": "https://github.com/HimesGroup/raqs",
    "bug_reports": "https://github.com/HimesGroup/raqs/issues",
    "repository": "https://cran.r-project.org/package=raqs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raqs Interface to the US EPA Air Quality System (AQS) API Offers functions for fetching JSON data from the US EPA Air Quality\n             System (AQS) API with options to comply with the API rate limits.\n             See <https://aqs.epa.gov/aqsweb/documents/data_api.html> for\n             details of the AQS API.  "
  },
  {
    "id": 19014,
    "package_name": "rasciidoc",
    "title": "Create Reports Using R and 'asciidoc'",
    "description": "Inspired by Karl Broman`s reader on using 'knitr'\n    with 'asciidoc'\n    (<https://kbroman.org/knitr_knutshell/pages/asciidoc.html>), this is\n    merely a wrapper to 'knitr' and 'asciidoc'.",
    "version": "4.1.1",
    "maintainer": "Andreas Dominik Cullmann <fvafrcu@mailbox.org>",
    "author": "Andreas Dominik Cullmann [aut, cre]",
    "url": "https://gitlab.com/fvafrcu/rasciidoc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rasciidoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasciidoc Create Reports Using R and 'asciidoc' Inspired by Karl Broman`s reader on using 'knitr'\n    with 'asciidoc'\n    (<https://kbroman.org/knitr_knutshell/pages/asciidoc.html>), this is\n    merely a wrapper to 'knitr' and 'asciidoc'.  "
  },
  {
    "id": 19025,
    "package_name": "rasterpdf",
    "title": "Plot Raster Graphics in PDF Files",
    "description": "The ability to plot raster graphics in PDF files can be useful\n    when one needs multi-page documents, but the plots contain so many\n    individual elements that (the usual) use of vector graphics results in\n    inconveniently large file sizes. Internally, the package plots each\n    individual page as a PNG, and then combines them in one PDF file.",
    "version": "0.1.1",
    "maintainer": "Ilari Scheinin <ilari.scheinin+rasterpdf@gmail.com>",
    "author": "Ilari Scheinin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4696-9066>)",
    "url": "https://ilarischeinin.github.io/rasterpdf,\nhttps://github.com/ilarischeinin/rasterpdf",
    "bug_reports": "https://github.com/ilarischeinin/rasterpdf/issues",
    "repository": "https://cran.r-project.org/package=rasterpdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rasterpdf Plot Raster Graphics in PDF Files The ability to plot raster graphics in PDF files can be useful\n    when one needs multi-page documents, but the plots contain so many\n    individual elements that (the usual) use of vector graphics results in\n    inconveniently large file sizes. Internally, the package plots each\n    individual page as a PNG, and then combines them in one PDF file.  "
  },
  {
    "id": 19039,
    "package_name": "ravelRy",
    "title": "An Interface to the 'Ravelry' API",
    "description": "Provides access to the 'Ravelry' API <https://www.ravelry.com/groups/ravelry-api>. An R wrapper for pulling data from 'Ravelry.com', an organizational tool for crocheters, knitters, spinners, and weavers. You can retrieve pattern, yarn, author, and shop information by search or by a given id.  ",
    "version": "0.1.0",
    "maintainer": "Kaylin Pavlik <walkerkq2@gmail.com>",
    "author": "Kaylin Pavlik [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ravelRy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ravelRy An Interface to the 'Ravelry' API Provides access to the 'Ravelry' API <https://www.ravelry.com/groups/ravelry-api>. An R wrapper for pulling data from 'Ravelry.com', an organizational tool for crocheters, knitters, spinners, and weavers. You can retrieve pattern, yarn, author, and shop information by search or by a given id.    "
  },
  {
    "id": 19041,
    "package_name": "ravepipeline",
    "title": "Reproducible Pipeline Infrastructure for Neuroscience",
    "description": "Defines the underlying pipeline structure for reproducible \n    neuroscience, adopted by 'RAVE' (reproducible analysis and visualization \n    of intracranial electroencephalography); provides high-level class \n    definition to build, compile, set, execute, and share analysis pipelines. \n    Both R and 'Python' are supported, with 'Markdown' and 'shiny' dashboard \n    templates for extending and building customized pipelines. See the full \n    documentations at <https://rave.wiki>; to cite us, \n    check out our paper by Magnotti, Wang, and Beauchamp (2020, \n    <doi:10.1016/j.neuroimage.2020.117341>), or run \n    citation(\"ravepipeline\") for details.",
    "version": "0.0.3",
    "maintainer": "Zhengjia Wang <dipterix.wang@gmail.com>",
    "author": "Zhengjia Wang [aut, cre, cph],\n  John Magnotti [ctb, res],\n  Xiang Zhang [ctb, res],\n  Michael Beauchamp [ctb, res],\n  Trustees of University of Pennsylvania [cph] (Copyright Holder)",
    "url": "https://dipterix.org/ravepipeline/, https://rave.wiki",
    "bug_reports": "https://github.com/dipterix/ravepipeline/issues",
    "repository": "https://cran.r-project.org/package=ravepipeline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ravepipeline Reproducible Pipeline Infrastructure for Neuroscience Defines the underlying pipeline structure for reproducible \n    neuroscience, adopted by 'RAVE' (reproducible analysis and visualization \n    of intracranial electroencephalography); provides high-level class \n    definition to build, compile, set, execute, and share analysis pipelines. \n    Both R and 'Python' are supported, with 'Markdown' and 'shiny' dashboard \n    templates for extending and building customized pipelines. See the full \n    documentations at <https://rave.wiki>; to cite us, \n    check out our paper by Magnotti, Wang, and Beauchamp (2020, \n    <doi:10.1016/j.neuroimage.2020.117341>), or run \n    citation(\"ravepipeline\") for details.  "
  },
  {
    "id": 19042,
    "package_name": "ravetools",
    "title": "Signal and Image Processing Toolbox for Analyzing Intracranial\nElectroencephalography Data",
    "description": "Implemented fast and memory-efficient Notch-filter, \n    Welch-periodogram, discrete wavelet spectrogram for minutes of \n    high-resolution signals, fast 3D convolution, image registration,\n    3D mesh manipulation; providing fundamental toolbox for intracranial \n    Electroencephalography (iEEG) pipelines. \n    Documentation and examples about 'RAVE' project are provided at \n    <https://rave.wiki>, and the paper by John F. Magnotti, \n    Zhengjia Wang, Michael S. Beauchamp (2020) \n    <doi:10.1016/j.neuroimage.2020.117341>; see 'citation(\"ravetools\")' for \n    details.",
    "version": "0.2.4",
    "maintainer": "Zhengjia Wang <dipterix.wang@gmail.com>",
    "author": "Zhengjia Wang [aut, cre],\n  John Magnotti [aut],\n  Michael Beauchamp [aut],\n  Trustees of the University of Pennsylvania [cph] (All files in this\n    package unless explicitly stated in the file or listed in the\n    'Copyright' section below.),\n  Karim Rahim [cph, ctb] (Contributed to src/ffts.h and stc/ffts.cpp),\n  Thomas Possidente [cph, ctb] (Contributed to R/multitaper.R),\n  Michael Prerau [cph, ctb] (Contributed to R/multitaper.R),\n  Marcus Geelnard [ctb, cph] (TinyThread library,\n    tinythreadpp.bitsnbites.eu, located at inst/include/tthread/),\n  Stefan Schlager [ctb, cph] (R-vcg interface, located at\n    src/vcgCommon.h),\n  Visual Computing Lab, ISTI [ctb, cph] (Copyright holder of vcglib,\n    located at src/vcglib/)",
    "url": "https://rave.wiki, https://dipterix.org/ravetools/",
    "bug_reports": "https://github.com/dipterix/ravetools/issues",
    "repository": "https://cran.r-project.org/package=ravetools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ravetools Signal and Image Processing Toolbox for Analyzing Intracranial\nElectroencephalography Data Implemented fast and memory-efficient Notch-filter, \n    Welch-periodogram, discrete wavelet spectrogram for minutes of \n    high-resolution signals, fast 3D convolution, image registration,\n    3D mesh manipulation; providing fundamental toolbox for intracranial \n    Electroencephalography (iEEG) pipelines. \n    Documentation and examples about 'RAVE' project are provided at \n    <https://rave.wiki>, and the paper by John F. Magnotti, \n    Zhengjia Wang, Michael S. Beauchamp (2020) \n    <doi:10.1016/j.neuroimage.2020.117341>; see 'citation(\"ravetools\")' for \n    details.  "
  },
  {
    "id": 19043,
    "package_name": "raw",
    "title": "R Actuarial Workshops",
    "description": "In order to facilitate R instruction for actuaries, we have organized several \n  sets of publicly available data of interest to non-life actuaries. In addition, we suggest \n  a set of packages, which most practicing actuaries will use routinely. Finally, there is \n  an R markdown skeleton for basic reserve analysis.",
    "version": "0.1.8",
    "maintainer": "Brian A. Fannin <captain@PirateGrunt.com>",
    "author": "Brian A. Fannin [aut, cre]",
    "url": "http://casact.github.io/raw_package/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=raw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raw R Actuarial Workshops In order to facilitate R instruction for actuaries, we have organized several \n  sets of publicly available data of interest to non-life actuaries. In addition, we suggest \n  a set of packages, which most practicing actuaries will use routinely. Finally, there is \n  an R markdown skeleton for basic reserve analysis.  "
  },
  {
    "id": 19070,
    "package_name": "rbm25",
    "title": "A Light Wrapper Around the 'BM25' 'Rust' Crate for Okapi BM25\nText Search",
    "description": "\n    BM25 is a ranking function used by search engines to rank matching documents according to their relevance to a user's search query.\n    This package provides a light wrapper around the 'BM25' 'rust' crate for Okapi BM25 text search.\n    For more information, see Robertson et al. (1994) <https://trec.nist.gov/pubs/trec3/t3_proceedings.html>.",
    "version": "0.0.4",
    "maintainer": "David Zimmermann-Kollenda <david_j_zimmermann@hotmail.com>",
    "author": "David Zimmermann-Kollenda [aut, cre],\n  Michael Barlow [aut] (bm25 Rust library),\n  Authors of the dependency Rust crates [aut] (see AUTHORS file)",
    "url": "https://davzim.github.io/rbm25/, https://github.com/DavZim/rbm25/",
    "bug_reports": "https://github.com/DavZim/rbm25/issues",
    "repository": "https://cran.r-project.org/package=rbm25",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbm25 A Light Wrapper Around the 'BM25' 'Rust' Crate for Okapi BM25\nText Search \n    BM25 is a ranking function used by search engines to rank matching documents according to their relevance to a user's search query.\n    This package provides a light wrapper around the 'BM25' 'rust' crate for Okapi BM25 text search.\n    For more information, see Robertson et al. (1994) <https://trec.nist.gov/pubs/trec3/t3_proceedings.html>.  "
  },
  {
    "id": 19073,
    "package_name": "rbounds",
    "title": "Perform Rosenbaum Bounds Sensitivity Tests for Matched and\nUnmatched Data",
    "description": "Takes matched and unmatched data and calculates Rosenbaum bounds for the treatment effect.  Calculates bounds for binary outcome data, Hodges-Lehmann point estimates, Wilcoxon signed-rank test for matched data and matched IV estimators, Wilcoxon sum rank test, and for data with multiple matched controls. The sensitivity analysis methods in this package are documented in Rosenbaum (2002) Observational Studies, <doi:10.1007/978-1-4757-3692-2>, Springer-Verlag.",
    "version": "2.2",
    "maintainer": "Luke J. Keele <luke.keele@gmail.com>",
    "author": "Luke J. Keele",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rbounds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbounds Perform Rosenbaum Bounds Sensitivity Tests for Matched and\nUnmatched Data Takes matched and unmatched data and calculates Rosenbaum bounds for the treatment effect.  Calculates bounds for binary outcome data, Hodges-Lehmann point estimates, Wilcoxon signed-rank test for matched data and matched IV estimators, Wilcoxon sum rank test, and for data with multiple matched controls. The sensitivity analysis methods in this package are documented in Rosenbaum (2002) Observational Studies, <doi:10.1007/978-1-4757-3692-2>, Springer-Verlag.  "
  },
  {
    "id": 19074,
    "package_name": "rbranding",
    "title": "Manage Branding and Accessibility of R Projects",
    "description": "A tool for building projects that are visually consistent,\n    accessible, and easy to maintain. It provides functions for managing\n    branding assets, applying organization-wide themes using 'brand.yml',\n    and setting up new projects with accessibility features and correct\n    branding. It supports 'quarto', 'shiny', and 'rmarkdown' projects, and\n    integrates with 'ggplot2'. The accessibility features are based\n    on the Web Content Accessibility Guidelines\n    <https://www.w3.org/WAI/WCAG22/quickref/?versions=2.1>\n    and Accessible Rich Internet Applications (ARIA) specifications\n    <https://www.w3.org/WAI/ARIA/apg/>. The branding framework implements\n    the 'brand.yml' specification <https://posit-dev.github.io/brand-yml/>.",
    "version": "0.1.1",
    "maintainer": "Willy Ray <william.ray@hsc.utah.edu>",
    "author": "Willy Ray [aut, cre],\n  Andrew Pulsipher [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-0773-3210>),\n  Centers for Disease Control and Prevention's Center for Forecasting and\n    Outbreak Analytics [fnd] (Cooperative agreement CDC-RFA-FT-23-0069)",
    "url": "https://epiforesite.github.io/rbranding/,\nhttps://github.com/EpiForeSITE/rbranding",
    "bug_reports": "https://github.com/EpiForeSITE/rbranding/issues",
    "repository": "https://cran.r-project.org/package=rbranding",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbranding Manage Branding and Accessibility of R Projects A tool for building projects that are visually consistent,\n    accessible, and easy to maintain. It provides functions for managing\n    branding assets, applying organization-wide themes using 'brand.yml',\n    and setting up new projects with accessibility features and correct\n    branding. It supports 'quarto', 'shiny', and 'rmarkdown' projects, and\n    integrates with 'ggplot2'. The accessibility features are based\n    on the Web Content Accessibility Guidelines\n    <https://www.w3.org/WAI/WCAG22/quickref/?versions=2.1>\n    and Accessible Rich Internet Applications (ARIA) specifications\n    <https://www.w3.org/WAI/ARIA/apg/>. The branding framework implements\n    the 'brand.yml' specification <https://posit-dev.github.io/brand-yml/>.  "
  },
  {
    "id": 19084,
    "package_name": "rcausim",
    "title": "Generate Causally-Simulated Data",
    "description": "Generate causally-simulated data to serve as ground truth for evaluating methods in causal discovery and effect estimation. The package provides tools to assist in defining functions based on specified edges, and conversely, defining edges based on functions. It enables the generation of data according to these predefined functions and causal structures. This is particularly useful for researchers in fields such as artificial intelligence, statistics, biology, medicine, epidemiology, economics, and social sciences, who are developing a general or a domain-specific methods to discover causal structures and estimate causal effects. Data simulation adheres to principles of structural causal modeling. Detailed methodologies and examples are documented in our vignette, available at <https://htmlpreview.github.io/?https://github.com/herdiantrisufriyana/rcausim/blob/master/doc/causal_simulation_exemplar.html>.",
    "version": "0.1.1",
    "maintainer": "Herdiantri Sufriyana <herdi@tmu.edu.tw>",
    "author": "Herdiantri Sufriyana [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9178-0222>),\n  Emily Chia-Yu Su [aut] (ORCID: <https://orcid.org/0000-0003-4801-5159>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rcausim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcausim Generate Causally-Simulated Data Generate causally-simulated data to serve as ground truth for evaluating methods in causal discovery and effect estimation. The package provides tools to assist in defining functions based on specified edges, and conversely, defining edges based on functions. It enables the generation of data according to these predefined functions and causal structures. This is particularly useful for researchers in fields such as artificial intelligence, statistics, biology, medicine, epidemiology, economics, and social sciences, who are developing a general or a domain-specific methods to discover causal structures and estimate causal effects. Data simulation adheres to principles of structural causal modeling. Detailed methodologies and examples are documented in our vignette, available at <https://htmlpreview.github.io/?https://github.com/herdiantrisufriyana/rcausim/blob/master/doc/causal_simulation_exemplar.html>.  "
  },
  {
    "id": 19095,
    "package_name": "rcdo",
    "title": "Wrapper of 'CDO' Operators",
    "description": "Provides a translation layer between 'R' and 'CDO' operators. Each \n  operator is it's own function with documentation. Nested or piped functions\n  will be translated into 'CDO' chains. ",
    "version": "0.3.2",
    "maintainer": "Elio Campitelli <eliocampitelli@gmail.com>",
    "author": "Elio Campitelli [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-7742-9230>),\n  MPI f\u00fcr Meteorologie [cph]",
    "url": "https://eliocamp.github.io/rcdo/, https://github.com/eliocamp/rcdo",
    "bug_reports": "https://github.com/eliocamp/rcdo/issues",
    "repository": "https://cran.r-project.org/package=rcdo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcdo Wrapper of 'CDO' Operators Provides a translation layer between 'R' and 'CDO' operators. Each \n  operator is it's own function with documentation. Nested or piped functions\n  will be translated into 'CDO' chains.   "
  },
  {
    "id": 19097,
    "package_name": "rchallenge",
    "title": "A Simple Data Science Challenge System",
    "description": "A simple data science challenge system using R Markdown and 'Dropbox' <https://www.dropbox.com/>.\n    It requires no network configuration, does not depend on external platforms\n    like e.g. 'Kaggle' <https://www.kaggle.com/> and can be easily installed on a personal computer.",
    "version": "1.3.4",
    "maintainer": "Adrien Todeschini <adrien.todeschini@gmail.com>",
    "author": "Adrien Todeschini [aut, cre],\n  Robin Genuer [ctb]",
    "url": "https://adrien.tspace.fr/rchallenge/,\nhttps://github.com/adrtod/rchallenge",
    "bug_reports": "https://github.com/adrtod/rchallenge/issues",
    "repository": "https://cran.r-project.org/package=rchallenge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rchallenge A Simple Data Science Challenge System A simple data science challenge system using R Markdown and 'Dropbox' <https://www.dropbox.com/>.\n    It requires no network configuration, does not depend on external platforms\n    like e.g. 'Kaggle' <https://www.kaggle.com/> and can be easily installed on a personal computer.  "
  },
  {
    "id": 19106,
    "package_name": "rcoder",
    "title": "Lightweight Data Structure for Recoding Categorical Data without\nFactors",
    "description": "A data structure and toolkit for documenting and recoding\n    categorical data that can be shared in other statistical software.",
    "version": "0.3.0",
    "maintainer": "Patrick Anker <psanker@nyu.edu>",
    "author": "Patrick Anker [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2302-0445>),\n  Global TIES for Children [cph]\n    (https://steinhardt.nyu.edu/ihdsc/global-ties)",
    "url": "https://github.com/nyuglobalties/rcoder",
    "bug_reports": "https://github.com/nyuglobalties/rcoder/issues",
    "repository": "https://cran.r-project.org/package=rcoder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcoder Lightweight Data Structure for Recoding Categorical Data without\nFactors A data structure and toolkit for documenting and recoding\n    categorical data that can be shared in other statistical software.  "
  },
  {
    "id": 19148,
    "package_name": "rdocdump",
    "title": "Dump 'R' Package Source, Documentation, and Vignettes into One\nFile",
    "description": "Dump source code, documentation and vignettes of an 'R'\n    package into a single file. Supports installed packages, tar.gz\n    archives, and package source directories. If the package is not\n    installed, only its source is automatically downloaded from CRAN for\n    processing. The output is a single plain text file or a character\n    vector, which is useful to ingest complete package documentation and\n    source into a large language model (LLM) or pass it further to other\n    tools, such as 'ragnar' <https://github.com/tidyverse/ragnar> to\n    create a Retrieval-Augmented Generation (RAG) workflow.",
    "version": "0.1.1",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6690-5345>)",
    "url": "https://github.com/e-kotov/rdocdump,\nhttps://www.ekotov.pro/rdocdump/",
    "bug_reports": "https://github.com/e-kotov/rdocdump/issues",
    "repository": "https://cran.r-project.org/package=rdocdump",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdocdump Dump 'R' Package Source, Documentation, and Vignettes into One\nFile Dump source code, documentation and vignettes of an 'R'\n    package into a single file. Supports installed packages, tar.gz\n    archives, and package source directories. If the package is not\n    installed, only its source is automatically downloaded from CRAN for\n    processing. The output is a single plain text file or a character\n    vector, which is useful to ingest complete package documentation and\n    source into a large language model (LLM) or pass it further to other\n    tools, such as 'ragnar' <https://github.com/tidyverse/ragnar> to\n    create a Retrieval-Augmented Generation (RAG) workflow.  "
  },
  {
    "id": 19150,
    "package_name": "rdoxygen",
    "title": "Create Doxygen Documentation for Source Code",
    "description": "Create doxygen documentation for source code in R packages. \n  Includes a RStudio Addin, that allows to trigger the doxygenize process.",
    "version": "1.0.0",
    "maintainer": "Clemens Schmid <clemens@nevrome.de>",
    "author": "Clemens Schmid [cre, cph, aut]",
    "url": "https://github.com/nevrome/rdoxygen",
    "bug_reports": "https://github.com/nevrome/rdoxygen/issues",
    "repository": "https://cran.r-project.org/package=rdoxygen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdoxygen Create Doxygen Documentation for Source Code Create doxygen documentation for source code in R packages. \n  Includes a RStudio Addin, that allows to trigger the doxygenize process.  "
  },
  {
    "id": 19152,
    "package_name": "rdracor",
    "title": "Access to the 'DraCor' API",
    "description": "Provide an interface for 'Drama Corpora Project' ('DraCor') API: <https://dracor.org/documentation/api>.",
    "version": "1.0.6",
    "maintainer": "Ivan Pozdniakov <bucherr@yandex.ru>",
    "author": "Ivan Pozdniakov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2450-7004>)",
    "url": "https://github.com/dracor-org/rdracor",
    "bug_reports": "https://github.com/dracor-org/rdracor/issues",
    "repository": "https://cran.r-project.org/package=rdracor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdracor Access to the 'DraCor' API Provide an interface for 'Drama Corpora Project' ('DraCor') API: <https://dracor.org/documentation/api>.  "
  },
  {
    "id": 19164,
    "package_name": "reactRouter",
    "title": "'React Router' for 'shiny' Apps and 'Quarto'",
    "description": "You can easily share url pages using 'React Router' in 'shiny' applications\n  and 'Quarto' documents. The package wraps the 'react-router-dom' 'React' library and\n  provides access to hash routing to navigate on multiple url pages.",
    "version": "0.1.1",
    "maintainer": "Felix Luginbuhl <felix.luginbuhl@protonmail.ch>",
    "author": "Felix Luginbuhl [aut, cre] (ORCID:\n    <https://orcid.org/0009-0008-6625-2899>),\n  Andryas Waurzenczak [ctb],\n  Shopify Inc. [ctb, cph] (Shopify Inc. template\n    <https://reactrouter.com/>)",
    "url": "https://felixluginbuhl.com/reactRouter/",
    "bug_reports": "https://github.com/lgnbhl/reactRouter/issues",
    "repository": "https://cran.r-project.org/package=reactRouter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reactRouter 'React Router' for 'shiny' Apps and 'Quarto' You can easily share url pages using 'React Router' in 'shiny' applications\n  and 'Quarto' documents. The package wraps the 'react-router-dom' 'React' library and\n  provides access to hash routing to navigate on multiple url pages.  "
  },
  {
    "id": 19173,
    "package_name": "readMDTable",
    "title": "Read Markdown Tables into Tibbles",
    "description": "Efficient reading of raw markdown tables into tibbles. Designed to\n    accept content from strings, files, and URLs with the ability to extract\n    and read multiple tables from markdown for analysis.",
    "version": "0.3.2",
    "maintainer": "Jordan Bradford <jrdnbradford@gmail.com>",
    "author": "Jordan Bradford [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0000-8570-3474>)",
    "url": "https://github.com/jrdnbradford/readMDTable,\nhttps://jrdnbradford.github.io/readMDTable/",
    "bug_reports": "https://github.com/jrdnbradford/readMDTable/issues",
    "repository": "https://cran.r-project.org/package=readMDTable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "readMDTable Read Markdown Tables into Tibbles Efficient reading of raw markdown tables into tibbles. Designed to\n    accept content from strings, files, and URLs with the ability to extract\n    and read multiple tables from markdown for analysis.  "
  },
  {
    "id": 19197,
    "package_name": "ready4",
    "title": "Develop and Use Modular Health Economic Models",
    "description": "A template model module, tools to help find model modules\n    derived from this template and a programming syntax to use these\n    modules in health economic analyses.  These elements are the\n    foundation for a prototype software framework for developing living\n    and transferable models and using those models in reproducible health\n    economic analyses. The software framework is extended by other R\n    libraries.  For detailed documentation about the framework and how to\n    use it visit <https://www.ready4-dev.com/>. For a background to the\n    methodological issues that the framework is attempting to help solve,\n    see Hamilton et al. (2024) <doi:10.1007/s40273-024-01378-8>.",
    "version": "0.1.19",
    "maintainer": "Matthew Hamilton <matthew.hamilton1@monash.edu>",
    "author": "Matthew Hamilton [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7407-9194>),\n  Orygen [cph, fnd],\n  Australian Government Research Training Program [fnd],\n  VicHealth [fnd],\n  Victoria University [fnd]",
    "url": "https://ready4-dev.github.io/ready4/,\nhttps://github.com/ready4-dev/ready4,\nhttps://www.ready4-dev.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ready4",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ready4 Develop and Use Modular Health Economic Models A template model module, tools to help find model modules\n    derived from this template and a programming syntax to use these\n    modules in health economic analyses.  These elements are the\n    foundation for a prototype software framework for developing living\n    and transferable models and using those models in reproducible health\n    economic analyses. The software framework is extended by other R\n    libraries.  For detailed documentation about the framework and how to\n    use it visit <https://www.ready4-dev.com/>. For a background to the\n    methodological issues that the framework is attempting to help solve,\n    see Hamilton et al. (2024) <doi:10.1007/s40273-024-01378-8>.  "
  },
  {
    "id": 19204,
    "package_name": "reasonabletools",
    "title": "Clean Water Quality Data for NPDES Reasonable Potential Analyses",
    "description": "Functions for cleaning and summarising water quality data for use in National Pollutant Discharge Elimination Service (NPDES) permit reasonable potential analyses and water quality-based effluent limitation calculations. Procedures are based on those contained in the \"Technical Support Document for Water Quality-based Toxics Control\", United States Environmental Protection Agency (1991). ",
    "version": "0.1",
    "maintainer": "Matthew Reusswig <matt.reusswig@gmail.com>",
    "author": "Matthew Reusswig [aut, cre]",
    "url": "https://github.com/mattreusswig/reasonabletools",
    "bug_reports": "https://github.com/mattreusswig/reasonabletools/issues",
    "repository": "https://cran.r-project.org/package=reasonabletools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reasonabletools Clean Water Quality Data for NPDES Reasonable Potential Analyses Functions for cleaning and summarising water quality data for use in National Pollutant Discharge Elimination Service (NPDES) permit reasonable potential analyses and water quality-based effluent limitation calculations. Procedures are based on those contained in the \"Technical Support Document for Water Quality-based Toxics Control\", United States Environmental Protection Agency (1991).   "
  },
  {
    "id": 19216,
    "package_name": "recexcavAAR",
    "title": "3D Reconstruction of Archaeological Excavations",
    "description": "A toolset for 3D reconstruction and analysis of excavations. It provides methods to reconstruct natural and artificial surfaces based on field measurements. This allows to spatially contextualize documented subunits and features. Intended to be part of a 3D visualization workflow.",
    "version": "0.3.0",
    "maintainer": "Clemens Schmid <clemens@nevrome.de>",
    "author": "Clemens Schmid [cre, cph, aut],\n  Benjamin Serbe [aut]",
    "url": "https://github.com/ISAAKiel/recexcavAAR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=recexcavAAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "recexcavAAR 3D Reconstruction of Archaeological Excavations A toolset for 3D reconstruction and analysis of excavations. It provides methods to reconstruct natural and artificial surfaces based on field measurements. This allows to spatially contextualize documented subunits and features. Intended to be part of a 3D visualization workflow.  "
  },
  {
    "id": 19217,
    "package_name": "recforest",
    "title": "Random Survival Forest for Recurrent Events",
    "description": "Analyze recurrent events with right-censored data and the potential presence of a terminal event (that prevents further occurrences, like death). 'recofest' extends the random survival forest algorithm, adapting splitting rules and node estimators to handle complexities of recurrent events. The methodology is fully described in Murris, J., Bouaziz, O., Jakubczak, M., Katsahian, S., & Lavenu, A. (2024) (<https://hal.science/hal-04612431v1/document>).",
    "version": "1.0.0",
    "maintainer": "Juliette Murris <murris.juliette@gmail.com>",
    "author": "Juliette Murris [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7017-9865>),\n  Guillaume Desachy [aut] (ORCID:\n    <https://orcid.org/0000-0002-2000-1436>),\n  Colin Fay [aut] (ORCID: <https://orcid.org/0000-0001-7343-1846>),\n  Yohann Mansiaux [aut] (ORCID: <https://orcid.org/0000-0002-8905-6603>),\n  Audrey Lavenu [aut] (ORCID: <https://orcid.org/0000-0002-0049-2397>),\n  Sandrine Katsahian [aut] (ORCID:\n    <https://orcid.org/0000-0002-7261-0671>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=recforest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "recforest Random Survival Forest for Recurrent Events Analyze recurrent events with right-censored data and the potential presence of a terminal event (that prevents further occurrences, like death). 'recofest' extends the random survival forest algorithm, adapting splitting rules and node estimators to handle complexities of recurrent events. The methodology is fully described in Murris, J., Bouaziz, O., Jakubczak, M., Katsahian, S., & Lavenu, A. (2024) (<https://hal.science/hal-04612431v1/document>).  "
  },
  {
    "id": 19222,
    "package_name": "recmap",
    "title": "Compute the Rectangular Statistical Cartogram",
    "description": "Implements the RecMap MP2 construction heuristic\n  <doi:10.1109/INFVIS.2004.57>.\n  This algorithm draws maps according to a given statistical\n  value, e.g., election results, population, or epidemiological data.\n  The basic idea of the RecMap algorithm is that each map region,\n  e.g., different countries, is represented by a rectangle.\n  The area of each rectangle represents the statistical value provided\n  as input to maintain zero cartographic error.\n  Computationally intensive tasks are implemented in C++.\n  The included vignette documents recmap algorithm usage.",
    "version": "1.0.20",
    "maintainer": "Christian Panse <Christian.Panse@gmail.com>",
    "author": "Christian Panse [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1975-3064>)",
    "url": "",
    "bug_reports": "https://github.com/cpanse/recmap/issues",
    "repository": "https://cran.r-project.org/package=recmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "recmap Compute the Rectangular Statistical Cartogram Implements the RecMap MP2 construction heuristic\n  <doi:10.1109/INFVIS.2004.57>.\n  This algorithm draws maps according to a given statistical\n  value, e.g., election results, population, or epidemiological data.\n  The basic idea of the RecMap algorithm is that each map region,\n  e.g., different countries, is represented by a rectangle.\n  The area of each rectangle represents the statistical value provided\n  as input to maintain zero cartographic error.\n  Computationally intensive tasks are implemented in C++.\n  The included vignette documents recmap algorithm usage.  "
  },
  {
    "id": 19246,
    "package_name": "redcas",
    "title": "An Interface to the Computer Algebra System 'REDUCE'",
    "description": "'REDUCE' is a portable general-purpose computer algebra system supporting scalar, vector, matrix and tensor algebra, symbolic differential and integral calculus, arbitrary precision numerical calculations and output in 'LaTeX' format. 'REDUCE' is based on 'Lisp' and is available on the two dialects 'Portable Standard Lisp' ('PSL') and 'Codemist Standard Lisp' ('CSL'). The 'redcas' package provides an interface for executing arbitrary 'REDUCE' code interactively from 'R', returning output as character vectors. 'R' code and 'REDUCE' code can be interspersed. It also provides a specialized function for calling the 'REDUCE' feature for solving systems of equations, returning the output as an 'R' object designed for the purpose. A further specialized function uses 'REDUCE' features to generate 'LaTeX' output and post-processes this for direct use in 'LaTeX' documents, e.g. using 'Sweave'.",
    "version": "0.1.1",
    "maintainer": "Martin Gregory <mairtin.macghreagoir@gmail.com>",
    "author": "Martin Gregory [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=redcas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redcas An Interface to the Computer Algebra System 'REDUCE' 'REDUCE' is a portable general-purpose computer algebra system supporting scalar, vector, matrix and tensor algebra, symbolic differential and integral calculus, arbitrary precision numerical calculations and output in 'LaTeX' format. 'REDUCE' is based on 'Lisp' and is available on the two dialects 'Portable Standard Lisp' ('PSL') and 'Codemist Standard Lisp' ('CSL'). The 'redcas' package provides an interface for executing arbitrary 'REDUCE' code interactively from 'R', returning output as character vectors. 'R' code and 'REDUCE' code can be interspersed. It also provides a specialized function for calling the 'REDUCE' feature for solving systems of equations, returning the output as an 'R' object designed for the purpose. A further specialized function uses 'REDUCE' features to generate 'LaTeX' output and post-processes this for direct use in 'LaTeX' documents, e.g. using 'Sweave'.  "
  },
  {
    "id": 19252,
    "package_name": "redoc",
    "title": "Generates 'Redoc' Documentation from an 'OpenAPI' Specification",
    "description": "A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'Redoc' documentation from an 'OpenAPI' Specification:\n   <https://redocly.com/redoc/>.",
    "version": "2.0.0.75",
    "maintainer": "Bruno Tremblay <openr@neoxone.com>",
    "author": "Bruno Tremblay [aut, cre],\n  Barret Schloerke [ctb] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\n  Rebilly [aut, cph]",
    "url": "https://github.com/meztez/redoc",
    "bug_reports": "https://github.com/meztez/redoc/issues",
    "repository": "https://cran.r-project.org/package=redoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redoc Generates 'Redoc' Documentation from an 'OpenAPI' Specification A collection of 'HTML', 'JavaScript', 'CSS' and fonts\n  assets that generate 'Redoc' documentation from an 'OpenAPI' Specification:\n   <https://redocly.com/redoc/>.  "
  },
  {
    "id": 19254,
    "package_name": "redux",
    "title": "R Bindings to 'hiredis'",
    "description": "A 'hiredis' wrapper that includes support for\n    transactions, pipelining, blocking subscription, serialisation of\n    all keys and values, 'Redis' error handling with R errors.\n    Includes an automatically generated 'R6' interface to the full\n    'hiredis' API.  Generated functions are faithful to the\n    'hiredis' documentation while attempting to match R's argument\n    semantics.  Serialisation must be explicitly done by the user, but\n    both binary and text-mode serialisation is supported.",
    "version": "1.1.5",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre]",
    "url": "https://github.com/richfitz/redux,\nhttps://richfitz.github.io/redux/",
    "bug_reports": "https://github.com/richfitz/redux/issues",
    "repository": "https://cran.r-project.org/package=redux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redux R Bindings to 'hiredis' A 'hiredis' wrapper that includes support for\n    transactions, pipelining, blocking subscription, serialisation of\n    all keys and values, 'Redis' error handling with R errors.\n    Includes an automatically generated 'R6' interface to the full\n    'hiredis' API.  Generated functions are faithful to the\n    'hiredis' documentation while attempting to match R's argument\n    semantics.  Serialisation must be explicitly done by the user, but\n    both binary and text-mode serialisation is supported.  "
  },
  {
    "id": 19260,
    "package_name": "refitME",
    "title": "Measurement Error Modelling using MCEM",
    "description": "Fits measurement error models using Monte Carlo Expectation Maximization (MCEM). For specific details on the methodology, see: Greg C. G. Wei & Martin A. Tanner (1990) A Monte Carlo Implementation of the EM Algorithm and the Poor Man's Data Augmentation Algorithms, Journal of the American Statistical Association, 85:411, 699-704 <doi:10.1080/01621459.1990.10474930> For more examples on measurement error modelling using MCEM, see the 'RMarkdown' vignette: \"'refitME' R-package tutorial\".",
    "version": "1.3.1",
    "maintainer": "Jakub Stoklosa <j.stoklosa@unsw.edu.au>",
    "author": "Jakub Stoklosa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6523-4524>),\n  Wenhan Hwang [aut, ctb],\n  David Warton [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=refitME",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "refitME Measurement Error Modelling using MCEM Fits measurement error models using Monte Carlo Expectation Maximization (MCEM). For specific details on the methodology, see: Greg C. G. Wei & Martin A. Tanner (1990) A Monte Carlo Implementation of the EM Algorithm and the Poor Man's Data Augmentation Algorithms, Journal of the American Statistical Association, 85:411, 699-704 <doi:10.1080/01621459.1990.10474930> For more examples on measurement error modelling using MCEM, see the 'RMarkdown' vignette: \"'refitME' R-package tutorial\".  "
  },
  {
    "id": 19268,
    "package_name": "refuge",
    "title": "Locate Trans and Intersex-Friendly Toilets",
    "description": "Access the 'Refuge' API, a web-application for locating trans and \n    intersex-friendly restrooms, including unisex and accessible restrooms. \n    Includes data on the location of restrooms, along with directions, \n    comments, user ratings and amenities. Coverage is global, but data is \n    most comprehensive in the United States.\n    See <https://www.refugerestrooms.org/api/docs/> for full API documentation.",
    "version": "0.3.3",
    "maintainer": "Evan Odell <evanodell91@gmail.com>",
    "author": "Evan Odell [aut, cre] (ORCID: <https://orcid.org/0000-0003-1845-808X>)",
    "url": "https://docs.evanodell.com/refuge",
    "bug_reports": "https://github.com/evanodell/refuge/issues",
    "repository": "https://cran.r-project.org/package=refuge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "refuge Locate Trans and Intersex-Friendly Toilets Access the 'Refuge' API, a web-application for locating trans and \n    intersex-friendly restrooms, including unisex and accessible restrooms. \n    Includes data on the location of restrooms, along with directions, \n    comments, user ratings and amenities. Coverage is global, but data is \n    most comprehensive in the United States.\n    See <https://www.refugerestrooms.org/api/docs/> for full API documentation.  "
  },
  {
    "id": 19279,
    "package_name": "reghelper",
    "title": "Helper Functions for Regression Analysis",
    "description": "A set of functions used to automate commonly used methods in\n    regression analysis. This includes plotting interactions, and calculating\n    simple slopes, standardized coefficients, regions of significance\n    (Johnson & Neyman, 1936; cf. Spiller et al., 2012), etc. See the reghelper\n    documentation for more information, documentation, and examples.",
    "version": "1.1.2",
    "maintainer": "Jeffrey Hughes <jeff.hughes@gmail.com>",
    "author": "Jeffrey Hughes [aut, cre],\n  David Beiner [aut]",
    "url": "https://github.com/jeff-hughes/reghelper",
    "bug_reports": "https://github.com/jeff-hughes/reghelper/issues",
    "repository": "https://cran.r-project.org/package=reghelper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reghelper Helper Functions for Regression Analysis A set of functions used to automate commonly used methods in\n    regression analysis. This includes plotting interactions, and calculating\n    simple slopes, standardized coefficients, regions of significance\n    (Johnson & Neyman, 1936; cf. Spiller et al., 2012), etc. See the reghelper\n    documentation for more information, documentation, and examples.  "
  },
  {
    "id": 19292,
    "package_name": "regrap",
    "title": "Reverse Graphical Approaches",
    "description": "The graphical approach is proposed as a general framework for clinical trial designs involving multiple hypotheses, where decisions are made only based on the observed marginal p-values. A reverse graphical approach starts from a set of singleton graphs, and gradually add vertices into graphs until rejection of a set of hypotheses is made. See Gou, J. (2020). Reverse graphical approaches for multiple test procedures. Technical Report. ",
    "version": "1.0.1",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=regrap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "regrap Reverse Graphical Approaches The graphical approach is proposed as a general framework for clinical trial designs involving multiple hypotheses, where decisions are made only based on the observed marginal p-values. A reverse graphical approach starts from a set of singleton graphs, and gradually add vertices into graphs until rejection of a set of hypotheses is made. See Gou, J. (2020). Reverse graphical approaches for multiple test procedures. Technical Report.   "
  },
  {
    "id": 19307,
    "package_name": "reinsureR",
    "title": "Reinsurance Treaties Application",
    "description": "Application of reinsurance treaties to claims portfolios. \n            The package creates a class Claims whose objective is to \n            store claims and premiums, on which different treaties can be applied.\n            A statistical analysis can then be applied to measure the impact of\n            reinsurance, producing a table or graphical output. This package can\n            be used for estimating the impact of reinsurance on several portfolios\n            or for pricing treaties through statistical analysis. Documentation\n            for the implemented methods can be found in \"Reinsurance: Actuarial\n            and Statistical Aspects\" by Hansj\u00f6erg Albrecher, Jan Beirlant,\n            Jozef L. Teugels (2017, ISBN: 978-0-470-77268-3) and \n            \"REINSURANCE: A Basic Guide to Facultative and Treaty Reinsurance\"\n            by Munich Re (2010) <https://www.munichre.com/site/mram/get/documents_E96160999/mram/assetpool.mr_america/PDFs/3_Publications/reinsurance_basic_guide.pdf>.",
    "version": "0.1.0",
    "maintainer": "Arnaud Buzzi <arnaud.buzzi@sia-partners.com>",
    "author": "Arnaud Buzzi",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reinsureR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reinsureR Reinsurance Treaties Application Application of reinsurance treaties to claims portfolios. \n            The package creates a class Claims whose objective is to \n            store claims and premiums, on which different treaties can be applied.\n            A statistical analysis can then be applied to measure the impact of\n            reinsurance, producing a table or graphical output. This package can\n            be used for estimating the impact of reinsurance on several portfolios\n            or for pricing treaties through statistical analysis. Documentation\n            for the implemented methods can be found in \"Reinsurance: Actuarial\n            and Statistical Aspects\" by Hansj\u00f6erg Albrecher, Jan Beirlant,\n            Jozef L. Teugels (2017, ISBN: 978-0-470-77268-3) and \n            \"REINSURANCE: A Basic Guide to Facultative and Treaty Reinsurance\"\n            by Munich Re (2010) <https://www.munichre.com/site/mram/get/documents_E96160999/mram/assetpool.mr_america/PDFs/3_Publications/reinsurance_basic_guide.pdf>.  "
  },
  {
    "id": 19316,
    "package_name": "relevance",
    "title": "Calculate Relevance and Significance Measures",
    "description": "Calculates relevance and significance values for\n  simple models and for many types of regression models.\n  These are introduced in\n  'Stahel, Werner A.' (2021)\n  \"Measuring Significance and Relevance instead of p-values.\"\n  <https://stat.ethz.ch/~stahel/relevance/stahel-relevance2103.pdf>.\n  These notions are also applied to replication studies,\n  as described in the manuscript\n  'Stahel, Werner A.' (2022)\n  \"'Replicability': Terminology, Measuring Success, and Strategy\"\n  available in the documentation.",
    "version": "2.1",
    "maintainer": "Werner A. Stahel <stahel@stat.math.ethz.ch>",
    "author": "Werner A. Stahel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=relevance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "relevance Calculate Relevance and Significance Measures Calculates relevance and significance values for\n  simple models and for many types of regression models.\n  These are introduced in\n  'Stahel, Werner A.' (2021)\n  \"Measuring Significance and Relevance instead of p-values.\"\n  <https://stat.ethz.ch/~stahel/relevance/stahel-relevance2103.pdf>.\n  These notions are also applied to replication studies,\n  as described in the manuscript\n  'Stahel, Werner A.' (2022)\n  \"'Replicability': Terminology, Measuring Success, and Strategy\"\n  available in the documentation.  "
  },
  {
    "id": 19328,
    "package_name": "remedy",
    "title": "'RStudio' Addins to Simplify 'Markdown' Writing",
    "description": "An 'RStudio' addin providing shortcuts for writing in 'Markdown'. This package provides a series of \n             functions that allow the user to be more efficient when using 'Markdown'. For example, you can select \n             a word, and put it in bold or in italics, or change the alignment of elements inside you Rmd. The idea\n             is to map all the functionalities from 'remedy' on keyboard shortcuts, so that it provides an interface \n             close to what you can find in any other text editor. ",
    "version": "0.1.0",
    "maintainer": "Colin Fay <contact@colinfay.me>",
    "author": "Colin Fay [aut, cre] (ORCID: <https://orcid.org/0000-0001-7343-1846>),\n  Jonathan Sidi [aut] (ORCID: <https://orcid.org/0000-0002-4222-1819>),\n  Luke Smith [aut] (author of seasmith/AlignAssign),\n  Jonathan Carroll [ctb] (ORCID: <https://orcid.org/0000-0002-1404-5264>),\n  Andrzej Ole\u015b [ctb] (ORCID: <https://orcid.org/0000-0003-0285-2787>),\n  Daniel Possenriede [ctb] (ORCID:\n    <https://orcid.org/0000-0002-6738-9845>)",
    "url": "https://github.com/ThinkR-open/remedy",
    "bug_reports": "https://github.com/ThinkR-open/remedy/issues",
    "repository": "https://cran.r-project.org/package=remedy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "remedy 'RStudio' Addins to Simplify 'Markdown' Writing An 'RStudio' addin providing shortcuts for writing in 'Markdown'. This package provides a series of \n             functions that allow the user to be more efficient when using 'Markdown'. For example, you can select \n             a word, and put it in bold or in italics, or change the alignment of elements inside you Rmd. The idea\n             is to map all the functionalities from 'remedy' on keyboard shortcuts, so that it provides an interface \n             close to what you can find in any other text editor.   "
  },
  {
    "id": 19332,
    "package_name": "remmy",
    "title": "API Client for 'Lemmy'",
    "description": "An HTTP API client for 'Lemmy'\n    (<https://github.com/LemmyNet/lemmy>) in R. Code and documentation are\n    generated from the official 'JavaScript' client source\n    (<https://github.com/LemmyNet/lemmy-js-client>).",
    "version": "0.1.0",
    "maintainer": "Long Nguyen <long.nguyen@uni-bielefeld.de>",
    "author": "Long Nguyen [aut, cre] (ORCID: <https://orcid.org/0000-0001-8878-7386>)",
    "url": "https://github.com/long39ng/remmy,\nhttps://long39ng.github.io/remmy/",
    "bug_reports": "https://github.com/long39ng/remmy/issues",
    "repository": "https://cran.r-project.org/package=remmy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "remmy API Client for 'Lemmy' An HTTP API client for 'Lemmy'\n    (<https://github.com/LemmyNet/lemmy>) in R. Code and documentation are\n    generated from the official 'JavaScript' client source\n    (<https://github.com/LemmyNet/lemmy-js-client>).  "
  },
  {
    "id": 19335,
    "package_name": "rempsyc",
    "title": "Convenience Functions for Psychology",
    "description": "Make your workflow faster and easier. Easily customizable\n    plots (via 'ggplot2'), nice APA tables (following the style of the\n    *American Psychological Association*) exportable to Word (via\n    'flextable'), easily run statistical tests or check assumptions, and\n    automatize various other tasks.",
    "version": "0.2.0",
    "maintainer": "R\u00e9mi Th\u00e9riault <remi.theriault@mail.mcgill.ca>",
    "author": "R\u00e9mi Th\u00e9riault [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4315-6788>)",
    "url": "https://rempsyc.remi-theriault.com",
    "bug_reports": "https://github.com/rempsyc/rempsyc/issues",
    "repository": "https://cran.r-project.org/package=rempsyc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rempsyc Convenience Functions for Psychology Make your workflow faster and easier. Easily customizable\n    plots (via 'ggplot2'), nice APA tables (following the style of the\n    *American Psychological Association*) exportable to Word (via\n    'flextable'), easily run statistical tests or check assumptions, and\n    automatize various other tasks.  "
  },
  {
    "id": 19344,
    "package_name": "repana",
    "title": "Repeatable Analysis in R",
    "description": "Set of utilities to facilitate the reproduction of analysis in R.\n It allow to make_structure(), clean_structure(), and run and log programs in a\n predefined order to allow secondary files, analysis and reports be constructed in\n an ordered and reproducible form.",
    "version": "2.2.1",
    "maintainer": "John J. Aponte <john.j.aponte@gmail.com>",
    "author": "John J. Aponte [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3014-3673>)",
    "url": "https://github.com/johnaponte/repana,\nhttps://johnaponte.github.io/repana/",
    "bug_reports": "https://github.com/johnaponte/repana/issues",
    "repository": "https://cran.r-project.org/package=repana",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "repana Repeatable Analysis in R Set of utilities to facilitate the reproduction of analysis in R.\n It allow to make_structure(), clean_structure(), and run and log programs in a\n predefined order to allow secondary files, analysis and reports be constructed in\n an ordered and reproducible form.  "
  },
  {
    "id": 19347,
    "package_name": "repello",
    "title": "Reports from Trello in R",
    "description": "Creates reports from Trello, a collaborative, project organization \n             and list-making application. <https://trello.com/>\n             Reports are created by comparing individual Trello board\n             cards from two different points in time and documenting any changes made\n             to the cards.",
    "version": "1.0.1",
    "maintainer": "Andrew Guide <andrew.guide@vumc.org>",
    "author": "Andrew Guide [aut, cre],\n  Thomas Stewart [aut]",
    "url": "https://github.com/thomasgstewart/repello",
    "bug_reports": "https://github.com/thomasgstewart/repello/issues",
    "repository": "https://cran.r-project.org/package=repello",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "repello Reports from Trello in R Creates reports from Trello, a collaborative, project organization \n             and list-making application. <https://trello.com/>\n             Reports are created by comparing individual Trello board\n             cards from two different points in time and documenting any changes made\n             to the cards.  "
  },
  {
    "id": 19353,
    "package_name": "repmod",
    "title": "Create Report Table from Different Objects",
    "description": "Tools for generating descriptives and report tables for different models,\n    data.frames and tables and exporting them to different formats.",
    "version": "0.1.7",
    "maintainer": "David Hervas Marin <ddhervas@yahoo.es>",
    "author": "David Hervas Marin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=repmod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "repmod Create Report Table from Different Objects Tools for generating descriptives and report tables for different models,\n    data.frames and tables and exporting them to different formats.  "
  },
  {
    "id": 19355,
    "package_name": "repoRter.nih",
    "title": "R Interface to the 'NIH RePORTER Project' API",
    "description": "Methods to easily build requests in the non-standard JSON\n    schema required by the National Institute of Health (NIH)'s 'RePORTER\n    Project API' <https://api.reporter.nih.gov/#/Search/post_v2_projects_search>.\n    Also retrieve and process result sets as either a ragged or flattened 'tibble'.",
    "version": "0.1.4",
    "maintainer": "\"Michael Barr, ACAS, MAAA, CPCU\" <mike@bikeactuary.com>",
    "author": "Michael Barr, ACAS, MAAA, CPCU [cre, aut]",
    "url": "https://github.com/bikeactuary/repoRter.nih",
    "bug_reports": "https://github.com/bikeactuary/repoRter.nih/issues",
    "repository": "https://cran.r-project.org/package=repoRter.nih",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "repoRter.nih R Interface to the 'NIH RePORTER Project' API Methods to easily build requests in the non-standard JSON\n    schema required by the National Institute of Health (NIH)'s 'RePORTER\n    Project API' <https://api.reporter.nih.gov/#/Search/post_v2_projects_search>.\n    Also retrieve and process result sets as either a ragged or flattened 'tibble'.  "
  },
  {
    "id": 19357,
    "package_name": "reportROC",
    "title": "An Easy Way to Report ROC Analysis",
    "description": "Provides an easy way to report the results of ROC analysis, including:\n    1. an ROC curve. \n    2. the value of Cutoff, AUC (Area Under Curve), ACC (accuracy),\n    SEN (sensitivity), SPE (specificity),\n    PLR (positive likelihood ratio), NLR (negative likelihood ratio),\n    PPV (positive predictive value), NPV (negative predictive value),\n    PPA (percentage of positive accordance), NPA (percentage of negative accordance), TPA (percentage of total accordance),\n    KAPPA (kappa value).",
    "version": "3.6",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reportROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reportROC An Easy Way to Report ROC Analysis Provides an easy way to report the results of ROC analysis, including:\n    1. an ROC curve. \n    2. the value of Cutoff, AUC (Area Under Curve), ACC (accuracy),\n    SEN (sensitivity), SPE (specificity),\n    PLR (positive likelihood ratio), NLR (negative likelihood ratio),\n    PPV (positive predictive value), NPV (negative predictive value),\n    PPA (percentage of positive accordance), NPA (percentage of negative accordance), TPA (percentage of total accordance),\n    KAPPA (kappa value).  "
  },
  {
    "id": 19358,
    "package_name": "reportReg",
    "title": "An Easy Way to Report Regression Analysis",
    "description": "Provides an easy way to report the results of regression analysis, including:\n    1. Proportional hazards regression from function 'coxph' of package 'survival';\n    2. Conditional logistic regression from function 'clogit' of package 'survival';\n    3. Ordered logistic regression from function 'polr' of package 'MASS';\n    4. Binary logistic regression from function 'glm' of package 'stats';\n    5. Linear regression from function 'lm' of package 'stats';\n    6. Risk regression model for survival analysis with competing risks from function 'FGR' of package 'riskRegression';\n    7. Multilevel model from function 'lme' of package 'nlme'.",
    "version": "0.3.0",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reportReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reportReg An Easy Way to Report Regression Analysis Provides an easy way to report the results of regression analysis, including:\n    1. Proportional hazards regression from function 'coxph' of package 'survival';\n    2. Conditional logistic regression from function 'clogit' of package 'survival';\n    3. Ordered logistic regression from function 'polr' of package 'MASS';\n    4. Binary logistic regression from function 'glm' of package 'stats';\n    5. Linear regression from function 'lm' of package 'stats';\n    6. Risk regression model for survival analysis with competing risks from function 'FGR' of package 'riskRegression';\n    7. Multilevel model from function 'lme' of package 'nlme'.  "
  },
  {
    "id": 19359,
    "package_name": "reportRmd",
    "title": "Tidy Presentation of Clinical Reporting",
    "description": "Streamlined statistical reporting in 'Rmarkdown' environments. \n    Facilitates the automated reporting of descriptive statistics, multiple \n    univariate models, multivariable models and tables combining these outputs. \n    Plotting functions include customisable survival curves, forest plots from \n    logistic and ordinal regression and bivariate comparison plots.",
    "version": "0.1.1",
    "maintainer": "Lisa Avery <lisa.avery@uhn.ca>",
    "author": "Lisa Avery [cre, aut] (ORCID: <https://orcid.org/0000-0002-8431-5143>),\n  Ryan Del Bel [aut],\n  Osvaldo Espin-Garcia [aut],\n  Katherine Lajkosz [aut] (ORCID:\n    <https://orcid.org/0000-0003-3760-5401>),\n  Clarina Ong [aut],\n  Tyler Pittman [aut] (ORCID: <https://orcid.org/0000-0002-5013-6980>),\n  Anna Santiago [aut] (ORCID: <https://orcid.org/0000-0002-0932-2386>),\n  Yanning Wang [ctr],\n  Jessica Weiss [aut],\n  Wei Xu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reportRmd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reportRmd Tidy Presentation of Clinical Reporting Streamlined statistical reporting in 'Rmarkdown' environments. \n    Facilitates the automated reporting of descriptive statistics, multiple \n    univariate models, multivariable models and tables combining these outputs. \n    Plotting functions include customisable survival curves, forest plots from \n    logistic and ordinal regression and bivariate comparison plots.  "
  },
  {
    "id": 19360,
    "package_name": "reporter",
    "title": "Creates Statistical Reports",
    "description": "Contains functions to create regulatory-style statistical reports.\n    Originally designed to create tables, listings, and figures for the \n    pharmaceutical, biotechnology, and medical device industries, these\n    reports are generalized enough that they could be used in any industry.\n    Generates text, rich-text, PDF, HTML, and Microsoft Word file formats.  \n    The package specializes \n    in printing wide and long tables with automatic page wrapping and splitting.  \n    Reports can be produced with a minimum of function calls, and without \n    relying on other table packages.  The package supports titles, footnotes, \n    page header, page footers, spanning headers, page by variables, \n    and automatic page numbering.",
    "version": "1.4.5",
    "maintainer": "David Bosak <dbosak01@gmail.com>",
    "author": "David Bosak [aut, cre],\n  Bill Huang [aut],\n  Kevin Kramer [ctb],\n  Duong Tran [ctb],\n  Raphael Huang [ctb],\n  Archytas Clinical Solutions [cph]",
    "url": "https://reporter.r-sassy.org, https://github.com/dbosak01/reporter",
    "bug_reports": "https://github.com/dbosak01/reporter/issues",
    "repository": "https://cran.r-project.org/package=reporter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reporter Creates Statistical Reports Contains functions to create regulatory-style statistical reports.\n    Originally designed to create tables, listings, and figures for the \n    pharmaceutical, biotechnology, and medical device industries, these\n    reports are generalized enough that they could be used in any industry.\n    Generates text, rich-text, PDF, HTML, and Microsoft Word file formats.  \n    The package specializes \n    in printing wide and long tables with automatic page wrapping and splitting.  \n    Reports can be produced with a minimum of function calls, and without \n    relying on other table packages.  The package supports titles, footnotes, \n    page header, page footers, spanning headers, page by variables, \n    and automatic page numbering.  "
  },
  {
    "id": 19361,
    "package_name": "reportr",
    "title": "A General Message and Error Reporting System",
    "description": "Provides a system for reporting messages, which provides certain useful features over the standard R system, such as the incorporation of output consolidation, message filtering, assertions, expression substitution, automatic generation of stack traces for debugging, and conditional reporting based on the current \"output level\".",
    "version": "1.3.1",
    "maintainer": "Jon Clayden <code@clayden.org>",
    "author": "Jon Clayden [aut, cre] (ORCID: <https://orcid.org/0000-0002-6608-0619>)",
    "url": "https://github.com/jonclayden/reportr",
    "bug_reports": "https://github.com/jonclayden/reportr/issues",
    "repository": "https://cran.r-project.org/package=reportr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reportr A General Message and Error Reporting System Provides a system for reporting messages, which provides certain useful features over the standard R system, such as the incorporation of output consolidation, message filtering, assertions, expression substitution, automatic generation of stack traces for debugging, and conditional reporting based on the current \"output level\".  "
  },
  {
    "id": 19362,
    "package_name": "reporttools",
    "title": "Generate \"LaTeX\"\" Tables of Descriptive Statistics",
    "description": "These functions are especially helpful when writing reports of data analysis using \"Sweave\".",
    "version": "1.1.4",
    "maintainer": "Kaspar Rufibach <kaspar.rufibach@gmail.com>",
    "author": "Kaspar Rufibach [aut, cre]",
    "url": "http://www.kasparrufibach.ch",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reporttools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reporttools Generate \"LaTeX\"\" Tables of Descriptive Statistics These functions are especially helpful when writing reports of data analysis using \"Sweave\".  "
  },
  {
    "id": 19366,
    "package_name": "represtools",
    "title": "Reproducible Research Tools",
    "description": "Reproducible research tools automates the creation of an analysis directory structure and work flow. There are R markdown\n  skeletons which encapsulate typical analytic work flow steps. Functions will create appropriate modules which may\n  pass data from one step to another.",
    "version": "0.1.3",
    "maintainer": "Brian Fannin <captain@pirategrunt.com>",
    "author": "Brian Fannin [aut, cre]",
    "url": "https://pirategrunt.com/represtools/",
    "bug_reports": "https://github.com/PirateGrunt/represtools/issues",
    "repository": "https://cran.r-project.org/package=represtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "represtools Reproducible Research Tools Reproducible research tools automates the creation of an analysis directory structure and work flow. There are R markdown\n  skeletons which encapsulate typical analytic work flow steps. Functions will create appropriate modules which may\n  pass data from one step to another.  "
  },
  {
    "id": 19369,
    "package_name": "reproducibleRchunks",
    "title": "Automated Reproducibility Checks for R Markdown Documents",
    "description": "Provide reproducible R chunks in R Markdown document that automatically check computational results for reproducibility. This is achieved by creating json files storing metadata about computational results. A comprehensive tutorial to the package is available as preprint by Brandmaier & Peikert (2024, <doi:10.31234/osf.io/3zjvf>).",
    "version": "1.2.0",
    "maintainer": "Andreas M. Brandmaier <andreas.brandmaier@medicalschool-berlin.de>",
    "author": "Andreas M. Brandmaier [aut, cre],\n  Aaron Peikert [ctb]",
    "url": "",
    "bug_reports": "https://github.com/brandmaier/reproducibleRchunks/issues",
    "repository": "https://cran.r-project.org/package=reproducibleRchunks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reproducibleRchunks Automated Reproducibility Checks for R Markdown Documents Provide reproducible R chunks in R Markdown document that automatically check computational results for reproducibility. This is achieved by creating json files storing metadata about computational results. A comprehensive tutorial to the package is available as preprint by Brandmaier & Peikert (2024, <doi:10.31234/osf.io/3zjvf>).  "
  },
  {
    "id": 19371,
    "package_name": "repsd",
    "title": "Root Expected Proportion Squared Difference for Detecting DIF",
    "description": "Root Expected Proportion Squared Difference (REPSD) is a nonparametric\n  differential item functioning (DIF) method that (a) allows practitioners \n  to explore for DIF related to small, fine-grained focal groups of examinees, \n  and (b) compares the focal group directly to the composite group that will be \n  used to develop the reported test score scale. Using your provided response \n  matrix with a column that identifies focal group membership, this package\n  provides the REPSD values, a simulated null distribution of possible REPSD\n  values, and the simulated p-values identifying items possibly displaying DIF \n  without requiring enormous sample sizes.",
    "version": "1.0.1",
    "maintainer": "Anthony William Raborn <anthony.w.raborn@gmail.com>",
    "author": "Anne Corrine Huggins-Manley [aut],\n  Anthony William Raborn [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8083-4739>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=repsd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "repsd Root Expected Proportion Squared Difference for Detecting DIF Root Expected Proportion Squared Difference (REPSD) is a nonparametric\n  differential item functioning (DIF) method that (a) allows practitioners \n  to explore for DIF related to small, fine-grained focal groups of examinees, \n  and (b) compares the focal group directly to the composite group that will be \n  used to develop the reported test score scale. Using your provided response \n  matrix with a column that identifies focal group membership, this package\n  provides the REPSD values, a simulated null distribution of possible REPSD\n  values, and the simulated p-values identifying items possibly displaying DIF \n  without requiring enormous sample sizes.  "
  },
  {
    "id": 19389,
    "package_name": "reservr",
    "title": "Fit Distributions and Neural Networks to Censored and Truncated\nData",
    "description": "Define distribution families and fit them to\n    interval-censored and interval-truncated data, where the truncation\n    bounds may depend on the individual observation. The defined\n    distributions feature density, probability, sampling and fitting\n    methods as well as efficient implementations of the log-density log\n    f(x) and log-probability log P(x0 <= X <= x1) for use in 'TensorFlow'\n    neural networks via the 'tensorflow' package. Allows training\n    parametric neural networks on interval-censored and interval-truncated\n    data with flexible parameterization. Applications include Claims\n    Development in Non-Life Insurance, e.g. modelling reporting delay\n    distributions from incomplete data, see B\u00fccher, Rosenstock (2022)\n    <doi:10.1007/s13385-022-00314-4>.",
    "version": "0.0.3",
    "maintainer": "Alexander Rosenstock <alexander.rosenstock@web.de>",
    "author": "Alexander Rosenstock [aut, cre, cph]",
    "url": "https://ashesitr.github.io/reservr/,\nhttps://github.com/AshesITR/reservr",
    "bug_reports": "https://github.com/AshesITR/reservr/issues",
    "repository": "https://cran.r-project.org/package=reservr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reservr Fit Distributions and Neural Networks to Censored and Truncated\nData Define distribution families and fit them to\n    interval-censored and interval-truncated data, where the truncation\n    bounds may depend on the individual observation. The defined\n    distributions feature density, probability, sampling and fitting\n    methods as well as efficient implementations of the log-density log\n    f(x) and log-probability log P(x0 <= X <= x1) for use in 'TensorFlow'\n    neural networks via the 'tensorflow' package. Allows training\n    parametric neural networks on interval-censored and interval-truncated\n    data with flexible parameterization. Applications include Claims\n    Development in Non-Life Insurance, e.g. modelling reporting delay\n    distributions from incomplete data, see B\u00fccher, Rosenstock (2022)\n    <doi:10.1007/s13385-022-00314-4>.  "
  },
  {
    "id": 19399,
    "package_name": "respR",
    "title": "Import, Process, Analyse, and Calculate Rates from Respirometry\nData",
    "description": "Provides a structural, reproducible workflow for the\n    processing and analysis of respirometry data. It contains analytical\n    functions and utilities for working with oxygen time-series to determine\n    respiration or oxygen production rates, and to make it easier to report and\n    share analyses. See Harianto et al. 2019 <doi:10.1111/2041-210X.13162>.",
    "version": "2.3.4",
    "maintainer": "Nicholas Carey <nicholascarey@gmail.com>",
    "author": "Nicholas Carey [aut, cre],\n  Januar Harianto [aut]",
    "url": "https://github.com/januarharianto/respr,\nhttps://januarharianto.github.io/respR/,\nhttps://doi.org/10.1111/2041-210X.13162",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=respR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "respR Import, Process, Analyse, and Calculate Rates from Respirometry\nData Provides a structural, reproducible workflow for the\n    processing and analysis of respirometry data. It contains analytical\n    functions and utilities for working with oxygen time-series to determine\n    respiration or oxygen production rates, and to make it easier to report and\n    share analyses. See Harianto et al. 2019 <doi:10.1111/2041-210X.13162>.  "
  },
  {
    "id": 19418,
    "package_name": "retmort",
    "title": "Estimate User-Based Tagging Mortality and Tag Loss in\nMark-Recapture Studies",
    "description": "We provide several avenues to predict and account for user-based mortality and tag loss during mark-recapture studies. When planning a study on a target species, the retentionmort_generation() function can be used to produce multiple synthetic mark-recapture datasets to anticipate the error associated with a planned field study to guide method development to reduce error. Similarly, if field data was already collected, the retentionmort() function can be used to predict the error from already generated data to adjust for user-based mortality and tag loss. The test_dataset_retentionmort() function will provide an example dataset of how data should be inputted into the function to run properly. Lastly, the retentionmort_figure() function can be used on any dataset generated from either model function to produce an 'rmarkdown' printout of preliminary analysis associated with the model, including summary statistics and figures. Methods and results pertaining to the formation of this package can be found in McCutcheon et al. (in review, \"Predicting tagging-related mortality and tag loss during mark-recapture studies\").",
    "version": "1.0.0",
    "maintainer": "Brendan Campbell <bpc@udel.edu>",
    "author": "Brendan Campbell [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5604-7658>),\n  Jasper McCutcheon [aut] (ORCID:\n    <https://orcid.org/0009-0004-4138-532X>),\n  Rileigh Hudock [aut] (ORCID: <https://orcid.org/0009-0009-2769-2880>),\n  Noah Motz [aut] (ORCID: <https://orcid.org/0009-0002-2480-9712>),\n  Madison Windsor [aut] (ORCID: <https://orcid.org/0009-0000-4517-0677>),\n  Aaron Carlisle [aut] (ORCID: <https://orcid.org/0000-0003-0796-6564>),\n  Edward Hale [aut] (ORCID: <https://orcid.org/0000-0002-7176-6420>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=retmort",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "retmort Estimate User-Based Tagging Mortality and Tag Loss in\nMark-Recapture Studies We provide several avenues to predict and account for user-based mortality and tag loss during mark-recapture studies. When planning a study on a target species, the retentionmort_generation() function can be used to produce multiple synthetic mark-recapture datasets to anticipate the error associated with a planned field study to guide method development to reduce error. Similarly, if field data was already collected, the retentionmort() function can be used to predict the error from already generated data to adjust for user-based mortality and tag loss. The test_dataset_retentionmort() function will provide an example dataset of how data should be inputted into the function to run properly. Lastly, the retentionmort_figure() function can be used on any dataset generated from either model function to produce an 'rmarkdown' printout of preliminary analysis associated with the model, including summary statistics and figures. Methods and results pertaining to the formation of this package can be found in McCutcheon et al. (in review, \"Predicting tagging-related mortality and tag loss during mark-recapture studies\").  "
  },
  {
    "id": 19421,
    "package_name": "retroharmonize",
    "title": "Ex Post Survey Data Harmonization",
    "description": "Assist in reproducible retrospective (ex-post) harmonization\n    of data, particularly individual level survey data, by providing tools\n    for organizing metadata, standardizing the coding of variables, and\n    variable names and value labels, including missing values, and\n    documenting the data transformations, with the help of comprehensive\n    s3 classes.",
    "version": "0.2.0",
    "maintainer": "Daniel Antal <daniel.antal@ceemid.eu>",
    "author": "Daniel Antal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7513-6760>),\n  Marta Kolczynska [ctb] (ORCID: <https://orcid.org/0000-0003-4981-0437>),\n  Pyry Kantanen [ctb] (ORCID: <https://orcid.org/0000-0003-2853-2765>),\n  Diego Hernang\u00f3mez Herrero [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>)",
    "url": "https://retroharmonize.dataobservatory.eu/,\nhttps://ropengov.github.io/retroharmonize/,\nhttps://github.com/rOpenGov/retroharmonize",
    "bug_reports": "https://github.com/rOpenGov/retroharmonize/issues",
    "repository": "https://cran.r-project.org/package=retroharmonize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "retroharmonize Ex Post Survey Data Harmonization Assist in reproducible retrospective (ex-post) harmonization\n    of data, particularly individual level survey data, by providing tools\n    for organizing metadata, standardizing the coding of variables, and\n    variable names and value labels, including missing values, and\n    documenting the data transformations, with the help of comprehensive\n    s3 classes.  "
  },
  {
    "id": 19427,
    "package_name": "reveneraR",
    "title": "Connect to Your 'Revenera' (Formerly 'Revulytics') Data",
    "description": "Facilitates making a connection to the 'Revenera' \n  API and executing various queries. You can use it to get \n  event data and metadata. The 'Revenera' documentation \n  is available at <https://rui-api.redoc.ly/>. This package is \n  not supported by 'Flexera' (owner of the software). ",
    "version": "1.0.1",
    "maintainer": "Chris Umphlett <christopher.umphlett@gmail.com>",
    "author": "Chris Umphlett [aut, cre],\n  Avinash Panigrahi [aut]",
    "url": "https://github.com/chrisumphlett/reveneraR",
    "bug_reports": "https://github.com/chrisumphlett/reveneraR/issues",
    "repository": "https://cran.r-project.org/package=reveneraR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reveneraR Connect to Your 'Revenera' (Formerly 'Revulytics') Data Facilitates making a connection to the 'Revenera' \n  API and executing various queries. You can use it to get \n  event data and metadata. The 'Revenera' documentation \n  is available at <https://rui-api.redoc.ly/>. This package is \n  not supported by 'Flexera' (owner of the software).   "
  },
  {
    "id": 19428,
    "package_name": "revengc",
    "title": "Reverse Engineering Summarized Data",
    "description": "Decoupled (e.g. separate averages) and censored (e.g. > 100 species) variables are continually reported by many well-established organizations (e.g. World Health Organization (WHO), Centers for Disease Control and Prevention (CDC), World Bank, and various national censuses).  The challenge therefore is to infer what the original data could have been given summarized information.  We present an R package that reverse engineers decoupled and/or censored count data with two main functions.  The cnbinom.pars function estimates the average and dispersion parameter of a censored univariate frequency table.  The rec function reverse engineers summarized data into an uncensored bivariate table of probabilities.",
    "version": "1.0.4",
    "maintainer": "Samantha Duchscherer <sam.duchscherer@gmail.com>",
    "author": "Samantha Duchscherer [aut, cre],\n  UT-Battelle, LLC [cph]",
    "url": "https://github.com/GIST-ORNL/revengc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=revengc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "revengc Reverse Engineering Summarized Data Decoupled (e.g. separate averages) and censored (e.g. > 100 species) variables are continually reported by many well-established organizations (e.g. World Health Organization (WHO), Centers for Disease Control and Prevention (CDC), World Bank, and various national censuses).  The challenge therefore is to infer what the original data could have been given summarized information.  We present an R package that reverse engineers decoupled and/or censored count data with two main functions.  The cnbinom.pars function estimates the average and dispersion parameter of a censored univariate frequency table.  The rec function reverse engineers summarized data into an uncensored bivariate table of probabilities.  "
  },
  {
    "id": 19430,
    "package_name": "revise",
    "title": "Dynamic Revision Letters for 'Rmarkdown' Manuscripts",
    "description": "Extracts tagged text from markdown manuscripts for inclusion in dynamically generated revision letters. Provides an R markdown template based on papaja::revision_letter_pdf() with comment cross-referencing, a system for managing multiple sections of extracted text, and a way to automatically determine the page number of quoted sections from PDF manuscripts.",
    "version": "0.1.0",
    "maintainer": "James Conigrave <james.conigrave@gmail.com>",
    "author": "James Conigrave [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8816-6229>),\n  Philip Parker [aut] (ORCID: <https://orcid.org/0000-0002-4604-8566>),\n  Taren Sanders [aut] (ORCID: <https://orcid.org/0000-0002-4504-6008>),\n  Michael Noetel [aut] (ORCID: <https://orcid.org/0000-0002-6563-8203>),\n  Caspar J. Van Lissa [aut] (ORCID:\n    <https://orcid.org/0000-0002-0808-5024>)",
    "url": "https://github.com/conig/revise",
    "bug_reports": "https://github.com/conig/revise/issues",
    "repository": "https://cran.r-project.org/package=revise",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "revise Dynamic Revision Letters for 'Rmarkdown' Manuscripts Extracts tagged text from markdown manuscripts for inclusion in dynamically generated revision letters. Provides an R markdown template based on papaja::revision_letter_pdf() with comment cross-referencing, a system for managing multiple sections of extracted text, and a way to automatically determine the page number of quoted sections from PDF manuscripts.  "
  },
  {
    "id": 19434,
    "package_name": "revulyticsR",
    "title": "Connect to Your 'Revulytics' Data",
    "description": "Facilitates making a connection to the \n  'Revulytics' API and executing various queries. You can use it to\n  get event data and metadata. The Revulytics documentation \n  is available at <https://docs.revenera.com/ui560/report/>. This\n  package is not supported by 'Flexera' (owner of the software). ",
    "version": "0.0.3",
    "maintainer": "Chris Umphlett <christopher.umphlett@gmail.com>",
    "author": "Chris Umphlett [aut, cre],\n  Avinash Panigrahi [aut]",
    "url": "https://github.com/chrisumphlett/revulyticsR",
    "bug_reports": "https://github.com/chrisumphlett/revulyticsR/issues",
    "repository": "https://cran.r-project.org/package=revulyticsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "revulyticsR Connect to Your 'Revulytics' Data Facilitates making a connection to the \n  'Revulytics' API and executing various queries. You can use it to\n  get event data and metadata. The Revulytics documentation \n  is available at <https://docs.revenera.com/ui560/report/>. This\n  package is not supported by 'Flexera' (owner of the software).   "
  },
  {
    "id": 19436,
    "package_name": "rexer",
    "title": "Random Exercises and Exams Generator",
    "description": "The main purpose of this package is to streamline the\n    generation of exams that include random elements in exercises.\n    Exercises can be defined in a table, based on text and figures, and\n    may contain gaps to be filled with provided options. Exam documents\n    can be generated in various formats. It allows us to generate a\n    version for conducting the assessment and another version that\n    facilitates correction, linked through a code.",
    "version": "1.0.0",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/rexer/,\nhttps://github.com/josesamos/rexer",
    "bug_reports": "https://github.com/josesamos/rexer/issues",
    "repository": "https://cran.r-project.org/package=rexer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rexer Random Exercises and Exams Generator The main purpose of this package is to streamline the\n    generation of exams that include random elements in exercises.\n    Exercises can be defined in a table, based on text and figures, and\n    may contain gaps to be filled with provided options. Exam documents\n    can be generated in various formats. It allows us to generate a\n    version for conducting the assessment and another version that\n    facilitates correction, linked through a code.  "
  },
  {
    "id": 19441,
    "package_name": "rfacebookstat",
    "title": "Load Data from Facebook API Marketing",
    "description": "Load data by campaigns, ads, ad sets and insights, ad account and business manager \n    from Facebook Marketing API into R. For more details see official documents by Facebook \n    Marketing API <https://developers.facebook.com/docs/marketing-api>.",
    "version": "2.13.1",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0410-7385>)",
    "url": "https://selesnow.github.io/rfacebookstat/,\nhttps://www.youtube.com/playlist?list=PLD2LDq8edf4pItOb-vZTG5AXZK2niJ8_R",
    "bug_reports": "https://github.com/selesnow/rfacebookstat/issues",
    "repository": "https://cran.r-project.org/package=rfacebookstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rfacebookstat Load Data from Facebook API Marketing Load data by campaigns, ads, ad sets and insights, ad account and business manager \n    from Facebook Marketing API into R. For more details see official documents by Facebook \n    Marketing API <https://developers.facebook.com/docs/marketing-api>.  "
  },
  {
    "id": 19460,
    "package_name": "rgeedim",
    "title": "Search, Composite, and Download 'Google Earth Engine' Imagery\nwith the 'Python' Module 'geedim'",
    "description": "Search, composite, and download 'Google Earth Engine' imagery with 'reticulate' bindings for the 'Python' module 'geedim' by Dugal Harris. Read the 'geedim' documentation here: <https://geedim.readthedocs.io/>.\n    Wrapper functions are provided to make it more convenient to use 'geedim' to download images larger than the 'Google Earth Engine' size limit <https://developers.google.com/earth-engine/apidocs/ee-image-getdownloadurl>.\n    By default the \"High Volume\" API endpoint <https://developers.google.com/earth-engine/cloud/highvolume> is used to download data and this URL can be customized during initialization of the package.",
    "version": "0.2.7",
    "maintainer": "Andrew Brown <brown.andrewg@gmail.com>",
    "author": "Andrew Brown [aut, cre],\n  Dugal Harris [cph] ('geedim' 'Python' module)",
    "url": "https://humus.rocks/rgeedim/, https://github.com/brownag/rgeedim,\nhttps://geedim.readthedocs.io/",
    "bug_reports": "https://github.com/brownag/rgeedim/issues",
    "repository": "https://cran.r-project.org/package=rgeedim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgeedim Search, Composite, and Download 'Google Earth Engine' Imagery\nwith the 'Python' Module 'geedim' Search, composite, and download 'Google Earth Engine' imagery with 'reticulate' bindings for the 'Python' module 'geedim' by Dugal Harris. Read the 'geedim' documentation here: <https://geedim.readthedocs.io/>.\n    Wrapper functions are provided to make it more convenient to use 'geedim' to download images larger than the 'Google Earth Engine' size limit <https://developers.google.com/earth-engine/apidocs/ee-image-getdownloadurl>.\n    By default the \"High Volume\" API endpoint <https://developers.google.com/earth-engine/cloud/highvolume> is used to download data and this URL can be customized during initialization of the package.  "
  },
  {
    "id": 19464,
    "package_name": "rgeoda",
    "title": "R Library for Spatial Data Analysis",
    "description": "Provides spatial data analysis functionalities including Exploratory Spatial Data Analysis, \n    Spatial Cluster Detection and Clustering Analysis, Regionalization, etc. based on the C++ source code \n    of 'GeoDa', which is an open-source software tool that serves as an introduction to spatial data analysis.\n    The 'GeoDa' software and its documentation are available at <https://geodacenter.github.io>.",
    "version": "0.1.0",
    "maintainer": "Xun Li <lixun910@gmail.com>",
    "author": "Xun Li [aut, cre],\n  Luc Anselin [aut]",
    "url": "https://github.com/geodacenter/rgeoda/,\nhttps://geodacenter.github.io/rgeoda/",
    "bug_reports": "https://github.com/geodacenter/rgeoda/issues/",
    "repository": "https://cran.r-project.org/package=rgeoda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgeoda R Library for Spatial Data Analysis Provides spatial data analysis functionalities including Exploratory Spatial Data Analysis, \n    Spatial Cluster Detection and Clustering Analysis, Regionalization, etc. based on the C++ source code \n    of 'GeoDa', which is an open-source software tool that serves as an introduction to spatial data analysis.\n    The 'GeoDa' software and its documentation are available at <https://geodacenter.github.io>.  "
  },
  {
    "id": 19475,
    "package_name": "rgoogleads",
    "title": "Loading Data from 'Google Ads API'",
    "description": "Interface for loading data from 'Google Ads API', \n    see <https://developers.google.com/google-ads/api/docs/start>. \n    Package provide function for authorization and loading reports.",
    "version": "0.13.3",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0410-7385>),\n  Netpeak [cph]",
    "url": "https://selesnow.github.io/rgoogleads/,\nhttps://selesnow.github.io/rgoogleads/docs/,\nhttps://github.com/selesnow/rgoogleads",
    "bug_reports": "https://github.com/selesnow/rgoogleads/issues",
    "repository": "https://cran.r-project.org/package=rgoogleads",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgoogleads Loading Data from 'Google Ads API' Interface for loading data from 'Google Ads API', \n    see <https://developers.google.com/google-ads/api/docs/start>. \n    Package provide function for authorization and loading reports.  "
  },
  {
    "id": 19476,
    "package_name": "rgoogleclassroom",
    "title": "API Wrapper for Google Classroom and Google Forms",
    "description": "This is a Google Forms and Google Classroom API Wrapper for R for managing Google Classrooms from R. The documentation for these APIs is here <https://developers.google.com/forms/api/guides> .",
    "version": "1.0.0",
    "maintainer": "Candace Savonen <csavonen@fredhutch.org>",
    "author": "Candace Savonen [cre, aut]",
    "url": "https://github.com/datatrail-jhu/rgoogleclassroom",
    "bug_reports": "https://github.com/datatrail-jhu/rgoogleclassroom/issues",
    "repository": "https://cran.r-project.org/package=rgoogleclassroom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgoogleclassroom API Wrapper for Google Classroom and Google Forms This is a Google Forms and Google Classroom API Wrapper for R for managing Google Classrooms from R. The documentation for these APIs is here <https://developers.google.com/forms/api/guides> .  "
  },
  {
    "id": 19481,
    "package_name": "rgtmx",
    "title": "Manage GTmetrix Tests in R",
    "description": "This is a library to access the current API of the web speed test service 'GTmetrix'.\n    It provides a convenient wrapper to start tests, get reports, and access all kinds of meta data. \n    For more information about using the API please visit <https://gtmetrix.com/api/docs/2.0/>.",
    "version": "0.1.4",
    "maintainer": "Roman A. Abashin <roman@nougat.ai>",
    "author": "Roman A. Abashin [cre, aut]",
    "url": "https://github.com/RomanAbashin/rgtmx",
    "bug_reports": "https://github.com/RomanAbashin/rgtmx/issues",
    "repository": "https://cran.r-project.org/package=rgtmx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgtmx Manage GTmetrix Tests in R This is a library to access the current API of the web speed test service 'GTmetrix'.\n    It provides a convenient wrapper to start tests, get reports, and access all kinds of meta data. \n    For more information about using the API please visit <https://gtmetrix.com/api/docs/2.0/>.  "
  },
  {
    "id": 19515,
    "package_name": "rim",
    "title": "Interface to 'Maxima', Enabling Symbolic Computation",
    "description": "An interface to the powerful and fairly complete computer algebra system 'Maxima'.\n    It can be used to start and control 'Maxima' from within R by entering 'Maxima' commands. \n    Results from 'Maxima' can be parsed and evaluated in R. \n    It facilitates outputting results from 'Maxima' in 'LaTeX' and 'MathML'. \n    2D and 3D plots can be displayed directly. \n    This package also registers a 'knitr'-engine enabling 'Maxima' code chunks \n    to be written in 'RMarkdown' documents.",
    "version": "0.8.1",
    "maintainer": "Eric Stemmler <stemmler.eric@gmail.com>",
    "author": "Eric Stemmler [aut, cre],\n  Kseniia Shumelchyk [aut],\n  Hans W. Borchers [aut]",
    "url": "https://rcst.github.io/rim/",
    "bug_reports": "https://github.com/rcst/rim/issues",
    "repository": "https://cran.r-project.org/package=rim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rim Interface to 'Maxima', Enabling Symbolic Computation An interface to the powerful and fairly complete computer algebra system 'Maxima'.\n    It can be used to start and control 'Maxima' from within R by entering 'Maxima' commands. \n    Results from 'Maxima' can be parsed and evaluated in R. \n    It facilitates outputting results from 'Maxima' in 'LaTeX' and 'MathML'. \n    2D and 3D plots can be displayed directly. \n    This package also registers a 'knitr'-engine enabling 'Maxima' code chunks \n    to be written in 'RMarkdown' documents.  "
  },
  {
    "id": 19520,
    "package_name": "rintimg",
    "title": "View Images on Full Screen in 'RMarkdown' Documents and 'shiny'\nApplications",
    "description": "Allows the user to view an image in full screen when clicking on it in 'RMarkdown' documents and 'shiny' applications. \n  The package relies on the 'JavaScript' library 'intense-images'. See <https://tholman.com/intense-images/> for more information.",
    "version": "0.1.0",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden [aut, cre],\n  Tim Holman [ctb, cph] (intense-images.js library developer)",
    "url": "https://github.com/feddelegrand7/rintimg",
    "bug_reports": "https://github.com/feddelegrand7/rintimg/issues",
    "repository": "https://cran.r-project.org/package=rintimg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rintimg View Images on Full Screen in 'RMarkdown' Documents and 'shiny'\nApplications Allows the user to view an image in full screen when clicking on it in 'RMarkdown' documents and 'shiny' applications. \n  The package relies on the 'JavaScript' library 'intense-images'. See <https://tholman.com/intense-images/> for more information.  "
  },
  {
    "id": 19529,
    "package_name": "risk.assessr",
    "title": "Assessing Package Risk Metrics",
    "description": "A reliable and validated tool that captures detailed risk metrics \n    such as R 'CMD' check, test coverage, traceability matrix, documentation, dependencies, \n    reverse dependencies, suggested dependency analysis, repository data, \n    and enhanced reporting for R packages that are local or stored \n    on remote repositories such as GitHub, CRAN, and Bioconductor.",
    "version": "3.0.1",
    "maintainer": "Edward Gillian <edward.gillian-ext@sanofi.com>",
    "author": "Edward Gillian [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-2732-5107>),\n  Hugo Bottois [aut] (ORCID: <https://orcid.org/0000-0003-4674-0875>),\n  Paulin Charliquart [aut],\n  Andre Couturier [aut],\n  Sanofi [cph, fnd]",
    "url": "https://sanofi-public.github.io/risk.assessr/",
    "bug_reports": "https://github.com/Sanofi-Public/risk.assessr/issues",
    "repository": "https://cran.r-project.org/package=risk.assessr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "risk.assessr Assessing Package Risk Metrics A reliable and validated tool that captures detailed risk metrics \n    such as R 'CMD' check, test coverage, traceability matrix, documentation, dependencies, \n    reverse dependencies, suggested dependency analysis, repository data, \n    and enhanced reporting for R packages that are local or stored \n    on remote repositories such as GitHub, CRAN, and Bioconductor.  "
  },
  {
    "id": 19531,
    "package_name": "riskParityPortfolio",
    "title": "Design of Risk Parity Portfolios",
    "description": "Fast design of risk parity portfolios for financial investment.\n    The goal of the risk parity portfolio formulation is to equalize or distribute\n    the risk contributions of the different assets, which is missing if we simply\n    consider the overall volatility of the portfolio as in the mean-variance\n    Markowitz portfolio. In addition to the vanilla formulation, where the risk\n    contributions are perfectly equalized subject to no shortselling and budget\n    constraints, many other formulations are considered that allow for box\n    constraints and shortselling, as well as the inclusion of additional\n    objectives like the expected return and overall variance. See vignette for\n    a detailed documentation and comparison, with several illustrative examples.\n    The package is based on the papers:\n    Y. Feng, and D. P. Palomar (2015). SCRIP: Successive Convex Optimization Methods\n    for Risk Parity Portfolio Design. IEEE Trans. on Signal Processing, vol. 63,\n    no. 19, pp. 5285-5300. <doi:10.1109/TSP.2015.2452219>.\n    F. Spinu (2013), An Algorithm for Computing Risk Parity Weights.\n    <doi:10.2139/ssrn.2297383>.\n    T. Griveau-Billion, J. Richard, and T. Roncalli (2013). A fast algorithm for computing\n    High-dimensional risk parity portfolios. <arXiv:1311.4057>.",
    "version": "0.2.2",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Ze Vinicius [aut],\n  Daniel P. Palomar [cre, aut]",
    "url": "https://CRAN.R-project.org/package=riskParityPortfolio,\nhttps://github.com/dppalomar/riskParityPortfolio,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2015.2452219",
    "bug_reports": "https://github.com/dppalomar/riskParityPortfolio/issues",
    "repository": "https://cran.r-project.org/package=riskParityPortfolio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "riskParityPortfolio Design of Risk Parity Portfolios Fast design of risk parity portfolios for financial investment.\n    The goal of the risk parity portfolio formulation is to equalize or distribute\n    the risk contributions of the different assets, which is missing if we simply\n    consider the overall volatility of the portfolio as in the mean-variance\n    Markowitz portfolio. In addition to the vanilla formulation, where the risk\n    contributions are perfectly equalized subject to no shortselling and budget\n    constraints, many other formulations are considered that allow for box\n    constraints and shortselling, as well as the inclusion of additional\n    objectives like the expected return and overall variance. See vignette for\n    a detailed documentation and comparison, with several illustrative examples.\n    The package is based on the papers:\n    Y. Feng, and D. P. Palomar (2015). SCRIP: Successive Convex Optimization Methods\n    for Risk Parity Portfolio Design. IEEE Trans. on Signal Processing, vol. 63,\n    no. 19, pp. 5285-5300. <doi:10.1109/TSP.2015.2452219>.\n    F. Spinu (2013), An Algorithm for Computing Risk Parity Weights.\n    <doi:10.2139/ssrn.2297383>.\n    T. Griveau-Billion, J. Richard, and T. Roncalli (2013). A fast algorithm for computing\n    High-dimensional risk parity portfolios. <arXiv:1311.4057>.  "
  },
  {
    "id": 19542,
    "package_name": "ritalic",
    "title": "Interface to the ITALIC Database of Lichen Biodiversity",
    "description": "A programmatic interface to the Web Service methods provided by ITALIC (<https://italic.units.it>).\n    ITALIC is a database of lichen data in Italy and bordering European countries. 'ritalic' includes functions for retrieving information\n    about lichen scientific names, geographic distribution, ecological data, morpho-functional traits and identification keys.\n    More information about the data is available at <https://italic.units.it/?procedure=base&t=59&c=60>.\n    The API documentation is available at <https://italic.units.it/?procedure=api>.",
    "version": "0.11.0",
    "maintainer": "Matteo Conti <matt.ciao@gmail.com>",
    "author": "Matteo Conti [aut, cre] (ORCID:\n    <https://orcid.org/0009-0003-4917-2639>),\n  Luana Francesconi [aut],\n  Alice Musina [aut],\n  Luca Di Nuzzo [aut],\n  Gabriele Gheza [aut],\n  Chiara Pistocchi [aut],\n  Juri Nascimbene [aut],\n  Pier Luigi Nimis [aut],\n  Stefano Martellos [aut]",
    "url": "https://github.com/plant-data/ritalic",
    "bug_reports": "https://github.com/plant-data/ritalic/issues",
    "repository": "https://cran.r-project.org/package=ritalic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ritalic Interface to the ITALIC Database of Lichen Biodiversity A programmatic interface to the Web Service methods provided by ITALIC (<https://italic.units.it>).\n    ITALIC is a database of lichen data in Italy and bordering European countries. 'ritalic' includes functions for retrieving information\n    about lichen scientific names, geographic distribution, ecological data, morpho-functional traits and identification keys.\n    More information about the data is available at <https://italic.units.it/?procedure=base&t=59&c=60>.\n    The API documentation is available at <https://italic.units.it/?procedure=api>.  "
  },
  {
    "id": 19552,
    "package_name": "rjdmarkdown",
    "title": "'rmarkdown' Extension for Formatted 'RJDemetra' Outputs",
    "description": "Functions to have nice 'rmarkdown' outputs of the \n  seasonal and trading day adjustment models made with 'RJDemetra'.",
    "version": "0.2.2",
    "maintainer": "Alain Quartier-la-Tente <alain.quartier@yahoo.fr>",
    "author": "Alain Quartier-la-Tente [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7890-3857>)",
    "url": "https://github.com/AQLT/rjdmarkdown",
    "bug_reports": "https://github.com/AQLT/rjdmarkdown/issues",
    "repository": "https://cran.r-project.org/package=rjdmarkdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rjdmarkdown 'rmarkdown' Extension for Formatted 'RJDemetra' Outputs Functions to have nice 'rmarkdown' outputs of the \n  seasonal and trading day adjustment models made with 'RJDemetra'.  "
  },
  {
    "id": 19560,
    "package_name": "rjtools",
    "title": "Preparing, Checking, and Submitting Articles to the 'R Journal'",
    "description": "Create an 'R Journal' 'Rmarkdown' template article, that will\n  generate html and pdf versions of your paper. Check that the paper folder \n  has all the required components needed for submission. \n  Examples of 'R Journal' publications can be found at \n  <https://journal.r-project.org>. ",
    "version": "1.0.18.1",
    "maintainer": "Di Cook <dicook@monash.edu>",
    "author": "Mitchell O'Hara-Wild [aut],\n  Stephanie Kobakian [aut],\n  H. Sherry Zhang [aut],\n  Di Cook [aut, cre] (ORCID: <https://orcid.org/0000-0002-3813-7155>),\n  Simon Urbanek [aut],\n  Christophe Dervieux [aut] (ORCID:\n    <https://orcid.org/0000-0003-4474-2498>)",
    "url": "https://github.com/rjournal/rjtools",
    "bug_reports": "https://github.com/rjournal/rjtools/issues",
    "repository": "https://cran.r-project.org/package=rjtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rjtools Preparing, Checking, and Submitting Articles to the 'R Journal' Create an 'R Journal' 'Rmarkdown' template article, that will\n  generate html and pdf versions of your paper. Check that the paper folder \n  has all the required components needed for submission. \n  Examples of 'R Journal' publications can be found at \n  <https://journal.r-project.org>.   "
  },
  {
    "id": 19591,
    "package_name": "rmapzen",
    "title": "Client for 'Mapzen' and Related Map APIs",
    "description": "Provides an interface to 'Mapzen'-based APIs (including \n    geocode.earth, Nextzen, and NYC GeoSearch) for geographic search \n    and geocoding, isochrone calculation, and vector data to draw map tiles. \n    See <https://www.mapzen.com/documentation/> for more information. The original \n    Mapzen has gone out of business, but 'rmapzen' can be set up to work with \n    any provider who implements the Mapzen API. ",
    "version": "0.5.1",
    "maintainer": "Tarak Shah <tarak.shah@gmail.com>",
    "author": "Tarak Shah [aut, cre],\n  Daniel Possenriede [ctb]",
    "url": "https://tarakc02.github.io/rmapzen/",
    "bug_reports": "https://github.com/tarakc02/rmapzen/issues",
    "repository": "https://cran.r-project.org/package=rmapzen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmapzen Client for 'Mapzen' and Related Map APIs Provides an interface to 'Mapzen'-based APIs (including \n    geocode.earth, Nextzen, and NYC GeoSearch) for geographic search \n    and geocoding, isochrone calculation, and vector data to draw map tiles. \n    See <https://www.mapzen.com/documentation/> for more information. The original \n    Mapzen has gone out of business, but 'rmapzen' can be set up to work with \n    any provider who implements the Mapzen API.   "
  },
  {
    "id": 19598,
    "package_name": "rmcorr",
    "title": "Repeated Measures Correlation",
    "description": "Compute the repeated measures correlation, a statistical technique\n    for determining the overall within-individual relationship among paired measures\n    assessed on two or more occasions, first introduced by Bland and Altman (1995).\n    Includes functions for diagnostics, p-value, effect size with confidence\n    interval including optional bootstrapping, as well as graphing. Also includes\n    several example datasets. For more details, see the web documentation \n    <https://lmarusich.github.io/rmcorr/index.html> and the \n    original paper: Bakdash and Marusich (2017) <doi:10.3389/fpsyg.2017.00456>.",
    "version": "0.7.0",
    "maintainer": "Laura R. Marusich <lmarusich@gmail.com>",
    "author": "Jonathan Z. Bakdash [aut] (ORCID:\n    <https://orcid.org/0000-0002-1409-4779>),\n  Laura R. Marusich [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3524-6110>)",
    "url": "https://github.com/lmarusich/rmcorr",
    "bug_reports": "https://github.com/lmarusich/rmcorr/issues",
    "repository": "https://cran.r-project.org/package=rmcorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmcorr Repeated Measures Correlation Compute the repeated measures correlation, a statistical technique\n    for determining the overall within-individual relationship among paired measures\n    assessed on two or more occasions, first introduced by Bland and Altman (1995).\n    Includes functions for diagnostics, p-value, effect size with confidence\n    interval including optional bootstrapping, as well as graphing. Also includes\n    several example datasets. For more details, see the web documentation \n    <https://lmarusich.github.io/rmcorr/index.html> and the \n    original paper: Bakdash and Marusich (2017) <doi:10.3389/fpsyg.2017.00456>.  "
  },
  {
    "id": 19599,
    "package_name": "rmdHelpers",
    "title": "Helper Functions for Rmd Documents",
    "description": "A series of functions to aid in repeated tasks for Rmd documents. All details are to my personal preference, though I am happy to add flexibility if there are use cases I am missing. I will continue updating with new functions as I add utility functions for myself.",
    "version": "1.3.1",
    "maintainer": "Mark Peterson <mark.phillip.peterson@gmail.com>",
    "author": "Mark Peterson",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rmdHelpers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmdHelpers Helper Functions for Rmd Documents A series of functions to aid in repeated tasks for Rmd documents. All details are to my personal preference, though I am happy to add flexibility if there are use cases I am missing. I will continue updating with new functions as I add utility functions for myself.  "
  },
  {
    "id": 19601,
    "package_name": "rmdfiltr",
    "title": "'Lua'-Filters for R Markdown",
    "description": "A collection of 'Lua' filters that extend the functionality\n  of R Markdown templates (e.g., count words or post-process citations).",
    "version": "0.1.5",
    "maintainer": "Frederik Aust <frederik.aust@uni-koeln.de>",
    "author": "Frederik Aust [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4900-788X>),\n  Marius Barth [ctb]",
    "url": "https://github.com/crsh/rmdfiltr",
    "bug_reports": "https://github.com/crsh/rmdfiltr/issues",
    "repository": "https://cran.r-project.org/package=rmdfiltr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmdfiltr 'Lua'-Filters for R Markdown A collection of 'Lua' filters that extend the functionality\n  of R Markdown templates (e.g., count words or post-process citations).  "
  },
  {
    "id": 19602,
    "package_name": "rmdformats",
    "title": "HTML Output Formats and Templates for 'rmarkdown' Documents",
    "description": "HTML formats and templates for 'rmarkdown' documents, with some extra\n    features such as automatic table of contents, lightboxed figures, dynamic\n    crosstab helper.",
    "version": "1.0.4",
    "maintainer": "Julien Barnier <julien.barnier@cnrs.fr>",
    "author": "Julien Barnier [aut, cre]",
    "url": "https://github.com/juba/rmdformats",
    "bug_reports": "https://github.com/juba/rmdformats/issues",
    "repository": "https://cran.r-project.org/package=rmdformats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmdformats HTML Output Formats and Templates for 'rmarkdown' Documents HTML formats and templates for 'rmarkdown' documents, with some extra\n    features such as automatic table of contents, lightboxed figures, dynamic\n    crosstab helper.  "
  },
  {
    "id": 19604,
    "package_name": "rmdpartials",
    "title": "Partial 'rmarkdown' Documents to Prettify your Reports",
    "description": "\n\t\tUse 'rmarkdown' partials, also know as child documents in\n\t\t'knitr', so you can make components for HTML, PDF, and Word documents. \n\t\tThe package provides various helper functions to make certain functions easier. \n\t\tYou may want to use this package, if you want to flexibly summarise objects \n\t\tusing a combination of figures, tables, text, and HTML widgets. \n\t\tUnlike HTML widgets, the output is Markdown and can hence be turn into other\n\t\toutput formats than HTML.",
    "version": "0.5.8",
    "maintainer": "Ruben Arslan <ruben.arslan@gmail.com>",
    "author": "Ruben Arslan [aut, cre],\n  Gjalt-Jorn Peters [aut, ctb]",
    "url": "https://github.com/rubenarslan/rmdpartials",
    "bug_reports": "https://github.com/rubenarslan/rmdpartials/issues",
    "repository": "https://cran.r-project.org/package=rmdpartials",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmdpartials Partial 'rmarkdown' Documents to Prettify your Reports \n\t\tUse 'rmarkdown' partials, also know as child documents in\n\t\t'knitr', so you can make components for HTML, PDF, and Word documents. \n\t\tThe package provides various helper functions to make certain functions easier. \n\t\tYou may want to use this package, if you want to flexibly summarise objects \n\t\tusing a combination of figures, tables, text, and HTML widgets. \n\t\tUnlike HTML widgets, the output is Markdown and can hence be turn into other\n\t\toutput formats than HTML.  "
  },
  {
    "id": 19605,
    "package_name": "rmdplugr",
    "title": "Plugins for R Markdown Formats",
    "description": "Formats for R Markdown that undo modifications by\n    'pandoc' and 'rmarkdown' to original 'latex' templates, such as\n    smaller margins, paragraph spacing, and compact titles. In addition, \n    enhancements such as author blocks with affiliations and\n    headers and footers are introduced. All of this functionality is built \n    around plugins that modify the default 'pandoc' template without relying on \n    custom templates.",
    "version": "0.4.1",
    "maintainer": "Johan Larsson <johanlarsson@outlook.com>",
    "author": "Johan Larsson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4029-5945>)",
    "url": "https://github.com/jolars/rmdplugr,\nhttps://jolars.github.io/rmdplugr/",
    "bug_reports": "https://github.com/jolars/rmdplugr/issues",
    "repository": "https://cran.r-project.org/package=rmdplugr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmdplugr Plugins for R Markdown Formats Formats for R Markdown that undo modifications by\n    'pandoc' and 'rmarkdown' to original 'latex' templates, such as\n    smaller margins, paragraph spacing, and compact titles. In addition, \n    enhancements such as author blocks with affiliations and\n    headers and footers are introduced. All of this functionality is built \n    around plugins that modify the default 'pandoc' template without relying on \n    custom templates.  "
  },
  {
    "id": 19606,
    "package_name": "rmdwc",
    "title": "Count Words and Characters in R Markdown and Jupyter Notebooks",
    "description": "Computes word, character, and non-whitespace character counts in R Markdown documents and Jupyter notebooks, with or without code chunks. Returns results as a data frame.",
    "version": "0.3.1",
    "maintainer": "Sigbert Klinke <sigbert@hu-berlin.de>",
    "author": "Sigbert Klinke [aut, cre]",
    "url": "https://github.com/sigbertklinke/rmdwc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rmdwc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmdwc Count Words and Characters in R Markdown and Jupyter Notebooks Computes word, character, and non-whitespace character counts in R Markdown documents and Jupyter notebooks, with or without code chunks. Returns results as a data frame.  "
  },
  {
    "id": 19615,
    "package_name": "rmlnomogram",
    "title": "Construct Explainable Nomogram for a Machine Learning Model",
    "description": "Construct an explainable nomogram for a machine learning (ML) model to improve availability of an ML prediction model in addition to a computer application, particularly in a situation where a computer, a mobile phone, an internet connection, or the application accessibility are unreliable. This package enables a nomogram creation for any ML prediction models, which is conventionally limited to only a linear/logistic regression model. This nomogram may indicate the explainability value per feature, e.g., the Shapley additive explanation value, for each individual. However, this package only allows a nomogram creation for a model using categorical without or with single numerical predictors. Detailed methodologies and examples are documented in our vignette, available at <https://htmlpreview.github.io/?https://github.com/herdiantrisufriyana/rmlnomogram/blob/master/doc/ml_nomogram_exemplar.html>.",
    "version": "0.1.2",
    "maintainer": "Herdiantri Sufriyana <herdi@nycu.edu.tw>",
    "author": "Herdiantri Sufriyana [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9178-0222>),\n  Emily Chia-Yu Su [aut] (ORCID: <https://orcid.org/0000-0003-4801-5159>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rmlnomogram",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmlnomogram Construct Explainable Nomogram for a Machine Learning Model Construct an explainable nomogram for a machine learning (ML) model to improve availability of an ML prediction model in addition to a computer application, particularly in a situation where a computer, a mobile phone, an internet connection, or the application accessibility are unreliable. This package enables a nomogram creation for any ML prediction models, which is conventionally limited to only a linear/logistic regression model. This nomogram may indicate the explainability value per feature, e.g., the Shapley additive explanation value, for each individual. However, this package only allows a nomogram creation for a model using categorical without or with single numerical predictors. Detailed methodologies and examples are documented in our vignette, available at <https://htmlpreview.github.io/?https://github.com/herdiantrisufriyana/rmlnomogram/blob/master/doc/ml_nomogram_exemplar.html>.  "
  },
  {
    "id": 19622,
    "package_name": "rmsMD",
    "title": "Output Results from 'rms' Models for Medical Journals",
    "description": "Provides streamlined functions for summarising and visualising \n  regression models fitted with the 'rms' package, in the preferred format for medical journals. \n  The 'modelsummary_rms()' function produces concise summaries for linear, logistic, and \n  Cox regression models, including automatic handling of models containing restricted \n  cubic spline (RCS) terms. The resulting summary dataframe can be easily converted into \n  publication-ready documents using the 'flextable' and 'officer' packages. The 'ggrmsMD()' \n  function creates clear and customizable plots ('ggplot2' objects) to visualise RCS terms.",
    "version": "1.0.1",
    "maintainer": "Samuel Tingle <samjamestingle@gmail.com>",
    "author": "Samuel Tingle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5529-7815>),\n  Georgios Kourounis [aut] (ORCID:\n    <https://orcid.org/0000-0002-1051-078X>)",
    "url": "https://rmsmd.github.io/rmsMD/",
    "bug_reports": "https://github.com/rmsMD/rmsMD/issues/",
    "repository": "https://cran.r-project.org/package=rmsMD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmsMD Output Results from 'rms' Models for Medical Journals Provides streamlined functions for summarising and visualising \n  regression models fitted with the 'rms' package, in the preferred format for medical journals. \n  The 'modelsummary_rms()' function produces concise summaries for linear, logistic, and \n  Cox regression models, including automatic handling of models containing restricted \n  cubic spline (RCS) terms. The resulting summary dataframe can be easily converted into \n  publication-ready documents using the 'flextable' and 'officer' packages. The 'ggrmsMD()' \n  function creates clear and customizable plots ('ggplot2' objects) to visualise RCS terms.  "
  },
  {
    "id": 19664,
    "package_name": "robmedExtra",
    "title": "Extra Functionality for (Robust) Mediation Analysis",
    "description": "This companion package extends the package 'robmed' (Alfons, Ates & Groenen, 2022b; <doi:10.18637/jss.v103.i13>) in various ways.  Most notably, it provides a graphical user interface for the robust bootstrap test ROBMED (Alfons, Ates & Groenen, 2022a; <doi:10.1177/1094428121999096>) to make the method more accessible to less proficient 'R' users, as well as functions to export the results as a table in a 'Microsoft Word' or 'Microsoft Powerpoint' document, or as a 'LaTeX' table. Furthermore, the package contains a 'shiny' app to compare various bootstrap procedures for mediation analysis on simulated data.",
    "version": "0.1.1",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "author": "Andreas Alfons [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2513-3788>),\n  Aurore Archimbaud [aut] (ORCID:\n    <https://orcid.org/0000-0002-6511-9091>),\n  Vincent Drenth [ctb] (initial prototype of GUI)",
    "url": "https://github.com/aalfons/robmedExtra",
    "bug_reports": "https://github.com/aalfons/robmedExtra/issues",
    "repository": "https://cran.r-project.org/package=robmedExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "robmedExtra Extra Functionality for (Robust) Mediation Analysis This companion package extends the package 'robmed' (Alfons, Ates & Groenen, 2022b; <doi:10.18637/jss.v103.i13>) in various ways.  Most notably, it provides a graphical user interface for the robust bootstrap test ROBMED (Alfons, Ates & Groenen, 2022a; <doi:10.1177/1094428121999096>) to make the method more accessible to less proficient 'R' users, as well as functions to export the results as a table in a 'Microsoft Word' or 'Microsoft Powerpoint' document, or as a 'LaTeX' table. Furthermore, the package contains a 'shiny' app to compare various bootstrap procedures for mediation analysis on simulated data.  "
  },
  {
    "id": 19669,
    "package_name": "robotoolbox",
    "title": "Client for the 'KoboToolbox' API",
    "description": "Suite of utilities for accessing and manipulating\n    data from the 'KoboToolbox' API. 'KoboToolbox' is a robust\n    platform designed for field data collection in various disciplines.\n    This package aims to simplify the process of fetching and handling\n    data from the API. Detailed documentation for the 'KoboToolbox' API\n    can be found at <https://support.kobotoolbox.org/api.html>.",
    "version": "1.4",
    "maintainer": "Ahmadou Dicko <mail@ahmadoudicko.com>",
    "author": "Ahmadou Dicko [aut, cre, cph],\n  Hisham Galal [ctb]",
    "url": "https://dickoa.gitlab.io/robotoolbox,\nhttps://gitlab.com/dickoa/robotoolbox",
    "bug_reports": "https://gitlab.com/dickoa/robotoolbox/-/issues",
    "repository": "https://cran.r-project.org/package=robotoolbox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "robotoolbox Client for the 'KoboToolbox' API Suite of utilities for accessing and manipulating\n    data from the 'KoboToolbox' API. 'KoboToolbox' is a robust\n    platform designed for field data collection in various disciplines.\n    This package aims to simplify the process of fetching and handling\n    data from the API. Detailed documentation for the 'KoboToolbox' API\n    can be found at <https://support.kobotoolbox.org/api.html>.  "
  },
  {
    "id": 19674,
    "package_name": "robservable",
    "title": "Import an Observable Notebook as HTML Widget",
    "description": "Allows loading and displaying an Observable notebook (online JavaScript  \n    notebooks powered by <https://observablehq.com>) as an HTML Widget in an R \n    session, 'shiny' application or 'rmarkdown' document.",
    "version": "0.2.2",
    "maintainer": "Julien Barnier <julien.barnier@cnrs.fr>",
    "author": "Julien Barnier [aut, cre],\n  Kenton Russell [aut]",
    "url": "https://juba.github.io/robservable/",
    "bug_reports": "https://github.com/juba/robservable/issues",
    "repository": "https://cran.r-project.org/package=robservable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "robservable Import an Observable Notebook as HTML Widget Allows loading and displaying an Observable notebook (online JavaScript  \n    notebooks powered by <https://observablehq.com>) as an HTML Widget in an R \n    session, 'shiny' application or 'rmarkdown' document.  "
  },
  {
    "id": 19706,
    "package_name": "rock",
    "title": "Reproducible Open Coding Kit",
    "description": "The Reproducible Open Coding Kit ('ROCK', and this package, 'rock')\n  was developed to facilitate reproducible and open coding, specifically\n  geared towards qualitative research methods. It was developed to be both\n  human- and machine-readable, in the spirit of MarkDown and 'YAML'. The idea is\n  that this makes it relatively easy to write other functions and packages\n  to process 'ROCK' files. The 'rock' package contains functions for basic\n  coding and analysis, such as collecting and showing coded fragments and\n  prettifying sources, as well as a number of advanced analyses such as the\n  Qualitative Network Approach and Qualitative/Unified Exploration of State\n  Transitions. The 'ROCK' and this 'rock' package are described in the ROCK\n  book (Z\u00f6rg\u0151 & Peters, 2022; <https://rockbook.org>), in Z\u00f6rg\u0151 & Peters\n  (2024) <doi:10.1080/21642850.2022.2119144> and Peters, Z\u00f6rg\u0151 and van der\n  Maas (2022) <doi:10.31234/osf.io/cvf52>, and more information and\n  tutorials are available at <https://rock.science>.",
    "version": "0.9.6",
    "maintainer": "Gjalt-Jorn Peters <rock@opens.science>",
    "author": "Gjalt-Jorn Peters [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0336-9589>),\n  Szilvia Z\u00f6rg\u0151 [aut] (ORCID: <https://orcid.org/0000-0002-6916-2097>)",
    "url": "https://rock.opens.science",
    "bug_reports": "https://codeberg.org/R-packages/rock/issues",
    "repository": "https://cran.r-project.org/package=rock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rock Reproducible Open Coding Kit The Reproducible Open Coding Kit ('ROCK', and this package, 'rock')\n  was developed to facilitate reproducible and open coding, specifically\n  geared towards qualitative research methods. It was developed to be both\n  human- and machine-readable, in the spirit of MarkDown and 'YAML'. The idea is\n  that this makes it relatively easy to write other functions and packages\n  to process 'ROCK' files. The 'rock' package contains functions for basic\n  coding and analysis, such as collecting and showing coded fragments and\n  prettifying sources, as well as a number of advanced analyses such as the\n  Qualitative Network Approach and Qualitative/Unified Exploration of State\n  Transitions. The 'ROCK' and this 'rock' package are described in the ROCK\n  book (Z\u00f6rg\u0151 & Peters, 2022; <https://rockbook.org>), in Z\u00f6rg\u0151 & Peters\n  (2024) <doi:10.1080/21642850.2022.2119144> and Peters, Z\u00f6rg\u0151 and van der\n  Maas (2022) <doi:10.31234/osf.io/cvf52>, and more information and\n  tutorials are available at <https://rock.science>.  "
  },
  {
    "id": 19709,
    "package_name": "rockr",
    "title": "'Rock' R Server Client",
    "description": "Connector to the REST API of a 'Rock' R server, to perform operations \n  on a remote R server session, or administration tasks. See 'Rock' documentation at\n  <https://rockdoc.obiba.org/>.",
    "version": "1.0.0",
    "maintainer": "Yannick Marcon <yannick.marcon@obiba.org>",
    "author": "Yannick Marcon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0138-2023>),\n  OBiBa group [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rockr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rockr 'Rock' R Server Client Connector to the REST API of a 'Rock' R server, to perform operations \n  on a remote R server session, or administration tasks. See 'Rock' documentation at\n  <https://rockdoc.obiba.org/>.  "
  },
  {
    "id": 19712,
    "package_name": "roclang",
    "title": "Functions for Diffusing Function Documentations into 'Roxygen'\nComments",
    "description": "Efficient diffusing of content across function documentations. Sections, parameters or dot parameters are extracted from function documentations and turned into valid Rd character strings, which are ready to diffuse into the 'roxygen' comments of another function by inserting inline code. ",
    "version": "0.2.3",
    "maintainer": "Xiurui Zhu <zxr6@163.com>",
    "author": "Xiurui Zhu [aut, cre]",
    "url": "https://github.com/zhuxr11/roclang",
    "bug_reports": "https://github.com/zhuxr11/roclang/issues",
    "repository": "https://cran.r-project.org/package=roclang",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roclang Functions for Diffusing Function Documentations into 'Roxygen'\nComments Efficient diffusing of content across function documentations. Sections, parameters or dot parameters are extracted from function documentations and turned into valid Rd character strings, which are ready to diffuse into the 'roxygen' comments of another function by inserting inline code.   "
  },
  {
    "id": 19716,
    "package_name": "rocsvm.path",
    "title": "The Entire Solution Paths for ROC-SVM",
    "description": "We develop the entire solution paths for ROC-SVM presented by Rakotomamonjy. The ROC-SVM solution path algorithm greatly facilitates the tuning procedure for regularization parameter, lambda in ROC-SVM by avoiding grid search algorithm which may be computationally too intensive. For more information on the ROC-SVM, see the report in the ROC Analysis in AI workshop(ROCAI-2004) : Hern\u00e0ndez-Orallo, Jos\u00e9, et al. (2004) <doi:10.1145/1046456.1046489>.",
    "version": "0.1.0",
    "maintainer": "Seung Jun Shin <sjshin@korea.ac.kr>",
    "author": "Seung Jun Shin [aut, cre],\n  Do Hyun Kim [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rocsvm.path",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rocsvm.path The Entire Solution Paths for ROC-SVM We develop the entire solution paths for ROC-SVM presented by Rakotomamonjy. The ROC-SVM solution path algorithm greatly facilitates the tuning procedure for regularization parameter, lambda in ROC-SVM by avoiding grid search algorithm which may be computationally too intensive. For more information on the ROC-SVM, see the report in the ROC Analysis in AI workshop(ROCAI-2004) : Hern\u00e0ndez-Orallo, Jos\u00e9, et al. (2004) <doi:10.1145/1046456.1046489>.  "
  },
  {
    "id": 19720,
    "package_name": "roger",
    "title": "Automated Grading of R Scripts",
    "description": "Tools for grading the coding style and documentation of R\n  scripts. This is the R component of Roger the Omni Grader, an\n  automated grading system for computer programming projects based on\n  Unix shell scripts; see <https://gitlab.com/roger-project>. The\n  package also provides an R interface to the shell scripts. Inspired by\n  the lintr package.",
    "version": "1.5-1",
    "maintainer": "Vincent Goulet <vincent.goulet@act.ulaval.ca>",
    "author": "Vincent Goulet [aut, cre],\n  Samuel Fr\u00e9chette [aut],\n  Jean-Christophe Langlois [aut],\n  Jim Hester [ctb]",
    "url": "https://roger-project.gitlab.io",
    "bug_reports": "https://gitlab.com/roger-project/roger-rpkg/-/issues",
    "repository": "https://cran.r-project.org/package=roger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roger Automated Grading of R Scripts Tools for grading the coding style and documentation of R\n  scripts. This is the R component of Roger the Omni Grader, an\n  automated grading system for computer programming projects based on\n  Unix shell scripts; see <https://gitlab.com/roger-project>. The\n  package also provides an R interface to the shell scripts. Inspired by\n  the lintr package.  "
  },
  {
    "id": 19724,
    "package_name": "rollbar",
    "title": "Error Tracking and Logging",
    "description": "Reports errors and messages to Rollbar, the error tracking platform <https://rollbar.com>.",
    "version": "0.1.0",
    "maintainer": "Andrew Kane <andrew@chartkick.com>",
    "author": "Andrew Kane [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rollbar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rollbar Error Tracking and Logging Reports errors and messages to Rollbar, the error tracking platform <https://rollbar.com>.  "
  },
  {
    "id": 19725,
    "package_name": "rolldown",
    "title": "R Markdown Output Formats for Storytelling",
    "description": "R Markdown output formats based on JavaScript libraries such as\n    'Scrollama' (<https://github.com/russellsamora/scrollama>) for storytelling.",
    "version": "0.2",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>,\n    URL: https://yihui.org),\n  Siqi Zhang [ctb],\n  Russell Goldenberg [ctb] (The JS library\n    inst/resources/scrollama/scrollama.min.js)",
    "url": "https://github.com/yihui/rolldown",
    "bug_reports": "https://github.com/yihui/rolldown/issues",
    "repository": "https://cran.r-project.org/package=rolldown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rolldown R Markdown Output Formats for Storytelling R Markdown output formats based on JavaScript libraries such as\n    'Scrollama' (<https://github.com/russellsamora/scrollama>) for storytelling.  "
  },
  {
    "id": 19765,
    "package_name": "roxut",
    "title": "Document Unit Tests Roxygen-Style",
    "description": "Much as 'roxygen2' allows one to document functions in the same file as the function itself, 'roxut'  allows one to write the unit tests in the same file as the function.  Once processed, the unit tests are moved to the appropriate directory.  Currently supports 'testthat' and 'tinytest' frameworks. The 'roxygen2' package provides much of the infrastructure.",
    "version": "0.4.0",
    "maintainer": "Bryan A. Hanson <hanson@depauw.edu>",
    "author": "Bryan A. Hanson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3536-8246>),\n  Claudia Beleites [ctb],\n  Hadley Wickham [aut, cph] (roxygen2 code),\n  Peter Danenberg [aut, cph] (roxygen2 code),\n  G\u00e1bor Cs\u00e1rdi [aut] (roxygen2 code),\n  Manuel Eugster [aut, cph] (roxygen2 code),\n  RStudio [cph] (roxygen2 code)",
    "url": "https://github.com/bryanhanson/roxut",
    "bug_reports": "https://github.com/bryanhanson/roxut/issues",
    "repository": "https://cran.r-project.org/package=roxut",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roxut Document Unit Tests Roxygen-Style Much as 'roxygen2' allows one to document functions in the same file as the function itself, 'roxut'  allows one to write the unit tests in the same file as the function.  Once processed, the unit tests are moved to the appropriate directory.  Currently supports 'testthat' and 'tinytest' frameworks. The 'roxygen2' package provides much of the infrastructure.  "
  },
  {
    "id": 19766,
    "package_name": "roxy.shinylive",
    "title": "A 'roxygen2' Extension for 'Shinylive'",
    "description": "An extension for 'roxygen2' to embed 'Shinylive' applications\n    in the package documentation.",
    "version": "1.0.0",
    "maintainer": "Pawel Rucki <pawel.rucki@roche.com>",
    "author": "Pawel Rucki [aut, cre],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/insightsengineering/roxy.shinylive/",
    "bug_reports": "https://github.com/insightsengineering/roxy.shinylive/issues",
    "repository": "https://cran.r-project.org/package=roxy.shinylive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roxy.shinylive A 'roxygen2' Extension for 'Shinylive' An extension for 'roxygen2' to embed 'Shinylive' applications\n    in the package documentation.  "
  },
  {
    "id": 19768,
    "package_name": "roxylint",
    "title": "Lint 'roxygen2'-Generated Documentation",
    "description": "Provides formatting linting to 'roxygen2' tags. Linters report\n    'roxygen2' tags that do not conform to a standard style. These linters\n    can be a helpful check for building more consistent documentation and \n    to provide reminders about best practices or checks for typos. Default \n    linting suites are provided for common style guides such as the one \n    followed by the 'tidyverse', though custom linters can be registered by \n    other packages or be custom-tailored to a specific package.",
    "version": "0.1.0",
    "maintainer": "Doug Kelkhoff <doug.kelkhoff@gmail.com>",
    "author": "Doug Kelkhoff [aut, cre]",
    "url": "https://github.com/openpharma/roxylint,\nhttps://openpharma.github.io/roxylint/",
    "bug_reports": "https://github.com/openpharma/roxylint/issues",
    "repository": "https://cran.r-project.org/package=roxylint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roxylint Lint 'roxygen2'-Generated Documentation Provides formatting linting to 'roxygen2' tags. Linters report\n    'roxygen2' tags that do not conform to a standard style. These linters\n    can be a helpful check for building more consistent documentation and \n    to provide reminders about best practices or checks for typos. Default \n    linting suites are provided for common style guides such as the one \n    followed by the 'tidyverse', though custom linters can be registered by \n    other packages or be custom-tailored to a specific package.  "
  },
  {
    "id": 19769,
    "package_name": "roxytest",
    "title": "Various Tests with 'roxygen2'",
    "description": "Various tests as 'roxygen2' roclets: e.g. 'testthat' and 'tinytest' tests. \n  Also other static analysis tools as checking parameter documentation consistency and others.",
    "version": "0.0.2",
    "maintainer": "Mikkel Meyer Andersen <mikl@math.aau.dk>",
    "author": "Mikkel Meyer Andersen [aut, cre],\n  Ege Rubak [ctb]",
    "url": "",
    "bug_reports": "https://github.com/mikldk/roxytest/issues",
    "repository": "https://cran.r-project.org/package=roxytest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roxytest Various Tests with 'roxygen2' Various tests as 'roxygen2' roclets: e.g. 'testthat' and 'tinytest' tests. \n  Also other static analysis tools as checking parameter documentation consistency and others.  "
  },
  {
    "id": 19770,
    "package_name": "roxytypes",
    "title": "Typed Parameter Tags for Integration with 'roxygen2'",
    "description": "Provides typed parameter documentation tags for integration\n    with 'roxygen2'. Typed parameter tags provide a consistent interface for\n    annotating expected types for parameters and returned values. Tools for\n    converting from existing styles are also provided to easily adapt projects\n    which implement typed documentation by convention rather than tag. Use the\n    default format or provide your own.",
    "version": "0.1.2",
    "maintainer": "Doug Kelkhoff <doug.kelkhoff@gmail.com>",
    "author": "Doug Kelkhoff [aut, cre]",
    "url": "https://github.com/openpharma/roxytypes,\nhttps://openpharma.github.io/roxytypes/",
    "bug_reports": "https://github.com/openpharma/roxytypes/issues",
    "repository": "https://cran.r-project.org/package=roxytypes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roxytypes Typed Parameter Tags for Integration with 'roxygen2' Provides typed parameter documentation tags for integration\n    with 'roxygen2'. Typed parameter tags provide a consistent interface for\n    annotating expected types for parameters and returned values. Tools for\n    converting from existing styles are also provided to easily adapt projects\n    which implement typed documentation by convention rather than tag. Use the\n    default format or provide your own.  "
  },
  {
    "id": 19784,
    "package_name": "rplanes",
    "title": "Plausibility Analysis of Epidemiological Signals",
    "description": "Provides functionality to prepare data and analyze plausibility of both forecasted and reported epidemiological signals. The functions implement a set of plausibility algorithms that are agnostic to geographic and time resolutions and are calculated independently then presented as a combined score.",
    "version": "0.1.0",
    "maintainer": "VP Nagraj <nagraj@nagraj.net>",
    "author": "VP Nagraj [aut, cre] (ORCID: <https://orcid.org/0000-0003-0060-566X>),\n  Desiree Williams [aut],\n  Amy Benefield [aut]",
    "url": "https://signaturescience.github.io/rplanes/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rplanes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rplanes Plausibility Analysis of Epidemiological Signals Provides functionality to prepare data and analyze plausibility of both forecasted and reported epidemiological signals. The functions implement a set of plausibility algorithms that are agnostic to geographic and time resolutions and are calculated independently then presented as a combined score.  "
  },
  {
    "id": 19785,
    "package_name": "rplec",
    "title": "Placental Epigenetic Clock to Estimate Aging by DNA Methylation",
    "description": "Placental epigenetic clock to estimate aging based on gestational age using DNA methylation levels, so called placental epigenetic clock (PlEC). We developed a PlEC for the 2024 Placental Clock DREAM Challenge (<https://www.synapse.org/Synapse:syn59520082/wiki/628063>). Our PlEC achieved the top performance based on an independent test set. PlEC can be used to identify accelerated/decelerated aging of placenta for understanding placental dysfunction-related conditions, e.g., great obstetrical syndromes including preeclampsia, fetal growth restriction, preterm labor, preterm premature rupture of the membranes, late spontaneous abortion, and placental abruption. Detailed methodologies and examples are documented in our vignette, available at <https://herdiantrisufriyana.github.io/rplec/doc/placental_aging_analysis.html>.",
    "version": "0.1.3",
    "maintainer": "Herdiantri Sufriyana <herdi@nycu.edu.tw>",
    "author": "Herdiantri Sufriyana [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9178-0222>),\n  Emily Chia-Yu Su [aut] (ORCID: <https://orcid.org/0000-0003-4801-5159>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rplec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rplec Placental Epigenetic Clock to Estimate Aging by DNA Methylation Placental epigenetic clock to estimate aging based on gestational age using DNA methylation levels, so called placental epigenetic clock (PlEC). We developed a PlEC for the 2024 Placental Clock DREAM Challenge (<https://www.synapse.org/Synapse:syn59520082/wiki/628063>). Our PlEC achieved the top performance based on an independent test set. PlEC can be used to identify accelerated/decelerated aging of placenta for understanding placental dysfunction-related conditions, e.g., great obstetrical syndromes including preeclampsia, fetal growth restriction, preterm labor, preterm premature rupture of the membranes, late spontaneous abortion, and placental abruption. Detailed methodologies and examples are documented in our vignette, available at <https://herdiantrisufriyana.github.io/rplec/doc/placental_aging_analysis.html>.  "
  },
  {
    "id": 19807,
    "package_name": "rquery",
    "title": "Relational Query Generator for Data Manipulation at Scale",
    "description": "A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.",
    "version": "1.4.99",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/rquery/,\nhttps://winvector.github.io/rquery/",
    "bug_reports": "https://github.com/WinVector/rquery/issues",
    "repository": "https://cran.r-project.org/package=rquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rquery Relational Query Generator for Data Manipulation at Scale A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.  "
  },
  {
    "id": 19828,
    "package_name": "rroad",
    "title": "Road Condition Analysis",
    "description": "Computation of the International Roughness Index (IRI) given a\n    longitudinal road profile. The IRI can be calculated for a single road segment\n    or for a sequence of segments with a fixed length (e. g. 100m). For the latter,\n    an overlap of the segments can be selected. The IRI and likewise the algorithms\n    for its determination are defined in Sayers, Michael W; Gillespie, Thomas D;\n    Queiroz, Cesar A.V. 1986. The International Road Roughness Experiment (IRRE) :\n    establishing correlation and a calibration standard for measurements. World\n    Bank technical paper; no. WTP 45. Washington, DC : The World Bank. (ISBN\n    0-8213-0589-1) available from <http://documents.worldbank.org/curated/en/326081468740204115>.",
    "version": "0.0.5",
    "maintainer": "Viliam Simko <viliam.simko@gmail.com>",
    "author": "Viliam Simko [cre, aut],\n  Kevin Laubis [aut]",
    "url": "http://github.com/vsimko/rroad",
    "bug_reports": "http://github.com/vsimko/rroad/issues",
    "repository": "https://cran.r-project.org/package=rroad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rroad Road Condition Analysis Computation of the International Roughness Index (IRI) given a\n    longitudinal road profile. The IRI can be calculated for a single road segment\n    or for a sequence of segments with a fixed length (e. g. 100m). For the latter,\n    an overlap of the segments can be selected. The IRI and likewise the algorithms\n    for its determination are defined in Sayers, Michael W; Gillespie, Thomas D;\n    Queiroz, Cesar A.V. 1986. The International Road Roughness Experiment (IRRE) :\n    establishing correlation and a calibration standard for measurements. World\n    Bank technical paper; no. WTP 45. Washington, DC : The World Bank. (ISBN\n    0-8213-0589-1) available from <http://documents.worldbank.org/curated/en/326081468740204115>.  "
  },
  {
    "id": 19832,
    "package_name": "rrtable",
    "title": "Reproducible Research with a Table of R Codes",
    "description": "Makes documents containing plots and tables from a table of R codes. \n    Can make \"HTML\", \"pdf('LaTex')\", \"docx('MS Word')\" and \"pptx('MS Powerpoint')\" documents with or without R code.\n    In the package, modularized 'shiny' app codes are provided. These modules are intended for reuse across applications.",
    "version": "0.3.0",
    "maintainer": "Keon-Woong Moon <cardiomoon@gmail.com>",
    "author": "Keon-Woong Moon [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rrtable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rrtable Reproducible Research with a Table of R Codes Makes documents containing plots and tables from a table of R codes. \n    Can make \"HTML\", \"pdf('LaTex')\", \"docx('MS Word')\" and \"pptx('MS Powerpoint')\" documents with or without R code.\n    In the package, modularized 'shiny' app codes are provided. These modules are intended for reuse across applications.  "
  },
  {
    "id": 19840,
    "package_name": "rscorecard",
    "title": "A Method to Download Department of Education College Scorecard\nData",
    "description": "A method to download Department of Education College\n     Scorecard data using the public API\n     <https://collegescorecard.ed.gov/data/data-documentation/>. It is based on\n     the 'dplyr' model of piped commands to select and filter data in a\n     single chained function call.  An API key from the U.S. Department of\n     Education is required.",
    "version": "0.32.0",
    "maintainer": "Benjamin Skinner <ben@btskinner.io>",
    "author": "Benjamin Skinner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0337-7415>)",
    "url": "https://www.btskinner.io/rscorecard/",
    "bug_reports": "https://github.com/btskinner/rscorecard/issues",
    "repository": "https://cran.r-project.org/package=rscorecard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rscorecard A Method to Download Department of Education College Scorecard\nData A method to download Department of Education College\n     Scorecard data using the public API\n     <https://collegescorecard.ed.gov/data/data-documentation/>. It is based on\n     the 'dplyr' model of piped commands to select and filter data in a\n     single chained function call.  An API key from the U.S. Department of\n     Education is required.  "
  },
  {
    "id": 19843,
    "package_name": "rsdmx",
    "title": "Tools for Reading SDMX Data and Metadata",
    "description": "Set of classes and methods to read data and metadata documents\n  exchanged through the Statistical Data and Metadata Exchange (SDMX) framework,\n  currently focusing on the SDMX XML standard format (SDMX-ML).",
    "version": "0.6-5",
    "maintainer": "Emmanuel Blondel <emmanuel.blondel1@gmail.com>",
    "author": "Emmanuel Blondel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5870-5762>),\n  Matthieu Stigler [ctb] (ORCID: <https://orcid.org/0000-0002-6802-4290>),\n  Eric Persson [ctb]",
    "url": "https://github.com/opensdmx/rsdmx, https://sdmx.org",
    "bug_reports": "https://github.com/opensdmx/rsdmx/issues",
    "repository": "https://cran.r-project.org/package=rsdmx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsdmx Tools for Reading SDMX Data and Metadata Set of classes and methods to read data and metadata documents\n  exchanged through the Statistical Data and Metadata Exchange (SDMX) framework,\n  currently focusing on the SDMX XML standard format (SDMX-ML).  "
  },
  {
    "id": 19846,
    "package_name": "rsf",
    "title": "Report of Statistical Findings in 'bookdown'",
    "description": "A report of statistical findings (RSF) project template is\n    generated using a 'bookdown' format. 'YAML' fields can be further\n    customized. Additional helper functions provide extra features to the\n    RSF.",
    "version": "0.3.0",
    "maintainer": "Derek Chiu <dchiu@bccrc.ca>",
    "author": "Derek Chiu [aut, cre]",
    "url": "https://github.com/dchiu911/rsf/, https://dchiu911.github.io/rsf/",
    "bug_reports": "https://github.com/dchiu911/rsf/issues",
    "repository": "https://cran.r-project.org/package=rsf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsf Report of Statistical Findings in 'bookdown' A report of statistical findings (RSF) project template is\n    generated using a 'bookdown' format. 'YAML' fields can be further\n    customized. Additional helper functions provide extra features to the\n    RSF.  "
  },
  {
    "id": 19854,
    "package_name": "rsm",
    "title": "Response-Surface Analysis",
    "description": "Provides functions to generate response-surface designs, \n    fit first- and second-order response-surface models, \n    make surface plots, obtain the path of steepest ascent, \n    and do canonical analysis. A good reference on these methods \n    is Chapter 10 of Wu, C-F J and Hamada, M (2009) \n    \"Experiments: Planning, Analysis, and Parameter Design Optimization\"\n    ISBN 978-0-471-69946-0. An early version of the package is\n    documented in Journal of Statistical Software <doi:10.18637/jss.v032.i07>.",
    "version": "2.10.6",
    "maintainer": "Russell Lenth <russell-lenth@uiowa.edu>",
    "author": "Russell Lenth [aut, cre]",
    "url": "https://github.com/rvlenth/rsm,https://rvlenth.github.io/rsm/",
    "bug_reports": "https://github.com/rvlenth/rsm/issues",
    "repository": "https://cran.r-project.org/package=rsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsm Response-Surface Analysis Provides functions to generate response-surface designs, \n    fit first- and second-order response-surface models, \n    make surface plots, obtain the path of steepest ascent, \n    and do canonical analysis. A good reference on these methods \n    is Chapter 10 of Wu, C-F J and Hamada, M (2009) \n    \"Experiments: Planning, Analysis, and Parameter Design Optimization\"\n    ISBN 978-0-471-69946-0. An early version of the package is\n    documented in Journal of Statistical Software <doi:10.18637/jss.v032.i07>.  "
  },
  {
    "id": 19863,
    "package_name": "rspacer",
    "title": "'RSpace' API Wrapper",
    "description": "Wrapper for the 'RSpace' Electronic Lab Notebook (<https://www.researchspace.com/>)\n    API. This packages provides convenience functions to browse, search, create,\n    and edit your 'RSpace' documents. In addition, it enables filling 'RSpace' templates\n    from R Markdown/Quarto templates or tabular data (e.g., 'Excel' files).\n    This R package is not developed or endorsed by 'Research Space'.",
    "version": "0.3.1",
    "maintainer": "Gerhard Burger <burger.ga@gmail.com>",
    "author": "Gerhard Burger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1062-5576>),\n  Hanneke Leegwater [aut] (ORCID:\n    <https://orcid.org/0000-0001-6003-1544>),\n  Leiden University [cph, fnd] (ROR: <https://ror.org/027bh9e22>)",
    "url": "https://github.com/lacdr/rspacer, https://lacdr.github.io/rspacer/",
    "bug_reports": "https://github.com/lacdr/rspacer/issues",
    "repository": "https://cran.r-project.org/package=rspacer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rspacer 'RSpace' API Wrapper Wrapper for the 'RSpace' Electronic Lab Notebook (<https://www.researchspace.com/>)\n    API. This packages provides convenience functions to browse, search, create,\n    and edit your 'RSpace' documents. In addition, it enables filling 'RSpace' templates\n    from R Markdown/Quarto templates or tabular data (e.g., 'Excel' files).\n    This R package is not developed or endorsed by 'Research Space'.  "
  },
  {
    "id": 19868,
    "package_name": "rsppfp",
    "title": "R's Shortest Path Problem with Forbidden Subpaths",
    "description": "An implementation of functionalities to transform directed graphs that are bound to a set of\n  known forbidden paths. There are several transformations, following the rules provided by Villeneuve \n  and Desaulniers (2005) <doi: 10.1016/j.ejor.2004.01.032>, and Hsu et al. (2009) <doi: 10.1007/978-3-642-03095-6_60>. \n  The resulting graph is generated in a data-frame format. See rsppfp website for more information, \n  documentation an examples.",
    "version": "1.0.4",
    "maintainer": "Melina Vidoni <melinavidoni@santafe-conicet.gov.ar>",
    "author": "Melina Vidoni [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4099-1430>),\n  Aldo Vecchietti [aut]",
    "url": "https://github.com/melvidoni/rsppfp",
    "bug_reports": "https://github.com/melvidoni/rsppfp/issues",
    "repository": "https://cran.r-project.org/package=rsppfp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsppfp R's Shortest Path Problem with Forbidden Subpaths An implementation of functionalities to transform directed graphs that are bound to a set of\n  known forbidden paths. There are several transformations, following the rules provided by Villeneuve \n  and Desaulniers (2005) <doi: 10.1016/j.ejor.2004.01.032>, and Hsu et al. (2009) <doi: 10.1007/978-3-642-03095-6_60>. \n  The resulting graph is generated in a data-frame format. See rsppfp website for more information, \n  documentation an examples.  "
  },
  {
    "id": 19869,
    "package_name": "rsprite2",
    "title": "Identify Distributions that Match Reported Sample Parameters\n(SPRITE)",
    "description": "The SPRITE algorithm creates possible distributions of discrete responses\n    based on reported sample parameters, such as mean, standard deviation and range \n    (Heathers et al., 2018, <doi:10.7287/peerj.preprints.26968v1>). This package implements it, \n    drawing heavily on the code for Nick Brown's 'rSPRITE' Shiny app <https://shiny.ieis.tue.nl/sprite/>. \n    In addition, it supports the modeling of distributions based on multi-item (Likert-type) \n    scales and the use of restrictions on the frequency of particular responses.",
    "version": "0.2.1",
    "maintainer": "Lukas Wallrich <lukas.wallrich@gmail.com>",
    "author": "Lukas Wallrich [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2121-5177>),\n  Aur\u00e9lien Allard [ctb]",
    "url": "https://lukaswallrich.github.io/rsprite2/",
    "bug_reports": "https://github.com/LukasWallrich/rsprite2/issues",
    "repository": "https://cran.r-project.org/package=rsprite2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsprite2 Identify Distributions that Match Reported Sample Parameters\n(SPRITE) The SPRITE algorithm creates possible distributions of discrete responses\n    based on reported sample parameters, such as mean, standard deviation and range \n    (Heathers et al., 2018, <doi:10.7287/peerj.preprints.26968v1>). This package implements it, \n    drawing heavily on the code for Nick Brown's 'rSPRITE' Shiny app <https://shiny.ieis.tue.nl/sprite/>. \n    In addition, it supports the modeling of distributions based on multi-item (Likert-type) \n    scales and the use of restrictions on the frequency of particular responses.  "
  },
  {
    "id": 19873,
    "package_name": "rstackdeque",
    "title": "Persistent Fast Amortized Stack and Queue Data Structures",
    "description": "Provides fast, persistent (side-effect-free) stack, queue and\n    deque (double-ended-queue) data structures. While deques include a superset\n    of functionality provided by queues, in these implementations queues are\n    more efficient in some specialized situations. See the documentation for\n    rstack, rdeque, and rpqueue for details.",
    "version": "1.1.1",
    "maintainer": "Shawn T. O'Neil <shawn.oneil@cgrb.oregonstate.edu>",
    "author": "Shawn T. O'Neil",
    "url": "https://github.com/oneilsh/rstackdeque",
    "bug_reports": "https://github.com/oneilsh/rstackdeque/issues",
    "repository": "https://cran.r-project.org/package=rstackdeque",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rstackdeque Persistent Fast Amortized Stack and Queue Data Structures Provides fast, persistent (side-effect-free) stack, queue and\n    deque (double-ended-queue) data structures. While deques include a superset\n    of functionality provided by queues, in these implementations queues are\n    more efficient in some specialized situations. See the documentation for\n    rstack, rdeque, and rpqueue for details.  "
  },
  {
    "id": 19889,
    "package_name": "rsyncrosim",
    "title": "The R Interface to 'SyncroSim'",
    "description": "'SyncroSim' is a generalized framework for managing scenario-based \n    datasets (<https://syncrosim.com/>). 'rsyncrosim' provides an interface to \n    'SyncroSim'. Simulation models can be added to 'SyncroSim' in order to \n    transform these datasets, taking advantage of general features such as \n    defining scenarios of model inputs, running Monte Carlo simulations, and \n    summarizing model outputs. 'rsyncrosim' requires 'SyncroSim' 2.3.5 or higher \n    (API documentation: <https://docs.syncrosim.com/>).",
    "version": "2.1.9",
    "maintainer": "Katie Birchard <katie.birchard@apexrms.com>",
    "author": "Colin Daniel [aut],\n  Josie Hughes [aut],\n  Valentin Lucet [aut],\n  Alex Embrey [aut],\n  Katie Birchard [aut, cre],\n  Leonardo Frid [aut],\n  Tabitha Kennedy [aut],\n  Shreeram Senthivasan [aut],\n  ApexRMS [cph]",
    "url": "https://syncrosim.github.io/rsyncrosim/",
    "bug_reports": "https://github.com/syncrosim/rsyncrosim/issues/",
    "repository": "https://cran.r-project.org/package=rsyncrosim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsyncrosim The R Interface to 'SyncroSim' 'SyncroSim' is a generalized framework for managing scenario-based \n    datasets (<https://syncrosim.com/>). 'rsyncrosim' provides an interface to \n    'SyncroSim'. Simulation models can be added to 'SyncroSim' in order to \n    transform these datasets, taking advantage of general features such as \n    defining scenarios of model inputs, running Monte Carlo simulations, and \n    summarizing model outputs. 'rsyncrosim' requires 'SyncroSim' 2.3.5 or higher \n    (API documentation: <https://docs.syncrosim.com/>).  "
  },
  {
    "id": 19896,
    "package_name": "rtables.officer",
    "title": "Exporting Tools for 'rtables'",
    "description": "Designed to create and display complex tables with R, the\n    'rtables' R package allows cells in an 'rtables' object to contain any\n    high-dimensional data structure, which can then be displayed with\n    cell-specific formatting instructions. Additionally, the\n    'rtables.officer' package supports export formats related to the\n    Microsoft Office software suite, including Microsoft Word ('docx') and\n    Microsoft PowerPoint ('pptx').",
    "version": "0.1.1",
    "maintainer": "Joe Zhu <joe.zhu@roche.com>",
    "author": "Gabriel Becker [ctb],\n  Davide Garolini [aut] (ORCID: <https://orcid.org/0000-0002-1445-1369>),\n  Emily de la Rua [aut] (ORCID: <https://orcid.org/0009-0000-8738-5561>),\n  Abinaya Yogasekaram [aut] (ORCID:\n    <https://orcid.org/0009-0005-2083-1105>),\n  Joe Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0001-7566-2787>),\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/insightsengineering/rtables.officer,\nhttps://insightsengineering.github.io/rtables.officer/",
    "bug_reports": "https://github.com/insightsengineering/rtables.officer/issues",
    "repository": "https://cran.r-project.org/package=rtables.officer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtables.officer Exporting Tools for 'rtables' Designed to create and display complex tables with R, the\n    'rtables' R package allows cells in an 'rtables' object to contain any\n    high-dimensional data structure, which can then be displayed with\n    cell-specific formatting instructions. Additionally, the\n    'rtables.officer' package supports export formats related to the\n    Microsoft Office software suite, including Microsoft Word ('docx') and\n    Microsoft PowerPoint ('pptx').  "
  },
  {
    "id": 19897,
    "package_name": "rtabulator",
    "title": "R Bindings for 'Tabulator JS'",
    "description": "Provides R bindings for 'Tabulator JS' <https://tabulator.info/>.\n  Makes it a breeze to create highly customizable interactive tables in 'rmarkdown' documents\n  and 'shiny' applications. It includes filtering, grouping, editing, input validation,\n  history recording, column formatters, packaged themes and more.",
    "version": "0.1.2",
    "maintainer": "Stefan Kuethe <crazycapivara@gmail.com>",
    "author": "Stefan Kuethe [aut, cre, cph],\n  Nico Friess [aut],\n  Oli Folkerd [cph] (Author of included tabulator.js library)",
    "url": "https://github.com/eodaGmbH/rtabulator\nhttps://eodagmbh.github.io/rtabulator/",
    "bug_reports": "https://github.com/eodaGmbH/rtabulator/issues",
    "repository": "https://cran.r-project.org/package=rtabulator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtabulator R Bindings for 'Tabulator JS' Provides R bindings for 'Tabulator JS' <https://tabulator.info/>.\n  Makes it a breeze to create highly customizable interactive tables in 'rmarkdown' documents\n  and 'shiny' applications. It includes filtering, grouping, editing, input validation,\n  history recording, column formatters, packaged themes and more.  "
  },
  {
    "id": 19901,
    "package_name": "rtemps",
    "title": "R Templates for Reproducible Data Analyses",
    "description": "A collection of R Markdown templates for nicely structured, reproducible \n    data analyses in R. The templates have embedded examples on how to write \n    citations, footnotes, equations and use colored message/info boxes, how to \n    cross-reference different parts/sections in the report, provide a nice \n    table of contents (toc) with a References section and proper R session \n    information as well as examples using DT tables and ggplot2 graphs. \n    The bookdown Lite template theme supports code folding.",
    "version": "0.8.0",
    "maintainer": "John Zobolas <bblodfon@gmail.com>",
    "author": "John Zobolas [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-3609-8674>)",
    "url": "https://github.com/bblodfon/rtemps",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rtemps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtemps R Templates for Reproducible Data Analyses A collection of R Markdown templates for nicely structured, reproducible \n    data analyses in R. The templates have embedded examples on how to write \n    citations, footnotes, equations and use colored message/info boxes, how to \n    cross-reference different parts/sections in the report, provide a nice \n    table of contents (toc) with a References section and proper R session \n    information as well as examples using DT tables and ggplot2 graphs. \n    The bookdown Lite template theme supports code folding.  "
  },
  {
    "id": 19907,
    "package_name": "rtiddlywiki",
    "title": "R Interface for 'TiddlyWiki'",
    "description": "'TiddlyWiki' is a unique non-linear notebook for capturing, organising and sharing complex information. 'rtiddlywiki' is a R interface of 'TiddlyWiki' <https://tiddlywiki.com> to create new tiddler from 'R Markdown' file, and then put into a local 'TiddlyWiki' server if it is available.",
    "version": "0.5.2",
    "maintainer": "Bangyou Zheng <bangyou.zheng@csiro.au>",
    "author": "Bangyou Zheng [aut, cre]",
    "url": "https://rtiddlywiki.bangyou.me/,\nhttps://github.com/byzheng/rtiddlywiki",
    "bug_reports": "https://github.com/byzheng/rtiddlywiki/issues",
    "repository": "https://cran.r-project.org/package=rtiddlywiki",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtiddlywiki R Interface for 'TiddlyWiki' 'TiddlyWiki' is a unique non-linear notebook for capturing, organising and sharing complex information. 'rtiddlywiki' is a R interface of 'TiddlyWiki' <https://tiddlywiki.com> to create new tiddler from 'R Markdown' file, and then put into a local 'TiddlyWiki' server if it is available.  "
  },
  {
    "id": 19909,
    "package_name": "rtiktokads",
    "title": "Load Data from 'tiktok Marketing API'",
    "description": "Loading data from 'tiktok Marketing API' <https://business-api.tiktok.com/portal> \n    by business centers, advertisers, budgets and reports.",
    "version": "0.3.4",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0410-7385>),\n  Netpeak [cph]",
    "url": "https://github.com/selesnow/rtiktokads",
    "bug_reports": "https://github.com/selesnow/rtiktokads/issues",
    "repository": "https://cran.r-project.org/package=rtiktokads",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtiktokads Load Data from 'tiktok Marketing API' Loading data from 'tiktok Marketing API' <https://business-api.tiktok.com/portal> \n    by business centers, advertisers, budgets and reports.  "
  },
  {
    "id": 19930,
    "package_name": "rtson",
    "title": "Typed JSON",
    "description": "TSON, short for Typed JSON, is a binary-encoded serialization of\n    JSON like document that support JavaScript typed data (https://github.com/tercen/TSON).",
    "version": "1.3",
    "maintainer": "Alexandre Maurel <alexandre.maurel@gmail.com>",
    "author": "Alexandre Maurel",
    "url": "https://github.com/tercen/TSON",
    "bug_reports": "https://github.com/tercen/TSON/issues",
    "repository": "https://cran.r-project.org/package=rtson",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtson Typed JSON TSON, short for Typed JSON, is a binary-encoded serialization of\n    JSON like document that support JavaScript typed data (https://github.com/tercen/TSON).  "
  },
  {
    "id": 19937,
    "package_name": "ruler",
    "title": "Tidy Data Validation Reports",
    "description": "Tools for creating data validation pipelines and\n    tidy reports. This package offers a framework for exploring and\n    validating data frame like objects using 'dplyr' grammar of data\n    manipulation.",
    "version": "0.3.1",
    "maintainer": "Evgeni Chasnovski <evgeni.chasnovski@gmail.com>",
    "author": "Evgeni Chasnovski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1617-4019>)",
    "url": "https://echasnovski.github.io/ruler/,\nhttps://github.com/echasnovski/ruler",
    "bug_reports": "https://github.com/echasnovski/ruler/issues",
    "repository": "https://cran.r-project.org/package=ruler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ruler Tidy Data Validation Reports Tools for creating data validation pipelines and\n    tidy reports. This package offers a framework for exploring and\n    validating data frame like objects using 'dplyr' grammar of data\n    manipulation.  "
  },
  {
    "id": 19958,
    "package_name": "rvalues",
    "title": "R-Values for Ranking in High-Dimensional Settings",
    "description": "A collection of functions for computing \"r-values\" from various\n    kinds of user input such as MCMC output or a list of effect size estimates\n    and associated standard errors. Given a large collection of measurement units,\n    the r-value, r, of a particular unit is a reported percentile that may be\n    interpreted as the smallest percentile at which the unit should be placed in the\n    top r-fraction of units.",
    "version": "0.7.1",
    "maintainer": "Nicholas Henderson <nchender@umich.edu>",
    "author": "Nicholas Henderson [cre],\n  Michael Newton [aut]",
    "url": "https://doi.org/10.1111/rssb.12131",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rvalues",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvalues R-Values for Ranking in High-Dimensional Settings A collection of functions for computing \"r-values\" from various\n    kinds of user input such as MCMC output or a list of effect size estimates\n    and associated standard errors. Given a large collection of measurement units,\n    the r-value, r, of a particular unit is a reported percentile that may be\n    interpreted as the smallest percentile at which the unit should be placed in the\n    top r-fraction of units.  "
  },
  {
    "id": 19961,
    "package_name": "rvg",
    "title": "R Graphics Devices for 'Office' Vector Graphics Output",
    "description": "Vector Graphics devices for 'Microsoft PowerPoint' and\n    'Microsoft Excel'. Functions extending package 'officer' are provided\n    to embed 'DrawingML' graphics into 'Microsoft PowerPoint'\n    presentations and 'Microsoft Excel' workbooks.",
    "version": "0.4.0",
    "maintainer": "David Gohel <david.gohel@ardata.fr>",
    "author": "David Gohel [aut, cre],\n  ArData [cph],\n  Bob Rudis [ctb] (the javascript code used by function set_attr),\n  Francois Brunetti [ctb] (clipping algorithms)",
    "url": "https://ardata-fr.github.io/officeverse/,\nhttps://davidgohel.github.io/rvg/",
    "bug_reports": "https://github.com/davidgohel/rvg/issues",
    "repository": "https://cran.r-project.org/package=rvg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvg R Graphics Devices for 'Office' Vector Graphics Output Vector Graphics devices for 'Microsoft PowerPoint' and\n    'Microsoft Excel'. Functions extending package 'officer' are provided\n    to embed 'DrawingML' graphics into 'Microsoft PowerPoint'\n    presentations and 'Microsoft Excel' workbooks.  "
  },
  {
    "id": 19966,
    "package_name": "rvkstat",
    "title": "R Interface to API 'vk.com'",
    "description": "Load data from vk.com api about your communiti users and views,\n    ads performance, post on user wall and etc.\tFor more information \n    see API Documentation <https://vk.com/dev/first_guide>.",
    "version": "3.2.0",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev",
    "url": "https://selesnow.github.io/rvkstat/",
    "bug_reports": "https://github.com/selesnow/rvkstat/issues",
    "repository": "https://cran.r-project.org/package=rvkstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rvkstat R Interface to API 'vk.com' Load data from vk.com api about your communiti users and views,\n    ads performance, post on user wall and etc.\tFor more information \n    see API Documentation <https://vk.com/dev/first_guide>.  "
  },
  {
    "id": 19979,
    "package_name": "rworkflows",
    "title": "Test, Document, Containerise, and Deploy R Packages",
    "description": "Reproducibility is essential to the progress of research, \n  yet achieving it remains elusive even in computational fields. \n  Continuous Integration (CI) platforms offer a powerful way to launch automated workflows \n  to check and document code, but often require considerable time, effort, \n  and technical expertise to setup. We therefore developed the rworkflows suite \n  to make robust CI workflows easy and freely accessible to all R package developers. \n  rworkflows consists of 1) a CRAN/Bioconductor-compatible R package template, \n  2) an R package to quickly implement a standardised workflow, and \n  3) a centrally maintained GitHub Action. ",
    "version": "1.0.6",
    "maintainer": "Brian Schilder <brian_schilder@alumni.brown.edu>",
    "author": "Brian Schilder [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5949-2191>),\n  Alan Murphy [aut, ctb] (ORCID: <https://orcid.org/0000-0002-2487-8753>),\n  Nathan Skene [aut] (ORCID: <https://orcid.org/0000-0002-6807-3180>)",
    "url": "https://github.com/neurogenomics/rworkflows,\nhttps://CRAN.R-project.org/package=rworkflows",
    "bug_reports": "https://github.com/neurogenomics/rworkflows/issues",
    "repository": "https://cran.r-project.org/package=rworkflows",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rworkflows Test, Document, Containerise, and Deploy R Packages Reproducibility is essential to the progress of research, \n  yet achieving it remains elusive even in computational fields. \n  Continuous Integration (CI) platforms offer a powerful way to launch automated workflows \n  to check and document code, but often require considerable time, effort, \n  and technical expertise to setup. We therefore developed the rworkflows suite \n  to make robust CI workflows easy and freely accessible to all R package developers. \n  rworkflows consists of 1) a CRAN/Bioconductor-compatible R package template, \n  2) an R package to quickly implement a standardised workflow, and \n  3) a centrally maintained GitHub Action.   "
  },
  {
    "id": 19989,
    "package_name": "ryandexdirect",
    "title": "Load Data From 'Yandex Direct'",
    "description": "Load data from 'Yandex Direct' API V5 \n    <https://yandex.ru/dev/direct/doc/dg/concepts/about-docpage> into R.\n\tProvide function for load lists of campaings, ads, keywords and other \n\tobjects from 'Yandex Direct' account. Also you can load statistic from\n\tAPI 'Reports Service' <https://yandex.ru/dev/direct/doc/reports/reports-docpage>.\n\tAnd allows keyword bids management.",
    "version": "3.6.2",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0410-7385>)",
    "url": "https://selesnow.github.io/ryandexdirect/,\nhttps://t.me/R4marketing,\nhttps://www.youtube.com/playlist?list=PLD2LDq8edf4oUo0L9Kw77ZXf0KcV1hu67",
    "bug_reports": "https://github.com/selesnow/ryandexdirect/issues",
    "repository": "https://cran.r-project.org/package=ryandexdirect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ryandexdirect Load Data From 'Yandex Direct' Load data from 'Yandex Direct' API V5 \n    <https://yandex.ru/dev/direct/doc/dg/concepts/about-docpage> into R.\n\tProvide function for load lists of campaings, ads, keywords and other \n\tobjects from 'Yandex Direct' account. Also you can load statistic from\n\tAPI 'Reports Service' <https://yandex.ru/dev/direct/doc/reports/reports-docpage>.\n\tAnd allows keyword bids management.  "
  },
  {
    "id": 19990,
    "package_name": "rym",
    "title": "R Interface to Yandex Metrica API",
    "description": "Allows work with 'Management API' for load counters, segments, filters,\n\tuser permissions and goals list from Yandex Metrica, 'Reporting API' allows you to get \n\tinformation about the statistics of site visits and other data without\n\tusing the web interface, 'Logs API' allows to receive non-aggregated data and \n\t'Compatible with Google Analytics Core Reporting API v3' allows \n\treceive information about site traffic and other data using field names \n\tfrom Google Analytics Core API.\tFor more information see official \n\tdocuments <https://yandex.ru/dev/metrika/doc/api2/concept/about-docpage>.",
    "version": "1.0.6",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0410-7385>),\n  Netpeak [cph]",
    "url": "https://selesnow.github.io/rym/",
    "bug_reports": "https://github.com/selesnow/rym/issues",
    "repository": "https://cran.r-project.org/package=rym",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rym R Interface to Yandex Metrica API Allows work with 'Management API' for load counters, segments, filters,\n\tuser permissions and goals list from Yandex Metrica, 'Reporting API' allows you to get \n\tinformation about the statistics of site visits and other data without\n\tusing the web interface, 'Logs API' allows to receive non-aggregated data and \n\t'Compatible with Google Analytics Core Reporting API v3' allows \n\treceive information about site traffic and other data using field names \n\tfrom Google Analytics Core API.\tFor more information see official \n\tdocuments <https://yandex.ru/dev/metrika/doc/api2/concept/about-docpage>.  "
  },
  {
    "id": 19991,
    "package_name": "rytstat",
    "title": "Work with 'YouTube API'",
    "description": "Provide function for get data from 'YouTube Data API' \n    <https://developers.google.com/youtube/v3/docs/>, 'YouTube Analytics API' \n    <https://developers.google.com/youtube/analytics/reference/> and \n    'YouTube Reporting API' <https://developers.google.com/youtube/reporting/v1/reports>.",
    "version": "0.3.2",
    "maintainer": "Alexey Seleznev <selesnow@gmail.com>",
    "author": "Alexey Seleznev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0410-7385>),\n  Netpeak [cph]",
    "url": "https://selesnow.github.io/rytstat/docs/",
    "bug_reports": "https://github.com/selesnow/rytstat/issues",
    "repository": "https://cran.r-project.org/package=rytstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rytstat Work with 'YouTube API' Provide function for get data from 'YouTube Data API' \n    <https://developers.google.com/youtube/v3/docs/>, 'YouTube Analytics API' \n    <https://developers.google.com/youtube/analytics/reference/> and \n    'YouTube Reporting API' <https://developers.google.com/youtube/reporting/v1/reports>.  "
  },
  {
    "id": 20000,
    "package_name": "sClust",
    "title": "R Toolbox for Unsupervised Spectral Clustering",
    "description": "Toolbox containing a variety of spectral clustering tools functions. Among the tools available are the hierarchical spectral clustering algorithm, the Shi and Malik clustering algorithm, the Perona and Freeman algorithm, the non-normalized clustering, the Von Luxburg algorithm, the Partition Around Medoids clustering algorithm, a multi-level clustering algorithm, recursive clustering and the fast method for all clustering algorithm. As well as other tools needed to run these algorithms or useful for unsupervised spectral clustering. This toolbox aims to gather the main tools for unsupervised spectral classification. See <http://mawenzi.univ-littoral.fr/> for more information and documentation. ",
    "version": "1.0",
    "maintainer": "Emilie Poisson-Caillault <emilie.caillault@univ-littoral.fr>",
    "author": "Emilie Poisson-Caillault [aut, cre, cph],\n  Alain Lefebvre [ctb],\n  Erwan Vincent [aut],\n  Pierre-Alexandre Hebert [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sClust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sClust R Toolbox for Unsupervised Spectral Clustering Toolbox containing a variety of spectral clustering tools functions. Among the tools available are the hierarchical spectral clustering algorithm, the Shi and Malik clustering algorithm, the Perona and Freeman algorithm, the non-normalized clustering, the Von Luxburg algorithm, the Partition Around Medoids clustering algorithm, a multi-level clustering algorithm, recursive clustering and the fast method for all clustering algorithm. As well as other tools needed to run these algorithms or useful for unsupervised spectral clustering. This toolbox aims to gather the main tools for unsupervised spectral classification. See <http://mawenzi.univ-littoral.fr/> for more information and documentation.   "
  },
  {
    "id": 20025,
    "package_name": "sae4health",
    "title": "Small Area Estimation for Key Health and Demographic Indicators\nfrom Household Surveys",
    "description": "Enables small area estimation (SAE) of health and demographic indicators in low- and middle-income countries (LMICs). It powers an R 'shiny' application for generating subnational estimates and prevalence maps of 150+ binary indicators from Demographic and Health Surveys (DHS). It builds on the SAE analysis workflow from the 'surveyPrev' package. For documentation, visit <https://sae4health.stat.uw.edu/>. Methodological details can be found at Wu et al. (2025) <doi:10.48550/arXiv.2505.01467>.",
    "version": "1.2.3",
    "maintainer": "Yunhan Wu <wu-thomas@outlook.com>",
    "author": "Yunhan Wu [cre, aut],\n  Qianyu Dong [aut],\n  Zehang R Li [aut],\n  Jon Wakefield [aut]",
    "url": "https://sae4health.stat.uw.edu/,\nhttps://github.com/wu-thomas/sae4health",
    "bug_reports": "https://github.com/wu-thomas/sae4health/issues",
    "repository": "https://cran.r-project.org/package=sae4health",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sae4health Small Area Estimation for Key Health and Demographic Indicators\nfrom Household Surveys Enables small area estimation (SAE) of health and demographic indicators in low- and middle-income countries (LMICs). It powers an R 'shiny' application for generating subnational estimates and prevalence maps of 150+ binary indicators from Demographic and Health Surveys (DHS). It builds on the SAE analysis workflow from the 'surveyPrev' package. For documentation, visit <https://sae4health.stat.uw.edu/>. Methodological details can be found at Wu et al. (2025) <doi:10.48550/arXiv.2505.01467>.  "
  },
  {
    "id": 20047,
    "package_name": "saemix",
    "title": "Stochastic Approximation Expectation Maximization (SAEM)\nAlgorithm",
    "description": "The 'saemix' package implements the Stochastic Approximation EM algorithm for parameter estimation in (non)linear mixed effects models. It (i) computes the maximum likelihood estimator of the population parameters, without any approximation of the model (linearisation, quadrature approximation,...), using the Stochastic Approximation Expectation Maximization (SAEM) algorithm, (ii) provides standard errors for the maximum likelihood estimator (iii) estimates the conditional modes, the conditional means and the conditional standard deviations of the individual parameters, using the Hastings-Metropolis algorithm (see Comets et al. (2017) <doi:10.18637/jss.v080.i03>). Many applications of SAEM in agronomy, animal breeding and PKPD analysis have been published by members of the Monolix group. The full PDF documentation for the package including references about the algorithm and examples can be downloaded on the github of the IAME research institute for 'saemix': <https://github.com/iame-researchCenter/saemix/blob/7638e1b09ccb01cdff173068e01c266e906f76eb/docsaem.pdf>.",
    "version": "3.4",
    "maintainer": "Emmanuelle Comets <emmanuelle.comets@inserm.fr>",
    "author": "Emmanuelle Comets [aut, cre],\n  Audrey Lavenu [aut],\n  Marc Lavielle [aut],\n  Belhal Karimi [aut],\n  Maud Delattre [ctb],\n  Alexandra Lavalley-Morelle [ctb],\n  Marilou Chanel [ctb],\n  Johannes Ranke [ctb] (ORCID: <https://orcid.org/0000-0003-4371-6538>),\n  Sofia Kaisaridi [ctb],\n  Lucie Fayette [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=saemix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "saemix Stochastic Approximation Expectation Maximization (SAEM)\nAlgorithm The 'saemix' package implements the Stochastic Approximation EM algorithm for parameter estimation in (non)linear mixed effects models. It (i) computes the maximum likelihood estimator of the population parameters, without any approximation of the model (linearisation, quadrature approximation,...), using the Stochastic Approximation Expectation Maximization (SAEM) algorithm, (ii) provides standard errors for the maximum likelihood estimator (iii) estimates the conditional modes, the conditional means and the conditional standard deviations of the individual parameters, using the Hastings-Metropolis algorithm (see Comets et al. (2017) <doi:10.18637/jss.v080.i03>). Many applications of SAEM in agronomy, animal breeding and PKPD analysis have been published by members of the Monolix group. The full PDF documentation for the package including references about the algorithm and examples can be downloaded on the github of the IAME research institute for 'saemix': <https://github.com/iame-researchCenter/saemix/blob/7638e1b09ccb01cdff173068e01c266e906f76eb/docsaem.pdf>.  "
  },
  {
    "id": 20062,
    "package_name": "salesforcer",
    "title": "An Implementation of 'Salesforce' APIs Using Tidy Principles",
    "description": "Functions connecting to the 'Salesforce' Platform APIs (REST, SOAP, \n    Bulk 1.0, Bulk 2.0, Metadata, Reports and Dashboards) \n    <https://trailhead.salesforce.com/content/learn/modules/api_basics/api_basics_overview>. \n    \"API\" is an acronym for \"application programming interface\". Most all calls \n    from these APIs are supported as they use CSV, XML or JSON data that can be \n    parsed into R data structures. For more details please see the 'Salesforce' \n    API documentation and this package's website \n    <https://stevenmmortimer.github.io/salesforcer/> for more information, \n    documentation, and examples.",
    "version": "1.0.2",
    "maintainer": "Steven M. Mortimer <mortimer.steven.m@gmail.com>",
    "author": "Steven M. Mortimer [aut, cre],\n  Takekatsu Hiramura [ctb],\n  Jennifer Bryan [ctb, cph],\n  Joanna Zhao [ctb, cph]",
    "url": "https://github.com/StevenMMortimer/salesforcer,\nhttps://stevenmmortimer.github.io/salesforcer/",
    "bug_reports": "https://github.com/StevenMMortimer/salesforcer/issues",
    "repository": "https://cran.r-project.org/package=salesforcer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "salesforcer An Implementation of 'Salesforce' APIs Using Tidy Principles Functions connecting to the 'Salesforce' Platform APIs (REST, SOAP, \n    Bulk 1.0, Bulk 2.0, Metadata, Reports and Dashboards) \n    <https://trailhead.salesforce.com/content/learn/modules/api_basics/api_basics_overview>. \n    \"API\" is an acronym for \"application programming interface\". Most all calls \n    from these APIs are supported as they use CSV, XML or JSON data that can be \n    parsed into R data structures. For more details please see the 'Salesforce' \n    API documentation and this package's website \n    <https://stevenmmortimer.github.io/salesforcer/> for more information, \n    documentation, and examples.  "
  },
  {
    "id": 20082,
    "package_name": "samplezoo",
    "title": "Generate Samples with a Variety of Probability Distributions",
    "description": "Simplifies the process of generating samples from a variety of probability distributions, allowing users to quickly create data frames for demonstrations, troubleshooting, or teaching purposes. Data is available in multiple sizes\u2014small, medium, and large. For more information, refer to the package documentation.",
    "version": "1.2.1",
    "maintainer": "Nicholas Vietto <nicholasvietto@gmail.com>",
    "author": "Nicholas Vietto [aut, cre, cph]",
    "url": "https://github.com/nvietto/samplezoo,\nhttps://nvietto.github.io/samplezoo/",
    "bug_reports": "https://github.com/nvietto/samplezoo/issues",
    "repository": "https://cran.r-project.org/package=samplezoo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "samplezoo Generate Samples with a Variety of Probability Distributions Simplifies the process of generating samples from a variety of probability distributions, allowing users to quickly create data frames for demonstrations, troubleshooting, or teaching purposes. Data is available in multiple sizes\u2014small, medium, and large. For more information, refer to the package documentation.  "
  },
  {
    "id": 20106,
    "package_name": "santaR",
    "title": "Short Asynchronous Time-Series Analysis",
    "description": "A graphical and automated pipeline for the analysis \n\t\tof short time-series in R ('santaR'). This approach is designed to accommodate asynchronous \n\t\ttime sampling (i.e. different time points for different individuals), \n\t\tinter-individual variability, noisy measurements and large numbers of variables. \n\t\tBased on a smoothing splines functional model, 'santaR' is able to detect variables\n\t\thighlighting significantly different temporal trajectories between study groups.\n\t\tDesigned initially for metabolic phenotyping, 'santaR' is also suited for other Systems Biology \n\t\tdisciplines. Command line and graphical analysis (via a 'shiny' application) enable fast and\n\t\tparallel automated analysis and reporting, intuitive visualisation and comprehensive plotting\n\t\toptions for non-specialist users.",
    "version": "1.2.4",
    "maintainer": "Arnaud Wolfer <adwolfer@gmail.com>",
    "author": "Arnaud Wolfer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5856-3218>),\n  Timothy Ebbels [ctb],\n  Joe Cheng [ctb] (Shiny javascript custom-input control)",
    "url": "https://github.com/adwolfer/santaR,\nhttps://adwolfer.github.io/santaR/",
    "bug_reports": "https://github.com/adwolfer/santaR/issues/new",
    "repository": "https://cran.r-project.org/package=santaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "santaR Short Asynchronous Time-Series Analysis A graphical and automated pipeline for the analysis \n\t\tof short time-series in R ('santaR'). This approach is designed to accommodate asynchronous \n\t\ttime sampling (i.e. different time points for different individuals), \n\t\tinter-individual variability, noisy measurements and large numbers of variables. \n\t\tBased on a smoothing splines functional model, 'santaR' is able to detect variables\n\t\thighlighting significantly different temporal trajectories between study groups.\n\t\tDesigned initially for metabolic phenotyping, 'santaR' is also suited for other Systems Biology \n\t\tdisciplines. Command line and graphical analysis (via a 'shiny' application) enable fast and\n\t\tparallel automated analysis and reporting, intuitive visualisation and comprehensive plotting\n\t\toptions for non-specialist users.  "
  },
  {
    "id": 20114,
    "package_name": "saros",
    "title": "Semi-Automatic Reporting of Ordinary Surveys",
    "description": "Offers a systematic way for conditional reporting of figures and tables for many\n    (and bivariate combinations of) variables, typically from survey data.\n    Contains interactive 'ggiraph'-based \n    (<https://CRAN.R-project.org/package=ggiraph>) plotting functions and\n    data frame-based summary tables (bivariate significance tests, \n    frequencies/proportions, unique open ended responses, etc) with\n    many arguments for customization, and extensions possible. Uses a global \n    options() system for neatly reducing redundant code.\n    Also contains tools for immediate saving of objects and returning a hashed link to the object,\n    useful for creating download links to high resolution images upon rendering in 'Quarto'. \n    Suitable for highly customized reports, primarily intended for survey\n    research.",
    "version": "1.6.0",
    "maintainer": "Stephan Daus <stephus.daus@gmail.com>",
    "author": "Stephan Daus [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0230-6997>),\n  Julia Silge [ctb] (Author of internal scale_x_reordered),\n  David Robinson [ctb] (Author of internal scale_x_reordered),\n  Nordic Institute for The Studies of Innovation, Research and Education\n    (NIFU) [fnd],\n  Kristiania University College [fnd]",
    "url": "https://nifu-no.github.io/saros/, https://github.com/NIFU-NO/saros",
    "bug_reports": "https://github.com/NIFU-NO/saros/issues",
    "repository": "https://cran.r-project.org/package=saros",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "saros Semi-Automatic Reporting of Ordinary Surveys Offers a systematic way for conditional reporting of figures and tables for many\n    (and bivariate combinations of) variables, typically from survey data.\n    Contains interactive 'ggiraph'-based \n    (<https://CRAN.R-project.org/package=ggiraph>) plotting functions and\n    data frame-based summary tables (bivariate significance tests, \n    frequencies/proportions, unique open ended responses, etc) with\n    many arguments for customization, and extensions possible. Uses a global \n    options() system for neatly reducing redundant code.\n    Also contains tools for immediate saving of objects and returning a hashed link to the object,\n    useful for creating download links to high resolution images upon rendering in 'Quarto'. \n    Suitable for highly customized reports, primarily intended for survey\n    research.  "
  },
  {
    "id": 20115,
    "package_name": "saros.base",
    "title": "Base Tools for Semi-Automatic Reporting of Ordinary Surveys",
    "description": "Scaffold an entire web-based report using template chunks, based on a small chapter overview and a dataset. \n    Highly adaptable with prefixes, suffixes, translations, etc. Also contains tools for password-protecting,\n    e.g. for each organization's report on a website. Developed for the common case of a survey across multiple organizations/sites\n    where each organization wants to obtain results for their organization compared with everyone else.\n    See 'saros' (<https://CRAN.R-project.org/package=saros>) for tools used for authors in the drafted reports.",
    "version": "1.2.0",
    "maintainer": "Stephan Daus <stephus.daus@gmail.com>",
    "author": "Stephan Daus [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0230-6997>),\n  Nordic Institute for The Studies of Innovation, Research and Education\n    (NIFU) [fnd],\n  Kristiania University College [fnd]",
    "url": "https://nifu-no.github.io/saros.base/,\nhttps://github.com/NIFU-NO/saros.base",
    "bug_reports": "https://github.com/NIFU-NO/saros.base/issues",
    "repository": "https://cran.r-project.org/package=saros.base",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "saros.base Base Tools for Semi-Automatic Reporting of Ordinary Surveys Scaffold an entire web-based report using template chunks, based on a small chapter overview and a dataset. \n    Highly adaptable with prefixes, suffixes, translations, etc. Also contains tools for password-protecting,\n    e.g. for each organization's report on a website. Developed for the common case of a survey across multiple organizations/sites\n    where each organization wants to obtain results for their organization compared with everyone else.\n    See 'saros' (<https://CRAN.R-project.org/package=saros>) for tools used for authors in the drafted reports.  "
  },
  {
    "id": 20120,
    "package_name": "sas7bdat",
    "title": "sas7bdat Reverse Engineering Documentation",
    "description": "Documentation and prototypes for the earliest (circa 2010) open-source effort to reverse engineer the sas7bdat file format. The package includes a prototype reader for sas7bdat files. However, newer packages may contain more robust readers for sas7bdat files.",
    "version": "0.8",
    "maintainer": "Matt Shotwell <matt.shotwell@vanderbilt.edu>",
    "author": "Matt Shotwell [aut, cre],\n  Clint Cummins [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sas7bdat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sas7bdat sas7bdat Reverse Engineering Documentation Documentation and prototypes for the earliest (circa 2010) open-source effort to reverse engineer the sas7bdat file format. The package includes a prototype reader for sas7bdat files. However, newer packages may contain more robust readers for sas7bdat files.  "
  },
  {
    "id": 20126,
    "package_name": "satellite",
    "title": "Handling and Manipulating Remote Sensing Data",
    "description": "Herein, we provide a broad variety of functions which are useful\n    for handling, manipulating, and visualizing satellite-based remote sensing \n    data. These operations range from mere data import and layer handling (eg \n    subsetting), over Raster* typical data wrangling (eg crop, extend), to more \n    sophisticated (pre-)processing tasks typically applied to satellite imagery \n    (eg atmospheric and topographic correction). This functionality is \n    complemented by a full access to the satellite layers' metadata at any \n    stage and the documentation of performed actions in a separate log file. \n    Currently available sensors include Landsat 4-5 (TM), 7 (ETM+), and 8 \n    (OLI/TIRS Combined), and additional compatibility is ensured for the Landsat \n    Global Land Survey data set. ",
    "version": "1.0.6",
    "maintainer": "Florian Detsch <fdetsch@web.de>",
    "author": "Thomas Nauss [aut],\n  Hanna Meyer [aut],\n  Tim Appelhans [aut],\n  Florian Detsch [aut, cre]",
    "url": "https://github.com/environmentalinformatics-marburg/satellite",
    "bug_reports": "https://github.com/environmentalinformatics-marburg/satellite/issues",
    "repository": "https://cran.r-project.org/package=satellite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "satellite Handling and Manipulating Remote Sensing Data Herein, we provide a broad variety of functions which are useful\n    for handling, manipulating, and visualizing satellite-based remote sensing \n    data. These operations range from mere data import and layer handling (eg \n    subsetting), over Raster* typical data wrangling (eg crop, extend), to more \n    sophisticated (pre-)processing tasks typically applied to satellite imagery \n    (eg atmospheric and topographic correction). This functionality is \n    complemented by a full access to the satellite layers' metadata at any \n    stage and the documentation of performed actions in a separate log file. \n    Currently available sensors include Landsat 4-5 (TM), 7 (ETM+), and 8 \n    (OLI/TIRS Combined), and additional compatibility is ensured for the Landsat \n    Global Land Survey data set.   "
  },
  {
    "id": 20165,
    "package_name": "scPipeline",
    "title": "A Wrapper for 'Seurat' and Related R Packages for End-to-End\nSingle Cell Analysis",
    "description": "Reports markers list, differentially expressed genes, associated pathways, cell-type annotations, does batch correction and other related single cell analyses all wrapped within 'Seurat'.",
    "version": "0.2.0.0",
    "maintainer": "Viswanadham Sridhara <Sridhara.Omics@gmail.com>",
    "author": "Viswanadham Sridhara [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0688-6140>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scPipeline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scPipeline A Wrapper for 'Seurat' and Related R Packages for End-to-End\nSingle Cell Analysis Reports markers list, differentially expressed genes, associated pathways, cell-type annotations, does batch correction and other related single cell analyses all wrapped within 'Seurat'.  "
  },
  {
    "id": 20212,
    "package_name": "schoRsch",
    "title": "Tools for Analyzing Factorial Experiments",
    "description": "Offers a helping hand to psychologists and other behavioral scientists who routinely deal with experimental data from factorial experiments. It includes several functions to format output from other R functions according to the style guidelines of the APA (American Psychological Association). This formatted output can be copied directly into manuscripts to facilitate data reporting. These features are backed up by a toolkit of several small helper functions, e.g., offering out-of-the-box outlier removal. The package lends its name to Georg \"Schorsch\" Schuessler, ingenious technician at the Department of Psychology III, University of Wuerzburg. For details on the implemented methods, see Roland Pfister and Markus Janczyk (2016) <doi: 10.20982/tqmp.12.2.p147>.",
    "version": "1.11",
    "maintainer": "Roland Pfister <mail@roland-pfister.net>",
    "author": "Roland Pfister [aut, cre],\n  Markus Janczyk [aut]",
    "url": "https://www.tqmp.org/RegularArticles/vol12-2/p147/index.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=schoRsch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "schoRsch Tools for Analyzing Factorial Experiments Offers a helping hand to psychologists and other behavioral scientists who routinely deal with experimental data from factorial experiments. It includes several functions to format output from other R functions according to the style guidelines of the APA (American Psychological Association). This formatted output can be copied directly into manuscripts to facilitate data reporting. These features are backed up by a toolkit of several small helper functions, e.g., offering out-of-the-box outlier removal. The package lends its name to Georg \"Schorsch\" Schuessler, ingenious technician at the Department of Psychology III, University of Wuerzburg. For details on the implemented methods, see Roland Pfister and Markus Janczyk (2016) <doi: 10.20982/tqmp.12.2.p147>.  "
  },
  {
    "id": 20219,
    "package_name": "sciRmdTheme",
    "title": "Upgraded 'Rmarkdown' Themes for Scientific Writing",
    "description": "A set of 'Rmarkdown' themes for creating scientific and professional documents. Simple interface with features to ease navigation across the page and sub-pages.",
    "version": "0.1",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom",
    "url": "https://github.com/oobianom/sciRmdTheme",
    "bug_reports": "https://github.com/oobianom/sciRmdTheme",
    "repository": "https://cran.r-project.org/package=sciRmdTheme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sciRmdTheme Upgraded 'Rmarkdown' Themes for Scientific Writing A set of 'Rmarkdown' themes for creating scientific and professional documents. Simple interface with features to ease navigation across the page and sub-pages.  "
  },
  {
    "id": 20223,
    "package_name": "scientific",
    "title": "Two Highly Customizable 'rmarkdown' Themes for Scientific\nReports",
    "description": "Offers 'markdown' output formats designed with various styles, allowing users to generate HTML reports tailored for scientific or machine learning showcase. The output has a contemporary appearance with vibrant visuals, providing numerous styles for effective highlighting. Created using the 'tufte' <https://rstudio.github.io/tufte/> package code as a starting point.",
    "version": "2025.1",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://scientific.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/scientific/issues",
    "repository": "https://cran.r-project.org/package=scientific",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scientific Two Highly Customizable 'rmarkdown' Themes for Scientific\nReports Offers 'markdown' output formats designed with various styles, allowing users to generate HTML reports tailored for scientific or machine learning showcase. The output has a contemporary appearance with vibrant visuals, providing numerous styles for effective highlighting. Created using the 'tufte' <https://rstudio.github.io/tufte/> package code as a starting point.  "
  },
  {
    "id": 20233,
    "package_name": "scorecard",
    "title": "Credit Risk Scorecard",
    "description": "\n  The `scorecard` package makes the development of credit risk scorecard \n  easier and efficient by providing functions for some common tasks, \n  such as data partition, variable selection, woe binning, scorecard scaling,\n  performance evaluation and report generation. These functions can also used\n  in the development of machine learning models.\n    The references including: \n  1. Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: \n  Development and Implementation Using SAS. \n  2. Siddiqi, N. (2006, ISBN: 9780471754510). Credit risk scorecards. \n  Developing and Implementing Intelligent Credit Scoring.",
    "version": "0.4.5",
    "maintainer": "Shichen Xie <xie@shichen.name>",
    "author": "Shichen Xie [aut, cre]",
    "url": "https://github.com/ShichenXie/scorecard,\nhttp://shichen.name/scorecard/",
    "bug_reports": "https://github.com/ShichenXie/scorecard/issues",
    "repository": "https://cran.r-project.org/package=scorecard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scorecard Credit Risk Scorecard \n  The `scorecard` package makes the development of credit risk scorecard \n  easier and efficient by providing functions for some common tasks, \n  such as data partition, variable selection, woe binning, scorecard scaling,\n  performance evaluation and report generation. These functions can also used\n  in the development of machine learning models.\n    The references including: \n  1. Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: \n  Development and Implementation Using SAS. \n  2. Siddiqi, N. (2006, ISBN: 9780471754510). Credit risk scorecards. \n  Developing and Implementing Intelligent Credit Scoring.  "
  },
  {
    "id": 20240,
    "package_name": "scoringfunctions",
    "title": "A Collection of Loss Functions for Assessing Point Forecasts",
    "description": "\n    Implements multiple consistent scoring functions\n    (Gneiting T (2011) <doi:10.1198/jasa.2011.r10138>) for assessing point\n    forecasts and point predictions. Detailed documentation of scoring\n    functions' properties is included for facilitating interpretation of\n    results.",
    "version": "1.1",
    "maintainer": "Hristos Tyralis <montchrister@gmail.com>",
    "author": "Hristos Tyralis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8932-4997>),\n  Georgia Papacharalampous [aut] (ORCID:\n    <https://orcid.org/0000-0001-5446-954X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scoringfunctions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scoringfunctions A Collection of Loss Functions for Assessing Point Forecasts \n    Implements multiple consistent scoring functions\n    (Gneiting T (2011) <doi:10.1198/jasa.2011.r10138>) for assessing point\n    forecasts and point predictions. Detailed documentation of scoring\n    functions' properties is included for facilitating interpretation of\n    results.  "
  },
  {
    "id": 20304,
    "package_name": "searchAnalyzeR",
    "title": "Advanced Analytics and Testing Framework for Systematic Review\nSearch Strategies",
    "description": "Provides comprehensive analytics, reporting, and testing capabilities \n    for systematic review search strategies. The package focuses on validating search \n    performance, generating standardized 'PRISMA'-compliant reports, and ensuring \n    reproducibility in evidence synthesis. Features include precision-recall analysis, \n    cross-database performance comparison, benchmark validation against gold standards, \n    sensitivity analysis, temporal coverage assessment, automated report generation,\n    and statistical comparison of search strategies. Supports multiple export formats \n    including 'CSV', 'Excel', 'RIS', 'BibTeX', and 'EndNote'. Includes tools for duplicate \n    detection, search strategy optimization, cross-validation frameworks, meta-analysis \n    of benchmark results, power analysis for study design, and reproducibility package \n    creation. Optionally connects to 'PubMed' for direct database searching and real-time \n    strategy comparison using the 'E-utilities' 'API'. Enhanced with bootstrap comparison \n    methods, 'McNemar' test for strategy evaluation, and comprehensive visualization tools \n    for performance assessment. Methods based on Manning et al. (2008) \n    for information retrieval metrics, Moher et al. (2009)\n    for 'PRISMA' guidelines, and Sampson et al. (2006) for \n    systematic review search methodology.",
    "version": "0.1.0",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre] (ORCID: <https://orcid.org/0000-0002-9979-8272>)",
    "url": "https://github.com/chaoliu-cl/searchAnalyzeR",
    "bug_reports": "https://github.com/chaoliu-cl/searchAnalyzeR/issues",
    "repository": "https://cran.r-project.org/package=searchAnalyzeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "searchAnalyzeR Advanced Analytics and Testing Framework for Systematic Review\nSearch Strategies Provides comprehensive analytics, reporting, and testing capabilities \n    for systematic review search strategies. The package focuses on validating search \n    performance, generating standardized 'PRISMA'-compliant reports, and ensuring \n    reproducibility in evidence synthesis. Features include precision-recall analysis, \n    cross-database performance comparison, benchmark validation against gold standards, \n    sensitivity analysis, temporal coverage assessment, automated report generation,\n    and statistical comparison of search strategies. Supports multiple export formats \n    including 'CSV', 'Excel', 'RIS', 'BibTeX', and 'EndNote'. Includes tools for duplicate \n    detection, search strategy optimization, cross-validation frameworks, meta-analysis \n    of benchmark results, power analysis for study design, and reproducibility package \n    creation. Optionally connects to 'PubMed' for direct database searching and real-time \n    strategy comparison using the 'E-utilities' 'API'. Enhanced with bootstrap comparison \n    methods, 'McNemar' test for strategy evaluation, and comprehensive visualization tools \n    for performance assessment. Methods based on Manning et al. (2008) \n    for information retrieval metrics, Moher et al. (2009)\n    for 'PRISMA' guidelines, and Sampson et al. (2006) for \n    systematic review search methodology.  "
  },
  {
    "id": 20357,
    "package_name": "selenium",
    "title": "Low-Level Browser Automation Interface",
    "description": "An implementation of 'W3C WebDriver 2.0'\n  (<https://w3c.github.io/webdriver/>), allowing interaction\n  with a 'Selenium Server' (<https://www.selenium.dev/documentation/grid/>)\n  instance from 'R'. Allows a web browser to be automated from 'R'.",
    "version": "0.2.0",
    "maintainer": "Ashby Thorpe <ashbythorpe@gmail.com>",
    "author": "Ashby Thorpe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3106-099X>)",
    "url": "https://ashbythorpe.github.io/selenium-r/,\nhttps://github.com/ashbythorpe/selenium-r",
    "bug_reports": "https://github.com/ashbythorpe/selenium-r/issues",
    "repository": "https://cran.r-project.org/package=selenium",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "selenium Low-Level Browser Automation Interface An implementation of 'W3C WebDriver 2.0'\n  (<https://w3c.github.io/webdriver/>), allowing interaction\n  with a 'Selenium Server' (<https://www.selenium.dev/documentation/grid/>)\n  instance from 'R'. Allows a web browser to be automated from 'R'.  "
  },
  {
    "id": 20395,
    "package_name": "sendmailR",
    "title": "Send Email Using R",
    "description": "Package contains a simple SMTP client with minimal dependencies which \n        provides a portable solution for sending email, including file attachments and inline html reports, \n        from within R. SMTP Authentication and SSL/STARTTLS is implemented using curl.",
    "version": "1.4-0",
    "maintainer": "Olaf Mersmann <olafm@p-value.net>",
    "author": "Olaf Mersmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7720-4939>),\n  Quinn Weber [ctb],\n  Marius Barth [ctb] (ORCID: <https://orcid.org/0000-0002-3421-6665>),\n  Are Edvardsen [ctb] (ORCID: <https://orcid.org/0000-0002-5210-3656>),\n  Alexander Bartel [ctb] (ORCID: <https://orcid.org/0000-0002-1280-6138>)",
    "url": "https://github.com/olafmersmann/sendmailR",
    "bug_reports": "https://github.com/olafmersmann/sendmailR/issues",
    "repository": "https://cran.r-project.org/package=sendmailR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sendmailR Send Email Using R Package contains a simple SMTP client with minimal dependencies which \n        provides a portable solution for sending email, including file attachments and inline html reports, \n        from within R. SMTP Authentication and SSL/STARTTLS is implemented using curl.  "
  },
  {
    "id": 20400,
    "package_name": "sensiPhy",
    "title": "Sensitivity Analysis for Comparative Methods",
    "description": "An implementation of sensitivity analysis for phylogenetic comparative\n methods. The package is an umbrella of statistical and graphical methods that \n estimate and report different types of uncertainty in PCM:\n (i) Species Sampling uncertainty (sample size; influential species and clades).\n (ii) Phylogenetic uncertainty (different topologies and/or branch lengths).\n (iii) Data uncertainty (intraspecific variation and measurement error).",
    "version": "0.8.5",
    "maintainer": "Gustavo Paterno <paternogbc@gmail.com>",
    "author": "Gustavo Paterno [cre, aut],\n  Gijsbert Werner [aut],\n  Caterina Penone [aut],\n  Pablo Martinez [ctb]",
    "url": "https://github.com/paternogbc/sensiPhy",
    "bug_reports": "https://github.com/paternogbc/sensiPhy/issues",
    "repository": "https://cran.r-project.org/package=sensiPhy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sensiPhy Sensitivity Analysis for Comparative Methods An implementation of sensitivity analysis for phylogenetic comparative\n methods. The package is an umbrella of statistical and graphical methods that \n estimate and report different types of uncertainty in PCM:\n (i) Species Sampling uncertainty (sample size; influential species and clades).\n (ii) Phylogenetic uncertainty (different topologies and/or branch lengths).\n (iii) Data uncertainty (intraspecific variation and measurement error).  "
  },
  {
    "id": 20425,
    "package_name": "seqHMM",
    "title": "Mixture Hidden Markov Models for Social Sequence Data and Other\nMultivariate, Multichannel Categorical Time Series",
    "description": "Designed for estimating variants of hidden (latent) Markov models\n    (HMMs), mixture HMMs, and non-homogeneous HMMs (NHMMs) for social sequence\n    data and other categorical time series. Special cases include\n    feedback-augmented NHMMs, Markov models without latent layer, mixture\n    Markov models, and latent class models. The package supports models for one\n    or multiple subjects with one or multiple parallel sequences (channels).\n    External covariates can be added to explain cluster membership in mixture \n    models as well as initial, transition and emission probabilities in NHMMs.\n    The package provides functions for evaluating and comparing models, as well\n    as functions for visualizing of multichannel sequence data and HMMs. For\n    NHMMs, methods for computing average causal effects and marginal state and\n    emission probabilities are available. Models are estimated using maximum\n    likelihood via the EM algorithm or direct numerical maximization with\n    analytical gradients. Documentation is available via several vignettes,\n    and Helske and Helske (2019, <doi:10.18637/jss.v088.i03>). For methodology\n    behind the NHMMs, see Helske (2025, <doi:10.48550/arXiv.2503.16014>).",
    "version": "2.1.0",
    "maintainer": "Jouni Helske <jouni.helske@iki.fi>",
    "author": "Jouni Helske [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7130-793X>),\n  Satu Helske [aut] (ORCID: <https://orcid.org/0000-0003-0532-0153>)",
    "url": "",
    "bug_reports": "https://github.com/helske/seqHMM/issues",
    "repository": "https://cran.r-project.org/package=seqHMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seqHMM Mixture Hidden Markov Models for Social Sequence Data and Other\nMultivariate, Multichannel Categorical Time Series Designed for estimating variants of hidden (latent) Markov models\n    (HMMs), mixture HMMs, and non-homogeneous HMMs (NHMMs) for social sequence\n    data and other categorical time series. Special cases include\n    feedback-augmented NHMMs, Markov models without latent layer, mixture\n    Markov models, and latent class models. The package supports models for one\n    or multiple subjects with one or multiple parallel sequences (channels).\n    External covariates can be added to explain cluster membership in mixture \n    models as well as initial, transition and emission probabilities in NHMMs.\n    The package provides functions for evaluating and comparing models, as well\n    as functions for visualizing of multichannel sequence data and HMMs. For\n    NHMMs, methods for computing average causal effects and marginal state and\n    emission probabilities are available. Models are estimated using maximum\n    likelihood via the EM algorithm or direct numerical maximization with\n    analytical gradients. Documentation is available via several vignettes,\n    and Helske and Helske (2019, <doi:10.18637/jss.v088.i03>). For methodology\n    behind the NHMMs, see Helske (2025, <doi:10.48550/arXiv.2503.16014>).  "
  },
  {
    "id": 20449,
    "package_name": "servr",
    "title": "A Simple HTTP Server to Serve Static Files or Dynamic Documents",
    "description": "Start an HTTP server in R to serve static files, or dynamic\n    documents that can be converted to HTML files (e.g., R Markdown) under a\n    given directory.",
    "version": "0.32",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\n  Carson Sievert [ctb],\n  Jesse Anderson [ctb],\n  Ramnath Vaidyanathan [ctb],\n  Romain Lesur [ctb],\n  Posit Software, PBC [cph, fnd]",
    "url": "https://github.com/yihui/servr",
    "bug_reports": "https://github.com/yihui/servr/issues",
    "repository": "https://cran.r-project.org/package=servr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "servr A Simple HTTP Server to Serve Static Files or Dynamic Documents Start an HTTP server in R to serve static files, or dynamic\n    documents that can be converted to HTML files (e.g., R Markdown) under a\n    given directory.  "
  },
  {
    "id": 20457,
    "package_name": "sevenbridges2",
    "title": "The 'Seven Bridges Platform' API Client",
    "description": "R client and utilities for 'Seven Bridges Platform' API, from 'Cancer Genomics Cloud' \n    to other 'Seven Bridges' supported platforms. API documentation is hosted publicly \n    at <https://docs.sevenbridges.com/docs/the-api>.",
    "version": "0.4.0",
    "maintainer": "Marko Trifunovic <marko.trifunovic@velsera.com>",
    "author": "Marko Trifunovic [aut, cre],\n  Marija Gacic [aut],\n  Vladimir Obucina [aut],\n  Velsera [cph, fnd]",
    "url": "https://www.sevenbridges.com,\nhttps://sbg.github.io/sevenbridges2/,\nhttps://github.com/sbg/sevenbridges2",
    "bug_reports": "https://github.com/sbg/sevenbridges2/issues",
    "repository": "https://cran.r-project.org/package=sevenbridges2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sevenbridges2 The 'Seven Bridges Platform' API Client R client and utilities for 'Seven Bridges Platform' API, from 'Cancer Genomics Cloud' \n    to other 'Seven Bridges' supported platforms. API documentation is hosted publicly \n    at <https://docs.sevenbridges.com/docs/the-api>.  "
  },
  {
    "id": 20482,
    "package_name": "sg",
    "title": "'SendGrid' Email API Client",
    "description": "Simple 'SendGrid' Email API client for creating and sending emails. \n    For more information, visit the official 'SendGrid' Email API documentation: <https://sendgrid.com/en-us/solutions/email-api>.",
    "version": "0.2.0",
    "maintainer": "Botan A\u011f\u0131n <aginbotan@gmail.com>",
    "author": "Botan A\u011f\u0131n [aut, cre]",
    "url": "https://github.com/botan/sg, https://botan.github.io/sg/",
    "bug_reports": "https://github.com/botan/sg/issues",
    "repository": "https://cran.r-project.org/package=sg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sg 'SendGrid' Email API Client Simple 'SendGrid' Email API client for creating and sending emails. \n    For more information, visit the official 'SendGrid' Email API documentation: <https://sendgrid.com/en-us/solutions/email-api>.  "
  },
  {
    "id": 20507,
    "package_name": "shadowr",
    "title": "Selenium Plugin to Manage Multi Level Shadow Elements on Web\nPage",
    "description": "Shadow Document Object Model is a web standard that offers component style and markup encapsulation. \n  It is a critically important piece of the Web Components story as it ensures that a component will work in any environment even if other CSS or JavaScript is at play on the page.\n  Custom HTML tags can't be directly identified with selenium tools, because Selenium doesn't provide any way to deal with shadow elements. \n  Using this plugin you can handle any custom HTML tags.",
    "version": "0.0.2",
    "maintainer": "Ricardo Landolt <ricardo.landolt1@gmail.com>",
    "author": "Ricardo Landolt [cre, aut],\n  Sushil Gupta [ctb] (shadow-automation-selenium plugin)",
    "url": "https://github.com/ricilandolt/shadowr",
    "bug_reports": "https://github.com/ricilandolt/shadowr/issues",
    "repository": "https://cran.r-project.org/package=shadowr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shadowr Selenium Plugin to Manage Multi Level Shadow Elements on Web\nPage Shadow Document Object Model is a web standard that offers component style and markup encapsulation. \n  It is a critically important piece of the Web Components story as it ensures that a component will work in any environment even if other CSS or JavaScript is at play on the page.\n  Custom HTML tags can't be directly identified with selenium tools, because Selenium doesn't provide any way to deal with shadow elements. \n  Using this plugin you can handle any custom HTML tags.  "
  },
  {
    "id": 20515,
    "package_name": "shapley",
    "title": "Weighted Mean SHAP and CI for Robust Feature Assessment in ML\nGrid",
    "description": "This R package introduces Weighted Mean SHapley Additive exPlanations (WMSHAP), an innovative method for calculating SHAP values for a grid of fine-tuned base-learner machine learning models as well as stacked ensembles, a method not previously available due to the common reliance on single best-performing models. By integrating the weighted mean SHAP values from individual base-learners comprising the ensemble or individual base-learners in a tuning grid search, the package weights SHAP contributions according to each model's performance, assessed by multiple either R squared (for both regression and classification models). alternatively, this software also offers weighting SHAP values based on the area under the precision-recall curve (AUCPR), the area under the curve (AUC), and F2 measures for binary classifiers. It further extends this framework to implement weighted confidence intervals for weighted mean SHAP values, offering a more comprehensive and robust feature importance evaluation over a grid of machine learning models, instead of solely computing SHAP values for the best model. This methodology is particularly beneficial for addressing the severe class imbalance (class rarity) problem by providing a transparent, generalized measure of feature importance that mitigates the risk of reporting SHAP values for an overfitted or biased model and maintains robustness under severe class imbalance, where there is no universal criteria of identifying the absolute best model. Furthermore, the package implements hypothesis testing to ascertain the statistical significance of SHAP values for individual features, as well as comparative significance testing of SHAP contributions between features. Additionally, it tackles a critical gap in feature selection literature by presenting criteria for the automatic feature selection of the most important features across a grid of models or stacked ensembles, eliminating the need for arbitrary determination of the number of top features to be extracted. This utility is invaluable for researchers analyzing feature significance, particularly within severely imbalanced outcomes where conventional methods fall short. Moreover, it is also expected to report democratic feature importance across a grid of models, resulting in a more comprehensive and generalizable feature selection. The package further implements a novel method for visualizing SHAP values both at subject level and feature level as well as a plot for feature selection based on the weighted mean SHAP ratios.",
    "version": "0.5.1",
    "maintainer": "E. F. Haghish <haghish@hotmail.com>",
    "author": "E. F. Haghish [aut, cre, cph]",
    "url": "https://github.com/haghish/shapley",
    "bug_reports": "https://github.com/haghish/shapley/issues",
    "repository": "https://cran.r-project.org/package=shapley",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shapley Weighted Mean SHAP and CI for Robust Feature Assessment in ML\nGrid This R package introduces Weighted Mean SHapley Additive exPlanations (WMSHAP), an innovative method for calculating SHAP values for a grid of fine-tuned base-learner machine learning models as well as stacked ensembles, a method not previously available due to the common reliance on single best-performing models. By integrating the weighted mean SHAP values from individual base-learners comprising the ensemble or individual base-learners in a tuning grid search, the package weights SHAP contributions according to each model's performance, assessed by multiple either R squared (for both regression and classification models). alternatively, this software also offers weighting SHAP values based on the area under the precision-recall curve (AUCPR), the area under the curve (AUC), and F2 measures for binary classifiers. It further extends this framework to implement weighted confidence intervals for weighted mean SHAP values, offering a more comprehensive and robust feature importance evaluation over a grid of machine learning models, instead of solely computing SHAP values for the best model. This methodology is particularly beneficial for addressing the severe class imbalance (class rarity) problem by providing a transparent, generalized measure of feature importance that mitigates the risk of reporting SHAP values for an overfitted or biased model and maintains robustness under severe class imbalance, where there is no universal criteria of identifying the absolute best model. Furthermore, the package implements hypothesis testing to ascertain the statistical significance of SHAP values for individual features, as well as comparative significance testing of SHAP contributions between features. Additionally, it tackles a critical gap in feature selection literature by presenting criteria for the automatic feature selection of the most important features across a grid of models or stacked ensembles, eliminating the need for arbitrary determination of the number of top features to be extracted. This utility is invaluable for researchers analyzing feature significance, particularly within severely imbalanced outcomes where conventional methods fall short. Moreover, it is also expected to report democratic feature importance across a grid of models, resulting in a more comprehensive and generalizable feature selection. The package further implements a novel method for visualizing SHAP values both at subject level and feature level as well as a plot for feature selection based on the weighted mean SHAP ratios.  "
  },
  {
    "id": 20523,
    "package_name": "sharpr2",
    "title": "Estimating Regulatory Scores and Identifying ATAC-STARR Data",
    "description": "An algorithm for identifying high-resolution driver elements for datasets from a high-definition reporter assay library. Xinchen Wang, Liang He, Sarah Goggin, Alham Saadat, Li Wang, Melina Claussnitzer, Manolis Kellis (2017) <doi:10.1101/193136>.",
    "version": "1.1.1.0",
    "maintainer": "Liang He <lianghe@mit.edu>",
    "author": "Liang He",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sharpr2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sharpr2 Estimating Regulatory Scores and Identifying ATAC-STARR Data An algorithm for identifying high-resolution driver elements for datasets from a high-definition reporter assay library. Xinchen Wang, Liang He, Sarah Goggin, Alham Saadat, Li Wang, Melina Claussnitzer, Manolis Kellis (2017) <doi:10.1101/193136>.  "
  },
  {
    "id": 20524,
    "package_name": "sharpshootR",
    "title": "A Soil Survey Toolkit",
    "description": "A collection of data processing, visualization, and export functions to support soil survey operations. Many of the functions build on the `SoilProfileCollection` S4 class provided by the aqp package, extending baseline visualization to more elaborate depictions in the context of spatial and taxonomic data. While this package is primarily developed by and for the USDA-NRCS, in support of the National Cooperative Soil Survey, the authors strive for generalization sufficient to support any soil survey operation. Many of the included functions are used by the SoilWeb suite of websites and movile applications. These functions are provided here, with additional documentation, to enable others to replicate high quality versions of these figures for their own purposes.",
    "version": "2.4",
    "maintainer": "Dylan Beaudette <dylan.beaudette@usda.gov>",
    "author": "Dylan Beaudette [cre, aut],\n  Jay Skovlin [aut],\n  Stephen Roecker [aut],\n  Andrew Brown [aut],\n  USDA-NRCS Soil Survey Staff [ctb]",
    "url": "https://github.com/ncss-tech/sharpshootR",
    "bug_reports": "https://github.com/ncss-tech/sharpshootR/issues",
    "repository": "https://cran.r-project.org/package=sharpshootR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sharpshootR A Soil Survey Toolkit A collection of data processing, visualization, and export functions to support soil survey operations. Many of the functions build on the `SoilProfileCollection` S4 class provided by the aqp package, extending baseline visualization to more elaborate depictions in the context of spatial and taxonomic data. While this package is primarily developed by and for the USDA-NRCS, in support of the National Cooperative Soil Survey, the authors strive for generalization sufficient to support any soil survey operation. Many of the included functions are used by the SoilWeb suite of websites and movile applications. These functions are provided here, with additional documentation, to enable others to replicate high quality versions of these figures for their own purposes.  "
  },
  {
    "id": 20541,
    "package_name": "shiny.i18n",
    "title": "Shiny Applications Internationalization",
    "description": "It provides easy internationalization of Shiny\n    applications. It can be used as standalone translation package\n    to translate reports, interactive visualizations or\n    graphical elements as well.",
    "version": "0.3.0",
    "maintainer": "Jakub Nowicki <opensource+kuba@appsilon.com>",
    "author": "Jakub Nowicki [cre, aut],\n  Dominik Krzemi\u0144ski [aut],\n  Krystian Igras [aut],\n  Jakub Sobolewski [aut],\n  Appsilon Sp. z o.o. [cph]",
    "url": "https://appsilon.github.io/shiny.i18n/,\nhttps://github.com/Appsilon/shiny.i18n",
    "bug_reports": "https://github.com/Appsilon/shiny.i18n/issues",
    "repository": "https://cran.r-project.org/package=shiny.i18n",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shiny.i18n Shiny Applications Internationalization It provides easy internationalization of Shiny\n    applications. It can be used as standalone translation package\n    to translate reports, interactive visualizations or\n    graphical elements as well.  "
  },
  {
    "id": 20571,
    "package_name": "shinyInvoice",
    "title": "Shiny App - Generate a Pdf Invoice with 'Rmarkdown'",
    "description": "Generate an invoice containing a header with invoice number and businesses details. The invoice table contains\n  any of: salary, one-liner costs, grouped costs. Under the table signature and bank account details appear. \n  Pages are numbered when more than one. Source .json and .Rmd files are editable in the app. A .csv file with raw data can be downloaded.\n  This package includes functions for getting exchange rates between currencies based on \n  'quantmod' (Ryan and Ulrich, 2023 <https://CRAN.R-project.org/package=quantmod>).",
    "version": "0.0.5",
    "maintainer": "Fernando Roa <froao@unal.edu.co>",
    "author": "Fernando Roa [aut, cre]",
    "url": "https://github.com/fernandoroa/invoice-public",
    "bug_reports": "https://github.com/fernandoroa/invoice-public/issues",
    "repository": "https://cran.r-project.org/package=shinyInvoice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinyInvoice Shiny App - Generate a Pdf Invoice with 'Rmarkdown' Generate an invoice containing a header with invoice number and businesses details. The invoice table contains\n  any of: salary, one-liner costs, grouped costs. Under the table signature and bank account details appear. \n  Pages are numbered when more than one. Source .json and .Rmd files are editable in the app. A .csv file with raw data can be downloaded.\n  This package includes functions for getting exchange rates between currencies based on \n  'quantmod' (Ryan and Ulrich, 2023 <https://CRAN.R-project.org/package=quantmod>).  "
  },
  {
    "id": 20592,
    "package_name": "shinySbm",
    "title": "'shiny' Application to Use the Stochastic Block Model",
    "description": "A 'shiny' interface for a simpler use of the 'sbm' R package. \n    It also contains useful functions to easily explore the 'sbm' package results. \n    With this package you should be able to use the stochastic block model \n    without any knowledge in R, get automatic reports and nice visuals, as \n    well as learning the basic functions of 'sbm'.",
    "version": "0.1.5",
    "maintainer": "Theodore Vanrenterghem <shiny.sbm.dev@gmail.com>",
    "author": "Theodore Vanrenterghem [cre, aut],\n  Julie Aubert [aut] (ORCID: <https://orcid.org/0000-0001-5203-5748>),\n  Saint-Clair Chabert-Liddell [aut] (ORCID:\n    <https://orcid.org/0000-0001-5604-7308>),\n  gro\u00dfBM team [ctb],\n  Golem User [cph]",
    "url": "",
    "bug_reports": "https://github.com/Jo-Theo/shinySbm/issues",
    "repository": "https://cran.r-project.org/package=shinySbm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinySbm 'shiny' Application to Use the Stochastic Block Model A 'shiny' interface for a simpler use of the 'sbm' R package. \n    It also contains useful functions to easily explore the 'sbm' package results. \n    With this package you should be able to use the stochastic block model \n    without any knowledge in R, get automatic reports and nice visuals, as \n    well as learning the basic functions of 'sbm'.  "
  },
  {
    "id": 20607,
    "package_name": "shinybody",
    "title": "An Interactive Anatomography Widget for 'shiny'",
    "description": "An 'htmlwidget' of the human body that allows you\n    to hide/show and assign colors to 79 different body parts. The 'human'\n    widget is an 'htmlwidget', so it works in Quarto documents,\n    R Markdown documents, or any other HTML medium.\n    It also functions as an input/output widget in a 'shiny' app.",
    "version": "0.1.3",
    "maintainer": "Robert Norberg <Robert.Norberg@moffitt.org>",
    "author": "Robert Norberg [aut, cre],\n  Sebastian Zapata-Tamayo [aut, ctb],\n  Mehrun Huda [aut, ctb] (ORCID: <https://orcid.org/0000-0002-4951-8906>),\n  Moffitt Cancer Center [cph]",
    "url": "https://github.com/robert-norberg/shinybody",
    "bug_reports": "https://github.com/robert-norberg/shinybody/issues",
    "repository": "https://cran.r-project.org/package=shinybody",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinybody An Interactive Anatomography Widget for 'shiny' An 'htmlwidget' of the human body that allows you\n    to hide/show and assign colors to 79 different body parts. The 'human'\n    widget is an 'htmlwidget', so it works in Quarto documents,\n    R Markdown documents, or any other HTML medium.\n    It also functions as an input/output widget in a 'shiny' app.  "
  },
  {
    "id": 20619,
    "package_name": "shinyhelper",
    "title": "Easily Add Markdown Help Files to 'shiny' App Elements",
    "description": "Creates a lightweight way to add markdown helpfiles to 'shiny' apps,\n    using modal dialog boxes, with no need to observe each help button separately.",
    "version": "0.3.2",
    "maintainer": "Chris Mason-Thom <christopher.w.thom@outlook.com>",
    "author": "Chris Mason-Thom [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/cwthom/shinyhelper/issues",
    "repository": "https://cran.r-project.org/package=shinyhelper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinyhelper Easily Add Markdown Help Files to 'shiny' App Elements Creates a lightweight way to add markdown helpfiles to 'shiny' apps,\n    using modal dialog boxes, with no need to observe each help button separately.  "
  },
  {
    "id": 20627,
    "package_name": "shinymgr",
    "title": "A Framework for Building, Managing, and Stitching 'shiny'\nModules into Reproducible Workflows",
    "description": "A unifying framework for managing and deploying 'shiny' applications \n    that consist of modules, where an \"app\" is a tab-based workflow that guides \n    a user step-by-step through an analysis. The 'shinymgr' app builder \n    \"stitches\" 'shiny' modules together so that outputs from one module serve as \n    inputs to the next, creating an analysis pipeline that is easy to implement \n    and maintain. Users of 'shinymgr' apps can save analyses as an RDS file that\n    fully reproduces the analytic steps and can be ingested into an R Markdown \n    report for rapid reporting. In short, developers use the 'shinymgr' \n    framework to write modules and seamlessly combine them into 'shiny' apps, and \n    users of these apps can execute reproducible analyses that can be \n    incorporated into reports for rapid dissemination. ",
    "version": "1.1.0",
    "maintainer": "Laurence Clarfeld <laurence.clarfeld@uvm.edu>",
    "author": "Laurence Clarfeld [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3927-9411>),\n  Caroline Tang [aut] (ORCID: <https://orcid.org/0000-0001-7966-5854>),\n  Therese Donovan [aut, org, rth] (ORCID:\n    <https://orcid.org/0000-0001-8124-9251>)",
    "url": "https://code.usgs.gov/vtcfwru/shinymgr",
    "bug_reports": "https://code.usgs.gov/vtcfwru/shinymgr/-/issues",
    "repository": "https://cran.r-project.org/package=shinymgr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinymgr A Framework for Building, Managing, and Stitching 'shiny'\nModules into Reproducible Workflows A unifying framework for managing and deploying 'shiny' applications \n    that consist of modules, where an \"app\" is a tab-based workflow that guides \n    a user step-by-step through an analysis. The 'shinymgr' app builder \n    \"stitches\" 'shiny' modules together so that outputs from one module serve as \n    inputs to the next, creating an analysis pipeline that is easy to implement \n    and maintain. Users of 'shinymgr' apps can save analyses as an RDS file that\n    fully reproduces the analytic steps and can be ingested into an R Markdown \n    report for rapid reporting. In short, developers use the 'shinymgr' \n    framework to write modules and seamlessly combine them into 'shiny' apps, and \n    users of these apps can execute reproducible analyses that can be \n    incorporated into reports for rapid dissemination.   "
  },
  {
    "id": 20674,
    "package_name": "sievetest",
    "title": "Laboratory Sieve Test Reporting Functions",
    "description": "Functions for making particle-size analysis. Sieve tests are widely used to obtain particle-size distribution of powders or granular materials.",
    "version": "1.2.3",
    "maintainer": "Petr Matousu <pmatousu@more-praha.cz>",
    "author": "Petr Matousu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sievetest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sievetest Laboratory Sieve Test Reporting Functions Functions for making particle-size analysis. Sieve tests are widely used to obtain particle-size distribution of powders or granular materials.  "
  },
  {
    "id": 20689,
    "package_name": "sigr",
    "title": "Succinct and Correct Statistical Summaries for Reports",
    "description": "Succinctly and correctly format statistical summaries of\n    various models and tests (F-test, Chi-Sq-test, Fisher-test, T-test, and rank-significance). \n    This package also includes empirical tests, such as Monte Carlo and bootstrap distribution estimates.",
    "version": "1.1.5",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Nina Zumel [aut],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/sigr/,\nhttps://winvector.github.io/sigr/",
    "bug_reports": "https://github.com/WinVector/sigr/issues",
    "repository": "https://cran.r-project.org/package=sigr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sigr Succinct and Correct Statistical Summaries for Reports Succinctly and correctly format statistical summaries of\n    various models and tests (F-test, Chi-Sq-test, Fisher-test, T-test, and rank-significance). \n    This package also includes empirical tests, such as Monte Carlo and bootstrap distribution estimates.  "
  },
  {
    "id": 20718,
    "package_name": "simaerep",
    "title": "Detect Clinical Trial Sites Over- or Under-Reporting Clinical\nEvents",
    "description": "Monitoring reporting rates of subject-level clinical events (e.g.\n  adverse events, protocol deviations) reported by clinical trial sites is an\n  important aspect of risk-based quality monitoring strategy. Sites that are \n  under-reporting or over-reporting events can be detected using bootstrap\n  simulations during which patients are redistributed between sites. Site-specific\n  distributions of event reporting rates  are generated that are used to assign\n  probabilities to the observed reporting rates.\n  (Koneswarakantha 2024 <doi:10.1007/s43441-024-00631-8>).",
    "version": "1.0.0",
    "maintainer": "Bjoern Koneswarakantha <bjoern.koneswarakantha@roche.com>",
    "author": "Bjoern Koneswarakantha [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4585-7799>),\n  F. Hoffmann-La Roche Ltd [cph]",
    "url": "https://openpharma.github.io/simaerep/,\nhttps://github.com/openpharma/simaerep/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simaerep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simaerep Detect Clinical Trial Sites Over- or Under-Reporting Clinical\nEvents Monitoring reporting rates of subject-level clinical events (e.g.\n  adverse events, protocol deviations) reported by clinical trial sites is an\n  important aspect of risk-based quality monitoring strategy. Sites that are \n  under-reporting or over-reporting events can be detected using bootstrap\n  simulations during which patients are redistributed between sites. Site-specific\n  distributions of event reporting rates  are generated that are used to assign\n  probabilities to the observed reporting rates.\n  (Koneswarakantha 2024 <doi:10.1007/s43441-024-00631-8>).  "
  },
  {
    "id": 20720,
    "package_name": "simcausal",
    "title": "Simulating Longitudinal Data with Causal Inference Applications",
    "description": "A flexible tool for simulating complex longitudinal data using\n    structural equations, with emphasis on problems in causal inference.\n    Specify interventions and simulate from intervened data generating\n    distributions. Define and evaluate treatment-specific means, the average\n    treatment effects and coefficients from working marginal structural models.\n    User interface designed to facilitate the conduct of transparent and\n    reproducible simulation studies, and allows concise expression of complex\n    functional dependencies for a large number of time-varying nodes. See the\n    package vignette for more information, documentation and examples.",
    "version": "0.5.7",
    "maintainer": "Fred Gruber <fgruber@gmail.com>",
    "author": "Oleg Sofrygin [aut],\n  Mark J. van der Laan [aut],\n  Romain Neugebauer [aut],\n  Fred Gruber [ctb, cre]",
    "url": "https://github.com/osofr/simcausal",
    "bug_reports": "https://github.com/osofr/simcausal/issues",
    "repository": "https://cran.r-project.org/package=simcausal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simcausal Simulating Longitudinal Data with Causal Inference Applications A flexible tool for simulating complex longitudinal data using\n    structural equations, with emphasis on problems in causal inference.\n    Specify interventions and simulate from intervened data generating\n    distributions. Define and evaluate treatment-specific means, the average\n    treatment effects and coefficients from working marginal structural models.\n    User interface designed to facilitate the conduct of transparent and\n    reproducible simulation studies, and allows concise expression of complex\n    functional dependencies for a large number of time-varying nodes. See the\n    package vignette for more information, documentation and examples.  "
  },
  {
    "id": 20738,
    "package_name": "simmer",
    "title": "Discrete-Event Simulation for R",
    "description": "A process-oriented and trajectory-based Discrete-Event Simulation\n    (DES) package for R. It is designed as a generic yet powerful framework. The\n    architecture encloses a robust and fast simulation core written in 'C++' with\n    automatic monitoring capabilities. It provides a rich and flexible R API that\n    revolves around the concept of trajectory, a common path in the simulation\n    model for entities of the same type. Documentation about 'simmer' is provided\n    by several vignettes included in this package, via the paper by Ucar, Smeets\n    & Azcorra (2019, <doi:10.18637/jss.v090.i02>), and the paper by Ucar,\n    Hern\u00e1ndez, Serrano & Azcorra (2018, <doi:10.1109/MCOM.2018.1700960>);\n    see 'citation(\"simmer\")' for details.",
    "version": "4.4.7",
    "maintainer": "I\u00f1aki Ucar <iucar@fedoraproject.org>",
    "author": "I\u00f1aki Ucar [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-6403-5550>),\n  Bart Smeets [aut, cph]",
    "url": "https://r-simmer.org, https://github.com/r-simmer/simmer",
    "bug_reports": "https://github.com/r-simmer/simmer/issues",
    "repository": "https://cran.r-project.org/package=simmer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simmer Discrete-Event Simulation for R A process-oriented and trajectory-based Discrete-Event Simulation\n    (DES) package for R. It is designed as a generic yet powerful framework. The\n    architecture encloses a robust and fast simulation core written in 'C++' with\n    automatic monitoring capabilities. It provides a rich and flexible R API that\n    revolves around the concept of trajectory, a common path in the simulation\n    model for entities of the same type. Documentation about 'simmer' is provided\n    by several vignettes included in this package, via the paper by Ucar, Smeets\n    & Azcorra (2019, <doi:10.18637/jss.v090.i02>), and the paper by Ucar,\n    Hern\u00e1ndez, Serrano & Azcorra (2018, <doi:10.1109/MCOM.2018.1700960>);\n    see 'citation(\"simmer\")' for details.  "
  },
  {
    "id": 20759,
    "package_name": "simplermarkdown",
    "title": "Simple Engine for Generating Reports using R",
    "description": "Runs R-code present in a pandoc markdown file and \n  includes the resulting output in the resulting markdown file. This\n  file can then be converted into any of the output formats \n  supported by pandoc. The package can also be used as an engine\n  for writing package vignettes. ",
    "version": "0.0.6",
    "maintainer": "Jan van der Laan <r@eoos.dds.nl>",
    "author": "Jan van der Laan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0693-1514>)",
    "url": "https://github.com/djvanderlaan/simplermarkdown",
    "bug_reports": "https://github.com/djvanderlaan/simplermarkdown/issues",
    "repository": "https://cran.r-project.org/package=simplermarkdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simplermarkdown Simple Engine for Generating Reports using R Runs R-code present in a pandoc markdown file and \n  includes the resulting output in the resulting markdown file. This\n  file can then be converted into any of the output formats \n  supported by pandoc. The package can also be used as an engine\n  for writing package vignettes.   "
  },
  {
    "id": 20760,
    "package_name": "simpletex",
    "title": "Mathematical Formulas and Character Recognition",
    "description": "By calling the 'SimpleTex' <https://simpletex.cn/> open API implements \n    text and mathematical formula recognition on the image, and the output formula \n    can be used directly with 'Markdown' and 'LaTeX'.",
    "version": "1.0.5",
    "maintainer": "Xinyuan Chu <chuxinyuan@outlook.com>",
    "author": "Xinyuan Chu [aut, cre]",
    "url": "https://github.com/chuxinyuan/simpletex",
    "bug_reports": "https://github.com/chuxinyuan/simpletex/issues",
    "repository": "https://cran.r-project.org/package=simpletex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simpletex Mathematical Formulas and Character Recognition By calling the 'SimpleTex' <https://simpletex.cn/> open API implements \n    text and mathematical formula recognition on the image, and the output formula \n    can be used directly with 'Markdown' and 'LaTeX'.  "
  },
  {
    "id": 20770,
    "package_name": "sims",
    "title": "Simulate Data from R or 'JAGS' Code",
    "description": "Generates data from R or 'JAGS' code for use in simulation\n    studies.  The data are returned as an 'nlist::nlists' object and/or\n    saved to file as individual '.rds' files.  Parallelization is\n    implemented using the 'future' package.  Progress is reported using\n    the 'progressr' package.",
    "version": "0.0.4",
    "maintainer": "Audrey Beliveau <audrey.beliveau@uwaterloo.ca>",
    "author": "Audrey Beliveau [aut, cre],\n  Joe Thorley [aut] (ORCID: <https://orcid.org/0000-0002-7683-4592>)",
    "url": "https://github.com/poissonconsulting/sims,\nhttps://poissonconsulting.github.io/sims/",
    "bug_reports": "https://github.com/poissonconsulting/sims/issues",
    "repository": "https://cran.r-project.org/package=sims",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sims Simulate Data from R or 'JAGS' Code Generates data from R or 'JAGS' code for use in simulation\n    studies.  The data are returned as an 'nlist::nlists' object and/or\n    saved to file as individual '.rds' files.  Parallelization is\n    implemented using the 'future' package.  Progress is reported using\n    the 'progressr' package.  "
  },
  {
    "id": 20790,
    "package_name": "sinew",
    "title": "Package Development Documentation and Namespace Management",
    "description": "Manage package documentation and namespaces from the command line. \n             Programmatically attach namespaces in R and Rmd script, populates \n             'Roxygen2' skeletons with information scraped from within functions and \n             populate the Imports field of the DESCRIPTION file.",
    "version": "0.4.0",
    "maintainer": "Jonathan Sidi <yonicd@gmail.com>",
    "author": "Jonathan Sidi [aut, cre],\n  Anton Grishin [ctb],\n  Lorenzo Busetto [ctb],\n  Alexey Shiklomanov [ctb],\n  Stephen Holsenbeck [ctb]",
    "url": "https://github.com/yonicd/sinew",
    "bug_reports": "https://github.com/yonicd/sinew/issues",
    "repository": "https://cran.r-project.org/package=sinew",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sinew Package Development Documentation and Namespace Management Manage package documentation and namespaces from the command line. \n             Programmatically attach namespaces in R and Rmd script, populates \n             'Roxygen2' skeletons with information scraped from within functions and \n             populate the Imports field of the DESCRIPTION file.  "
  },
  {
    "id": 20793,
    "package_name": "singleCellHaystack",
    "title": "A Universal Differential Expression Prediction Tool for\nSingle-Cell and Spatial Genomics Data",
    "description": "One key exploratory analysis step in single-cell genomics data analysis\n    is the prediction of features with different activity levels. For example, we want \n    to predict differentially expressed genes (DEGs) in single-cell RNA-seq data, \n    spatial DEGs in spatial transcriptomics data, or differentially accessible \n    regions (DARs) in single-cell ATAC-seq data. 'singleCellHaystack' predicts differentially\n    active features in single cell omics datasets without relying on the clustering\n    of cells into arbitrary clusters. 'singleCellHaystack' uses Kullback-Leibler \n    divergence to find features (e.g., genes, genomic regions, etc) that are active\n    in subsets of cells that are non-randomly positioned inside an input space (such as \n    1D trajectories, 2D tissue sections, multi-dimensional embeddings, etc). For \n    the theoretical background of 'singleCellHaystack' we refer to our original paper\n    Vandenbon and Diez (Nature Communications, 2020) <doi:10.1038/s41467-020-17900-3>\n    and our update Vandenbon and Diez (Scientific Reports, 2023) <doi:10.1038/s41598-023-38965-2>.",
    "version": "1.0.3",
    "maintainer": "Alexis Vandenbon <alexis.vandenbon@gmail.com>",
    "author": "Alexis Vandenbon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2180-5732>),\n  Diego Diez [aut] (ORCID: <https://orcid.org/0000-0002-2325-4893>)",
    "url": "https://alexisvdb.github.io/singleCellHaystack/,\nhttps://github.com/alexisvdb/singleCellHaystack",
    "bug_reports": "https://github.com/alexisvdb/singleCellHaystack/issues",
    "repository": "https://cran.r-project.org/package=singleCellHaystack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "singleCellHaystack A Universal Differential Expression Prediction Tool for\nSingle-Cell and Spatial Genomics Data One key exploratory analysis step in single-cell genomics data analysis\n    is the prediction of features with different activity levels. For example, we want \n    to predict differentially expressed genes (DEGs) in single-cell RNA-seq data, \n    spatial DEGs in spatial transcriptomics data, or differentially accessible \n    regions (DARs) in single-cell ATAC-seq data. 'singleCellHaystack' predicts differentially\n    active features in single cell omics datasets without relying on the clustering\n    of cells into arbitrary clusters. 'singleCellHaystack' uses Kullback-Leibler \n    divergence to find features (e.g., genes, genomic regions, etc) that are active\n    in subsets of cells that are non-randomly positioned inside an input space (such as \n    1D trajectories, 2D tissue sections, multi-dimensional embeddings, etc). For \n    the theoretical background of 'singleCellHaystack' we refer to our original paper\n    Vandenbon and Diez (Nature Communications, 2020) <doi:10.1038/s41467-020-17900-3>\n    and our update Vandenbon and Diez (Scientific Reports, 2023) <doi:10.1038/s41598-023-38965-2>.  "
  },
  {
    "id": 20809,
    "package_name": "siteymlgen",
    "title": "Automatically Generate _site.yml File for 'R Markdown'",
    "description": "The goal of 'siteymlgen' is to make it easy to organise\n  the building of your 'R Markdown' website.\n  The init() function placed within the first code chunk of the index.Rmd\n  file of an 'R' project directory will initiate the generation of an automatically\n  written _site.yml file. 'siteymlgen' recommends a specific naming\n  convention for your 'R Markdown' files. This naming will ensure that\n  your navbar layout is ordered according to a hierarchy.",
    "version": "1.0.0",
    "maintainer": "Adam Cribbs <adam.cribbs@ndorms.ox.ac.uk>",
    "author": "Adam Cribbs [aut, cre] (ORCID: <https://orcid.org/0000-0001-5288-3077>)",
    "url": "https://github.com/Acribbs/siteymlgen",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=siteymlgen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "siteymlgen Automatically Generate _site.yml File for 'R Markdown' The goal of 'siteymlgen' is to make it easy to organise\n  the building of your 'R Markdown' website.\n  The init() function placed within the first code chunk of the index.Rmd\n  file of an 'R' project directory will initiate the generation of an automatically\n  written _site.yml file. 'siteymlgen' recommends a specific naming\n  convention for your 'R Markdown' files. This naming will ensure that\n  your navbar layout is ordered according to a hierarchy.  "
  },
  {
    "id": 20823,
    "package_name": "sjtable2df",
    "title": "Convert 'sjPlot' HTML-Tables to R 'data.frame'",
    "description": "A small set of helper functions to convert 'sjPlot'\n    HTML-tables to R data.frame objects / knitr::kable-tables.",
    "version": "0.0.4",
    "maintainer": "Lorenz A. Kapsner <lorenz.kapsner@gmail.com>",
    "author": "Lorenz A. Kapsner [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0003-1866-860X>)",
    "url": "https://github.com/kapsner/sjtable2df",
    "bug_reports": "https://github.com/kapsner/sjtable2df/issues",
    "repository": "https://cran.r-project.org/package=sjtable2df",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sjtable2df Convert 'sjPlot' HTML-Tables to R 'data.frame' A small set of helper functions to convert 'sjPlot'\n    HTML-tables to R data.frame objects / knitr::kable-tables.  "
  },
  {
    "id": 20828,
    "package_name": "sketch",
    "title": "Interactive Sketches",
    "description": "Creates static / animated / interactive visualisations embeddable\n    in R Markdown documents. It implements an R-to-JavaScript transpiler and\n    enables users to write JavaScript applications using the syntax of R.",
    "version": "1.1.20.3",
    "maintainer": "Chun Fung Kwok <jkwok@svi.edu.au>",
    "author": "Chun Fung Kwok [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0716-3879>),\n  Kate Saunders [ctb]",
    "url": "",
    "bug_reports": "https://github.com/kcf-jackson/sketch",
    "repository": "https://cran.r-project.org/package=sketch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sketch Interactive Sketches Creates static / animated / interactive visualisations embeddable\n    in R Markdown documents. It implements an R-to-JavaScript transpiler and\n    enables users to write JavaScript applications using the syntax of R.  "
  },
  {
    "id": 20837,
    "package_name": "skilljaR",
    "title": "Connect to Your 'Skilljar' Data",
    "description": "Functions that simplify calls to the 'Skilljar' API. See \n    <https://api.skilljar.com/docs/> for documentation on the 'Skilljar' API.\n    This package is not supported by 'Skilljar'.",
    "version": "0.1.2",
    "maintainer": "Chris Umphlett <christopher.umphlett@gmail.com>",
    "author": "Chris Umphlett [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=skilljaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "skilljaR Connect to Your 'Skilljar' Data Functions that simplify calls to the 'Skilljar' API. See \n    <https://api.skilljar.com/docs/> for documentation on the 'Skilljar' API.\n    This package is not supported by 'Skilljar'.  "
  },
  {
    "id": 20865,
    "package_name": "slowraker",
    "title": "A Slow Version of the Rapid Automatic Keyword Extraction (RAKE)\nAlgorithm",
    "description": "A mostly pure-R implementation of the RAKE algorithm (Rose, S., Engel, D., Cramer, N. and Cowley, W. (2010) <doi:10.1002/9780470689646.ch1>), which can be used to extract keywords from documents without any training data.",
    "version": "0.1.1",
    "maintainer": "Christopher Baker <chriscrewbaker@gmail.com>",
    "author": "Christopher Baker [aut, cre]",
    "url": "https://crew102.github.io/slowraker/index.html",
    "bug_reports": "https://github.com/crew102/slowraker/issues",
    "repository": "https://cran.r-project.org/package=slowraker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "slowraker A Slow Version of the Rapid Automatic Keyword Extraction (RAKE)\nAlgorithm A mostly pure-R implementation of the RAKE algorithm (Rose, S., Engel, D., Cramer, N. and Cowley, W. (2010) <doi:10.1002/9780470689646.ch1>), which can be used to extract keywords from documents without any training data.  "
  },
  {
    "id": 20875,
    "package_name": "smallsets",
    "title": "Visual Documentation for Data Preprocessing",
    "description": "Data practitioners regularly use the 'R' and 'Python' programming languages to \n    prepare data for analyses. Thus, they encode important data preprocessing decisions in \n    'R' and 'Python' code. The 'smallsets' package subsequently decodes these decisions into \n    a Smallset Timeline, a static, compact visualisation of data preprocessing decisions \n    (Lucchesi et al. (2022) <doi:10.1145/3531146.3533175>). The visualisation consists of \n    small data snapshots of different preprocessing steps. The 'smallsets' package builds this \n    visualisation from a user's dataset and preprocessing code located in an 'R', 'R Markdown', \n    'Python', or 'Jupyter Notebook' file. Users simply add structured comments with snapshot  \n    instructions to the preprocessing code. One optional feature in 'smallsets' requires \n    installation of the 'Gurobi' optimisation software and 'gurobi' 'R' package, available \n    from <https://www.gurobi.com>. More information regarding the optional feature and \n    'gurobi' installation can be found in the 'smallsets' vignette.",
    "version": "2.0.0",
    "maintainer": "Lydia R. Lucchesi <Lydia.Lucchesi@anu.edu.au>",
    "author": "Lydia R. Lucchesi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1901-4301>),\n  Petra M. Kuhnert [ths],\n  Jenny L. Davis [ths],\n  Lexing Xie [ths]",
    "url": "https://lydialucchesi.github.io/smallsets/,\nhttps://github.com/lydialucchesi/smallsets",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=smallsets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smallsets Visual Documentation for Data Preprocessing Data practitioners regularly use the 'R' and 'Python' programming languages to \n    prepare data for analyses. Thus, they encode important data preprocessing decisions in \n    'R' and 'Python' code. The 'smallsets' package subsequently decodes these decisions into \n    a Smallset Timeline, a static, compact visualisation of data preprocessing decisions \n    (Lucchesi et al. (2022) <doi:10.1145/3531146.3533175>). The visualisation consists of \n    small data snapshots of different preprocessing steps. The 'smallsets' package builds this \n    visualisation from a user's dataset and preprocessing code located in an 'R', 'R Markdown', \n    'Python', or 'Jupyter Notebook' file. Users simply add structured comments with snapshot  \n    instructions to the preprocessing code. One optional feature in 'smallsets' requires \n    installation of the 'Gurobi' optimisation software and 'gurobi' 'R' package, available \n    from <https://www.gurobi.com>. More information regarding the optional feature and \n    'gurobi' installation can be found in the 'smallsets' vignette.  "
  },
  {
    "id": 20969,
    "package_name": "socialrisk",
    "title": "Identifying Patient Social Risk from Administrative Health Care\nData",
    "description": "Social risks are increasingly becoming a critical component of health\n    care research. One of the most common ways to identify social needs is by using\n    ICD-10-CM \"Z-codes.\" This package identifies social risks using varying taxonomies\n    of ICD-10-CM Z-codes from administrative health care data. The conceptual\n    taxonomies come from:\n    Centers for Medicare and Medicaid Services (2021) <https://www.cms.gov/files/document/zcodes-infographic.pdf>,\n    Reidhead (2018) <https://web.mhanet.com/>,\n    A Arons, S DeSilvey, C Fichtenberg, L Gottlieb (2018) <https://sirenetwork.ucsf.edu/tools-resources/resources/compendium-medical-terminology-codes-social-risk-factors>.",
    "version": "0.5.1",
    "maintainer": "Wyatt Bensken <wpb27@case.edu>",
    "author": "Wyatt Bensken [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2597-9732>)",
    "url": "https://github.com/WYATTBENSKEN/multimorbidity",
    "bug_reports": "https://github.com/WYATTBENSKEN/multimorbidity/issues",
    "repository": "https://cran.r-project.org/package=socialrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "socialrisk Identifying Patient Social Risk from Administrative Health Care\nData Social risks are increasingly becoming a critical component of health\n    care research. One of the most common ways to identify social needs is by using\n    ICD-10-CM \"Z-codes.\" This package identifies social risks using varying taxonomies\n    of ICD-10-CM Z-codes from administrative health care data. The conceptual\n    taxonomies come from:\n    Centers for Medicare and Medicaid Services (2021) <https://www.cms.gov/files/document/zcodes-infographic.pdf>,\n    Reidhead (2018) <https://web.mhanet.com/>,\n    A Arons, S DeSilvey, C Fichtenberg, L Gottlieb (2018) <https://sirenetwork.ucsf.edu/tools-resources/resources/compendium-medical-terminology-codes-social-risk-factors>.  "
  },
  {
    "id": 20990,
    "package_name": "solrium",
    "title": "General Purpose R Interface to 'Solr'",
    "description": "Provides a set of functions for querying and parsing data\n    from 'Solr' (<https://solr.apache.org/>) 'endpoints' (local and\n    remote), including search, 'faceting', 'highlighting', 'stats', and\n    'more like this'. In addition, some functionality is included for\n    creating, deleting, and updating documents in a 'Solr' 'database'.",
    "version": "1.2.0",
    "maintainer": "Scott Chamberlain <myrmecocystus@gmail.com>",
    "author": "Scott Chamberlain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1444-9135>),\n  rOpenSci [fnd] (https://ropensci.org/)",
    "url": "https://github.com/ropensci/solrium (devel),\nhttps://docs.ropensci.org/solrium/ (user manual)",
    "bug_reports": "https://github.com/ropensci/solrium/issues",
    "repository": "https://cran.r-project.org/package=solrium",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "solrium General Purpose R Interface to 'Solr' Provides a set of functions for querying and parsing data\n    from 'Solr' (<https://solr.apache.org/>) 'endpoints' (local and\n    remote), including search, 'faceting', 'highlighting', 'stats', and\n    'more like this'. In addition, some functionality is included for\n    creating, deleting, and updating documents in a 'Solr' 'database'.  "
  },
  {
    "id": 20997,
    "package_name": "somhca",
    "title": "Self-Organising Maps Coupled with Hierarchical Cluster Analysis",
    "description": "Implements self-organising maps combined with hierarchical cluster analysis (SOM-HCA) for clustering and visualization of high-dimensional data.\n    The package includes functions to estimate the optimal map size based on various quality measures\n    and to generate a model using the selected dimensions.\n    It also performs hierarchical clustering on the map nodes to group similar units.\n    Documentation about the SOM-HCA method is provided in Pastorelli et al. (2024)\n    <doi:10.1002/xrs.3388>.",
    "version": "0.2.0",
    "maintainer": "Gianluca Pastorelli <gianluca.pastorelli@gmail.com>",
    "author": "Gianluca Pastorelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6926-1952>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=somhca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "somhca Self-Organising Maps Coupled with Hierarchical Cluster Analysis Implements self-organising maps combined with hierarchical cluster analysis (SOM-HCA) for clustering and visualization of high-dimensional data.\n    The package includes functions to estimate the optimal map size based on various quality measures\n    and to generate a model using the selected dimensions.\n    It also performs hierarchical clustering on the map nodes to group similar units.\n    Documentation about the SOM-HCA method is provided in Pastorelli et al. (2024)\n    <doi:10.1002/xrs.3388>.  "
  },
  {
    "id": 21011,
    "package_name": "sotu",
    "title": "United States Presidential State of the Union Addresses",
    "description": "The President of the United States is constitutionally obligated to provide\n  a report known as the 'State of the Union'. The report summarizes the current challenges\n  facing the country and the president's upcoming legislative agenda. While historically\n  the State of the Union was often a written document, in recent decades it has always\n  taken the form of an oral address to a joint session of the United States Congress.\n  This package provides the raw text from every such address with the intention of\n  being used for meaningful examples of text analysis in R. The corpus is well suited\n  to the task as it is historically important, includes material intended to be read\n  and material intended to be spoken, and it falls in the public domain. As the corpus\n  spans over two centuries it is also a good test of how well various methods hold up\n  to the idiosyncrasies of historical texts. Associated data about each address, such\n  as the year, president, party, and format, are also included.",
    "version": "1.0.4",
    "maintainer": "Taylor B. Arnold <tarnold2@richmond.edu>",
    "author": "Taylor B. Arnold [aut, cre]",
    "url": "https://github.com/statsmaths/sotu/",
    "bug_reports": "https://github.com/statsmaths/sotu/issues/",
    "repository": "https://cran.r-project.org/package=sotu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sotu United States Presidential State of the Union Addresses The President of the United States is constitutionally obligated to provide\n  a report known as the 'State of the Union'. The report summarizes the current challenges\n  facing the country and the president's upcoming legislative agenda. While historically\n  the State of the Union was often a written document, in recent decades it has always\n  taken the form of an oral address to a joint session of the United States Congress.\n  This package provides the raw text from every such address with the intention of\n  being used for meaningful examples of text analysis in R. The corpus is well suited\n  to the task as it is historically important, includes material intended to be read\n  and material intended to be spoken, and it falls in the public domain. As the corpus\n  spans over two centuries it is also a good test of how well various methods hold up\n  to the idiosyncrasies of historical texts. Associated data about each address, such\n  as the year, president, party, and format, are also included.  "
  },
  {
    "id": 21019,
    "package_name": "sp",
    "title": "Classes and Methods for Spatial Data",
    "description": "Classes and methods for spatial\n  data; the classes document where the spatial location information\n  resides, for 2D or 3D data. Utility functions are provided, e.g. for\n  plotting data as maps, spatial selection, as well as methods for\n  retrieving coordinates, for subsetting, print, summary, etc. From this\n  version, 'rgdal', 'maptools', and 'rgeos' are no longer used at all,\n  see <https://r-spatial.org/r/2023/05/15/evolution4.html> for details.",
    "version": "2.2-0",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Edzer Pebesma [aut, cre],\n  Roger Bivand [aut],\n  Barry Rowlingson [ctb],\n  Virgilio Gomez-Rubio [ctb],\n  Robert Hijmans [ctb],\n  Michael Sumner [ctb],\n  Don MacQueen [ctb],\n  Jim Lemon [ctb],\n  Finn Lindgren [ctb],\n  Josh O'Brien [ctb],\n  Joseph O'Rourke [ctb],\n  Patrick Hausmann [ctb]",
    "url": "https://github.com/edzer/sp/ https://edzer.github.io/sp/",
    "bug_reports": "https://github.com/edzer/sp/issues",
    "repository": "https://cran.r-project.org/package=sp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sp Classes and Methods for Spatial Data Classes and methods for spatial\n  data; the classes document where the spatial location information\n  resides, for 2D or 3D data. Utility functions are provided, e.g. for\n  plotting data as maps, spatial selection, as well as methods for\n  retrieving coordinates, for subsetting, print, summary, etc. From this\n  version, 'rgdal', 'maptools', and 'rgeos' are no longer used at all,\n  see <https://r-spatial.org/r/2023/05/15/evolution4.html> for details.  "
  },
  {
    "id": 21023,
    "package_name": "spAddins",
    "title": "A Set of RStudio Addins",
    "description": "A set of RStudio addins that are designed to be used in\n             combination with user-defined RStudio keyboard shortcuts. These\n             addins either:\n             1) insert text at a cursor position (e.g. insert\n             operators %>%, <<-, %$%, etc.),\n             2) replace symbols in selected pieces of text (e.g., convert\n             backslashes to forward slashes which results in stings like\n             \"c:\\data\\\" converted into \"c:/data/\") or\n             3) enclose text with special symbols (e.g., converts \"bold\" into\n             \"**bold**\") which is convenient for editing R Markdown files.",
    "version": "0.2.0",
    "maintainer": "Vilmantas Gegzna <GegznaV@gmail.com>",
    "author": "Vilmantas Gegzna [aut, cre, cph]",
    "url": "https://github.com/GegznaV/spAddins",
    "bug_reports": "https://github.com/GegznaV/spAddins/issues",
    "repository": "https://cran.r-project.org/package=spAddins",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spAddins A Set of RStudio Addins A set of RStudio addins that are designed to be used in\n             combination with user-defined RStudio keyboard shortcuts. These\n             addins either:\n             1) insert text at a cursor position (e.g. insert\n             operators %>%, <<-, %$%, etc.),\n             2) replace symbols in selected pieces of text (e.g., convert\n             backslashes to forward slashes which results in stings like\n             \"c:\\data\\\" converted into \"c:/data/\") or\n             3) enclose text with special symbols (e.g., converts \"bold\" into\n             \"**bold**\") which is convenient for editing R Markdown files.  "
  },
  {
    "id": 21056,
    "package_name": "spam",
    "title": "SPArse Matrix",
    "description": "Set of functions for sparse matrix algebra.\n    Differences with other sparse matrix packages are:\n    (1) we only support (essentially) one sparse matrix format,\n    (2) based on transparent and simple structure(s),\n    (3) tailored for MCMC calculations within G(M)RF.\n    (4) and it is fast and scalable (with the extension package spam64).\n    Documentation about 'spam' is provided by vignettes included in this package, see also Furrer and Sain (2010) <doi:10.18637/jss.v036.i10>; see 'citation(\"spam\")' for details.",
    "version": "2.11-1",
    "maintainer": "Reinhard Furrer <reinhard.furrer@math.uzh.ch>",
    "author": "Reinhard Furrer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6319-2332>),\n  Florian Gerber [aut] (ORCID: <https://orcid.org/0000-0001-8545-5263>),\n  Roman Flury [aut] (ORCID: <https://orcid.org/0000-0002-0349-8698>),\n  Daniel Gerber [ctb],\n  Kaspar Moesinger [ctb],\n  Annina Cincera [ctb],\n  Youcef Saad [ctb] (SPARSEKIT\n    http://www-users.cs.umn.edu/~saad/software/SPARSKIT/),\n  Esmond G. Ng [ctb] (Fortran Cholesky routines),\n  Barry W. Peyton [ctb] (Fortran Cholesky routines),\n  Joseph W.H. Liu [ctb] (Fortran Cholesky routines),\n  Alan D. George [ctb] (Fortran Cholesky routines),\n  Lehoucq B. Rich [ctb] (ARPACK),\n  Maschhoff Kristi [ctb] (ARPACK),\n  Sorensen C. Danny [ctb] (ARPACK),\n  Yang Chao [ctb] (ARPACK)",
    "url": "https://www.math.uzh.ch/pages/spam/",
    "bug_reports": "https://git.math.uzh.ch/reinhard.furrer/spam/-/issues",
    "repository": "https://cran.r-project.org/package=spam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spam SPArse Matrix Set of functions for sparse matrix algebra.\n    Differences with other sparse matrix packages are:\n    (1) we only support (essentially) one sparse matrix format,\n    (2) based on transparent and simple structure(s),\n    (3) tailored for MCMC calculations within G(M)RF.\n    (4) and it is fast and scalable (with the extension package spam64).\n    Documentation about 'spam' is provided by vignettes included in this package, see also Furrer and Sain (2010) <doi:10.18637/jss.v036.i10>; see 'citation(\"spam\")' for details.  "
  },
  {
    "id": 21057,
    "package_name": "spam64",
    "title": "64-Bit Extension of the SPArse Matrix R Package 'spam'",
    "description": "Provides the Fortran code of the R package 'spam'\n    with 64-bit integers. Loading this package together with the R package\n    spam enables the sparse matrix class spam to handle huge sparse matrices\n    with more than 2^31-1 non-zero elements.\n    Documentation is provided in Gerber, Moesinger and Furrer (2017) <doi:10.1016/j.cageo.2016.11.015>.",
    "version": "2.10-0",
    "maintainer": "Reinhard Furrer <reinhard.furrer@math.uzh.ch>",
    "author": "Reinhard Furrer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6319-2332>),\n  Florian Gerber [aut] (ORCID: <https://orcid.org/0000-0001-8545-5263>),\n  Roman Flury [aut] (ORCID: <https://orcid.org/0000-0002-0349-8698>),\n  Daniel Gerber [ctb],\n  Kaspar Moesinger [ctb],\n  Youcef Saad [ctb] (SPARSEKIT\n    http://www-users.cs.umn.edu/~saad/software/SPARSKIT/),\n  Esmond G. Ng [ctb] (Fortran Cholesky routines),\n  Barry W. Peyton [ctb] (Fortran Cholesky routines),\n  Joseph W.H. Liu [ctb] (Fortran Cholesky routines),\n  Alan D. George [ctb] (Fortran Cholesky routines),\n  Lehoucq B. Rich [ctb] (ARPACK),\n  Maschhoff Kristi [ctb] (ARPACK),\n  Sorensen C. Danny [ctb] (ARPACK),\n  Yang Chao [ctb] (ARPACK)",
    "url": "https://git.math.uzh.ch/reinhard.furrer/spam",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spam64",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spam64 64-Bit Extension of the SPArse Matrix R Package 'spam' Provides the Fortran code of the R package 'spam'\n    with 64-bit integers. Loading this package together with the R package\n    spam enables the sparse matrix class spam to handle huge sparse matrices\n    with more than 2^31-1 non-zero elements.\n    Documentation is provided in Gerber, Moesinger and Furrer (2017) <doi:10.1016/j.cageo.2016.11.015>.  "
  },
  {
    "id": 21059,
    "package_name": "spanishoddata",
    "title": "Get Spanish Origin-Destination Data",
    "description": "Gain seamless access to origin-destination (OD) data from the\n    Spanish Ministry of Transport, hosted at\n    <https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad>.\n    This package simplifies the management of these large datasets by\n    providing tools to download zone boundaries, handle associated\n    origin-destination data, and process it efficiently with the 'duckdb'\n    database interface.  Local caching minimizes repeated downloads,\n    streamlining workflows for researchers and analysts. Extensive\n    documentation is available at\n    <https://ropenspain.github.io/spanishoddata/index.html>, offering\n    guides on creating static and dynamic mobility flow visualizations and\n    transforming large datasets into analysis-ready formats.",
    "version": "0.2.1",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre] (ORCID: <https://orcid.org/0000-0001-6690-5345>),\n  Robin Lovelace [aut] (ORCID: <https://orcid.org/0000-0001-5679-6536>),\n  Eugeni Vidal-Tortosa [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5199-4103>)",
    "url": "https://rOpenSpain.github.io/spanishoddata/,\nhttps://github.com/rOpenSpain/spanishoddata",
    "bug_reports": "https://github.com/rOpenSpain/spanishoddata/issues",
    "repository": "https://cran.r-project.org/package=spanishoddata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spanishoddata Get Spanish Origin-Destination Data Gain seamless access to origin-destination (OD) data from the\n    Spanish Ministry of Transport, hosted at\n    <https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad>.\n    This package simplifies the management of these large datasets by\n    providing tools to download zone boundaries, handle associated\n    origin-destination data, and process it efficiently with the 'duckdb'\n    database interface.  Local caching minimizes repeated downloads,\n    streamlining workflows for researchers and analysts. Extensive\n    documentation is available at\n    <https://ropenspain.github.io/spanishoddata/index.html>, offering\n    guides on creating static and dynamic mobility flow visualizations and\n    transforming large datasets into analysis-ready formats.  "
  },
  {
    "id": 21079,
    "package_name": "sparseEigen",
    "title": "Computation of Sparse Eigenvectors of a Matrix",
    "description": "Computation of sparse eigenvectors of a matrix (aka sparse PCA)\n    with running time 2-3 orders of magnitude lower than existing methods and\n    better final performance in terms of recovery of sparsity pattern and \n    estimation of numerical values. Can handle covariance matrices as well as \n    data matrices with real or complex-valued entries. Different levels of \n    sparsity can be specified for each individual ordered eigenvector and the \n    method is robust in parameter selection. See vignette for a detailed \n    documentation and comparison, with several illustrative examples. \n    The package is based on the paper:\n    K. Benidis, Y. Sun, P. Babu, and D. P. Palomar (2016). \"Orthogonal Sparse PCA \n    and Covariance Estimation via Procrustes Reformulation,\" IEEE Transactions on \n    Signal Processing <doi:10.1109/TSP.2016.2605073>.",
    "version": "0.1.0",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Konstantinos Benidis [aut],\n  Daniel P. Palomar [cre, aut]",
    "url": "https://github.com/dppalomar/sparseEigen,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2016.2605073",
    "bug_reports": "https://github.com/dppalomar/sparseEigen/issues",
    "repository": "https://cran.r-project.org/package=sparseEigen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparseEigen Computation of Sparse Eigenvectors of a Matrix Computation of sparse eigenvectors of a matrix (aka sparse PCA)\n    with running time 2-3 orders of magnitude lower than existing methods and\n    better final performance in terms of recovery of sparsity pattern and \n    estimation of numerical values. Can handle covariance matrices as well as \n    data matrices with real or complex-valued entries. Different levels of \n    sparsity can be specified for each individual ordered eigenvector and the \n    method is robust in parameter selection. See vignette for a detailed \n    documentation and comparison, with several illustrative examples. \n    The package is based on the paper:\n    K. Benidis, Y. Sun, P. Babu, and D. P. Palomar (2016). \"Orthogonal Sparse PCA \n    and Covariance Estimation via Procrustes Reformulation,\" IEEE Transactions on \n    Signal Processing <doi:10.1109/TSP.2016.2605073>.  "
  },
  {
    "id": 21083,
    "package_name": "sparseIndexTracking",
    "title": "Design of Portfolio of Stocks to Track an Index",
    "description": "Computation of sparse portfolios for financial index tracking, i.e., joint\n    selection of a subset of the assets that compose the index and computation\n    of their relative weights (capital allocation). The level of sparsity of the\n    portfolios, i.e., the number of selected assets, is controlled through a\n    regularization parameter. Different tracking measures are available, namely,\n    the empirical tracking error (ETE), downside risk (DR), Huber empirical\n    tracking error (HETE), and Huber downside risk (HDR). See vignette for a\n    detailed documentation and comparison, with several illustrative examples.\n    The package is based on the paper:\n    K. Benidis, Y. Feng, and D. P. Palomar, \"Sparse Portfolios for High-Dimensional\n    Financial Index Tracking,\" IEEE Trans. on Signal Processing, vol. 66, no. 1,\n    pp. 155-170, Jan. 2018. <doi:10.1109/TSP.2017.2762286>.",
    "version": "0.1.1",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Konstantinos Benidis [aut],\n  Daniel P. Palomar [cre, aut]",
    "url": "https://CRAN.R-project.org/package=sparseIndexTracking,\nhttps://github.com/dppalomar/sparseIndexTracking,\nhttps://www.danielppalomar.com,\nhttps://doi.org/10.1109/TSP.2017.2762286",
    "bug_reports": "https://github.com/dppalomar/sparseIndexTracking/issues",
    "repository": "https://cran.r-project.org/package=sparseIndexTracking",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sparseIndexTracking Design of Portfolio of Stocks to Track an Index Computation of sparse portfolios for financial index tracking, i.e., joint\n    selection of a subset of the assets that compose the index and computation\n    of their relative weights (capital allocation). The level of sparsity of the\n    portfolios, i.e., the number of selected assets, is controlled through a\n    regularization parameter. Different tracking measures are available, namely,\n    the empirical tracking error (ETE), downside risk (DR), Huber empirical\n    tracking error (HETE), and Huber downside risk (HDR). See vignette for a\n    detailed documentation and comparison, with several illustrative examples.\n    The package is based on the paper:\n    K. Benidis, Y. Feng, and D. P. Palomar, \"Sparse Portfolios for High-Dimensional\n    Financial Index Tracking,\" IEEE Trans. on Signal Processing, vol. 66, no. 1,\n    pp. 155-170, Jan. 2018. <doi:10.1109/TSP.2017.2762286>.  "
  },
  {
    "id": 21157,
    "package_name": "speakr",
    "title": "A Wrapper for the Phonetic Software 'Praat'",
    "description": "It allows running 'Praat' scripts from R and it provides some\n    wrappers for basic plotting. It also adds support for literate markdown\n    tangling. The package is designed to bring reproducible phonetic research\n    into R.",
    "version": "3.2.4",
    "maintainer": "Stefano Coretta <stefano.coretta@gmail.com>",
    "author": "Stefano Coretta [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9627-5532>)",
    "url": "https://github.com/stefanocoretta/speakr",
    "bug_reports": "https://github.com/stefanocoretta/speakr/issues",
    "repository": "https://cran.r-project.org/package=speakr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "speakr A Wrapper for the Phonetic Software 'Praat' It allows running 'Praat' scripts from R and it provides some\n    wrappers for basic plotting. It also adds support for literate markdown\n    tangling. The package is designed to bring reproducible phonetic research\n    into R.  "
  },
  {
    "id": 21199,
    "package_name": "spheredata",
    "title": "Students' Performance Dataset in Physics Education Research\n(SPHERE)",
    "description": "A multidimensional dataset of students' performance assessment in high school physics. The SPHERE dataset was collected from 497 students in four public high schools specifically measuring their conceptual understanding, scientific ability, and attitude toward physics [see Santoso et al. (2024) <doi:10.17632/88d7m2fv7p.1>]. The data collection was conducted using some research based assessments established by the physics education research community. They include the Force Concept Inventory, the Force and Motion Conceptual Evaluation, the Rotational and Rolling Motion Conceptual Survey, the Fluid Mechanics Concept Inventory, the Mechanical Waves Conceptual Survey, the Thermal Concept Evaluation, the Survey of Thermodynamic Processes and First and Second Laws, the Scientific Abilities Assessment Rubrics, and the Colorado Learning Attitudes about Science Survey. Students' attributes related to gender, age, socioeconomic status, domicile, literacy, physics identity, and test results administered using teachers' developed items are also reported in this dataset.",
    "version": "0.1.3",
    "maintainer": "Purwoko Haryadi Santoso <purwokoharyadisantoso@unsulbar.ac.id>",
    "author": "Purwoko Haryadi Santoso [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7093-5309>),\n  Edi Istiyono [ctb],\n  Haryanto Haryanto [ctb]",
    "url": "https://github.com/santosoph/spheredata",
    "bug_reports": "https://github.com/santosoph/spheredata/issues",
    "repository": "https://cran.r-project.org/package=spheredata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spheredata Students' Performance Dataset in Physics Education Research\n(SPHERE) A multidimensional dataset of students' performance assessment in high school physics. The SPHERE dataset was collected from 497 students in four public high schools specifically measuring their conceptual understanding, scientific ability, and attitude toward physics [see Santoso et al. (2024) <doi:10.17632/88d7m2fv7p.1>]. The data collection was conducted using some research based assessments established by the physics education research community. They include the Force Concept Inventory, the Force and Motion Conceptual Evaluation, the Rotational and Rolling Motion Conceptual Survey, the Fluid Mechanics Concept Inventory, the Mechanical Waves Conceptual Survey, the Thermal Concept Evaluation, the Survey of Thermodynamic Processes and First and Second Laws, the Scientific Abilities Assessment Rubrics, and the Colorado Learning Attitudes about Science Survey. Students' attributes related to gender, age, socioeconomic status, domicile, literacy, physics identity, and test results administered using teachers' developed items are also reported in this dataset.  "
  },
  {
    "id": 21205,
    "package_name": "spicy",
    "title": "Descriptive Statistics and Data Management Tools",
    "description": "Extracts and summarizes metadata from data frames, including variable names, labels, types, and missing values. Computes compact descriptive statistics, frequency tables, and cross-tabulations to assist with efficient data exploration. Includes an interactive and exportable codebook generator for documenting variable metadata. Facilitates the identification of missing data patterns and structural issues in datasets. Designed to streamline initial data management and exploratory analysis workflows within 'R'.",
    "version": "0.3.0",
    "maintainer": "Amal Tawfik <amal.tawfik@hesav.ch>",
    "author": "Amal Tawfik [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0006-2422-1555>, ROR:\n    <https://ror.org/04j47fz63>)",
    "url": "https://github.com/amaltawfik/spicy/,\nhttps://amaltawfik.github.io/spicy/",
    "bug_reports": "https://github.com/amaltawfik/spicy/issues",
    "repository": "https://cran.r-project.org/package=spicy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spicy Descriptive Statistics and Data Management Tools Extracts and summarizes metadata from data frames, including variable names, labels, types, and missing values. Computes compact descriptive statistics, frequency tables, and cross-tabulations to assist with efficient data exploration. Includes an interactive and exportable codebook generator for documenting variable metadata. Facilitates the identification of missing data patterns and structural issues in datasets. Designed to streamline initial data management and exploratory analysis workflows within 'R'.  "
  },
  {
    "id": 21208,
    "package_name": "spiderbar",
    "title": "Parse and Test Robots Exclusion Protocol Files and Rules",
    "description": "The 'Robots Exclusion Protocol' <https://www.robotstxt.org/orig.html> documents\n    a set of standards for allowing or excluding robot/spider crawling of different areas of\n    site content. Tools are provided which wrap The 'rep-cpp' <https://github.com/seomoz/rep-cpp>\n    C++ library for processing these 'robots.txt' files.",
    "version": "0.2.5",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis (bob@rud.is) [aut, cre], SEOmoz, Inc [aut]",
    "url": "https://github.com/hrbrmstr/spiderbar",
    "bug_reports": "https://github.com/hrbrmstr/spiderbar/issues",
    "repository": "https://cran.r-project.org/package=spiderbar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spiderbar Parse and Test Robots Exclusion Protocol Files and Rules The 'Robots Exclusion Protocol' <https://www.robotstxt.org/orig.html> documents\n    a set of standards for allowing or excluding robot/spider crawling of different areas of\n    site content. Tools are provided which wrap The 'rep-cpp' <https://github.com/seomoz/rep-cpp>\n    C++ library for processing these 'robots.txt' files.  "
  },
  {
    "id": 21219,
    "package_name": "spiritR",
    "title": "Template for Clinical Trial Protocol",
    "description": "Contains an R Markdown template for a clinical trial \n    protocol adhering to the SPIRIT statement. The SPIRIT (Standard Protocol \n    Items for Interventional Trials) statement outlines recommendations for a \n    minimum set of elements to be addressed in a  clinical trial protocol. \n    Also contains functions to create a xml document from the template and \n    upload it to clinicaltrials.gov<https://www.clinicaltrials.gov/> for \n    trial registration.",
    "version": "0.1.1",
    "maintainer": "Aaron Conway <aaron.conway@utoronto.ca>",
    "author": "Aaron Conway [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9583-8636>)",
    "url": "https://github.com/awconway/spiritR",
    "bug_reports": "https://github.com/awconway/spiritR/issues",
    "repository": "https://cran.r-project.org/package=spiritR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spiritR Template for Clinical Trial Protocol Contains an R Markdown template for a clinical trial \n    protocol adhering to the SPIRIT statement. The SPIRIT (Standard Protocol \n    Items for Interventional Trials) statement outlines recommendations for a \n    minimum set of elements to be addressed in a  clinical trial protocol. \n    Also contains functions to create a xml document from the template and \n    upload it to clinicaltrials.gov<https://www.clinicaltrials.gov/> for \n    trial registration.  "
  },
  {
    "id": 21254,
    "package_name": "spotidy",
    "title": "Providing Convenience Functions to Connect R with the Spotify\nAPI",
    "description": "Providing convenience functions to connect R with the 'Spotify' application programming interface ('API'). At first it aims to help setting up \n  the OAuth2.0 Authentication flow. The default output of the get_*() functions is tidy, but optionally the functions could return the raw response from the \n  'API' as well. The search_*() and get_*() functions can be combined. See the vignette for more information and examples and the official Spotify for \n  Developers website <https://developer.spotify.com/documentation/web-api/> for information about the Web 'API'.",
    "version": "0.1.0",
    "maintainer": "Lennard van Wanrooij <lennardvanwanrooij@gmail.com>",
    "author": "Lennard van Wanrooij",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spotidy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spotidy Providing Convenience Functions to Connect R with the Spotify\nAPI Providing convenience functions to connect R with the 'Spotify' application programming interface ('API'). At first it aims to help setting up \n  the OAuth2.0 Authentication flow. The default output of the get_*() functions is tidy, but optionally the functions could return the raw response from the \n  'API' as well. The search_*() and get_*() functions can be combined. See the vignette for more information and examples and the official Spotify for \n  Developers website <https://developer.spotify.com/documentation/web-api/> for information about the Web 'API'.  "
  },
  {
    "id": 21255,
    "package_name": "spotifyr",
    "title": "R Wrapper for the 'Spotify' Web API",
    "description": "An R wrapper for pulling data from the 'Spotify' Web API \n  <https://developer.spotify.com/documentation/web-api/> in bulk, or post items on a\n  'Spotify' user's playlist.",
    "version": "2.2.5",
    "maintainer": "Daniel Antal <daniel.antal@dataobservatory.eu>",
    "author": "Charlie Thompson [aut],\n  Daniel Antal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7513-6760>),\n  Josiah Parry [aut] (ORCID: <https://orcid.org/0000-0001-9910-865X>),\n  Donal Phipps [aut],\n  Tom Wolff [aut],\n  Stephen Holsenbeck [ctb],\n  Peter Harrison [ctb]",
    "url": "https://github.com/charlie86/spotifyr",
    "bug_reports": "https://github.com/charlie86/spotifyr/issues",
    "repository": "https://cran.r-project.org/package=spotifyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spotifyr R Wrapper for the 'Spotify' Web API An R wrapper for pulling data from the 'Spotify' Web API \n  <https://developer.spotify.com/documentation/web-api/> in bulk, or post items on a\n  'Spotify' user's playlist.  "
  },
  {
    "id": 21295,
    "package_name": "srcpkgs",
    "title": "R Source Packages Manager",
    "description": "Manage a collection/library of R source packages. Discover, document, load, test \n  source packages. Enable to use those packages as if they were actually installed. Quickly reload\n  only what is needed on source code change. Run tests and checks in parallel.",
    "version": "0.2",
    "maintainer": "Karl Forner <karl.forner@gmail.com>",
    "author": "Karl Forner [aut, cre, cph]",
    "url": "https://github.com/kforner/srcpkgs",
    "bug_reports": "https://github.com/kforner/srcpkgs/issues",
    "repository": "https://cran.r-project.org/package=srcpkgs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "srcpkgs R Source Packages Manager Manage a collection/library of R source packages. Discover, document, load, test \n  source packages. Enable to use those packages as if they were actually installed. Quickly reload\n  only what is needed on source code change. Run tests and checks in parallel.  "
  },
  {
    "id": 21302,
    "package_name": "srppp",
    "title": "Read the Swiss Register of Plant Protection Products",
    "description": "Generate data objects from XML versions of the Swiss\n  Register of Plant Protection Products. An online version of the\n  register can be accessed at <https://www.psm.admin.ch/de/produkte>. There is no\n  guarantee of correspondence of the data read in using this package with that\n  online version, or with the original registration documents.  Also, the\n  Federal Food Safety and Veterinary Office, coordinating the authorisation of\n  plant protection products in Switzerland, does not answer requests regarding\n  this package. ",
    "version": "2.0.0",
    "maintainer": "Johannes Ranke <johannes.ranke@agroscope.admin.ch>",
    "author": "Daniel Baumgartner [ctb] (Provided feedback, explanations and\n    background information),\n  Marcel Mathis [rev, ctb],\n  Romualdus Kasteel [rev] (Provided feedback to version 0.3.4),\n  Elisabeth Lutz [ctb],\n  Johannes Ranke [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4371-6538>),\n  Agroscope [cph]",
    "url": "https://agroscope-ch.github.io/srppp/",
    "bug_reports": "https://github.com/agroscope-ch/srppp/issues",
    "repository": "https://cran.r-project.org/package=srppp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "srppp Read the Swiss Register of Plant Protection Products Generate data objects from XML versions of the Swiss\n  Register of Plant Protection Products. An online version of the\n  register can be accessed at <https://www.psm.admin.ch/de/produkte>. There is no\n  guarantee of correspondence of the data read in using this package with that\n  online version, or with the original registration documents.  Also, the\n  Federal Food Safety and Veterinary Office, coordinating the authorisation of\n  plant protection products in Switzerland, does not answer requests regarding\n  this package.   "
  },
  {
    "id": 21367,
    "package_name": "standardlastprofile",
    "title": "Data Package for BDEW Standard Load Profiles in Electricity",
    "description": "Data on standard load profiles from the German Association of \n  Energy and Water Industries (BDEW Bundesverband der Energie- und \n  Wasserwirtschaft e.V.) in a tidy format. The data and methodology are \n  described in VDEW (1999), \"Repr\u00e4sentative VDEW-Lastprofile\", \n  <https://www.bdew.de/media/documents/1999_Repraesentative-VDEW-Lastprofile.pdf>.\n  The package also offers an interface for generating a standard load profile \n  over a user-defined period. For the algorithm, see VDEW (2000), \n  \"Anwendung der Repr\u00e4sentativen VDEW-Lastprofile step-by-step\",\n  <https://www.bdew.de/media/documents/2000131_Anwendung-repraesentativen_Lastprofile-Step-by-step.pdf>.",
    "version": "1.0.0",
    "maintainer": "Markus D\u00f6ring <m4rkus.doering@gmail.com>",
    "author": "Markus D\u00f6ring [aut, cre, cph]",
    "url": "https://github.com/flrd/standardlastprofile,\nhttps://flrd.github.io/standardlastprofile/",
    "bug_reports": "https://github.com/flrd/standardlastprofile/issues",
    "repository": "https://cran.r-project.org/package=standardlastprofile",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "standardlastprofile Data Package for BDEW Standard Load Profiles in Electricity Data on standard load profiles from the German Association of \n  Energy and Water Industries (BDEW Bundesverband der Energie- und \n  Wasserwirtschaft e.V.) in a tidy format. The data and methodology are \n  described in VDEW (1999), \"Repr\u00e4sentative VDEW-Lastprofile\", \n  <https://www.bdew.de/media/documents/1999_Repraesentative-VDEW-Lastprofile.pdf>.\n  The package also offers an interface for generating a standard load profile \n  over a user-defined period. For the algorithm, see VDEW (2000), \n  \"Anwendung der Repr\u00e4sentativen VDEW-Lastprofile step-by-step\",\n  <https://www.bdew.de/media/documents/2000131_Anwendung-repraesentativen_Lastprofile-Step-by-step.pdf>.  "
  },
  {
    "id": 21373,
    "package_name": "staplr",
    "title": "A Toolkit for PDF Files",
    "description": "Provides functions to manipulate PDF files: \n    fill out PDF forms;\n    merge multiple PDF files into one; \n    remove selected pages from a file;\n    rename multiple files in a directory;\n    rotate entire pdf document; \n    rotate selected pages of a pdf file;\n    Select pages from a file;\n    splits single input PDF document into individual pages;\n    splits single input PDF document into parts from given points.",
    "version": "3.2.2",
    "maintainer": "Priyanga Dilini Talagala <pritalagala@gmail.com>",
    "author": "Priyanga Dilini Talagala [aut, cre],\n  Ogan Mancarci [aut],\n  Daniel Padfield [aut],\n  Granville Matheson [aut],\n  Pedro Rafael D. Marinho [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1591-8300>),\n  Marc Vinyals [cph, aut] (Author and copyright holder of included\n    pdftk-java package)",
    "url": "",
    "bug_reports": "https://github.com/pridiltal/staplr/issues",
    "repository": "https://cran.r-project.org/package=staplr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "staplr A Toolkit for PDF Files Provides functions to manipulate PDF files: \n    fill out PDF forms;\n    merge multiple PDF files into one; \n    remove selected pages from a file;\n    rename multiple files in a directory;\n    rotate entire pdf document; \n    rotate selected pages of a pdf file;\n    Select pages from a file;\n    splits single input PDF document into individual pages;\n    splits single input PDF document into parts from given points.  "
  },
  {
    "id": 21393,
    "package_name": "statcheck",
    "title": "Extract Statistics from Articles and Recompute P-Values",
    "description": "A \"spellchecker\" for statistics. It checks whether your\n    p-values match their accompanying test statistic and degrees of\n    freedom. statcheck searches for null-hypothesis significance test\n    (NHST) in APA style (e.g., t(28) = 2.2, p < .05). It recalculates the\n    p-value using the reported test statistic and degrees of freedom. If\n    the reported and computed p-values don't match, statcheck will flag\n    the result as an error. If the reported p-value is statistically\n    significant and the recomputed one is not, or vice versa, the result\n    will be flagged as a decision error.  You can use statcheck directly\n    on a string of text, but you can also scan a PDF or HTML file, or even\n    a folder of PDF and/or HTML files.  Statcheck needs an external\n    program to convert PDF to text: Xpdf. Instructions on where and how to\n    download this program, how to install statcheck, and more details on\n    what statcheck can and cannot do can be found in the online manual:\n    <https://rpubs.com/michelenuijten/statcheckmanual>.  You can find a\n    point-and-click web interface to scan PDF or HTML or DOCX articles on\n    <http://statcheck.io>.",
    "version": "1.5.0",
    "maintainer": "Michele B. Nuijten <m.b.nuijten@uvt.nl>",
    "author": "Michele B. Nuijten [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1468-8585>),\n  Sacha Epskamp [aut] (ORCID: <https://orcid.org/0000-0003-4884-8118>),\n  Willem Sleegers [ctb] (ORCID: <https://orcid.org/0000-0001-9058-3817>),\n  Edoardo Costantini [ctb],\n  Paul van der Laken [ctb] (ORCID:\n    <https://orcid.org/0000-0002-0404-9114>),\n  Sean Rife [ctb] (ORCID: <https://orcid.org/0000-0002-6748-0841>),\n  John Sakaluk [ctb] (ORCID: <https://orcid.org/0000-0002-2515-9822>),\n  Chris Hartgerink [ctb] (ORCID: <https://orcid.org/0000-0003-1050-6809>),\n  Steve Haroz [ctb] (ORCID: <https://orcid.org/0000-0002-2725-9173>)",
    "url": "https://github.com/MicheleNuijten/statcheck",
    "bug_reports": "https://github.com/MicheleNuijten/statcheck/issues",
    "repository": "https://cran.r-project.org/package=statcheck",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statcheck Extract Statistics from Articles and Recompute P-Values A \"spellchecker\" for statistics. It checks whether your\n    p-values match their accompanying test statistic and degrees of\n    freedom. statcheck searches for null-hypothesis significance test\n    (NHST) in APA style (e.g., t(28) = 2.2, p < .05). It recalculates the\n    p-value using the reported test statistic and degrees of freedom. If\n    the reported and computed p-values don't match, statcheck will flag\n    the result as an error. If the reported p-value is statistically\n    significant and the recomputed one is not, or vice versa, the result\n    will be flagged as a decision error.  You can use statcheck directly\n    on a string of text, but you can also scan a PDF or HTML file, or even\n    a folder of PDF and/or HTML files.  Statcheck needs an external\n    program to convert PDF to text: Xpdf. Instructions on where and how to\n    download this program, how to install statcheck, and more details on\n    what statcheck can and cannot do can be found in the online manual:\n    <https://rpubs.com/michelenuijten/statcheckmanual>.  You can find a\n    point-and-click web interface to scan PDF or HTML or DOCX articles on\n    <http://statcheck.io>.  "
  },
  {
    "id": 21397,
    "package_name": "statebins",
    "title": "Create United States Uniform Cartogram Heatmaps",
    "description": "The 'cartogram' heatmaps generated by the included methods \n    are an alternative to choropleth maps for the United States\n    and are based on work by the Washington Post graphics department in their report\n    on \"The states most threatened by trade\" \n    (<http://www.washingtonpost.com/wp-srv/special/business/states-most-threatened-by-trade/>).\n    \"State bins\" preserve as much of the geographic placement of the states as \n    possible but have the look and feel of a traditional heatmap. Functions are \n    provided that allow for use of a binned, discrete scale, a continuous scale \n    or manually specified colors depending on what is needed for the underlying data.",
    "version": "1.4.0",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis [aut, cre] (ORCID: <https://orcid.org/0000-0001-5670-2640>),\n  Harold Gil [ctb] (fix for show.guide),\n  Brian Adams [ctb] (theme testing & feedback),\n  Thomas Wood [ctb] (Significant suggestions & testing that made new\n    features possible),\n  Mathew Kiang [ctb] (Minor fix for NA handling)",
    "url": "https://gitlab.com/hrbrmstr/statebins",
    "bug_reports": "https://gitlab.com/hrbrmstr/statebins/issues",
    "repository": "https://cran.r-project.org/package=statebins",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statebins Create United States Uniform Cartogram Heatmaps The 'cartogram' heatmaps generated by the included methods \n    are an alternative to choropleth maps for the United States\n    and are based on work by the Washington Post graphics department in their report\n    on \"The states most threatened by trade\" \n    (<http://www.washingtonpost.com/wp-srv/special/business/states-most-threatened-by-trade/>).\n    \"State bins\" preserve as much of the geographic placement of the states as \n    possible but have the look and feel of a traditional heatmap. Functions are \n    provided that allow for use of a binned, discrete scale, a continuous scale \n    or manually specified colors depending on what is needed for the underlying data.  "
  },
  {
    "id": 21408,
    "package_name": "staticryptR",
    "title": "Encrypt HTML Files Using 'staticrypt'",
    "description": "Provides a convenient interface to the 'staticrypt' by Robin Moisson <https://github.com/robinmoisson/staticrypt>---'Node.js' package for \n    adding a password protection layer to static HTML pages. This package can be \n    integrated into the post-render process of 'quarto' documents to secure them \n    with a password. ",
    "version": "0.1.0",
    "maintainer": "Nikita Tkachenko <nikita@nikitoshina.com>",
    "author": "Nikita Tkachenko [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0003-8681-3335>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=staticryptR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "staticryptR Encrypt HTML Files Using 'staticrypt' Provides a convenient interface to the 'staticrypt' by Robin Moisson <https://github.com/robinmoisson/staticrypt>---'Node.js' package for \n    adding a password protection layer to static HTML pages. This package can be \n    integrated into the post-render process of 'quarto' documents to secure them \n    with a password.   "
  },
  {
    "id": 21421,
    "package_name": "statquotes",
    "title": "Quotes on Statistics, Data Visualization and Science",
    "description": "Generates a random quotation from a database of quotes on topics\n    in statistics, data visualization and science. Other functions allow searching\n    the quotes database by key term tags, or authors or creating a word cloud.\n    The output is designed to be suitable for use at the console, in Rmarkdown\n    and LaTeX.",
    "version": "0.3.3",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>, Twitter: @datavisFriendly),\n  Kevin Wright [aut] (ORCID: <https://orcid.org/0000-0002-0617-8673>),\n  Phil Chalmers [aut],\n  Matthew Sigal [ctb]",
    "url": "https://github.com/friendly/statquotes/",
    "bug_reports": "https://github.com/friendly/statquotes/issues",
    "repository": "https://cran.r-project.org/package=statquotes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statquotes Quotes on Statistics, Data Visualization and Science Generates a random quotation from a database of quotes on topics\n    in statistics, data visualization and science. Other functions allow searching\n    the quotes database by key term tags, or authors or creating a word cloud.\n    The output is designed to be suitable for use at the console, in Rmarkdown\n    and LaTeX.  "
  },
  {
    "id": 21442,
    "package_name": "stencilaschema",
    "title": "Bindings for Stencila Schema",
    "description": "Provides R bindings for the Stencila Schema <https://schema.stenci.la>. This package is primarily aimed at R developers wanting to programmatically generate, or modify, executable documents.",
    "version": "1.0.0",
    "maintainer": "Nokome Bentley <nokome@stenci.la>",
    "author": "Nokome Bentley [aut, cre]",
    "url": "https://github.com/stencila/schema#readme",
    "bug_reports": "https://github.com/stencila/schema/issues",
    "repository": "https://cran.r-project.org/package=stencilaschema",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stencilaschema Bindings for Stencila Schema Provides R bindings for the Stencila Schema <https://schema.stenci.la>. This package is primarily aimed at R developers wanting to programmatically generate, or modify, executable documents.  "
  },
  {
    "id": 21443,
    "package_name": "stenographer",
    "title": "Flexible and Customisable Logging System",
    "description": "A comprehensive logging framework for R applications that provides hierarchical logging levels, database integration, and contextual logging capabilities. The package supports 'SQLite' storage for persistent logs, provides colour-coded console output for better readability, includes parallel processing support, and implements structured error reporting with 'JSON' formatting.",
    "version": "1.0.0",
    "maintainer": "Dereck Mezquita <dereck@mezquita.io>",
    "author": "Dereck Mezquita [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9307-6762>)",
    "url": "https://github.com/dereckmezquita/stenographer",
    "bug_reports": "https://github.com/dereckmezquita/stenographer/issues",
    "repository": "https://cran.r-project.org/package=stenographer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stenographer Flexible and Customisable Logging System A comprehensive logging framework for R applications that provides hierarchical logging levels, database integration, and contextual logging capabilities. The package supports 'SQLite' storage for persistent logs, provides colour-coded console output for better readability, includes parallel processing support, and implements structured error reporting with 'JSON' formatting.  "
  },
  {
    "id": 21461,
    "package_name": "stevetemplates",
    "title": "Steve's R Markdown Templates",
    "description": "These are my collection of 'R Markdown' templates, mostly for compilation to PDF.\n    These are useful for all things academic and professional, if you are using 'R Markdown'\n    for things like your CV or your articles and manuscripts.",
    "version": "1.1.0",
    "maintainer": "Steven Miller <steve@svmiller.com>",
    "author": "Steven Miller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4072-6263>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stevetemplates",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stevetemplates Steve's R Markdown Templates These are my collection of 'R Markdown' templates, mostly for compilation to PDF.\n    These are useful for all things academic and professional, if you are using 'R Markdown'\n    for things like your CV or your articles and manuscripts.  "
  },
  {
    "id": 21476,
    "package_name": "stm",
    "title": "Estimation of the Structural Topic Model",
    "description": "The Structural Topic Model (STM) allows researchers \n  to estimate topic models with document-level covariates. \n  The package also includes tools for model selection, visualization,\n  and estimation of topic-covariate regressions. Methods developed in\n  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and \n  Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette\n  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>.",
    "version": "1.3.8",
    "maintainer": "Brandon Stewart <bms4@princeton.edu>",
    "author": "Margaret Roberts [aut],\n  Brandon Stewart [aut, cre],\n  Dustin Tingley [aut],\n  Kenneth Benoit [ctb]",
    "url": "http://www.structuraltopicmodel.com/",
    "bug_reports": "https://github.com/bstewart/stm/issues",
    "repository": "https://cran.r-project.org/package=stm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stm Estimation of the Structural Topic Model The Structural Topic Model (STM) allows researchers \n  to estimate topic models with document-level covariates. \n  The package also includes tools for model selection, visualization,\n  and estimation of topic-covariate regressions. Methods developed in\n  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and \n  Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette\n  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>.  "
  },
  {
    "id": 21507,
    "package_name": "stranslate",
    "title": "Simple Translation Between Different Languages",
    "description": "Message translation is often managed with 'po' files and the 'gettext' programme, \n    but sometimes another solution is needed. In contrast to 'po' files, a more flexible approach \n    is used as in the Fluent <https://projectfluent.org/> project with R Markdown snippets. \n    The key-value approach allows easier handling of the translated messages.",
    "version": "0.1.3",
    "maintainer": "Sigbert Klinke <sigbert@hu-berlin.de>",
    "author": "Sigbert Klinke [aut, cre]",
    "url": "https://github.com/sigbertklinke/stranslate (development version)",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stranslate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stranslate Simple Translation Between Different Languages Message translation is often managed with 'po' files and the 'gettext' programme, \n    but sometimes another solution is needed. In contrast to 'po' files, a more flexible approach \n    is used as in the Fluent <https://projectfluent.org/> project with R Markdown snippets. \n    The key-value approach allows easier handling of the translated messages.  "
  },
  {
    "id": 21531,
    "package_name": "streamy",
    "title": "Inline Asynchronous Generator Results into Documents",
    "description": "Given a 'coro' asynchronous generator instance that produces text,\n    write that text into a document selection in 'RStudio' and 'Positron'. This\n    is particularly helpful for streaming large language model responses into\n    the user's editor.",
    "version": "0.2.1",
    "maintainer": "Simon Couch <simon.couch@posit.co>",
    "author": "Simon Couch [aut, cre],\n  Posit Software, PBC [cph, fnd]",
    "url": "https://github.com/simonpcouch/streamy,\nhttps://simonpcouch.github.io/streamy/",
    "bug_reports": "https://github.com/simonpcouch/streamy/issues",
    "repository": "https://cran.r-project.org/package=streamy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "streamy Inline Asynchronous Generator Results into Documents Given a 'coro' asynchronous generator instance that produces text,\n    write that text into a document selection in 'RStudio' and 'Positron'. This\n    is particularly helpful for streaming large language model responses into\n    the user's editor.  "
  },
  {
    "id": 21532,
    "package_name": "streetscape",
    "title": "Collect and Investigate Street Views for Urban Science",
    "description": "A collection of functions to search and download street view imagery \n             ('Mapilary' <https://www.mapillary.com/developer/api-documentation>) and \n             to extract, quantify, and visualize visual features. Moreover, there are \n             functions provided to generate Qualtrics survey in TXT format using \n             the collection of street views for various research purposes. ",
    "version": "1.0.5",
    "maintainer": "Xiaohao Yang <xiaohaoy@umich.edu>",
    "author": "Xiaohao Yang [aut, cre, cph],\n  Derek Van Berkel [aut],\n  Mark Lindquist [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=streetscape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "streetscape Collect and Investigate Street Views for Urban Science A collection of functions to search and download street view imagery \n             ('Mapilary' <https://www.mapillary.com/developer/api-documentation>) and \n             to extract, quantify, and visualize visual features. Moreover, there are \n             functions provided to generate Qualtrics survey in TXT format using \n             the collection of street views for various research purposes.   "
  },
  {
    "id": 21542,
    "package_name": "stringi",
    "title": "Fast and Portable Character String Processing Facilities",
    "description": "A collection of character string/text/natural language\n    processing tools for pattern searching (e.g., with 'Java'-like regular\n    expressions or the 'Unicode' collation algorithm), random string generation,\n    case mapping, string transliteration, concatenation, sorting, padding,\n    wrapping, Unicode normalisation, date-time formatting and parsing,\n    and many more. They are fast, consistent, convenient, and -\n    thanks to 'ICU' (International Components for Unicode) -\n    portable across all locales and platforms. Documentation about 'stringi' is\n    provided via its website at <https://stringi.gagolewski.com/> and\n    the paper by Gagolewski (2022, <doi:10.18637/jss.v103.i02>).",
    "version": "1.8.7",
    "maintainer": "Marek Gagolewski <marek@gagolewski.com>",
    "author": "Marek Gagolewski [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0637-6028>),\n  Bartek Tartanus [ctb],\n  Unicode, Inc. and others [ctb] (ICU4C source code, Unicode Character\n    Database)",
    "url": "https://stringi.gagolewski.com/,\nhttps://github.com/gagolews/stringi, https://icu.unicode.org/",
    "bug_reports": "https://github.com/gagolews/stringi/issues",
    "repository": "https://cran.r-project.org/package=stringi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stringi Fast and Portable Character String Processing Facilities A collection of character string/text/natural language\n    processing tools for pattern searching (e.g., with 'Java'-like regular\n    expressions or the 'Unicode' collation algorithm), random string generation,\n    case mapping, string transliteration, concatenation, sorting, padding,\n    wrapping, Unicode normalisation, date-time formatting and parsing,\n    and many more. They are fast, consistent, convenient, and -\n    thanks to 'ICU' (International Components for Unicode) -\n    portable across all locales and platforms. Documentation about 'stringi' is\n    provided via its website at <https://stringi.gagolewski.com/> and\n    the paper by Gagolewski (2022, <doi:10.18637/jss.v103.i02>).  "
  },
  {
    "id": 21555,
    "package_name": "strvalidator",
    "title": "Process Control and Validation of Forensic STR Kits",
    "description": "An open source platform for validation and process control.\n    Tools to analyze data from internal validation of forensic short tandem\n    repeat (STR) kits are provided. The tools are developed to provide\n    the necessary data to conform with guidelines for internal validation\n    issued by the European Network of Forensic Science Institutes (ENFSI)\n    DNA Working Group, and the Scientific Working Group on DNA Analysis Methods\n    (SWGDAM). A front-end graphical user interface is provided.\n    More information about each function can be found in the\n    respective help documentation.",
    "version": "2.4.2",
    "maintainer": "Oskar Hansson <oskhan@ous-hf.no>",
    "author": "Oskar Hansson [aut, cre]",
    "url": "https://sites.google.com/site/forensicapps/strvalidator",
    "bug_reports": "https://github.com/OskarHansson/strvalidator/issues",
    "repository": "https://cran.r-project.org/package=strvalidator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strvalidator Process Control and Validation of Forensic STR Kits An open source platform for validation and process control.\n    Tools to analyze data from internal validation of forensic short tandem\n    repeat (STR) kits are provided. The tools are developed to provide\n    the necessary data to conform with guidelines for internal validation\n    issued by the European Network of Forensic Science Institutes (ENFSI)\n    DNA Working Group, and the Scientific Working Group on DNA Analysis Methods\n    (SWGDAM). A front-end graphical user interface is provided.\n    More information about each function can be found in the\n    respective help documentation.  "
  },
  {
    "id": 21556,
    "package_name": "sts",
    "title": "Estimation of the Structural Topic and Sentiment-Discourse Model\nfor Text Analysis",
    "description": "The Structural Topic and Sentiment-Discourse (STS) model allows researchers to estimate topic models with document-level metadata that determines both topic prevalence and sentiment-discourse. The sentiment-discourse is modeled as a document-level latent variable for each topic that modulates the word frequency within a topic. These latent topic sentiment-discourse variables are controlled by the document-level metadata. The STS model can be useful for regression analysis with text data in addition to topic modeling\u2019s traditional use of descriptive analysis. The method was developed in Chen and Mankad (2024) <doi:10.1287/mnsc.2022.00261>. ",
    "version": "1.4",
    "maintainer": "Shawn Mankad <smankad@ncsu.edu>",
    "author": "Shawn Mankad [aut, cre],\n  Li Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sts Estimation of the Structural Topic and Sentiment-Discourse Model\nfor Text Analysis The Structural Topic and Sentiment-Discourse (STS) model allows researchers to estimate topic models with document-level metadata that determines both topic prevalence and sentiment-discourse. The sentiment-discourse is modeled as a document-level latent variable for each topic that modulates the word frequency within a topic. These latent topic sentiment-discourse variables are controlled by the document-level metadata. The STS model can be useful for regression analysis with text data in addition to topic modeling\u2019s traditional use of descriptive analysis. The method was developed in Chen and Mankad (2024) <doi:10.1287/mnsc.2022.00261>.   "
  },
  {
    "id": 21573,
    "package_name": "subscore",
    "title": "Computing Subscores in Classical Test Theory and Item Response\nTheory",
    "description": "Functions for computing test subscores using different\n    methods in both classical test theory (CTT) and item response theory (IRT). This\n    package enables three types of subscoring methods within the framework of CTT\n    and IRT, including (1) Wainer's augmentation method (Wainer et. al., 2001) \n    <doi:10.4324/9781410604729>, (2) Haberman's subscoring methods (Haberman, 2008) \n    <doi:10.3102/1076998607302636>, and (3) Yen's objective performance index (OPI; Yen, 1987) \n    <https://www.ets.org/research/policy_research_reports/publications/paper/1987/hrap>. \n    It also includes functions to compute Proportional Reduction \n    of Mean Squared Errors (PRMSEs) in Haberman's methods which are used to \n    examine whether test subscores are of added value. In addition, the package includes \n    a function to assess the local independence assumption of IRT with\n    Yen's Q3 statistic (Yen, 1984 <doi:10.1177/014662168400800201>; Yen, 1993 \n    <doi:10.1111/j.1745-3984.1993.tb00423.x>).",
    "version": "3.3",
    "maintainer": "Shenghai Dai <s.dai@wsu.edu>",
    "author": "Shenghai Dai [aut, cre],\n  Xiaolin Wang [aut],\n  Dubravka Svetina [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=subscore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "subscore Computing Subscores in Classical Test Theory and Item Response\nTheory Functions for computing test subscores using different\n    methods in both classical test theory (CTT) and item response theory (IRT). This\n    package enables three types of subscoring methods within the framework of CTT\n    and IRT, including (1) Wainer's augmentation method (Wainer et. al., 2001) \n    <doi:10.4324/9781410604729>, (2) Haberman's subscoring methods (Haberman, 2008) \n    <doi:10.3102/1076998607302636>, and (3) Yen's objective performance index (OPI; Yen, 1987) \n    <https://www.ets.org/research/policy_research_reports/publications/paper/1987/hrap>. \n    It also includes functions to compute Proportional Reduction \n    of Mean Squared Errors (PRMSEs) in Haberman's methods which are used to \n    examine whether test subscores are of added value. In addition, the package includes \n    a function to assess the local independence assumption of IRT with\n    Yen's Q3 statistic (Yen, 1984 <doi:10.1177/014662168400800201>; Yen, 1993 \n    <doi:10.1111/j.1745-3984.1993.tb00423.x>).  "
  },
  {
    "id": 21577,
    "package_name": "substackR",
    "title": "Access Substack Data via API",
    "description": "An interface to access data from Substack publications via API. \n    Users can fetch the latest, top, search for specific posts, or retrieve a single post by its slug. \n    This functionality is useful for developers and researchers looking to analyze Substack content or \n    integrate it into their applications. For more information, visit the API documentation at <https://substackapi.dev/introduction>.",
    "version": "0.1.15",
    "maintainer": "Jason Rand <info@posocap.com>",
    "author": "Jason Rand [aut, cre]",
    "url": "https://github.com/posocap/substackR",
    "bug_reports": "https://github.com/posocap/substackR/issues",
    "repository": "https://cran.r-project.org/package=substackR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "substackR Access Substack Data via API An interface to access data from Substack publications via API. \n    Users can fetch the latest, top, search for specific posts, or retrieve a single post by its slug. \n    This functionality is useful for developers and researchers looking to analyze Substack content or \n    integrate it into their applications. For more information, visit the API documentation at <https://substackapi.dev/introduction>.  "
  },
  {
    "id": 21595,
    "package_name": "summarytools",
    "title": "Tools to Quickly and Neatly Summarize Data",
    "description": "Data frame summaries, cross-tabulations,\n  weight-enabled frequency tables and common descriptive\n  (univariate) statistics in concise tables available in a\n  variety of formats (plain ASCII, Markdown and HTML). A good \n  point-of-entry for exploring data, both for experienced\n  and new R users.",
    "version": "1.1.4",
    "maintainer": "Dominic Comtois <dominic.comtois@gmail.com>",
    "author": "Dominic Comtois [aut, cre]",
    "url": "https://github.com/dcomtois/summarytools",
    "bug_reports": "https://github.com/dcomtois/summarytools/issues",
    "repository": "https://cran.r-project.org/package=summarytools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "summarytools Tools to Quickly and Neatly Summarize Data Data frame summaries, cross-tabulations,\n  weight-enabled frequency tables and common descriptive\n  (univariate) statistics in concise tables available in a\n  variety of formats (plain ASCII, Markdown and HTML). A good \n  point-of-entry for exploring data, both for experienced\n  and new R users.  "
  },
  {
    "id": 21603,
    "package_name": "superb",
    "title": "Summary Plots with Adjusted Error Bars",
    "description": "\n        Computes standard error and confidence interval of various descriptive statistics under \n        various designs and sampling schemes. The main function, superb(), return a plot. It can \n        also be used to obtain a dataframe with the statistics and their precision intervals \n        so that other plotting environments (e.g., Excel)\n        can be used. See Cousineau and colleagues (2021) <doi:10.1177/25152459211035109> \n        or Cousineau (2017) <doi:10.5709/acp-0214-z> for a review as well as Cousineau (2005)\n        <doi:10.20982/tqmp.01.1.p042>, Morey (2008) <doi:10.20982/tqmp.04.2.p061>, Baguley (2012)\n        <doi:10.3758/s13428-011-0123-7>, Cousineau & Laurencelle (2016) <doi:10.1037/met0000055>,\n        Cousineau & O'Brien (2014) <doi:10.3758/s13428-013-0441-z>, Calderini & Harding \n        <doi:10.20982/tqmp.15.1.p001> for specific references.\n        The documentation is available at <https://dcousin3.github.io/superb/> .",
    "version": "1.0.1",
    "maintainer": "Denis Cousineau <denis.cousineau@uottawa.ca>",
    "author": "Denis Cousineau [aut, cre],\n  Bradley Harding [ctb],\n  Marc-Andre Goulet [ctb],\n  Jesika Walker [art, pre]",
    "url": "https://github.com/dcousin3/superb/,\nhttps://CRAN.R-project.org/package=superb,\nhttps://dcousin3.github.io/superb/",
    "bug_reports": "https://github.com/dcousin3/superb/issues/",
    "repository": "https://cran.r-project.org/package=superb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "superb Summary Plots with Adjusted Error Bars \n        Computes standard error and confidence interval of various descriptive statistics under \n        various designs and sampling schemes. The main function, superb(), return a plot. It can \n        also be used to obtain a dataframe with the statistics and their precision intervals \n        so that other plotting environments (e.g., Excel)\n        can be used. See Cousineau and colleagues (2021) <doi:10.1177/25152459211035109> \n        or Cousineau (2017) <doi:10.5709/acp-0214-z> for a review as well as Cousineau (2005)\n        <doi:10.20982/tqmp.01.1.p042>, Morey (2008) <doi:10.20982/tqmp.04.2.p061>, Baguley (2012)\n        <doi:10.3758/s13428-011-0123-7>, Cousineau & Laurencelle (2016) <doi:10.1037/met0000055>,\n        Cousineau & O'Brien (2014) <doi:10.3758/s13428-013-0441-z>, Calderini & Harding \n        <doi:10.20982/tqmp.15.1.p001> for specific references.\n        The documentation is available at <https://dcousin3.github.io/superb/> .  "
  },
  {
    "id": 21662,
    "package_name": "surveydown",
    "title": "Markdown-Based Programmable Surveys Using 'Quarto' and 'shiny'",
    "description": "Generate programmable surveys using markdown and R code chunks. Surveys\n    are composed of two files: a survey.qmd 'Quarto' file defining the\n    survey content (pages, questions, etc), and an app.R file defining a\n    'shiny' app with global settings (libraries, database configuration,\n    etc.) and server configuration options (e.g., conditional skipping /\n    display, etc.). Survey data collected from respondents is stored in a\n    'PostgreSQL' database. Features include controls for conditional skip\n    logic (skip to a page based on an answer to a question), conditional\n    display logic (display a question based on an answer to a question), a\n    customizable progress bar, and a wide variety of question types,\n    including multiple choice (single choice and multiple choices),\n    select, text, numeric, multiple choice buttons, text area, and dates.\n    Because the surveys render into a 'shiny' app, designers can also\n    leverage the reactive capabilities of 'shiny' to create dynamic and\n    interactive surveys.",
    "version": "0.14.0",
    "maintainer": "John Paul Helveston <john.helveston@gmail.com>",
    "author": "John Paul Helveston [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2657-9191>),\n  Pingfan Hu [aut, cph] (ORCID: <https://orcid.org/0009-0001-4877-4844>),\n  Bogdan Bunea [aut, cph] (ORCID:\n    <https://orcid.org/0009-0006-2942-0588>),\n  Stefan Munnes [ctb]",
    "url": "https://pkg.surveydown.org",
    "bug_reports": "https://github.com/surveydown-dev/surveydown/issues",
    "repository": "https://cran.r-project.org/package=surveydown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "surveydown Markdown-Based Programmable Surveys Using 'Quarto' and 'shiny' Generate programmable surveys using markdown and R code chunks. Surveys\n    are composed of two files: a survey.qmd 'Quarto' file defining the\n    survey content (pages, questions, etc), and an app.R file defining a\n    'shiny' app with global settings (libraries, database configuration,\n    etc.) and server configuration options (e.g., conditional skipping /\n    display, etc.). Survey data collected from respondents is stored in a\n    'PostgreSQL' database. Features include controls for conditional skip\n    logic (skip to a page based on an answer to a question), conditional\n    display logic (display a question based on an answer to a question), a\n    customizable progress bar, and a wide variety of question types,\n    including multiple choice (single choice and multiple choices),\n    select, text, numeric, multiple choice buttons, text area, and dates.\n    Because the surveys render into a 'shiny' app, designers can also\n    leverage the reactive capabilities of 'shiny' to create dynamic and\n    interactive surveys.  "
  },
  {
    "id": 21678,
    "package_name": "survivalVignettes",
    "title": "Survival Analysis Vignettes and Optional Datasets",
    "description": "Vignettes for the 'survival' package. Split from the 'survival' \n    package since the vignettes were getting large. Also, since 'survival' is a \n    recommended package it cannot make use of other packages outside of \n    base+recommended (e.g. 'rmarkdown').",
    "version": "0.1.6",
    "maintainer": "Elizabeth Atkinson <atkinson@mayo.edu>",
    "author": "Terry M Therneau [aut],\n  Elizabeth Atkinson [cre, ctb],\n  Cynthia Crowson [ctb]",
    "url": "https://github.com/bethatkinson/survivalVignettes",
    "bug_reports": "https://github.com/bethatkinson/survivalVignettes/issues",
    "repository": "https://cran.r-project.org/package=survivalVignettes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survivalVignettes Survival Analysis Vignettes and Optional Datasets Vignettes for the 'survival' package. Split from the 'survival' \n    package since the vignettes were getting large. Also, since 'survival' is a \n    recommended package it cannot make use of other packages outside of \n    base+recommended (e.g. 'rmarkdown').  "
  },
  {
    "id": 21700,
    "package_name": "svSweave",
    "title": "'SciViews' - 'Sweave', 'Knitr' and R Markdown Companion\nFunctions",
    "description": "Functions to enumerate and reference figures, tables and equations\n  in R Markdown documents that do not support these features (thus not\n  'bookdown' or 'quarto'. Supporting functions for using 'Sweave' and 'Knitr'\n  with 'LyX'. ",
    "version": "1.0.0",
    "maintainer": "Philippe Grosjean <phgrosjean@sciviews.org>",
    "author": "Philippe Grosjean [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2694-9471>)",
    "url": "https://github.com/SciViews/svSweave,\nhttps://www.sciviews.org/svSweave/",
    "bug_reports": "https://github.com/SciViews/svSweave/issues",
    "repository": "https://cran.r-project.org/package=svSweave",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svSweave 'SciViews' - 'Sweave', 'Knitr' and R Markdown Companion\nFunctions Functions to enumerate and reference figures, tables and equations\n  in R Markdown documents that do not support these features (thus not\n  'bookdown' or 'quarto'. Supporting functions for using 'Sweave' and 'Knitr'\n  with 'LyX'.   "
  },
  {
    "id": 21736,
    "package_name": "swdpwr",
    "title": "Power Calculation for Stepped Wedge Cluster Randomized Trials",
    "description": "To meet the needs of statistical power calculation for stepped wedge cluster randomized trials, we developed this software. Different parameters can be specified by users for different scenarios, including: cross-sectional and cohort designs, binary and continuous outcomes, marginal (GEE) and conditional models (mixed effects model), three link functions (identity, log, logit links), with and without time effects (the default specification assumes no-time-effect) under exchangeable, nested exchangeable and block exchangeable correlation structures. Unequal numbers of clusters per sequence are also allowed. The methods included in this package: Zhou et al. (2020) <doi:10.1093/biostatistics/kxy031>, Li et al. (2018) <doi:10.1111/biom.12918>. Supplementary documents can be found at: <https://ysph.yale.edu/cmips/research/software/study-design-power-calculation/swdpwr/>. The Shiny app for swdpwr can be accessed at: <https://jiachenchen322.shinyapps.io/swdpwr_shinyapp/>. The package also includes functions that perform calculations for the intra-cluster correlation coefficients based on the random effects variances as input variables for continuous and binary outcomes, respectively. ",
    "version": "1.11",
    "maintainer": "Jiachen Chen <jiachen.chen322@gmail.com>",
    "author": "Jiachen Chen [cre, aut],\n  Xin Zhou [aut],\n  Fan Li [aut],\n  Donna Spiegelman. [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=swdpwr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "swdpwr Power Calculation for Stepped Wedge Cluster Randomized Trials To meet the needs of statistical power calculation for stepped wedge cluster randomized trials, we developed this software. Different parameters can be specified by users for different scenarios, including: cross-sectional and cohort designs, binary and continuous outcomes, marginal (GEE) and conditional models (mixed effects model), three link functions (identity, log, logit links), with and without time effects (the default specification assumes no-time-effect) under exchangeable, nested exchangeable and block exchangeable correlation structures. Unequal numbers of clusters per sequence are also allowed. The methods included in this package: Zhou et al. (2020) <doi:10.1093/biostatistics/kxy031>, Li et al. (2018) <doi:10.1111/biom.12918>. Supplementary documents can be found at: <https://ysph.yale.edu/cmips/research/software/study-design-power-calculation/swdpwr/>. The Shiny app for swdpwr can be accessed at: <https://jiachenchen322.shinyapps.io/swdpwr_shinyapp/>. The package also includes functions that perform calculations for the intra-cluster correlation coefficients based on the random effects variances as input variables for continuous and binary outcomes, respectively.   "
  },
  {
    "id": 21744,
    "package_name": "swipeR",
    "title": "Carousels using the 'JavaScript' Library 'Swiper'",
    "description": "Create carousels using the 'JavaScript' library 'Swiper' and\n    the package 'htmlwidgets'. The carousels can be displayed in the\n    'RStudio' viewer pane, in 'Shiny' applications and in 'R markdown'\n    documents. The package also provides a 'RStudio' addin allowing to \n    choose image files and to display them in the viewer pane.",
    "version": "1.1.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Vladimir Kharlampidi [cph] ('swiper' library)",
    "url": "https://github.com/stla/swipeR",
    "bug_reports": "https://github.com/stla/swipeR/issues",
    "repository": "https://cran.r-project.org/package=swipeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "swipeR Carousels using the 'JavaScript' Library 'Swiper' Create carousels using the 'JavaScript' library 'Swiper' and\n    the package 'htmlwidgets'. The carousels can be displayed in the\n    'RStudio' viewer pane, in 'Shiny' applications and in 'R markdown'\n    documents. The package also provides a 'RStudio' addin allowing to \n    choose image files and to display them in the viewer pane.  "
  },
  {
    "id": 21751,
    "package_name": "sylcount",
    "title": "Syllable Counting and Readability Measurements",
    "description": "An English language syllable counter, plus readability score\n    measure-er. For readability, we support 'Flesch' Reading Ease and\n    'Flesch-Kincaid' Grade Level ('Kincaid' 'et al'. 1975)\n    <https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1055&context=istlibrary>,\n    Automated Readability Index ('Senter' and Smith 1967)\n    <https://apps.dtic.mil/sti/citations/AD0667273>,\n    Simple Measure of Gobbledygook (McLaughlin 1969),\n    and 'Coleman-Liau' (Coleman and 'Liau' 1975) <doi:10.1037/h0076540>. The\n    package has been carefully optimized and should be very efficient, both in\n    terms of run time performance and memory consumption. The main methods are\n    'vectorized' by document, and scores for multiple documents are computed in\n    parallel via 'OpenMP'.",
    "version": "0.2-6",
    "maintainer": "Drew Schmidt <wrathematics@gmail.com>",
    "author": "Drew Schmidt [aut, cre]",
    "url": "https://github.com/wrathematics/sylcount",
    "bug_reports": "https://github.com/wrathematics/sylcount/issues",
    "repository": "https://cran.r-project.org/package=sylcount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sylcount Syllable Counting and Readability Measurements An English language syllable counter, plus readability score\n    measure-er. For readability, we support 'Flesch' Reading Ease and\n    'Flesch-Kincaid' Grade Level ('Kincaid' 'et al'. 1975)\n    <https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1055&context=istlibrary>,\n    Automated Readability Index ('Senter' and Smith 1967)\n    <https://apps.dtic.mil/sti/citations/AD0667273>,\n    Simple Measure of Gobbledygook (McLaughlin 1969),\n    and 'Coleman-Liau' (Coleman and 'Liau' 1975) <doi:10.1037/h0076540>. The\n    package has been carefully optimized and should be very efficient, both in\n    terms of run time performance and memory consumption. The main methods are\n    'vectorized' by document, and scores for multiple documents are computed in\n    parallel via 'OpenMP'.  "
  },
  {
    "id": 21753,
    "package_name": "syllogi",
    "title": "Collection of Data Sets for Teaching Purposes",
    "description": "Collection (syllogi in greek) of real and fictitious data sets for teaching purposes.\n\t     The datasets were manually entered by the author from the respective references as listed in the individual dataset documentation.\n\t     The fictions datasets are the creation of the author, that he has found useful for teaching statistics. ",
    "version": "1.0.4",
    "maintainer": "Jared Studyvin <studyvinstat@gmail.com>",
    "author": "Jared Studyvin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=syllogi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "syllogi Collection of Data Sets for Teaching Purposes Collection (syllogi in greek) of real and fictitious data sets for teaching purposes.\n\t     The datasets were manually entered by the author from the respective references as listed in the individual dataset documentation.\n\t     The fictions datasets are the creation of the author, that he has found useful for teaching statistics.   "
  },
  {
    "id": 21754,
    "package_name": "sylly",
    "title": "Hyphenation and Syllable Counting for Text Analysis",
    "description": "Provides the hyphenation algorithm used for 'TeX'/'LaTeX' and similar software, as proposed by Liang (1983, <https://tug.org/docs/liang/>). Mainly contains the\n                    function hyphen() to be used for hyphenation/syllable counting of text objects. It was originally developed for and part of the 'koRpus' package, but later\n                    released as a separate package so it's lighter to have this particular functionality available for other packages. Support for various languages needs be added\n                    on-the-fly or by plugin packages (<https://undocumeantit.github.io/repos/>); this package does not include any language specific data. Due to some restrictions\n                    on CRAN, the full package sources are only available from the project homepage. To ask for help, report bugs, request features, or discuss the development of\n                    the package, please subscribe to the koRpus-dev mailing list (<http://korpusml.reaktanz.de>).",
    "version": "0.1-6",
    "maintainer": "Meik Michalke <meik.michalke@hhu.de>",
    "author": "Meik Michalke [aut, cre]",
    "url": "https://reaktanz.de/?c=hacking&s=sylly",
    "bug_reports": "https://github.com/unDocUMeantIt/sylly/issues",
    "repository": "https://cran.r-project.org/package=sylly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sylly Hyphenation and Syllable Counting for Text Analysis Provides the hyphenation algorithm used for 'TeX'/'LaTeX' and similar software, as proposed by Liang (1983, <https://tug.org/docs/liang/>). Mainly contains the\n                    function hyphen() to be used for hyphenation/syllable counting of text objects. It was originally developed for and part of the 'koRpus' package, but later\n                    released as a separate package so it's lighter to have this particular functionality available for other packages. Support for various languages needs be added\n                    on-the-fly or by plugin packages (<https://undocumeantit.github.io/repos/>); this package does not include any language specific data. Due to some restrictions\n                    on CRAN, the full package sources are only available from the project homepage. To ask for help, report bugs, request features, or discuss the development of\n                    the package, please subscribe to the koRpus-dev mailing list (<http://korpusml.reaktanz.de>).  "
  },
  {
    "id": 21755,
    "package_name": "sylly.en",
    "title": "Language Support for 'sylly' Package: English",
    "description": "Adds support for the English language to the 'sylly'\n        package. Due to some restrictions on CRAN, the full package\n        sources are only available from the project homepage. To ask\n        for help, report bugs, suggest feature improvements, or discuss\n        the global development of the package, please consider\n        subscribing to the koRpus-dev mailing list\n        (<http://korpusml.reaktanz.de>).",
    "version": "0.1-3",
    "maintainer": "Meik Michalke <meik.michalke@hhu.de>",
    "author": "Meik Michalke [aut, cre]",
    "url": "http://reaktanz.de/?c=hacking&s=koRpus",
    "bug_reports": "https://github.com/unDocUMeantIt/sylly.en/issues",
    "repository": "https://cran.r-project.org/package=sylly.en",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sylly.en Language Support for 'sylly' Package: English Adds support for the English language to the 'sylly'\n        package. Due to some restrictions on CRAN, the full package\n        sources are only available from the project homepage. To ask\n        for help, report bugs, suggest feature improvements, or discuss\n        the global development of the package, please consider\n        subscribing to the koRpus-dev mailing list\n        (<http://korpusml.reaktanz.de>).  "
  },
  {
    "id": 21759,
    "package_name": "symbol.equation.gpt",
    "title": "Simple User Interface to Build Equations and Add Symbols",
    "description": "Powerful user interface for adding symbols, smileys, arrows, building mathematical equations using 'LaTeX' or 'r2symbols'. Built for use in development of 'Markdown' and 'Shiny' Outputs.",
    "version": "1.1.4",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://symbols-ui.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/symbol.equation.gpt/issues",
    "repository": "https://cran.r-project.org/package=symbol.equation.gpt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "symbol.equation.gpt Simple User Interface to Build Equations and Add Symbols Powerful user interface for adding symbols, smileys, arrows, building mathematical equations using 'LaTeX' or 'r2symbols'. Built for use in development of 'Markdown' and 'Shiny' Outputs.  "
  },
  {
    "id": 21786,
    "package_name": "syuzhet",
    "title": "Extracts Sentiment and Sentiment-Derived Plot Arcs from Text",
    "description": "Extracts sentiment and sentiment-derived plot arcs\n    from text using a variety of sentiment dictionaries conveniently\n    packaged for consumption by R users.  Implemented dictionaries include\n    \"syuzhet\" (default) developed in the Nebraska Literary Lab\n    \"afinn\" developed by Finn \u00c5rup Nielsen, \"bing\" developed by Minqing Hu\n    and Bing Liu, and \"nrc\" developed by Mohammad, Saif M. and Turney, Peter D.\n    Applicable references are available in README.md and in the documentation\n    for the \"get_sentiment\" function.  The package also provides a hack for\n    implementing Stanford's coreNLP sentiment parser. The package provides\n    several methods for plot arc normalization.",
    "version": "1.0.7",
    "maintainer": "Matthew Jockers <mjockers@gmail.com>",
    "author": "Matthew Jockers [aut, cre]",
    "url": "https://github.com/mjockers/syuzhet",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=syuzhet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "syuzhet Extracts Sentiment and Sentiment-Derived Plot Arcs from Text Extracts sentiment and sentiment-derived plot arcs\n    from text using a variety of sentiment dictionaries conveniently\n    packaged for consumption by R users.  Implemented dictionaries include\n    \"syuzhet\" (default) developed in the Nebraska Literary Lab\n    \"afinn\" developed by Finn \u00c5rup Nielsen, \"bing\" developed by Minqing Hu\n    and Bing Liu, and \"nrc\" developed by Mohammad, Saif M. and Turney, Peter D.\n    Applicable references are available in README.md and in the documentation\n    for the \"get_sentiment\" function.  The package also provides a hack for\n    implementing Stanford's coreNLP sentiment parser. The package provides\n    several methods for plot arc normalization.  "
  },
  {
    "id": 21794,
    "package_name": "tab",
    "title": "Create Summary Tables for Statistical Reports",
    "description": "Contains functions for creating various types of summary tables, e.g. comparing characteristics across levels of a categorical variable and summarizing fitted generalized linear models, generalized estimating equations, and Cox proportional hazards models. Functions are available to handle data from simple random samples as well as complex surveys.",
    "version": "5.1.1",
    "maintainer": "Dane R. Van Domelen <vandomed@gmail.com>",
    "author": "Dane R. Van Domelen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tab Create Summary Tables for Statistical Reports Contains functions for creating various types of summary tables, e.g. comparing characteristics across levels of a categorical variable and summarizing fitted generalized linear models, generalized estimating equations, and Cox proportional hazards models. Functions are available to handle data from simple random samples as well as complex surveys.  "
  },
  {
    "id": 21795,
    "package_name": "tab2xml",
    "title": "XML Generation from Tables",
    "description": "Converting structured data from tables into XML format using\n    predefined templates ensures consistency and flexibility, making it\n    ideal for data exchange, reporting, and automated workflows.",
    "version": "1.1.0",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/tab2xml/,\nhttps://github.com/josesamos/tab2xml",
    "bug_reports": "https://github.com/josesamos/tab2xml/issues",
    "repository": "https://cran.r-project.org/package=tab2xml",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tab2xml XML Generation from Tables Converting structured data from tables into XML format using\n    predefined templates ensures consistency and flexibility, making it\n    ideal for data exchange, reporting, and automated workflows.  "
  },
  {
    "id": 21801,
    "package_name": "table.glue",
    "title": "Make and Apply Customized Rounding Specifications for Tables",
    "description": "Translate double and integer valued data into\n    character values formatted for tabulation in manuscripts\n    or other types of academic reports. ",
    "version": "0.0.5",
    "maintainer": "Byron Jaeger <bjaeger@wakehealth.edu>",
    "author": "Byron Jaeger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7399-2299>)",
    "url": "https://github.com/bcjaeger/table.glue",
    "bug_reports": "https://github.com/bcjaeger/table.glue/issues",
    "repository": "https://cran.r-project.org/package=table.glue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "table.glue Make and Apply Customized Rounding Specifications for Tables Translate double and integer valued data into\n    character values formatted for tabulation in manuscripts\n    or other types of academic reports.   "
  },
  {
    "id": 21803,
    "package_name": "tableHTML",
    "title": "A Tool to Create HTML Tables",
    "description": "A tool to create and style HTML tables with CSS. These can\n    be exported and used in any application that accepts HTML (e.g. 'shiny',\n    'rmarkdown', 'PowerPoint'). It also provides functions to create CSS files\n    (which also work with shiny).",
    "version": "2.1.3",
    "maintainer": "Theo Boutaris <teoboot2007@hotmail.com>",
    "author": "Theo Boutaris [aut, cre, cph],\n  Clemens Zauchner [aut],\n  Dana Jomar [aut]",
    "url": "https://github.com/LyzandeR/tableHTML",
    "bug_reports": "https://github.com/LyzandeR/tableHTML/issues",
    "repository": "https://cran.r-project.org/package=tableHTML",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tableHTML A Tool to Create HTML Tables A tool to create and style HTML tables with CSS. These can\n    be exported and used in any application that accepts HTML (e.g. 'shiny',\n    'rmarkdown', 'PowerPoint'). It also provides functions to create CSS files\n    (which also work with shiny).  "
  },
  {
    "id": 21817,
    "package_name": "tabr",
    "title": "Music Notation Syntax, Manipulation, Analysis and Transcription\nin R",
    "description": "Provides a music notation syntax and a collection of music\n    programming functions for generating, manipulating, organizing, and analyzing\n    musical information in R. Music syntax can be entered directly in character\n    strings, for example to quickly transcribe short pieces of music. The\n    package contains functions for directly performing various mathematical,\n    logical and organizational operations and musical transformations on special\n    object classes that facilitate working with music data and notation. The\n    same music data can be organized in tidy data frames for a familiar and\n    powerful approach to the analysis of large amounts of structured music data.\n    Functions are available for mapping seamlessly between these formats and\n    their representations of musical information. The package also provides an\n    API to 'LilyPond' (<https://lilypond.org/>) for transcribing musical\n    representations in R into tablature (\"tabs\") and sheet music. 'LilyPond' is\n    open source music engraving software for generating high quality sheet music\n    based on markup syntax. The package generates 'LilyPond' files from R code\n    and can pass them to the 'LilyPond' command line interface to be rendered\n    into sheet music PDF files or inserted into R markdown documents. The\n    package offers nominal MIDI file output support in conjunction with\n    rendering sheet music. The package can read MIDI files and attempts to\n    structure the MIDI data to integrate as best as possible with the data\n    structures and functionality found throughout the package.",
    "version": "0.5.4",
    "maintainer": "Matthew Leonawicz <rpkgs@pm.me>",
    "author": "Matthew Leonawicz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9452-2771>)",
    "url": "https://github.com/leonawicz/tabr,\nhttps://leonawicz.github.io/tabr/",
    "bug_reports": "https://github.com/leonawicz/tabr/issues",
    "repository": "https://cran.r-project.org/package=tabr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabr Music Notation Syntax, Manipulation, Analysis and Transcription\nin R Provides a music notation syntax and a collection of music\n    programming functions for generating, manipulating, organizing, and analyzing\n    musical information in R. Music syntax can be entered directly in character\n    strings, for example to quickly transcribe short pieces of music. The\n    package contains functions for directly performing various mathematical,\n    logical and organizational operations and musical transformations on special\n    object classes that facilitate working with music data and notation. The\n    same music data can be organized in tidy data frames for a familiar and\n    powerful approach to the analysis of large amounts of structured music data.\n    Functions are available for mapping seamlessly between these formats and\n    their representations of musical information. The package also provides an\n    API to 'LilyPond' (<https://lilypond.org/>) for transcribing musical\n    representations in R into tablature (\"tabs\") and sheet music. 'LilyPond' is\n    open source music engraving software for generating high quality sheet music\n    based on markup syntax. The package generates 'LilyPond' files from R code\n    and can pass them to the 'LilyPond' command line interface to be rendered\n    into sheet music PDF files or inserted into R markdown documents. The\n    package offers nominal MIDI file output support in conjunction with\n    rendering sheet music. The package can read MIDI files and attempts to\n    structure the MIDI data to integrate as best as possible with the data\n    structures and functionality found throughout the package.  "
  },
  {
    "id": 21819,
    "package_name": "tabshiftr",
    "title": "Reshape Disorganised Messy Data",
    "description": "Helps the user to build and register schema descriptions of \n    disorganised (messy) tables. Disorganised tables are tables that are \n    not in a topologically coherent form, where packages such as 'tidyr' could \n    be used for reshaping. The schema description documents the arrangement of \n    input tables and is used to reshape them into a standardised (tidy) output \n    format.",
    "version": "0.4.1",
    "maintainer": "Steffen Ehrmann <steffen.ehrmann@posteo.de>",
    "author": "Steffen Ehrmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2958-0796>),\n  Tsvetelina Tomova [ctb],\n  Carsten Meyer [aut] (ORCID: <https://orcid.org/0000-0003-3927-5856>),\n  Abdualmaged Alhemiary [ctb],\n  Amelie Haas [ctb],\n  Annika Ertel [ctb],\n  Arne R\u00fcmmler [ctb] (ORCID: <https://orcid.org/0000-0001-8637-9071>),\n  Caroline Busse [ctb]",
    "url": "https://github.com/luckinet/tabshiftr",
    "bug_reports": "https://github.com/luckinet/tabshiftr/issues",
    "repository": "https://cran.r-project.org/package=tabshiftr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabshiftr Reshape Disorganised Messy Data Helps the user to build and register schema descriptions of \n    disorganised (messy) tables. Disorganised tables are tables that are \n    not in a topologically coherent form, where packages such as 'tidyr' could \n    be used for reshaping. The schema description documents the arrangement of \n    input tables and is used to reshape them into a standardised (tidy) output \n    format.  "
  },
  {
    "id": 21820,
    "package_name": "tabtibble",
    "title": "Simplify Reporting Many Tables",
    "description": "Simplify reporting many tables by creating tibbles of tables. With\n  'tabtibble', a tibble of tables is created with captions and automatic\n  printing using 'knit_print()'.",
    "version": "0.0.1",
    "maintainer": "Bill Denney <wdenney@humanpredictions.com>",
    "author": "Bill Denney [aut, cre] (ORCID: <https://orcid.org/0000-0002-5759-428X>)",
    "url": "https://github.com/humanpred/tabtibble,\nhttps://humanpred.github.io/tabtibble/",
    "bug_reports": "https://github.com/humanpred/tabtibble/issues",
    "repository": "https://cran.r-project.org/package=tabtibble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabtibble Simplify Reporting Many Tables Simplify reporting many tables by creating tibbles of tables. With\n  'tabtibble', a tibble of tables is created with captions and automatic\n  printing using 'knit_print()'.  "
  },
  {
    "id": 21842,
    "package_name": "tangram",
    "title": "The Grammar of Tables",
    "description": "Provides an extensible formula system to quickly and easily create\n    production quality tables. The processing steps are a formula parser,\n    statistical content generation from data as defined by formula, followed by\n    rendering into a table. Each step of the processing is separate and user\n    definable thus creating a set of composable building blocks for\n    highly customizable table generation. A user is not limited by any of the \n    choices of the package creator other than the formula grammar. For example,\n    one could chose to add a different S3 rendering function and output a format\n    not provided in the default package, or possibly one would rather have Gini\n    coefficients for their statistical content in a resulting table. Routines to\n    achieve New England Journal of Medicine style, Lancet style and\n    Hmisc::summaryM() statistics are provided. The package contains rendering\n    for HTML5, Rmarkdown and an indexing format for use in tracing and tracking\n    are provided.",
    "version": "0.8.3",
    "maintainer": "Shawn Garbett <Shawn.Garbett@vumc.org>",
    "author": "Shawn Garbett [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4079-5621>),\n  Thomas Stewart [ctb],\n  Jennifer Thompson [ctb],\n  Frank Harrell [ctb],\n  Ahra Kim [ctb]",
    "url": "https://github.com/spgarbet/tangram",
    "bug_reports": "https://github.com/spgarbet/tangram/issues",
    "repository": "https://cran.r-project.org/package=tangram",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tangram The Grammar of Tables Provides an extensible formula system to quickly and easily create\n    production quality tables. The processing steps are a formula parser,\n    statistical content generation from data as defined by formula, followed by\n    rendering into a table. Each step of the processing is separate and user\n    definable thus creating a set of composable building blocks for\n    highly customizable table generation. A user is not limited by any of the \n    choices of the package creator other than the formula grammar. For example,\n    one could chose to add a different S3 rendering function and output a format\n    not provided in the default package, or possibly one would rather have Gini\n    coefficients for their statistical content in a resulting table. Routines to\n    achieve New England Journal of Medicine style, Lancet style and\n    Hmisc::summaryM() statistics are provided. The package contains rendering\n    for HTML5, Rmarkdown and an indexing format for use in tracing and tracking\n    are provided.  "
  },
  {
    "id": 21856,
    "package_name": "tatoo",
    "title": "Combine and Export Data Frames",
    "description": "Functions to combine data.frames in ways that require\n    additional effort in base R, and to add metadata (id, title, ...) that\n    can be used for printing and xlsx export. The 'Tatoo_report' class is\n    provided as a convenient helper to write several such tables to a\n    workbook, one table per worksheet. Tatoo is built on top of\n    'openxlsx', but intimate knowledge of that package is not required to\n    use tatoo.",
    "version": "1.1.3",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre]",
    "url": "https://github.com/statistikat/tatoo",
    "bug_reports": "https://github.com/statistikat/tatoo/issues",
    "repository": "https://cran.r-project.org/package=tatoo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tatoo Combine and Export Data Frames Functions to combine data.frames in ways that require\n    additional effort in base R, and to add metadata (id, title, ...) that\n    can be used for printing and xlsx export. The 'Tatoo_report' class is\n    provided as a convenient helper to write several such tables to a\n    workbook, one table per worksheet. Tatoo is built on top of\n    'openxlsx', but intimate knowledge of that package is not required to\n    use tatoo.  "
  },
  {
    "id": 21859,
    "package_name": "tauturri",
    "title": "Get Data Out of 'Tautulli' (Formerly 'PlexPy')",
    "description": "'Tautulli' (<http://tautulli.com>) is a monitoring application for 'Plex' Media Servers \n  (<https://www.plex.tv>) which collects a lot of data about media items and server \n  usage such as play counts. This package interacts with the 'Tautulli' API of any \n  specified server to get said data into R. The 'Tautulli' API documentation is available\n  at <https://github.com/Tautulli/Tautulli/blob/master/API.md>.",
    "version": "0.3.0",
    "maintainer": "Lukas Burk <lukas@quantenbrot.de>",
    "author": "Lukas Burk [aut, cre]",
    "url": "https://github.com/jemus42/tauturri",
    "bug_reports": "https://github.com/jemus42/tauturri/issues",
    "repository": "https://cran.r-project.org/package=tauturri",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tauturri Get Data Out of 'Tautulli' (Formerly 'PlexPy') 'Tautulli' (<http://tautulli.com>) is a monitoring application for 'Plex' Media Servers \n  (<https://www.plex.tv>) which collects a lot of data about media items and server \n  usage such as play counts. This package interacts with the 'Tautulli' API of any \n  specified server to get said data into R. The 'Tautulli' API documentation is available\n  at <https://github.com/Tautulli/Tautulli/blob/master/API.md>.  "
  },
  {
    "id": 21861,
    "package_name": "taxnames",
    "title": "Formatting Taxonomic Names in Markdown",
    "description": "\n    A collection of functions used to format taxonomic names in Markdown\n    documents. Those functions work with data structured according to\n    Alvarez and Luebert (2018) <doi:10.3897/bdj.6.e23635>.",
    "version": "0.1.0",
    "maintainer": "Miguel Alvarez <kamapu78@gmail.com>",
    "author": "Miguel Alvarez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1500-1834>)",
    "url": "https://github.com/kamapu/taxnames",
    "bug_reports": "https://github.com/kamapu/taxnames/issues",
    "repository": "https://cran.r-project.org/package=taxnames",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "taxnames Formatting Taxonomic Names in Markdown \n    A collection of functions used to format taxonomic names in Markdown\n    documents. Those functions work with data structured according to\n    Alvarez and Luebert (2018) <doi:10.3897/bdj.6.e23635>.  "
  },
  {
    "id": 21898,
    "package_name": "teal.modules.clinical",
    "title": "'teal' Modules for Standard Clinical Outputs",
    "description": "Provides user-friendly tools for creating and customizing\n    clinical trial reports. By leveraging the 'teal' framework, this\n    package provides 'teal' modules to easily create an interactive panel\n    that allows for seamless adjustments to data presentation, thereby\n    streamlining the creation of detailed and accurate reports.",
    "version": "0.12.0",
    "maintainer": "Dawid Kaledkowski <dawid.kaledkowski@roche.com>",
    "author": "Joe Zhu [aut] (ORCID: <https://orcid.org/0000-0001-7566-2787>),\n  Jana Stoilova [aut],\n  Davide Garolini [aut],\n  Emily de la Rua [aut],\n  Abinaya Yogasekaram [aut],\n  Mahmoud Hallal [aut],\n  Dawid Kaledkowski [aut, cre],\n  Rosemary Li [aut],\n  Heng Wang [aut],\n  Pawel Rucki [aut],\n  Nikolas Burkoff [aut],\n  Konrad Pagacz [aut],\n  Vaakesan Sundrelingam [ctb],\n  Francois Collin [ctb],\n  Imanol Zubizarreta [ctb],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://insightsengineering.github.io/teal.modules.clinical/,\nhttps://github.com/insightsengineering/teal.modules.clinical/",
    "bug_reports": "https://github.com/insightsengineering/teal.modules.clinical/issues",
    "repository": "https://cran.r-project.org/package=teal.modules.clinical",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "teal.modules.clinical 'teal' Modules for Standard Clinical Outputs Provides user-friendly tools for creating and customizing\n    clinical trial reports. By leveraging the 'teal' framework, this\n    package provides 'teal' modules to easily create an interactive panel\n    that allows for seamless adjustments to data presentation, thereby\n    streamlining the creation of detailed and accurate reports.  "
  },
  {
    "id": 21900,
    "package_name": "teal.reporter",
    "title": "Reporting Tools for 'shiny' Modules",
    "description": "Prebuilt 'shiny' modules containing tools for the generation\n    of 'rmarkdown' reports, supporting reproducible research and analysis.",
    "version": "0.6.0",
    "maintainer": "Dawid Kaledkowski <dawid.kaledkowski@roche.com>",
    "author": "Dawid Kaledkowski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9533-457X>),\n  Kartikeya Kirar [aut] (ORCID: <https://orcid.org/0009-0005-1258-4757>),\n  Marcin Kosinski [aut],\n  Maciej Nasinski [aut],\n  Konrad Pagacz [aut],\n  Mahmoud Hallal [aut],\n  Chendi Liao [rev],\n  Dony Unardi [rev],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/insightsengineering/teal.reporter,\nhttps://insightsengineering.github.io/teal.reporter/",
    "bug_reports": "https://github.com/insightsengineering/teal.reporter/issues",
    "repository": "https://cran.r-project.org/package=teal.reporter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "teal.reporter Reporting Tools for 'shiny' Modules Prebuilt 'shiny' modules containing tools for the generation\n    of 'rmarkdown' reports, supporting reproducible research and analysis.  "
  },
  {
    "id": 21947,
    "package_name": "testDriveR",
    "title": "Teaching Data for Statistics and Data Science",
    "description": "Provides data sets for teaching statistics and data science courses. \n    It includes a sample of data from John Edmund Kerrich's famous \n    coinflip experiment. These are data that I used for statistics. The package \n    also contains an R Markdown template with the required formatting for \n    assignments in my former courses.",
    "version": "0.5.3",
    "maintainer": "Christopher Prener <chris.prener@gmail.com>",
    "author": "Christopher Prener [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4310-9888>),\n  Bill Bradley [dtc],\n  NORC at the University of Chicago [dtc],\n  UN Inter-agency Group for Child Mortality Estimation [dtc],\n  U.S. Department of Energy [dtc]",
    "url": "https://chris-prener.github.io/testDriveR/,\nhttps://github.com/chris-prener/testDriveR",
    "bug_reports": "https://github.com/chris-prener/testDriveR/issues",
    "repository": "https://cran.r-project.org/package=testDriveR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testDriveR Teaching Data for Statistics and Data Science Provides data sets for teaching statistics and data science courses. \n    It includes a sample of data from John Edmund Kerrich's famous \n    coinflip experiment. These are data that I used for statistics. The package \n    also contains an R Markdown template with the required formatting for \n    assignments in my former courses.  "
  },
  {
    "id": 21951,
    "package_name": "testdat",
    "title": "Data Unit Testing for R",
    "description": "Test your data! An extension of the 'testthat' unit testing\n    framework with a family of functions and reporting tools for checking\n    and validating data frames.",
    "version": "0.4.4",
    "maintainer": "Danny Smith <danny@gorcha.org>",
    "author": "Danny Smith [aut, cre],\n  Kinto Behr [aut],\n  The Social Research Centre [cph]",
    "url": "https://socialresearchcentre.github.io/testdat/,\nhttps://github.com/socialresearchcentre/testdat",
    "bug_reports": "https://github.com/socialresearchcentre/testdat/issues",
    "repository": "https://cran.r-project.org/package=testdat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testdat Data Unit Testing for R Test your data! An extension of the 'testthat' unit testing\n    framework with a family of functions and reporting tools for checking\n    and validating data frames.  "
  },
  {
    "id": 21957,
    "package_name": "testthatdocs",
    "title": "Automated and Idempotent Unit Tests Documentation for\nReproducible Quality Assurance",
    "description": "\n    Automates documentation of test_that() calls within R test files. \n    The package scans test sources, extracts human-readable test titles (even when \n    composed with functions like paste() or glue::glue(), ... etc.), and generates \n    reproducible roxygen2-style listings that can be inserted both globally and \n    per-section. It ensures idempotent updates and supports customizable numbering \n    templates with hierarchical indices. Designed for developers, QA teams, and package\n    maintainers seeking consistent, self-documenting test inventories. ",
    "version": "1.0.23",
    "maintainer": "Rafal Urniaz <rafal.urniaz@cantab.net>",
    "author": "Rafal Urniaz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0192-2165>)",
    "url": "https://github.com/urniaz/testthatdocs",
    "bug_reports": "https://github.com/urniaz/testthatdocs/issues",
    "repository": "https://cran.r-project.org/package=testthatdocs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testthatdocs Automated and Idempotent Unit Tests Documentation for\nReproducible Quality Assurance \n    Automates documentation of test_that() calls within R test files. \n    The package scans test sources, extracts human-readable test titles (even when \n    composed with functions like paste() or glue::glue(), ... etc.), and generates \n    reproducible roxygen2-style listings that can be inserted both globally and \n    per-section. It ensures idempotent updates and supports customizable numbering \n    templates with hierarchical indices. Designed for developers, QA teams, and package\n    maintainers seeking consistent, self-documenting test inventories.   "
  },
  {
    "id": 21965,
    "package_name": "texPreview",
    "title": "Compile and Preview Snippets of 'LaTeX'",
    "description": "Compile snippets of 'LaTeX' directly into images\n    from the R console to view in the 'RStudio' viewer pane, Shiny apps\n    and 'RMarkdown' documents.",
    "version": "2.1.0",
    "maintainer": "Jonathan Sidi <yonicd@gmail.com>",
    "author": "Jonathan Sidi [aut, cre],\n  Daniel Polhamus [aut]",
    "url": "https://github.com/yonicd/texPreview",
    "bug_reports": "https://github.com/yonicd/texPreview/issues",
    "repository": "https://cran.r-project.org/package=texPreview",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "texPreview Compile and Preview Snippets of 'LaTeX' Compile snippets of 'LaTeX' directly into images\n    from the R console to view in the 'RStudio' viewer pane, Shiny apps\n    and 'RMarkdown' documents.  "
  },
  {
    "id": 21968,
    "package_name": "texreg",
    "title": "Conversion of R Regression Output to LaTeX or HTML Tables",
    "description": "Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. Details can be found in Leifeld (2013), JStatSoft <doi:10.18637/jss.v055.i08>.)",
    "version": "1.39.4",
    "maintainer": "Philip Leifeld <philip.leifeld@manchester.ac.uk>",
    "author": "Philip Leifeld [aut, cre],\n  Claudia Zucca [ctb]",
    "url": "https://github.com/leifeld/texreg/",
    "bug_reports": "https://github.com/leifeld/texreg/issues/",
    "repository": "https://cran.r-project.org/package=texreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "texreg Conversion of R Regression Output to LaTeX or HTML Tables Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. Details can be found in Leifeld (2013), JStatSoft <doi:10.18637/jss.v055.i08>.)  "
  },
  {
    "id": 21971,
    "package_name": "text2emotion",
    "title": "Emotion Analysis and Emoji Mapping for Text",
    "description": "Allows users to analyze text and classify emotions\n    such as happiness, sadness, anger, fear, and neutrality.\n    It combines text preprocessing, TF-IDF (Term Frequency-Inverse Document Frequency) \n    feature extraction, and Random Forest classification to predict emotions\n    and map them to corresponding emojis for enhanced sentiment visualization.",
    "version": "0.1.0",
    "maintainer": "Fangyi Wang <123090550@link.cuhk.edu.cn>",
    "author": "Yusong Zhao [aut],\n  Fangyi Wang [aut, cre],\n  Zisheng Qu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=text2emotion",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2emotion Emotion Analysis and Emoji Mapping for Text Allows users to analyze text and classify emotions\n    such as happiness, sadness, anger, fear, and neutrality.\n    It combines text preprocessing, TF-IDF (Term Frequency-Inverse Document Frequency) \n    feature extraction, and Random Forest classification to predict emotions\n    and map them to corresponding emojis for enhanced sentiment visualization.  "
  },
  {
    "id": 21972,
    "package_name": "text2map",
    "title": "R Tools for Text Matrices, Embeddings, and Networks",
    "description": "This is a collection of functions optimized for working with\n             with various kinds of text matrices. Focusing on \n             the text matrix as the primary object - represented \n             either as a base R dense matrix or a 'Matrix' package sparse \n             matrix - allows for a consistent and intuitive interface \n             that stays close to the underlying mathematical foundation \n             of computational text analysis. In particular, the package\n             includes functions for working with word embeddings, \n             text networks, and document-term matrices. Methods developed in \n             Stoltz and Taylor (2019) <doi:10.1007/s42001-019-00048-6>, \n             Taylor and Stoltz (2020) <doi:10.1007/s42001-020-00075-8>, \n             Taylor and Stoltz (2020) <doi:10.15195/v7.a23>, and\n             Stoltz and Taylor (2021) <doi:10.1016/j.poetic.2021.101567>.",
    "version": "0.2.0",
    "maintainer": "Dustin Stoltz <dss219@lehigh.edu>",
    "author": "Dustin Stoltz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4774-0765>),\n  Marshall Taylor [aut] (ORCID: <https://orcid.org/0000-0002-7440-0723>)",
    "url": "https://gitlab.com/culturalcartography/text2map",
    "bug_reports": "https://gitlab.com/culturalcartography/text2map/-/issues",
    "repository": "https://cran.r-project.org/package=text2map",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2map R Tools for Text Matrices, Embeddings, and Networks This is a collection of functions optimized for working with\n             with various kinds of text matrices. Focusing on \n             the text matrix as the primary object - represented \n             either as a base R dense matrix or a 'Matrix' package sparse \n             matrix - allows for a consistent and intuitive interface \n             that stays close to the underlying mathematical foundation \n             of computational text analysis. In particular, the package\n             includes functions for working with word embeddings, \n             text networks, and document-term matrices. Methods developed in \n             Stoltz and Taylor (2019) <doi:10.1007/s42001-019-00048-6>, \n             Taylor and Stoltz (2020) <doi:10.1007/s42001-020-00075-8>, \n             Taylor and Stoltz (2020) <doi:10.15195/v7.a23>, and\n             Stoltz and Taylor (2021) <doi:10.1016/j.poetic.2021.101567>.  "
  },
  {
    "id": 21976,
    "package_name": "text2vec",
    "title": "Modern Text Mining Framework for R",
    "description": "Fast and memory-friendly tools for text vectorization, topic\n    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package\n    provides a source-agnostic streaming API, which allows researchers to perform\n    analysis of collections of documents which are larger than available RAM. All\n    core functions are parallelized to benefit from multicore machines.",
    "version": "0.6.6",
    "maintainer": "Dmitriy Selivanov <selivanov.dmitriy@gmail.com>",
    "author": "Dmitriy Selivanov [aut, cre, cph],\n  Manuel Bickel [aut, cph] (Coherence measures for topic models),\n  Qing Wang [aut, cph] (Author of the WaprLDA C++ code)",
    "url": "http://text2vec.org",
    "bug_reports": "https://github.com/dselivanov/text2vec/issues",
    "repository": "https://cran.r-project.org/package=text2vec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "text2vec Modern Text Mining Framework for R Fast and memory-friendly tools for text vectorization, topic\n    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package\n    provides a source-agnostic streaming API, which allows researchers to perform\n    analysis of collections of documents which are larger than available RAM. All\n    core functions are parallelized to benefit from multicore machines.  "
  },
  {
    "id": 21977,
    "package_name": "textAnnotatoR",
    "title": "Interactive Text Annotation Tool with 'shiny' GUI",
    "description": "A lightweight and focused text annotation tool built with 'shiny'. Provides an \n    interactive graphical user interface for coding text documents,\n    managing code hierarchies, creating memos, and analyzing coding patterns.\n    Features include code co-occurrence analysis, visualization of coding patterns,\n    comparison of multiple coding sets, and export capabilities. Supports\n    collaborative qualitative research through standardized annotation formats\n    and analysis tools.",
    "version": "1.0.1",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9979-8272>)",
    "url": "https://github.com/chaoliu-cl/textAnnotatoR,\nhttp://liu-chao.site/textAnnotatoR/",
    "bug_reports": "https://github.com/chaoliu-cl/textAnnotatoR/issues",
    "repository": "https://cran.r-project.org/package=textAnnotatoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textAnnotatoR Interactive Text Annotation Tool with 'shiny' GUI A lightweight and focused text annotation tool built with 'shiny'. Provides an \n    interactive graphical user interface for coding text documents,\n    managing code hierarchies, creating memos, and analyzing coding patterns.\n    Features include code co-occurrence analysis, visualization of coding patterns,\n    comparison of multiple coding sets, and export capabilities. Supports\n    collaborative qualitative research through standardized annotation formats\n    and analysis tools.  "
  },
  {
    "id": 21979,
    "package_name": "textTinyR",
    "title": "Text Processing for Small or Big Data Files",
    "description": "It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.",
    "version": "1.1.8",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>)",
    "url": "https://github.com/mlampros/textTinyR",
    "bug_reports": "https://github.com/mlampros/textTinyR/issues",
    "repository": "https://cran.r-project.org/package=textTinyR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textTinyR Text Processing for Small or Big Data Files It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.  "
  },
  {
    "id": 21988,
    "package_name": "textir",
    "title": "Inverse Regression for Text Analysis",
    "description": "Multinomial (inverse) regression inference for text documents and associated attributes.  For details see: Taddy (2013 JASA) Multinomial Inverse Regression for Text Analysis <arXiv:1012.2098> and Taddy (2015, AoAS), Distributed Multinomial Regression, <arXiv:1311.6139>. A minimalist partial least squares routine is also included.  Note that the topic modeling capability of earlier 'textir' is now a separate package, 'maptpx'.",
    "version": "2.0-5",
    "maintainer": "Matt Taddy <mataddy@gmail.com>",
    "author": "Matt Taddy <mataddy@gmail.com>",
    "url": "http://taddylab.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textir Inverse Regression for Text Analysis Multinomial (inverse) regression inference for text documents and associated attributes.  For details see: Taddy (2013 JASA) Multinomial Inverse Regression for Text Analysis <arXiv:1012.2098> and Taddy (2015, AoAS), Distributed Multinomial Regression, <arXiv:1311.6139>. A minimalist partial least squares routine is also included.  Note that the topic modeling capability of earlier 'textir' is now a separate package, 'maptpx'.  "
  },
  {
    "id": 22029,
    "package_name": "this.path",
    "title": "Get Executing Script's Path",
    "description": "Determine the path of the executing script. Compatible\n        with several popular GUIs: 'Rgui', 'RStudio', 'Positron',\n        'VSCode', 'Jupyter', 'Emacs', and 'Rscript' (shell). Compatible\n        with several functions and packages: 'source()',\n        'sys.source()', 'debugSource()' in 'RStudio',\n        'compiler::loadcmp()', 'utils::Sweave()', 'box::use()',\n        'knitr::knit()', 'plumber::plumb()', 'shiny::runApp()',\n        'package:targets', and 'testthat::source_file()'.",
    "version": "2.7.1",
    "maintainer": "Iris Simmons <ikwsimmo@gmail.com>",
    "author": "Iris Simmons [aut, cre]",
    "url": "https://github.com/ArcadeAntics/this.path",
    "bug_reports": "https://github.com/ArcadeAntics/this.path/issues",
    "repository": "https://cran.r-project.org/package=this.path",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "this.path Get Executing Script's Path Determine the path of the executing script. Compatible\n        with several popular GUIs: 'Rgui', 'RStudio', 'Positron',\n        'VSCode', 'Jupyter', 'Emacs', and 'Rscript' (shell). Compatible\n        with several functions and packages: 'source()',\n        'sys.source()', 'debugSource()' in 'RStudio',\n        'compiler::loadcmp()', 'utils::Sweave()', 'box::use()',\n        'knitr::knit()', 'plumber::plumb()', 'shiny::runApp()',\n        'package:targets', and 'testthat::source_file()'.  "
  },
  {
    "id": 22032,
    "package_name": "thorn",
    "title": "'HTMLwidgets' Displaying Some 'WebGL' Shaders",
    "description": "Creates some 'WebGL' shaders. They can be used as the background of a 'Shiny' app. They also can be visualized in the 'RStudio' viewer pane or included in 'Rmd' documents, but this is pretty useless, besides contemplating them.",
    "version": "0.2.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent [aut, cre],\n  Scott Boyle [ctb, cph] ('Hamster.js' library),\n  Mathew Groves [ctb, cph] ('PixiJS' library),\n  Chad Engler [ctb, cph] ('PixiJS' library)",
    "url": "https://github.com/stla/thorn",
    "bug_reports": "https://github.com/stla/thorn/issues",
    "repository": "https://cran.r-project.org/package=thorn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "thorn 'HTMLwidgets' Displaying Some 'WebGL' Shaders Creates some 'WebGL' shaders. They can be used as the background of a 'Shiny' app. They also can be visualized in the 'RStudio' viewer pane or included in 'Rmd' documents, but this is pretty useless, besides contemplating them.  "
  },
  {
    "id": 22061,
    "package_name": "tidySEM",
    "title": "Tidy Structural Equation Modeling",
    "description": "A tidy workflow for generating, estimating, reporting,\n    and plotting structural equation models using 'lavaan', 'OpenMx', or\n    'Mplus'. Throughout this workflow, elements of syntax, results, and graphs\n    are represented as 'tidy' data, making them easy to customize.\n    Includes functionality to estimate latent class analyses, and to plot\n    'dagitty' and 'igraph' objects.",
    "version": "0.2.9",
    "maintainer": "Caspar J. van Lissa <c.j.vanlissa@tilburguniversity.edu>",
    "author": "Caspar J. van Lissa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0808-5024>),\n  Mauricio Garnier-Villarreal [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2951-6647>),\n  Frank C Gootjes [ctb] (ORCID: <https://orcid.org/0000-0002-0639-1001>)",
    "url": "https://cjvanlissa.github.io/tidySEM/",
    "bug_reports": "https://github.com/cjvanlissa/tidySEM/issues",
    "repository": "https://cran.r-project.org/package=tidySEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidySEM Tidy Structural Equation Modeling A tidy workflow for generating, estimating, reporting,\n    and plotting structural equation models using 'lavaan', 'OpenMx', or\n    'Mplus'. Throughout this workflow, elements of syntax, results, and graphs\n    are represented as 'tidy' data, making them easy to customize.\n    Includes functionality to estimate latent class analyses, and to plot\n    'dagitty' and 'igraph' objects.  "
  },
  {
    "id": 22069,
    "package_name": "tidychangepoint",
    "title": "A Tidy Framework for Changepoint Detection Analysis",
    "description": "Changepoint detection algorithms for R are widespread but have\n    different interfaces and reporting conventions. \n    This makes the comparative analysis of results difficult. \n    We solve this problem by providing a tidy, unified interface for several \n    different changepoint detection algorithms. \n    We also provide consistent numerical and graphical reporting leveraging \n    the 'broom' and 'ggplot2' packages. ",
    "version": "1.0.2",
    "maintainer": "Benjamin S. Baumer <ben.baumer@gmail.com>",
    "author": "Benjamin S. Baumer [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3279-0516>),\n  Biviana Marcela Su\u00e1rez Sierra [aut] (ORCID:\n    <https://orcid.org/0000-0003-2151-3537>),\n  Arrigo Coen [aut] (ORCID: <https://orcid.org/0000-0001-7798-7104>),\n  Carlos A. Taimal [aut] (ORCID: <https://orcid.org/0000-0002-8716-1282>),\n  Xueheng Shi [ctb]",
    "url": "https://beanumber.github.io/tidychangepoint/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidychangepoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidychangepoint A Tidy Framework for Changepoint Detection Analysis Changepoint detection algorithms for R are widespread but have\n    different interfaces and reporting conventions. \n    This makes the comparative analysis of results difficult. \n    We solve this problem by providing a tidy, unified interface for several \n    different changepoint detection algorithms. \n    We also provide consistent numerical and graphical reporting leveraging \n    the 'broom' and 'ggplot2' packages.   "
  },
  {
    "id": 22070,
    "package_name": "tidycharts",
    "title": "Generate Tidy Charts Inspired by 'IBCS'",
    "description": "There is a wide range of R packages created for data visualization, but still, there was no simple and easily accessible way to create clean and transparent charts - up to now. The 'tidycharts' package enables the user to generate charts compliant with International Business Communication Standards ('IBCS').\n    It means unified bar widths, colors, chart sizes, etc. Creating homogeneous reports has never been that easy! Additionally, users can apply semantic notation to indicate different data scenarios (plan, budget, forecast). What's more, it is possible to customize the charts by creating a personal color pallet with the possibility of switching to default options after the experiments.\n    We wanted the package to be helpful in writing reports, so we also made joining charts in a one, clear image possible.\n    All charts are generated in SVG format and can be shown in the 'RStudio' viewer pane or exported to HTML output of 'knitr'/'markdown'.",
    "version": "0.1.3",
    "maintainer": "Bartosz Sawicki <sawicki.bartosz@interia.pl>",
    "author": "Przemys\u0142aw Biecek [aut] (ORCID:\n    <https://orcid.org/0000-0001-8423-1823>),\n  Piotr Pi\u0105tyszek [aut],\n  Kinga U\u0142asik [aut],\n  Bartosz Sawicki [aut, cre]",
    "url": "https://mi2datalab.github.io/tidycharts/,\nhttps://github.com/MI2DataLab/tidycharts",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidycharts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidycharts Generate Tidy Charts Inspired by 'IBCS' There is a wide range of R packages created for data visualization, but still, there was no simple and easily accessible way to create clean and transparent charts - up to now. The 'tidycharts' package enables the user to generate charts compliant with International Business Communication Standards ('IBCS').\n    It means unified bar widths, colors, chart sizes, etc. Creating homogeneous reports has never been that easy! Additionally, users can apply semantic notation to indicate different data scenarios (plan, budget, forecast). What's more, it is possible to customize the charts by creating a personal color pallet with the possibility of switching to default options after the experiments.\n    We wanted the package to be helpful in writing reports, so we also made joining charts in a one, clear image possible.\n    All charts are generated in SVG format and can be shown in the 'RStudio' viewer pane or exported to HTML output of 'knitr'/'markdown'.  "
  },
  {
    "id": 22083,
    "package_name": "tidyedgar",
    "title": "Tidy Fundamental Financial Data from 'SEC's 'EDGAR' 'API'",
    "description": "Streamline the process of accessing fundamental financial data from the United States Securities and Exchange Commission's ('SEC') Electronic Data Gathering, Analysis, and Retrieval system ('EDGAR') 'API' <https://www.sec.gov/edgar/sec-api-documentation>, transforming it into a tidy, analysis-ready format.",
    "version": "1.0.1",
    "maintainer": "Gerard Gimenez-Adsuar <gerard@solucionsdedades.cat>",
    "author": "Gerard Gimenez-Adsuar [aut, cre]",
    "url": "https://gerardgimenezadsuar.github.io/tidyedgar/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidyedgar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyedgar Tidy Fundamental Financial Data from 'SEC's 'EDGAR' 'API' Streamline the process of accessing fundamental financial data from the United States Securities and Exchange Commission's ('SEC') Electronic Data Gathering, Analysis, and Retrieval system ('EDGAR') 'API' <https://www.sec.gov/edgar/sec-api-documentation>, transforming it into a tidy, analysis-ready format.  "
  },
  {
    "id": 22103,
    "package_name": "tidylo",
    "title": "Weighted Tidy Log Odds Ratio",
    "description": "How can we measure how the usage or frequency of some\n    feature, such as words, differs across some group or set, such as\n    documents? One option is to use the log odds ratio, but the log odds\n    ratio alone does not account for sampling variability; we haven't\n    counted every feature the same number of times so how do we know which\n    differences are meaningful? Enter the weighted log odds, which\n    'tidylo' provides an implementation for, using tidy data principles.\n    In particular, here we use the method outlined in Monroe, Colaresi,\n    and Quinn (2008) <doi:10.1093/pan/mpn018> to weight the log odds ratio\n    by a prior. By default, the prior is estimated from the data itself,\n    an empirical Bayes approach, but an uninformative prior is also\n    available.",
    "version": "0.2.0",
    "maintainer": "Julia Silge <julia.silge@gmail.com>",
    "author": "Tyler Schnoebelen [aut],\n  Julia Silge [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3671-836X>),\n  Alex Hayes [aut] (ORCID: <https://orcid.org/0000-0002-4985-5160>)",
    "url": "https://juliasilge.github.io/tidylo/,\nhttps://github.com/juliasilge/tidylo",
    "bug_reports": "https://github.com/juliasilge/tidylo/issues",
    "repository": "https://cran.r-project.org/package=tidylo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidylo Weighted Tidy Log Odds Ratio How can we measure how the usage or frequency of some\n    feature, such as words, differs across some group or set, such as\n    documents? One option is to use the log odds ratio, but the log odds\n    ratio alone does not account for sampling variability; we haven't\n    counted every feature the same number of times so how do we know which\n    differences are meaningful? Enter the weighted log odds, which\n    'tidylo' provides an implementation for, using tidy data principles.\n    In particular, here we use the method outlined in Monroe, Colaresi,\n    and Quinn (2008) <doi:10.1093/pan/mpn018> to weight the log odds ratio\n    by a prior. By default, the prior is estimated from the data itself,\n    an empirical Bayes approach, but an uninformative prior is also\n    available.  "
  },
  {
    "id": 22133,
    "package_name": "tidytransit",
    "title": "Read, Validate, Analyze, and Map GTFS Feeds",
    "description": "Read General Transit Feed Specification (GTFS) zipfiles into a list of R dataframes. Perform validation of the data structure against the specification. Analyze the headways and frequencies at routes and stops. Create maps and perform spatial analysis on the routes and stops. Please see the GTFS documentation here for more detail: <https://gtfs.org/>.",
    "version": "1.7.1",
    "maintainer": "Flavio Poletti <flavio.poletti@hotmail.ch>",
    "author": "Flavio Poletti [aut, cre],\n  Daniel Herszenhut [aut] (ORCID:\n    <https://orcid.org/0000-0001-8066-1105>),\n  Mark Padgham [aut],\n  Tom Buckley [aut],\n  Danton Noriega-Goodwin [aut],\n  Angela Li [ctb],\n  Elaine McVey [ctb],\n  Charles Hans Thompson [ctb],\n  Michael Sumner [ctb],\n  Patrick Hausmann [ctb],\n  Bob Rudis [ctb],\n  James Lamb [ctb],\n  Alexandra Kapp [ctb],\n  Kearey Smith [ctb],\n  Dave Vautin [ctb],\n  Kyle Walker [ctb],\n  Davis Vaughan [ctb],\n  Ryan Rymarczyk [ctb],\n  Kirill M\u00fcller [ctb]",
    "url": "https://github.com/r-transit/tidytransit,\nhttps://r-transit.github.io/tidytransit/",
    "bug_reports": "https://github.com/r-transit/tidytransit/issues",
    "repository": "https://cran.r-project.org/package=tidytransit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidytransit Read, Validate, Analyze, and Map GTFS Feeds Read General Transit Feed Specification (GTFS) zipfiles into a list of R dataframes. Perform validation of the data structure against the specification. Analyze the headways and frequencies at routes and stops. Create maps and perform spatial analysis on the routes and stops. Please see the GTFS documentation here for more detail: <https://gtfs.org/>.  "
  },
  {
    "id": 22139,
    "package_name": "tidywater",
    "title": "Water Quality Models for Drinking Water Treatment Processes",
    "description": "Provides multiple water chemistry-based models and published empirical models in one standard format.\n    As many models have been included as possible, however, users should be aware that models have varying degrees of accuracy and applicability.\n    To learn more, read the references provided below for the models implemented.\n    Functions can be chained together to model a complete treatment process and are designed to work in a 'tidyverse' workflow.\n    Models are primarily based on these sources:\n    Benjamin, M. M. (2002, ISBN:147862308X),\n    Crittenden, J. C., Trussell, R., Hand, D., Howe, J. K., & Tchobanoglous, G., Borchardt, J. H. (2012, ISBN:9781118131473),\n    USEPA. (2001) <https://www.epa.gov/sites/default/files/2017-03/documents/wtp_model_v._2.0_manual_508.pdf>.",
    "version": "0.10.0",
    "maintainer": "Sierra Johnson <sjohnson2@brwncald.com>",
    "author": "Sierra Johnson [aut, cre],\n  Libby McKenna [aut],\n  Riley Mulhern [aut] (ORCID: <https://orcid.org/0000-0001-6293-3672>),\n  Chris Corwin [aut] (ORCID: <https://orcid.org/0000-0002-9462-0352>),\n  Rachel Merrifield [ctb],\n  Mayuri Namasivayam [ctb],\n  Phoebe Chen [ctb],\n  Jiaming Yuan [ctb],\n  USEPA [cph] (Copyright holder of included TELSS fragments (dissolve_pb\n    function)),\n  Brown and Caldwell [fnd, cph]",
    "url": "https://github.com/BrownandCaldwell-Public/tidywater",
    "bug_reports": "https://github.com/BrownandCaldwell-Public/tidywater/issues",
    "repository": "https://cran.r-project.org/package=tidywater",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidywater Water Quality Models for Drinking Water Treatment Processes Provides multiple water chemistry-based models and published empirical models in one standard format.\n    As many models have been included as possible, however, users should be aware that models have varying degrees of accuracy and applicability.\n    To learn more, read the references provided below for the models implemented.\n    Functions can be chained together to model a complete treatment process and are designed to work in a 'tidyverse' workflow.\n    Models are primarily based on these sources:\n    Benjamin, M. M. (2002, ISBN:147862308X),\n    Crittenden, J. C., Trussell, R., Hand, D., Howe, J. K., & Tchobanoglous, G., Borchardt, J. H. (2012, ISBN:9781118131473),\n    USEPA. (2001) <https://www.epa.gov/sites/default/files/2017-03/documents/wtp_model_v._2.0_manual_508.pdf>.  "
  },
  {
    "id": 22150,
    "package_name": "tikatuwq",
    "title": "Water Quality Assessment and Environmental Compliance in Brazil",
    "description": "Tools to import, clean, validate, and analyze freshwater quality data in Brazil. Implements water quality indices including the Water Quality Index (WQI/IQA), the Trophic State Index (TSI/IET) after Carlson (1977) <doi:10.4319/lo.1977.22.2.0361> and Lamparelli (2004) <https://www.teses.usp.br/teses/disponiveis/41/41134/tde-20032006-075813/publico/TeseLamparelli2004.pdf>, and the National Sanitation Foundation Water Quality Index (NSF WQI) <doi:10.1007/s11157-023-09650-7>. The package also checks compliance with Brazilian standard CONAMA Resolution 357/2005 <https://conama.mma.gov.br/?id=450&option=com_sisconama&task=arquivo.download> and generates reproducible reports for routine monitoring workflows.\n    The example dataset (`wq_demo`) is now a real subset from monitoring data (BURANHEM river, 2020-2024, 4 points, 20 rows, 14 columns including extra `rio`, `lat`, `lon`). All core examples and vignettes use this realistic sample, improving reproducibility and documentation value for users.",
    "version": "0.8.0",
    "maintainer": "Vinicius Saraiva Santos <vinisaraiva@gmail.com>",
    "author": "Vinicius Saraiva Santos [aut, cre] (ORCID:\n    <https://orcid.org/0009-0007-1387-7927>)",
    "url": "https://github.com/tikatuwq/tikatuwq,\nhttps://tikatuwq.github.io/tikatuwq/",
    "bug_reports": "https://github.com/tikatuwq/tikatuwq/issues",
    "repository": "https://cran.r-project.org/package=tikatuwq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tikatuwq Water Quality Assessment and Environmental Compliance in Brazil Tools to import, clean, validate, and analyze freshwater quality data in Brazil. Implements water quality indices including the Water Quality Index (WQI/IQA), the Trophic State Index (TSI/IET) after Carlson (1977) <doi:10.4319/lo.1977.22.2.0361> and Lamparelli (2004) <https://www.teses.usp.br/teses/disponiveis/41/41134/tde-20032006-075813/publico/TeseLamparelli2004.pdf>, and the National Sanitation Foundation Water Quality Index (NSF WQI) <doi:10.1007/s11157-023-09650-7>. The package also checks compliance with Brazilian standard CONAMA Resolution 357/2005 <https://conama.mma.gov.br/?id=450&option=com_sisconama&task=arquivo.download> and generates reproducible reports for routine monitoring workflows.\n    The example dataset (`wq_demo`) is now a real subset from monitoring data (BURANHEM river, 2020-2024, 4 points, 20 rows, 14 columns including extra `rio`, `lat`, `lon`). All core examples and vignettes use this realistic sample, improving reproducibility and documentation value for users.  "
  },
  {
    "id": 22152,
    "package_name": "tikzDevice",
    "title": "R Graphics Output in LaTeX Format",
    "description": "Provides a graphics output device for R that records plots\n        in a LaTeX-friendly format. The device transforms plotting\n        commands issued by R functions into LaTeX code blocks. When\n        included in a LaTeX document, these blocks are interpreted with\n        the help of 'TikZ'---a graphics package for TeX and friends\n        written by Till Tantau. Using the 'tikzDevice', the text of R\n        plots can contain LaTeX commands such as mathematical formula.\n        The device also allows arbitrary LaTeX code to be inserted into\n        the output stream.",
    "version": "0.12.6",
    "maintainer": "Ralf Stubner <ralf.stubner@gmail.com>",
    "author": "Charlie Sharpsteen [aut],\n  Cameron Bracken [aut],\n  Kirill M\u00fcller [ctb],\n  Yihui Xie [ctb],\n  Ralf Stubner [cre],\n  Nico Bellack [ctb]",
    "url": "https://github.com/daqana/tikzDevice",
    "bug_reports": "https://github.com/daqana/tikzDevice/issues",
    "repository": "https://cran.r-project.org/package=tikzDevice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tikzDevice R Graphics Output in LaTeX Format Provides a graphics output device for R that records plots\n        in a LaTeX-friendly format. The device transforms plotting\n        commands issued by R functions into LaTeX code blocks. When\n        included in a LaTeX document, these blocks are interpreted with\n        the help of 'TikZ'---a graphics package for TeX and friends\n        written by Till Tantau. Using the 'tikzDevice', the text of R\n        plots can contain LaTeX commands such as mathematical formula.\n        The device also allows arbitrary LaTeX code to be inserted into\n        the output stream.  "
  },
  {
    "id": 22167,
    "package_name": "timechecker",
    "title": "Visualization of Processing Time with Standard Output",
    "description": "Displays processing time in a clear and structured way. One function supports iterative workflows by predicting and showing the total time required, while another reports the time taken for individual steps within a process.",
    "version": "1.1.5",
    "maintainer": "Toda Yusuke <yusuke0320.research@gmail.com>",
    "author": "Toda Yusuke [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=timechecker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timechecker Visualization of Processing Time with Standard Output Displays processing time in a clear and structured way. One function supports iterative workflows by predicting and showing the total time required, while another reports the time taken for individual steps within a process.  "
  },
  {
    "id": 22181,
    "package_name": "tint",
    "title": "'tint' is not 'Tufte'",
    "description": "A 'tufte'-alike style for 'rmarkdown'.\n A modern take on the 'Tufte' design for pdf and html vignettes,\n building on the 'tufte' package with additional contributions\n from the 'knitr' and 'ggtufte' package, and also acknowledging\n the key influence of 'envisioned css'.",
    "version": "0.1.6",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Jonathan Gilligan [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1375-6686>)",
    "url": "https://github.com/eddelbuettel/tint/,\nhttps://dirk.eddelbuettel.com/code/tint.html",
    "bug_reports": "https://github.com/eddelbuettel/tint/issues",
    "repository": "https://cran.r-project.org/package=tint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tint 'tint' is not 'Tufte' A 'tufte'-alike style for 'rmarkdown'.\n A modern take on the 'Tufte' design for pdf and html vignettes,\n building on the 'tufte' package with additional contributions\n from the 'knitr' and 'ggtufte' package, and also acknowledging\n the key influence of 'envisioned css'.  "
  },
  {
    "id": 22193,
    "package_name": "tinytest",
    "title": "Lightweight and Feature Complete Unit Testing Framework",
    "description": "Provides a lightweight (zero-dependency) and easy to use \n    unit testing framework. Main features: install tests with \n    the package. Test results are treated as data that can be stored and \n    manipulated. Test files are R scripts interspersed with test commands, that\n    can be programmed over. Fully automated build-install-test sequence for \n    packages. Skip tests when not run locally (e.g. on CRAN). Flexible and \n    configurable output printing. Compare computed output with output stored \n    with the package. Run tests in parallel. Extensible by other packages.\n    Report side effects.",
    "version": "1.4.1",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9807-4686>)",
    "url": "https://github.com/markvanderloo/tinytest",
    "bug_reports": "https://github.com/markvanderloo/tinytest/issues",
    "repository": "https://cran.r-project.org/package=tinytest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tinytest Lightweight and Feature Complete Unit Testing Framework Provides a lightweight (zero-dependency) and easy to use \n    unit testing framework. Main features: install tests with \n    the package. Test results are treated as data that can be stored and \n    manipulated. Test files are R scripts interspersed with test commands, that\n    can be programmed over. Fully automated build-install-test sequence for \n    packages. Skip tests when not run locally (e.g. on CRAN). Flexible and \n    configurable output printing. Compare computed output with output stored \n    with the package. Run tests in parallel. Extensible by other packages.\n    Report side effects.  "
  },
  {
    "id": 22201,
    "package_name": "tippy",
    "title": "Add Tooltips to 'R markdown' Documents or 'Shiny' Apps",
    "description": "'Htmlwidget' of 'Tippyjs' to add tooltips to 'Shiny' apps and 'R markdown' documents.",
    "version": "0.1.0",
    "maintainer": "John Coene <jcoenep@gmail.com>",
    "author": "John Coene [aut, cre]",
    "url": "https://tippy.john-coene.com/",
    "bug_reports": "https://github.com/JohnCoene/tippy/issues",
    "repository": "https://cran.r-project.org/package=tippy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tippy Add Tooltips to 'R markdown' Documents or 'Shiny' Apps 'Htmlwidget' of 'Tippyjs' to add tooltips to 'Shiny' apps and 'R markdown' documents.  "
  },
  {
    "id": 22212,
    "package_name": "tlda",
    "title": "Tools for Language Data Analysis",
    "description": "Support functions and datasets to facilitate the analysis of linguistic\n    data. The current focus is on the calculation of corpus-linguistic dispersion\n    measures as described in Gries (2021) <doi:10.1007/978-3-030-46216-1_5> and \n    Soenning (2025) <doi:10.3366/cor.2025.0326>. The most commonly used parts-based \n    indices are implemented, including different formulas and modifications that are \n    found in the literature, with the additional option to obtain frequency-adjusted \n    scores. Dispersion scores can be computed based on individual count variables or \n    a term-document matrix.  ",
    "version": "0.1.0",
    "maintainer": "Lukas Soenning <lukas.soenning@uni-bamberg.de>",
    "author": "Lukas Soenning [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2705-395X>),\n  German Research Foundation (DFG) [fnd] (ROR:\n    <https://ror.org/018mejw64>, Grant number 548274092)",
    "url": "https://github.com/lsoenning/tlda",
    "bug_reports": "https://github.com/lsoenning/tlda/issues",
    "repository": "https://cran.r-project.org/package=tlda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tlda Tools for Language Data Analysis Support functions and datasets to facilitate the analysis of linguistic\n    data. The current focus is on the calculation of corpus-linguistic dispersion\n    measures as described in Gries (2021) <doi:10.1007/978-3-030-46216-1_5> and \n    Soenning (2025) <doi:10.3366/cor.2025.0326>. The most commonly used parts-based \n    indices are implemented, including different formulas and modifications that are \n    found in the literature, with the additional option to obtain frequency-adjusted \n    scores. Dispersion scores can be computed based on individual count variables or \n    a term-document matrix.    "
  },
  {
    "id": 22213,
    "package_name": "tldr",
    "title": "T Loux Doing R: Functions to Simplify Data Analysis and\nReporting",
    "description": "Gives a number of functions to aid common data \n    analysis processes and reporting statistical results in an 'RMarkdown' file. \n    Data analysis functions combine multiple base R functions used to describe \n    simple bivariate relationships into a single, easy to use function. \n    Reporting functions will return character strings to report p-values, \n    confidence intervals, and hypothesis test and regression results. Strings \n    will be LaTeX-formatted as necessary and will knit pretty in an 'RMarkdown' \n    document. The package also provides wrappers function in the 'tableone' \n    package to make the results knit-able.",
    "version": "0.4.0",
    "maintainer": "Travis Loux <travis.loux@slu.edu>",
    "author": "Travis Loux [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tldr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tldr T Loux Doing R: Functions to Simplify Data Analysis and\nReporting Gives a number of functions to aid common data \n    analysis processes and reporting statistical results in an 'RMarkdown' file. \n    Data analysis functions combine multiple base R functions used to describe \n    simple bivariate relationships into a single, easy to use function. \n    Reporting functions will return character strings to report p-values, \n    confidence intervals, and hypothesis test and regression results. Strings \n    will be LaTeX-formatted as necessary and will knit pretty in an 'RMarkdown' \n    document. The package also provides wrappers function in the 'tableone' \n    package to make the results knit-able.  "
  },
  {
    "id": 22214,
    "package_name": "tldrDocs",
    "title": "Provide 'tldrPages' Documentation for Base R Functions",
    "description": "\n    Documentation for commonly-used objects included in the base distribution of \n    R. Note that 'tldrDocs' does not export any functions itself, its purpose is\n    to write .Rd files during its installation for 'tldr()' to find.",
    "version": "0.0.1",
    "maintainer": "James Otto <jamesotto852@gmail.com>",
    "author": "James Otto [aut, cre] (ORCID: <https://orcid.org/0000-0002-0665-2515>),\n  David Kahle [aut] (ORCID: <https://orcid.org/0000-0002-9999-1558>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tldrDocs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tldrDocs Provide 'tldrPages' Documentation for Base R Functions \n    Documentation for commonly-used objects included in the base distribution of \n    R. Note that 'tldrDocs' does not export any functions itself, its purpose is\n    to write .Rd files during its installation for 'tldr()' to find.  "
  },
  {
    "id": 22222,
    "package_name": "tm.plugin.koRpus",
    "title": "Full Corpus Support for the 'koRpus' Package",
    "description": "Enhances 'koRpus' text object classes and methods to also support large\n          corpora. Hierarchical ordering of corpus texts into arbitrary categories will\n          be preserved. Provided classes and methods also improve the ability of using\n          the 'koRpus' package together with the 'tm' package. To ask for help, report\n          bugs, suggest feature improvements, or discuss the global development of the\n          package, please subscribe to the koRpus-dev mailing list\n          (<https://korpusml.reaktanz.de>).",
    "version": "0.4-2",
    "maintainer": "m.eik michalke <meik.michalke@hhu.de>",
    "author": "m.eik michalke [aut, cre]",
    "url": "https://reaktanz.de/?c=hacking&s=koRpus",
    "bug_reports": "https://github.com/unDocUMeantIt/tm.plugin.koRpus/issues",
    "repository": "https://cran.r-project.org/package=tm.plugin.koRpus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tm.plugin.koRpus Full Corpus Support for the 'koRpus' Package Enhances 'koRpus' text object classes and methods to also support large\n          corpora. Hierarchical ordering of corpus texts into arbitrary categories will\n          be preserved. Provided classes and methods also improve the ability of using\n          the 'koRpus' package together with the 'tm' package. To ask for help, report\n          bugs, suggest feature improvements, or discuss the global development of the\n          package, please subscribe to the koRpus-dev mailing list\n          (<https://korpusml.reaktanz.de>).  "
  },
  {
    "id": 22236,
    "package_name": "tmle",
    "title": "Targeted Maximum Likelihood Estimation",
    "description": "Targeted maximum likelihood estimation of point treatment effects (Targeted Maximum Likelihood Learning, The International Journal of Biostatistics, 2(1), 2006.  This version automatically estimates the additive treatment effect among the treated (ATT) and among the controls (ATC).  The tmle() function calculates the adjusted marginal difference in mean outcome associated with a binary point treatment, for continuous or binary outcomes.  Relative risk and odds ratio estimates are also reported for binary outcomes. Missingness in the outcome is allowed, but not in treatment assignment or baseline covariate values.  The population mean is calculated when there is missingness, and no variation in the treatment assignment. The tmleMSM() function estimates the parameters of a marginal structural model for a binary point treatment effect. Effect estimation stratified by a binary mediating variable is also available. An ID argument can be used to identify repeated measures. Default settings call 'SuperLearner' to estimate the Q and g portions of the likelihood, unless values or a user-supplied regression function are passed in as arguments. ",
    "version": "2.1.1",
    "maintainer": "Susan Gruber <sgruber@cal.berkeley.edu>",
    "author": "Susan Gruber [aut, cre],\n  Mark van der Laan [aut],\n  Chris Kennedy [ctr]",
    "url": "https://CRAN.R-project.org/package=tmle",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tmle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmle Targeted Maximum Likelihood Estimation Targeted maximum likelihood estimation of point treatment effects (Targeted Maximum Likelihood Learning, The International Journal of Biostatistics, 2(1), 2006.  This version automatically estimates the additive treatment effect among the treated (ATT) and among the controls (ATC).  The tmle() function calculates the adjusted marginal difference in mean outcome associated with a binary point treatment, for continuous or binary outcomes.  Relative risk and odds ratio estimates are also reported for binary outcomes. Missingness in the outcome is allowed, but not in treatment assignment or baseline covariate values.  The population mean is calculated when there is missingness, and no variation in the treatment assignment. The tmleMSM() function estimates the parameters of a marginal structural model for a binary point treatment effect. Effect estimation stratified by a binary mediating variable is also available. An ID argument can be used to identify repeated measures. Default settings call 'SuperLearner' to estimate the Q and g portions of the likelihood, unless values or a user-supplied regression function are passed in as arguments.   "
  },
  {
    "id": 22238,
    "package_name": "tmplate",
    "title": "Code Generation Based on Templates",
    "description": "Define general templates with tags that can be replaced by content depending on arguments and objects to modify the final output of the document.",
    "version": "0.0.3",
    "maintainer": "Mario A. Martinez Araya <r@marioma.me>",
    "author": "Mario A. Martinez Araya [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4821-9314>)",
    "url": "<https://marioma.me?i=soft>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tmplate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmplate Code Generation Based on Templates Define general templates with tags that can be replaced by content depending on arguments and objects to modify the final output of the document.  "
  },
  {
    "id": 22259,
    "package_name": "toolStability",
    "title": "Tool for Stability Indices Calculation",
    "description": "Tools to calculate stability indices with parametric,\n non-parametric and probabilistic approaches. The basic data format requirement for 'toolStability' is a data frame with 3 columns including numeric trait values, genotype,and environmental labels. Output format of each function is the dataframe with chosen stability index for each genotype. \n Function \"table_stability\" offers the summary table of all stability indices in this package. \n This R package toolStability is part of the main publication: \n Wang, Casadebaig and Chen (2023) <doi:10.1007/s00122-023-04264-7>. \n Analysis pipeline for main publication can be found on github: <https://github.com/Illustratien/Wang_2023_TAAG>. \n Sample dataset in this package is derived from another publication:  \n Casadebaig P, Zheng B, Chapman S et al. (2016) <doi:10.1371/journal.pone.0146385>. \n For detailed documentation of dataset, please see on Zenodo <doi:10.5281/zenodo.4729636>. \n Indices used in this package are from: \n D\u00f6ring TF, Reckling M (2018) <doi:10.1016/j.eja.2018.06.007>. \n Eberhart SA, Russell WA (1966) <doi:10.2135/cropsci1966.0011183X000600010011x>. \n Eskridge KM (1990) <doi:10.2135/cropsci1990.0011183X003000020025x>. \n Finlay KW, Wilkinson GN (1963) <doi:10.1071/AR9630742>. \n Hanson WD (1970) Genotypic stability. <doi:10.1007/BF00285245>. \n Lin CS, Binns MR (1988). \n Nassar R, H\u00fchn M (1987). \n Pinthus MJ (1973) <doi:10.1007/BF00021563>. \n R\u00f6mer T (1917). \n Shukla GK (1972). \n Wricke G (1962). ",
    "version": "0.1.3",
    "maintainer": "Tien-Cheng Wang <wangtien@student.hu-berlin.de>",
    "author": "Tien-Cheng Wang [aut, cre],\n  Tsu-Wei Chen [com]",
    "url": "https://illustratien.github.io/toolStability/,\nhttps://link.springer.com/article/10.1007/s00122-023-04264-7,\nhttps://github.com/Illustratien/toolStability,\nhttps://github.com/Illustratien/Wang_2023_TAAG,\nhttps://doi.org/10.5281/zenodo.4729636",
    "bug_reports": "https://github.com/Illustratien/toolStability/issues",
    "repository": "https://cran.r-project.org/package=toolStability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "toolStability Tool for Stability Indices Calculation Tools to calculate stability indices with parametric,\n non-parametric and probabilistic approaches. The basic data format requirement for 'toolStability' is a data frame with 3 columns including numeric trait values, genotype,and environmental labels. Output format of each function is the dataframe with chosen stability index for each genotype. \n Function \"table_stability\" offers the summary table of all stability indices in this package. \n This R package toolStability is part of the main publication: \n Wang, Casadebaig and Chen (2023) <doi:10.1007/s00122-023-04264-7>. \n Analysis pipeline for main publication can be found on github: <https://github.com/Illustratien/Wang_2023_TAAG>. \n Sample dataset in this package is derived from another publication:  \n Casadebaig P, Zheng B, Chapman S et al. (2016) <doi:10.1371/journal.pone.0146385>. \n For detailed documentation of dataset, please see on Zenodo <doi:10.5281/zenodo.4729636>. \n Indices used in this package are from: \n D\u00f6ring TF, Reckling M (2018) <doi:10.1016/j.eja.2018.06.007>. \n Eberhart SA, Russell WA (1966) <doi:10.2135/cropsci1966.0011183X000600010011x>. \n Eskridge KM (1990) <doi:10.2135/cropsci1990.0011183X003000020025x>. \n Finlay KW, Wilkinson GN (1963) <doi:10.1071/AR9630742>. \n Hanson WD (1970) Genotypic stability. <doi:10.1007/BF00285245>. \n Lin CS, Binns MR (1988). \n Nassar R, H\u00fchn M (1987). \n Pinthus MJ (1973) <doi:10.1007/BF00021563>. \n R\u00f6mer T (1917). \n Shukla GK (1972). \n Wricke G (1962).   "
  },
  {
    "id": 22267,
    "package_name": "topiclabels",
    "title": "Automated Topic Labeling with Language Models",
    "description": "Leveraging (large) language models for automatic topic labeling. The main function converts a list of top terms into a label for each topic. Hence, it is complementary to any topic modeling package that produces a list of top terms for each topic. While human judgement is indispensable for topic validation (i.e., inspecting top terms and most representative documents), automatic topic labeling can be a valuable tool for researchers in various scenarios.",
    "version": "0.3.0",
    "maintainer": "Jonas Rieger <rieger@statistik.tu-dortmund.de>",
    "author": "Jonas Rieger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0007-4478>),\n  Fritz Peters [aut] (ORCID: <https://orcid.org/0009-0003-8471-4931>),\n  Andreas Fischer [aut] (ORCID: <https://orcid.org/0009-0006-0748-6076>),\n  Tim Lauer [aut] (ORCID: <https://orcid.org/0009-0003-1625-1672>),\n  Andr\u00e9 Bittermann [aut] (ORCID: <https://orcid.org/0000-0003-2942-9831>)",
    "url": "https://github.com/PetersFritz/topiclabels",
    "bug_reports": "https://github.com/PetersFritz/topiclabels/issues",
    "repository": "https://cran.r-project.org/package=topiclabels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topiclabels Automated Topic Labeling with Language Models Leveraging (large) language models for automatic topic labeling. The main function converts a list of top terms into a label for each topic. Hence, it is complementary to any topic modeling package that produces a list of top terms for each topic. While human judgement is indispensable for topic validation (i.e., inspecting top terms and most representative documents), automatic topic labeling can be a valuable tool for researchers in various scenarios.  "
  },
  {
    "id": 22319,
    "package_name": "trackdown",
    "title": "Collaborative Editing of Rmd (or Rnw) Documents in Google Drive",
    "description": "Collaborative writing and editing of R Markdown (or Sweave) documents. The local .Rmd (or .Rnw) is uploaded as a plain-text file to Google Drive. By taking advantage of the easily readable Markdown (or LaTeX) syntax and the well-known online interface offered by Google Docs, collaborators can easily contribute to the writing and editing process. After integrating all authors\u2019 contributions, the final document can be downloaded and rendered locally.",
    "version": "1.1.1",
    "maintainer": "Claudio Zandonella Callegher <claudiozandonella@gmail.com>",
    "author": "Emily Kothe [aut] (ORCID: <https://orcid.org/0000-0003-1210-0554>),\n  Claudio Zandonella Callegher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7721-6318>),\n  Filippo Gambarota [aut] (ORCID:\n    <https://orcid.org/0000-0002-6666-1747>),\n  Janosch Linkersd\u00f6rfer [aut] (ORCID:\n    <https://orcid.org/0000-0002-1577-1233>),\n  Mathew Ling [aut] (ORCID: <https://orcid.org/0000-0002-0940-2538>)",
    "url": "https://github.com/claudiozandonella/trackdown/,\nhttps://claudiozandonella.github.io/trackdown/",
    "bug_reports": "https://github.com/claudiozandonella/trackdown/issues",
    "repository": "https://cran.r-project.org/package=trackdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trackdown Collaborative Editing of Rmd (or Rnw) Documents in Google Drive Collaborative writing and editing of R Markdown (or Sweave) documents. The local .Rmd (or .Rnw) is uploaded as a plain-text file to Google Drive. By taking advantage of the easily readable Markdown (or LaTeX) syntax and the well-known online interface offered by Google Docs, collaborators can easily contribute to the writing and editing process. After integrating all authors\u2019 contributions, the final document can be downloaded and rendered locally.  "
  },
  {
    "id": 22325,
    "package_name": "tradepolicy",
    "title": "Replication of 'An Advanced Guide To Trade Policy Analysis'",
    "description": "Datasets from Yotov, et al. (2016, ISBN:978-92-870-4367-2) \"An\n    Advanced Guide to Trade Policy Analysis\" and functions to report regression\n    summaries with clustered robust standard errors.",
    "version": "0.7.0",
    "maintainer": "Mauricio Vargas Sepulveda <m.sepulveda@mail.utoronto.ca>",
    "author": "Mauricio Vargas Sepulveda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1017-7574>),\n  Alexey Kravchenko [ths],\n  Constanza Prado [ths],\n  Yoto Yotov [ths],\n  The United Nations [dtc, cph, fnd]",
    "url": "https://github.com/pachadotdev/tradepolicy/",
    "bug_reports": "https://github.com/pachadotdev/tradepolicy/issues/",
    "repository": "https://cran.r-project.org/package=tradepolicy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tradepolicy Replication of 'An Advanced Guide To Trade Policy Analysis' Datasets from Yotov, et al. (2016, ISBN:978-92-870-4367-2) \"An\n    Advanced Guide to Trade Policy Analysis\" and functions to report regression\n    summaries with clustered robust standard errors.  "
  },
  {
    "id": 22354,
    "package_name": "transplantr",
    "title": "Audit and Research Functions for Transplantation",
    "description": "A set of vectorised functions to calculate medical equations used in transplantation, \n    focused mainly on transplantation of abdominal organs. These functions include donor and recipient\n    risk indices as used by NHS Blood & Transplant, OPTN/UNOS and Eurotransplant, tools for \n    quantifying HLA mismatches, functions for calculating estimated glomerular filtration rate (eGFR), \n    a function to calculate the APRI (AST to platelet ratio) score used in initial screening of suitability to receive a \n    transplant from a hepatitis C seropositive donor and some biochemical unit converter functions. \n    All functions are designed to work with either US or international units.\n    References for the equations are provided in the vignettes and function documentation.",
    "version": "0.2.0",
    "maintainer": "John Asher <john.asher@outlook.com>",
    "author": "John Asher [aut, cre] (ORCID: <https://orcid.org/0000-0001-8735-6453>)",
    "url": "https://transplantr.txtools.net,\nhttps://github.com/johnasher/transplantr",
    "bug_reports": "https://github.com/johnasher/transplantr/issues",
    "repository": "https://cran.r-project.org/package=transplantr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "transplantr Audit and Research Functions for Transplantation A set of vectorised functions to calculate medical equations used in transplantation, \n    focused mainly on transplantation of abdominal organs. These functions include donor and recipient\n    risk indices as used by NHS Blood & Transplant, OPTN/UNOS and Eurotransplant, tools for \n    quantifying HLA mismatches, functions for calculating estimated glomerular filtration rate (eGFR), \n    a function to calculate the APRI (AST to platelet ratio) score used in initial screening of suitability to receive a \n    transplant from a hepatitis C seropositive donor and some biochemical unit converter functions. \n    All functions are designed to work with either US or international units.\n    References for the equations are provided in the vignettes and function documentation.  "
  },
  {
    "id": 22410,
    "package_name": "trimr",
    "title": "An Implementation of Common Response Time Trimming Methods",
    "description": "Provides various commonly-used response time trimming\n    methods, including the recursive / moving-criterion methods reported by\n    Van Selst and Jolicoeur (1994). By passing trimming functions raw data files,\n    the package will return trimmed data ready for inferential testing.",
    "version": "1.1.1",
    "maintainer": "James Grange <grange.jim@gmail.com>",
    "author": "James Grange [cre, aut],\n  Ed Berry [ctb]",
    "url": "https://github.com/JimGrange/trimr",
    "bug_reports": "https://github.com/JimGrange/trimr/issues",
    "repository": "https://cran.r-project.org/package=trimr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trimr An Implementation of Common Response Time Trimming Methods Provides various commonly-used response time trimming\n    methods, including the recursive / moving-criterion methods reported by\n    Van Selst and Jolicoeur (1994). By passing trimming functions raw data files,\n    the package will return trimmed data ready for inferential testing.  "
  },
  {
    "id": 22416,
    "package_name": "triplesmatch",
    "title": "Match Triples Consisting of Two Controls and a Treated Unit or\nVice Versa",
    "description": "Attain excellent covariate balance by matching two treated units\n    and one control unit or vice versa within strata. Using such triples, as\n    opposed to also allowing pairs of treated and control units, \n    allows easier interpretation of the two possible \n    weights of observations and better insensitivity to unmeasured bias in the test\n    statistic. Using triples instead of matching in a fixed 1:2 or 2:1 ratio\n    allows for the match to be feasible in more situations.\n    The 'rrelaxiv' package, which provides an alternative solver for the underlying \n    network flow problems, carries an academic license and is not available on CRAN, but\n    may be downloaded from 'GitHub' at <https://github.com/josherrickson/rrelaxiv/>.\n    The 'Gurobi' commercial optimization software is required to use the two functions\n    [infsentrip()] and [triplesIP()]. These functions are not essential\n    to the main purpose of this package. A free academic license can be obtained at \n    <https://www.gurobi.com/features/academic-named-user-license/>. \n    The 'gurobi' R package can then be installed following \n    the instructions at <https://www.gurobi.com/documentation/9.1/refman/ins_the_r_package.html>.",
    "version": "1.1.0",
    "maintainer": "Katherine Brumberg <kbrum@umich.edu>",
    "author": "Katherine Brumberg [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=triplesmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "triplesmatch Match Triples Consisting of Two Controls and a Treated Unit or\nVice Versa Attain excellent covariate balance by matching two treated units\n    and one control unit or vice versa within strata. Using such triples, as\n    opposed to also allowing pairs of treated and control units, \n    allows easier interpretation of the two possible \n    weights of observations and better insensitivity to unmeasured bias in the test\n    statistic. Using triples instead of matching in a fixed 1:2 or 2:1 ratio\n    allows for the match to be feasible in more situations.\n    The 'rrelaxiv' package, which provides an alternative solver for the underlying \n    network flow problems, carries an academic license and is not available on CRAN, but\n    may be downloaded from 'GitHub' at <https://github.com/josherrickson/rrelaxiv/>.\n    The 'Gurobi' commercial optimization software is required to use the two functions\n    [infsentrip()] and [triplesIP()]. These functions are not essential\n    to the main purpose of this package. A free academic license can be obtained at \n    <https://www.gurobi.com/features/academic-named-user-license/>. \n    The 'gurobi' R package can then be installed following \n    the instructions at <https://www.gurobi.com/documentation/9.1/refman/ins_the_r_package.html>.  "
  },
  {
    "id": 22426,
    "package_name": "truelies",
    "title": "Bayesian Methods to Estimate the Proportion of Liars in Coin\nFlip Experiments",
    "description": "Implements Bayesian methods, described in\n    Hugh-Jones (2019) <doi:10.1007/s40881-019-00069-x>, for estimating the\n    proportion of liars in coin flip-style experiments, where subjects\n    report a random outcome and are paid for reporting a \"good\" outcome.",
    "version": "0.2.0",
    "maintainer": "David Hugh-Jones <davidhughjones@gmail.com>",
    "author": "David Hugh-Jones <davidhughjones@gmail.com>",
    "url": "https://github.com/hughjonesd/truelies",
    "bug_reports": "https://github.com/hughjonesd/truelies/issues",
    "repository": "https://cran.r-project.org/package=truelies",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "truelies Bayesian Methods to Estimate the Proportion of Liars in Coin\nFlip Experiments Implements Bayesian methods, described in\n    Hugh-Jones (2019) <doi:10.1007/s40881-019-00069-x>, for estimating the\n    proportion of liars in coin flip-style experiments, where subjects\n    report a random outcome and are paid for reporting a \"good\" outcome.  "
  },
  {
    "id": 22500,
    "package_name": "ttbbeer",
    "title": "US Beer Statistics from TTB",
    "description": "U.S. Department of the Treasury, Alcohol and Tobacco Tax and\n    Trade Bureau (TTB) collects data and reports on monthly beer\n    industry production and operations. This data package includes\n    a collection of 10 years (2006 - 2015) worth of data on materials\n    used at U.S. breweries in pounds reported by the Brewer's Report\n    of Operations and the Quarterly Brewer's Report of Operations\n    forms, ready for data analysis. This package also includes historical\n    tax rates on distilled spirits, wine, beer, champagne, and tobacco\n    products as individual data sets.",
    "version": "1.1.0",
    "maintainer": "Jasmine Dumas <jasmine.dumas@gmail.com>",
    "author": "Jasmine Dumas [aut, cre]",
    "url": "https://github.com/jasdumas/ttbbeer",
    "bug_reports": "https://github.com/jasdumas/ttbbeer/issues",
    "repository": "https://cran.r-project.org/package=ttbbeer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ttbbeer US Beer Statistics from TTB U.S. Department of the Treasury, Alcohol and Tobacco Tax and\n    Trade Bureau (TTB) collects data and reports on monthly beer\n    industry production and operations. This data package includes\n    a collection of 10 years (2006 - 2015) worth of data on materials\n    used at U.S. breweries in pounds reported by the Brewer's Report\n    of Operations and the Quarterly Brewer's Report of Operations\n    forms, ready for data analysis. This package also includes historical\n    tax rates on distilled spirits, wine, beer, champagne, and tobacco\n    products as individual data sets.  "
  },
  {
    "id": 22510,
    "package_name": "tubern",
    "title": "R Client for the YouTube Analytics and Reporting API",
    "description": "Get statistics and reports from YouTube. To learn more about\n    the YouTube Analytics and Reporting API, see <https://developers.google.com/youtube/reporting/>.",
    "version": "0.2.1",
    "maintainer": "Gaurav Sood <gsood07@gmail.com>",
    "author": "Gaurav Sood [aut, cre]",
    "url": "https://github.com/gojiplus/tubern",
    "bug_reports": "https://github.com/gojiplus/tubern/issues",
    "repository": "https://cran.r-project.org/package=tubern",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tubern R Client for the YouTube Analytics and Reporting API Get statistics and reports from YouTube. To learn more about\n    the YouTube Analytics and Reporting API, see <https://developers.google.com/youtube/reporting/>.  "
  },
  {
    "id": 22512,
    "package_name": "tufterhandout",
    "title": "Tufte-style html document format for rmarkdown",
    "description": "Custom template and output formats for use with rmarkdown. Produce\n    Edward Tufte-style handouts in html formats with full support for rmarkdown\n    features",
    "version": "1.2.1",
    "maintainer": "Michael C Sachs <sachsmc@gmail.com>",
    "author": "Michael C Sachs",
    "url": "http://sachsmc.github.io/tufterhandout",
    "bug_reports": "http://github.com/sachsmc/tufterhandout/issues",
    "repository": "https://cran.r-project.org/package=tufterhandout",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tufterhandout Tufte-style html document format for rmarkdown Custom template and output formats for use with rmarkdown. Produce\n    Edward Tufte-style handouts in html formats with full support for rmarkdown\n    features  "
  },
  {
    "id": 22528,
    "package_name": "tvem",
    "title": "Time-Varying Effect Models",
    "description": "Fits time-varying effect models (TVEM). These are a kind of application of varying-coefficient models in the context of longitudinal data, allowing the strength of linear, logistic, or Poisson regression relationships to change over time.  These models are described further in Tan, Shiyko, Li, Li & Dierker (2012) <doi:10.1037/a0025814>.  We thank Kaylee Litson, Patricia Berglund, Yajnaseni Chakraborti, and Hanjoo Kim for their valuable help with testing the package and the documentation. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.",
    "version": "1.4.1",
    "maintainer": "John J. Dziak <dziakj1@gmail.com>",
    "author": "John J. Dziak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0762-5495>),\n  Donna L. Coffman [aut] (ORCID: <https://orcid.org/0000-0001-6305-6579>),\n  Runze Li [aut] (ORCID: <https://orcid.org/0000-0002-0154-2202>),\n  Kaylee Litson [aut] (ORCID: <https://orcid.org/0000-0003-1296-4811>),\n  Yajnaseni Chakraborti [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tvem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tvem Time-Varying Effect Models Fits time-varying effect models (TVEM). These are a kind of application of varying-coefficient models in the context of longitudinal data, allowing the strength of linear, logistic, or Poisson regression relationships to change over time.  These models are described further in Tan, Shiyko, Li, Li & Dierker (2012) <doi:10.1037/a0025814>.  We thank Kaylee Litson, Patricia Berglund, Yajnaseni Chakraborti, and Hanjoo Kim for their valuable help with testing the package and the documentation. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.  "
  },
  {
    "id": 22551,
    "package_name": "twitterwidget",
    "title": "Render a Twitter Status in R Markdown Pages",
    "description": "Include the Twitter status widgets in HTML pages created\n  using R markdown. The package uses the Twitter javascript APIs to\n  embed in your document Twitter cards associated to specific statuses.\n  The main targets are regular HTML pages or dashboards.",
    "version": "0.1.1",
    "maintainer": "Guido Volpi <idrivefoxes@gmail.com>",
    "author": "Guido Volpi [aut, cre]",
    "url": "https://github.com/guivo/twitterwidget",
    "bug_reports": "https://github.com/guivo/twitterwidget/issues",
    "repository": "https://cran.r-project.org/package=twitterwidget",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twitterwidget Render a Twitter Status in R Markdown Pages Include the Twitter status widgets in HTML pages created\n  using R markdown. The package uses the Twitter javascript APIs to\n  embed in your document Twitter cards associated to specific statuses.\n  The main targets are regular HTML pages or dashboards.  "
  },
  {
    "id": 22570,
    "package_name": "types",
    "title": "Type Annotations",
    "description": "Provides a simple type annotation for R that is usable in scripts,\n    in the R console and in packages. It is intended as a convention to allow other\n    packages to use the type information to provide error checking,\n    automatic documentation or optimizations.",
    "version": "1.0.0",
    "maintainer": "Jim Hester <james.f.hester@gmail.com>",
    "author": "Jim Hester [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=types",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "types Type Annotations Provides a simple type annotation for R that is usable in scripts,\n    in the R console and in packages. It is intended as a convention to allow other\n    packages to use the type information to provide error checking,\n    automatic documentation or optimizations.  "
  },
  {
    "id": 22572,
    "package_name": "typr",
    "title": "Write and Render 'Typst' Documents",
    "description": "Compile 'Typst' files using the 'typst-cli' (<https://typst.app>)\n    command line tool. Automatically falls back to rendering via embedded 'Typst'\n    from 'Quarto' (<https://quarto.org>) if 'Typst' is not installed. Includes\n    utilities to check for 'typst-cli' availability and run 'Typst' commands.",
    "version": "0.0.4",
    "maintainer": "Christopher T. Kenny <ctkenny@proton.me>",
    "author": "Christopher T. Kenny [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9386-6860>)",
    "url": "http://christophertkenny.com/typr/,\nhttps://github.com/christopherkenny/typr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=typr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "typr Write and Render 'Typst' Documents Compile 'Typst' files using the 'typst-cli' (<https://typst.app>)\n    command line tool. Automatically falls back to rendering via embedded 'Typst'\n    from 'Quarto' (<https://quarto.org>) if 'Typst' is not installed. Includes\n    utilities to check for 'typst-cli' availability and run 'Typst' commands.  "
  },
  {
    "id": 22580,
    "package_name": "ubiquity",
    "title": "PKPD, PBPK, and Systems Pharmacology Modeling Tools",
    "description": "Complete work flow for the analysis of pharmacokinetic pharmacodynamic (PKPD), physiologically-based pharmacokinetic (PBPK) and systems pharmacology models including: creation of ordinary differential equation-based models, pooled parameter estimation, individual/population based simulations, rule-based simulations for clinical trial design and modeling assays, deployment with a customizable 'Shiny' app, and non-compartmental analysis. System-specific analysis templates can be generated and each element includes integrated reporting with 'PowerPoint' and 'Word'. ",
    "version": "2.1.0",
    "maintainer": "John Harrold <john.m.harrold@gmail.com>",
    "author": "John Harrold [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2052-4373>)",
    "url": "https://r.ubiquity.tools",
    "bug_reports": "https://github.com/john-harrold/ubiquity/issues",
    "repository": "https://cran.r-project.org/package=ubiquity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ubiquity PKPD, PBPK, and Systems Pharmacology Modeling Tools Complete work flow for the analysis of pharmacokinetic pharmacodynamic (PKPD), physiologically-based pharmacokinetic (PBPK) and systems pharmacology models including: creation of ordinary differential equation-based models, pooled parameter estimation, individual/population based simulations, rule-based simulations for clinical trial design and modeling assays, deployment with a customizable 'Shiny' app, and non-compartmental analysis. System-specific analysis templates can be generated and each element includes integrated reporting with 'PowerPoint' and 'Word'.   "
  },
  {
    "id": 22584,
    "package_name": "ucimlrepo",
    "title": "Explore UCI ML Repository Datasets",
    "description": "Find and import datasets from the\n    University of California Irvine Machine Learning (UCI ML) Repository into R.\n    Supports working with data from UCI ML repository inside of R scripts, notebooks, \n    and 'Quarto'/'RMarkdown' documents. Access the UCI ML repository directly at\n    <https://archive.ics.uci.edu/>.",
    "version": "0.0.2",
    "maintainer": "James Joseph Balamuta <james.balamuta@gmail.com>",
    "author": "James Joseph Balamuta [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>),\n  Philip Truong [aut, cph]",
    "url": "https://r-pkg.thecoatlessprofessor.com/ucimlrepo/,\nhttps://github.com/coatless-rpkg/ucimlrepo,\nhttps://archive.ics.uci.edu/",
    "bug_reports": "https://github.com/coatless-rpkg/ucimlrepo/issues",
    "repository": "https://cran.r-project.org/package=ucimlrepo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ucimlrepo Explore UCI ML Repository Datasets Find and import datasets from the\n    University of California Irvine Machine Learning (UCI ML) Repository into R.\n    Supports working with data from UCI ML repository inside of R scripts, notebooks, \n    and 'Quarto'/'RMarkdown' documents. Access the UCI ML repository directly at\n    <https://archive.ics.uci.edu/>.  "
  },
  {
    "id": 22587,
    "package_name": "udpipe",
    "title": "Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit",
    "description": "This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.",
    "version": "0.8.15",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  Institute of Formal and Applied Linguistics, Faculty of Mathematics and\n    Physics, Charles University in Prague, Czech Republic [cph]\n    (src/udpipe.cpp & src/udpipe.h),\n  Milan Straka [aut, cph] (src/udpipe.cpp & src/udpipe.h),\n  Jana Strakov\u00e1 [ctb, cph] (src/udpipe.cpp & src/udpipe.h)",
    "url": "https://bnosac.github.io/udpipe/en/index.html,\nhttps://github.com/bnosac/udpipe",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=udpipe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "udpipe Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.  "
  },
  {
    "id": 22594,
    "package_name": "uisapi",
    "title": "Access the UNESCO Institute for Statistics API",
    "description": "Retrieve data from the UNESCO Institute for Statistics (UIS) API\n    <https://api.uis.unesco.org/api/public/documentation/>. UIS provides public \n    access to more than 4,000 indicators focusing on education, science and\n    technology, culture, and communication.",
    "version": "0.1.1",
    "maintainer": "Christoph Scheuch <christoph@tidy-intelligence.com>",
    "author": "Christoph Scheuch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>)",
    "url": "https://github.com/tidy-intelligence/r-uisapi,\nhttps://tidy-intelligence.github.io/r-uisapi/",
    "bug_reports": "https://github.com/tidy-intelligence/r-uisapi/issues",
    "repository": "https://cran.r-project.org/package=uisapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uisapi Access the UNESCO Institute for Statistics API Retrieve data from the UNESCO Institute for Statistics (UIS) API\n    <https://api.uis.unesco.org/api/public/documentation/>. UIS provides public \n    access to more than 4,000 indicators focusing on education, science and\n    technology, culture, and communication.  "
  },
  {
    "id": 22595,
    "package_name": "uiucthemes",
    "title": "'R' 'Markdown' Themes for 'UIUC' Documents and Presentations",
    "description": "A set of custom 'R' 'Markdown' templates for documents and\n   presentations with the University of Illinois at Urbana-Champaign (UIUC)\n   color scheme and identity standards.",
    "version": "0.3.3",
    "maintainer": "James Balamuta <balamut2@illinois.edu>",
    "author": "James Balamuta [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2826-8458>),\n  Steven Andrew Culpepper [ctb] (Provided the Minimal Orange Beamer\n    Theme),\n  David Dalpiaz [ctb] (Collaborated on the LaTeX Journal Theme),\n  Jose Luis Rodriguez [ctb] (Provided the Market Information Lab (MIL)\n    Beamer Theme)",
    "url": "https://github.com/illinois-r/uiucthemes,\nhttps://blog.thecoatlessprofessor.com",
    "bug_reports": "https://github.com/illinois-r/uiucthemes/issues",
    "repository": "https://cran.r-project.org/package=uiucthemes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uiucthemes 'R' 'Markdown' Themes for 'UIUC' Documents and Presentations A set of custom 'R' 'Markdown' templates for documents and\n   presentations with the University of Illinois at Urbana-Champaign (UIUC)\n   color scheme and identity standards.  "
  },
  {
    "id": 22602,
    "package_name": "ukpolice",
    "title": "Download Data on UK Police and Crime",
    "description": "Downloads data from the 'UK Police' public data API, \n    the full docs of which are available at <https://data.police.uk/docs/>. \n    Includes data on police forces and police force areas, crime reports, \n    and the use of stop-and-search powers.",
    "version": "0.2.2",
    "maintainer": "Evan Odell <evanodell91@gmail.com>",
    "author": "Evan Odell [aut, cre] (ORCID: <https://orcid.org/0000-0003-1845-808X>),\n  Tierney Nicholas [aut] (ORCID: <https://orcid.org/0000-0003-1460-8722>)",
    "url": "https://github.com/EvanOdell/ukpolice/,\nhttps://docs.evanodell.com/ukpolice",
    "bug_reports": "https://github.com/EvanOdell/ukpolice/issues",
    "repository": "https://cran.r-project.org/package=ukpolice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ukpolice Download Data on UK Police and Crime Downloads data from the 'UK Police' public data API, \n    the full docs of which are available at <https://data.police.uk/docs/>. \n    Includes data on police forces and police force areas, crime reports, \n    and the use of stop-and-search powers.  "
  },
  {
    "id": 22611,
    "package_name": "umx",
    "title": "Structural Equation Modeling and Twin Modeling in R",
    "description": "Quickly create, run, and report structural equation models, and twin models.\n    See '?umx' for help, and umx_open_CRAN_page(\"umx\") for NEWS.\n    Timothy C. Bates, Michael C. Neale, Hermine H. Maes, (2019). umx: A library for Structural Equation and Twin Modelling in R.\n    Twin Research and Human Genetics, 22, 27-41. <doi:10.1017/thg.2019.2>.",
    "version": "4.60.0",
    "maintainer": "Timothy C. Bates <timothy.c.bates@gmail.com>",
    "author": "Timothy C. Bates [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1153-9007>),\n  Nathan Gillespie [wit],\n  Hermine Maes [ctb],\n  Michael C. Neale [ctb],\n  Joshua N. Pritikin [ctb],\n  Luis De Araujo [ctb],\n  Brenton Wiernik [ctb],\n  Michael Zakharin [wit]",
    "url": "https://github.com/tbates/umx#readme",
    "bug_reports": "https://github.com/tbates/umx/issues",
    "repository": "https://cran.r-project.org/package=umx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "umx Structural Equation Modeling and Twin Modeling in R Quickly create, run, and report structural equation models, and twin models.\n    See '?umx' for help, and umx_open_CRAN_page(\"umx\") for NEWS.\n    Timothy C. Bates, Michael C. Neale, Hermine H. Maes, (2019). umx: A library for Structural Equation and Twin Modelling in R.\n    Twin Research and Human Genetics, 22, 27-41. <doi:10.1017/thg.2019.2>.  "
  },
  {
    "id": 22613,
    "package_name": "uncertainUCDP",
    "title": "Parametric Mixture Models for Uncertainty Estimation of\nFatalities in UCDP Conflict Data",
    "description": "Provides functions for estimating uncertainty in the number of fatalities in the Uppsala Conflict Data Program (UCDP) data. The package implements a parametric reported-value Gumbel mixture distribution that accounts for the uncertainty in the number of fatalities in the UCDP data. The model is based on information from a survey on UCDP coders and how they view the uncertainty of the number of fatalities from UCDP events. The package provides functions for making random draws of fatalities from the mixture distribution, as well as to estimate percentiles, quantiles, means, and other statistics of the distribution. Full details on the survey and estimation procedure can be found in Vesco et al (2024).",
    "version": "0.6.1",
    "maintainer": "David Randahl <david.randahl@pcr.uu.se>",
    "author": "David Randahl [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=uncertainUCDP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uncertainUCDP Parametric Mixture Models for Uncertainty Estimation of\nFatalities in UCDP Conflict Data Provides functions for estimating uncertainty in the number of fatalities in the Uppsala Conflict Data Program (UCDP) data. The package implements a parametric reported-value Gumbel mixture distribution that accounts for the uncertainty in the number of fatalities in the UCDP data. The model is based on information from a survey on UCDP coders and how they view the uncertainty of the number of fatalities from UCDP events. The package provides functions for making random draws of fatalities from the mixture distribution, as well as to estimate percentiles, quantiles, means, and other statistics of the distribution. Full details on the survey and estimation procedure can be found in Vesco et al (2024).  "
  },
  {
    "id": 22616,
    "package_name": "understandBPMN",
    "title": "Calculator of Understandability Metrics for BPMN",
    "description": "Calculate several understandability metrics of BPMN models. BPMN stands for business process modelling notation and is a language for expressing business processes into business process diagrams. Examples of these understandability metrics are: average connector degree, maximum connector degree, sequentiality, cyclicity, diameter, depth, token split, control flow complexity, connector mismatch, connector heterogeneity, separability, structuredness and cross connectivity. See R documentation and paper on metric implementation included in this package for more information concerning the metrics.",
    "version": "1.1.1",
    "maintainer": "Gert Janssenswillen <gert.janssenswillen@uhasselt.be>",
    "author": "Jonas Lieben [aut],\n  Gert Janssenswillen [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=understandBPMN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "understandBPMN Calculator of Understandability Metrics for BPMN Calculate several understandability metrics of BPMN models. BPMN stands for business process modelling notation and is a language for expressing business processes into business process diagrams. Examples of these understandability metrics are: average connector degree, maximum connector degree, sequentiality, cyclicity, diameter, depth, token split, control flow complexity, connector mismatch, connector heterogeneity, separability, structuredness and cross connectivity. See R documentation and paper on metric implementation included in this package for more information concerning the metrics.  "
  },
  {
    "id": 22622,
    "package_name": "unhcrdown",
    "title": "UNHCR Branded Templates for R Markdown Documents",
    "description": "Create United Nations High Commissioner for Refugees (UNHCR)\n    branded documents, presentations, and reports using R Markdown templates.\n    This package provides customized formats that align with UNHCR's official\n    brand guidelines for creating professional PDF reports, Word documents,\n    PowerPoint presentations, and HTML outputs.",
    "version": "0.6.0",
    "maintainer": "C\u00e9dric Vidonne <cedric@vidonne.me>",
    "author": "C\u00e9dric Vidonne [aut, cre],\n  Ahmadou Dicko [aut],\n  Edouard Legoupil [aut],\n  UNHCR [cph]",
    "url": "https://github.com/unhcr-dataviz/unhcrdown,\nhttps://unhcr-dataviz.github.io/unhcrdown/",
    "bug_reports": "https://github.com/unhcr-dataviz/unhcrdown/issues",
    "repository": "https://cran.r-project.org/package=unhcrdown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unhcrdown UNHCR Branded Templates for R Markdown Documents Create United Nations High Commissioner for Refugees (UNHCR)\n    branded documents, presentations, and reports using R Markdown templates.\n    This package provides customized formats that align with UNHCR's official\n    brand guidelines for creating professional PDF reports, Word documents,\n    PowerPoint presentations, and HTML outputs.  "
  },
  {
    "id": 22643,
    "package_name": "units",
    "title": "Measurement Units for R Vectors",
    "description": "Support for measurement units in R vectors, matrices\n    and arrays: automatic propagation, conversion, derivation\n    and simplification of units; raising errors in case of unit\n    incompatibility. Compatible with the POSIXct, Date and difftime \n    classes. Uses the UNIDATA udunits library and unit database for \n    unit compatibility checking and conversion.\n    Documentation about 'units' is provided in the paper by Pebesma, Mailund &\n    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a\n    vignette; see 'citation(\"units\")' for details.",
    "version": "1.0-0",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Edzer Pebesma [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8049-7069>),\n  Thomas Mailund [aut],\n  Tomasz Kalinowski [aut],\n  James Hiebert [ctb],\n  I\u00f1aki Ucar [aut] (ORCID: <https://orcid.org/0000-0001-6403-5550>),\n  Thomas Lin Pedersen [ctb]",
    "url": "https://r-quantities.github.io/units/,\nhttps://github.com/r-quantities/units",
    "bug_reports": "https://github.com/r-quantities/units/issues",
    "repository": "https://cran.r-project.org/package=units",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "units Measurement Units for R Vectors Support for measurement units in R vectors, matrices\n    and arrays: automatic propagation, conversion, derivation\n    and simplification of units; raising errors in case of unit\n    incompatibility. Compatible with the POSIXct, Date and difftime \n    classes. Uses the UNIDATA udunits library and unit database for \n    unit compatibility checking and conversion.\n    Documentation about 'units' is provided in the paper by Pebesma, Mailund &\n    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a\n    vignette; see 'citation(\"units\")' for details.  "
  },
  {
    "id": 22657,
    "package_name": "unsum",
    "title": "Reconstruct Raw Data from Summary Statistics",
    "description": "Reconstructs all possible raw data that could have led to reported\n    summary statistics. Provides a wrapper for the 'Rust' implementation of the\n    'CLOSURE' algorithm.",
    "version": "0.2.0",
    "maintainer": "Lukas Jung <jung-lukas@gmx.net>",
    "author": "Lukas Jung [aut, cre],\n  Nathanael Larigaldie [ctb]",
    "url": "https://github.com/lhdjung/unsum, https://lhdjung.github.io/unsum/",
    "bug_reports": "https://github.com/lhdjung/unsum/issues",
    "repository": "https://cran.r-project.org/package=unsum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unsum Reconstruct Raw Data from Summary Statistics Reconstructs all possible raw data that could have led to reported\n    summary statistics. Provides a wrapper for the 'Rust' implementation of the\n    'CLOSURE' algorithm.  "
  },
  {
    "id": 22671,
    "package_name": "uptasticsearch",
    "title": "Get Data Frame Representations of 'Elasticsearch' Results",
    "description": "\n    'Elasticsearch' is an open-source, distributed, document-based datastore\n    (<https://www.elastic.co/products/elasticsearch>).\n    It provides an 'HTTP' 'API' for querying the database and extracting datasets, but that\n    'API' was not designed for common data science workflows like pulling large batches of\n    records and normalizing those documents into a data frame that can be used as a training\n    dataset for statistical models. 'uptasticsearch' provides an interface for 'Elasticsearch'\n    that is explicitly designed to make these data science workflows easy and fun.",
    "version": "1.0.0",
    "maintainer": "James Lamb <jaylamb20@gmail.com>",
    "author": "James Lamb [aut, cre],\n  Nick Paras [aut],\n  Austin Dickey [aut],\n  Michael Frasco [ctb],\n  Weiwen Gu [ctb],\n  Will Dearden [ctb],\n  Uptake Technologies Inc. [cph]",
    "url": "https://github.com/uptake/uptasticsearch",
    "bug_reports": "https://github.com/uptake/uptasticsearch/issues",
    "repository": "https://cran.r-project.org/package=uptasticsearch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uptasticsearch Get Data Frame Representations of 'Elasticsearch' Results \n    'Elasticsearch' is an open-source, distributed, document-based datastore\n    (<https://www.elastic.co/products/elasticsearch>).\n    It provides an 'HTTP' 'API' for querying the database and extracting datasets, but that\n    'API' was not designed for common data science workflows like pulling large batches of\n    records and normalizing those documents into a data frame that can be used as a training\n    dataset for statistical models. 'uptasticsearch' provides an interface for 'Elasticsearch'\n    that is explicitly designed to make these data science workflows easy and fun.  "
  },
  {
    "id": 22686,
    "package_name": "usdampr",
    "title": "Request USDA MPR Historical Data via the 'LMR' API",
    "description": "Interface to easily access data via the United States Department of Agriculture (USDA)'s Livestock Mandatory Reporting ('LMR')\n  Data API at <https://mpr.datamart.ams.usda.gov/>. The downloaded data can be saved for later off-line use. \n  Also provide relevant information and metadata for each of the input variables needed for sending the data inquiry.   ",
    "version": "1.0.1",
    "maintainer": "Bowen Chen <bwchen0719@gmail.com>",
    "author": "Bowen Chen [aut, cre] (ORCID: <https://orcid.org/0000-0003-0370-2756>),\n  Elliott Dennis [aut]",
    "url": "https://github.com/cbw1243/usdampr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=usdampr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "usdampr Request USDA MPR Historical Data via the 'LMR' API Interface to easily access data via the United States Department of Agriculture (USDA)'s Livestock Mandatory Reporting ('LMR')\n  Data API at <https://mpr.datamart.ams.usda.gov/>. The downloaded data can be saved for later off-line use. \n  Also provide relevant information and metadata for each of the input variables needed for sending the data inquiry.     "
  },
  {
    "id": 22691,
    "package_name": "usdoj",
    "title": "For Accessing U.S. Department of Justice (DOJ) Open Data",
    "description": "Fetch data from the <https://www.justice.gov/developer/api-documentation/api_v1> API such as press releases, blog entries, and speeches. Optional parameters allow users to specify the number of results starting from the earliest or latest entries, and whether these results contain keywords. Data is cleaned for analysis and returned in a dataframe. ",
    "version": "1.1.2",
    "maintainer": "Steph Buongiorno <steph.buon@gmail.com>",
    "author": "Steph Buongiorno [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6965-0787>)",
    "url": "https://github.com/ropengov/usdoj",
    "bug_reports": "https://github.com/ropengov/usdoj/issues",
    "repository": "https://cran.r-project.org/package=usdoj",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "usdoj For Accessing U.S. Department of Justice (DOJ) Open Data Fetch data from the <https://www.justice.gov/developer/api-documentation/api_v1> API such as press releases, blog entries, and speeches. Optional parameters allow users to specify the number of results starting from the earliest or latest entries, and whether these results contain keywords. Data is cleaned for analysis and returned in a dataframe.   "
  },
  {
    "id": 22693,
    "package_name": "usedthese",
    "title": "Summarises Package & Function Usage",
    "description": "Consistent with 'knitr' syntax highlighting, 'usedthese' adds\n    a summary table of package & function usage to a Quarto document and\n    enables aggregation of usage across a website.",
    "version": "0.5.0",
    "maintainer": "Carl Goodwin <carl.goodwin@quantumjitter.com>",
    "author": "Carl Goodwin [aut, cre, cph]",
    "url": "https://cgoo4.github.io/usedthese/,\nhttps://github.com/cgoo4/usedthese",
    "bug_reports": "https://github.com/cgoo4/usedthese/issues",
    "repository": "https://cran.r-project.org/package=usedthese",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "usedthese Summarises Package & Function Usage Consistent with 'knitr' syntax highlighting, 'usedthese' adds\n    a summary table of package & function usage to a Quarto document and\n    enables aggregation of usage across a website.  "
  },
  {
    "id": 22718,
    "package_name": "uwedragon",
    "title": "Data Research, Access, Governance Network : Statistical\nDisclosure Control",
    "description": "A tool for checking how much information is disclosed when\n    reporting summary statistics.",
    "version": "0.1.0",
    "maintainer": "Ben Derrick <ben.derrick@uwe.ac.uk>",
    "author": "Ben Derrick",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=uwedragon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uwedragon Data Research, Access, Governance Network : Statistical\nDisclosure Control A tool for checking how much information is disclosed when\n    reporting summary statistics.  "
  },
  {
    "id": 22719,
    "package_name": "uwot",
    "title": "The Uniform Manifold Approximation and Projection (UMAP) Method\nfor Dimensionality Reduction",
    "description": "An implementation of the Uniform Manifold Approximation and\n    Projection dimensionality reduction by McInnes et al. (2018)\n    <doi:10.48550/arXiv.1802.03426>. It also provides means to transform new data and\n    to carry out supervised dimensionality reduction. An implementation of\n    the related LargeVis method of Tang et al. (2016) <doi:10.48550/arXiv.1602.00370>\n    is also provided. This is a complete re-implementation in R (and C++,\n    via the 'Rcpp' package): no Python installation is required. See the\n    uwot website (<https://github.com/jlmelville/uwot>) for more\n    documentation and examples.",
    "version": "0.2.4",
    "maintainer": "James Melville <jlmelville@gmail.com>",
    "author": "James Melville [aut, cre, cph],\n  Aaron Lun [ctb],\n  Mohamed Nadhir Djekidel [ctb],\n  Yuhan Hao [ctb],\n  Dirk Eddelbuettel [ctb],\n  Wouter van der Bijl [ctb],\n  Hugo Gruson [ctb]",
    "url": "https://github.com/jlmelville/uwot,\nhttps://jlmelville.github.io/uwot/",
    "bug_reports": "https://github.com/jlmelville/uwot/issues",
    "repository": "https://cran.r-project.org/package=uwot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uwot The Uniform Manifold Approximation and Projection (UMAP) Method\nfor Dimensionality Reduction An implementation of the Uniform Manifold Approximation and\n    Projection dimensionality reduction by McInnes et al. (2018)\n    <doi:10.48550/arXiv.1802.03426>. It also provides means to transform new data and\n    to carry out supervised dimensionality reduction. An implementation of\n    the related LargeVis method of Tang et al. (2016) <doi:10.48550/arXiv.1602.00370>\n    is also provided. This is a complete re-implementation in R (and C++,\n    via the 'Rcpp' package): no Python installation is required. See the\n    uwot website (<https://github.com/jlmelville/uwot>) for more\n    documentation and examples.  "
  },
  {
    "id": 22720,
    "package_name": "uxr",
    "title": "User Experience Research",
    "description": "Provides convenience functions for user experience research\n    with an emphasis on quantitative user experience testing and reporting.\n    The functions are designed to translate statistical approaches to \n    applied user experience research.",
    "version": "0.2.0",
    "maintainer": "Joe Chelladurai <joe.chelladurai@yahoo.com>",
    "author": "Joe Chelladurai [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8477-3753>)",
    "url": "https://joe-chelladurai.github.io/uxr/",
    "bug_reports": "https://github.com/joe-chelladurai/uxr/issues",
    "repository": "https://cran.r-project.org/package=uxr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uxr User Experience Research Provides convenience functions for user experience research\n    with an emphasis on quantitative user experience testing and reporting.\n    The functions are designed to translate statistical approaches to \n    applied user experience research.  "
  },
  {
    "id": 22729,
    "package_name": "vaersNDvax",
    "title": "Non-Domestic Vaccine Adverse Event Reporting System (VAERS)\nVaccine Data for Present",
    "description": "Non-Domestic VAERS vaccine data for 01/01/2016 - 06/14/2016. If\n    you want to explore the full VAERS data for 1990 - Present (data, symptoms,\n    and vaccines), then check out the 'vaersND' package from the URL below. The\n    URL and BugReports below correspond to the 'vaersND' package, of which\n    'vaersNDvax' is a small subset (2016 only). 'vaersND' is not hosted on CRAN\n    due to the large size of the data set. To install the Suggested 'vaers' and\n    'vaersND' packages, use the following R code:\n    'devtools::install_git(\"https://gitlab.com/iembry/vaers.git\",\n    build_vignettes = TRUE)' and\n    'devtools::install_git(\"https://gitlab.com/iembry/vaersND.git\",\n    build_vignettes = TRUE)'. \"VAERS is a national vaccine safety\n    surveillance program co-sponsored by the US Centers for Disease Control and\n    Prevention (CDC) and the US Food and Drug Administration (FDA). VAERS is a\n    post-marketing safety surveillance program, collecting information about\n    adverse events (possible side effects) that occur after the administration\n    of vaccines licensed for use in the United States.\" For more information\n    about the data, visit <https://vaers.hhs.gov/index>. For information about\n    vaccination/immunization hazards, visit\n    <http://www.questionuniverse.com/rethink.html/#vaccine>.",
    "version": "1.0.4",
    "maintainer": "Irucka Embry <iembry@ecoccs.com>",
    "author": "Irucka Embry [aut, cre]",
    "url": "https://gitlab.com/iembry/vaersND",
    "bug_reports": "https://gitlab.com/iembry/vaersND/issues",
    "repository": "https://cran.r-project.org/package=vaersNDvax",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vaersNDvax Non-Domestic Vaccine Adverse Event Reporting System (VAERS)\nVaccine Data for Present Non-Domestic VAERS vaccine data for 01/01/2016 - 06/14/2016. If\n    you want to explore the full VAERS data for 1990 - Present (data, symptoms,\n    and vaccines), then check out the 'vaersND' package from the URL below. The\n    URL and BugReports below correspond to the 'vaersND' package, of which\n    'vaersNDvax' is a small subset (2016 only). 'vaersND' is not hosted on CRAN\n    due to the large size of the data set. To install the Suggested 'vaers' and\n    'vaersND' packages, use the following R code:\n    'devtools::install_git(\"https://gitlab.com/iembry/vaers.git\",\n    build_vignettes = TRUE)' and\n    'devtools::install_git(\"https://gitlab.com/iembry/vaersND.git\",\n    build_vignettes = TRUE)'. \"VAERS is a national vaccine safety\n    surveillance program co-sponsored by the US Centers for Disease Control and\n    Prevention (CDC) and the US Food and Drug Administration (FDA). VAERS is a\n    post-marketing safety surveillance program, collecting information about\n    adverse events (possible side effects) that occur after the administration\n    of vaccines licensed for use in the United States.\" For more information\n    about the data, visit <https://vaers.hhs.gov/index>. For information about\n    vaccination/immunization hazards, visit\n    <http://www.questionuniverse.com/rethink.html/#vaccine>.  "
  },
  {
    "id": 22730,
    "package_name": "vaersvax",
    "title": "US Vaccine Adverse Event Reporting System (VAERS) Vaccine Data\nfor Present",
    "description": "US VAERS vaccine data for 01/01/2018 - 06/14/2018. If you want to\n    explore the full VAERS data for 1990 - Present (data, symptoms, and\n    vaccines), then check out the 'vaers' package from the URL below. The URL\n    and BugReports below correspond to the 'vaers' package, of which 'vaersvax'\n    is a small subset (2018 only). 'vaers' is not hosted on CRAN due to the\n    large size of the data set. To install the Suggested 'vaers' and 'vaersND'\n    packages, use the following R code:\n    'devtools::install_git(\"<https://gitlab.com/iembry/vaers.git>\",\n    build_vignettes = TRUE)' and\n    'devtools::install_git(\"<https://gitlab.com/iembry/vaersND.git>\",\n    build_vignettes = TRUE)'. \"The Vaccine Adverse Event Reporting System (VAERS)\n    is a national early warning system to detect possible safety problems in\n    U.S.-licensed vaccines. VAERS is co-managed by the Centers for Disease Control\n    and Prevention (CDC) and the U.S. Food and Drug Administration (FDA).\" For\n    more information about the data, visit <https://vaers.hhs.gov/>. For\n    information about\n    vaccination/immunization hazards, visit\n    <http://www.questionuniverse.com/rethink.html#vaccine>.",
    "version": "1.0.5",
    "maintainer": "Irucka Embry <iembry@ecoccs.com>",
    "author": "Irucka Embry [aut, cre]",
    "url": "https://gitlab.com/iembry/vaers",
    "bug_reports": "https://gitlab.com/iembry/vaers/issues",
    "repository": "https://cran.r-project.org/package=vaersvax",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vaersvax US Vaccine Adverse Event Reporting System (VAERS) Vaccine Data\nfor Present US VAERS vaccine data for 01/01/2018 - 06/14/2018. If you want to\n    explore the full VAERS data for 1990 - Present (data, symptoms, and\n    vaccines), then check out the 'vaers' package from the URL below. The URL\n    and BugReports below correspond to the 'vaers' package, of which 'vaersvax'\n    is a small subset (2018 only). 'vaers' is not hosted on CRAN due to the\n    large size of the data set. To install the Suggested 'vaers' and 'vaersND'\n    packages, use the following R code:\n    'devtools::install_git(\"<https://gitlab.com/iembry/vaers.git>\",\n    build_vignettes = TRUE)' and\n    'devtools::install_git(\"<https://gitlab.com/iembry/vaersND.git>\",\n    build_vignettes = TRUE)'. \"The Vaccine Adverse Event Reporting System (VAERS)\n    is a national early warning system to detect possible safety problems in\n    U.S.-licensed vaccines. VAERS is co-managed by the Centers for Disease Control\n    and Prevention (CDC) and the U.S. Food and Drug Administration (FDA).\" For\n    more information about the data, visit <https://vaers.hhs.gov/>. For\n    information about\n    vaccination/immunization hazards, visit\n    <http://www.questionuniverse.com/rethink.html#vaccine>.  "
  },
  {
    "id": 22782,
    "package_name": "vatcheckapi",
    "title": "Client for the 'vatcheckapi.com' VAT Validation API",
    "description": "An R client for the 'vatcheckapi.com' VAT number validation API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://vatcheckapi.com/docs> .",
    "version": "0.1.0",
    "maintainer": "Dominik Kukacka <dominik@everapi.com>",
    "author": "Dominik Kukacka [aut, cre]",
    "url": "https://vatcheckapi.com, https://vatcheckapi.com/docs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vatcheckapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vatcheckapi Client for the 'vatcheckapi.com' VAT Validation API An R client for the 'vatcheckapi.com' VAT number validation API. The API requires registration of an API key. Basic features are free, some require a paid subscription. You can find the full API documentation at <https://vatcheckapi.com/docs> .  "
  },
  {
    "id": 22790,
    "package_name": "vcdExtra",
    "title": "'vcd' Extensions and Additions",
    "description": "Provides additional data sets, methods and documentation to complement the 'vcd' package for Visualizing Categorical Data\n    and the 'gnm' package for Generalized Nonlinear Models.\n\tIn particular, 'vcdExtra' extends mosaic, assoc and sieve plots from 'vcd' to handle 'glm()' and 'gnm()' models and\n\tadds a 3D version in 'mosaic3d'.  Additionally, methods are provided for comparing and visualizing lists of\n\t'glm' and 'loglm' objects. This package is now a support package for the book, \"Discrete Data Analysis with R\" by\n  Michael Friendly and David Meyer.",
    "version": "0.8.7",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>),\n  Heather Turner [ctb],\n  David Meyer [ctb],\n  Achim Zeileis [ctb] (ORCID: <https://orcid.org/0000-0003-0918-3766>),\n  Duncan Murdoch [ctb],\n  David Firth [ctb],\n  Matt Kumar [ctb],\n  Shuguang Sun [ctb],\n  Daniel Sabanes Bove [ctb]",
    "url": "https://friendly.github.io/vcdExtra/,\nhttps://github.com/friendly/vcdExtra",
    "bug_reports": "https://github.com/friendly/vcdExtra/issues",
    "repository": "https://cran.r-project.org/package=vcdExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vcdExtra 'vcd' Extensions and Additions Provides additional data sets, methods and documentation to complement the 'vcd' package for Visualizing Categorical Data\n    and the 'gnm' package for Generalized Nonlinear Models.\n\tIn particular, 'vcdExtra' extends mosaic, assoc and sieve plots from 'vcd' to handle 'glm()' and 'gnm()' models and\n\tadds a 3D version in 'mosaic3d'.  Additionally, methods are provided for comparing and visualizing lists of\n\t'glm' and 'loglm' objects. This package is now a support package for the book, \"Discrete Data Analysis with R\" by\n  Michael Friendly and David Meyer.  "
  },
  {
    "id": 22801,
    "package_name": "veccompare",
    "title": "Perform Set Operations on Vectors, Automatically Generating All\nn-Wise Comparisons, and Create Markdown Output",
    "description": "Automates set operations (i.e., comparisons of overlap) between multiple vectors.\n    It also contains a function for automating reporting in 'RMarkdown', by generating markdown output for easy analysis, as well as an 'RMarkdown' template for use with 'RStudio'.",
    "version": "0.1.0",
    "maintainer": "Jacob Gerard Levernier <jlevern@upenn.edu>",
    "author": "Jacob Gerard Levernier [aut, cre] (Designed and authored the package\n    source code and documentation. Roles: author, creator, designer,\n    engineer, programmer),\n  Heather Gaile Wacha [aut] (Provided intellectual overview and\n    consultation during development for use with medieval cartographic\n    datasets. Roles: conceptor, consultant, data contributor)",
    "url": "https://github.com/publicus/r-veccompare",
    "bug_reports": "https://github.com/publicus/r-veccompare/issues",
    "repository": "https://cran.r-project.org/package=veccompare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "veccompare Perform Set Operations on Vectors, Automatically Generating All\nn-Wise Comparisons, and Create Markdown Output Automates set operations (i.e., comparisons of overlap) between multiple vectors.\n    It also contains a function for automating reporting in 'RMarkdown', by generating markdown output for easy analysis, as well as an 'RMarkdown' template for use with 'RStudio'.  "
  },
  {
    "id": 22821,
    "package_name": "vembedr",
    "title": "Embed Video in HTML",
    "description": "A set of functions for generating HTML to\n    embed hosted video in your R Markdown documents or Shiny applications.",
    "version": "0.1.5",
    "maintainer": "Ian Lyttle <ian.lyttle@se.com>",
    "author": "Ian Lyttle [aut, cre] (ORCID: <https://orcid.org/0000-0001-9962-4849>),\n  Schneider Electric [cph]",
    "url": "https://github.com/ijlyttle/vembedr",
    "bug_reports": "https://github.com/ijlyttle/vembedr/issues",
    "repository": "https://cran.r-project.org/package=vembedr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vembedr Embed Video in HTML A set of functions for generating HTML to\n    embed hosted video in your R Markdown documents or Shiny applications.  "
  },
  {
    "id": 22827,
    "package_name": "verdadecu",
    "title": "Data from the Ecuador Truth Commission",
    "description": "Provides access to data collected by the Ecuadorian Truth Commission.\n    Allows users to extract and analyze systematized information for human rights \n    research in Ecuador. The package contains datasets documenting human rights \n    violations from 1984-2008, including victim information, violation types, \n    perpetrators, and geographic distribution.",
    "version": "1.0.0",
    "maintainer": "Javier Borja <javier@demografiando.pro>",
    "author": "Adriana Robles [aut] (ORCID: <https://orcid.org/0000-0001-7202-6523>),\n  Javier Borja [aut, cre]",
    "url": "https://github.com/Demografiando/verdadecu",
    "bug_reports": "https://github.com/Demografiando/verdadecu/issues",
    "repository": "https://cran.r-project.org/package=verdadecu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "verdadecu Data from the Ecuador Truth Commission Provides access to data collected by the Ecuadorian Truth Commission.\n    Allows users to extract and analyze systematized information for human rights \n    research in Ecuador. The package contains datasets documenting human rights \n    violations from 1984-2008, including victim information, violation types, \n    perpetrators, and geographic distribution.  "
  },
  {
    "id": 22845,
    "package_name": "viafoundry",
    "title": "R Client for 'Via Foundry' API",
    "description": "'Via Foundry' API provides streamlined tools for interacting with and extracting data from structured responses, particularly for use cases involving hierarchical data from Foundry's API. It includes functions to fetch and parse process-level and file-level metadata, allowing users to efficiently query and manipulate nested data structures. Key features include the ability to list all unique process names, retrieve file metadata for specific or all processes, and dynamically load or download files based on their type. With built-in support for handling various file formats (e.g., tabular and non-tabular files) and seamless integration with API through authentication, this package is designed to enhance workflows involving large-scale data management and analysis. Robust error handling and flexible configuration ensure reliable performance across diverse data environments. Please consult the documentation for the API endpoint for your installation. ",
    "version": "1.0.1",
    "maintainer": "Alper Kucukural <alper@viascientific.com>",
    "author": "Alper Kucukural [aut, cre],\n  Via Scientific [aut, cph]",
    "url": "https://github.com/ViaScientific/viafoundry-R-SDK",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=viafoundry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "viafoundry R Client for 'Via Foundry' API 'Via Foundry' API provides streamlined tools for interacting with and extracting data from structured responses, particularly for use cases involving hierarchical data from Foundry's API. It includes functions to fetch and parse process-level and file-level metadata, allowing users to efficiently query and manipulate nested data structures. Key features include the ability to list all unique process names, retrieve file metadata for specific or all processes, and dynamically load or download files based on their type. With built-in support for handling various file formats (e.g., tabular and non-tabular files) and seamless integration with API through authentication, this package is designed to enhance workflows involving large-scale data management and analysis. Robust error handling and flexible configuration ensure reliable performance across diverse data environments. Please consult the documentation for the API endpoint for your installation.   "
  },
  {
    "id": 22853,
    "package_name": "viewscape",
    "title": "Viewscape Analysis",
    "description": "A collection of functions to make R a more effective viewscape analysis \n    tool for calculating viewscape metrics based on computing the viewable area for \n    given a point/multiple viewpoints and a digital elevation model.The method of calculating \n    viewscape metrics implemented in this package are based on the work of \n    Tabrizian et al. (2020) <doi:10.1016/j.landurbplan.2019.103704>. The algorithm of computing \n    viewshed is based on the work of \n    Franklin & Ray. (1994) <https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=555780f6f5d7e537eb1edb28862c86d1519af2be>.",
    "version": "2.0.2",
    "maintainer": "Xiaohao Yang <xiaohaoy@umich.edu>",
    "author": "Xiaohao Yang [aut, cre, cph],\n  Nathan Fox [aut],\n  Derek Van Berkel [aut],\n  Mark Lindquist [aut]",
    "url": "https://github.com/land-info-lab/viewscape",
    "bug_reports": "https://github.com/land-info-lab/viewscape/issues",
    "repository": "https://cran.r-project.org/package=viewscape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "viewscape Viewscape Analysis A collection of functions to make R a more effective viewscape analysis \n    tool for calculating viewscape metrics based on computing the viewable area for \n    given a point/multiple viewpoints and a digital elevation model.The method of calculating \n    viewscape metrics implemented in this package are based on the work of \n    Tabrizian et al. (2020) <doi:10.1016/j.landurbplan.2019.103704>. The algorithm of computing \n    viewshed is based on the work of \n    Franklin & Ray. (1994) <https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=555780f6f5d7e537eb1edb28862c86d1519af2be>.  "
  },
  {
    "id": 22884,
    "package_name": "vistime",
    "title": "Pretty Timelines in R",
    "description": "A library for creating time based charts, like Gantt or timelines. Possible outputs \n  include 'ggplot2' diagrams, 'plotly.js' graphs, 'Highcharts.js' widgets and data.frames. Results can be\n  used in the 'RStudio' viewer pane, in 'RMarkdown' documents or in Shiny apps. In the \n  interactive outputs created by vistime() and hc_vistime(), you can interact with the \n  plot using mouse hover or zoom.",
    "version": "1.2.4",
    "maintainer": "Sandro Raabe <sa.ra.online@posteo.de>",
    "author": "Sandro Raabe [aut, cre]",
    "url": "https://shosaco.github.io/vistime/",
    "bug_reports": "https://github.com/shosaco/vistime/issues",
    "repository": "https://cran.r-project.org/package=vistime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vistime Pretty Timelines in R A library for creating time based charts, like Gantt or timelines. Possible outputs \n  include 'ggplot2' diagrams, 'plotly.js' graphs, 'Highcharts.js' widgets and data.frames. Results can be\n  used in the 'RStudio' viewer pane, in 'RMarkdown' documents or in Shiny apps. In the \n  interactive outputs created by vistime() and hc_vistime(), you can interact with the \n  plot using mouse hover or zoom.  "
  },
  {
    "id": 22890,
    "package_name": "visvaR",
    "title": "Shiny-Based Statistical Solutions for Agricultural Research",
    "description": "Visualize Variance is an intuitive 'shiny' applications tailored for agricultural research data analysis, including one-way and two-way analysis of variance, correlation, and other essential statistical tools. Users can easily upload their datasets, perform analyses, and download the results as a well-formatted document, streamlining the process of data analysis and reporting in agricultural research.The experimental design methods are based on classical work by Fisher (1925) and Scheffe (1959). The correlation visualization approaches follow methods developed by Wei & Simko (2021) and Friendly (2002) <doi:10.1198/000313002533>.",
    "version": "1.0.0",
    "maintainer": "Ramesh Ramasamy <ramesh.rahu96@gmail.com>",
    "author": "Ramesh Ramasamy [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5433-1417>),\n  Mathiyarasi Kulandaivadivel [ctb],\n  Tamilselvan Arumugam [ctb]",
    "url": "https://github.com/rameshram96/visvaR,\nhttps://rameshram96.github.io/visvaR/",
    "bug_reports": "https://github.com/rameshram96/visvaR/issues",
    "repository": "https://cran.r-project.org/package=visvaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "visvaR Shiny-Based Statistical Solutions for Agricultural Research Visualize Variance is an intuitive 'shiny' applications tailored for agricultural research data analysis, including one-way and two-way analysis of variance, correlation, and other essential statistical tools. Users can easily upload their datasets, perform analyses, and download the results as a well-formatted document, streamlining the process of data analysis and reporting in agricultural research.The experimental design methods are based on classical work by Fisher (1925) and Scheffe (1959). The correlation visualization approaches follow methods developed by Wei & Simko (2021) and Friendly (2002) <doi:10.1198/000313002533>.  "
  },
  {
    "id": 22894,
    "package_name": "vitae",
    "title": "Curriculum Vitae for R Markdown",
    "description": "Provides templates and functions to simplify the production and maintenance of curriculum vitae.",
    "version": "0.6.0",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "author": "Mitchell O'Hara-Wild [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6729-7695>),\n  Rob Hyndman [aut] (ORCID: <https://orcid.org/0000-0002-2140-5352>),\n  Yihui Xie [ctb] (ORCID: <https://orcid.org/0000-0003-0645-5666>),\n  Albert Krewinkel [cph] (Multiple bibliographies lua filter),\n  JooYoung Seo [ctb] (ORCID: <https://orcid.org/0000-0002-4064-6012>),\n  Isabelle Greco [ctb] (ORCID: <https://orcid.org/0000-0003-1604-6639>)",
    "url": "https://pkg.mitchelloharawild.com/vitae/,\nhttps://github.com/mitchelloharawild/vitae",
    "bug_reports": "https://github.com/mitchelloharawild/vitae/issues",
    "repository": "https://cran.r-project.org/package=vitae",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vitae Curriculum Vitae for R Markdown Provides templates and functions to simplify the production and maintenance of curriculum vitae.  "
  },
  {
    "id": 22903,
    "package_name": "vmTools",
    "title": "Version Management Tools on the File System",
    "description": "Data version management on the file system for smaller projects.\n    Manage data pipeline outputs with symbolic folder links, structured logging\n    and reports, using 'R6' classes for encapsulation and 'data.table' for\n    speed. Directory-specific logs used as source of truth to allow portability\n    of versioned data folders.",
    "version": "1.0.1",
    "maintainer": "Sam Byrne <ssbyrne@uw.edu>",
    "author": "Sam Byrne [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0008-1067-307X>)",
    "url": "https://github.com/epi-sam/vmTools",
    "bug_reports": "https://github.com/epi-sam/vmTools/issues",
    "repository": "https://cran.r-project.org/package=vmTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vmTools Version Management Tools on the File System Data version management on the file system for smaller projects.\n    Manage data pipeline outputs with symbolic folder links, structured logging\n    and reports, using 'R6' classes for encapsulation and 'data.table' for\n    speed. Directory-specific logs used as source of truth to allow portability\n    of versioned data folders.  "
  },
  {
    "id": 22912,
    "package_name": "voiceR",
    "title": "Voice Analytics for Social Scientists",
    "description": "Simplifies and largely automates practical voice analytics for social science research. This package offers an accessible and easy-to-use interface, including an interactive Shiny app, that simplifies the processing, extraction, analysis, and reporting of voice recording data in the behavioral and social sciences. The package includes batch processing capabilities to read and analyze multiple voice files in parallel, automates the extraction of key vocal features for further analysis, and automatically generates APA formatted reports for typical between-group comparisons in experimental social science research. A more extensive methodological introduction that inspired the development of the 'voiceR' package is provided in Hildebrand et al. 2020 <doi:10.1016/j.jbusres.2020.09.020>.  ",
    "version": "0.1.0",
    "maintainer": "Francesc Busquet <francesc.busquet@unisg.ch>",
    "author": "Francesc Busquet [aut, cre],\n  Christian Hildebrand [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=voiceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "voiceR Voice Analytics for Social Scientists Simplifies and largely automates practical voice analytics for social science research. This package offers an accessible and easy-to-use interface, including an interactive Shiny app, that simplifies the processing, extraction, analysis, and reporting of voice recording data in the behavioral and social sciences. The package includes batch processing capabilities to read and analyze multiple voice files in parallel, automates the extraction of key vocal features for further analysis, and automatically generates APA formatted reports for typical between-group comparisons in experimental social science research. A more extensive methodological introduction that inspired the development of the 'voiceR' package is provided in Hildebrand et al. 2020 <doi:10.1016/j.jbusres.2020.09.020>.    "
  },
  {
    "id": 22918,
    "package_name": "volker",
    "title": "High-Level Functions for Tabulating, Charting and Reporting\nSurvey Data",
    "description": "Craft polished tables and plots in Markdown reports. \n             Simply choose whether to treat your data as counts or metrics, \n             and the package will automatically generate well-designed default tables and plots for you.\n             Boiled down to the basics, with labeling features and simple interactive reports.\n             All functions are 'tidyverse' compatible.",
    "version": "3.2.0",
    "maintainer": "Jakob J\u00fcnger <jakob.juenger@uni-muenster.de>",
    "author": "Jakob J\u00fcnger [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1860-6695>),\n  Henrieke Kotthoff [aut, ctb],\n  Chantal G\u00e4rtner [ctb] (ORCID: <https://orcid.org/0000-0002-3653-6013>)",
    "url": "https://github.com/strohne/volker,\nhttps://strohne.github.io/volker/",
    "bug_reports": "https://github.com/strohne/volker/issues",
    "repository": "https://cran.r-project.org/package=volker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "volker High-Level Functions for Tabulating, Charting and Reporting\nSurvey Data Craft polished tables and plots in Markdown reports. \n             Simply choose whether to treat your data as counts or metrics, \n             and the package will automatically generate well-designed default tables and plots for you.\n             Boiled down to the basics, with labeling features and simple interactive reports.\n             All functions are 'tidyverse' compatible.  "
  },
  {
    "id": 22945,
    "package_name": "vtable",
    "title": "Variable Table for Variable Documentation",
    "description": "Automatically generates HTML variable documentation including variable names, labels, classes, value labels (if applicable), value ranges, and summary statistics. See the vignette \"vtable\" for a package overview.",
    "version": "1.4.8",
    "maintainer": "Nick Huntington-Klein <nhuntington-klein@seattleu.edu>",
    "author": "Nick Huntington-Klein [aut, cre]",
    "url": "https://nickch-k.github.io/vtable/",
    "bug_reports": "https://github.com/NickCH-K/vtable/issues",
    "repository": "https://cran.r-project.org/package=vtable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vtable Variable Table for Variable Documentation Automatically generates HTML variable documentation including variable names, labels, classes, value labels (if applicable), value ranges, and summary statistics. See the vignette \"vtable\" for a package overview.  "
  },
  {
    "id": 22971,
    "package_name": "waetr",
    "title": "'WebAIM' 'WAVE' Accessibility Evaluation Tool",
    "description": "An R interface to the 'WebAIM' 'WAVE' accessibility evaluation\n    API <https://wave.webaim.org/api/>. This package provides tools for analyzing web pages for\n    accessibility issues, generating reports, and comparing accessibility\n    across multiple websites.",
    "version": "0.1.0",
    "maintainer": "Benjamin Listyg <listyg.ben@gmail.com>",
    "author": "Benjamin Listyg [aut, cre],\n  Brennah V. Ross [aut]",
    "url": "https://github.com/benjaminlistyg/waetr",
    "bug_reports": "https://github.com/benjaminlistyg/waetr/issues",
    "repository": "https://cran.r-project.org/package=waetr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "waetr 'WebAIM' 'WAVE' Accessibility Evaluation Tool An R interface to the 'WebAIM' 'WAVE' accessibility evaluation\n    API <https://wave.webaim.org/api/>. This package provides tools for analyzing web pages for\n    accessibility issues, generating reports, and comparing accessibility\n    across multiple websites.  "
  },
  {
    "id": 22985,
    "package_name": "warabandi",
    "title": "Roster Generation of Turn for Weekdays:'warabandi'",
    "description": "It generates the roster of turn for an outlet which is flowing \n        (water) 24X7 or 168 hours towards the area under command or agricutural \n        area (to be irrigated). The area under command is differentially owned \n        by different individual farmers. The Outlet runs for free of cost to \n        irrigate the area under command 24X7.\n        So, flow time of the outlet has to be divided based on an area owned by \n        an individual farmer and the location of his land or farm. This roster \n        is known as 'warabandi' and its generation in agriculture practices is a \n        very tedious task. Calculations of time in microseconds are more \n        error-prone, especially whenever it is performed by hands. That division\n        of flow time for an individual farmer can be calculated by 'warabandi'. \n        However, it generates a full publishable report for an outlet and all the \n        farmers who have farms subjected to be irrigated. \n        It reduces error risk and makes a more reproducible roster. For more \n        details about warabandi system you can found elsewhere in \n        Bandaragoda DJ(1995) <https://publications.iwmi.org/pdf/H_17571i.pdf>.",
    "version": "0.1.0",
    "maintainer": "Harvinder Singh <harvindermaan4@gmail.com>",
    "author": "Harvinder Singh",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=warabandi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "warabandi Roster Generation of Turn for Weekdays:'warabandi' It generates the roster of turn for an outlet which is flowing \n        (water) 24X7 or 168 hours towards the area under command or agricutural \n        area (to be irrigated). The area under command is differentially owned \n        by different individual farmers. The Outlet runs for free of cost to \n        irrigate the area under command 24X7.\n        So, flow time of the outlet has to be divided based on an area owned by \n        an individual farmer and the location of his land or farm. This roster \n        is known as 'warabandi' and its generation in agriculture practices is a \n        very tedious task. Calculations of time in microseconds are more \n        error-prone, especially whenever it is performed by hands. That division\n        of flow time for an individual farmer can be calculated by 'warabandi'. \n        However, it generates a full publishable report for an outlet and all the \n        farmers who have farms subjected to be irrigated. \n        It reduces error risk and makes a more reproducible roster. For more \n        details about warabandi system you can found elsewhere in \n        Bandaragoda DJ(1995) <https://publications.iwmi.org/pdf/H_17571i.pdf>.  "
  },
  {
    "id": 22992,
    "package_name": "washi",
    "title": "Washington Soil Health Initiative Branding",
    "description": "Create plots and tables in a consistent style with WaSHI\n    (Washington Soil Health Initiative) branding. Use 'washi' to easily\n    style your 'ggplot2' plots and 'flextable' tables.",
    "version": "0.2.1",
    "maintainer": "Jadey Ryan <jryan@agr.wa.gov>",
    "author": "Jadey Ryan [aut, cre],\n  Molly McIlquham [aut],\n  Dani Gelardi [aut],\n  Washington State Department of Agriculture [cph, fnd]",
    "url": "https://github.com/WA-Department-of-Agriculture/washi,\nhttps://wa-department-of-agriculture.github.io/washi/",
    "bug_reports": "https://github.com/WA-Department-of-Agriculture/washi/issues/",
    "repository": "https://cran.r-project.org/package=washi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "washi Washington Soil Health Initiative Branding Create plots and tables in a consistent style with WaSHI\n    (Washington Soil Health Initiative) branding. Use 'washi' to easily\n    style your 'ggplot2' plots and 'flextable' tables.  "
  },
  {
    "id": 22993,
    "package_name": "washr",
    "title": "Publication Toolkit for Water, Sanitation and Hygiene (WASH)\nData",
    "description": "A toolkit to set up an R data package in a consistent structure. Automates tasks like tidy data export, data dictionary documentation, README and website creation, and citation management.",
    "version": "1.0.1",
    "maintainer": "Colin Walder <cwalder@ethz.ch>",
    "author": "Mian Zhong [aut] (ORCID: <https://orcid.org/0009-0009-4546-7214>),\n  Margaux G\u00f6tschmann [aut] (ORCID:\n    <https://orcid.org/0009-0002-2567-3343>),\n  Colin Walder [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-0969-1954>),\n  Lars Sch\u00f6bitz [aut] (ORCID: <https://orcid.org/0000-0003-2196-5015>),\n  Global Health Engineering, ETH Zurich [cph]",
    "url": "https://openwashdata.github.io/washr/",
    "bug_reports": "https://github.com/openwashdata/washr/issues",
    "repository": "https://cran.r-project.org/package=washr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "washr Publication Toolkit for Water, Sanitation and Hygiene (WASH)\nData A toolkit to set up an R data package in a consistent structure. Automates tasks like tidy data export, data dictionary documentation, README and website creation, and citation management.  "
  },
  {
    "id": 23007,
    "package_name": "waves",
    "title": "Vis-NIR Spectral Analysis Wrapper",
    "description": "Originally designed application in the context of\n    resource-limited plant research and breeding programs, 'waves'\n    provides an open-source solution to spectral data processing and model\n    development by bringing useful packages together into a streamlined\n    pipeline.  This package is wrapper for functions related to the\n    analysis of point visible and near-infrared reflectance measurements.\n    It includes visualization, filtering, aggregation, preprocessing,\n    cross-validation set formation, model training, and prediction\n    functions to enable open-source association of spectral and reference\n    data. This package is documented in a peer-reviewed manuscript in the\n    Plant Phenome Journal <doi:10.1002/ppj2.20012>.  Specialized\n    cross-validation schemes are described in detail in Jarqu\u00edn et al.\n    (2017) <doi:10.3835/plantgenome2016.12.0130>. Example data is from\n    Ikeogu et al. (2017) <doi:10.1371/journal.pone.0188918>.",
    "version": "0.2.6",
    "maintainer": "Jenna Hershberger <jmh579@cornell.edu>",
    "author": "Jenna Hershberger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3147-6867>),\n  Michael Gore [ths],\n  NSF BREAD IOS-1543958 [fnd]",
    "url": "https://GoreLab.github.io/waves/, https://github.com/GoreLab/waves",
    "bug_reports": "https://github.com/GoreLab/waves/issues",
    "repository": "https://cran.r-project.org/package=waves",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "waves Vis-NIR Spectral Analysis Wrapper Originally designed application in the context of\n    resource-limited plant research and breeding programs, 'waves'\n    provides an open-source solution to spectral data processing and model\n    development by bringing useful packages together into a streamlined\n    pipeline.  This package is wrapper for functions related to the\n    analysis of point visible and near-infrared reflectance measurements.\n    It includes visualization, filtering, aggregation, preprocessing,\n    cross-validation set formation, model training, and prediction\n    functions to enable open-source association of spectral and reference\n    data. This package is documented in a peer-reviewed manuscript in the\n    Plant Phenome Journal <doi:10.1002/ppj2.20012>.  Specialized\n    cross-validation schemes are described in detail in Jarqu\u00edn et al.\n    (2017) <doi:10.3835/plantgenome2016.12.0130>. Example data is from\n    Ikeogu et al. (2017) <doi:10.1371/journal.pone.0188918>.  "
  },
  {
    "id": 23017,
    "package_name": "wbwdi",
    "title": "Seamless Access to World Bank World Development Indicators (WDI)",
    "description": "Access and analyze the World Bank's World Development Indicators \n  (WDI) using the corresponding API <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation>. \n  WDI provides more than 24,000 country or region-level indicators for various\n  contexts. 'wbwdi' enables users to download, process and work with WDI\n  series across multiple countries, aggregates, and time periods. ",
    "version": "1.0.3",
    "maintainer": "Christoph Scheuch <christoph@tidy-intelligence.com>",
    "author": "Christoph Scheuch [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>)",
    "url": "https://github.com/tidy-intelligence/r-wbwdi,\nhttps://tidy-intelligence.github.io/r-wbwdi/",
    "bug_reports": "https://github.com/tidy-intelligence/r-wbwdi/issues",
    "repository": "https://cran.r-project.org/package=wbwdi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wbwdi Seamless Access to World Bank World Development Indicators (WDI) Access and analyze the World Bank's World Development Indicators \n  (WDI) using the corresponding API <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation>. \n  WDI provides more than 24,000 country or region-level indicators for various\n  contexts. 'wbwdi' enables users to download, process and work with WDI\n  series across multiple countries, aggregates, and time periods.   "
  },
  {
    "id": 23036,
    "package_name": "webexercises",
    "title": "Create Interactive Web Exercises in 'R Markdown' (Formerly\n'webex')",
    "description": "Functions for easily creating interactive web pages using\n    'R Markdown' that students can use in self-guided learning.",
    "version": "1.1.0",
    "maintainer": "Lisa DeBruine <debruine@gmail.com>",
    "author": "Dale Barr [aut],\n  Lisa DeBruine [aut, cre]",
    "url": "https://github.com/psyteachr/webexercises",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=webexercises",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "webexercises Create Interactive Web Exercises in 'R Markdown' (Formerly\n'webex') Functions for easily creating interactive web pages using\n    'R Markdown' that students can use in self-guided learning.  "
  },
  {
    "id": 23043,
    "package_name": "webshot",
    "title": "Take Screenshots of Web Pages",
    "description": "Takes screenshots of web pages, including Shiny applications and R\n    Markdown documents.",
    "version": "0.5.5",
    "maintainer": "Winston Chang <winston@rstudio.com>",
    "author": "Winston Chang [aut, cre],\n  Yihui Xie [ctb],\n  Francois Guillem [ctb],\n  Barret Schloerke [ctb],\n  Nicolas Perriault [ctb] (The CasperJS library)",
    "url": "https://wch.github.io/webshot/, https://github.com/wch/webshot/",
    "bug_reports": "https://github.com/wch/webshot/issues",
    "repository": "https://cran.r-project.org/package=webshot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "webshot Take Screenshots of Web Pages Takes screenshots of web pages, including Shiny applications and R\n    Markdown documents.  "
  },
  {
    "id": 23073,
    "package_name": "whapi",
    "title": "R Client for 'whapi.cloud'",
    "description": "Provides an 'R' interface to the 'Whapi' 'API' <https://whapi.cloud>, enabling sending and receiving 'WhatsApp' messages directly from 'R'. Functions include sending text, images, documents, stickers, geographic locations, and interactive messages (buttons and lists). Also includes 'webhook' parsing utilities and channel health checks.",
    "version": "0.0.2",
    "maintainer": "Andre Leite <leite@castlab.org>",
    "author": "Andre Leite [aut, cre],\n  Hugo Vaconcelos [aut],\n  Diogo Bezerra [aut]",
    "url": "https://github.com/StrategicProjects/whapi/",
    "bug_reports": "https://github.com/StrategicProjects/whapi/issues/",
    "repository": "https://cran.r-project.org/package=whapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "whapi R Client for 'whapi.cloud' Provides an 'R' interface to the 'Whapi' 'API' <https://whapi.cloud>, enabling sending and receiving 'WhatsApp' messages directly from 'R'. Functions include sending text, images, documents, stickers, geographic locations, and interactive messages (buttons and lists). Also includes 'webhook' parsing utilities and channel health checks.  "
  },
  {
    "id": 23080,
    "package_name": "whereami",
    "title": "Reliably Return the Source and Call Location of a Command",
    "description": "Robust and reliable functions to return informative outputs to \n        console with the run or source location of a command. This can be from\n        the  'RScript'/R terminal commands or 'RStudio' console, source editor, \n        'Rmarkdown' document and a Shiny application.",
    "version": "0.2.0",
    "maintainer": "Jonathan Sidi <yonicd@gmail.com>",
    "author": "Jonathan Sidi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4222-1819>),\n  Colin Fay [ctb] (ORCID: <https://orcid.org/0000-0001-7343-1846>),\n  Kirill M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-1416-3412>)",
    "url": "https://github.com/yonicd/whereami",
    "bug_reports": "https://github.com/yonicd/whereami/issues",
    "repository": "https://cran.r-project.org/package=whereami",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "whereami Reliably Return the Source and Call Location of a Command Robust and reliable functions to return informative outputs to \n        console with the run or source location of a command. This can be from\n        the  'RScript'/R terminal commands or 'RStudio' console, source editor, \n        'Rmarkdown' document and a Shiny application.  "
  },
  {
    "id": 23083,
    "package_name": "whirl",
    "title": "Log Execution of Scripts",
    "description": "Logging of scripts suitable for clinical trials using\n    'Quarto' to create nice human readable logs. 'whirl' enables\n    execution of scripts in batch, while simultaneously creating logs for\n    the execution of each script, and providing an overview summary log of\n    the entire batch execution.",
    "version": "0.3.1",
    "maintainer": "Aksel Thomsen <oath@novonordisk.com>",
    "author": "Aksel Thomsen [aut, cre],\n  Lovemore Gakava [aut],\n  Cervan Girard [aut],\n  Kristian Troejelsgaard [aut],\n  Steffen Falgreen Larsen [aut],\n  Vladimir Obucina [aut],\n  Michael Svingel [aut],\n  Skander Mulder [aut],\n  Oliver Lundsgaard [aut],\n  Novo Nordisk A/S [cph]",
    "url": "https://novonordisk-opensource.github.io/whirl/,\nhttps://github.com/novonordisk-opensource/whirl",
    "bug_reports": "https://github.com/NovoNordisk-OpenSource/whirl/issues",
    "repository": "https://cran.r-project.org/package=whirl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "whirl Log Execution of Scripts Logging of scripts suitable for clinical trials using\n    'Quarto' to create nice human readable logs. 'whirl' enables\n    execution of scripts in batch, while simultaneously creating logs for\n    the execution of each script, and providing an overview summary log of\n    the entire batch execution.  "
  },
  {
    "id": 23133,
    "package_name": "woodValuationDE",
    "title": "Wood Valuation Germany",
    "description": "Monetary valuation of wood in German forests\n             (stumpage values), including estimations of harvest quantities, \n             wood revenues, and harvest costs. The functions are sensitive to\n             tree species, mean diameter of the harvested trees, stand quality,\n             and logging method. The functions include estimations for the\n             consequences of disturbances on revenues and costs. The underlying\n             assortment tables are taken from Offer and Staupendahl (2018) with\n             corresponding functions for salable and skidded volume derived in\n             Fuchs et al. (2023). Wood revenue and harvest cost\n             functions were taken from v. Bodelschwingh (2018). The consequences\n             of disturbances refer to Dieter (2001), Moellmann and Moehring\n             (2017), and Fuchs et al. (2022a, 2022b). For the full references \n             see documentation of the functions, package README, and Fuchs et\n             al. (2023). Apart from Dieter (2001) and Moellmann and\n             Moehring (2017), all functions and factors are based on data from\n             HessenForst, the forest administration of the Federal State of\n             Hesse in Germany.",
    "version": "1.0.2",
    "maintainer": "Jasper M. Fuchs <jasper.fuchs@usys.ethz.ch>",
    "author": "Jasper M. Fuchs [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5951-7897>),\n  Kai Husmann [aut] (ORCID: <https://orcid.org/0000-0003-2970-4709>),\n  Hilmar v. Bodelschwingh [aut],\n  Roman Koster [aut] (ORCID: <https://orcid.org/0000-0002-0439-2086>),\n  Kai Staupendahl [aut] (ORCID: <https://orcid.org/0000-0001-6595-3601>),\n  Armin Offer [aut],\n  Bernhard M\u00f6hring [aut],\n  Carola Paul [aut] (ORCID: <https://orcid.org/0000-0002-6257-2026>),\n  University Goettingen - Department of Forest Economics and Sustainable\n    Land-use Planning [fnd],\n  Federal Ministry of Education and Research Germany (BMBF) [fnd],\n  BiodivClim ERA-Net COFUND BiodivERsA call [fnd],\n  ETH Zurich - Institute of Terrestrial Ecosystems - Forest Resources\n    Management [fnd]",
    "url": "https://github.com/Forest-Economics-Goettingen/woodValuationDE",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=woodValuationDE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "woodValuationDE Wood Valuation Germany Monetary valuation of wood in German forests\n             (stumpage values), including estimations of harvest quantities, \n             wood revenues, and harvest costs. The functions are sensitive to\n             tree species, mean diameter of the harvested trees, stand quality,\n             and logging method. The functions include estimations for the\n             consequences of disturbances on revenues and costs. The underlying\n             assortment tables are taken from Offer and Staupendahl (2018) with\n             corresponding functions for salable and skidded volume derived in\n             Fuchs et al. (2023). Wood revenue and harvest cost\n             functions were taken from v. Bodelschwingh (2018). The consequences\n             of disturbances refer to Dieter (2001), Moellmann and Moehring\n             (2017), and Fuchs et al. (2022a, 2022b). For the full references \n             see documentation of the functions, package README, and Fuchs et\n             al. (2023). Apart from Dieter (2001) and Moellmann and\n             Moehring (2017), all functions and factors are based on data from\n             HessenForst, the forest administration of the Federal State of\n             Hesse in Germany.  "
  },
  {
    "id": 23134,
    "package_name": "wooldridge",
    "title": "115 Data Sets from \"Introductory Econometrics: A Modern\nApproach, 7e\" by Jeffrey M. Wooldridge",
    "description": "Students learning both econometrics and R may find the introduction \n    to both challenging. The wooldridge data package aims to lighten the task by efficiently \n    loading any data set found in the text with a single command. Data sets have been \n    compressed to a fraction of their original size. Documentation files contain page numbers, \n    the original source, time of publication, and notes from the author suggesting avenues for \n    further analysis and research. If one needs an introduction to R model syntax, a \n    vignette contains solutions to examples from chapters of the text. \n    Data sets are from the 7th edition (Wooldridge 2020, ISBN-13 978-1-337-55886-0), \n    and are backwards compatible with all previous versions of the text.",
    "version": "1.4-4",
    "maintainer": "Justin M. Shea <jshea@roosevelt.edu>",
    "author": "Justin M. Shea [aut, cre],\n  Kennth H. Brown [ctb]",
    "url": "https://justinmshea.github.io/wooldridge/",
    "bug_reports": "https://github.com/JustinMShea/wooldridge/issues/",
    "repository": "https://cran.r-project.org/package=wooldridge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wooldridge 115 Data Sets from \"Introductory Econometrics: A Modern\nApproach, 7e\" by Jeffrey M. Wooldridge Students learning both econometrics and R may find the introduction \n    to both challenging. The wooldridge data package aims to lighten the task by efficiently \n    loading any data set found in the text with a single command. Data sets have been \n    compressed to a fraction of their original size. Documentation files contain page numbers, \n    the original source, time of publication, and notes from the author suggesting avenues for \n    further analysis and research. If one needs an introduction to R model syntax, a \n    vignette contains solutions to examples from chapters of the text. \n    Data sets are from the 7th edition (Wooldridge 2020, ISBN-13 978-1-337-55886-0), \n    and are backwards compatible with all previous versions of the text.  "
  },
  {
    "id": 23139,
    "package_name": "wordcloud",
    "title": "Word Clouds",
    "description": "Functionality to create pretty word clouds, visualize differences and similarity between documents, and avoid over-plotting in scatter plots with text.",
    "version": "2.6",
    "maintainer": "Ian Fellows <ian@fellstat.com>",
    "author": "Ian Fellows",
    "url": "http://blog.fellstat.com/?cat=11 http://www.fellstat.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wordcloud",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordcloud Word Clouds Functionality to create pretty word clouds, visualize differences and similarity between documents, and avoid over-plotting in scatter plots with text.  "
  },
  {
    "id": 23142,
    "package_name": "wordmap",
    "title": "Feature Extraction and Document Classification with Noisy Labels",
    "description": "Extract features and classify documents with noisy labels given by document-meta data or keyword matching Watanabe & Zhou (2020) <doi:10.1177/0894439320907027>.",
    "version": "0.9.5",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph]",
    "url": "https://github.com/koheiw/wordmap",
    "bug_reports": "https://github.com/koheiw/wordmap/issues",
    "repository": "https://cran.r-project.org/package=wordmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordmap Feature Extraction and Document Classification with Noisy Labels Extract features and classify documents with noisy labels given by document-meta data or keyword matching Watanabe & Zhou (2020) <doi:10.1177/0894439320907027>.  "
  },
  {
    "id": 23149,
    "package_name": "wordvector",
    "title": "Word and Document Vector Models",
    "description": "Create dense vector representation of words and documents using 'quanteda'. Currently implements Word2vec (Mikolov et al., 2013) <doi:10.48550/arXiv.1310.4546> and Latent Semantic Analysis (Deerwester et al., 1990) <doi:10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9>.",
    "version": "0.6.0",
    "maintainer": "Kohei Watanabe <watanabe.kohei@gmail.com>",
    "author": "Kohei Watanabe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6519-5265>),\n  Jan Wijffels [aut] (Original R code),\n  BNOSAC [cph] (Original R code),\n  Max Fomichev [ctb, cph] (Original C++ code)",
    "url": "https://github.com/koheiw/wordvector",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wordvector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordvector Word and Document Vector Models Create dense vector representation of words and documents using 'quanteda'. Currently implements Word2vec (Mikolov et al., 2013) <doi:10.48550/arXiv.1310.4546> and Latent Semantic Analysis (Deerwester et al., 1990) <doi:10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9>.  "
  },
  {
    "id": 23150,
    "package_name": "workflowr",
    "title": "A Framework for Reproducible and Collaborative Data Science",
    "description": "Provides a workflow for your analysis projects by combining\n  literate programming ('knitr' and 'rmarkdown') and version control\n  ('Git', via 'git2r') to generate a website containing time-stamped,\n  versioned, and documented results.",
    "version": "1.7.2",
    "maintainer": "John Blischak <jdblischak@gmail.com>",
    "author": "John Blischak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2634-9879>),\n  Peter Carbonetto [aut] (ORCID: <https://orcid.org/0000-0003-1144-6780>),\n  Matthew Stephens [aut] (ORCID: <https://orcid.org/0000-0001-5397-9257>),\n  Luke Zappia [ctb] (Instructions for hosting with GitLab),\n  Pierre Formont [ctb] (Support for hosting with Shiny Server),\n  Tim Trice [ctb] (Instructions for sharing common code),\n  Jiaxiang Li [ctb] (Function wflow_toc() to create table of contents),\n  Michael J. Kane [ctb] (ORCID: <https://orcid.org/0000-0003-1899-6662>,\n    Option suppress_report),\n  Anh Tran [ctb] (Updated RStudio Project Template),\n  Sydney Purdue [ctb] (Improved wflow_start() error handling),\n  Giorgio Comai [ctb] (Added argument only_published to wflow_toc()),\n  Zaynaib Giwa [ctb] (Multiple enhancements),\n  Xiongbing Jin [ctb] (Fixed bug in versions table for figures),\n  Yihui Xie [ctb] (ORCID: <https://orcid.org/0000-0003-0645-5666>, Design\n    advice; maintenance)",
    "url": "https://workflowr.github.io/workflowr/,\nhttps://github.com/workflowr/workflowr",
    "bug_reports": "https://github.com/workflowr/workflowr/issues",
    "repository": "https://cran.r-project.org/package=workflowr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "workflowr A Framework for Reproducible and Collaborative Data Science Provides a workflow for your analysis projects by combining\n  literate programming ('knitr' and 'rmarkdown') and version control\n  ('Git', via 'git2r') to generate a website containing time-stamped,\n  versioned, and documented results.  "
  },
  {
    "id": 23152,
    "package_name": "worldbank",
    "title": "Client for World Banks's 'Indicators' and 'Poverty and\nInequality Platform (PIP)' APIs",
    "description": "Download and search data from the 'World Bank Indicators\n    API', which provides access to nearly 16,000 time series indicators.\n    See\n    <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation>\n    for further details about the API.",
    "version": "0.7.1",
    "maintainer": "Maximilian M\u00fccke <muecke.maximilian@gmail.com>",
    "author": "Maximilian M\u00fccke [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-9432-9795>)",
    "url": "https://m-muecke.github.io/worldbank/,\nhttps://github.com/m-muecke/worldbank",
    "bug_reports": "https://github.com/m-muecke/worldbank/issues",
    "repository": "https://cran.r-project.org/package=worldbank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "worldbank Client for World Banks's 'Indicators' and 'Poverty and\nInequality Platform (PIP)' APIs Download and search data from the 'World Bank Indicators\n    API', which provides access to nearly 16,000 time series indicators.\n    See\n    <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation>\n    for further details about the API.  "
  },
  {
    "id": 23159,
    "package_name": "wpa",
    "title": "Tools for Analysing and Visualising Viva Insights Data",
    "description": "Opinionated functions that enable easier and faster\n    analysis of Viva Insights data. There are three main types of functions in 'wpa':\n    (i) Standard functions create a 'ggplot' visual or a summary table based on a specific\n    Viva Insights metric; (2) Report Generation functions generate HTML reports on\n    a specific analysis area, e.g. Collaboration; (3) Other miscellaneous functions cover\n    more specific applications (e.g. Subject Line text mining) of Viva Insights data.\n    This package adheres to 'tidyverse' principles and works well with the pipe syntax.\n    'wpa' is built with the beginner-to-intermediate R users in mind, and is optimised for\n    simplicity. ",
    "version": "1.10.0",
    "maintainer": "Martin Chan <martin.chan@microsoft.com>",
    "author": "Martin Chan [aut, cre],\n  Carlos Morales [aut],\n  Mark Powers [ctb],\n  Ainize Cidoncha [ctb],\n  Rosamary Ochoa Vargas [ctb],\n  Tannaz Sattari [ctb],\n  Lucas Hogner [ctb],\n  Jasminder Thind [ctb],\n  Simone Liebal [ctb],\n  Aleksey Ashikhmin [ctb],\n  Ellen Trinklein [ctb],\n  Microsoft Corporation [cph]",
    "url": "https://github.com/microsoft/wpa/,\nhttps://microsoft.github.io/wpa/",
    "bug_reports": "https://github.com/microsoft/wpa/issues/",
    "repository": "https://cran.r-project.org/package=wpa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wpa Tools for Analysing and Visualising Viva Insights Data Opinionated functions that enable easier and faster\n    analysis of Viva Insights data. There are three main types of functions in 'wpa':\n    (i) Standard functions create a 'ggplot' visual or a summary table based on a specific\n    Viva Insights metric; (2) Report Generation functions generate HTML reports on\n    a specific analysis area, e.g. Collaboration; (3) Other miscellaneous functions cover\n    more specific applications (e.g. Subject Line text mining) of Viva Insights data.\n    This package adheres to 'tidyverse' principles and works well with the pipe syntax.\n    'wpa' is built with the beginner-to-intermediate R users in mind, and is optimised for\n    simplicity.   "
  },
  {
    "id": 23196,
    "package_name": "wyz.code.rdoc",
    "title": "Wizardry Code Offensive Programming R Documentation",
    "description": "Allows to generate on-demand or by batch, any R documentation file,\n    whatever is kind, data, function, class or package. It populates\n    documentation sections, either automatically or by considering\n    your input. Input code could be standard R code or offensive programming code. \n    Documentation content completeness depends on the type of code you use. With\n    offensive programming code, expect generated documentation to be fully \n    completed, from a format and content point of view. With some standard R \n    code, you will have to activate post processing to fill-in any section that \n    requires complements. Produced manual page validity is automatically tested \n    against R documentation compliance rules. Documentation language \n    proficiency, wording style, and phrasal adjustments remains your job. ",
    "version": "1.1.19",
    "maintainer": "Fabien Gelineau <neonira@gmail.com>",
    "author": "Fabien Gelineau <neonira@gmail.com>",
    "url": "https://neonira.github.io/offensiveProgrammingBook_v1.2.2/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wyz.code.rdoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wyz.code.rdoc Wizardry Code Offensive Programming R Documentation Allows to generate on-demand or by batch, any R documentation file,\n    whatever is kind, data, function, class or package. It populates\n    documentation sections, either automatically or by considering\n    your input. Input code could be standard R code or offensive programming code. \n    Documentation content completeness depends on the type of code you use. With\n    offensive programming code, expect generated documentation to be fully \n    completed, from a format and content point of view. With some standard R \n    code, you will have to activate post processing to fill-in any section that \n    requires complements. Produced manual page validity is automatically tested \n    against R documentation compliance rules. Documentation language \n    proficiency, wording style, and phrasal adjustments remains your job.   "
  },
  {
    "id": 23207,
    "package_name": "xaringan",
    "title": "Presentation Ninja",
    "description": "Create HTML5 slides with R Markdown and the JavaScript library\n    'remark.js' (<https://remarkjs.com>).",
    "version": "0.31",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>,\n    URL: https://yihui.org),\n  Posit Software, PBC [cph, fnd],\n  Alessandro Gasparini [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8319-7624>),\n  Benjie Gillam [ctb],\n  Claus Thorn Ekstr\u00f8m [ctb],\n  Daniel Anderson [ctb],\n  Dawei Lang [ctb],\n  Deo Salil [ctb],\n  Emi Tanaka [ctb],\n  Garrick Aden-Buie [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>),\n  I\u00f1aki Ucar [ctb] (ORCID: <https://orcid.org/0000-0001-6403-5550>),\n  John Little [ctb],\n  Joselyn Ch\u00e1vez [ctb] (ORCID: <https://orcid.org/0000-0002-4974-4591>),\n  Joseph Casillas [ctb],\n  JooYoung Seo [ctb] (ORCID: <https://orcid.org/0000-0002-4064-6012>),\n  Lucy D'Agostino McGowan [ctb] (ORCID:\n    <https://orcid.org/0000-0001-7297-9359>),\n  Malcolm Barrett [ctb] (ORCID: <https://orcid.org/0000-0003-0299-5825>),\n  Matthew Mark Strasiotto [ctb] (github: mstr3336),\n  Michael Wayne Kearney [ctb],\n  Nan-Hung Hsieh [ctb],\n  Ole Petter Bang [ctb] (CSS in\n    rmarkdown/templates/xaringan/resources/default.css),\n  Orlando Olaya Bucaro [ctb],\n  Patrick Schratz [ctb],\n  Paul Klemm [ctb] (ORCID: <https://orcid.org/0000-0002-5985-1737>),\n  Paul Lemmens [ctb],\n  Robert Fromont [ctb] (ORCID: <https://orcid.org/0000-0001-5271-5487>),\n  Sean Lopp [ctb],\n  Silvia Canelon [ctb] (ORCID: <https://orcid.org/0000-0003-1709-1394>),\n  Susan VanderPlas [ctb] (ORCID: <https://orcid.org/0000-0002-3803-0972>),\n  Tuo Wang [ctb],\n  Waldir Leoncio [ctb],\n  Yongfu Liao [ctb],\n  Yue Jiang [ctb] (ORCID: <https://orcid.org/0000-0002-9798-5517>),\n  Zhian N. Kamvar [ctb] (ORCID: <https://orcid.org/0000-0003-1458-7108>)",
    "url": "https://github.com/yihui/xaringan",
    "bug_reports": "https://github.com/yihui/xaringan/issues",
    "repository": "https://cran.r-project.org/package=xaringan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xaringan Presentation Ninja Create HTML5 slides with R Markdown and the JavaScript library\n    'remark.js' (<https://remarkjs.com>).  "
  },
  {
    "id": 23209,
    "package_name": "xaringanthemer",
    "title": "Custom 'xaringan' CSS Themes",
    "description": "Create beautifully color-coordinated and customized themes\n    for your 'xaringan' slides, without writing any CSS. Complete your\n    slide theme with 'ggplot2' themes that match the font and colors used\n    in your slides.  Customized styles can be created directly in your\n    slides' 'R Markdown' source file or in a separate external script.",
    "version": "0.4.4",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>)",
    "url": "https://pkg.garrickadenbuie.com/xaringanthemer/,\nhttps://github.com/gadenbuie/xaringanthemer",
    "bug_reports": "https://github.com/gadenbuie/xaringanthemer/issues",
    "repository": "https://cran.r-project.org/package=xaringanthemer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xaringanthemer Custom 'xaringan' CSS Themes Create beautifully color-coordinated and customized themes\n    for your 'xaringan' slides, without writing any CSS. Complete your\n    slide theme with 'ggplot2' themes that match the font and colors used\n    in your slides.  Customized styles can be created directly in your\n    slides' 'R Markdown' source file or in a separate external script.  "
  },
  {
    "id": 23211,
    "package_name": "xdvir",
    "title": "Render 'LaTeX' in Plots",
    "description": "High-level functions to render 'LaTeX' fragments in plots, \n             including as labels and data symbols in 'ggplot2' plots, plus\n             low-level functions to \n             author 'LaTeX' fragments (to produce 'LaTeX' documents), \n             typeset 'LaTeX' documents (to produce 'DVI' files), \n             read 'DVI' files (to produce \"DVI\" objects), \n             and render \"DVI\" objects.",
    "version": "0.1-3",
    "maintainer": "Paul Murrell <paul@stat.auckland.ac.nz>",
    "author": "Paul Murrell [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3224-8858>)",
    "url": "https://stattech.wordpress.fos.auckland.ac.nz/2025/03/06/2025-01-latex-typesetting-in-r/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xdvir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xdvir Render 'LaTeX' in Plots High-level functions to render 'LaTeX' fragments in plots, \n             including as labels and data symbols in 'ggplot2' plots, plus\n             low-level functions to \n             author 'LaTeX' fragments (to produce 'LaTeX' documents), \n             typeset 'LaTeX' documents (to produce 'DVI' files), \n             read 'DVI' files (to produce \"DVI\" objects), \n             and render \"DVI\" objects.  "
  },
  {
    "id": 23214,
    "package_name": "xegaBNF",
    "title": "Compile a Backus-Naur Form Specification into an R Grammar\nObject",
    "description": "Translates a BNF (Backus-Naur Form) specification of a \n             context-free language into an R grammar object\n             which consists of the start symbol, the symbol table, \n             the production table, and a short production table.\n             The short production table is non-recursive. \n             The grammar object contains the file name from \n             which it was generated (without a path).\n             In addition, it provides functions to determine the type \n             of a symbol (isTerminal() and isNonterminal()) and functions\n             to access the production table (rules() and derives()).\n             For the BNF specification, see Backus, John et al. (1962)\n             \"Revised Report on the Algorithmic Language ALGOL 60\".\n             (ALGOL60 standards page <http://www.algol60.org/2standards.htm>,\n              html-edition <https://www.masswerk.at/algol60/report.htm>)\n             A preprocessor for macros which expand to standard BNF \n             is included. \n             The grammar compiler is an extension of the APL2 implementation in \n             Geyer-Schulz, Andreas (1997, ISBN:978-3-7908-0830-X).",
    "version": "1.0.0.5",
    "maintainer": "Andreas Geyer-Schulz <Andreas.Geyer-Schulz@kit.edu>",
    "author": "Andreas Geyer-Schulz [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-5237-3579>)",
    "url": "https://github.com/ageyerschulz/xegaBNF",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xegaBNF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xegaBNF Compile a Backus-Naur Form Specification into an R Grammar\nObject Translates a BNF (Backus-Naur Form) specification of a \n             context-free language into an R grammar object\n             which consists of the start symbol, the symbol table, \n             the production table, and a short production table.\n             The short production table is non-recursive. \n             The grammar object contains the file name from \n             which it was generated (without a path).\n             In addition, it provides functions to determine the type \n             of a symbol (isTerminal() and isNonterminal()) and functions\n             to access the production table (rules() and derives()).\n             For the BNF specification, see Backus, John et al. (1962)\n             \"Revised Report on the Algorithmic Language ALGOL 60\".\n             (ALGOL60 standards page <http://www.algol60.org/2standards.htm>,\n              html-edition <https://www.masswerk.at/algol60/report.htm>)\n             A preprocessor for macros which expand to standard BNF \n             is included. \n             The grammar compiler is an extension of the APL2 implementation in \n             Geyer-Schulz, Andreas (1997, ISBN:978-3-7908-0830-X).  "
  },
  {
    "id": 23234,
    "package_name": "xlsform2word",
    "title": "Convert 'XLSForm' to Structured 'Word' Document",
    "description": "Converts an 'XLSForm' (survey in 'Excel') into a well-structured 'Word' document, including sections, skip logic, options, and question labels.\n    Designed to support survey documentation, training materials, and data collection workflows.\n    The package was developed based on field experience with 'XLSForm' and humanitarian operations, aiming to streamline documentation and enhance training efficiency.",
    "version": "0.1.0",
    "maintainer": "NyAvo RATOVO-ANDRIANARISOA <ratovoandry1@gmail.com>",
    "author": "NyAvo RATOVO-ANDRIANARISOA [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xlsform2word",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xlsform2word Convert 'XLSForm' to Structured 'Word' Document Converts an 'XLSForm' (survey in 'Excel') into a well-structured 'Word' document, including sections, skip logic, options, and question labels.\n    Designed to support survey documentation, training materials, and data collection workflows.\n    The package was developed based on field experience with 'XLSForm' and humanitarian operations, aiming to streamline documentation and enhance training efficiency.  "
  },
  {
    "id": 23238,
    "package_name": "xmap",
    "title": "Transforming Data Between Statistical Classifications",
    "description": "Provides support for transformations of numeric aggregates\n   between statistical classifications (e.g. occupation or industry categorisations) using the 'Crossmaps' framework. \n   Implements classes for representing transformations between a source and target classification\n   as graph structures, and methods for validating and applying crossmaps to transform\n   data collected under the source classification into data indexed using the target classification codes.\n   Documentation about the 'Crossmaps' framework is provided in the included vignettes\n   and in Huang (2024, <doi:10.48550/arXiv.2406.14163>).",
    "version": "0.1.0",
    "maintainer": "Cynthia A. Huang <cynthiahqy@gmail.com>",
    "author": "Cynthia A. Huang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9218-987X>),\n  Laura Puzzello [aut, fnd]",
    "url": "https://github.com/cynthiahqy/xmap,\nhttps://cynthiahqy.github.io/xmap/",
    "bug_reports": "https://github.com/cynthiahqy/xmap/issues",
    "repository": "https://cran.r-project.org/package=xmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xmap Transforming Data Between Statistical Classifications Provides support for transformations of numeric aggregates\n   between statistical classifications (e.g. occupation or industry categorisations) using the 'Crossmaps' framework. \n   Implements classes for representing transformations between a source and target classification\n   as graph structures, and methods for validating and applying crossmaps to transform\n   data collected under the source classification into data indexed using the target classification codes.\n   Documentation about the 'Crossmaps' framework is provided in the included vignettes\n   and in Huang (2024, <doi:10.48550/arXiv.2406.14163>).  "
  },
  {
    "id": 23240,
    "package_name": "xml2relational",
    "title": "Converting XML Documents into Relational Data Models",
    "description": "Import an XML document with nested object structures and convert\n    it into a relational data model. The result is a set of R dataframes \n    with foreign key relationships. The data model and the data can be exported as\n    SQL code of different SQL flavors.",
    "version": "0.1.1",
    "maintainer": "Joachim Zuckarelli <joachim@zuckarelli.de>",
    "author": "Joachim Zuckarelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9280-3016>)",
    "url": "https://github.com/jsugarelli/xml2relational/",
    "bug_reports": "https://github.com/jsugarelli/xml2relational/issues",
    "repository": "https://cran.r-project.org/package=xml2relational",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xml2relational Converting XML Documents into Relational Data Models Import an XML document with nested object structures and convert\n    it into a relational data model. The result is a set of R dataframes \n    with foreign key relationships. The data model and the data can be exported as\n    SQL code of different SQL flavors.  "
  },
  {
    "id": 23241,
    "package_name": "xmlconvert",
    "title": "Comfortably Converting XML Documents to Dataframes and Vice\nVersa",
    "description": "Converts XML documents to R dataframes and dataframes to XML documents.\n  A wide variety of options allows for different XML formats and flexible control\n  of the conversion process. Results can be exported to CSV and Excel, if desired. \n  Also converts XML data to R lists.",
    "version": "0.1.2",
    "maintainer": "Joachim Zuckarelli <joachim@zuckarelli.de>",
    "author": "Joachim Zuckarelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9280-3016>)",
    "url": "https://github.com/jsugarelli/xmlconvert/",
    "bug_reports": "https://github.com/jsugarelli/xmlconvert/issues",
    "repository": "https://cran.r-project.org/package=xmlconvert",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xmlconvert Comfortably Converting XML Documents to Dataframes and Vice\nVersa Converts XML documents to R dataframes and dataframes to XML documents.\n  A wide variety of options allows for different XML formats and flexible control\n  of the conversion process. Results can be exported to CSV and Excel, if desired. \n  Also converts XML data to R lists.  "
  },
  {
    "id": 23244,
    "package_name": "xmlwriter",
    "title": "Fast and Elegant XML Generation",
    "description": "Provides a fast and elegant interface for generating XML\n fragments and documents. It can be used in companion with R packages 'XML'\n or 'xml2' to generate XML documents. The fast XML generation is implemented using the\n 'Rcpp' package. ",
    "version": "0.1.1",
    "maintainer": "Edwin de Jonge <edwindjonge@gmail.com>",
    "author": "Edwin de Jonge [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6580-4718>)",
    "url": "https://edwindj.github.io/xmlwriter/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xmlwriter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xmlwriter Fast and Elegant XML Generation Provides a fast and elegant interface for generating XML\n fragments and documents. It can be used in companion with R packages 'XML'\n or 'xml2' to generate XML documents. The fast XML generation is implemented using the\n 'Rcpp' package.   "
  },
  {
    "id": 23245,
    "package_name": "xmpdf",
    "title": "Edit 'XMP' Metadata and 'PDF' Bookmarks and Documentation Info",
    "description": "Edit 'XMP' metadata <https://en.wikipedia.org/wiki/Extensible_Metadata_Platform> \n    in a variety of media file formats as well as \n    edit bookmarks (aka outline aka table of contents) and documentation info entries in 'pdf' files.\n    Can detect and use a variety of command-line tools to perform these operations such as\n    'exiftool' <https://exiftool.org/>, 'ghostscript' <https://www.ghostscript.com/>, \n    and/or 'pdftk' <https://gitlab.com/pdftk-java/pdftk>.",
    "version": "0.2.1",
    "maintainer": "Trevor L Davis <trevor.l.davis@gmail.com>",
    "author": "Trevor L Davis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6341-4639>),\n  Linux Foundation [dtc] (Uses some data from the \"SPDX License List\"\n    <https://github.com/spdx/license-list-XML>)",
    "url": "https://trevorldavis.com/R/xmpdf/dev/",
    "bug_reports": "https://github.com/trevorld/r-xmpdf/issues",
    "repository": "https://cran.r-project.org/package=xmpdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xmpdf Edit 'XMP' Metadata and 'PDF' Bookmarks and Documentation Info Edit 'XMP' metadata <https://en.wikipedia.org/wiki/Extensible_Metadata_Platform> \n    in a variety of media file formats as well as \n    edit bookmarks (aka outline aka table of contents) and documentation info entries in 'pdf' files.\n    Can detect and use a variety of command-line tools to perform these operations such as\n    'exiftool' <https://exiftool.org/>, 'ghostscript' <https://www.ghostscript.com/>, \n    and/or 'pdftk' <https://gitlab.com/pdftk-java/pdftk>.  "
  },
  {
    "id": 23268,
    "package_name": "xxIRT",
    "title": "Item Response Theory and Computer-Based Testing in R",
    "description": "A suite of psychometric analysis tools for research and operation, including:\n    (1) computation of probability, information, and likelihood for the 3PL, GPCM, and GRM;\n    (2) parameter estimation using joint or marginal likelihood estimation method;\n    (3) simulation of computerized adaptive testing using built-in or customized algorithms;\n    (4) assembly and simulation of multistage testing. \n    The full documentation and tutorials are at <https://github.com/xluo11/xxIRT>.",
    "version": "2.1.2",
    "maintainer": "Xiao Luo <xluo1986@gmail.com>",
    "author": "Xiao Luo [aut, cre]",
    "url": "https://github.com/xluo11/xxIRT",
    "bug_reports": "https://github.com/xluo11/xxIRT/issues",
    "repository": "https://cran.r-project.org/package=xxIRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xxIRT Item Response Theory and Computer-Based Testing in R A suite of psychometric analysis tools for research and operation, including:\n    (1) computation of probability, information, and likelihood for the 3PL, GPCM, and GRM;\n    (2) parameter estimation using joint or marginal likelihood estimation method;\n    (3) simulation of computerized adaptive testing using built-in or customized algorithms;\n    (4) assembly and simulation of multistage testing. \n    The full documentation and tutorials are at <https://github.com/xluo11/xxIRT>.  "
  },
  {
    "id": 23277,
    "package_name": "yamlme",
    "title": "Writing 'YAML' Headers for 'R-Markdown' Documents",
    "description": "Setting layout through 'YAML' headers in 'R-Markdown' documents,\n    enabling their automatic generation.\n    Functions and methods may summarize 'R' objects in automatic reports, for\n    instance check-lists and further reports applied to the packages 'taxlist'\n    and 'vegtable'.",
    "version": "0.1.2",
    "maintainer": "Miguel Alvarez <kamapu78@gmail.com>",
    "author": "Miguel Alvarez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1500-1834>),\n  Bisrat Haile Gebrekidan [ctb]",
    "url": "https://github.com/kamapu/yamlme,\nhttps://kamapu.github.io/rpkg/yamlme/,\nhttps://kamapu.github.io/yamlme/",
    "bug_reports": "https://github.com/kamapu/yamlme/issues",
    "repository": "https://cran.r-project.org/package=yamlme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yamlme Writing 'YAML' Headers for 'R-Markdown' Documents Setting layout through 'YAML' headers in 'R-Markdown' documents,\n    enabling their automatic generation.\n    Functions and methods may summarize 'R' objects in automatic reports, for\n    instance check-lists and further reports applied to the packages 'taxlist'\n    and 'vegtable'.  "
  },
  {
    "id": 23281,
    "package_name": "ycevo",
    "title": "Nonparametric Estimation of the Yield Curve Evolution",
    "description": "Nonparametric estimation of discount functions and yield curves from \n    transaction data of coupon paying bonds. \n    Koo, B., La Vecchia, D., & Linton, O. B. (2021) <doi:10.1016/j.jeconom.2020.04.014> \n    describe an application of this package using the Center for Research in \n    Security Prices (CRSP) Bond Data and document its implementation.",
    "version": "0.3.0",
    "maintainer": "Yangzhuoran Fin Yang <yangyangzhuoran@gmail.com>",
    "author": "Bonsoo Koo [aut],\n  Nathaniel Tomasetti [ctb],\n  Kai-Yang Goh [ctb],\n  Yangzhuoran Fin Yang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1232-8017>)",
    "url": "https://github.com/bonsook/ycevo",
    "bug_reports": "https://github.com/bonsook/ycevo/issues",
    "repository": "https://cran.r-project.org/package=ycevo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ycevo Nonparametric Estimation of the Yield Curve Evolution Nonparametric estimation of discount functions and yield curves from \n    transaction data of coupon paying bonds. \n    Koo, B., La Vecchia, D., & Linton, O. B. (2021) <doi:10.1016/j.jeconom.2020.04.014> \n    describe an application of this package using the Center for Research in \n    Security Prices (CRSP) Bond Data and document its implementation.  "
  },
  {
    "id": 23296,
    "package_name": "yum",
    "title": "Utilities to Extract and Process 'YAML' Fragments",
    "description": "Provides a number of functions to facilitate\n  extracting information in 'YAML' fragments from one or \n  multiple files, optionally structuring the information\n  in a 'data.tree'. 'YAML' (recursive acronym for \"YAML ain't\n  Markup Language\") is a convention for specifying structured\n  data in a format that is both machine- and human-readable.\n  'YAML' therefore lends itself well for embedding (meta)data\n  in plain text files, such as Markdown files. This principle\n  is implemented in 'yum' with minimal dependencies (i.e. only\n  the 'yaml' packages, and the 'data.tree' package can be\n  used to enable additional functionality).",
    "version": "0.1.0",
    "maintainer": "Gjalt-Jorn Peters <gjalt-jorn@userfriendlyscience.com>",
    "author": "Gjalt-Jorn Peters [aut, cre]",
    "url": "https://r-packages.gitlab.io/yum",
    "bug_reports": "https://gitlab.com/r-packages/yum/-/issues",
    "repository": "https://cran.r-project.org/package=yum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yum Utilities to Extract and Process 'YAML' Fragments Provides a number of functions to facilitate\n  extracting information in 'YAML' fragments from one or \n  multiple files, optionally structuring the information\n  in a 'data.tree'. 'YAML' (recursive acronym for \"YAML ain't\n  Markup Language\") is a convention for specifying structured\n  data in a format that is both machine- and human-readable.\n  'YAML' therefore lends itself well for embedding (meta)data\n  in plain text files, such as Markdown files. This principle\n  is implemented in 'yum' with minimal dependencies (i.e. only\n  the 'yaml' packages, and the 'data.tree' package can be\n  used to enable additional functionality).  "
  },
  {
    "id": 23305,
    "package_name": "zdeskR",
    "title": "Connect to Your 'Zendesk' Data",
    "description": "Facilitates making a connection to the \n  'Zendesk' API and executing various queries. You can use it to\n  get ticket, ticket metrics, and user data. The 'Zendesk' documentation is \n  available at <https://developer.zendesk.com/rest_api\n  /docs/support/introduction>. This package is not supported by \n  'Zendesk' (owner of the software).",
    "version": "0.6.0",
    "maintainer": "Chris Umphlett <christopher.umphlett@gmail.com>",
    "author": "Chris Umphlett [aut, cre],\n  Avinash Panigrahi [aut]",
    "url": "https://github.com/chrisumphlett/zdeskR",
    "bug_reports": "https://github.com/chrisumphlett/zdeskR/issues",
    "repository": "https://cran.r-project.org/package=zdeskR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zdeskR Connect to Your 'Zendesk' Data Facilitates making a connection to the \n  'Zendesk' API and executing various queries. You can use it to\n  get ticket, ticket metrics, and user data. The 'Zendesk' documentation is \n  available at <https://developer.zendesk.com/rest_api\n  /docs/support/introduction>. This package is not supported by \n  'Zendesk' (owner of the software).  "
  },
  {
    "id": 23331,
    "package_name": "zonebuilder",
    "title": "Create and Explore Geographic Zoning Systems",
    "description": "Functions, documentation and example data to help divide\n    geographic space into discrete polygons (zones).\n    The package supports new zoning systems that are documented in the\n    accompanying paper,\n    \"ClockBoard: A zoning system for urban analysis\",\n    by Lovelace et al. (2022) <doi:10.5311/JOSIS.2022.24.172>.\n    The functions are motivated by research into the merits of different zoning systems\n    (Openshaw, 1977) <doi:10.1068/a090169>. A flexible ClockBoard zoning system is\n    provided, which breaks-up space by concentric rings\n    and radial lines emanating from a central point.\n    By default, the diameter of the rings grow according to the triangular number sequence\n    (Ross & Knott, 2019) <doi:10.1080/26375451.2019.1598687> with the first 4 doughnuts\n    (or annuli) measuring 1, 3, 6, and 10 km wide.\n    These annuli are subdivided into equal segments (12 by default), creating the\n    visual impression of a dartboard. Zones are labelled according to\n    distance to the centre and angular distance from North, creating a simple\n    geographic zoning and labelling system useful for visualising geographic\n    phenomena with a clearly demarcated central location such as cities.",
    "version": "0.1.0",
    "maintainer": "Robin Lovelace <rob00x@gmail.com>",
    "author": "Robin Lovelace [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5679-6536>),\n  Martijn Tennekes [aut]",
    "url": "https://github.com/zonebuilders/zonebuilder,\nhttps://zonebuilders.github.io/zonebuilder/",
    "bug_reports": "https://github.com/zonebuilders/zonebuilder/issues",
    "repository": "https://cran.r-project.org/package=zonebuilder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zonebuilder Create and Explore Geographic Zoning Systems Functions, documentation and example data to help divide\n    geographic space into discrete polygons (zones).\n    The package supports new zoning systems that are documented in the\n    accompanying paper,\n    \"ClockBoard: A zoning system for urban analysis\",\n    by Lovelace et al. (2022) <doi:10.5311/JOSIS.2022.24.172>.\n    The functions are motivated by research into the merits of different zoning systems\n    (Openshaw, 1977) <doi:10.1068/a090169>. A flexible ClockBoard zoning system is\n    provided, which breaks-up space by concentric rings\n    and radial lines emanating from a central point.\n    By default, the diameter of the rings grow according to the triangular number sequence\n    (Ross & Knott, 2019) <doi:10.1080/26375451.2019.1598687> with the first 4 doughnuts\n    (or annuli) measuring 1, 3, 6, and 10 km wide.\n    These annuli are subdivided into equal segments (12 by default), creating the\n    visual impression of a dartboard. Zones are labelled according to\n    distance to the centre and angular distance from North, creating a simple\n    geographic zoning and labelling system useful for visualising geographic\n    phenomena with a clearly demarcated central location such as cities.  "
  },
  {
    "id": 23338,
    "package_name": "zoomr",
    "title": "Connect to Your 'Zoom' Data",
    "description": "Facilitates making a connection to the 'Zoom' API and executing\n  various queries. You can use it to get data on 'Zoom' webinars and 'Zoom'\n  meetings. The 'Zoom' documentation is available at \n  <https://developers.zoom.us/docs/api/>. This package is \n  not supported by 'Zoom' (owner of the software).",
    "version": "0.4.0",
    "maintainer": "Chris Umphlett <christopher.umphlett@gmail.com>",
    "author": "Chris Umphlett [aut, cre],\n  Paul Schmidt [ctb]",
    "url": "https://github.com/chrisumphlett/zoomr",
    "bug_reports": "https://github.com/chrisumphlett/zoomr/issues",
    "repository": "https://cran.r-project.org/package=zoomr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zoomr Connect to Your 'Zoom' Data Facilitates making a connection to the 'Zoom' API and executing\n  various queries. You can use it to get data on 'Zoom' webinars and 'Zoom'\n  meetings. The 'Zoom' documentation is available at \n  <https://developers.zoom.us/docs/api/>. This package is \n  not supported by 'Zoom' (owner of the software).  "
  }
]