[
  {
    "id": 1028,
    "package_name": "purrr",
    "title": "Functional Programming Tools",
    "description": "A complete and consistent functional programming toolkit\nfor R.",
    "version": "1.2.0.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4757-117X>),\nLionel Henry [aut],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://purrr.tidyverse.org/, https://github.com/tidyverse/purrr",
    "bug_reports": "https://github.com/tidyverse/purrr/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "%||%"
      ],
      [
        "accumulate"
      ],
      [
        "accumulate2"
      ],
      [
        "array_branch"
      ],
      [
        "array_tree"
      ],
      [
        "as_mapper"
      ],
      [
        "as_vector"
      ],
      [
        "assign_in"
      ],
      [
        "attr_getter"
      ],
      [
        "auto_browse"
      ],
      [
        "chuck"
      ],
      [
        "compact"
      ],
      [
        "compose"
      ],
      [
        "cross"
      ],
      [
        "cross_df"
      ],
      [
        "cross2"
      ],
      [
        "cross3"
      ],
      [
        "detect"
      ],
      [
        "detect_index"
      ],
      [
        "discard"
      ],
      [
        "discard_at"
      ],
      [
        "done"
      ],
      [
        "every"
      ],
      [
        "exec"
      ],
      [
        "flatten"
      ],
      [
        "flatten_chr"
      ],
      [
        "flatten_dbl"
      ],
      [
        "flatten_df"
      ],
      [
        "flatten_dfc"
      ],
      [
        "flatten_dfr"
      ],
      [
        "flatten_int"
      ],
      [
        "flatten_lgl"
      ],
      [
        "flatten_raw"
      ],
      [
        "has_element"
      ],
      [
        "head_while"
      ],
      [
        "imap"
      ],
      [
        "imap_chr"
      ],
      [
        "imap_dbl"
      ],
      [
        "imap_dfc"
      ],
      [
        "imap_dfr"
      ],
      [
        "imap_int"
      ],
      [
        "imap_lgl"
      ],
      [
        "imap_raw"
      ],
      [
        "imap_vec"
      ],
      [
        "imodify"
      ],
      [
        "in_parallel"
      ],
      [
        "insistently"
      ],
      [
        "invoke"
      ],
      [
        "invoke_map"
      ],
      [
        "invoke_map_chr"
      ],
      [
        "invoke_map_dbl"
      ],
      [
        "invoke_map_df"
      ],
      [
        "invoke_map_dfc"
      ],
      [
        "invoke_map_dfr"
      ],
      [
        "invoke_map_int"
      ],
      [
        "invoke_map_lgl"
      ],
      [
        "invoke_map_raw"
      ],
      [
        "is_atomic"
      ],
      [
        "is_bare_atomic"
      ],
      [
        "is_bare_character"
      ],
      [
        "is_bare_double"
      ],
      [
        "is_bare_integer"
      ],
      [
        "is_bare_list"
      ],
      [
        "is_bare_logical"
      ],
      [
        "is_bare_numeric"
      ],
      [
        "is_bare_vector"
      ],
      [
        "is_character"
      ],
      [
        "is_double"
      ],
      [
        "is_empty"
      ],
      [
        "is_formula"
      ],
      [
        "is_function"
      ],
      [
        "is_integer"
      ],
      [
        "is_list"
      ],
      [
        "is_logical"
      ],
      [
        "is_null"
      ],
      [
        "is_rate"
      ],
      [
        "is_scalar_atomic"
      ],
      [
        "is_scalar_character"
      ],
      [
        "is_scalar_double"
      ],
      [
        "is_scalar_integer"
      ],
      [
        "is_scalar_list"
      ],
      [
        "is_scalar_logical"
      ],
      [
        "is_scalar_vector"
      ],
      [
        "is_vector"
      ],
      [
        "iwalk"
      ],
      [
        "keep"
      ],
      [
        "keep_at"
      ],
      [
        "lift"
      ],
      [
        "lift_dl"
      ],
      [
        "lift_dv"
      ],
      [
        "lift_ld"
      ],
      [
        "lift_lv"
      ],
      [
        "lift_vd"
      ],
      [
        "lift_vl"
      ],
      [
        "list_along"
      ],
      [
        "list_assign"
      ],
      [
        "list_c"
      ],
      [
        "list_cbind"
      ],
      [
        "list_flatten"
      ],
      [
        "list_merge"
      ],
      [
        "list_modify"
      ],
      [
        "list_rbind"
      ],
      [
        "list_simplify"
      ],
      [
        "list_transpose"
      ],
      [
        "lmap"
      ],
      [
        "lmap_at"
      ],
      [
        "lmap_if"
      ],
      [
        "map"
      ],
      [
        "map_at"
      ],
      [
        "map_chr"
      ],
      [
        "map_dbl"
      ],
      [
        "map_depth"
      ],
      [
        "map_df"
      ],
      [
        "map_dfc"
      ],
      [
        "map_dfr"
      ],
      [
        "map_if"
      ],
      [
        "map_int"
      ],
      [
        "map_lgl"
      ],
      [
        "map_raw"
      ],
      [
        "map_vec"
      ],
      [
        "map2"
      ],
      [
        "map2_chr"
      ],
      [
        "map2_dbl"
      ],
      [
        "map2_df"
      ],
      [
        "map2_dfc"
      ],
      [
        "map2_dfr"
      ],
      [
        "map2_int"
      ],
      [
        "map2_lgl"
      ],
      [
        "map2_raw"
      ],
      [
        "map2_vec"
      ],
      [
        "modify"
      ],
      [
        "modify_at"
      ],
      [
        "modify_depth"
      ],
      [
        "modify_if"
      ],
      [
        "modify_in"
      ],
      [
        "modify_tree"
      ],
      [
        "modify2"
      ],
      [
        "negate"
      ],
      [
        "none"
      ],
      [
        "partial"
      ],
      [
        "pluck"
      ],
      [
        "pluck_depth"
      ],
      [
        "pluck_exists"
      ],
      [
        "pluck<-"
      ],
      [
        "pmap"
      ],
      [
        "pmap_chr"
      ],
      [
        "pmap_dbl"
      ],
      [
        "pmap_df"
      ],
      [
        "pmap_dfc"
      ],
      [
        "pmap_dfr"
      ],
      [
        "pmap_int"
      ],
      [
        "pmap_lgl"
      ],
      [
        "pmap_raw"
      ],
      [
        "pmap_vec"
      ],
      [
        "possibly"
      ],
      [
        "prepend"
      ],
      [
        "pwalk"
      ],
      [
        "quietly"
      ],
      [
        "rate_backoff"
      ],
      [
        "rate_delay"
      ],
      [
        "rate_reset"
      ],
      [
        "rate_sleep"
      ],
      [
        "rbernoulli"
      ],
      [
        "rdunif"
      ],
      [
        "reduce"
      ],
      [
        "reduce2"
      ],
      [
        "rep_along"
      ],
      [
        "rerun"
      ],
      [
        "safely"
      ],
      [
        "set_names"
      ],
      [
        "simplify"
      ],
      [
        "simplify_all"
      ],
      [
        "slowly"
      ],
      [
        "some"
      ],
      [
        "splice"
      ],
      [
        "tail_while"
      ],
      [
        "transpose"
      ],
      [
        "update_list"
      ],
      [
        "vec_depth"
      ],
      [
        "walk"
      ],
      [
        "walk2"
      ],
      [
        "when"
      ],
      [
        "zap"
      ]
    ],
    "topics": [
      [
        "functional-programming"
      ]
    ],
    "score": 23.0932,
    "stars": 1361,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "purrr Functional Programming Tools A complete and consistent functional programming toolkit\nfor R. %>% %||% accumulate accumulate2 array_branch array_tree as_mapper as_vector assign_in attr_getter auto_browse chuck compact compose cross cross_df cross2 cross3 detect detect_index discard discard_at done every exec flatten flatten_chr flatten_dbl flatten_df flatten_dfc flatten_dfr flatten_int flatten_lgl flatten_raw has_element head_while imap imap_chr imap_dbl imap_dfc imap_dfr imap_int imap_lgl imap_raw imap_vec imodify in_parallel insistently invoke invoke_map invoke_map_chr invoke_map_dbl invoke_map_df invoke_map_dfc invoke_map_dfr invoke_map_int invoke_map_lgl invoke_map_raw is_atomic is_bare_atomic is_bare_character is_bare_double is_bare_integer is_bare_list is_bare_logical is_bare_numeric is_bare_vector is_character is_double is_empty is_formula is_function is_integer is_list is_logical is_null is_rate is_scalar_atomic is_scalar_character is_scalar_double is_scalar_integer is_scalar_list is_scalar_logical is_scalar_vector is_vector iwalk keep keep_at lift lift_dl lift_dv lift_ld lift_lv lift_vd lift_vl list_along list_assign list_c list_cbind list_flatten list_merge list_modify list_rbind list_simplify list_transpose lmap lmap_at lmap_if map map_at map_chr map_dbl map_depth map_df map_dfc map_dfr map_if map_int map_lgl map_raw map_vec map2 map2_chr map2_dbl map2_df map2_dfc map2_dfr map2_int map2_lgl map2_raw map2_vec modify modify_at modify_depth modify_if modify_in modify_tree modify2 negate none partial pluck pluck_depth pluck_exists pluck<- pmap pmap_chr pmap_dbl pmap_df pmap_dfc pmap_dfr pmap_int pmap_lgl pmap_raw pmap_vec possibly prepend pwalk quietly rate_backoff rate_delay rate_reset rate_sleep rbernoulli rdunif reduce reduce2 rep_along rerun safely set_names simplify simplify_all slowly some splice tail_while transpose update_list vec_depth walk walk2 when zap functional-programming"
  },
  {
    "id": 1086,
    "package_name": "renv",
    "title": "Project Environments",
    "description": "A dependency management toolkit for R. Using 'renv', you\ncan create and manage project-local R libraries, save the state\nof these libraries to a 'lockfile', and later restore your\nlibrary as required. Together, these tools can help make your\nprojects more isolated, portable, and reproducible.",
    "version": "1.1.5.9000",
    "maintainer": "Kevin Ushey <kevin@rstudio.com>",
    "author": "Kevin Ushey [aut, cre] (ORCID: <https://orcid.org/0000-0003-2880-7407>),\nHadley Wickham [aut] (ORCID: <https://orcid.org/0000-0003-4757-117X>),\nPosit Software, PBC [cph, fnd]",
    "url": "https://rstudio.github.io/renv/, https://github.com/rstudio/renv",
    "bug_reports": "https://github.com/rstudio/renv/issues",
    "repository": "",
    "exports": [
      [
        "activate"
      ],
      [
        "autoload"
      ],
      [
        "checkout"
      ],
      [
        "clean"
      ],
      [
        "config"
      ],
      [
        "consent"
      ],
      [
        "deactivate"
      ],
      [
        "dependencies"
      ],
      [
        "diagnostics"
      ],
      [
        "embed"
      ],
      [
        "equip"
      ],
      [
        "history"
      ],
      [
        "hydrate"
      ],
      [
        "init"
      ],
      [
        "install"
      ],
      [
        "isolate"
      ],
      [
        "load"
      ],
      [
        "lockfile_create"
      ],
      [
        "lockfile_modify"
      ],
      [
        "lockfile_read"
      ],
      [
        "lockfile_validate"
      ],
      [
        "lockfile_write"
      ],
      [
        "migrate"
      ],
      [
        "modify"
      ],
      [
        "paths"
      ],
      [
        "project"
      ],
      [
        "purge"
      ],
      [
        "rebuild"
      ],
      [
        "record"
      ],
      [
        "refresh"
      ],
      [
        "rehash"
      ],
      [
        "remove"
      ],
      [
        "repair"
      ],
      [
        "restore"
      ],
      [
        "retrieve"
      ],
      [
        "revert"
      ],
      [
        "run"
      ],
      [
        "sandbox"
      ],
      [
        "scaffold"
      ],
      [
        "settings"
      ],
      [
        "snapshot"
      ],
      [
        "status"
      ],
      [
        "sysreqs"
      ],
      [
        "update"
      ],
      [
        "upgrade"
      ],
      [
        "use"
      ],
      [
        "use_python"
      ],
      [
        "vulns"
      ]
    ],
    "topics": [],
    "score": 18.4969,
    "stars": 1112,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "renv Project Environments A dependency management toolkit for R. Using 'renv', you\ncan create and manage project-local R libraries, save the state\nof these libraries to a 'lockfile', and later restore your\nlibrary as required. Together, these tools can help make your\nprojects more isolated, portable, and reproducible. activate autoload checkout clean config consent deactivate dependencies diagnostics embed equip history hydrate init install isolate load lockfile_create lockfile_modify lockfile_read lockfile_validate lockfile_write migrate modify paths project purge rebuild record refresh rehash remove repair restore retrieve revert run sandbox scaffold settings snapshot status sysreqs update upgrade use use_python vulns "
  },
  {
    "id": 498,
    "package_name": "dtplyr",
    "title": "Data Table Back-End for 'dplyr'",
    "description": "Provides a data.table backend for 'dplyr'. The goal of\n'dtplyr' is to allow you to write 'dplyr' code that is\nautomatically translated to the equivalent, but usually much\nfaster, data.table code.",
    "version": "1.3.2.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [cre, aut],\nMaximilian Girlich [aut],\nMark Fairbanks [aut],\nRyan Dickerson [aut],\nPosit Software, PBC [cph, fnd]",
    "url": "https://dtplyr.tidyverse.org, https://github.com/tidyverse/dtplyr",
    "bug_reports": "https://github.com/tidyverse/dtplyr/issues",
    "repository": "",
    "exports": [
      [
        ".datatable.aware"
      ],
      [
        "lazy_dt"
      ]
    ],
    "topics": [
      [
        "datatable"
      ],
      [
        "dplyr"
      ]
    ],
    "score": 16.5488,
    "stars": 672,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "dtplyr Data Table Back-End for 'dplyr' Provides a data.table backend for 'dplyr'. The goal of\n'dtplyr' is to allow you to write 'dplyr' code that is\nautomatically translated to the equivalent, but usually much\nfaster, data.table code. .datatable.aware lazy_dt datatable dplyr"
  },
  {
    "id": 1050,
    "package_name": "ragg",
    "title": "Graphic Devices Based on AGG",
    "description": "Anti-Grain Geometry (AGG) is a high-quality and\nhigh-performance 2D drawing library. The 'ragg' package\nprovides a set of graphic devices based on AGG to use as\nalternative to the raster devices provided through the\n'grDevices' package.",
    "version": "1.5.0.9000",
    "maintainer": "Thomas Lin Pedersen <thomas.pedersen@posit.co>",
    "author": "Thomas Lin Pedersen [cre, aut] (ORCID:\n<https://orcid.org/0000-0002-5147-4711>),\nMaxim Shemanarev [aut, cph] (Author of AGG),\nTony Juricic [ctb, cph] (Contributor to AGG),\nMilan Marusinec [ctb, cph] (Contributor to AGG),\nSpencer Garrett [ctb] (Contributor to AGG),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://ragg.r-lib.org, https://github.com/r-lib/ragg",
    "bug_reports": "https://github.com/r-lib/ragg/issues",
    "repository": "",
    "exports": [
      [
        "agg_capture"
      ],
      [
        "agg_jpeg"
      ],
      [
        "agg_png"
      ],
      [
        "agg_ppm"
      ],
      [
        "agg_record"
      ],
      [
        "agg_supertransparent"
      ],
      [
        "agg_tiff"
      ],
      [
        "agg_webp"
      ],
      [
        "agg_webp_anim"
      ],
      [
        "font_feature"
      ],
      [
        "get_font_features"
      ],
      [
        "register_font"
      ],
      [
        "register_variant"
      ]
    ],
    "topics": [
      [
        "drawing"
      ],
      [
        "graphics"
      ],
      [
        "vector-graphics"
      ],
      [
        "freetype"
      ],
      [
        "libpng"
      ],
      [
        "tiff"
      ],
      [
        "libjpeg-turbo"
      ],
      [
        "libwebp"
      ],
      [
        "cpp"
      ]
    ],
    "score": 15.5783,
    "stars": 181,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "ragg Graphic Devices Based on AGG Anti-Grain Geometry (AGG) is a high-quality and\nhigh-performance 2D drawing library. The 'ragg' package\nprovides a set of graphic devices based on AGG to use as\nalternative to the raster devices provided through the\n'grDevices' package. agg_capture agg_jpeg agg_png agg_ppm agg_record agg_supertransparent agg_tiff agg_webp agg_webp_anim font_feature get_font_features register_font register_variant drawing graphics vector-graphics freetype libpng tiff libjpeg-turbo libwebp cpp"
  },
  {
    "id": 1356,
    "package_name": "timetk",
    "title": "A Tool Kit for Working with Time Series",
    "description": "Easy visualization, wrangling, and feature engineering of\ntime series data for forecasting and machine learning\nprediction. Consolidates and extends time series functionality\nfrom packages including 'dplyr', 'stats', 'xts', 'forecast',\n'slider', 'padr', 'recipes', and 'rsample'.",
    "version": "2.9.1.9000",
    "maintainer": "Matt Dancho <mdancho@business-science.io>",
    "author": "Matt Dancho [aut, cre],\nDavis Vaughan [aut]",
    "url": "https://github.com/business-science/timetk,\nhttps://business-science.github.io/timetk/",
    "bug_reports": "https://github.com/business-science/timetk/issues",
    "repository": "",
    "exports": [
      [
        ":="
      ],
      [
        ".data"
      ],
      [
        "%-time%"
      ],
      [
        "%+time%"
      ],
      [
        "%||%"
      ],
      [
        "add_time"
      ],
      [
        "anomalize"
      ],
      [
        "as_label"
      ],
      [
        "as_name"
      ],
      [
        "auto_lambda"
      ],
      [
        "between_time"
      ],
      [
        "box_cox_inv_vec"
      ],
      [
        "box_cox_vec"
      ],
      [
        "condense_period"
      ],
      [
        "diff_inv_vec"
      ],
      [
        "diff_vec"
      ],
      [
        "enquo"
      ],
      [
        "enquos"
      ],
      [
        "expr"
      ],
      [
        "filter_by_time"
      ],
      [
        "filter_period"
      ],
      [
        "fourier_vec"
      ],
      [
        "future_frame"
      ],
      [
        "get_tk_time_scale_template"
      ],
      [
        "has_timetk_idx"
      ],
      [
        "is_date_class"
      ],
      [
        "lag_vec"
      ],
      [
        "lead_vec"
      ],
      [
        "log_interval_inv_vec"
      ],
      [
        "log_interval_vec"
      ],
      [
        "mutate_by_time"
      ],
      [
        "normalize_inv_vec"
      ],
      [
        "normalize_vec"
      ],
      [
        "pad_by_time"
      ],
      [
        "parse_date2"
      ],
      [
        "parse_datetime2"
      ],
      [
        "plot_acf_diagnostics"
      ],
      [
        "plot_anomalies"
      ],
      [
        "plot_anomalies_cleaned"
      ],
      [
        "plot_anomalies_decomp"
      ],
      [
        "plot_anomaly_diagnostics"
      ],
      [
        "plot_seasonal_diagnostics"
      ],
      [
        "plot_stl_diagnostics"
      ],
      [
        "plot_time_series"
      ],
      [
        "plot_time_series_boxplot"
      ],
      [
        "plot_time_series_cv_plan"
      ],
      [
        "plot_time_series_regression"
      ],
      [
        "set_tk_time_scale_template"
      ],
      [
        "slice_period"
      ],
      [
        "slidify"
      ],
      [
        "slidify_vec"
      ],
      [
        "smooth_vec"
      ],
      [
        "standardize_inv_vec"
      ],
      [
        "standardize_vec"
      ],
      [
        "step_box_cox"
      ],
      [
        "step_diff"
      ],
      [
        "step_fourier"
      ],
      [
        "step_holiday_signature"
      ],
      [
        "step_log_interval"
      ],
      [
        "step_slidify"
      ],
      [
        "step_slidify_augment"
      ],
      [
        "step_smooth"
      ],
      [
        "step_timeseries_signature"
      ],
      [
        "step_ts_clean"
      ],
      [
        "step_ts_impute"
      ],
      [
        "step_ts_pad"
      ],
      [
        "subtract_time"
      ],
      [
        "summarise_by_time"
      ],
      [
        "summarize_by_time"
      ],
      [
        "sym"
      ],
      [
        "syms"
      ],
      [
        "time_series_cv"
      ],
      [
        "time_series_split"
      ],
      [
        "tk_acf_diagnostics"
      ],
      [
        "tk_anomaly_diagnostics"
      ],
      [
        "tk_augment_differences"
      ],
      [
        "tk_augment_fourier"
      ],
      [
        "tk_augment_holiday_signature"
      ],
      [
        "tk_augment_lags"
      ],
      [
        "tk_augment_leads"
      ],
      [
        "tk_augment_slidify"
      ],
      [
        "tk_augment_timeseries_signature"
      ],
      [
        "tk_get_frequency"
      ],
      [
        "tk_get_holiday_signature"
      ],
      [
        "tk_get_holidays_by_year"
      ],
      [
        "tk_get_timeseries_signature"
      ],
      [
        "tk_get_timeseries_summary"
      ],
      [
        "tk_get_timeseries_unit_frequency"
      ],
      [
        "tk_get_timeseries_variables"
      ],
      [
        "tk_get_trend"
      ],
      [
        "tk_index"
      ],
      [
        "tk_make_future_timeseries"
      ],
      [
        "tk_make_holiday_sequence"
      ],
      [
        "tk_make_timeseries"
      ],
      [
        "tk_make_weekday_sequence"
      ],
      [
        "tk_make_weekend_sequence"
      ],
      [
        "tk_seasonal_diagnostics"
      ],
      [
        "tk_stl_diagnostics"
      ],
      [
        "tk_summary_diagnostics"
      ],
      [
        "tk_tbl"
      ],
      [
        "tk_time_scale_template"
      ],
      [
        "tk_time_series_cv_plan"
      ],
      [
        "tk_ts"
      ],
      [
        "tk_ts_"
      ],
      [
        "tk_ts_.data.frame"
      ],
      [
        "tk_ts_.default"
      ],
      [
        "tk_ts_dispatch_"
      ],
      [
        "tk_tsfeatures"
      ],
      [
        "tk_xts"
      ],
      [
        "tk_xts_"
      ],
      [
        "tk_zoo"
      ],
      [
        "tk_zoo_"
      ],
      [
        "tk_zooreg"
      ],
      [
        "tk_zooreg_"
      ],
      [
        "tk_zooreg_.data.frame"
      ],
      [
        "tk_zooreg_.default"
      ],
      [
        "tk_zooreg_dispatch_"
      ],
      [
        "ts_clean_vec"
      ],
      [
        "ts_impute_vec"
      ]
    ],
    "topics": [
      [
        "coercion"
      ],
      [
        "coercion-functions"
      ],
      [
        "data-mining"
      ],
      [
        "dplyr"
      ],
      [
        "forecast"
      ],
      [
        "forecasting"
      ],
      [
        "forecasting-models"
      ],
      [
        "machine-learning"
      ],
      [
        "series-decomposition"
      ],
      [
        "series-signature"
      ],
      [
        "tibble"
      ],
      [
        "tidy"
      ],
      [
        "tidyquant"
      ],
      [
        "tidyverse"
      ],
      [
        "time"
      ],
      [
        "time-series"
      ],
      [
        "timeseries"
      ]
    ],
    "score": 14.4312,
    "stars": 632,
    "primary_category": "tidyverse",
    "source_universe": "business-science",
    "search_text": "timetk A Tool Kit for Working with Time Series Easy visualization, wrangling, and feature engineering of\ntime series data for forecasting and machine learning\nprediction. Consolidates and extends time series functionality\nfrom packages including 'dplyr', 'stats', 'xts', 'forecast',\n'slider', 'padr', 'recipes', and 'rsample'. := .data %-time% %+time% %||% add_time anomalize as_label as_name auto_lambda between_time box_cox_inv_vec box_cox_vec condense_period diff_inv_vec diff_vec enquo enquos expr filter_by_time filter_period fourier_vec future_frame get_tk_time_scale_template has_timetk_idx is_date_class lag_vec lead_vec log_interval_inv_vec log_interval_vec mutate_by_time normalize_inv_vec normalize_vec pad_by_time parse_date2 parse_datetime2 plot_acf_diagnostics plot_anomalies plot_anomalies_cleaned plot_anomalies_decomp plot_anomaly_diagnostics plot_seasonal_diagnostics plot_stl_diagnostics plot_time_series plot_time_series_boxplot plot_time_series_cv_plan plot_time_series_regression set_tk_time_scale_template slice_period slidify slidify_vec smooth_vec standardize_inv_vec standardize_vec step_box_cox step_diff step_fourier step_holiday_signature step_log_interval step_slidify step_slidify_augment step_smooth step_timeseries_signature step_ts_clean step_ts_impute step_ts_pad subtract_time summarise_by_time summarize_by_time sym syms time_series_cv time_series_split tk_acf_diagnostics tk_anomaly_diagnostics tk_augment_differences tk_augment_fourier tk_augment_holiday_signature tk_augment_lags tk_augment_leads tk_augment_slidify tk_augment_timeseries_signature tk_get_frequency tk_get_holiday_signature tk_get_holidays_by_year tk_get_timeseries_signature tk_get_timeseries_summary tk_get_timeseries_unit_frequency tk_get_timeseries_variables tk_get_trend tk_index tk_make_future_timeseries tk_make_holiday_sequence tk_make_timeseries tk_make_weekday_sequence tk_make_weekend_sequence tk_seasonal_diagnostics tk_stl_diagnostics tk_summary_diagnostics tk_tbl tk_time_scale_template tk_time_series_cv_plan tk_ts tk_ts_ tk_ts_.data.frame tk_ts_.default tk_ts_dispatch_ tk_tsfeatures tk_xts tk_xts_ tk_zoo tk_zoo_ tk_zooreg tk_zooreg_ tk_zooreg_.data.frame tk_zooreg_.default tk_zooreg_dispatch_ ts_clean_vec ts_impute_vec coercion coercion-functions data-mining dplyr forecast forecasting forecasting-models machine-learning series-decomposition series-signature tibble tidy tidyquant tidyverse time time-series timeseries"
  },
  {
    "id": 1173,
    "package_name": "s2",
    "title": "Spherical Geometry Operators Using the S2 Geometry Library",
    "description": "Provides R bindings for Google's s2 library for geometric\ncalculations on the sphere. High-performance constructors and\nexporters provide high compatibility with existing spatial\npackages, transformers construct new geometries from existing\ngeometries, predicates provide a means to select geometries\nbased on spatial relationships, and accessors extract\ninformation about geometries.",
    "version": "1.1.9",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Dewey Dunnington [aut] (ORCID: <https://orcid.org/0000-0002-9415-4582>),\nEdzer Pebesma [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8049-7069>),\nEge Rubak [aut],\nJeroen Ooms [ctb] (configure script),\nGoogle, Inc. [cph] (Original s2geometry.io source code)",
    "url": "https://r-spatial.github.io/s2/, https://github.com/r-spatial/s2,\nhttp://s2geometry.io/",
    "bug_reports": "https://github.com/r-spatial/s2/issues",
    "repository": "",
    "exports": [
      [
        "as_s2_cell"
      ],
      [
        "as_s2_cell_union"
      ],
      [
        "as_s2_geography"
      ],
      [
        "as_s2_lnglat"
      ],
      [
        "as_s2_point"
      ],
      [
        "new_s2_cell"
      ],
      [
        "s2_area"
      ],
      [
        "s2_as_binary"
      ],
      [
        "s2_as_text"
      ],
      [
        "s2_boundary"
      ],
      [
        "s2_bounds_cap"
      ],
      [
        "s2_bounds_rect"
      ],
      [
        "s2_buffer_cells"
      ],
      [
        "s2_cell"
      ],
      [
        "s2_cell_area"
      ],
      [
        "s2_cell_area_approx"
      ],
      [
        "s2_cell_boundary"
      ],
      [
        "s2_cell_center"
      ],
      [
        "s2_cell_child"
      ],
      [
        "s2_cell_common_ancestor_level"
      ],
      [
        "s2_cell_common_ancestor_level_agg"
      ],
      [
        "s2_cell_contains"
      ],
      [
        "s2_cell_debug_string"
      ],
      [
        "s2_cell_distance"
      ],
      [
        "s2_cell_edge_neighbour"
      ],
      [
        "s2_cell_invalid"
      ],
      [
        "s2_cell_is_face"
      ],
      [
        "s2_cell_is_leaf"
      ],
      [
        "s2_cell_is_valid"
      ],
      [
        "s2_cell_level"
      ],
      [
        "s2_cell_max_distance"
      ],
      [
        "s2_cell_may_intersect"
      ],
      [
        "s2_cell_parent"
      ],
      [
        "s2_cell_polygon"
      ],
      [
        "s2_cell_sentinel"
      ],
      [
        "s2_cell_to_lnglat"
      ],
      [
        "s2_cell_union"
      ],
      [
        "s2_cell_union_contains"
      ],
      [
        "s2_cell_union_difference"
      ],
      [
        "s2_cell_union_intersection"
      ],
      [
        "s2_cell_union_intersects"
      ],
      [
        "s2_cell_union_normalize"
      ],
      [
        "s2_cell_union_union"
      ],
      [
        "s2_cell_vertex"
      ],
      [
        "s2_centroid"
      ],
      [
        "s2_centroid_agg"
      ],
      [
        "s2_closest_edges"
      ],
      [
        "s2_closest_feature"
      ],
      [
        "s2_closest_point"
      ],
      [
        "s2_contains"
      ],
      [
        "s2_contains_matrix"
      ],
      [
        "s2_convex_hull"
      ],
      [
        "s2_convex_hull_agg"
      ],
      [
        "s2_coverage_union_agg"
      ],
      [
        "s2_covered_by"
      ],
      [
        "s2_covered_by_matrix"
      ],
      [
        "s2_covering_cell_ids"
      ],
      [
        "s2_covering_cell_ids_agg"
      ],
      [
        "s2_covers"
      ],
      [
        "s2_covers_matrix"
      ],
      [
        "s2_data_cities"
      ],
      [
        "s2_data_countries"
      ],
      [
        "s2_data_timezones"
      ],
      [
        "s2_difference"
      ],
      [
        "s2_dimension"
      ],
      [
        "s2_disjoint"
      ],
      [
        "s2_disjoint_matrix"
      ],
      [
        "s2_distance"
      ],
      [
        "s2_distance_matrix"
      ],
      [
        "s2_dwithin"
      ],
      [
        "s2_dwithin_matrix"
      ],
      [
        "s2_earth_radius_meters"
      ],
      [
        "s2_equals"
      ],
      [
        "s2_equals_matrix"
      ],
      [
        "s2_farthest_feature"
      ],
      [
        "s2_geog_from_text"
      ],
      [
        "s2_geog_from_wkb"
      ],
      [
        "s2_geog_point"
      ],
      [
        "s2_geography"
      ],
      [
        "s2_geography_writer"
      ],
      [
        "s2_hemisphere"
      ],
      [
        "s2_interpolate"
      ],
      [
        "s2_interpolate_normalized"
      ],
      [
        "s2_intersection"
      ],
      [
        "s2_intersects"
      ],
      [
        "s2_intersects_box"
      ],
      [
        "s2_intersects_matrix"
      ],
      [
        "s2_is_collection"
      ],
      [
        "s2_is_empty"
      ],
      [
        "s2_is_valid"
      ],
      [
        "s2_is_valid_detail"
      ],
      [
        "s2_length"
      ],
      [
        "s2_lnglat"
      ],
      [
        "s2_make_line"
      ],
      [
        "s2_make_polygon"
      ],
      [
        "s2_max_distance"
      ],
      [
        "s2_max_distance_matrix"
      ],
      [
        "s2_may_intersect_matrix"
      ],
      [
        "s2_minimum_clearance_line_between"
      ],
      [
        "s2_num_points"
      ],
      [
        "s2_options"
      ],
      [
        "s2_perimeter"
      ],
      [
        "s2_plot"
      ],
      [
        "s2_point"
      ],
      [
        "s2_point_crs"
      ],
      [
        "s2_point_on_surface"
      ],
      [
        "s2_prepared_dwithin"
      ],
      [
        "s2_project"
      ],
      [
        "s2_project_normalized"
      ],
      [
        "s2_projection_mercator"
      ],
      [
        "s2_projection_orthographic"
      ],
      [
        "s2_projection_plate_carree"
      ],
      [
        "s2_rebuild"
      ],
      [
        "s2_rebuild_agg"
      ],
      [
        "s2_simplify"
      ],
      [
        "s2_snap_distance"
      ],
      [
        "s2_snap_identity"
      ],
      [
        "s2_snap_level"
      ],
      [
        "s2_snap_precision"
      ],
      [
        "s2_snap_to_grid"
      ],
      [
        "s2_sym_difference"
      ],
      [
        "s2_tessellate_tol_default"
      ],
      [
        "s2_touches"
      ],
      [
        "s2_touches_matrix"
      ],
      [
        "s2_trans_lnglat"
      ],
      [
        "s2_trans_point"
      ],
      [
        "s2_union"
      ],
      [
        "s2_union_agg"
      ],
      [
        "s2_within"
      ],
      [
        "s2_within_matrix"
      ],
      [
        "s2_world_plate_carree"
      ],
      [
        "s2_x"
      ],
      [
        "s2_y"
      ]
    ],
    "topics": [
      [
        "openssl"
      ],
      [
        "cpp"
      ]
    ],
    "score": 14.4312,
    "stars": 77,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "s2 Spherical Geometry Operators Using the S2 Geometry Library Provides R bindings for Google's s2 library for geometric\ncalculations on the sphere. High-performance constructors and\nexporters provide high compatibility with existing spatial\npackages, transformers construct new geometries from existing\ngeometries, predicates provide a means to select geometries\nbased on spatial relationships, and accessors extract\ninformation about geometries. as_s2_cell as_s2_cell_union as_s2_geography as_s2_lnglat as_s2_point new_s2_cell s2_area s2_as_binary s2_as_text s2_boundary s2_bounds_cap s2_bounds_rect s2_buffer_cells s2_cell s2_cell_area s2_cell_area_approx s2_cell_boundary s2_cell_center s2_cell_child s2_cell_common_ancestor_level s2_cell_common_ancestor_level_agg s2_cell_contains s2_cell_debug_string s2_cell_distance s2_cell_edge_neighbour s2_cell_invalid s2_cell_is_face s2_cell_is_leaf s2_cell_is_valid s2_cell_level s2_cell_max_distance s2_cell_may_intersect s2_cell_parent s2_cell_polygon s2_cell_sentinel s2_cell_to_lnglat s2_cell_union s2_cell_union_contains s2_cell_union_difference s2_cell_union_intersection s2_cell_union_intersects s2_cell_union_normalize s2_cell_union_union s2_cell_vertex s2_centroid s2_centroid_agg s2_closest_edges s2_closest_feature s2_closest_point s2_contains s2_contains_matrix s2_convex_hull s2_convex_hull_agg s2_coverage_union_agg s2_covered_by s2_covered_by_matrix s2_covering_cell_ids s2_covering_cell_ids_agg s2_covers s2_covers_matrix s2_data_cities s2_data_countries s2_data_timezones s2_difference s2_dimension s2_disjoint s2_disjoint_matrix s2_distance s2_distance_matrix s2_dwithin s2_dwithin_matrix s2_earth_radius_meters s2_equals s2_equals_matrix s2_farthest_feature s2_geog_from_text s2_geog_from_wkb s2_geog_point s2_geography s2_geography_writer s2_hemisphere s2_interpolate s2_interpolate_normalized s2_intersection s2_intersects s2_intersects_box s2_intersects_matrix s2_is_collection s2_is_empty s2_is_valid s2_is_valid_detail s2_length s2_lnglat s2_make_line s2_make_polygon s2_max_distance s2_max_distance_matrix s2_may_intersect_matrix s2_minimum_clearance_line_between s2_num_points s2_options s2_perimeter s2_plot s2_point s2_point_crs s2_point_on_surface s2_prepared_dwithin s2_project s2_project_normalized s2_projection_mercator s2_projection_orthographic s2_projection_plate_carree s2_rebuild s2_rebuild_agg s2_simplify s2_snap_distance s2_snap_identity s2_snap_level s2_snap_precision s2_snap_to_grid s2_sym_difference s2_tessellate_tol_default s2_touches s2_touches_matrix s2_trans_lnglat s2_trans_point s2_union s2_union_agg s2_within s2_within_matrix s2_world_plate_carree s2_x s2_y openssl cpp"
  },
  {
    "id": 695,
    "package_name": "hunspell",
    "title": "High-Performance Stemmer, Tokenizer, and Spell Checker",
    "description": "Low level spell checker and morphological analyzer based\non the famous 'hunspell' library <https://hunspell.github.io>.\nThe package can analyze or check individual words as well as\nparse text, latex, html or xml documents. For a more\nuser-friendly interface use the 'spelling' package which builds\non this package to automate checking of files, documentation\nand vignettes in all common formats.",
    "version": "3.0.7",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre],\nAuthors of libhunspell [cph] (see AUTHORS file)",
    "url": "https://docs.ropensci.org/hunspell/\nhttps://ropensci.r-universe.dev/hunspell",
    "bug_reports": "https://github.com/ropensci/hunspell/issues",
    "repository": "",
    "exports": [
      [
        "dicpath"
      ],
      [
        "dictionary"
      ],
      [
        "en_stats"
      ],
      [
        "hunspell"
      ],
      [
        "hunspell_analyze"
      ],
      [
        "hunspell_check"
      ],
      [
        "hunspell_find"
      ],
      [
        "hunspell_info"
      ],
      [
        "hunspell_parse"
      ],
      [
        "hunspell_stem"
      ],
      [
        "hunspell_suggest"
      ],
      [
        "list_dictionaries"
      ]
    ],
    "topics": [
      [
        "hunspell"
      ],
      [
        "spell-check"
      ],
      [
        "spellchecker"
      ],
      [
        "stemmer"
      ],
      [
        "tokenizer"
      ],
      [
        "cpp"
      ]
    ],
    "score": 13.3629,
    "stars": 113,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "hunspell High-Performance Stemmer, Tokenizer, and Spell Checker Low level spell checker and morphological analyzer based\non the famous 'hunspell' library <https://hunspell.github.io>.\nThe package can analyze or check individual words as well as\nparse text, latex, html or xml documents. For a more\nuser-friendly interface use the 'spelling' package which builds\non this package to automate checking of files, documentation\nand vignettes in all common formats. dicpath dictionary en_stats hunspell hunspell_analyze hunspell_check hunspell_find hunspell_info hunspell_parse hunspell_stem hunspell_suggest list_dictionaries hunspell spell-check spellchecker stemmer tokenizer cpp"
  },
  {
    "id": 949,
    "package_name": "partykit",
    "title": "A Toolkit for Recursive Partytioning",
    "description": "A toolkit with infrastructure for representing,\nsummarizing, and visualizing tree-structured regression and\nclassification models. This unified infrastructure can be used\nfor reading/coercing tree models from different sources\n('rpart', 'RWeka', 'PMML') yielding objects that share\nfunctionality for print()/plot()/predict() methods.\nFurthermore, new and improved reimplementations of conditional\ninference trees (ctree()) and model-based recursive\npartitioning (mob()) from the 'party' package are provided\nbased on the new infrastructure. A description of this package\nwas published by Hothorn and Zeileis (2015)\n<https://jmlr.org/papers/v16/hothorn15a.html>.",
    "version": "1.2-24",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "author": "Torsten Hothorn [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8301-0471>),\nHeidi Seibold [ctb] (ORCID: <https://orcid.org/0000-0002-8960-9642>),\nAchim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>)",
    "url": "http://partykit.r-forge.r-project.org/partykit/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "as.constparty"
      ],
      [
        "as.party"
      ],
      [
        "as.party.rpart"
      ],
      [
        "as.party.Weka_tree"
      ],
      [
        "as.party.XMLNode"
      ],
      [
        "as.partynode"
      ],
      [
        "as.simpleparty"
      ],
      [
        "as.simpleparty.XMLNode"
      ],
      [
        "breaks_split"
      ],
      [
        "cforest"
      ],
      [
        "character_split"
      ],
      [
        "ctree"
      ],
      [
        "ctree_control"
      ],
      [
        "data_party"
      ],
      [
        "data_party.default"
      ],
      [
        "edge_simple"
      ],
      [
        "extree_data"
      ],
      [
        "extree_fit"
      ],
      [
        "fitted_node"
      ],
      [
        "formatinfo_node"
      ],
      [
        "get_paths"
      ],
      [
        "gettree"
      ],
      [
        "gettree.cforest"
      ],
      [
        "glmtree"
      ],
      [
        "id_node"
      ],
      [
        "index_split"
      ],
      [
        "info_node"
      ],
      [
        "info_split"
      ],
      [
        "is.constparty"
      ],
      [
        "is.partynode"
      ],
      [
        "is.simpleparty"
      ],
      [
        "is.terminal"
      ],
      [
        "kidids_node"
      ],
      [
        "kidids_split"
      ],
      [
        "kids_node"
      ],
      [
        "lmtree"
      ],
      [
        "mob"
      ],
      [
        "mob_control"
      ],
      [
        "model_frame_rpart"
      ],
      [
        "node_barplot"
      ],
      [
        "node_bivplot"
      ],
      [
        "node_boxplot"
      ],
      [
        "node_ecdf"
      ],
      [
        "node_inner"
      ],
      [
        "node_mvar"
      ],
      [
        "node_party"
      ],
      [
        "node_surv"
      ],
      [
        "node_terminal"
      ],
      [
        "nodeapply"
      ],
      [
        "nodeids"
      ],
      [
        "nodeprune"
      ],
      [
        "nodeprune.party"
      ],
      [
        "party"
      ],
      [
        "partynode"
      ],
      [
        "partysplit"
      ],
      [
        "plot.modelparty"
      ],
      [
        "plot.party"
      ],
      [
        "pmmlTreeModel"
      ],
      [
        "predict_party"
      ],
      [
        "predict_party.default"
      ],
      [
        "predict.cforest"
      ],
      [
        "predict.modelparty"
      ],
      [
        "predict.party"
      ],
      [
        "print.modelparty"
      ],
      [
        "print.party"
      ],
      [
        "prob_split"
      ],
      [
        "prune.lmtree"
      ],
      [
        "prune.modelparty"
      ],
      [
        "refit.modelparty"
      ],
      [
        "right_split"
      ],
      [
        "sctest.constparty"
      ],
      [
        "sctest.modelparty"
      ],
      [
        "split_node"
      ],
      [
        "surrogates_node"
      ],
      [
        "varid_split"
      ],
      [
        "varimp"
      ],
      [
        "varimp.cforest"
      ],
      [
        "varimp.constparty"
      ],
      [
        "width"
      ]
    ],
    "topics": [],
    "score": 12.9222,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "partykit A Toolkit for Recursive Partytioning A toolkit with infrastructure for representing,\nsummarizing, and visualizing tree-structured regression and\nclassification models. This unified infrastructure can be used\nfor reading/coercing tree models from different sources\n('rpart', 'RWeka', 'PMML') yielding objects that share\nfunctionality for print()/plot()/predict() methods.\nFurthermore, new and improved reimplementations of conditional\ninference trees (ctree()) and model-based recursive\npartitioning (mob()) from the 'party' package are provided\nbased on the new infrastructure. A description of this package\nwas published by Hothorn and Zeileis (2015)\n<https://jmlr.org/papers/v16/hothorn15a.html>. as.constparty as.party as.party.rpart as.party.Weka_tree as.party.XMLNode as.partynode as.simpleparty as.simpleparty.XMLNode breaks_split cforest character_split ctree ctree_control data_party data_party.default edge_simple extree_data extree_fit fitted_node formatinfo_node get_paths gettree gettree.cforest glmtree id_node index_split info_node info_split is.constparty is.partynode is.simpleparty is.terminal kidids_node kidids_split kids_node lmtree mob mob_control model_frame_rpart node_barplot node_bivplot node_boxplot node_ecdf node_inner node_mvar node_party node_surv node_terminal nodeapply nodeids nodeprune nodeprune.party party partynode partysplit plot.modelparty plot.party pmmlTreeModel predict_party predict_party.default predict.cforest predict.modelparty predict.party print.modelparty print.party prob_split prune.lmtree prune.modelparty refit.modelparty right_split sctest.constparty sctest.modelparty split_node surrogates_node varid_split varimp varimp.cforest varimp.constparty width "
  },
  {
    "id": 864,
    "package_name": "nanonext",
    "title": "NNG (Nanomsg Next Gen) Lightweight Messaging Library",
    "description": "R binding for NNG (Nanomsg Next Gen), a successor to\nZeroMQ. NNG is a socket library for reliable, high-performance\nmessaging over in-process, IPC, TCP, WebSocket and secure TLS\ntransports. Implements 'Scalability Protocols', a standard for\ncommon communications patterns including publish/subscribe,\nrequest/reply and service discovery. As its own threaded\nconcurrency framework, provides a toolkit for asynchronous\nprogramming and distributed computing. Intuitive 'aio' objects\nresolve automatically when asynchronous operations complete,\nand synchronisation primitives allow R to wait upon events\nsignalled by concurrent threads.",
    "version": "1.7.2.9000",
    "maintainer": "Charlie Gao <charlie.gao@posit.co>",
    "author": "Charlie Gao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0750-061X>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>),\nHibiki AI Limited [cph],\nR Consortium [fnd]",
    "url": "https://nanonext.r-lib.org, https://github.com/r-lib/nanonext",
    "bug_reports": "https://github.com/r-lib/nanonext/issues",
    "repository": "",
    "exports": [
      [
        ".advance"
      ],
      [
        ".context"
      ],
      [
        ".keep"
      ],
      [
        ".mark"
      ],
      [
        ".read_header"
      ],
      [
        ".read_marker"
      ],
      [
        ".unresolved"
      ],
      [
        "%~>%"
      ],
      [
        "call_aio"
      ],
      [
        "call_aio_"
      ],
      [
        "collect_aio"
      ],
      [
        "collect_aio_"
      ],
      [
        "context"
      ],
      [
        "cv"
      ],
      [
        "cv_reset"
      ],
      [
        "cv_signal"
      ],
      [
        "cv_value"
      ],
      [
        "dial"
      ],
      [
        "ip_addr"
      ],
      [
        "is_aio"
      ],
      [
        "is_error_value"
      ],
      [
        "is_nano"
      ],
      [
        "is_ncurl_session"
      ],
      [
        "is_nul_byte"
      ],
      [
        "listen"
      ],
      [
        "mclock"
      ],
      [
        "messenger"
      ],
      [
        "monitor"
      ],
      [
        "msleep"
      ],
      [
        "nano"
      ],
      [
        "ncurl"
      ],
      [
        "ncurl_aio"
      ],
      [
        "ncurl_session"
      ],
      [
        "nng_error"
      ],
      [
        "nng_version"
      ],
      [
        "opt"
      ],
      [
        "opt<-"
      ],
      [
        "parse_url"
      ],
      [
        "pipe_id"
      ],
      [
        "pipe_notify"
      ],
      [
        "random"
      ],
      [
        "read_monitor"
      ],
      [
        "read_stdin"
      ],
      [
        "reap"
      ],
      [
        "recv"
      ],
      [
        "recv_aio"
      ],
      [
        "reply"
      ],
      [
        "request"
      ],
      [
        "send"
      ],
      [
        "send_aio"
      ],
      [
        "serial_config"
      ],
      [
        "socket"
      ],
      [
        "stat"
      ],
      [
        "status_code"
      ],
      [
        "stop_aio"
      ],
      [
        "stop_request"
      ],
      [
        "stream"
      ],
      [
        "subscribe"
      ],
      [
        "survey_time"
      ],
      [
        "tls_config"
      ],
      [
        "transact"
      ],
      [
        "unresolved"
      ],
      [
        "unsubscribe"
      ],
      [
        "until"
      ],
      [
        "until_"
      ],
      [
        "wait"
      ],
      [
        "wait_"
      ],
      [
        "write_cert"
      ],
      [
        "write_stdout"
      ]
    ],
    "topics": [
      [
        "concurrency"
      ],
      [
        "https"
      ],
      [
        "ipc-message"
      ],
      [
        "messaging-library"
      ],
      [
        "nng"
      ],
      [
        "rpc"
      ],
      [
        "socket-communication"
      ],
      [
        "synchronization-primitives"
      ],
      [
        "tcp-protocol"
      ],
      [
        "websocket"
      ],
      [
        "mbedtls"
      ]
    ],
    "score": 12.3829,
    "stars": 75,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "nanonext NNG (Nanomsg Next Gen) Lightweight Messaging Library R binding for NNG (Nanomsg Next Gen), a successor to\nZeroMQ. NNG is a socket library for reliable, high-performance\nmessaging over in-process, IPC, TCP, WebSocket and secure TLS\ntransports. Implements 'Scalability Protocols', a standard for\ncommon communications patterns including publish/subscribe,\nrequest/reply and service discovery. As its own threaded\nconcurrency framework, provides a toolkit for asynchronous\nprogramming and distributed computing. Intuitive 'aio' objects\nresolve automatically when asynchronous operations complete,\nand synchronisation primitives allow R to wait upon events\nsignalled by concurrent threads. .advance .context .keep .mark .read_header .read_marker .unresolved %~>% call_aio call_aio_ collect_aio collect_aio_ context cv cv_reset cv_signal cv_value dial ip_addr is_aio is_error_value is_nano is_ncurl_session is_nul_byte listen mclock messenger monitor msleep nano ncurl ncurl_aio ncurl_session nng_error nng_version opt opt<- parse_url pipe_id pipe_notify random read_monitor read_stdin reap recv recv_aio reply request send send_aio serial_config socket stat status_code stop_aio stop_request stream subscribe survey_time tls_config transact unresolved unsubscribe until until_ wait wait_ write_cert write_stdout concurrency https ipc-message messaging-library nng rpc socket-communication synchronization-primitives tcp-protocol websocket mbedtls"
  },
  {
    "id": 308,
    "package_name": "bonsai",
    "title": "Model Wrappers for Tree-Based Models",
    "description": "Bindings for additional tree-based model engines for use\nwith the 'parsnip' package. Models include gradient boosted\ndecision trees with 'LightGBM' (Ke et al, 2017.), conditional\ninference trees and conditional random forests with 'partykit'\n(Hothorn and Zeileis, 2015. and Hothorn et al, 2006.\n<doi:10.1198/106186006X133933>), and accelerated oblique random\nforests with 'aorsf' (Jaeger et al, 2022\n<doi:10.5281/zenodo.7116854>).",
    "version": "0.4.0.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Daniel Falbel [aut],\nAthos Damiani [aut],\nRoel M. Hogervorst [aut] (ORCID:\n<https://orcid.org/0000-0001-7509-0328>),\nMax Kuhn [aut] (ORCID: <https://orcid.org/0000-0003-2402-136X>),\nSimon Couch [aut] (ORCID: <https://orcid.org/0000-0001-5676-5107>),\nEmil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://bonsai.tidymodels.org/,\nhttps://github.com/tidymodels/bonsai",
    "bug_reports": "https://github.com/tidymodels/bonsai/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "predict_catboost_classification_class"
      ],
      [
        "predict_catboost_classification_prob"
      ],
      [
        "predict_catboost_classification_raw"
      ],
      [
        "predict_catboost_regression_numeric"
      ],
      [
        "predict_lightgbm_classification_class"
      ],
      [
        "predict_lightgbm_classification_prob"
      ],
      [
        "predict_lightgbm_classification_raw"
      ],
      [
        "predict_lightgbm_regression_numeric"
      ],
      [
        "train_catboost"
      ],
      [
        "train_lightgbm"
      ]
    ],
    "topics": [],
    "score": 11.5025,
    "stars": 53,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "bonsai Model Wrappers for Tree-Based Models Bindings for additional tree-based model engines for use\nwith the 'parsnip' package. Models include gradient boosted\ndecision trees with 'LightGBM' (Ke et al, 2017.), conditional\ninference trees and conditional random forests with 'partykit'\n(Hothorn and Zeileis, 2015. and Hothorn et al, 2006.\n<doi:10.1198/106186006X133933>), and accelerated oblique random\nforests with 'aorsf' (Jaeger et al, 2022\n<doi:10.5281/zenodo.7116854>). %>% predict_catboost_classification_class predict_catboost_classification_prob predict_catboost_classification_raw predict_catboost_regression_numeric predict_lightgbm_classification_class predict_lightgbm_classification_prob predict_lightgbm_classification_raw predict_lightgbm_regression_numeric train_catboost train_lightgbm "
  },
  {
    "id": 1184,
    "package_name": "scoringutils",
    "title": "Utilities for Scoring and Assessing Predictions",
    "description": "Facilitate the evaluation of forecasts in a convenient\nframework based on data.table. It allows user to to check their\nforecasts and diagnose issues, to visualise forecasts and\nmissing data, to transform data before scoring, to handle\nmissing forecasts, to aggregate scores, and to visualise the\nresults of the evaluation. The package mostly focuses on the\nevaluation of probabilistic forecasts and allows evaluating\nseveral different forecast types and input formats. Find more\ninformation about the package in the Vignettes as well as in\nthe accompanying paper, <doi:10.48550/arXiv.2205.07090>.",
    "version": "2.1.2",
    "maintainer": "Nikos Bosse <nikosbosse@gmail.com>",
    "author": "Nikos Bosse [aut, cre] (ORCID: <https://orcid.org/0000-0002-7750-5280>),\nSam Abbott [aut] (ORCID: <https://orcid.org/0000-0001-8057-8037>),\nHugo Gruson [aut] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nJohannes Bracher [ctb] (ORCID: <https://orcid.org/0000-0002-3777-1410>),\nToshiaki Asakura [ctb] (ORCID: <https://orcid.org/0000-0001-8838-785X>),\nJames Mba Azam [ctb] (ORCID: <https://orcid.org/0000-0001-5782-7330>),\nSebastian Funk [aut],\nMichael Chirico [ctb] (ORCID: <https://orcid.org/0000-0003-0787-087X>)",
    "url": "https://doi.org/10.48550/arXiv.2205.07090,\nhttps://epiforecasts.io/scoringutils/,\nhttps://github.com/epiforecasts/scoringutils",
    "bug_reports": "https://github.com/epiforecasts/scoringutils/issues",
    "repository": "",
    "exports": [
      [
        "add_relative_skill"
      ],
      [
        "ae_median_quantile"
      ],
      [
        "ae_median_sample"
      ],
      [
        "as_forecast_binary"
      ],
      [
        "as_forecast_nominal"
      ],
      [
        "as_forecast_ordinal"
      ],
      [
        "as_forecast_point"
      ],
      [
        "as_forecast_quantile"
      ],
      [
        "as_forecast_sample"
      ],
      [
        "assert_forecast"
      ],
      [
        "bias_quantile"
      ],
      [
        "bias_sample"
      ],
      [
        "brier_score"
      ],
      [
        "crps_sample"
      ],
      [
        "dispersion_quantile"
      ],
      [
        "dispersion_sample"
      ],
      [
        "dss_sample"
      ],
      [
        "get_correlations"
      ],
      [
        "get_coverage"
      ],
      [
        "get_duplicate_forecasts"
      ],
      [
        "get_forecast_counts"
      ],
      [
        "get_forecast_unit"
      ],
      [
        "get_metrics"
      ],
      [
        "get_pairwise_comparisons"
      ],
      [
        "get_pit_histogram"
      ],
      [
        "interval_coverage"
      ],
      [
        "is_forecast"
      ],
      [
        "is_forecast_binary"
      ],
      [
        "is_forecast_nominal"
      ],
      [
        "is_forecast_ordinal"
      ],
      [
        "is_forecast_point"
      ],
      [
        "is_forecast_quantile"
      ],
      [
        "is_forecast_sample"
      ],
      [
        "log_shift"
      ],
      [
        "logs_binary"
      ],
      [
        "logs_categorical"
      ],
      [
        "logs_sample"
      ],
      [
        "mad_sample"
      ],
      [
        "new_forecast"
      ],
      [
        "overprediction_quantile"
      ],
      [
        "overprediction_sample"
      ],
      [
        "pit_histogram_sample"
      ],
      [
        "plot_correlations"
      ],
      [
        "plot_forecast_counts"
      ],
      [
        "plot_heatmap"
      ],
      [
        "plot_interval_coverage"
      ],
      [
        "plot_pairwise_comparisons"
      ],
      [
        "plot_quantile_coverage"
      ],
      [
        "plot_wis"
      ],
      [
        "quantile_score"
      ],
      [
        "rps_ordinal"
      ],
      [
        "score"
      ],
      [
        "se_mean_sample"
      ],
      [
        "select_metrics"
      ],
      [
        "summarise_scores"
      ],
      [
        "summarize_scores"
      ],
      [
        "theme_scoringutils"
      ],
      [
        "transform_forecasts"
      ],
      [
        "underprediction_quantile"
      ],
      [
        "underprediction_sample"
      ],
      [
        "wis"
      ]
    ],
    "topics": [
      [
        "forecast-evaluation"
      ],
      [
        "forecasting"
      ]
    ],
    "score": 11.3713,
    "stars": 60,
    "primary_category": "epidemiology",
    "source_universe": "epiforecasts",
    "search_text": "scoringutils Utilities for Scoring and Assessing Predictions Facilitate the evaluation of forecasts in a convenient\nframework based on data.table. It allows user to to check their\nforecasts and diagnose issues, to visualise forecasts and\nmissing data, to transform data before scoring, to handle\nmissing forecasts, to aggregate scores, and to visualise the\nresults of the evaluation. The package mostly focuses on the\nevaluation of probabilistic forecasts and allows evaluating\nseveral different forecast types and input formats. Find more\ninformation about the package in the Vignettes as well as in\nthe accompanying paper, <doi:10.48550/arXiv.2205.07090>. add_relative_skill ae_median_quantile ae_median_sample as_forecast_binary as_forecast_nominal as_forecast_ordinal as_forecast_point as_forecast_quantile as_forecast_sample assert_forecast bias_quantile bias_sample brier_score crps_sample dispersion_quantile dispersion_sample dss_sample get_correlations get_coverage get_duplicate_forecasts get_forecast_counts get_forecast_unit get_metrics get_pairwise_comparisons get_pit_histogram interval_coverage is_forecast is_forecast_binary is_forecast_nominal is_forecast_ordinal is_forecast_point is_forecast_quantile is_forecast_sample log_shift logs_binary logs_categorical logs_sample mad_sample new_forecast overprediction_quantile overprediction_sample pit_histogram_sample plot_correlations plot_forecast_counts plot_heatmap plot_interval_coverage plot_pairwise_comparisons plot_quantile_coverage plot_wis quantile_score rps_ordinal score se_mean_sample select_metrics summarise_scores summarize_scores theme_scoringutils transform_forecasts underprediction_quantile underprediction_sample wis forecast-evaluation forecasting"
  },
  {
    "id": 494,
    "package_name": "drake",
    "title": "A Pipeline Toolkit for Reproducible Computation at Scale",
    "description": "A general-purpose computational engine for data analysis,\ndrake rebuilds intermediate data objects when their\ndependencies change, and it skips work when the results are\nalready up to date.  Not every execution starts from scratch,\nthere is native support for parallel and distributed computing,\nand completed projects have tangible evidence that they are\nreproducible.  Extensive documentation, from beginner-friendly\ntutorials to practical examples and more, is available at the\nreference website <https://docs.ropensci.org/drake/> and the\nonline manual <https://books.ropensci.org/drake/>.",
    "version": "7.13.11",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1878-3253>),\nAlex Axthelm [ctb],\nJasper Clarkberg [ctb],\nKirill M\u00fcller [ctb],\nBen Bond-Lamberty [ctb] (ORCID:\n<https://orcid.org/0000-0001-9525-4633>),\nTristan Mahr [ctb] (ORCID: <https://orcid.org/0000-0002-8890-5116>),\nMiles McBain [ctb] (ORCID: <https://orcid.org/0000-0003-2865-2548>),\nNoam Ross [ctb] (ORCID: <https://orcid.org/0000-0002-2136-0000>),\nEllis Hughes [ctb],\nMatthew Mark Strasiotto [ctb],\nBen Marwick [rev],\nPeter Slaughter [rev],\nEli Lilly and Company [cph]",
    "url": "https://github.com/ropensci/drake,\nhttps://docs.ropensci.org/drake/,\nhttps://books.ropensci.org/drake/",
    "bug_reports": "https://github.com/ropensci/drake/issues",
    "repository": "",
    "exports": [
      [
        "all_of"
      ],
      [
        "analyses"
      ],
      [
        "analysis_wildcard"
      ],
      [
        "any_of"
      ],
      [
        "as_drake_filename"
      ],
      [
        "as_file"
      ],
      [
        "available_hash_algos"
      ],
      [
        "backend"
      ],
      [
        "bind_plans"
      ],
      [
        "build_drake_graph"
      ],
      [
        "build_graph"
      ],
      [
        "build_times"
      ],
      [
        "built"
      ],
      [
        "cache_namespaces"
      ],
      [
        "cache_path"
      ],
      [
        "cached"
      ],
      [
        "cached_planned"
      ],
      [
        "cached_unplanned"
      ],
      [
        "cancel"
      ],
      [
        "cancel_if"
      ],
      [
        "check"
      ],
      [
        "check_plan"
      ],
      [
        "clean"
      ],
      [
        "clean_main_example"
      ],
      [
        "clean_mtcars_example"
      ],
      [
        "cleaned_namespaces"
      ],
      [
        "cmq_build"
      ],
      [
        "code_to_function"
      ],
      [
        "code_to_plan"
      ],
      [
        "config"
      ],
      [
        "configure_cache"
      ],
      [
        "contains"
      ],
      [
        "dataframes_graph"
      ],
      [
        "dataset_wildcard"
      ],
      [
        "debug_and_run"
      ],
      [
        "default_graph_title"
      ],
      [
        "default_long_hash_algo"
      ],
      [
        "default_Makefile_args"
      ],
      [
        "default_Makefile_command"
      ],
      [
        "default_parallelism"
      ],
      [
        "default_recipe_command"
      ],
      [
        "default_short_hash_algo"
      ],
      [
        "default_system2_args"
      ],
      [
        "default_verbose"
      ],
      [
        "dependency_profile"
      ],
      [
        "deprecate_wildcard"
      ],
      [
        "deps"
      ],
      [
        "deps_code"
      ],
      [
        "deps_knitr"
      ],
      [
        "deps_profile"
      ],
      [
        "deps_profile_impl"
      ],
      [
        "deps_target"
      ],
      [
        "deps_target_impl"
      ],
      [
        "deps_targets"
      ],
      [
        "diagnose"
      ],
      [
        "do_prework"
      ],
      [
        "doc_of_function_call"
      ],
      [
        "drake_batchtools_tmpl_file"
      ],
      [
        "drake_build"
      ],
      [
        "drake_build_impl"
      ],
      [
        "drake_cache"
      ],
      [
        "drake_cache_log"
      ],
      [
        "drake_cache_log_file"
      ],
      [
        "drake_cancelled"
      ],
      [
        "drake_config"
      ],
      [
        "drake_debug"
      ],
      [
        "drake_done"
      ],
      [
        "drake_envir"
      ],
      [
        "drake_example"
      ],
      [
        "drake_examples"
      ],
      [
        "drake_failed"
      ],
      [
        "drake_gc"
      ],
      [
        "drake_get_session_info"
      ],
      [
        "drake_ggraph"
      ],
      [
        "drake_ggraph_impl"
      ],
      [
        "drake_graph_info"
      ],
      [
        "drake_graph_info_impl"
      ],
      [
        "drake_history"
      ],
      [
        "drake_hpc_template_file"
      ],
      [
        "drake_hpc_template_files"
      ],
      [
        "drake_meta"
      ],
      [
        "drake_palette"
      ],
      [
        "drake_plan"
      ],
      [
        "drake_plan_source"
      ],
      [
        "drake_progress"
      ],
      [
        "drake_quotes"
      ],
      [
        "drake_running"
      ],
      [
        "drake_script"
      ],
      [
        "drake_session"
      ],
      [
        "drake_slice"
      ],
      [
        "drake_strings"
      ],
      [
        "drake_tempfile"
      ],
      [
        "drake_tip"
      ],
      [
        "drake_unquote"
      ],
      [
        "ends_with"
      ],
      [
        "evaluate"
      ],
      [
        "evaluate_plan"
      ],
      [
        "everything"
      ],
      [
        "example_drake"
      ],
      [
        "examples_drake"
      ],
      [
        "expand"
      ],
      [
        "expand_plan"
      ],
      [
        "expose_imports"
      ],
      [
        "failed"
      ],
      [
        "file_in"
      ],
      [
        "file_out"
      ],
      [
        "file_store"
      ],
      [
        "find_cache"
      ],
      [
        "find_knitr_doc"
      ],
      [
        "find_project"
      ],
      [
        "from_plan"
      ],
      [
        "future_build"
      ],
      [
        "gather"
      ],
      [
        "gather_by"
      ],
      [
        "gather_plan"
      ],
      [
        "get_cache"
      ],
      [
        "get_trace"
      ],
      [
        "id_chr"
      ],
      [
        "ignore"
      ],
      [
        "imported"
      ],
      [
        "in_progress"
      ],
      [
        "is_function_call"
      ],
      [
        "isolate_example"
      ],
      [
        "knitr_deps"
      ],
      [
        "knitr_in"
      ],
      [
        "last_col"
      ],
      [
        "legend_nodes"
      ],
      [
        "load_basic_example"
      ],
      [
        "load_main_example"
      ],
      [
        "load_mtcars_example"
      ],
      [
        "loadd"
      ],
      [
        "long_hash"
      ],
      [
        "make"
      ],
      [
        "make_impl"
      ],
      [
        "make_imports"
      ],
      [
        "make_targets"
      ],
      [
        "make_with_config"
      ],
      [
        "Makefile_recipe"
      ],
      [
        "manage_memory"
      ],
      [
        "map_plan"
      ],
      [
        "matches"
      ],
      [
        "max_useful_jobs"
      ],
      [
        "migrate_drake_project"
      ],
      [
        "missed"
      ],
      [
        "missed_impl"
      ],
      [
        "new_cache"
      ],
      [
        "no_deps"
      ],
      [
        "num_range"
      ],
      [
        "one_of"
      ],
      [
        "outdated"
      ],
      [
        "outdated_impl"
      ],
      [
        "parallel_stages"
      ],
      [
        "parallelism_choices"
      ],
      [
        "plan"
      ],
      [
        "plan_analyses"
      ],
      [
        "plan_drake"
      ],
      [
        "plan_summaries"
      ],
      [
        "plan_to_code"
      ],
      [
        "plan_to_notebook"
      ],
      [
        "plot_graph"
      ],
      [
        "predict_load_balancing"
      ],
      [
        "predict_runtime"
      ],
      [
        "predict_runtime_impl"
      ],
      [
        "predict_workers"
      ],
      [
        "predict_workers_impl"
      ],
      [
        "process_import"
      ],
      [
        "progress"
      ],
      [
        "prune_drake_graph"
      ],
      [
        "r_deps_target"
      ],
      [
        "r_drake_build"
      ],
      [
        "r_drake_ggraph"
      ],
      [
        "r_drake_graph_info"
      ],
      [
        "r_make"
      ],
      [
        "r_missed"
      ],
      [
        "r_outdated"
      ],
      [
        "r_predict_runtime"
      ],
      [
        "r_predict_workers"
      ],
      [
        "r_recipe_wildcard"
      ],
      [
        "r_recoverable"
      ],
      [
        "r_sankey_drake_graph"
      ],
      [
        "r_text_drake_graph"
      ],
      [
        "r_vis_drake_graph"
      ],
      [
        "rate_limiting_times"
      ],
      [
        "read_config"
      ],
      [
        "read_drake_config"
      ],
      [
        "read_drake_graph"
      ],
      [
        "read_drake_meta"
      ],
      [
        "read_drake_plan"
      ],
      [
        "read_drake_seed"
      ],
      [
        "read_graph"
      ],
      [
        "read_plan"
      ],
      [
        "read_trace"
      ],
      [
        "readd"
      ],
      [
        "recover_cache"
      ],
      [
        "recoverable"
      ],
      [
        "recoverable_impl"
      ],
      [
        "reduce_by"
      ],
      [
        "reduce_plan"
      ],
      [
        "render_drake_ggraph"
      ],
      [
        "render_drake_graph"
      ],
      [
        "render_graph"
      ],
      [
        "render_sankey_drake_graph"
      ],
      [
        "render_static_drake_graph"
      ],
      [
        "render_text_drake_graph"
      ],
      [
        "rescue_cache"
      ],
      [
        "rs_addin_loadd"
      ],
      [
        "rs_addin_r_make"
      ],
      [
        "rs_addin_r_outdated"
      ],
      [
        "rs_addin_r_vis_drake_graph"
      ],
      [
        "running"
      ],
      [
        "sankey_drake_graph"
      ],
      [
        "sankey_drake_graph_impl"
      ],
      [
        "session"
      ],
      [
        "shell_file"
      ],
      [
        "short_hash"
      ],
      [
        "show_source"
      ],
      [
        "starts_with"
      ],
      [
        "static_drake_graph"
      ],
      [
        "subtargets"
      ],
      [
        "summaries"
      ],
      [
        "target"
      ],
      [
        "target_namespaces"
      ],
      [
        "text_drake_graph"
      ],
      [
        "text_drake_graph_impl"
      ],
      [
        "this_cache"
      ],
      [
        "tracked"
      ],
      [
        "transform_plan"
      ],
      [
        "trigger"
      ],
      [
        "triggers"
      ],
      [
        "type_sum.expr_list"
      ],
      [
        "use_drake"
      ],
      [
        "vis_drake_graph"
      ],
      [
        "vis_drake_graph_impl"
      ],
      [
        "which_clean"
      ],
      [
        "workflow"
      ],
      [
        "workplan"
      ]
    ],
    "topics": [
      [
        "data-science"
      ],
      [
        "drake"
      ],
      [
        "high-performance-computing"
      ],
      [
        "makefile"
      ],
      [
        "peer-reviewed"
      ],
      [
        "pipeline"
      ],
      [
        "reproducibility"
      ],
      [
        "reproducible-research"
      ],
      [
        "ropensci"
      ],
      [
        "workflow"
      ]
    ],
    "score": 11.2444,
    "stars": 1341,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "drake A Pipeline Toolkit for Reproducible Computation at Scale A general-purpose computational engine for data analysis,\ndrake rebuilds intermediate data objects when their\ndependencies change, and it skips work when the results are\nalready up to date.  Not every execution starts from scratch,\nthere is native support for parallel and distributed computing,\nand completed projects have tangible evidence that they are\nreproducible.  Extensive documentation, from beginner-friendly\ntutorials to practical examples and more, is available at the\nreference website <https://docs.ropensci.org/drake/> and the\nonline manual <https://books.ropensci.org/drake/>. all_of analyses analysis_wildcard any_of as_drake_filename as_file available_hash_algos backend bind_plans build_drake_graph build_graph build_times built cache_namespaces cache_path cached cached_planned cached_unplanned cancel cancel_if check check_plan clean clean_main_example clean_mtcars_example cleaned_namespaces cmq_build code_to_function code_to_plan config configure_cache contains dataframes_graph dataset_wildcard debug_and_run default_graph_title default_long_hash_algo default_Makefile_args default_Makefile_command default_parallelism default_recipe_command default_short_hash_algo default_system2_args default_verbose dependency_profile deprecate_wildcard deps deps_code deps_knitr deps_profile deps_profile_impl deps_target deps_target_impl deps_targets diagnose do_prework doc_of_function_call drake_batchtools_tmpl_file drake_build drake_build_impl drake_cache drake_cache_log drake_cache_log_file drake_cancelled drake_config drake_debug drake_done drake_envir drake_example drake_examples drake_failed drake_gc drake_get_session_info drake_ggraph drake_ggraph_impl drake_graph_info drake_graph_info_impl drake_history drake_hpc_template_file drake_hpc_template_files drake_meta drake_palette drake_plan drake_plan_source drake_progress drake_quotes drake_running drake_script drake_session drake_slice drake_strings drake_tempfile drake_tip drake_unquote ends_with evaluate evaluate_plan everything example_drake examples_drake expand expand_plan expose_imports failed file_in file_out file_store find_cache find_knitr_doc find_project from_plan future_build gather gather_by gather_plan get_cache get_trace id_chr ignore imported in_progress is_function_call isolate_example knitr_deps knitr_in last_col legend_nodes load_basic_example load_main_example load_mtcars_example loadd long_hash make make_impl make_imports make_targets make_with_config Makefile_recipe manage_memory map_plan matches max_useful_jobs migrate_drake_project missed missed_impl new_cache no_deps num_range one_of outdated outdated_impl parallel_stages parallelism_choices plan plan_analyses plan_drake plan_summaries plan_to_code plan_to_notebook plot_graph predict_load_balancing predict_runtime predict_runtime_impl predict_workers predict_workers_impl process_import progress prune_drake_graph r_deps_target r_drake_build r_drake_ggraph r_drake_graph_info r_make r_missed r_outdated r_predict_runtime r_predict_workers r_recipe_wildcard r_recoverable r_sankey_drake_graph r_text_drake_graph r_vis_drake_graph rate_limiting_times read_config read_drake_config read_drake_graph read_drake_meta read_drake_plan read_drake_seed read_graph read_plan read_trace readd recover_cache recoverable recoverable_impl reduce_by reduce_plan render_drake_ggraph render_drake_graph render_graph render_sankey_drake_graph render_static_drake_graph render_text_drake_graph rescue_cache rs_addin_loadd rs_addin_r_make rs_addin_r_outdated rs_addin_r_vis_drake_graph running sankey_drake_graph sankey_drake_graph_impl session shell_file short_hash show_source starts_with static_drake_graph subtargets summaries target target_namespaces text_drake_graph text_drake_graph_impl this_cache tracked transform_plan trigger triggers type_sum.expr_list use_drake vis_drake_graph vis_drake_graph_impl which_clean workflow workplan data-science drake high-performance-computing makefile peer-reviewed pipeline reproducibility reproducible-research ropensci workflow"
  },
  {
    "id": 746,
    "package_name": "leafgl",
    "title": "High-Performance 'WebGl' Rendering for Package 'leaflet'",
    "description": "Provides bindings to the 'Leaflet.glify' JavaScript\nlibrary which extends the 'leaflet' JavaScript library to\nrender large data in the browser using 'WebGl'.",
    "version": "0.2.2",
    "maintainer": "Tim Appelhans <tim.appelhans@gmail.com>",
    "author": "Tim Appelhans [cre, aut, cph],\nColin Fay [ctb] (ORCID: <https://orcid.org/0000-0001-7343-1846>),\nRobert Plummer [ctb] (Leaflet.glify plugin),\nKent Johnson [ctb],\nSebastian Gatscha [ctb],\nOlivier Roy [ctb]",
    "url": "https://github.com/r-spatial/leafgl,\nhttps://r-spatial.github.io/leafgl/",
    "bug_reports": "https://github.com/r-spatial/leafgl/issues",
    "repository": "",
    "exports": [
      [
        "addGlPoints"
      ],
      [
        "addGlPolygons"
      ],
      [
        "addGlPolylines"
      ],
      [
        "clearGlGroup"
      ],
      [
        "clearGlLayers"
      ],
      [
        "leafglOutput"
      ],
      [
        "makeColorMatrix"
      ],
      [
        "makePopup"
      ],
      [
        "removeGlPoints"
      ],
      [
        "removeGlPolygons"
      ],
      [
        "removeGlPolylines"
      ],
      [
        "renderLeafgl"
      ]
    ],
    "topics": [],
    "score": 10.9782,
    "stars": 285,
    "primary_category": "spatial",
    "source_universe": "r-spatial",
    "search_text": "leafgl High-Performance 'WebGl' Rendering for Package 'leaflet' Provides bindings to the 'Leaflet.glify' JavaScript\nlibrary which extends the 'leaflet' JavaScript library to\nrender large data in the browser using 'WebGl'. addGlPoints addGlPolygons addGlPolylines clearGlGroup clearGlLayers leafglOutput makeColorMatrix makePopup removeGlPoints removeGlPolygons removeGlPolylines renderLeafgl "
  },
  {
    "id": 1390,
    "package_name": "tsbox",
    "title": "Class-Agnostic Time Series",
    "description": "Time series toolkit with identical behavior for all time\nseries classes: 'ts','xts', 'data.frame', 'data.table',\n'tibble', 'zoo', 'timeSeries', 'tsibble', 'tis' or 'irts'. Also\nconverts reliably between these classes.",
    "version": "0.4.2",
    "maintainer": "Christoph Sax <christoph.sax@gmail.com>",
    "author": "Christoph Sax [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7192-7044>),\nCathy Chamberlin [rev],\nNunes Matt [rev]",
    "url": "https://docs.ropensci.org/tsbox/,\nhttps://github.com/ropensci/tsbox",
    "bug_reports": "https://github.com/ropensci/tsbox/issues",
    "repository": "",
    "exports": [
      [
        "%ts-%"
      ],
      [
        "%ts*%"
      ],
      [
        "%ts/%"
      ],
      [
        "%ts+%"
      ],
      [
        "check_ts_boxable"
      ],
      [
        "colors_tsbox"
      ],
      [
        "copy_class"
      ],
      [
        "load_suggested"
      ],
      [
        "relevant_class"
      ],
      [
        "scale_color_tsbox"
      ],
      [
        "scale_fill_tsbox"
      ],
      [
        "theme_tsbox"
      ],
      [
        "ts_"
      ],
      [
        "ts_apply"
      ],
      [
        "ts_bind"
      ],
      [
        "ts_boxable"
      ],
      [
        "ts_c"
      ],
      [
        "ts_chain"
      ],
      [
        "ts_compound"
      ],
      [
        "ts_data.frame"
      ],
      [
        "ts_data.table"
      ],
      [
        "ts_default"
      ],
      [
        "ts_df"
      ],
      [
        "ts_diff"
      ],
      [
        "ts_diffy"
      ],
      [
        "ts_dt"
      ],
      [
        "ts_dts"
      ],
      [
        "ts_dygraphs"
      ],
      [
        "ts_end"
      ],
      [
        "ts_first_of_period"
      ],
      [
        "ts_forecast"
      ],
      [
        "ts_frequency"
      ],
      [
        "ts_ggplot"
      ],
      [
        "ts_index"
      ],
      [
        "ts_irts"
      ],
      [
        "ts_lag"
      ],
      [
        "ts_long"
      ],
      [
        "ts_na_interpolation"
      ],
      [
        "ts_na_omit"
      ],
      [
        "ts_pc"
      ],
      [
        "ts_pca"
      ],
      [
        "ts_pcy"
      ],
      [
        "ts_pick"
      ],
      [
        "ts_plot"
      ],
      [
        "ts_prcomp"
      ],
      [
        "ts_regular"
      ],
      [
        "ts_save"
      ],
      [
        "ts_scale"
      ],
      [
        "ts_seas"
      ],
      [
        "ts_span"
      ],
      [
        "ts_start"
      ],
      [
        "ts_summary"
      ],
      [
        "ts_tbl"
      ],
      [
        "ts_tibbletime"
      ],
      [
        "ts_timeSeries"
      ],
      [
        "ts_tis"
      ],
      [
        "ts_trend"
      ],
      [
        "ts_ts"
      ],
      [
        "ts_tsibble"
      ],
      [
        "ts_tslist"
      ],
      [
        "ts_wide"
      ],
      [
        "ts_xts"
      ],
      [
        "ts_zoo"
      ],
      [
        "ts_zooreg"
      ]
    ],
    "topics": [
      [
        "graphics"
      ],
      [
        "time-series"
      ]
    ],
    "score": 10.5974,
    "stars": 149,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tsbox Class-Agnostic Time Series Time series toolkit with identical behavior for all time\nseries classes: 'ts','xts', 'data.frame', 'data.table',\n'tibble', 'zoo', 'timeSeries', 'tsibble', 'tis' or 'irts'. Also\nconverts reliably between these classes. %ts-% %ts*% %ts/% %ts+% check_ts_boxable colors_tsbox copy_class load_suggested relevant_class scale_color_tsbox scale_fill_tsbox theme_tsbox ts_ ts_apply ts_bind ts_boxable ts_c ts_chain ts_compound ts_data.frame ts_data.table ts_default ts_df ts_diff ts_diffy ts_dt ts_dts ts_dygraphs ts_end ts_first_of_period ts_forecast ts_frequency ts_ggplot ts_index ts_irts ts_lag ts_long ts_na_interpolation ts_na_omit ts_pc ts_pca ts_pcy ts_pick ts_plot ts_prcomp ts_regular ts_save ts_scale ts_seas ts_span ts_start ts_summary ts_tbl ts_tibbletime ts_timeSeries ts_tis ts_trend ts_ts ts_tsibble ts_tslist ts_wide ts_xts ts_zoo ts_zooreg graphics time-series"
  },
  {
    "id": 1230,
    "package_name": "sits",
    "title": "Satellite Image Time Series Analysis for Earth Observation Data\nCubes",
    "description": "An end-to-end toolkit for land use and land cover\nclassification using big Earth observation data. Builds\nsatellite image data cubes from cloud collections. Supports\nvisualization methods for images and time series and smoothing\nfilters for dealing with noisy time series. Enables merging of\nmulti-source imagery (SAR, optical, DEM). Includes functions\nfor quality assessment of training samples using self-organized\nmaps and to reduce training samples imbalance. Provides machine\nlearning algorithms including support vector machines, random\nforests, extreme gradient boosting, multi-layer perceptrons,\ntemporal convolution neural networks, and temporal attention\nencoders. Performs efficient classification of big Earth\nobservation data cubes and includes functions for\npost-classification smoothing based on Bayesian inference.\nEnables best practices for estimating area and assessing\naccuracy of land change. Includes object-based spatio-temporal\nsegmentation for space-time OBIA. Minimum recommended\nrequirements: 16 GB RAM and 4 CPU dual-core.",
    "version": "1.5.3-1",
    "maintainer": "Gilberto Camara <gilberto.camara.inpe@gmail.com>",
    "author": "Rolf Simoes [aut],\nGilberto Camara [aut, cre, ths],\nFelipe Souza [aut],\nFelipe Carlos [aut],\nLorena Santos [ctb],\nCharlotte Pelletier [ctb],\nEstefania Pizarro [ctb],\nKarine Ferreira [ctb, ths],\nAlber Sanchez [ctb],\nAlexandre Assuncao [ctb],\nDaniel Falbel [ctb],\nGilberto Queiroz [ctb],\nJohannes Reiche [ctb],\nPedro Andrade [ctb],\nPedro Brito [ctb],\nRenato Assuncao [ctb],\nRicardo Cartaxo [ctb]",
    "url": "https://github.com/e-sensing/sits/,\nhttps://e-sensing.github.io/sitsbook/,\nhttps://e-sensing.github.io/sits/",
    "bug_reports": "https://github.com/e-sensing/sits/issues",
    "repository": "",
    "exports": [
      [
        "impute_linear"
      ],
      [
        "sits_accuracy"
      ],
      [
        "sits_accuracy_summary"
      ],
      [
        "sits_add_base_cube"
      ],
      [
        "sits_apply"
      ],
      [
        "sits_as_sf"
      ],
      [
        "sits_as_stars"
      ],
      [
        "sits_as_terra"
      ],
      [
        "sits_bands"
      ],
      [
        "sits_bands<-"
      ],
      [
        "sits_bbox"
      ],
      [
        "sits_classify"
      ],
      [
        "sits_clean"
      ],
      [
        "sits_cluster_clean"
      ],
      [
        "sits_cluster_dendro"
      ],
      [
        "sits_cluster_frequency"
      ],
      [
        "sits_colors"
      ],
      [
        "sits_colors_qgis"
      ],
      [
        "sits_colors_reset"
      ],
      [
        "sits_colors_set"
      ],
      [
        "sits_colors_show"
      ],
      [
        "sits_combine_predictions"
      ],
      [
        "sits_confidence_sampling"
      ],
      [
        "sits_config"
      ],
      [
        "sits_config_show"
      ],
      [
        "sits_config_user_file"
      ],
      [
        "sits_cube"
      ],
      [
        "sits_cube_copy"
      ],
      [
        "sits_factory_function"
      ],
      [
        "sits_filter"
      ],
      [
        "sits_formula_linear"
      ],
      [
        "sits_formula_logref"
      ],
      [
        "sits_geo_dist"
      ],
      [
        "sits_get_class"
      ],
      [
        "sits_get_data"
      ],
      [
        "sits_get_probs"
      ],
      [
        "sits_impute"
      ],
      [
        "sits_kfold_validate"
      ],
      [
        "sits_label_classification"
      ],
      [
        "sits_labels"
      ],
      [
        "sits_labels_summary"
      ],
      [
        "sits_labels<-"
      ],
      [
        "sits_lightgbm"
      ],
      [
        "sits_lighttae"
      ],
      [
        "sits_list_collections"
      ],
      [
        "sits_merge"
      ],
      [
        "sits_mgrs_to_roi"
      ],
      [
        "sits_mixture_model"
      ],
      [
        "sits_mlp"
      ],
      [
        "sits_model_export"
      ],
      [
        "sits_mosaic"
      ],
      [
        "sits_patterns"
      ],
      [
        "sits_pred_features"
      ],
      [
        "sits_pred_normalize"
      ],
      [
        "sits_pred_references"
      ],
      [
        "sits_pred_sample"
      ],
      [
        "sits_predictors"
      ],
      [
        "sits_reclassify"
      ],
      [
        "sits_reduce"
      ],
      [
        "sits_reduce_imbalance"
      ],
      [
        "sits_regularize"
      ],
      [
        "sits_resnet"
      ],
      [
        "sits_rfor"
      ],
      [
        "sits_roi_to_mgrs"
      ],
      [
        "sits_roi_to_tiles"
      ],
      [
        "sits_run_examples"
      ],
      [
        "sits_run_tests"
      ],
      [
        "sits_sample"
      ],
      [
        "sits_sampling_design"
      ],
      [
        "sits_segment"
      ],
      [
        "sits_select"
      ],
      [
        "sits_sgolay"
      ],
      [
        "sits_show_prediction"
      ],
      [
        "sits_slic"
      ],
      [
        "sits_smooth"
      ],
      [
        "sits_som_clean_samples"
      ],
      [
        "sits_som_evaluate_cluster"
      ],
      [
        "sits_som_map"
      ],
      [
        "sits_som_remove_samples"
      ],
      [
        "sits_stats"
      ],
      [
        "sits_stratified_sampling"
      ],
      [
        "sits_svm"
      ],
      [
        "sits_tae"
      ],
      [
        "sits_tempcnn"
      ],
      [
        "sits_texture"
      ],
      [
        "sits_tiles_to_roi"
      ],
      [
        "sits_timeline"
      ],
      [
        "sits_timeseries_to_csv"
      ],
      [
        "sits_to_csv"
      ],
      [
        "sits_to_xlsx"
      ],
      [
        "sits_train"
      ],
      [
        "sits_tuning"
      ],
      [
        "sits_tuning_hparams"
      ],
      [
        "sits_uncertainty"
      ],
      [
        "sits_uncertainty_sampling"
      ],
      [
        "sits_validate"
      ],
      [
        "sits_variance"
      ],
      [
        "sits_view"
      ],
      [
        "sits_whittaker"
      ],
      [
        "sits_xgboost"
      ]
    ],
    "topics": [
      [
        "big-earth-data"
      ],
      [
        "cbers"
      ],
      [
        "earth-observation"
      ],
      [
        "eo-datacubes"
      ],
      [
        "geospatial"
      ],
      [
        "image-time-series"
      ],
      [
        "land-cover-classification"
      ],
      [
        "landsat"
      ],
      [
        "planetary-computer"
      ],
      [
        "r-spatial"
      ],
      [
        "remote-sensing"
      ],
      [
        "rspatial"
      ],
      [
        "satellite-image-time-series"
      ],
      [
        "satellite-imagery"
      ],
      [
        "sentinel-2"
      ],
      [
        "stac-api"
      ],
      [
        "stac-catalog"
      ],
      [
        "openblas"
      ],
      [
        "cpp"
      ],
      [
        "openmp"
      ]
    ],
    "score": 9.1713,
    "stars": 521,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "sits Satellite Image Time Series Analysis for Earth Observation Data\nCubes An end-to-end toolkit for land use and land cover\nclassification using big Earth observation data. Builds\nsatellite image data cubes from cloud collections. Supports\nvisualization methods for images and time series and smoothing\nfilters for dealing with noisy time series. Enables merging of\nmulti-source imagery (SAR, optical, DEM). Includes functions\nfor quality assessment of training samples using self-organized\nmaps and to reduce training samples imbalance. Provides machine\nlearning algorithms including support vector machines, random\nforests, extreme gradient boosting, multi-layer perceptrons,\ntemporal convolution neural networks, and temporal attention\nencoders. Performs efficient classification of big Earth\nobservation data cubes and includes functions for\npost-classification smoothing based on Bayesian inference.\nEnables best practices for estimating area and assessing\naccuracy of land change. Includes object-based spatio-temporal\nsegmentation for space-time OBIA. Minimum recommended\nrequirements: 16 GB RAM and 4 CPU dual-core. impute_linear sits_accuracy sits_accuracy_summary sits_add_base_cube sits_apply sits_as_sf sits_as_stars sits_as_terra sits_bands sits_bands<- sits_bbox sits_classify sits_clean sits_cluster_clean sits_cluster_dendro sits_cluster_frequency sits_colors sits_colors_qgis sits_colors_reset sits_colors_set sits_colors_show sits_combine_predictions sits_confidence_sampling sits_config sits_config_show sits_config_user_file sits_cube sits_cube_copy sits_factory_function sits_filter sits_formula_linear sits_formula_logref sits_geo_dist sits_get_class sits_get_data sits_get_probs sits_impute sits_kfold_validate sits_label_classification sits_labels sits_labels_summary sits_labels<- sits_lightgbm sits_lighttae sits_list_collections sits_merge sits_mgrs_to_roi sits_mixture_model sits_mlp sits_model_export sits_mosaic sits_patterns sits_pred_features sits_pred_normalize sits_pred_references sits_pred_sample sits_predictors sits_reclassify sits_reduce sits_reduce_imbalance sits_regularize sits_resnet sits_rfor sits_roi_to_mgrs sits_roi_to_tiles sits_run_examples sits_run_tests sits_sample sits_sampling_design sits_segment sits_select sits_sgolay sits_show_prediction sits_slic sits_smooth sits_som_clean_samples sits_som_evaluate_cluster sits_som_map sits_som_remove_samples sits_stats sits_stratified_sampling sits_svm sits_tae sits_tempcnn sits_texture sits_tiles_to_roi sits_timeline sits_timeseries_to_csv sits_to_csv sits_to_xlsx sits_train sits_tuning sits_tuning_hparams sits_uncertainty sits_uncertainty_sampling sits_validate sits_variance sits_view sits_whittaker sits_xgboost big-earth-data cbers earth-observation eo-datacubes geospatial image-time-series land-cover-classification landsat planetary-computer r-spatial remote-sensing rspatial satellite-image-time-series satellite-imagery sentinel-2 stac-api stac-catalog openblas cpp openmp"
  },
  {
    "id": 1305,
    "package_name": "taxadb",
    "title": "A High-Performance Local Taxonomic Database Interface",
    "description": "Creates a local database of many commonly used taxonomic\nauthorities and provides functions that can quickly query this\ndata.",
    "version": "0.2.1.99",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nKari Norman [aut] (ORCID: <https://orcid.org/0000-0002-2029-2325>),\nJorrit Poelen [aut] (ORCID: <https://orcid.org/0000-0003-3138-4118>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nNoam Ross [ctb] (ORCID: <https://orcid.org/0000-0002-2136-0000>),\nMattia Ghilardi [ctb] (ORCID: <https://orcid.org/0000-0001-9592-7252>)",
    "url": "<https://docs.ropensci.org/taxadb/>,\n<https://github.com/ropensci/taxadb>",
    "bug_reports": "https://github.com/ropensci/taxadb/issues",
    "repository": "",
    "exports": [
      [
        "clean_names"
      ],
      [
        "common_contains"
      ],
      [
        "common_starts_with"
      ],
      [
        "filter_by"
      ],
      [
        "filter_common"
      ],
      [
        "filter_id"
      ],
      [
        "filter_name"
      ],
      [
        "filter_rank"
      ],
      [
        "fuzzy_filter"
      ],
      [
        "get_ids"
      ],
      [
        "get_names"
      ],
      [
        "name_contains"
      ],
      [
        "name_starts_with"
      ],
      [
        "taxa_tbl"
      ],
      [
        "taxadb_dir"
      ],
      [
        "td_connect"
      ],
      [
        "td_create"
      ],
      [
        "td_disconnect"
      ],
      [
        "tl_import"
      ]
    ],
    "topics": [],
    "score": 8.2336,
    "stars": 43,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxadb A High-Performance Local Taxonomic Database Interface Creates a local database of many commonly used taxonomic\nauthorities and provides functions that can quickly query this\ndata. clean_names common_contains common_starts_with filter_by filter_common filter_id filter_name filter_rank fuzzy_filter get_ids get_names name_contains name_starts_with taxa_tbl taxadb_dir td_connect td_create td_disconnect tl_import "
  },
  {
    "id": 166,
    "package_name": "Rglpk",
    "title": "R/GNU Linear Programming Kit Interface",
    "description": "R interface to the GNU Linear Programming Kit. 'GLPK' is\nopen source software for solving large-scale linear programming\n(LP), mixed integer linear programming ('MILP') and other\nrelated problems.",
    "version": "0.6-5.1",
    "maintainer": "Stefan Theussl <Stefan.Theussl@R-project.org>",
    "author": "Stefan Theussl [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6523-4620>),\nKurt Hornik [aut] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\nChristian Buchta [ctb],\nFlorian Schwendinger [ctb] (ORCID:\n<https://orcid.org/0000-0002-3983-9773>),\nHeinrich Schuchardt [ctb]",
    "url": "https://R-Forge.R-project.org/projects/rglp/,\nhttps://www.gnu.org/software/glpk/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "Rglpk_read_file"
      ],
      [
        "Rglpk_solve_LP"
      ]
    ],
    "topics": [
      [
        "glpk"
      ]
    ],
    "score": 7.6784,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "Rglpk R/GNU Linear Programming Kit Interface R interface to the GNU Linear Programming Kit. 'GLPK' is\nopen source software for solving large-scale linear programming\n(LP), mixed integer linear programming ('MILP') and other\nrelated problems. Rglpk_read_file Rglpk_solve_LP glpk"
  },
  {
    "id": 1272,
    "package_name": "stantargets",
    "title": "Targets for Stan Workflows",
    "description": "Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'stantargets' R package leverages 'targets'\nand 'cmdstanr' to ease these burdens. 'stantargets' makes it\nsuper easy to set up scalable Stan pipelines that automatically\nparallelize the computation and skip expensive steps when the\nresults are already up to date. Minimal custom code is\nrequired, and there is no need to manually configure branching,\nso usage is much easier than 'targets' alone. 'stantargets' can\naccess all of 'cmdstanr''s major algorithms (MCMC, variational\nBayes, and optimization) and it supports both single-fit\nworkflows and multi-rep simulation studies. For the statistical\nmethodology, please refer to 'Stan' documentation (Stan\nDevelopment Team 2020) <https://mc-stan.org/>.",
    "version": "0.1.2.9000",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1878-3253>),\nKrzysztof Sakrejda [rev],\nMatthew T. Warkentin [rev],\nEli Lilly and Company [cph]",
    "url": "https://docs.ropensci.org/stantargets/,\nhttps://github.com/ropensci/stantargets,\nhttps://r-multiverse.org/topics/bayesian.html",
    "bug_reports": "https://github.com/ropensci/stantargets/issues",
    "repository": "",
    "exports": [
      [
        "tar_stan_compile"
      ],
      [
        "tar_stan_compile_run"
      ],
      [
        "tar_stan_example_data"
      ],
      [
        "tar_stan_example_file"
      ],
      [
        "tar_stan_gq"
      ],
      [
        "tar_stan_gq_rep_draws"
      ],
      [
        "tar_stan_gq_rep_run"
      ],
      [
        "tar_stan_gq_rep_summary"
      ],
      [
        "tar_stan_gq_run"
      ],
      [
        "tar_stan_mcmc"
      ],
      [
        "tar_stan_mcmc_rep_diagnostics"
      ],
      [
        "tar_stan_mcmc_rep_draws"
      ],
      [
        "tar_stan_mcmc_rep_run"
      ],
      [
        "tar_stan_mcmc_rep_summary"
      ],
      [
        "tar_stan_mcmc_run"
      ],
      [
        "tar_stan_mle"
      ],
      [
        "tar_stan_mle_rep_draws"
      ],
      [
        "tar_stan_mle_rep_run"
      ],
      [
        "tar_stan_mle_rep_summary"
      ],
      [
        "tar_stan_mle_run"
      ],
      [
        "tar_stan_output"
      ],
      [
        "tar_stan_rep_data_batch"
      ],
      [
        "tar_stan_summary"
      ],
      [
        "tar_stan_summary_join_data"
      ],
      [
        "tar_stan_vb"
      ],
      [
        "tar_stan_vb_rep_draws"
      ],
      [
        "tar_stan_vb_rep_run"
      ],
      [
        "tar_stan_vb_rep_summary"
      ],
      [
        "tar_stan_vb_run"
      ]
    ],
    "topics": [
      [
        "bayesian"
      ],
      [
        "high-performance-computing"
      ],
      [
        "make"
      ],
      [
        "r-targetopia"
      ],
      [
        "reproducibility"
      ],
      [
        "stan"
      ],
      [
        "statistics"
      ],
      [
        "targets"
      ]
    ],
    "score": 7.0092,
    "stars": 50,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "stantargets Targets for Stan Workflows Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'stantargets' R package leverages 'targets'\nand 'cmdstanr' to ease these burdens. 'stantargets' makes it\nsuper easy to set up scalable Stan pipelines that automatically\nparallelize the computation and skip expensive steps when the\nresults are already up to date. Minimal custom code is\nrequired, and there is no need to manually configure branching,\nso usage is much easier than 'targets' alone. 'stantargets' can\naccess all of 'cmdstanr''s major algorithms (MCMC, variational\nBayes, and optimization) and it supports both single-fit\nworkflows and multi-rep simulation studies. For the statistical\nmethodology, please refer to 'Stan' documentation (Stan\nDevelopment Team 2020) <https://mc-stan.org/>. tar_stan_compile tar_stan_compile_run tar_stan_example_data tar_stan_example_file tar_stan_gq tar_stan_gq_rep_draws tar_stan_gq_rep_run tar_stan_gq_rep_summary tar_stan_gq_run tar_stan_mcmc tar_stan_mcmc_rep_diagnostics tar_stan_mcmc_rep_draws tar_stan_mcmc_rep_run tar_stan_mcmc_rep_summary tar_stan_mcmc_run tar_stan_mle tar_stan_mle_rep_draws tar_stan_mle_rep_run tar_stan_mle_rep_summary tar_stan_mle_run tar_stan_output tar_stan_rep_data_batch tar_stan_summary tar_stan_summary_join_data tar_stan_vb tar_stan_vb_rep_draws tar_stan_vb_rep_run tar_stan_vb_rep_summary tar_stan_vb_run bayesian high-performance-computing make r-targetopia reproducibility stan statistics targets"
  },
  {
    "id": 1379,
    "package_name": "treedata.table",
    "title": "Manipulation of Matched Phylogenies and Data using 'data.table'",
    "description": "An implementation that combines trait data and a\nphylogenetic tree (or trees) into a single object of class\n'treedata.table'. The resulting object can be easily\nmanipulated to simultaneously change the trait- and tree-level\nsampling. Currently implemented functions allow users to use a\n'data.table' syntax when performing operations on the trait\ndataset within the 'treedata.table' object. For more details\nsee Roman-Palacios et al. (2021) <doi:10.7717/peerj.12450>.",
    "version": "0.1.1",
    "maintainer": "Cristian Roman-Palacios <cromanpa@arizona.edu>",
    "author": "Josef Uyeda [aut] (ORCID: <https://orcid.org/0000-0003-4624-9680>),\nCristian Roman-Palacios [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1696-4886>),\nApril Wright [aut] (ORCID: <https://orcid.org/0000-0003-4692-3225>),\nLuke Harmon [ctb],\nHugo Gruson [rev],\nKari Norman [rev]",
    "url": "https://ropensci.github.io/treedata.table/,\nhttps://docs.ropensci.org/treedata.table/,\nhttps://github.com/ropensci/treedata.table/",
    "bug_reports": "https://github.com/ropensci/treedata.table/issues",
    "repository": "",
    "exports": [
      [
        "as.treedata.table"
      ],
      [
        "detectAllCharacters"
      ],
      [
        "detectCharacterType"
      ],
      [
        "droptreedata.table"
      ],
      [
        "extractVector"
      ],
      [
        "filterMatrix"
      ],
      [
        "forceNames"
      ],
      [
        "hasNames"
      ],
      [
        "pulltreedata.table"
      ],
      [
        "tdt"
      ]
    ],
    "topics": [],
    "score": 6.4191,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "treedata.table Manipulation of Matched Phylogenies and Data using 'data.table' An implementation that combines trait data and a\nphylogenetic tree (or trees) into a single object of class\n'treedata.table'. The resulting object can be easily\nmanipulated to simultaneously change the trait- and tree-level\nsampling. Currently implemented functions allow users to use a\n'data.table' syntax when performing operations on the trait\ndataset within the 'treedata.table' object. For more details\nsee Roman-Palacios et al. (2021) <doi:10.7717/peerj.12450>. as.treedata.table detectAllCharacters detectCharacterType droptreedata.table extractVector filterMatrix forceNames hasNames pulltreedata.table tdt "
  },
  {
    "id": 330,
    "package_name": "c14bazAAR",
    "title": "Download and Prepare C14 Dates from Different Source Databases",
    "description": "Query different C14 date databases and apply basic data\ncleaning, merging and calibration steps. Currently available\ndatabases: 14cpalaeolithic, 14sea, adrac, agrichange, aida,\naustarch, bda, calpal, caribbean, eubar, euroevol, irdd, jomon,\nkatsianis, kiteeastafrica, medafricarbon, mesorad, neonet,\nneonetatl, nerd, p3k14c, pacea, palmisano, rado.nb, rxpand,\nsard, xronos.",
    "version": "5.2.0",
    "maintainer": "Clemens Schmid <clemens@nevrome.de>",
    "author": "Clemens Schmid [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-3448-5715>),\nDirk Seidensticker [aut] (ORCID:\n<https://orcid.org/0000-0002-8155-7702>),\nDaniel Knitter [aut] (ORCID: <https://orcid.org/0000-0003-3014-4497>),\nMartin Hinz [aut] (ORCID: <https://orcid.org/0000-0002-9904-6548>),\nDavid Matzig [aut] (ORCID: <https://orcid.org/0000-0001-7349-5401>),\nWolfgang Hamer [aut] (ORCID: <https://orcid.org/0000-0002-5943-5020>),\nKay Schmuetz [aut],\nThomas Huet [ctb] (ORCID: <https://orcid.org/0000-0002-1112-6122>),\nNils Mueller-Scheessel [ctb] (ORCID:\n<https://orcid.org/0000-0001-7992-8722>),\nJoe Roe [ctb] (ORCID: <https://orcid.org/0000-0002-1011-1244>),\nBen Marwick [rev] (ORCID: <https://orcid.org/0000-0001-7879-4531>),\nEnrico R. Crema [rev] (ORCID: <https://orcid.org/0000-0001-6727-5138>)",
    "url": "https://docs.ropensci.org/c14bazAAR,\nhttps://github.com/ropensci/c14bazAAR",
    "bug_reports": "https://github.com/ropensci/c14bazAAR/issues",
    "repository": "",
    "exports": [
      [
        "as.c14_date_list"
      ],
      [
        "as.sf"
      ],
      [
        "calibrate"
      ],
      [
        "classify_material"
      ],
      [
        "coordinate_precision"
      ],
      [
        "determine_country_by_coordinate"
      ],
      [
        "enforce_types"
      ],
      [
        "finalize_country_name"
      ],
      [
        "fix_database_country_name"
      ],
      [
        "fuse"
      ],
      [
        "get_14cpalaeolithic"
      ],
      [
        "get_14sea"
      ],
      [
        "get_adrac"
      ],
      [
        "get_agrichange"
      ],
      [
        "get_aida"
      ],
      [
        "get_all_dates"
      ],
      [
        "get_austarch"
      ],
      [
        "get_bda"
      ],
      [
        "get_c14data"
      ],
      [
        "get_calpal"
      ],
      [
        "get_caribbean"
      ],
      [
        "get_context"
      ],
      [
        "get_db_url"
      ],
      [
        "get_db_version"
      ],
      [
        "get_db_version_number"
      ],
      [
        "get_emedyd"
      ],
      [
        "get_eubar"
      ],
      [
        "get_euroevol"
      ],
      [
        "get_irdd"
      ],
      [
        "get_jomon"
      ],
      [
        "get_katsianis"
      ],
      [
        "get_kiteeastafrica"
      ],
      [
        "get_medafricarbon"
      ],
      [
        "get_mesorad"
      ],
      [
        "get_neonet"
      ],
      [
        "get_neonetatl"
      ],
      [
        "get_nerd"
      ],
      [
        "get_p3k14c"
      ],
      [
        "get_pacea"
      ],
      [
        "get_palmisano"
      ],
      [
        "get_rado.nb"
      ],
      [
        "get_radon"
      ],
      [
        "get_radonb"
      ],
      [
        "get_rxpand"
      ],
      [
        "get_sard"
      ],
      [
        "get_xronos"
      ],
      [
        "is.c14_date_list"
      ],
      [
        "mark_duplicates"
      ],
      [
        "order_variables"
      ],
      [
        "remove_duplicates"
      ],
      [
        "standardize_country_name"
      ],
      [
        "write_c14"
      ]
    ],
    "topics": [
      [
        "archaeology"
      ],
      [
        "radiocarbon-dates"
      ]
    ],
    "score": 6.2529,
    "stars": 31,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "c14bazAAR Download and Prepare C14 Dates from Different Source Databases Query different C14 date databases and apply basic data\ncleaning, merging and calibration steps. Currently available\ndatabases: 14cpalaeolithic, 14sea, adrac, agrichange, aida,\naustarch, bda, calpal, caribbean, eubar, euroevol, irdd, jomon,\nkatsianis, kiteeastafrica, medafricarbon, mesorad, neonet,\nneonetatl, nerd, p3k14c, pacea, palmisano, rado.nb, rxpand,\nsard, xronos. as.c14_date_list as.sf calibrate classify_material coordinate_precision determine_country_by_coordinate enforce_types finalize_country_name fix_database_country_name fuse get_14cpalaeolithic get_14sea get_adrac get_agrichange get_aida get_all_dates get_austarch get_bda get_c14data get_calpal get_caribbean get_context get_db_url get_db_version get_db_version_number get_emedyd get_eubar get_euroevol get_irdd get_jomon get_katsianis get_kiteeastafrica get_medafricarbon get_mesorad get_neonet get_neonetatl get_nerd get_p3k14c get_pacea get_palmisano get_rado.nb get_radon get_radonb get_rxpand get_sard get_xronos is.c14_date_list mark_duplicates order_variables remove_duplicates standardize_country_name write_c14 archaeology radiocarbon-dates"
  },
  {
    "id": 719,
    "package_name": "jagstargets",
    "title": "Targets for JAGS Pipelines",
    "description": "Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'jagstargets' R package is leverages\n'targets' and 'R2jags' to ease this burden. 'jagstargets' makes\nit super easy to set up scalable JAGS pipelines that\nautomatically parallelize the computation and skip expensive\nsteps when the results are already up to date. Minimal custom\ncode is required, and there is no need to manually configure\nbranching, so usage is much easier than 'targets' alone. For\nthe underlying methodology, please refer to the documentation\nof 'targets' <doi:10.21105/joss.02959> and 'JAGS' (Plummer\n2003)\n<https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf>.",
    "version": "1.2.3",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1878-3253>),\nDavid Lawrence Miller [rev],\nEli Lilly and Company [cph]",
    "url": "https://docs.ropensci.org/jagstargets/,\nhttps://github.com/ropensci/jagstargets,\nhttps://r-multiverse.org/topics/bayesian.html",
    "bug_reports": "https://github.com/ropensci/jagstargets/issues",
    "repository": "",
    "exports": [
      [
        "tar_jags"
      ],
      [
        "tar_jags_df"
      ],
      [
        "tar_jags_example_data"
      ],
      [
        "tar_jags_example_file"
      ],
      [
        "tar_jags_rep_data_batch"
      ],
      [
        "tar_jags_rep_dic"
      ],
      [
        "tar_jags_rep_draws"
      ],
      [
        "tar_jags_rep_run"
      ],
      [
        "tar_jags_rep_summary"
      ],
      [
        "tar_jags_run"
      ]
    ],
    "topics": [
      [
        "bayesian"
      ],
      [
        "high-performance-computing"
      ],
      [
        "jags"
      ],
      [
        "make"
      ],
      [
        "r-targetopia"
      ],
      [
        "reproducibility"
      ],
      [
        "rjags"
      ],
      [
        "statistics"
      ],
      [
        "targets"
      ],
      [
        "cpp"
      ]
    ],
    "score": 6.2455,
    "stars": 11,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "jagstargets Targets for JAGS Pipelines Bayesian data analysis usually incurs long runtimes and\ncumbersome custom code. A pipeline toolkit tailored to Bayesian\nstatisticians, the 'jagstargets' R package is leverages\n'targets' and 'R2jags' to ease this burden. 'jagstargets' makes\nit super easy to set up scalable JAGS pipelines that\nautomatically parallelize the computation and skip expensive\nsteps when the results are already up to date. Minimal custom\ncode is required, and there is no need to manually configure\nbranching, so usage is much easier than 'targets' alone. For\nthe underlying methodology, please refer to the documentation\nof 'targets' <doi:10.21105/joss.02959> and 'JAGS' (Plummer\n2003)\n<https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf>. tar_jags tar_jags_df tar_jags_example_data tar_jags_example_file tar_jags_rep_data_batch tar_jags_rep_dic tar_jags_rep_draws tar_jags_rep_run tar_jags_rep_summary tar_jags_run bayesian high-performance-computing jags make r-targetopia reproducibility rjags statistics targets cpp"
  },
  {
    "id": 804,
    "package_name": "mctq",
    "title": "Munich ChronoType Questionnaire Tools",
    "description": "A complete toolkit for processing the Munich ChronoType\nQuestionnaire (MCTQ) in its three versions: standard, micro,\nand shift. The MCTQ is a quantitative and validated tool used\nto assess chronotypes based on individuals' sleep behavior. It\nwas originally presented by Till Roenneberg, Anna Wirz-Justice,\nand Martha Merrow in 2003 (2003,\n<doi:10.1177/0748730402239679>).",
    "version": "0.3.2.9001",
    "maintainer": "Daniel Vartanian <danvartan@gmail.com>",
    "author": "Daniel Vartanian [aut, cre, ccp, cph] (ORCID:\n<https://orcid.org/0000-0001-7782-759X>),\nAna Amelia Benedito Silva [sad] (ORCID:\n<https://orcid.org/0000-0003-4976-2623>),\nMario Pedrazzoli [sad] (ORCID: <https://orcid.org/0000-0002-5257-591X>),\nJonathan Keane [rev] (ORCID: <https://orcid.org/0000-0001-7087-9776>),\nMario Andre Leocadio-Miguel [rev] (ORCID:\n<https://orcid.org/0000-0002-7248-3529>),\nUniversity of Sao Paulo (USP) [fnd]",
    "url": "https://docs.ropensci.org/mctq/, https://github.com/ropensci/mctq/",
    "bug_reports": "https://github.com/ropensci/mctq/issues/",
    "repository": "",
    "exports": [
      [
        "fd"
      ],
      [
        "gu"
      ],
      [
        "le_week"
      ],
      [
        "msf_sc"
      ],
      [
        "msl"
      ],
      [
        "napd"
      ],
      [
        "pretty_mctq"
      ],
      [
        "random_mctq"
      ],
      [
        "raw_data"
      ],
      [
        "sd_overall"
      ],
      [
        "sd_week"
      ],
      [
        "sd24"
      ],
      [
        "sdu"
      ],
      [
        "sjl"
      ],
      [
        "sjl_rel"
      ],
      [
        "sjl_sc"
      ],
      [
        "sjl_sc_rel"
      ],
      [
        "sjl_weighted"
      ],
      [
        "sloss_week"
      ],
      [
        "so"
      ],
      [
        "tbt"
      ]
    ],
    "topics": [
      [
        "infrastructure"
      ],
      [
        "preprocessing"
      ],
      [
        "visualization"
      ],
      [
        "biological-rhythm"
      ],
      [
        "chronobiology"
      ],
      [
        "chronotype"
      ],
      [
        "circadian-phenotype"
      ],
      [
        "circadian-rhythm"
      ],
      [
        "entrainment"
      ],
      [
        "mctq"
      ],
      [
        "peer-reviewed"
      ],
      [
        "sleep"
      ],
      [
        "temporal-phenotype"
      ]
    ],
    "score": 6.1351,
    "stars": 13,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "mctq Munich ChronoType Questionnaire Tools A complete toolkit for processing the Munich ChronoType\nQuestionnaire (MCTQ) in its three versions: standard, micro,\nand shift. The MCTQ is a quantitative and validated tool used\nto assess chronotypes based on individuals' sleep behavior. It\nwas originally presented by Till Roenneberg, Anna Wirz-Justice,\nand Martha Merrow in 2003 (2003,\n<doi:10.1177/0748730402239679>). fd gu le_week msf_sc msl napd pretty_mctq random_mctq raw_data sd_overall sd_week sd24 sdu sjl sjl_rel sjl_sc sjl_sc_rel sjl_weighted sloss_week so tbt infrastructure preprocessing visualization biological-rhythm chronobiology chronotype circadian-phenotype circadian-rhythm entrainment mctq peer-reviewed sleep temporal-phenotype"
  },
  {
    "id": 600,
    "package_name": "gbifdb",
    "title": "High Performance Interface to 'GBIF'",
    "description": "A high performance interface to the Global Biodiversity\nInformation Facility, 'GBIF'.  In contrast to 'rgbif', which\ncan access small subsets of 'GBIF' data through web-based\nqueries to a central server, 'gbifdb' provides enhanced\nperformance for R users performing large-scale analyses on\nservers and cloud computing providers, providing full support\nfor arbitrary 'SQL' or 'dplyr' operations on the complete\n'GBIF' data tables (now over 1 billion records, and over a\nterabyte in size). 'gbifdb' accesses a copy of the 'GBIF' data\nin 'parquet' format, which is already readily available in\ncommercial computing clouds such as the Amazon Open Data portal\nand the Microsoft Planetary Computer, or can be accessed\ndirectly without downloading, or downloaded to any server with\nsuitable bandwidth and storage space. The high-performance\ntechniques for local and remote access are described in\n<https://duckdb.org/why_duckdb> and\n<https://arrow.apache.org/docs/r/articles/fs.html>\nrespectively.",
    "version": "1.0.0.99",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nMarc-Olivier Beausoleil [ctb] (ORCID:\n<https://orcid.org/0000-0003-3717-3223>)",
    "url": "https://docs.ropensci.org/gbifdb/,\nhttps://github.com/ropensci/gbifdb",
    "bug_reports": "https://github.com/ropensci/gbifdb",
    "repository": "",
    "exports": [
      [
        "gbif_dir"
      ],
      [
        "gbif_download"
      ],
      [
        "gbif_example_data"
      ],
      [
        "gbif_local"
      ],
      [
        "gbif_remote"
      ],
      [
        "gbif_version"
      ]
    ],
    "topics": [],
    "score": 6.1072,
    "stars": 40,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "gbifdb High Performance Interface to 'GBIF' A high performance interface to the Global Biodiversity\nInformation Facility, 'GBIF'.  In contrast to 'rgbif', which\ncan access small subsets of 'GBIF' data through web-based\nqueries to a central server, 'gbifdb' provides enhanced\nperformance for R users performing large-scale analyses on\nservers and cloud computing providers, providing full support\nfor arbitrary 'SQL' or 'dplyr' operations on the complete\n'GBIF' data tables (now over 1 billion records, and over a\nterabyte in size). 'gbifdb' accesses a copy of the 'GBIF' data\nin 'parquet' format, which is already readily available in\ncommercial computing clouds such as the Amazon Open Data portal\nand the Microsoft Planetary Computer, or can be accessed\ndirectly without downloading, or downloaded to any server with\nsuitable bandwidth and storage space. The high-performance\ntechniques for local and remote access are described in\n<https://duckdb.org/why_duckdb> and\n<https://arrow.apache.org/docs/r/articles/fs.html>\nrespectively. gbif_dir gbif_download gbif_example_data gbif_local gbif_remote gbif_version "
  },
  {
    "id": 1419,
    "package_name": "virtuoso",
    "title": "Interface to 'Virtuoso' using 'ODBC'",
    "description": "Provides users with a simple and convenient mechanism to\nmanage and query a 'Virtuoso' database using the 'DBI'\n(Data-Base Interface) compatible 'ODBC' (Open Database\nConnectivity) interface. 'Virtuoso' is a high-performance\n\"universal server,\" which can act as both a relational\ndatabase, supporting standard Structured Query Language ('SQL')\nqueries, while also supporting data following the Resource\nDescription Framework ('RDF') model for Linked Data. 'RDF' data\ncan be queried using 'SPARQL' ('SPARQL' Protocol and 'RDF'\nQuery Language) queries, a graph-based query that supports\nsemantic reasoning. This allows users to leverage the\nperformance of local or remote 'Virtuoso' servers using popular\n'R' packages such as 'DBI' and 'dplyr', while also providing a\nhigh-performance solution for working with large 'RDF'\n'triplestores' from 'R.' The package also provides helper\nroutines to install, launch, and manage a 'Virtuoso' server\nlocally on 'Mac', 'Windows' and 'Linux' platforms using the\nstandard interactive installers from the 'R' command-line.  By\nautomatically handling these setup steps, the package can make\nusing 'Virtuoso' considerably faster and easier for a most\nusers to deploy in a local environment. Managing the bulk\nimport of triples from common serializations with a single\nintuitive command is another key feature of this package.  Bulk\nimport performance can be tens to hundreds of times faster than\nthe comparable imports using existing 'R' tools, including\n'rdflib' and 'redland' packages.",
    "version": "0.1.8",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nBryce Mecum [ctb] (ORCID: <https://orcid.org/0000-0002-0381-3766>)",
    "url": "https://github.com/ropensci/virtuoso",
    "bug_reports": "https://github.com/ropensci/virtuoso/issues",
    "repository": "",
    "exports": [
      [
        "has_virtuoso"
      ],
      [
        "vos_configure"
      ],
      [
        "vos_connect"
      ],
      [
        "vos_delete_db"
      ],
      [
        "vos_destroy_all"
      ],
      [
        "vos_import"
      ],
      [
        "vos_install"
      ],
      [
        "vos_kill"
      ],
      [
        "vos_list_graphs"
      ],
      [
        "vos_log"
      ],
      [
        "vos_odbcinst"
      ],
      [
        "vos_process"
      ],
      [
        "vos_query"
      ],
      [
        "vos_set_paths"
      ],
      [
        "vos_start"
      ],
      [
        "vos_status"
      ],
      [
        "vos_uninstall"
      ]
    ],
    "topics": [],
    "score": 6.0111,
    "stars": 9,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "virtuoso Interface to 'Virtuoso' using 'ODBC' Provides users with a simple and convenient mechanism to\nmanage and query a 'Virtuoso' database using the 'DBI'\n(Data-Base Interface) compatible 'ODBC' (Open Database\nConnectivity) interface. 'Virtuoso' is a high-performance\n\"universal server,\" which can act as both a relational\ndatabase, supporting standard Structured Query Language ('SQL')\nqueries, while also supporting data following the Resource\nDescription Framework ('RDF') model for Linked Data. 'RDF' data\ncan be queried using 'SPARQL' ('SPARQL' Protocol and 'RDF'\nQuery Language) queries, a graph-based query that supports\nsemantic reasoning. This allows users to leverage the\nperformance of local or remote 'Virtuoso' servers using popular\n'R' packages such as 'DBI' and 'dplyr', while also providing a\nhigh-performance solution for working with large 'RDF'\n'triplestores' from 'R.' The package also provides helper\nroutines to install, launch, and manage a 'Virtuoso' server\nlocally on 'Mac', 'Windows' and 'Linux' platforms using the\nstandard interactive installers from the 'R' command-line.  By\nautomatically handling these setup steps, the package can make\nusing 'Virtuoso' considerably faster and easier for a most\nusers to deploy in a local environment. Managing the bulk\nimport of triples from common serializations with a single\nintuitive command is another key feature of this package.  Bulk\nimport performance can be tens to hundreds of times faster than\nthe comparable imports using existing 'R' tools, including\n'rdflib' and 'redland' packages. has_virtuoso vos_configure vos_connect vos_delete_db vos_destroy_all vos_import vos_install vos_kill vos_list_graphs vos_log vos_odbcinst vos_process vos_query vos_set_paths vos_start vos_status vos_uninstall "
  },
  {
    "id": 619,
    "package_name": "getCRUCLdata",
    "title": "'CRU' 'CL' v. 2.0 Climatology Client",
    "description": "Provides functions that automate downloading and importing\nUniversity of East Anglia Climate Research Unit ('CRU') 'CL' v.\n2.0 climatology data, facilitates the calculation of minimum\ntemperature and maximum temperature and formats the data into a\ndata.table object or a list of 'terra' 'rast' objects for use.\n'CRU' 'CL' v. 2.0 data are a gridded climatology of 1961-1990\nmonthly means released in 2002 and cover all land areas\n(excluding Antarctica) at 10 arc minutes (0.1666667 degree)\nresolution.  For more information see the description of the\ndata provided by the University of East Anglia Climate Research\nUnit, <https://crudata.uea.ac.uk/cru/data/hrg/tmc/readme.txt>.",
    "version": "1.0.3",
    "maintainer": "Adam H. Sparks <adamhsparks@gmail.com>",
    "author": "Adam H. Sparks [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0061-8359>),\nCurtin University of Technology [cph] (Provided support through Adam\nSparks's time.),\nGrains Research and Development Corporation [cph] (GRDC Project\nCUR2210-005OPX (AAGI-CU))",
    "url": "https://github.com/ropensci/getCRUCLdata,\nhttps://docs.ropensci.org/getCRUCLdata/",
    "bug_reports": "https://github.com/ropensci/getCRUCLdata/issues",
    "repository": "",
    "exports": [
      [
        "create_cru_df"
      ],
      [
        "create_CRU_df"
      ],
      [
        "create_cru_stack"
      ],
      [
        "create_CRU_stack"
      ],
      [
        "get_cru_df"
      ],
      [
        "get_CRU_df"
      ],
      [
        "get_cru_stack"
      ],
      [
        "get_CRU_stack"
      ],
      [
        "manage_cache"
      ]
    ],
    "topics": [
      [
        "anglia-cru"
      ],
      [
        "climate-data"
      ],
      [
        "cru-cl2"
      ],
      [
        "temperature"
      ],
      [
        "rainfall"
      ],
      [
        "elevation"
      ],
      [
        "data-access"
      ],
      [
        "wind"
      ],
      [
        "relative-humidity"
      ],
      [
        "solar-radiation"
      ],
      [
        "diurnal-temperature"
      ],
      [
        "frost"
      ],
      [
        "cru"
      ],
      [
        "peer-reviewed"
      ]
    ],
    "score": 5.5649,
    "stars": 17,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "getCRUCLdata 'CRU' 'CL' v. 2.0 Climatology Client Provides functions that automate downloading and importing\nUniversity of East Anglia Climate Research Unit ('CRU') 'CL' v.\n2.0 climatology data, facilitates the calculation of minimum\ntemperature and maximum temperature and formats the data into a\ndata.table object or a list of 'terra' 'rast' objects for use.\n'CRU' 'CL' v. 2.0 data are a gridded climatology of 1961-1990\nmonthly means released in 2002 and cover all land areas\n(excluding Antarctica) at 10 arc minutes (0.1666667 degree)\nresolution.  For more information see the description of the\ndata provided by the University of East Anglia Climate Research\nUnit, <https://crudata.uea.ac.uk/cru/data/hrg/tmc/readme.txt>. create_cru_df create_CRU_df create_cru_stack create_CRU_stack get_cru_df get_CRU_df get_cru_stack get_CRU_stack manage_cache anglia-cru climate-data cru-cl2 temperature rainfall elevation data-access wind relative-humidity solar-radiation diurnal-temperature frost cru peer-reviewed"
  },
  {
    "id": 1366,
    "package_name": "toml",
    "title": "Read, Write, and Modify TOML Files",
    "description": "Simple toolkit for working with TOML text. Based on\ntomledit which allows for modifying TOML while preserving\norder, comments,and whitespace.",
    "version": "1.0.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre]",
    "url": "https://r-lib.r-universe.dev/toml",
    "bug_reports": "https://github.com/r-lib/toml/issues",
    "repository": "",
    "exports": [
      [
        "edit_toml"
      ],
      [
        "parse_toml"
      ],
      [
        "read_toml"
      ],
      [
        "write_toml"
      ]
    ],
    "topics": [],
    "score": 4.3644,
    "stars": 4,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "toml Read, Write, and Modify TOML Files Simple toolkit for working with TOML text. Based on\ntomledit which allows for modifying TOML while preserving\norder, comments,and whitespace. edit_toml parse_toml read_toml write_toml "
  },
  {
    "id": 562,
    "package_name": "fastMatMR",
    "title": "High-Performance Matrix Market File Operations",
    "description": "An interface to the 'fast_matrix_market' 'C++' library,\nthis package offers efficient read and write operations for\nMatrix Market files in R. It supports both sparse and dense\nmatrix formats. Peer-reviewed at ROpenSci\n(<https://github.com/ropensci/software-review/issues/606>).",
    "version": "1.2.5",
    "maintainer": "Rohit Goswami <rgoswami@ieee.org>",
    "author": "Rohit Goswami [aut, cre]\n(<https://orcid.org/0000-0002-2393-8056>), Ildiko Czeller [rev]\n(<https://orcid.org/0000-0002-9418-4589>), Adam Lugowski [ctb]\n(<https://orcid.org/0009-0004-0922-4067>)",
    "url": "https://github.com/ropensci/fastMatMR",
    "bug_reports": "https://github.com/ropensci/fastMatMR/issues",
    "repository": "",
    "exports": [
      [
        "fmm_to_mat"
      ],
      [
        "fmm_to_sparse_Matrix"
      ],
      [
        "fmm_to_vec"
      ],
      [
        "intmat_to_fmm"
      ],
      [
        "intvec_to_fmm"
      ],
      [
        "mat_to_fmm"
      ],
      [
        "sparse_Matrix_to_fmm"
      ],
      [
        "vec_to_fmm"
      ],
      [
        "write_fmm"
      ]
    ],
    "topics": [
      [
        "cpp17"
      ],
      [
        "matrix-market"
      ],
      [
        "matrix-market-format"
      ],
      [
        "r-cpp11"
      ],
      [
        "cpp"
      ]
    ],
    "score": 4.1973,
    "stars": 5,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "fastMatMR High-Performance Matrix Market File Operations An interface to the 'fast_matrix_market' 'C++' library,\nthis package offers efficient read and write operations for\nMatrix Market files in R. It supports both sparse and dense\nmatrix formats. Peer-reviewed at ROpenSci\n(<https://github.com/ropensci/software-review/issues/606>). fmm_to_mat fmm_to_sparse_Matrix fmm_to_vec intmat_to_fmm intvec_to_fmm mat_to_fmm sparse_Matrix_to_fmm vec_to_fmm write_fmm cpp17 matrix-market matrix-market-format r-cpp11 cpp"
  },
  {
    "id": 1384,
    "package_name": "trendeval",
    "title": "Evaluate Trending Models",
    "description": "Provides a coherent interface for evaluating models fit\nwith the trending package.  This package is part of the RECON\n(<https://www.repidemicsconsortium.org/>) toolkit for outbreak\nanalysis.",
    "version": "0.1.1",
    "maintainer": "Thibaut Jombart <thibautjombart@gmail.com>",
    "author": "Dirk Schumacher [aut],\nThibaut Jombart [aut, cre],\nTim Taylor [ctb] (ORCID: <https://orcid.org/0000-0002-8587-7113>)",
    "url": "https://github.com/reconverse/trendeval",
    "bug_reports": "https://github.com/reconverse/trendeval/issues",
    "repository": "",
    "exports": [
      [
        "brm_model"
      ],
      [
        "calculate_aic"
      ],
      [
        "calculate_mae"
      ],
      [
        "calculate_rmse"
      ],
      [
        "calculate_rsq"
      ],
      [
        "evaluate_aic"
      ],
      [
        "evaluate_resampling"
      ],
      [
        "glm_model"
      ],
      [
        "glm_nb_model"
      ],
      [
        "lm_model"
      ]
    ],
    "topics": [],
    "score": 3.301,
    "stars": 2,
    "primary_category": "epidemiology",
    "source_universe": "reconverse",
    "search_text": "trendeval Evaluate Trending Models Provides a coherent interface for evaluating models fit\nwith the trending package.  This package is part of the RECON\n(<https://www.repidemicsconsortium.org/>) toolkit for outbreak\nanalysis. brm_model calculate_aic calculate_mae calculate_rmse calculate_rsq evaluate_aic evaluate_resampling glm_model glm_nb_model lm_model "
  },
  {
    "id": 1319,
    "package_name": "testthat.buildkite",
    "title": "A testthat reporter for buildkite",
    "description": "A testthat reporter that prints progress output in a\nformat for use in buildkite logs.",
    "version": "0.0.1",
    "maintainer": "Robert Ashton <r.ashton@imperial.ac.uk>",
    "author": "Robert Ashton [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "BuildkiteReporter"
      ]
    ],
    "topics": [],
    "score": 2.5441,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "testthat.buildkite A testthat reporter for buildkite A testthat reporter that prints progress output in a\nformat for use in buildkite logs. BuildkiteReporter "
  },
  {
    "id": 61,
    "package_name": "FoRDM",
    "title": "Forest Many-Objective Robust Decision Making ('FoRDM')",
    "description": "Forest Many-Objective Robust Decision Making ('FoRDM') is a R toolkit for supporting robust forest management under deep uncertainty.\n    It provides a forestry-focused application of Many-Objective Robust Decision Making ('MORDM') to forest simulation outputs, \n    enabling users to evaluate robustness using regret- and 'satisficing'-based measures. 'FoRDM' identifies robust solutions, \n    generates Pareto fronts, and offers interactive 2D, 3D, and parallel-coordinate visualizations.",
    "version": "1.0.1",
    "maintainer": "Marc Djahangard <marc.djahangard@ife.uni-freiburg.de>",
    "author": "Marc Djahangard [aut, cre],\n  Rasoul Yousefpour [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FoRDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FoRDM Forest Many-Objective Robust Decision Making ('FoRDM') Forest Many-Objective Robust Decision Making ('FoRDM') is a R toolkit for supporting robust forest management under deep uncertainty.\n    It provides a forestry-focused application of Many-Objective Robust Decision Making ('MORDM') to forest simulation outputs, \n    enabling users to evaluate robustness using regret- and 'satisficing'-based measures. 'FoRDM' identifies robust solutions, \n    generates Pareto fronts, and offers interactive 2D, 3D, and parallel-coordinate visualizations.  "
  },
  {
    "id": 95,
    "package_name": "NUETON",
    "title": "Nitrogen Use Efficiency Toolkit on Numerics",
    "description": "A comprehensive toolkit for calculating and visualizing Nitrogen Use Efficiency (NUE) indicators in agricultural research. The package implements 23 parameters categorized into fertilizer-based, plant-based, soil-based, isotope-based, ecology-based, and system-based indicators based on Congreves et al. (2021) <doi:10.3389/fpls.2021.637108>. Key features include vectorized calculations for paired-plot experimental designs, batch processing capabilities for handling large datasets, and built-in visualization tools using 'ggplot2'. Designed to streamline the workflow from raw agronomic data to publication-ready metrics and plots.",
    "version": "0.2.0",
    "maintainer": "Shubham Love <shubhamlove2101@gmail.com>",
    "author": "Shubham Love [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NUETON",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NUETON Nitrogen Use Efficiency Toolkit on Numerics A comprehensive toolkit for calculating and visualizing Nitrogen Use Efficiency (NUE) indicators in agricultural research. The package implements 23 parameters categorized into fertilizer-based, plant-based, soil-based, isotope-based, ecology-based, and system-based indicators based on Congreves et al. (2021) <doi:10.3389/fpls.2021.637108>. Key features include vectorized calculations for paired-plot experimental designs, batch processing capabilities for handling large datasets, and built-in visualization tools using 'ggplot2'. Designed to streamline the workflow from raw agronomic data to publication-ready metrics and plots.  "
  },
  {
    "id": 189,
    "package_name": "Seurat",
    "title": "Tools for Single Cell Genomics",
    "description": "A toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. 'Seurat' aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. See Satija R, Farrell J, Gennert D, et al (2015) <doi:10.1038/nbt.3192>, Macosko E, Basu A, Satija R, et al (2015) <doi:10.1016/j.cell.2015.05.002>, Stuart T, Butler A, et al (2019) <doi:10.1016/j.cell.2019.05.031>, and Hao, Hao, et al (2020) <doi:10.1101/2020.10.12.335331> for more details.",
    "version": "5.4.0",
    "maintainer": "Rahul Satija <seurat@nygenome.org>",
    "author": "Andrew Butler [ctb] (ORCID: <https://orcid.org/0000-0003-3608-0463>),\n  Saket Choudhary [ctb] (ORCID: <https://orcid.org/0000-0001-5202-7633>),\n  David Collins [ctb] (ORCID: <https://orcid.org/0000-0001-9243-7821>),\n  Charlotte Darby [ctb] (ORCID: <https://orcid.org/0000-0003-2195-5300>),\n  Jeff Farrell [ctb],\n  Isabella Grabski [ctb] (ORCID: <https://orcid.org/0000-0002-0616-5469>),\n  Christoph Hafemeister [ctb] (ORCID:\n    <https://orcid.org/0000-0001-6365-8254>),\n  Yuhan Hao [ctb] (ORCID: <https://orcid.org/0000-0002-1810-0822>),\n  Austin Hartman [ctb] (ORCID: <https://orcid.org/0000-0001-7278-1852>),\n  Paul Hoffman [ctb] (ORCID: <https://orcid.org/0000-0002-7693-8957>),\n  Jaison Jain [ctb] (ORCID: <https://orcid.org/0000-0002-9478-5018>),\n  Longda Jiang [ctb] (ORCID: <https://orcid.org/0000-0003-4964-6497>),\n  Madeline Kowalski [ctb] (ORCID:\n    <https://orcid.org/0000-0002-5655-7620>),\n  Skylar Li [ctb],\n  Brian Zhang [ctb],\n  Gesmira Molla [ctb] (ORCID: <https://orcid.org/0000-0002-8628-5056>),\n  Efthymia Papalexi [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5898-694X>),\n  Patrick Roelli [ctb],\n  Rahul Satija [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9448-8833>),\n  Karthik Shekhar [ctb],\n  Anagha Shenoy [ctb] (ORCID: <https://orcid.org/0000-0002-0537-6862>),\n  Avi Srivastava [ctb] (ORCID: <https://orcid.org/0000-0001-9798-2079>),\n  Tim Stuart [ctb] (ORCID: <https://orcid.org/0000-0002-3044-0897>),\n  Kristof Torkenczy [ctb] (ORCID:\n    <https://orcid.org/0000-0002-4869-7957>),\n  Brian Zhang [ctb],\n  Shiwei Zheng [ctb] (ORCID: <https://orcid.org/0000-0001-6682-6743>),\n  Satija Lab and Collaborators [fnd]",
    "url": "https://satijalab.org/seurat, https://github.com/satijalab/seurat",
    "bug_reports": "https://github.com/satijalab/seurat/issues",
    "repository": "https://cran.r-project.org/package=Seurat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Seurat Tools for Single Cell Genomics A toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. 'Seurat' aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. See Satija R, Farrell J, Gennert D, et al (2015) <doi:10.1038/nbt.3192>, Macosko E, Basu A, Satija R, et al (2015) <doi:10.1016/j.cell.2015.05.002>, Stuart T, Butler A, et al (2019) <doi:10.1016/j.cell.2019.05.031>, and Hao, Hao, et al (2020) <doi:10.1101/2020.10.12.335331> for more details.  "
  },
  {
    "id": 193,
    "package_name": "SimDesign",
    "title": "Structure for Organizing Monte Carlo Simulation Designs",
    "description": "Provides tools to safely and efficiently organize and execute \n    Monte Carlo simulation experiments in R.\n    The package controls the structure and back-end of Monte Carlo simulation experiments\n    by utilizing a generate-analyse-summarise workflow. The workflow safeguards against \n    common simulation coding issues, such as automatically re-simulating non-convergent results, \n    prevents inadvertently overwriting simulation files, catches error and warning messages\n    during execution, implicitly supports parallel processing with high-quality random number \n    generation, and provides tools for managing high-performance computing (HPC) array jobs\n    submitted to schedulers such as SLURM. For a pedagogical introduction to the package see\n    Sigal and Chalmers (2016) <doi:10.1080/10691898.2016.1246953>. For a more in-depth overview of \n    the package and its design philosophy see Chalmers and Adkins (2020) <doi:10.20982/tqmp.16.4.p248>.",
    "version": "2.22",
    "maintainer": "Phil Chalmers <rphilip.chalmers@gmail.com>",
    "author": "Phil Chalmers [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5332-2810>),\n  Matthew Sigal [ctb],\n  Ogreden Oguzhan [ctb],\n  Mikko R\u00f6nkk\u00f6 [aut],\n  Moritz Ketzer [ctb]",
    "url": "http://philchalmers.github.io/SimDesign/,\nhttps://github.com/philchalmers/SimDesign/wiki",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimDesign Structure for Organizing Monte Carlo Simulation Designs Provides tools to safely and efficiently organize and execute \n    Monte Carlo simulation experiments in R.\n    The package controls the structure and back-end of Monte Carlo simulation experiments\n    by utilizing a generate-analyse-summarise workflow. The workflow safeguards against \n    common simulation coding issues, such as automatically re-simulating non-convergent results, \n    prevents inadvertently overwriting simulation files, catches error and warning messages\n    during execution, implicitly supports parallel processing with high-quality random number \n    generation, and provides tools for managing high-performance computing (HPC) array jobs\n    submitted to schedulers such as SLURM. For a pedagogical introduction to the package see\n    Sigal and Chalmers (2016) <doi:10.1080/10691898.2016.1246953>. For a more in-depth overview of \n    the package and its design philosophy see Chalmers and Adkins (2020) <doi:10.20982/tqmp.16.4.p248>.  "
  },
  {
    "id": 209,
    "package_name": "TraMineR",
    "title": "Trajectory Miner: a Sequence Analysis Toolkit",
    "description": "Set of sequence analysis tools for manipulating, describing and rendering categorical sequences, and more generally mining sequence data in the field of social sciences. Although this sequence analysis package is primarily intended for state or event sequences that describe time use or life courses such as family formation histories or professional careers, its features also apply to many other kinds of categorical sequence data. It accepts many different sequence representations as input and provides tools for converting sequences from one format to another. It offers several functions for describing and rendering sequences, for computing distances between sequences with different metrics (among which optimal matching), original dissimilarity-based analysis tools, and functions for extracting the most frequent event subsequences and identifying the most discriminating ones among them. A user's guide can be found on the TraMineR web page.",
    "version": "2.2-13",
    "maintainer": "Gilbert Ritschard <gilbert.ritschard@unige.ch>",
    "author": "Alexis Gabadinho [aut, cph],\n  Matthias Studer [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6269-1412>),\n  Nicolas M\u00fcller [aut],\n  Reto B\u00fcrgin [aut] (ORCID: <https://orcid.org/0000-0002-6212-1567>),\n  Pierre-Alexandre Fonta [ctb],\n  Gilbert Ritschard [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7776-0903>)",
    "url": "http://traminer.unige.ch",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TraMineR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TraMineR Trajectory Miner: a Sequence Analysis Toolkit Set of sequence analysis tools for manipulating, describing and rendering categorical sequences, and more generally mining sequence data in the field of social sciences. Although this sequence analysis package is primarily intended for state or event sequences that describe time use or life courses such as family formation histories or professional careers, its features also apply to many other kinds of categorical sequence data. It accepts many different sequence representations as input and provides tools for converting sequences from one format to another. It offers several functions for describing and rendering sequences, for computing distances between sequences with different metrics (among which optimal matching), original dissimilarity-based analysis tools, and functions for extracting the most frequent event subsequences and identifying the most discriminating ones among them. A user's guide can be found on the TraMineR web page.  "
  },
  {
    "id": 216,
    "package_name": "WARDEN",
    "title": "Workflows for Health Technology Assessments in R using Discrete\nEveNts",
    "description": "Toolkit to support and perform discrete event simulations with and without\n    resource constraints in the context of health technology assessments (HTA).\n    The package focuses on cost-effectiveness modelling and aims to be submission-ready\n    to relevant HTA bodies in alignment with 'NICE TSD 15'\n    <https://sheffield.ac.uk/nice-dsu/tsds/patient-level-simulation>.\n    More details an examples can be found in the package website <https://jsanchezalv.github.io/WARDEN/>.",
    "version": "2.0.1",
    "maintainer": "Javier Sanchez Alvarez <javiersanchezeco@gmail.com>",
    "author": "Javier Sanchez Alvarez [aut, cre],\n  Gabriel Lemyre [ctb],\n  Valerie Aponte Ribero [ctb]",
    "url": "https://jsanchezalv.github.io/WARDEN/",
    "bug_reports": "https://github.com/jsanchezalv/WARDEN/issues",
    "repository": "https://cran.r-project.org/package=WARDEN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WARDEN Workflows for Health Technology Assessments in R using Discrete\nEveNts Toolkit to support and perform discrete event simulations with and without\n    resource constraints in the context of health technology assessments (HTA).\n    The package focuses on cost-effectiveness modelling and aims to be submission-ready\n    to relevant HTA bodies in alignment with 'NICE TSD 15'\n    <https://sheffield.ac.uk/nice-dsu/tsds/patient-level-simulation>.\n    More details an examples can be found in the package website <https://jsanchezalv.github.io/WARDEN/>.  "
  },
  {
    "id": 315,
    "package_name": "bqtl",
    "title": "Bayesian QTL Mapping Toolkit",
    "description": "QTL mapping toolkit for inbred crosses and recombinant\n        inbred lines. Includes maximum likelihood and Bayesian tools.",
    "version": "1.0-39",
    "maintainer": "Charles C. Berry <cberry@ucsd.edu>",
    "author": "Charles C. Berry [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bqtl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bqtl Bayesian QTL Mapping Toolkit QTL mapping toolkit for inbred crosses and recombinant\n        inbred lines. Includes maximum likelihood and Bayesian tools.  "
  },
  {
    "id": 317,
    "package_name": "brickster",
    "title": "R Toolkit for 'Databricks'",
    "description": "Collection of utilities that improve using 'Databricks' from R. \n  Primarily functions that wrap specific 'Databricks' APIs\n  (<https://docs.databricks.com/api>), 'RStudio' connection pane support, quality\n  of life functions to make 'Databricks' simpler to use.",
    "version": "0.2.11",
    "maintainer": "Zac Davies <zac@databricks.com>",
    "author": "Zac Davies [aut, cre],\n  Rafi Kurlansik [aut],\n  Databricks [cph, fnd]",
    "url": "https://github.com/databrickslabs/brickster",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=brickster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brickster R Toolkit for 'Databricks' Collection of utilities that improve using 'Databricks' from R. \n  Primarily functions that wrap specific 'Databricks' APIs\n  (<https://docs.databricks.com/api>), 'RStudio' connection pane support, quality\n  of life functions to make 'Databricks' simpler to use.  "
  },
  {
    "id": 363,
    "package_name": "cifmodeling",
    "title": "Visualization and Polytomous Modeling of Survival and Competing\nRisks",
    "description": "A publication-ready toolkit for modern survival and competing risks\n    analysis with a minimal, formula-based interface. Both nonparametric\n    estimation and direct polytomous regression of cumulative incidence\n    functions (CIFs) are supported. The main functions 'cifcurve()', 'cifplot()',\n    and 'cifpanel()' estimate survival and CIF curves and produce high-quality\n    graphics with risk tables, censoring and competing-risk marks, and\n    multi-panel or inset layouts built on 'ggplot2' and 'ggsurvfit'. The modeling\n    function 'polyreg()' performs direct polytomous regression for coherent joint\n    modeling of all cause-specific CIFs to estimate risk ratios, odds ratios, or\n    subdistribution hazard ratios at user-specified time points. All core\n    functions adopt a formula-and-data syntax and return tidy and extensible\n    outputs that integrate smoothly with 'modelsummary', 'broom', and the broader\n    'tidyverse' ecosystem. Key numerical routines are implemented in C++ via\n    'Rcpp'.",
    "version": "0.9.6",
    "maintainer": "Shiro Tanaka <gestimation@gmail.com>",
    "author": "Shiro Tanaka [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6817-5235>),\n  Shigetaka Kobari [ctb],\n  Chisato Honda [ctb]",
    "url": "https://gestimation.github.io/cifmodeling/,\nhttps://github.com/gestimation/cifmodeling",
    "bug_reports": "https://github.com/gestimation/cifmodeling/issues",
    "repository": "https://cran.r-project.org/package=cifmodeling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cifmodeling Visualization and Polytomous Modeling of Survival and Competing\nRisks A publication-ready toolkit for modern survival and competing risks\n    analysis with a minimal, formula-based interface. Both nonparametric\n    estimation and direct polytomous regression of cumulative incidence\n    functions (CIFs) are supported. The main functions 'cifcurve()', 'cifplot()',\n    and 'cifpanel()' estimate survival and CIF curves and produce high-quality\n    graphics with risk tables, censoring and competing-risk marks, and\n    multi-panel or inset layouts built on 'ggplot2' and 'ggsurvfit'. The modeling\n    function 'polyreg()' performs direct polytomous regression for coherent joint\n    modeling of all cause-specific CIFs to estimate risk ratios, odds ratios, or\n    subdistribution hazard ratios at user-specified time points. All core\n    functions adopt a formula-and-data syntax and return tidy and extensible\n    outputs that integrate smoothly with 'modelsummary', 'broom', and the broader\n    'tidyverse' ecosystem. Key numerical routines are implemented in C++ via\n    'Rcpp'.  "
  },
  {
    "id": 545,
    "package_name": "examly",
    "title": "Statistical Metrics and Reporting Tool",
    "description": "A 'Shiny'-based toolkit for item/test analysis. It is designed for multiple-choice, true-false, and open-ended questions. The toolkit is usable with datasets in 1-0 or other formats. Key analyses include difficulty, discrimination, response-option analysis, reports. The classical test theory methods used are described in Ebel & Frisbie (1991, ISBN:978-0132892314).",
    "version": "0.2.0",
    "maintainer": "Ahmet \u00c7al\u0131\u015fkan <ahmetcaliskan1987@gmail.com>",
    "author": "Ahmet \u00c7al\u0131\u015fkan [aut, cre],\n  Abdullah Faruk K\u0131l\u0131\u00e7 [aut]",
    "url": "https://github.com/ahmetcaliskan1987/examly",
    "bug_reports": "https://github.com/ahmetcaliskan1987/examly/issues",
    "repository": "https://cran.r-project.org/package=examly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "examly Statistical Metrics and Reporting Tool A 'Shiny'-based toolkit for item/test analysis. It is designed for multiple-choice, true-false, and open-ended questions. The toolkit is usable with datasets in 1-0 or other formats. Key analyses include difficulty, discrimination, response-option analysis, reports. The classical test theory methods used are described in Ebel & Frisbie (1991, ISBN:978-0132892314).  "
  },
  {
    "id": 578,
    "package_name": "fjoin",
    "title": "Data Frame Joins Leveraging 'data.table'",
    "description": "Extends 'data.table' join functionality, lets it work with any\n  data frame class, and provides a familiar 'x'/'y'-style interface, enabling\n  broad use across R. Offers NA-safe matching by default, on-the-fly column\n  selection, multiple match-handling on both sides, 'x' or 'y' row order, and a\n  row origin indicator. Performs inner, left, right, full, semi- and anti-joins\n  with equality and inequality conditions, plus cross joins. Specific support\n  for 'data.table', (grouped) tibble, and 'sf'/'sfc' objects and their\n  attributes; returns a plain data frame otherwise. Avoids data-copying of\n  inputs and outputs. Allows displaying the 'data.table' code instead of (or as\n  well as) executing it.",
    "version": "0.1.0",
    "maintainer": "Toby Robertson <trobx@proton.me>",
    "author": "Toby Robertson [aut, cre]",
    "url": "https://trobx.github.io/fjoin/",
    "bug_reports": "https://github.com/trobx/fjoin/issues",
    "repository": "https://cran.r-project.org/package=fjoin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fjoin Data Frame Joins Leveraging 'data.table' Extends 'data.table' join functionality, lets it work with any\n  data frame class, and provides a familiar 'x'/'y'-style interface, enabling\n  broad use across R. Offers NA-safe matching by default, on-the-fly column\n  selection, multiple match-handling on both sides, 'x' or 'y' row order, and a\n  row origin indicator. Performs inner, left, right, full, semi- and anti-joins\n  with equality and inequality conditions, plus cross joins. Specific support\n  for 'data.table', (grouped) tibble, and 'sf'/'sfc' objects and their\n  attributes; returns a plain data frame otherwise. Avoids data-copying of\n  inputs and outputs. Allows displaying the 'data.table' code instead of (or as\n  well as) executing it.  "
  },
  {
    "id": 679,
    "package_name": "hdf5lib",
    "title": "Headers and Static Libraries for 'HDF5'",
    "description": "'HDF5' (Hierarchical Data Format 5) is a high-performance \n    library and file format for storing and managing large, complex \n    data. This package provides the static libraries and headers for \n    the 'HDF5' 'C' library (release 2.0.0). It is intended for R \n    package developers to use in the 'LinkingTo' field, which \n    eliminates the need for users to install system-level 'HDF5' \n    dependencies. This build is compiled with thread-safety enabled \n    and supports dynamic loading of external compression filters. \n    'HDF5' is developed by 'The HDF Group' <https://www.hdfgroup.org/>.",
    "version": "2.0.0.3",
    "maintainer": "Daniel P. Smith <dansmith01@gmail.com>",
    "author": "Daniel P. Smith [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2479-2044>),\n  The HDF Group [cph] (Copyright holder of the HDF5 library),\n  Jean-loup Mark [cph] (Copyright holder of the zlib library),\n  Gailly Adler [cph] (Copyright holder of the zlib library)",
    "url": "https://github.com/cmmr/hdf5lib, https://cmmr.github.io/hdf5lib/",
    "bug_reports": "https://github.com/cmmr/hdf5lib/issues",
    "repository": "https://cran.r-project.org/package=hdf5lib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdf5lib Headers and Static Libraries for 'HDF5' 'HDF5' (Hierarchical Data Format 5) is a high-performance \n    library and file format for storing and managing large, complex \n    data. This package provides the static libraries and headers for \n    the 'HDF5' 'C' library (release 2.0.0). It is intended for R \n    package developers to use in the 'LinkingTo' field, which \n    eliminates the need for users to install system-level 'HDF5' \n    dependencies. This build is compiled with thread-safety enabled \n    and supports dynamic loading of external compression filters. \n    'HDF5' is developed by 'The HDF Group' <https://www.hdfgroup.org/>.  "
  },
  {
    "id": 817,
    "package_name": "misha",
    "title": "Toolkit for Analysis of Genomic Data",
    "description": "A toolkit for analysis of genomic data. The 'misha' package\n    implements an efficient data structure for storing genomic data, and\n    provides a set of functions for data extraction, manipulation and\n    analysis. Some of the 2D genome algorithms were described in Yaffe and Tanay\n    (2011) <doi:10.1038/ng.947>. ",
    "version": "5.3.2",
    "maintainer": "Aviezer Lifshitz <aviezer.lifshitz@weizmann.ac.il>",
    "author": "Misha Hoichman [aut],\n  Aviezer Lifshitz [aut, cre],\n  Eitan Yaffe [aut],\n  Amos Tanay [aut],\n  Weizmann Institute of Science [cph]",
    "url": "https://tanaylab.github.io/misha/,\nhttps://github.com/tanaylab/misha",
    "bug_reports": "https://github.com/tanaylab/misha/issues",
    "repository": "https://cran.r-project.org/package=misha",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "misha Toolkit for Analysis of Genomic Data A toolkit for analysis of genomic data. The 'misha' package\n    implements an efficient data structure for storing genomic data, and\n    provides a set of functions for data extraction, manipulation and\n    analysis. Some of the 2D genome algorithms were described in Yaffe and Tanay\n    (2011) <doi:10.1038/ng.947>.   "
  },
  {
    "id": 943,
    "package_name": "parallelly",
    "title": "Enhancing the 'parallel' Package",
    "description": "Utility functions that enhance the 'parallel' package and support the built-in parallel backends of the 'future' package.  For example, availableCores() gives the number of CPU cores available to your R process as given by the operating system, 'cgroups' and Linux containers, R options, and environment variables, including those set by job schedulers on high-performance compute clusters. If none is set, it will fall back to parallel::detectCores(). Another example is makeClusterPSOCK(), which is backward compatible with parallel::makePSOCKcluster() while doing a better job in setting up remote cluster workers without the need for configuring the firewall to do port-forwarding to your local computer.",
    "version": "1.46.0",
    "maintainer": "Henrik Bengtsson <henrikb@braju.com>",
    "author": "Henrik Bengtsson [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7579-5165>),\n  Mike Cheng [ctb]",
    "url": "https://parallelly.futureverse.org,\nhttps://github.com/futureverse/parallelly",
    "bug_reports": "https://github.com/futureverse/parallelly/issues",
    "repository": "https://cran.r-project.org/package=parallelly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parallelly Enhancing the 'parallel' Package Utility functions that enhance the 'parallel' package and support the built-in parallel backends of the 'future' package.  For example, availableCores() gives the number of CPU cores available to your R process as given by the operating system, 'cgroups' and Linux containers, R options, and environment variables, including those set by job schedulers on high-performance compute clusters. If none is set, it will fall back to parallel::detectCores(). Another example is makeClusterPSOCK(), which is backward compatible with parallel::makePSOCKcluster() while doing a better job in setting up remote cluster workers without the need for configuring the firewall to do port-forwarding to your local computer.  "
  },
  {
    "id": 1118,
    "package_name": "rjd3jars",
    "title": "External jars for 'rjdverse' R Packages",
    "description": "It provides external jars required for the 'rjdverse' (as 'rjd3toolkit', 'rjd3x13' and 'rjd3tramoseats').",
    "version": "0.0.1",
    "maintainer": "Tanguy Barthelemy <tanguy.barthelemy@insee.fr>",
    "author": "Tanguy Barthelemy [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rjd3jars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rjd3jars External jars for 'rjdverse' R Packages It provides external jars required for the 'rjdverse' (as 'rjd3toolkit', 'rjd3x13' and 'rjd3tramoseats').  "
  },
  {
    "id": 1242,
    "package_name": "snic",
    "title": "Superpixel Segmentation with the Simple Non-Iterative Clustering\nAlgorithm",
    "description": "Implements the Simple Non-Iterative Clustering algorithm for \n    superpixel segmentation of multi-band images, as introduced by Achanta \n    and Susstrunk (2017) <doi:10.1109/CVPR.2017.520>. Supports both standard \n    image arrays and geospatial raster objects, with a design that can be \n    extended to other spatial data frameworks. The algorithm groups adjacent \n    pixels into compact, coherent regions based on spectral similarity and \n    spatial proximity. A high-performance implementation supports images \n    with arbitrary spectral bands.",
    "version": "0.6.1",
    "maintainer": "Rolf Simoes <rolfsimoes@gmail.com>",
    "author": "Rolf Simoes [aut, cre] (ORCID: <https://orcid.org/0000-0003-0953-4132>),\n  Felipe Souza [aut] (ORCID: <https://orcid.org/0000-0002-5826-1700>),\n  Felipe Carlos [aut] (ORCID: <https://orcid.org/0000-0002-3334-4315>),\n  Gilberto Camara [aut, rth] (ORCID:\n    <https://orcid.org/0000-0002-3681-487X>)",
    "url": "https://github.com/rolfsimoes/snic,\nhttps://rolfsimoes.github.io/snic/",
    "bug_reports": "https://github.com/rolfsimoes/snic/issues",
    "repository": "https://cran.r-project.org/package=snic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "snic Superpixel Segmentation with the Simple Non-Iterative Clustering\nAlgorithm Implements the Simple Non-Iterative Clustering algorithm for \n    superpixel segmentation of multi-band images, as introduced by Achanta \n    and Susstrunk (2017) <doi:10.1109/CVPR.2017.520>. Supports both standard \n    image arrays and geospatial raster objects, with a design that can be \n    extended to other spatial data frameworks. The algorithm groups adjacent \n    pixels into compact, coherent regions based on spectral similarity and \n    spatial proximity. A high-performance implementation supports images \n    with arbitrary spectral bands.  "
  },
  {
    "id": 1340,
    "package_name": "tidyfst",
    "title": "Tidy Verbs for Fast Data Manipulation",
    "description": "A toolkit of tidy data manipulation verbs with 'data.table' as the backend.\n  Combining the merits of syntax elegance from 'dplyr' and computing performance from 'data.table', \n  'tidyfst' intends to provide users with state-of-the-art data manipulation tools with least pain.\n  This package is an extension of 'data.table'. While enjoying a tidy syntax, \n  it also wraps combinations of efficient functions to facilitate frequently-used data operations.  ",
    "version": "1.8.3",
    "maintainer": "Tian-Yuan Huang <huang.tian-yuan@qq.com>",
    "author": "Tian-Yuan Huang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4151-3764>)",
    "url": "https://github.com/hope-data-science/tidyfst,\nhttps://hope-data-science.github.io/tidyfst/",
    "bug_reports": "https://github.com/hope-data-science/tidyfst/issues",
    "repository": "https://cran.r-project.org/package=tidyfst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyfst Tidy Verbs for Fast Data Manipulation A toolkit of tidy data manipulation verbs with 'data.table' as the backend.\n  Combining the merits of syntax elegance from 'dplyr' and computing performance from 'data.table', \n  'tidyfst' intends to provide users with state-of-the-art data manipulation tools with least pain.\n  This package is an extension of 'data.table'. While enjoying a tidy syntax, \n  it also wraps combinations of efficient functions to facilitate frequently-used data operations.    "
  },
  {
    "id": 1428,
    "package_name": "waysign",
    "title": "Multi-Purpose and High-Performance Routing",
    "description": "Provides routing based on the 'path-tree' 'Rust' crate. The routing\n    is general purpose in the sense that any type of R object can be associated\n    with a path, not just a handler function.",
    "version": "0.1.0",
    "maintainer": "Thomas Lin Pedersen <thomas.pedersen@posit.co>",
    "author": "Thomas Lin Pedersen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5147-4711>),\n  Posit, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/thomasp85/waysign",
    "bug_reports": "https://github.com/thomasp85/waysign/issues",
    "repository": "https://cran.r-project.org/package=waysign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "waysign Multi-Purpose and High-Performance Routing Provides routing based on the 'path-tree' 'Rust' crate. The routing\n    is general purpose in the sense that any type of R object can be associated\n    with a path, not just a handler function.  "
  },
  {
    "id": 1486,
    "package_name": "ABM",
    "title": "Agent Based Model Simulation Framework",
    "description": "A high-performance, flexible and extensible framework to\n    develop continuous-time agent based models. Its high performance\n    allows it to simulate millions of agents efficiently. Agents are\n    defined by their states (arbitrary R lists). The events are handled in\n    chronological order. This avoids the multi-event interaction problem\n    in a time step of discrete-time simulations, and gives precise\n    outcomes. The states are modified by provided or user-defined events.\n    The framework provides a flexible and customizable implementation of\n    state transitions (either spontaneous or caused by agent\n    interactions), making the framework suitable to apply to epidemiology\n    and ecology, e.g., to model life history stages, competition and\n    cooperation, and disease and information spread. The agent\n    interactions are flexible and extensible. The framework provides\n    random mixing and network interactions, and supports multi-level\n    mixing patterns.  It can be easily extended to other interactions such\n    as inter- and intra-households (or workplaces and schools) by\n    subclassing an R6 class. It can be used to study the effect of\n    age-specific, group-specific, and contact- specific intervention\n    strategies, and complex interactions between individual behavior and\n    population dynamics. This modeling concept can also be used in\n    business, economical and political models. As a generic event based\n    framework, it can be applied to many other fields. More information\n    about the implementation and examples can be found at\n    <https://github.com/junlingm/ABM>.",
    "version": "0.4.3",
    "maintainer": "Junling Ma <junlingm@uvic.ca>",
    "author": "Junling Ma [aut, cre]",
    "url": "https://github.com/junlingm/ABM",
    "bug_reports": "https://github.com/junlingm/ABM/issues",
    "repository": "https://cran.r-project.org/package=ABM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ABM Agent Based Model Simulation Framework A high-performance, flexible and extensible framework to\n    develop continuous-time agent based models. Its high performance\n    allows it to simulate millions of agents efficiently. Agents are\n    defined by their states (arbitrary R lists). The events are handled in\n    chronological order. This avoids the multi-event interaction problem\n    in a time step of discrete-time simulations, and gives precise\n    outcomes. The states are modified by provided or user-defined events.\n    The framework provides a flexible and customizable implementation of\n    state transitions (either spontaneous or caused by agent\n    interactions), making the framework suitable to apply to epidemiology\n    and ecology, e.g., to model life history stages, competition and\n    cooperation, and disease and information spread. The agent\n    interactions are flexible and extensible. The framework provides\n    random mixing and network interactions, and supports multi-level\n    mixing patterns.  It can be easily extended to other interactions such\n    as inter- and intra-households (or workplaces and schools) by\n    subclassing an R6 class. It can be used to study the effect of\n    age-specific, group-specific, and contact- specific intervention\n    strategies, and complex interactions between individual behavior and\n    population dynamics. This modeling concept can also be used in\n    business, economical and political models. As a generic event based\n    framework, it can be applied to many other fields. More information\n    about the implementation and examples can be found at\n    <https://github.com/junlingm/ABM>.  "
  },
  {
    "id": 1488,
    "package_name": "ABRSQOL",
    "title": "Quality-of-Life Solver for \"Measuring Quality of Life under\nSpatial Frictions\"",
    "description": "This toolkit implements a numerical solution algorithm \n    to invert a quality of life measure from observed data. Unlike\n    the traditional Rosen-Roback measure, this measure accounts for\n    mobility frictions\u2014generated by idiosyncratic tastes and local\n    ties \u2014 and trade frictions \u2014 generated by trade costs and\n    non-tradable services, thereby reducing non-classical\n    measurement error. The QoL measure is based on Ahlfeldt, Bald,\n    Roth, Seidel (2024)\n    <https://econpapers.repec.org/RePEc:boc:bocode:s459382>\n    \"Measuring Quality of Life under Spatial Frictions\". When using\n    this programme or the toolkit in your work, please cite the paper.",
    "version": "1.0.0",
    "maintainer": "Max von Mylius <max.mylius@hu-berlin.de>",
    "author": "Gabriel M Ahlfeldt [cph, aut] (Humboldt University of Berlin),\n  Fabian Bald [aut] (European University Viadrina),\n  Duncan Roth [aut] (Institute for Employment Research),\n  Tobias Seidel [aut] (University of Duisburg-Essen),\n  Max von Mylius [trl, cre] (Humboldt University of Berlin)",
    "url": "https://github.com/Ahlfeldt/ABRSQOL-toolkit#readme",
    "bug_reports": "https://github.com/Ahlfeldt/ABRSQOL-toolkit/issues",
    "repository": "https://cran.r-project.org/package=ABRSQOL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ABRSQOL Quality-of-Life Solver for \"Measuring Quality of Life under\nSpatial Frictions\" This toolkit implements a numerical solution algorithm \n    to invert a quality of life measure from observed data. Unlike\n    the traditional Rosen-Roback measure, this measure accounts for\n    mobility frictions\u2014generated by idiosyncratic tastes and local\n    ties \u2014 and trade frictions \u2014 generated by trade costs and\n    non-tradable services, thereby reducing non-classical\n    measurement error. The QoL measure is based on Ahlfeldt, Bald,\n    Roth, Seidel (2024)\n    <https://econpapers.repec.org/RePEc:boc:bocode:s459382>\n    \"Measuring Quality of Life under Spatial Frictions\". When using\n    this programme or the toolkit in your work, please cite the paper.  "
  },
  {
    "id": 1522,
    "package_name": "AFR",
    "title": "Toolkit for Regression Analysis of Kazakhstan Banking Sector\nData",
    "description": "Tool is created for regression, prediction and forecast analysis of macroeconomic and credit data.\n  The package includes functions from existing R packages adapted for banking sector of Kazakhstan.\n  The purpose of the package is to optimize statistical functions for easier interpretation for bank analysts and non-statisticians.",
    "version": "0.3.7",
    "maintainer": "Sultan Zhaparov <saldau.sultan@gmail.com>",
    "author": "Timur Abilkassymov [aut],\n  Shyngys Shuneyev [aut],\n  Alua Makhmetova [aut],\n  Sultan Zhaparov [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AFR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AFR Toolkit for Regression Analysis of Kazakhstan Banking Sector\nData Tool is created for regression, prediction and forecast analysis of macroeconomic and credit data.\n  The package includes functions from existing R packages adapted for banking sector of Kazakhstan.\n  The purpose of the package is to optimize statistical functions for easier interpretation for bank analysts and non-statisticians.  "
  },
  {
    "id": 1640,
    "package_name": "AdapDiscom",
    "title": "Adaptive Sparse Regression for Block Missing Multimodal Data",
    "description": "Provides adaptive direct sparse regression for high-dimensional multimodal data with heterogeneous missing patterns and measurement errors. 'AdapDISCOM' extends the 'DISCOM' framework with modality-specific adaptive weighting to handle varying data structures and error magnitudes across blocks. The method supports flexible block configurations (any K blocks) and includes robust variants for heavy-tailed distributions ('AdapDISCOM'-Huber) and fast implementations for large-scale applications (Fast-'AdapDISCOM'). Designed for realistic multimodal scenarios where different data sources exhibit distinct missing data patterns and contamination levels. Diakit\u00e9 et al. (2025) <doi:10.48550/arXiv.2508.00120>.",
    "version": "1.0.0",
    "maintainer": "Diakite Abdoul Oudouss <abdouloudoussdiakite@gmail.com>",
    "author": "Diakite Abdoul Oudouss [aut, cre, cph],\n  Barry Amadou [aut]",
    "url": "https://doi.org/10.48550/arXiv.2508.00120",
    "bug_reports": "https://github.com/AODiakite/AdapDiscom/issues",
    "repository": "https://cran.r-project.org/package=AdapDiscom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AdapDiscom Adaptive Sparse Regression for Block Missing Multimodal Data Provides adaptive direct sparse regression for high-dimensional multimodal data with heterogeneous missing patterns and measurement errors. 'AdapDISCOM' extends the 'DISCOM' framework with modality-specific adaptive weighting to handle varying data structures and error magnitudes across blocks. The method supports flexible block configurations (any K blocks) and includes robust variants for heavy-tailed distributions ('AdapDISCOM'-Huber) and fast implementations for large-scale applications (Fast-'AdapDISCOM'). Designed for realistic multimodal scenarios where different data sources exhibit distinct missing data patterns and contamination levels. Diakit\u00e9 et al. (2025) <doi:10.48550/arXiv.2508.00120>.  "
  },
  {
    "id": 1656,
    "package_name": "AggregateR",
    "title": "Aggregate Numeric, Date and Categorical Variables",
    "description": "Convenience functions for aggregating a data frame or data table.\n    Currently mean, sum and variance are supported. For Date variables, the recency\n    and duration are supported. There is also support for dummy variables in\n    predictive contexts. Code has been completely re-written in data.table for computational speed. ",
    "version": "0.1.1",
    "maintainer": "Matthias Bogaert <matthias.bogaert@ugent.be>",
    "author": "Matthias Bogaert, Michel Ballings, Dirk Van den Poel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AggregateR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AggregateR Aggregate Numeric, Date and Categorical Variables Convenience functions for aggregating a data frame or data table.\n    Currently mean, sum and variance are supported. For Date variables, the recency\n    and duration are supported. There is also support for dummy variables in\n    predictive contexts. Code has been completely re-written in data.table for computational speed.   "
  },
  {
    "id": 1747,
    "package_name": "AutoPlots",
    "title": "Creating Echarts Visualizations as Easy as Possible",
    "description": "Create beautiful and interactive visualizations in a single function call. The 'data.table' package is utilized to perform the data wrangling necessary to prepare your data for the plot types you wish to build, along with allowing fast processing for big data. There are two broad classes of plots available: standard plots and machine learning evaluation plots. There are lots of parameters available in each plot type function for customizing the plots (such as faceting) and data wrangling (such as variable transformations and aggregation).",
    "version": "1.0.0",
    "maintainer": "Adrian Antico <adrianantico@gmail.com>",
    "author": "Adrian Antico [aut, cre, cph]",
    "url": "https://github.com/AdrianAntico/AutoPlots",
    "bug_reports": "https://github.com/AdrianAntico/AutoPlots/issues",
    "repository": "https://cran.r-project.org/package=AutoPlots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AutoPlots Creating Echarts Visualizations as Easy as Possible Create beautiful and interactive visualizations in a single function call. The 'data.table' package is utilized to perform the data wrangling necessary to prepare your data for the plot types you wish to build, along with allowing fast processing for big data. There are two broad classes of plots available: standard plots and machine learning evaluation plots. There are lots of parameters available in each plot type function for customizing the plots (such as faceting) and data wrangling (such as variable transformations and aggregation).  "
  },
  {
    "id": 1871,
    "package_name": "BMAmevt",
    "title": "Multivariate Extremes: Bayesian Estimation of the Spectral\nMeasure",
    "description": "Toolkit for Bayesian estimation of the dependence structure in multivariate extreme value parametric models, following Sabourin and Naveau (2014) <doi:10.1016/j.csda.2013.04.021> and Sabourin, Naveau and Fougeres (2013) <doi:10.1007/s10687-012-0163-0>.",
    "version": "1.0.5",
    "maintainer": "Leo Belzile <belzilel@gmail.com>",
    "author": "Leo Belzile [cre] (ORCID: <https://orcid.org/0000-0002-9135-014X>),\n  Anne Sabourin [aut] (ORCID: <https://orcid.org/0000-0002-5096-9157>)",
    "url": "",
    "bug_reports": "https://github.com/lbelzile/BMAmevt/issues/",
    "repository": "https://cran.r-project.org/package=BMAmevt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BMAmevt Multivariate Extremes: Bayesian Estimation of the Spectral\nMeasure Toolkit for Bayesian estimation of the dependence structure in multivariate extreme value parametric models, following Sabourin and Naveau (2014) <doi:10.1016/j.csda.2013.04.021> and Sabourin, Naveau and Fougeres (2013) <doi:10.1007/s10687-012-0163-0>.  "
  },
  {
    "id": 1903,
    "package_name": "BSDA",
    "title": "Basic Statistics and Data Analysis",
    "description": "Data sets for book \"Basic Statistics and Data Analysis\" by\n    Larry J. Kitchens.",
    "version": "1.2.2",
    "maintainer": "Alan T. Arnholt <arnholtat@appstate.edu>",
    "author": "Alan T. Arnholt [aut, cre],\n  Ben Evans [aut]",
    "url": "https://github.com/alanarnholt/BSDA,\nhttps://alanarnholt.github.io/BSDA/",
    "bug_reports": "https://github.com/alanarnholt/BSDA/issues",
    "repository": "https://cran.r-project.org/package=BSDA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BSDA Basic Statistics and Data Analysis Data sets for book \"Basic Statistics and Data Analysis\" by\n    Larry J. Kitchens.  "
  },
  {
    "id": 2056,
    "package_name": "BiSEp",
    "title": "Toolkit to Identify Candidate Synthetic Lethality",
    "description": "Enables the user to infer potential synthetic lethal relationships\n    by analysing relationships between bimodally distributed gene pairs in big\n    gene expression datasets.  Enables the user to visualise these candidate\n    synthetic lethal relationships.",
    "version": "2.3",
    "maintainer": "Mark Wappett <m.wappett@qub.ac.uk>",
    "author": "Mark Wappett",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BiSEp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BiSEp Toolkit to Identify Candidate Synthetic Lethality Enables the user to infer potential synthetic lethal relationships\n    by analysing relationships between bimodally distributed gene pairs in big\n    gene expression datasets.  Enables the user to visualise these candidate\n    synthetic lethal relationships.  "
  },
  {
    "id": 2170,
    "package_name": "CACIMAR",
    "title": "Cross-Species Analysis of Cell Identities, Markers and\nRegulations",
    "description": "A toolkit to perform cross-species analysis based on scRNA-seq data. This package contains 5 main features. (1) identify Markers in each cluster. (2) Cell type annotation (3) identify conserved markers. (4) identify conserved cell types. (5) identify conserved modules of regulatory networks.",
    "version": "1.0.0",
    "maintainer": "Junyao Jiang <jiangjunyao789@163.com>",
    "author": "Junyao Jiang <jiangjunyao789@163.com>",
    "url": "https://github.com/jiang-junyao/CACIMAR",
    "bug_reports": "https://github.com/jiang-junyao/CACIMAR/issues",
    "repository": "https://cran.r-project.org/package=CACIMAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CACIMAR Cross-Species Analysis of Cell Identities, Markers and\nRegulations A toolkit to perform cross-species analysis based on scRNA-seq data. This package contains 5 main features. (1) identify Markers in each cluster. (2) Cell type annotation (3) identify conserved markers. (4) identify conserved cell types. (5) identify conserved modules of regulatory networks.  "
  },
  {
    "id": 2186,
    "package_name": "CARMS",
    "title": "Continuous Time Markov Rate Modeling for Reliability Analysis",
    "description": "Emulation of an application originally created by Paul Pukite. Computer Aided Rate Modeling and Simulation.  Jan Pukite and Paul Pukite, (1998, ISBN 978-0-7803-3482), William J. Stewart, (1994, ISBN: 0-691-03699-3).",
    "version": "1.0.1",
    "maintainer": "Jacob Ormerod <jake@openreliability.org>",
    "author": "David Silkworth [aut],\n  Paul Pukite [aut],\n  Jacob Ormerod [cre],\n  OpenReliability.org [cph]",
    "url": "http://www.openreliability.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CARMS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CARMS Continuous Time Markov Rate Modeling for Reliability Analysis Emulation of an application originally created by Paul Pukite. Computer Aided Rate Modeling and Simulation.  Jan Pukite and Paul Pukite, (1998, ISBN 978-0-7803-3482), William J. Stewart, (1994, ISBN: 0-691-03699-3).  "
  },
  {
    "id": 2305,
    "package_name": "CNVScope",
    "title": "A Versatile Toolkit for Copy Number Variation Relationship Data\nAnalysis and Visualization",
    "description": "Provides the ability to create interaction maps, discover CNV map domains (edges), gene annotate interactions, and create interactive visualizations of these CNV interaction maps.",
    "version": "3.7.2",
    "maintainer": "James Dalgleish <james.dalgleish@nih.gov>",
    "author": "James Dalgeish, Yonghong Wang, Jack Zhu, Paul Meltzer",
    "url": "https://github.com/jamesdalg/CNVScope/",
    "bug_reports": "https://github.com/jamesdalg/CNVScope/issues/",
    "repository": "https://cran.r-project.org/package=CNVScope",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CNVScope A Versatile Toolkit for Copy Number Variation Relationship Data\nAnalysis and Visualization Provides the ability to create interaction maps, discover CNV map domains (edges), gene annotate interactions, and create interactive visualizations of these CNV interaction maps.  "
  },
  {
    "id": 2309,
    "package_name": "COINT",
    "title": "Unit Root Tests with Structural Breaks and Fully-Modified\nEstimators",
    "description": "Procedures include Phillips (1995) FMVAR <doi:10.2307/2171721>,  Kitamura and Phillips (1997) FMGMM <doi:10.1016/S0304-4076(97)00004-3>, Park (1992) CCR <doi:10.2307/2951679>, and so on. Tests with 1 or 2 structural breaks include Gregory and Hansen (1996) <doi:10.1016/0304-4076(69)41685-7>, Zivot and Andrews (1992) <doi:10.2307/1391541>, and Kurozumi (2002) <doi:10.1016/S0304-4076(01)00106-3>.",
    "version": "0.0.2",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "author": "Ho Tsung-wu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=COINT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "COINT Unit Root Tests with Structural Breaks and Fully-Modified\nEstimators Procedures include Phillips (1995) FMVAR <doi:10.2307/2171721>,  Kitamura and Phillips (1997) FMGMM <doi:10.1016/S0304-4076(97)00004-3>, Park (1992) CCR <doi:10.2307/2951679>, and so on. Tests with 1 or 2 structural breaks include Gregory and Hansen (1996) <doi:10.1016/0304-4076(69)41685-7>, Zivot and Andrews (1992) <doi:10.2307/1391541>, and Kurozumi (2002) <doi:10.1016/S0304-4076(01)00106-3>.  "
  },
  {
    "id": 2382,
    "package_name": "CWT",
    "title": "Continuous Wavelet Transformation for Spectroscopy",
    "description": "Fast application of Continuous Wavelet Transformation ('CWT') on time\n    series with special attention to spectroscopy. It is written using\n    data.table and 'C++' language and in some functions it is possible to\n    use parallel processing to speed-up the computation over samples. Currently, \n    only the second derivative of a Gaussian wavelet function is implemented.",
    "version": "0.2.1",
    "maintainer": "J. Antonio Guzm\u00e1n Q. <antguz06@gmail.com>",
    "author": "J. Antonio Guzm\u00e1n Q. [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-0721-148X>)",
    "url": "https://github.com/Antguz/CWT",
    "bug_reports": "https://github.com/Antguz/CWT/issues",
    "repository": "https://cran.r-project.org/package=CWT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CWT Continuous Wavelet Transformation for Spectroscopy Fast application of Continuous Wavelet Transformation ('CWT') on time\n    series with special attention to spectroscopy. It is written using\n    data.table and 'C++' language and in some functions it is possible to\n    use parallel processing to speed-up the computation over samples. Currently, \n    only the second derivative of a Gaussian wavelet function is implemented.  "
  },
  {
    "id": 2401,
    "package_name": "CareDensity",
    "title": "Calculate the Care Density or Fragmented Care Density Given a\nPatient-Sharing Network",
    "description": "Given a patient-sharing network, calculate either the classic care density as\n\tproposed by Pollack et al. (2013) <doi:10.1007/s11606-012-2104-7> or the\n\tfragmented care density as proposed by Engels et al. (2024) <doi:10.1186/s12874-023-02106-0>.\n\tBy utilizing the 'igraph' and 'data.table' packages, the provided functions scale well for\n\tvery large graphs.",
    "version": "0.1.0",
    "maintainer": "Robin Denz <robin.denz@rub.de>",
    "author": "Robin Denz [aut, cre]",
    "url": "https://github.com/RobinDenz1/CareDensity,\nhttps://robindenz1.github.io/CareDensity/",
    "bug_reports": "https://github.com/RobinDenz1/CareDensity/issues",
    "repository": "https://cran.r-project.org/package=CareDensity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CareDensity Calculate the Care Density or Fragmented Care Density Given a\nPatient-Sharing Network Given a patient-sharing network, calculate either the classic care density as\n\tproposed by Pollack et al. (2013) <doi:10.1007/s11606-012-2104-7> or the\n\tfragmented care density as proposed by Engels et al. (2024) <doi:10.1186/s12874-023-02106-0>.\n\tBy utilizing the 'igraph' and 'data.table' packages, the provided functions scale well for\n\tvery large graphs.  "
  },
  {
    "id": 2410,
    "package_name": "CatEncoders",
    "title": "Encoders for Categorical Variables",
    "description": "Contains some commonly used categorical variable encoders, such as 'LabelEncoder' and 'OneHotEncoder'. Inspired by the encoders implemented in Python 'sklearn.preprocessing' package (see <http://scikit-learn.org/stable/modules/preprocessing.html>).",
    "version": "0.1.1",
    "maintainer": "nl zhang <setseed2016@gmail.com>",
    "author": "nl zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CatEncoders",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CatEncoders Encoders for Categorical Variables Contains some commonly used categorical variable encoders, such as 'LabelEncoder' and 'OneHotEncoder'. Inspired by the encoders implemented in Python 'sklearn.preprocessing' package (see <http://scikit-learn.org/stable/modules/preprocessing.html>).  "
  },
  {
    "id": 2545,
    "package_name": "CollapseLevels",
    "title": "Collapses Levels, Computes Information Value and WoE",
    "description": "Contains functions to help in selecting and exploring features ( or variables ) in binary classification problems.\n             Provides functions to compute and display information value and weight of evidence (WoE) of the variables , and to convert numeric variables to categorical variables by binning.\n             Functions are also provided  to determine which levels ( or categories ) of a categorical variable can be collapsed (or combined ) based on their response rates.\n             The functions provided only work for binary classification problems.",
    "version": "0.3.0",
    "maintainer": "Krishanu Mukherjee <toton1181@gmail.com>",
    "author": "Krishanu Mukherjee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CollapseLevels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CollapseLevels Collapses Levels, Computes Information Value and WoE Contains functions to help in selecting and exploring features ( or variables ) in binary classification problems.\n             Provides functions to compute and display information value and weight of evidence (WoE) of the variables , and to convert numeric variables to categorical variables by binning.\n             Functions are also provided  to determine which levels ( or categories ) of a categorical variable can be collapsed (or combined ) based on their response rates.\n             The functions provided only work for binary classification problems.  "
  },
  {
    "id": 2564,
    "package_name": "CompMix",
    "title": "A Comprehensive Toolkit for Environmental Mixtures Analysis\n('CompMix')",
    "description": "Quantitative characterization of the health impacts associated with exposure to chemical mixtures has received considerable attention in current environmental and epidemiological studies. 'CompMix' package allows practitioners to estimate the health impacts from exposure to chemical mixtures data through various statistical approaches, including Lasso, Elastic net, Bayeisan kernel machine regression (BKMR), hierNet, Quantile g-computation, Weighted quantile sum (WQS) and Random forest. Hao W, Cathey A, Aung M, Boss J, Meeker J, Mukherjee B. (2024) \"Statistical methods for chemical mixtures: a practitioners guide\". <DOI:10.1101/2024.03.03.24303677>.",
    "version": "0.1.0",
    "maintainer": "Wei Hao <weihao@umich.edu>",
    "author": "Wei Hao [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CompMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CompMix A Comprehensive Toolkit for Environmental Mixtures Analysis\n('CompMix') Quantitative characterization of the health impacts associated with exposure to chemical mixtures has received considerable attention in current environmental and epidemiological studies. 'CompMix' package allows practitioners to estimate the health impacts from exposure to chemical mixtures data through various statistical approaches, including Lasso, Elastic net, Bayeisan kernel machine regression (BKMR), hierNet, Quantile g-computation, Weighted quantile sum (WQS) and Random forest. Hao W, Cathey A, Aung M, Boss J, Meeker J, Mukherjee B. (2024) \"Statistical methods for chemical mixtures: a practitioners guide\". <DOI:10.1101/2024.03.03.24303677>.  "
  },
  {
    "id": 2650,
    "package_name": "CovidMutations",
    "title": "Mutation Analysis Toolkit for COVID-19 (Coronavirus Disease\n2019)",
    "description": "A feasible framework for mutation analysis and reverse transcription \n  polymerase chain reaction (RT-PCR) assay evaluation of COVID-19, including \n  mutation profile visualization, statistics and mutation ratio of each assay. \n  The mutation ratio is conducive to evaluating the coverage of RT-PCR assays in \n  large-sized samples. Mercatelli, D. and Giorgi, F. M. (2020) \n  <doi:10.20944/preprints202004.0529.v1>.",
    "version": "0.1.3",
    "maintainer": "Shaoqian Ma <shaoqianma@qq.com>",
    "author": "Shaoqian Ma [aut, cre] (ORCID: <https://orcid.org/0000-0001-8950-0711>),\n  Yongyou Zhang [aut] (ORCID: <https://orcid.org/0000-0003-2413-9106>)",
    "url": "https://github.com/MSQ-123/CovidMutations",
    "bug_reports": "https://github.com/MSQ-123/CovidMutations/issues",
    "repository": "https://cran.r-project.org/package=CovidMutations",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CovidMutations Mutation Analysis Toolkit for COVID-19 (Coronavirus Disease\n2019) A feasible framework for mutation analysis and reverse transcription \n  polymerase chain reaction (RT-PCR) assay evaluation of COVID-19, including \n  mutation profile visualization, statistics and mutation ratio of each assay. \n  The mutation ratio is conducive to evaluating the coverage of RT-PCR assays in \n  large-sized samples. Mercatelli, D. and Giorgi, F. M. (2020) \n  <doi:10.20944/preprints202004.0529.v1>.  "
  },
  {
    "id": 2678,
    "package_name": "CurricularComplexity",
    "title": "Toolkit for Analyzing Curricular Complexity",
    "description": "Enables educational researchers and practitioners to calculate the curricular complexity of a plan of study, visualize its prerequisite structure at scale, and conduct customizable analyses. The original tool can be found at <https://curricularanalytics.org>. Additional functions to explore curriculum complexity from the literature are also included.",
    "version": "1.0",
    "maintainer": "David Reeping <reepindp@ucmail.uc.edu>",
    "author": "David Reeping [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CurricularComplexity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CurricularComplexity Toolkit for Analyzing Curricular Complexity Enables educational researchers and practitioners to calculate the curricular complexity of a plan of study, visualize its prerequisite structure at scale, and conduct customizable analyses. The original tool can be found at <https://curricularanalytics.org>. Additional functions to explore curriculum complexity from the literature are also included.  "
  },
  {
    "id": 2703,
    "package_name": "DALEXtra",
    "title": "Extension for 'DALEX' Package",
    "description": "Provides wrapper of various machine learning models. \n  In applied machine learning, there \n  is a strong belief that we need to strike a balance \n  between interpretability and accuracy. \n  However, in field of the interpretable machine learning, \n  there are more and more new ideas for explaining black-box models, \n  that are implemented in 'R'. \n  'DALEXtra' creates 'DALEX' Biecek (2018) <arXiv:1806.08915> explainer for many type of models\n  including those created using 'python' 'scikit-learn' and 'keras' libraries, and 'java' 'h2o' library. \n  Important part of the package is Champion-Challenger analysis and innovative approach\n  to model performance across subsets of test data presented in Funnel Plot. ",
    "version": "2.3.0",
    "maintainer": "Szymon Maksymiuk <sz.maksymiuk@gmail.com>",
    "author": "Szymon Maksymiuk [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3120-1601>),\n  Przemyslaw Biecek [aut] (ORCID:\n    <https://orcid.org/0000-0001-8423-1823>),\n  Hubert Baniecki [aut],\n  Anna Kozak [ctb]",
    "url": "https://ModelOriented.github.io/DALEXtra/,\nhttps://github.com/ModelOriented/DALEXtra",
    "bug_reports": "https://github.com/ModelOriented/DALEXtra/issues",
    "repository": "https://cran.r-project.org/package=DALEXtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DALEXtra Extension for 'DALEX' Package Provides wrapper of various machine learning models. \n  In applied machine learning, there \n  is a strong belief that we need to strike a balance \n  between interpretability and accuracy. \n  However, in field of the interpretable machine learning, \n  there are more and more new ideas for explaining black-box models, \n  that are implemented in 'R'. \n  'DALEXtra' creates 'DALEX' Biecek (2018) <arXiv:1806.08915> explainer for many type of models\n  including those created using 'python' 'scikit-learn' and 'keras' libraries, and 'java' 'h2o' library. \n  Important part of the package is Champion-Challenger analysis and innovative approach\n  to model performance across subsets of test data presented in Funnel Plot.   "
  },
  {
    "id": 2769,
    "package_name": "DGEobj.utils",
    "title": "Differential Gene Expression (DGE) Analysis Utility Toolkit",
    "description": "\n    Provides a function toolkit to facilitate reproducible RNA-Seq Differential Gene Expression (DGE)\n    analysis (Law (2015) <doi:10.12688/f1000research.9005.3>).  The tools include both analysis \n    work-flow and utility functions: mapping/unit conversion, count normalization, accounting for \n    unknown covariates, and more.  This is a complement/cohort to the 'DGEobj' package that \n    provides a flexible container to manage and annotate Differential Gene Expression analysis results.",
    "version": "1.0.6",
    "maintainer": "Connie Brett <connie@aggregate-genius.com>",
    "author": "John Thompson [aut],\n  Connie Brett [aut, cre],\n  Isaac Neuhaus [aut],\n  Ryan Thompson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DGEobj.utils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DGEobj.utils Differential Gene Expression (DGE) Analysis Utility Toolkit \n    Provides a function toolkit to facilitate reproducible RNA-Seq Differential Gene Expression (DGE)\n    analysis (Law (2015) <doi:10.12688/f1000research.9005.3>).  The tools include both analysis \n    work-flow and utility functions: mapping/unit conversion, count normalization, accounting for \n    unknown covariates, and more.  This is a complement/cohort to the 'DGEobj' package that \n    provides a flexible container to manage and annotate Differential Gene Expression analysis results.  "
  },
  {
    "id": 2808,
    "package_name": "DLMtool",
    "title": "Data-Limited Methods Toolkit",
    "description": "A collection of data-limited management procedures that can be evaluated \n    with management strategy evaluation with the 'MSEtool' package, or applied to \n    fishery data to provide management recommendations.",
    "version": "6.0.6",
    "maintainer": "Adrian Hordyk <adrian@bluematterscience.com>",
    "author": "Tom Carruthers [aut],\n  Quang Huynh [aut],\n  Adrian Hordyk [aut, cre],\n  M. Bryan [ctb],\n  HF. Geremont [ctb],\n  C. Grandin [ctb],\n  W. Harford [ctb],\n  Q. Huynh [ctb],\n  C. Walters [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DLMtool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DLMtool Data-Limited Methods Toolkit A collection of data-limited management procedures that can be evaluated \n    with management strategy evaluation with the 'MSEtool' package, or applied to \n    fishery data to provide management recommendations.  "
  },
  {
    "id": 2882,
    "package_name": "DTSg",
    "title": "A Class for Working with Time Series Data Based on 'data.table'\nand 'R6' with Largely Optional Reference Semantics",
    "description": "Basic time series functionalities such as listing of missing\n    values, application of arbitrary aggregation as well as rolling (asymmetric)\n    window functions and automatic detection of periodicity. As it is mainly\n    based on 'data.table', it is fast and (in combination with the 'R6' package)\n    offers reference semantics. In addition to its native R6 interface, it\n    provides an S3 interface for those who prefer the latter. Finally yet\n    importantly, its functional approach allows for incorporating\n    functionalities from many other packages.",
    "version": "2.0.0",
    "maintainer": "Gerold Hepp <gisler@hepp.cc>",
    "author": "Gerold Hepp [aut, cre]",
    "url": "https://gisler.github.io/DTSg/",
    "bug_reports": "https://github.com/gisler/DTSg/issues",
    "repository": "https://cran.r-project.org/package=DTSg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DTSg A Class for Working with Time Series Data Based on 'data.table'\nand 'R6' with Largely Optional Reference Semantics Basic time series functionalities such as listing of missing\n    values, application of arbitrary aggregation as well as rolling (asymmetric)\n    window functions and automatic detection of periodicity. As it is mainly\n    based on 'data.table', it is fast and (in combination with the 'R6' package)\n    offers reference semantics. In addition to its native R6 interface, it\n    provides an S3 interface for those who prefer the latter. Finally yet\n    importantly, its functional approach allows for incorporating\n    functionalities from many other packages.  "
  },
  {
    "id": 2885,
    "package_name": "DTwrappers",
    "title": "Simplified Data Analysis with Wrapper Functions for the\n'Data.Table' Package",
    "description": "Provides functionality for users who are learning R or the techniques of data analysis.  Written as a collection of wrapper functions, the 'DTwrapper' package facilitates many core operations of data processing.  This is achieved with relatively few requirements about the order of the processing steps or knowledge of specialized syntax.  'DTwrappers' creates coding results along with translations to data.table's code.  This enables users to benefit from the speed and efficiency of data.table's calculations.  Furthermore, the package also provides the translated code for educational purposes so that users can review working examples of coding syntax and calculations.",
    "version": "0.0.2",
    "maintainer": "Mayur Bansal <mb4511@columbia.edu>",
    "author": "David Shilane [aut],\n  Mayur Bansal [ctb, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DTwrappers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DTwrappers Simplified Data Analysis with Wrapper Functions for the\n'Data.Table' Package Provides functionality for users who are learning R or the techniques of data analysis.  Written as a collection of wrapper functions, the 'DTwrapper' package facilitates many core operations of data processing.  This is achieved with relatively few requirements about the order of the processing steps or knowledge of specialized syntax.  'DTwrappers' creates coding results along with translations to data.table's code.  This enables users to benefit from the speed and efficiency of data.table's calculations.  Furthermore, the package also provides the translated code for educational purposes so that users can review working examples of coding syntax and calculations.  "
  },
  {
    "id": 2991,
    "package_name": "DisaggregateTS",
    "title": "High-Dimensional Temporal Disaggregation",
    "description": "Provides tools for temporal disaggregation, including:\n    (1) High-dimensional and low-dimensional series generation for simulation studies;\n    (2) A toolkit for temporal disaggregation and benchmarking using low-dimensional indicator series \n        as proposed by Dagum and Cholette (2006, ISBN:978-0-387-35439-2);\n    (3) Novel techniques by Mosley, Gibberd, and Eckley (2022, <doi:10.1111/rssa.12952>)\n        for disaggregating low-frequency series in the presence of high-dimensional indicator matrices.",
    "version": "3.0.1",
    "maintainer": "Kaveh Salehzadeh Nobari <k.salehzadeh-nobari@imperial.ac.uk>",
    "author": "Kaveh Salehzadeh Nobari [aut, cre],\n  Luke Mosley [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DisaggregateTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DisaggregateTS High-Dimensional Temporal Disaggregation Provides tools for temporal disaggregation, including:\n    (1) High-dimensional and low-dimensional series generation for simulation studies;\n    (2) A toolkit for temporal disaggregation and benchmarking using low-dimensional indicator series \n        as proposed by Dagum and Cholette (2006, ISBN:978-0-387-35439-2);\n    (3) Novel techniques by Mosley, Gibberd, and Eckley (2022, <doi:10.1111/rssa.12952>)\n        for disaggregating low-frequency series in the presence of high-dimensional indicator matrices.  "
  },
  {
    "id": 3101,
    "package_name": "EHRmuse",
    "title": "Multi-Cohort Selection Bias Correction using IPW and AIPW\nMethods",
    "description": "Comprehensive toolkit for addressing selection \n    bias in binary disease models across diverse non-probability samples, each \n    with unique selection mechanisms. It utilizes Inverse Probability Weighting \n    (IPW) and Augmented Inverse Probability Weighting (AIPW) methods to reduce \n    selection bias effectively in multiple non-probability cohorts by integrating \n    data from either individual-level or summary-level external sources. The \n    package also provides a variety of variance estimation techniques. Please \n    refer to Kundu et al. <doi:10.48550/arXiv.2412.00228>.",
    "version": "0.0.2.2",
    "maintainer": "Michael Kleinsasser <biostat-cran-manager@umich.edu>",
    "author": "Ritoban Kundu [aut],\n  Michael Kleinsasser [cre]",
    "url": "https://github.com/Ritoban1/EHRmuse",
    "bug_reports": "https://github.com/Ritoban1/EHRmuse/issues",
    "repository": "https://cran.r-project.org/package=EHRmuse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EHRmuse Multi-Cohort Selection Bias Correction using IPW and AIPW\nMethods Comprehensive toolkit for addressing selection \n    bias in binary disease models across diverse non-probability samples, each \n    with unique selection mechanisms. It utilizes Inverse Probability Weighting \n    (IPW) and Augmented Inverse Probability Weighting (AIPW) methods to reduce \n    selection bias effectively in multiple non-probability cohorts by integrating \n    data from either individual-level or summary-level external sources. The \n    package also provides a variety of variance estimation techniques. Please \n    refer to Kundu et al. <doi:10.48550/arXiv.2412.00228>.  "
  },
  {
    "id": 3108,
    "package_name": "ELIC",
    "title": "LIC for Distributed Elliptical Model",
    "description": "This comprehensive toolkit for Distributed Elliptical model is designated as \"ELIC\" (The LIC for Distributed Elliptical Model Analysis) analysis. It is predicated on the assumption that the error term adheres to a Elliptical distribution. The philosophy of the package is described in Guo G. (2020) <doi:10.1080/02664763.2022.2053949>.",
    "version": "0.1.0",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "author": "Guangbao Guo [aut, cre],\n  Xiyu Zhao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ELIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ELIC LIC for Distributed Elliptical Model This comprehensive toolkit for Distributed Elliptical model is designated as \"ELIC\" (The LIC for Distributed Elliptical Model Analysis) analysis. It is predicated on the assumption that the error term adheres to a Elliptical distribution. The philosophy of the package is described in Guo G. (2020) <doi:10.1080/02664763.2022.2053949>.  "
  },
  {
    "id": 3193,
    "package_name": "EcoMetrics",
    "title": "Econometrics Model Building",
    "description": "An intuitive and user-friendly package designed to aid undergraduate students in understanding and applying econometric methods in their studies,\n    Tailored specifically for Econometrics and Regression Modeling courses, it provides a practical toolkit for modeling and analyzing econometric data with detailed inference capabilities.",
    "version": "0.1.1",
    "maintainer": "Mutua Kilai <kilaimutua@gmail.com>",
    "author": "Mutua Kilai [aut, cre] (ORCID: <https://orcid.org/0000-0001-6290-9728>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EcoMetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EcoMetrics Econometrics Model Building An intuitive and user-friendly package designed to aid undergraduate students in understanding and applying econometric methods in their studies,\n    Tailored specifically for Econometrics and Regression Modeling courses, it provides a practical toolkit for modeling and analyzing econometric data with detailed inference capabilities.  "
  },
  {
    "id": 3483,
    "package_name": "FinePop2",
    "title": "Fine-Scale Population Analysis (Rewrite for\nGene-Trait-Environment Interaction Analysis)",
    "description": "Statistical tool set for population genetics. The package provides following functions: 1) estimators of genetic differentiation (FST), 2) regression analysis of environmental effects on genetic differentiation using generalized least squares (GLS) method, 3) interfaces to read and manipulate 'GENEPOP' format data files). For more information, see Kitada, Nakamichi and Kishino (2020) <doi:10.1101/2020.01.30.927186>.",
    "version": "0.4",
    "maintainer": "Reiichiro Nakamichi <nakamichi@affrc.go.jp>",
    "author": "Reiichiro Nakamichi, Shuichi Kitada, Hirohisa Kishino",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FinePop2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FinePop2 Fine-Scale Population Analysis (Rewrite for\nGene-Trait-Environment Interaction Analysis) Statistical tool set for population genetics. The package provides following functions: 1) estimators of genetic differentiation (FST), 2) regression analysis of environmental effects on genetic differentiation using generalized least squares (GLS) method, 3) interfaces to read and manipulate 'GENEPOP' format data files). For more information, see Kitada, Nakamichi and Kishino (2020) <doi:10.1101/2020.01.30.927186>.  "
  },
  {
    "id": 3562,
    "package_name": "FuzzyR",
    "title": "Fuzzy Logic Toolkit for R",
    "description": "Design and simulate fuzzy logic systems using Type-1 and Interval Type-2 Fuzzy Logic.\n    This toolkit includes with graphical user interface (GUI) and an adaptive neuro-\n    fuzzy inference system (ANFIS). This toolkit is a continuation from the previous\n    package ('FuzzyToolkitUoN'). Produced by the Intelligent Modelling & Analysis Group (IMA)\n    and Lab for UnCertainty In Data and decision making (LUCID), University of Nottingham. \n    A big thank you to the many people who have contributed to the development/evaluation of the toolbox.\n    Please cite the toolbox and the corresponding paper <doi:10.1109/FUZZ48607.2020.9177780> when using it.\n    More related papers can be found in the NEWS.",
    "version": "2.3.2",
    "maintainer": "Chao Chen <fuzzyr@cs.nott.ac.uk>",
    "author": "Chao Chen [aut, cre],\n  Jon Garibaldi [aut],\n  Tajul Razak [aut]",
    "url": "https://www.lucidresearch.org/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FuzzyR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FuzzyR Fuzzy Logic Toolkit for R Design and simulate fuzzy logic systems using Type-1 and Interval Type-2 Fuzzy Logic.\n    This toolkit includes with graphical user interface (GUI) and an adaptive neuro-\n    fuzzy inference system (ANFIS). This toolkit is a continuation from the previous\n    package ('FuzzyToolkitUoN'). Produced by the Intelligent Modelling & Analysis Group (IMA)\n    and Lab for UnCertainty In Data and decision making (LUCID), University of Nottingham. \n    A big thank you to the many people who have contributed to the development/evaluation of the toolbox.\n    Please cite the toolbox and the corresponding paper <doi:10.1109/FUZZ48607.2020.9177780> when using it.\n    More related papers can be found in the NEWS.  "
  },
  {
    "id": 3781,
    "package_name": "GWmodelVis",
    "title": "Visualization Tools for Geographically Weighted Models",
    "description": "\n    The increasing popularity of geographically weighted (GW) techniques has resulted in the development of several R packages, such as 'GWmodel'. To facilitate their usages, 'GWmodelVis' provides a 'shiny'-based interactive visualization toolkit for geographically weighted (GW) models.\n    It includes a number of visualization tools, including dynamic mapping of parameter surfaces, statistical visualization, sonification and exporting  videos via 'FFmpeg'. ",
    "version": "1.0.1",
    "maintainer": "Binbin Lu <binbinlu@whu.edu.cn>",
    "author": "Binbin Lu [aut, cre],\n  Huimei Wang [aut]",
    "url": "http://gwmodel.whu.edu.cn/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GWmodelVis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GWmodelVis Visualization Tools for Geographically Weighted Models \n    The increasing popularity of geographically weighted (GW) techniques has resulted in the development of several R packages, such as 'GWmodel'. To facilitate their usages, 'GWmodelVis' provides a 'shiny'-based interactive visualization toolkit for geographically weighted (GW) models.\n    It includes a number of visualization tools, including dynamic mapping of parameter surfaces, statistical visualization, sonification and exporting  videos via 'FFmpeg'.   "
  },
  {
    "id": 3805,
    "package_name": "GencoDymo2",
    "title": "Comprehensive Analysis of 'GENCODE' Annotations and Splice Site\nMotifs",
    "description": "A comprehensive suite of helper functions designed to facilitate the analysis of genomic annotations from the 'GENCODE' database <https://www.gencodegenes.org/>, supporting both human and mouse genomes. This toolkit enables users to extract, filter, and analyze a wide range of annotation features including genes, transcripts, exons, and introns across different 'GENCODE' releases. It provides functionality for cross-version comparisons, allowing researchers to systematically track annotation updates, structural changes, and feature-level differences between releases. In addition, the package can generate high-quality FASTA files containing donor and acceptor splice site motifs, which are formatted for direct input into the 'MaxEntScan' tool (Yeo and Burge, 2004 <doi:10.1089/1066527041410418>), enabling accurate calculation of splice site strength scores.",
    "version": "1.0.3",
    "maintainer": "Monah Abou Alezz <aboualezz.monah@hsr.it>",
    "author": "Monah Abou Alezz [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2006-4250>),\n  Lorenzo Salviati [ctb],\n  Roberta Alfieri [ctb],\n  Silvia Bione [ctb]",
    "url": "https://github.com/monahton/GencoDymo2,\nhttps://monahton.github.io/GencoDymo2/",
    "bug_reports": "https://github.com/monahton/GencoDymo2/issues",
    "repository": "https://cran.r-project.org/package=GencoDymo2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GencoDymo2 Comprehensive Analysis of 'GENCODE' Annotations and Splice Site\nMotifs A comprehensive suite of helper functions designed to facilitate the analysis of genomic annotations from the 'GENCODE' database <https://www.gencodegenes.org/>, supporting both human and mouse genomes. This toolkit enables users to extract, filter, and analyze a wide range of annotation features including genes, transcripts, exons, and introns across different 'GENCODE' releases. It provides functionality for cross-version comparisons, allowing researchers to systematically track annotation updates, structural changes, and feature-level differences between releases. In addition, the package can generate high-quality FASTA files containing donor and acceptor splice site motifs, which are formatted for direct input into the 'MaxEntScan' tool (Yeo and Burge, 2004 <doi:10.1089/1066527041410418>), enabling accurate calculation of splice site strength scores.  "
  },
  {
    "id": 3809,
    "package_name": "GeneNMF",
    "title": "Non-Negative Matrix Factorization for Single-Cell Omics",
    "description": "A collection of methods to extract gene programs from single-cell gene expression data using non-negative matrix factorization (NMF). 'GeneNMF' contains functions to directly interact with the 'Seurat' toolkit and derive interpretable gene program signatures.",
    "version": "0.9.2",
    "maintainer": "Massimo Andreatta <massimo.andreatta@unige.ch>",
    "author": "Massimo Andreatta [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8036-2647>),\n  Santiago Carmona [aut] (ORCID: <https://orcid.org/0000-0002-2495-0671>)",
    "url": "https://github.com/carmonalab/GeneNMF",
    "bug_reports": "https://github.com/carmonalab/GeneNMF/issues",
    "repository": "https://cran.r-project.org/package=GeneNMF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeneNMF Non-Negative Matrix Factorization for Single-Cell Omics A collection of methods to extract gene programs from single-cell gene expression data using non-negative matrix factorization (NMF). 'GeneNMF' contains functions to directly interact with the 'Seurat' toolkit and derive interpretable gene program signatures.  "
  },
  {
    "id": 3876,
    "package_name": "Goodreader",
    "title": "Scrape and Analyze 'Goodreads' Book Data",
    "description": "A comprehensive toolkit for scraping and analyzing book data from <https://www.goodreads.com/>. This package provides functions to search for books, scrape book details and reviews, perform sentiment analysis on reviews, and conduct topic modeling. It's designed for researchers, data analysts, and book enthusiasts who want to gain insights from 'Goodreads' data. ",
    "version": "0.1.2",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9979-8272>)",
    "url": "https://github.com/chaoliu-cl/Goodreader,\nhttp://liu-chao.site/Goodreader/",
    "bug_reports": "https://github.com/chaoliu-cl/Goodreader/issues",
    "repository": "https://cran.r-project.org/package=Goodreader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Goodreader Scrape and Analyze 'Goodreads' Book Data A comprehensive toolkit for scraping and analyzing book data from <https://www.goodreads.com/>. This package provides functions to search for books, scrape book details and reviews, perform sentiment analysis on reviews, and conduct topic modeling. It's designed for researchers, data analysts, and book enthusiasts who want to gain insights from 'Goodreads' data.   "
  },
  {
    "id": 3959,
    "package_name": "HLAtools",
    "title": "Toolkit for HLA Immunogenomics",
    "description": "A toolkit for the analysis and management of data for genes in the so-called \"Human Leukocyte Antigen\" (HLA) region. Functions extract reference data from the Anthony Nolan HLA Informatics Group/ImmunoGeneTics HLA 'GitHub' repository (ANHIG/IMGTHLA) <https://github.com/ANHIG/IMGTHLA>, validate Genotype List (GL) Strings, convert between UNIFORMAT and GL String Code (GLSC) formats, translate HLA alleles and GLSCs across ImmunoPolymorphism Database (IPD) IMGT/HLA Database release versions, identify differences between pairs of alleles at a locus, generate customized, multi-position sequence alignments, trim and convert allele-names across nomenclature epochs, and extend existing data-analysis methods.",
    "version": "1.6.3",
    "maintainer": "Steven Mack <Steven.Mack@ucsf.edu>",
    "author": "Livia Tran [aut],\n  Ryan Nickens [aut],\n  Leamon Crooms IV [aut],\n  Derek Pappas [aut],\n  Vinh Luu [ctb],\n  Josh Bredeweg [ctb],\n  Steven Mack [aut, cre] (ORCID: <https://orcid.org/0000-0001-9820-9547>)",
    "url": "<https://github.com/sjmack/HLAtools>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HLAtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HLAtools Toolkit for HLA Immunogenomics A toolkit for the analysis and management of data for genes in the so-called \"Human Leukocyte Antigen\" (HLA) region. Functions extract reference data from the Anthony Nolan HLA Informatics Group/ImmunoGeneTics HLA 'GitHub' repository (ANHIG/IMGTHLA) <https://github.com/ANHIG/IMGTHLA>, validate Genotype List (GL) Strings, convert between UNIFORMAT and GL String Code (GLSC) formats, translate HLA alleles and GLSCs across ImmunoPolymorphism Database (IPD) IMGT/HLA Database release versions, identify differences between pairs of alleles at a locus, generate customized, multi-position sequence alignments, trim and convert allele-names across nomenclature epochs, and extend existing data-analysis methods.  "
  },
  {
    "id": 4248,
    "package_name": "Information",
    "title": "Data Exploration with Information Theory (Weight-of-Evidence and\nInformation Value)",
    "description": "Performs exploratory data analysis and variable screening for\n    binary classification models using weight-of-evidence (WOE) and information\n    value (IV). In order to make the package as efficient as possible, aggregations\n    are done in data.table and creation of WOE vectors can be distributed across\n    multiple cores. The package also supports exploration for uplift models (NWOE\n    and NIV).",
    "version": "0.0.9",
    "maintainer": "Larsen Kim <kblarsen4@gmail.com>",
    "author": "Larsen Kim [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Information",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Information Data Exploration with Information Theory (Weight-of-Evidence and\nInformation Value) Performs exploratory data analysis and variable screening for\n    binary classification models using weight-of-evidence (WOE) and information\n    value (IV). In order to make the package as efficient as possible, aggregations\n    are done in data.table and creation of WOE vectors can be distributed across\n    multiple cores. The package also supports exploration for uplift models (NWOE\n    and NIV).  "
  },
  {
    "id": 4267,
    "package_name": "InterpolateR",
    "title": "A Comprehensive Toolkit for Fast and Efficient Spatial\nInterpolation",
    "description": "Spatial interpolation toolkit designed for environmental and geospatial applications. It includes a range of methods, from traditional techniques to advanced machine learning approaches, ensuring accurate and efficient estimation of values in unobserved locations.",
    "version": "1.4-3",
    "maintainer": "Jonnathan Landi <jonnathan.landi@outlook.com>",
    "author": "Jonnathan Landi [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0003-3162-6647>),\n  Marco Mogro [aut] (ORCID: <https://orcid.org/0009-0007-1802-9417>)",
    "url": "https://github.com/Jonnathan-Landi/InterpolateR",
    "bug_reports": "https://github.com/Jonnathan-Landi/InterpolateR/issues",
    "repository": "https://cran.r-project.org/package=InterpolateR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "InterpolateR A Comprehensive Toolkit for Fast and Efficient Spatial\nInterpolation Spatial interpolation toolkit designed for environmental and geospatial applications. It includes a range of methods, from traditional techniques to advanced machine learning approaches, ensuring accurate and efficient estimation of values in unobserved locations.  "
  },
  {
    "id": 4337,
    "package_name": "JuliaCall",
    "title": "Seamless Integration Between R and 'Julia'",
    "description": "Provides an R interface to 'Julia',\n    which is a high-level, high-performance dynamic programming language\n    for numerical computing, see <https://julialang.org/> for more information.\n    It provides a high-level interface as well as a low-level interface.\n    Using the high level interface, you could call any 'Julia' function just like\n    any R function with automatic type conversion. Using the low level interface,\n    you could deal with C-level SEXP directly while enjoying the convenience of\n    using a high-level programming language like 'Julia'.",
    "version": "0.17.6",
    "maintainer": "Changcheng Li <cxl508@psu.edu>",
    "author": "Changcheng Li [aut, cre],\n  Christopher Rackauckas [ctb],\n  Randy Lai [ctb],\n  Dmitri Grominski [ctb],\n  Nagi Teramo [ctb]",
    "url": "https://github.com/JuliaInterop/JuliaCall",
    "bug_reports": "https://github.com/JuliaInterop/JuliaCall/issues",
    "repository": "https://cran.r-project.org/package=JuliaCall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JuliaCall Seamless Integration Between R and 'Julia' Provides an R interface to 'Julia',\n    which is a high-level, high-performance dynamic programming language\n    for numerical computing, see <https://julialang.org/> for more information.\n    It provides a high-level interface as well as a low-level interface.\n    Using the high level interface, you could call any 'Julia' function just like\n    any R function with automatic type conversion. Using the low level interface,\n    you could deal with C-level SEXP directly while enjoying the convenience of\n    using a high-level programming language like 'Julia'.  "
  },
  {
    "id": 4411,
    "package_name": "L0Learn",
    "title": "Fast Algorithms for Best Subset Selection",
    "description": "Highly optimized toolkit for approximately solving L0-regularized learning problems (a.k.a. best subset selection).\n    The algorithms are based on coordinate descent and local combinatorial search.\n    For more details, check the paper by Hazimeh and Mazumder (2020) <doi:10.1287/opre.2019.1919>.",
    "version": "2.1.0",
    "maintainer": "Hussein Hazimeh <husseinhaz@gmail.com>",
    "author": "Hussein Hazimeh [aut, cre],\n  Rahul Mazumder [aut],\n  Tim Nonet [aut]",
    "url": "https://github.com/hazimehh/L0Learn\nhttps://pubsonline.informs.org/doi/10.1287/opre.2019.1919",
    "bug_reports": "https://github.com/hazimehh/L0Learn/issues",
    "repository": "https://cran.r-project.org/package=L0Learn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "L0Learn Fast Algorithms for Best Subset Selection Highly optimized toolkit for approximately solving L0-regularized learning problems (a.k.a. best subset selection).\n    The algorithms are based on coordinate descent and local combinatorial search.\n    For more details, check the paper by Hazimeh and Mazumder (2020) <doi:10.1287/opre.2019.1919>.  "
  },
  {
    "id": 4430,
    "package_name": "LBPG",
    "title": "The Length-Biased Power Garima Distribution",
    "description": "The Length-Biased Power Garima distribution for computes the probability density,\n    the cumulative density distribution and the quantile function of the distribution, \n    and generates sample values with random variables based on Kittipong and Sirinapa(2021)<DOI: 10.14456/sjst-psu.2021.89>.",
    "version": "0.1.2",
    "maintainer": "Kittipong Klinjan <kittipong_k@rmutt.ac.th>",
    "author": "Kittipong Klinjan [cre, aut],\n  Onrampa Thepdaeng [aut],\n  Tadaporn Sombunpen [aut],\n  Atchanut Rattanalertnusorn [aut],\n  Sirinapa Aryuyuen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LBPG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LBPG The Length-Biased Power Garima Distribution The Length-Biased Power Garima distribution for computes the probability density,\n    the cumulative density distribution and the quantile function of the distribution, \n    and generates sample values with random variables based on Kittipong and Sirinapa(2021)<DOI: 10.14456/sjst-psu.2021.89>.  "
  },
  {
    "id": 4456,
    "package_name": "LGDtoolkit",
    "title": "Collection of Tools for LGD Rating Model Development",
    "description": "The goal of this package is to cover the most common steps in Loss Given Default (LGD) rating model development.\n             The main procedures available are those that refer to bivariate and multivariate analysis. In particular two statistical methods for \n             multivariate analysis are currently implemented \u2013 OLS regression and fractional logistic regression.\n             Both methods are also available within different blockwise model designs and both have customized stepwise algorithms. \n             Descriptions of these customized designs are available in Siddiqi (2016) <doi:10.1002/9781119282396.ch10> and \n             Anderson, R.A. (2021) <doi:10.1093/oso/9780192844194.001.0001>. \n             Although they are explained for PD model, the same designs are applicable for LGD model with different underlying regression methods \n             (OLS and fractional logistic regression). To cover other important steps for LGD model development, it is recommended to use \n             'LGDtoolkit' package along with 'PDtoolkit', and 'monobin' (or 'monobinShiny') packages.\n             Additionally, 'LGDtoolkit' provides set of procedures handy for initial and periodical model validation. ",
    "version": "0.2.0",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "author": "Andrija Djurovic [aut, cre]",
    "url": "https://github.com/andrija-djurovic/LGDtoolkit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LGDtoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LGDtoolkit Collection of Tools for LGD Rating Model Development The goal of this package is to cover the most common steps in Loss Given Default (LGD) rating model development.\n             The main procedures available are those that refer to bivariate and multivariate analysis. In particular two statistical methods for \n             multivariate analysis are currently implemented \u2013 OLS regression and fractional logistic regression.\n             Both methods are also available within different blockwise model designs and both have customized stepwise algorithms. \n             Descriptions of these customized designs are available in Siddiqi (2016) <doi:10.1002/9781119282396.ch10> and \n             Anderson, R.A. (2021) <doi:10.1093/oso/9780192844194.001.0001>. \n             Although they are explained for PD model, the same designs are applicable for LGD model with different underlying regression methods \n             (OLS and fractional logistic regression). To cover other important steps for LGD model development, it is recommended to use \n             'LGDtoolkit' package along with 'PDtoolkit', and 'monobin' (or 'monobinShiny') packages.\n             Additionally, 'LGDtoolkit' provides set of procedures handy for initial and periodical model validation.   "
  },
  {
    "id": 4543,
    "package_name": "LambertW",
    "title": "Probabilistic Models to Analyze and Gaussianize Heavy-Tailed,\nSkewed Data",
    "description": "Lambert W x F distributions are a generalized framework to analyze\n    skewed, heavy-tailed data. It is based on an input/output system, where the\n    output random variable (RV) Y is a non-linearly transformed version of an input\n    RV X ~ F with similar properties as X, but slightly skewed (heavy-tailed).\n    The transformed RV Y has a Lambert W x F distribution. This package contains\n    functions to model and analyze skewed, heavy-tailed data the Lambert Way:\n    simulate random samples, estimate parameters, compute quantiles, and plot/\n    print results nicely. The most useful function is 'Gaussianize',\n    which works similarly to 'scale', but actually makes the data Gaussian.\n    A do-it-yourself toolkit allows users to define their own Lambert W x\n    'MyFavoriteDistribution' and use it in their analysis right away.",
    "version": "0.6.9-2",
    "maintainer": "Georg M. Goerg <im@gmge.org>",
    "author": "Georg M. Goerg [aut, cre]",
    "url": "https://github.com/gmgeorg/LambertW,\nhttps://arxiv.org/abs/0912.4554,\nhttps://arxiv.org/abs/1010.2265,\nhttps://arxiv.org/abs/1602.02200",
    "bug_reports": "https://github.com/gmgeorg/LambertW/issues",
    "repository": "https://cran.r-project.org/package=LambertW",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LambertW Probabilistic Models to Analyze and Gaussianize Heavy-Tailed,\nSkewed Data Lambert W x F distributions are a generalized framework to analyze\n    skewed, heavy-tailed data. It is based on an input/output system, where the\n    output random variable (RV) Y is a non-linearly transformed version of an input\n    RV X ~ F with similar properties as X, but slightly skewed (heavy-tailed).\n    The transformed RV Y has a Lambert W x F distribution. This package contains\n    functions to model and analyze skewed, heavy-tailed data the Lambert Way:\n    simulate random samples, estimate parameters, compute quantiles, and plot/\n    print results nicely. The most useful function is 'Gaussianize',\n    which works similarly to 'scale', but actually makes the data Gaussian.\n    A do-it-yourself toolkit allows users to define their own Lambert W x\n    'MyFavoriteDistribution' and use it in their analysis right away.  "
  },
  {
    "id": 4758,
    "package_name": "MHQoL",
    "title": "Mental Health Quality of Life Toolkit",
    "description": "Transforms, calculates, and presents results from the Mental Health Quality of Life Questionnaire (MHQoL), a measure of health-related quality of life for individuals with mental health conditions. Provides scoring functions, summary statistics, and visualization tools to facilitate interpretation. For more details see van Krugten et al.(2022) <doi:10.1007/s11136-021-02935-w>.",
    "version": "0.14.0",
    "maintainer": "Stijn Peeters <s.b.peeters@eshpm.eur.nl>",
    "author": "Stijn Peeters [aut, cre] (ORCID:\n    <https://orcid.org/0009-0004-3684-3584>),\n  Frederick Thielen [aut] (ORCID:\n    <https://orcid.org/0000-0002-0312-5891>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MHQoL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MHQoL Mental Health Quality of Life Toolkit Transforms, calculates, and presents results from the Mental Health Quality of Life Questionnaire (MHQoL), a measure of health-related quality of life for individuals with mental health conditions. Provides scoring functions, summary statistics, and visualization tools to facilitate interpretation. For more details see van Krugten et al.(2022) <doi:10.1007/s11136-021-02935-w>.  "
  },
  {
    "id": 4816,
    "package_name": "MMAD",
    "title": "An R Package of Minorization-Maximization Algorithm via the\nAssembly--Decomposition Technology",
    "description": "The minorization-maximization (MM) algorithm is a powerful tool for maximizing nonconcave target function. However, for most existing MM algorithms, the surrogate function in the minorization step is constructed in a case-specific manner and requires manual programming. To address this limitation, we develop the R package MMAD, which systematically integrates the assembly--decomposition technology in the MM framework. This new package provides a comprehensive computational toolkit for one-stop inference of complex target functions, including function construction, evaluation, minorization and optimization via MM algorithm. By representing the target function through a hierarchical composition of assembly functions, we design a hierarchical algorithmic structure that supports both bottom-up operations (construction, evaluation) and top-down operation (minorization).",
    "version": "2.0",
    "maintainer": "Jiaqi Gu <jiaqigu@usf.edu>",
    "author": "Jiaqi Gu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MMAD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MMAD An R Package of Minorization-Maximization Algorithm via the\nAssembly--Decomposition Technology The minorization-maximization (MM) algorithm is a powerful tool for maximizing nonconcave target function. However, for most existing MM algorithms, the surrogate function in the minorization step is constructed in a case-specific manner and requires manual programming. To address this limitation, we develop the R package MMAD, which systematically integrates the assembly--decomposition technology in the MM framework. This new package provides a comprehensive computational toolkit for one-stop inference of complex target functions, including function construction, evaluation, minorization and optimization via MM algorithm. By representing the target function through a hierarchical composition of assembly functions, we design a hierarchical algorithmic structure that supports both bottom-up operations (construction, evaluation) and top-down operation (minorization).  "
  },
  {
    "id": 4884,
    "package_name": "MSEtool",
    "title": "Management Strategy Evaluation Toolkit",
    "description": "Development, simulation testing, and implementation of management procedures for fisheries \n    (see Carruthers & Hordyk (2018) <doi:10.1111/2041-210X.13081>).",
    "version": "3.7.5",
    "maintainer": "Adrian Hordyk <adrian@bluematterscience.com>",
    "author": "Adrian Hordyk [aut, cre],\n  Quang Huynh [aut],\n  Tom Carruthers [aut],\n  Chris Grandin [ctb] (iSCAM functions)",
    "url": "https://msetool.openmse.com/",
    "bug_reports": "https://github.com/Blue-Matter/MSEtool/issues",
    "repository": "https://cran.r-project.org/package=MSEtool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSEtool Management Strategy Evaluation Toolkit Development, simulation testing, and implementation of management procedures for fisheries \n    (see Carruthers & Hordyk (2018) <doi:10.1111/2041-210X.13081>).  "
  },
  {
    "id": 4905,
    "package_name": "MSoutcomes",
    "title": "CORe Multiple Sclerosis Outcomes Toolkit",
    "description": "Enable operationalized evaluation of disease outcomes in\n    multiple sclerosis. \u2018MSoutcomes\u2019 requires longitudinally recorded\n    clinical data structured in long format. The package is based on the\n    research developed at Clinical Outcomes Research unit (CORe),\n    University of Melbourne and Neuroimmunology Centre, Royal Melbourne\n    Hospital. Kalincik et al. (2015) <doi:10.1093/brain/awv258>.\n    Lorscheider et al.  (2016) <doi:10.1093/brain/aww173>. Sharmin et al.\n    (2022) <doi:10.1111/ene.15406>.  Dzau et al. (2023)\n    <doi:10.1136/jnnp-2023-331748>.",
    "version": "0.2.1",
    "maintainer": "Tomas Kalincik <tomas.kalincik@unimelb.edu.au>",
    "author": "Sifat Sharmin [aut],\n  Johannes Lorscheider [aut],\n  Nathaniel Lizak [aut],\n  Sam Harding-Forrester [aut],\n  Winston Dzau [aut],\n  Jannis M\u00fcller [aut],\n  Izanne Roos [aut],\n  Tomas Kalincik [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3778-1376>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MSoutcomes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSoutcomes CORe Multiple Sclerosis Outcomes Toolkit Enable operationalized evaluation of disease outcomes in\n    multiple sclerosis. \u2018MSoutcomes\u2019 requires longitudinally recorded\n    clinical data structured in long format. The package is based on the\n    research developed at Clinical Outcomes Research unit (CORe),\n    University of Melbourne and Neuroimmunology Centre, Royal Melbourne\n    Hospital. Kalincik et al. (2015) <doi:10.1093/brain/awv258>.\n    Lorscheider et al.  (2016) <doi:10.1093/brain/aww173>. Sharmin et al.\n    (2022) <doi:10.1111/ene.15406>.  Dzau et al. (2023)\n    <doi:10.1136/jnnp-2023-331748>.  "
  },
  {
    "id": 4913,
    "package_name": "MTS",
    "title": "All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS)\nand Estimating Multivariate Volatility Models",
    "description": "Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model. ",
    "version": "1.2.1",
    "maintainer": "Ruey S. Tsay <ruey.tsay@chicagobooth.edu>",
    "author": "Ruey S. Tsay [aut, cre], David Wood [aut], Jon Lachmann [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MTS All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS)\nand Estimating Multivariate Volatility Models Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model.   "
  },
  {
    "id": 4955,
    "package_name": "ManyIVsNets",
    "title": "Environmental Phillips Curve Analysis with Multiple Instrumental\nVariables and Networks",
    "description": "Comprehensive toolkit for Environmental Phillips Curve analysis \n    featuring multidimensional instrumental variable creation, transfer entropy \n    causal discovery, network analysis, and state-of-the-art econometric methods.\n    Implements geographic, technological, migration, geopolitical, financial, \n    and natural risk instruments with robust diagnostics and visualization.\n    Provides 24 different instrumental variable approaches with empirical validation.\n    Methods based on Phillips (1958) <doi:10.1111/j.1468-0335.1958.tb00003.x>,\n    transfer entropy by Schreiber (2000) <doi:10.1103/PhysRevLett.85.461>, and \n    weak instrument tests by Stock and Yogo (2005) <doi:10.1017/CBO9780511614491.006>.",
    "version": "0.1.1",
    "maintainer": "Avishek Bhandari <bavisek@gmail.com>",
    "author": "Avishek Bhandari [aut, cre, cph]",
    "url": "https://github.com/avishekb9/ManyIVsNets,\nhttps://avishekb9.github.io/ManyIVsNets/",
    "bug_reports": "https://github.com/avishekb9/ManyIVsNets/issues",
    "repository": "https://cran.r-project.org/package=ManyIVsNets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ManyIVsNets Environmental Phillips Curve Analysis with Multiple Instrumental\nVariables and Networks Comprehensive toolkit for Environmental Phillips Curve analysis \n    featuring multidimensional instrumental variable creation, transfer entropy \n    causal discovery, network analysis, and state-of-the-art econometric methods.\n    Implements geographic, technological, migration, geopolitical, financial, \n    and natural risk instruments with robust diagnostics and visualization.\n    Provides 24 different instrumental variable approaches with empirical validation.\n    Methods based on Phillips (1958) <doi:10.1111/j.1468-0335.1958.tb00003.x>,\n    transfer entropy by Schreiber (2000) <doi:10.1103/PhysRevLett.85.461>, and \n    weak instrument tests by Stock and Yogo (2005) <doi:10.1017/CBO9780511614491.006>.  "
  },
  {
    "id": 5048,
    "package_name": "MicSim",
    "title": "Performing Continuous-Time Microsimulation",
    "description": "This toolkit allows performing continuous-time microsimulation for a wide range of life science (demography, social sciences, epidemiology) applications. Individual life-courses are specified by a continuous-time multi-state model as described in Zinn (2014) <doi:10.34196/IJM.00105>. ",
    "version": "3.0.0",
    "maintainer": "Sabine Zinn <szinn@diw.de>",
    "author": "Sabine Zinn [aut, cre],\n  Felix von Heusinger [ctb],\n  Camila Weber [ctb],\n  Claudio Bosco [ctb],\n  Maurizio Teobaldell [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MicSim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MicSim Performing Continuous-Time Microsimulation This toolkit allows performing continuous-time microsimulation for a wide range of life science (demography, social sciences, epidemiology) applications. Individual life-courses are specified by a continuous-time multi-state model as described in Zinn (2014) <doi:10.34196/IJM.00105>.   "
  },
  {
    "id": 5093,
    "package_name": "MixviR",
    "title": "Analysis and Exploration of Mixed Microbial Genomic Samples",
    "description": "Tool for exploring DNA and amino acid variation and inferring the presence of target lineages from microbial high-throughput genomic DNA samples that potentially contain mixtures of variants/lineages. MixviR was originally created to help analyze environmental SARS-CoV-2/Covid-19 samples from environmental sources such as wastewater or dust, but can be applied to any microbial group. Inputs include reference genome information in commonly-used file formats (fasta, bed) and one or more variant call format (VCF) files, which can be generated with programs such as Illumina's DRAGEN, the Genome Analysis Toolkit, or bcftools. See DePristo et al (2011) <doi:10.1038/ng.806> and Danecek et al (2021) <doi:10.1093/gigascience/giab008> for these tools, respectively. Available outputs include a table of mutations observed in the sample(s), estimates of proportions of target lineages in the sample(s), and an R Shiny dashboard to interactively explore the data. ",
    "version": "3.5.0",
    "maintainer": "Michael Sovic <sovic.1@osu.edu>",
    "author": "Michael Sovic [aut, ccp, cre] (ORCID:\n    <https://orcid.org/0000-0002-8556-3704>),\n  Francesca Savona [res],\n  Zuzana Bohrerova [res, fnd],\n  Seth Faith [ccp, ctb] (ORDIC: 0000-0002-0441-9859)",
    "url": "https://github.com/mikesovic/MixviR",
    "bug_reports": "https://groups.google.com/g/mixvir",
    "repository": "https://cran.r-project.org/package=MixviR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MixviR Analysis and Exploration of Mixed Microbial Genomic Samples Tool for exploring DNA and amino acid variation and inferring the presence of target lineages from microbial high-throughput genomic DNA samples that potentially contain mixtures of variants/lineages. MixviR was originally created to help analyze environmental SARS-CoV-2/Covid-19 samples from environmental sources such as wastewater or dust, but can be applied to any microbial group. Inputs include reference genome information in commonly-used file formats (fasta, bed) and one or more variant call format (VCF) files, which can be generated with programs such as Illumina's DRAGEN, the Genome Analysis Toolkit, or bcftools. See DePristo et al (2011) <doi:10.1038/ng.806> and Danecek et al (2021) <doi:10.1093/gigascience/giab008> for these tools, respectively. Available outputs include a table of mutations observed in the sample(s), estimates of proportions of target lineages in the sample(s), and an R Shiny dashboard to interactively explore the data.   "
  },
  {
    "id": 5251,
    "package_name": "NMdata",
    "title": "Preparation, Checking and Post-Processing Data for PK/PD\nModeling",
    "description": "Efficient tools for preparation, checking and post-processing of data in PK/PD (pharmacokinetics/pharmacodynamics) modeling, with focus on use of Nonmem, including consistency, traceability, and Nonmem compatibility of Data. Rigorously checks final Nonmem datasets. Implemented in 'data.table', but easily integrated with 'base' and 'tidyverse'.",
    "version": "0.2.2",
    "maintainer": "Philip Delff <philip@delff.dk>",
    "author": "Philip Delff [aut, cre],\n  Brian Reilly [ctb],\n  Eric Anderson [ctb]",
    "url": "https://nmautoverse.github.io/NMdata/",
    "bug_reports": "https://github.com/nmautoverse/NMdata/issues",
    "repository": "https://cran.r-project.org/package=NMdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NMdata Preparation, Checking and Post-Processing Data for PK/PD\nModeling Efficient tools for preparation, checking and post-processing of data in PK/PD (pharmacokinetics/pharmacodynamics) modeling, with focus on use of Nonmem, including consistency, traceability, and Nonmem compatibility of Data. Rigorously checks final Nonmem datasets. Implemented in 'data.table', but easily integrated with 'base' and 'tidyverse'.  "
  },
  {
    "id": 5258,
    "package_name": "NOVA",
    "title": "Neural Output Visualization and Analysis",
    "description": "A comprehensive toolkit for analyzing and visualizing neural data \n    outputs, including Principal Component Analysis (PCA) trajectory plotting, \n    Multi-Electrode Array (MEA) heatmap generation, and variable importance \n    analysis. Provides publication-ready visualizations with flexible \n    customization options for neuroscience research applications.",
    "version": "0.1.1",
    "maintainer": "Alex Tudoras <alex.tudorasmiravet@ucsf.edu>",
    "author": "Alex Tudoras [aut, cre]",
    "url": "https://github.com/atudoras/NOVA",
    "bug_reports": "https://github.com/atudoras/NOVA/issues",
    "repository": "https://cran.r-project.org/package=NOVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NOVA Neural Output Visualization and Analysis A comprehensive toolkit for analyzing and visualizing neural data \n    outputs, including Principal Component Analysis (PCA) trajectory plotting, \n    Multi-Electrode Array (MEA) heatmap generation, and variable importance \n    analysis. Provides publication-ready visualizations with flexible \n    customization options for neuroscience research applications.  "
  },
  {
    "id": 5299,
    "package_name": "Nematode",
    "title": "Ecological Indices Calculator for Nematode Communities",
    "description": "Provides a computational toolkit for analyzing nematode communities in ecological studies. \n    Includes methods to quantify nematode-based ecological indicators such as metabolic footprints, \n    energy flow metrics, and community structure. These tools support assessments of soil health, \n    ecosystem functioning, and trophic interactions, standardizing the use of nematodes as \n    bioindicators.",
    "version": "0.2.1",
    "maintainer": "Yuxuan He <heyuxuan@henu.edu.cn>",
    "author": "Yuxuan He [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0007-1149-5554>),\n  Dong Wang [ths, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Nematode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Nematode Ecological Indices Calculator for Nematode Communities Provides a computational toolkit for analyzing nematode communities in ecological studies. \n    Includes methods to quantify nematode-based ecological indicators such as metabolic footprints, \n    energy flow metrics, and community structure. These tools support assessments of soil health, \n    ecosystem functioning, and trophic interactions, standardizing the use of nematodes as \n    bioindicators.  "
  },
  {
    "id": 5311,
    "package_name": "NetMix",
    "title": "Dynamic Mixed-Membership Network Regression Model",
    "description": "Stochastic collapsed variational inference on mixed-membership stochastic blockmodel for networks,\n             incorporating node-level predictors of mixed-membership vectors, as well as \n             dyad-level predictors. For networks observed over time, the model defines a hidden\n             Markov process that allows the effects of node-level predictors to evolve in discrete,\n             historical periods. In addition, the package offers a variety of utilities for \n             exploring results of estimation, including tools for conducting posterior \n             predictive checks of goodness-of-fit and several plotting functions. The package \n             implements methods described in Olivella, Pratt and Imai (2019) 'Dynamic Stochastic\n             Blockmodel Regression for Social Networks: Application to International Conflicts',\n             available at <https://www.santiagoolivella.info/pdfs/socnet.pdf>.",
    "version": "0.2.0.3",
    "maintainer": "Santiago Olivella <olivella@unc.edu>",
    "author": "Santiago Olivella [aut, cre],\n  Adeline Lo [aut],\n  Tyler Pratt [aut],\n  Kosuke Imai [ctb]",
    "url": "",
    "bug_reports": "https://github.com/solivella/NetMix/issues",
    "repository": "https://cran.r-project.org/package=NetMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetMix Dynamic Mixed-Membership Network Regression Model Stochastic collapsed variational inference on mixed-membership stochastic blockmodel for networks,\n             incorporating node-level predictors of mixed-membership vectors, as well as \n             dyad-level predictors. For networks observed over time, the model defines a hidden\n             Markov process that allows the effects of node-level predictors to evolve in discrete,\n             historical periods. In addition, the package offers a variety of utilities for \n             exploring results of estimation, including tools for conducting posterior \n             predictive checks of goodness-of-fit and several plotting functions. The package \n             implements methods described in Olivella, Pratt and Imai (2019) 'Dynamic Stochastic\n             Blockmodel Regression for Social Networks: Application to International Conflicts',\n             available at <https://www.santiagoolivella.info/pdfs/socnet.pdf>.  "
  },
  {
    "id": 5391,
    "package_name": "OPL",
    "title": "Optimal Policy Learning",
    "description": "Provides functions for optimal policy learning in socioeconomic applications helping users to learn the most effective policies based \n\ton data in order to maximize empirical welfare. Specifically, 'OPL' allows to find \"treatment assignment rules\" that maximize the overall \n\twelfare, defined as the sum  of the policy effects estimated over all the policy beneficiaries. Documentation about 'OPL' is provided by  \n\tseveral international articles via Athey et al (2021, <doi:10.3982/ECTA15732>), Kitagawa et al (2018, <doi:10.3982/ECTA13288>),\n        Cerulli (2022, <doi:10.1080/13504851.2022.2032577>), the paper by Cerulli (2021, <doi:10.1080/13504851.2020.1820939>) \n\tand the book by Gareth et al (2013, <doi:10.1007/978-1-4614-7138-7>).",
    "version": "1.0.2",
    "maintainer": "Federico Brogi <federicobrogi@gmail.com>",
    "author": "Federico Brogi [aut, cre],\n  Barbara Guardabascio [aut],\n  Giovanni Cerulli [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OPL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OPL Optimal Policy Learning Provides functions for optimal policy learning in socioeconomic applications helping users to learn the most effective policies based \n\ton data in order to maximize empirical welfare. Specifically, 'OPL' allows to find \"treatment assignment rules\" that maximize the overall \n\twelfare, defined as the sum  of the policy effects estimated over all the policy beneficiaries. Documentation about 'OPL' is provided by  \n\tseveral international articles via Athey et al (2021, <doi:10.3982/ECTA15732>), Kitagawa et al (2018, <doi:10.3982/ECTA13288>),\n        Cerulli (2022, <doi:10.1080/13504851.2022.2032577>), the paper by Cerulli (2021, <doi:10.1080/13504851.2020.1820939>) \n\tand the book by Gareth et al (2013, <doi:10.1007/978-1-4614-7138-7>).  "
  },
  {
    "id": 5437,
    "package_name": "OmicsPrepR",
    "title": "Unified Preprocessing Toolkit for Proteomics and Metabolomics",
    "description": "Provides unified workflows for quality control, normalization, and visualization of proteomic and metabolomic data. The package simplifies preprocessing through automated imputation, scaling, and principal component analysis (PCA)-based exploratory analysis, enabling researchers to prepare omics datasets efficiently for downstream statistical and machine learning analyses.",
    "version": "0.1.1",
    "maintainer": "Isaac Osei <ikemillar65@gmail.com>",
    "author": "Isaac Osei [aut, cre],\n  Dennis Opoku Boadu [aut],\n  Chettupally Anil Carie [aut]",
    "url": "https://github.com/ikemillar/OmicsPrepR",
    "bug_reports": "https://github.com/ikemillar/OmicsPrepR/issues",
    "repository": "https://cran.r-project.org/package=OmicsPrepR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OmicsPrepR Unified Preprocessing Toolkit for Proteomics and Metabolomics Provides unified workflows for quality control, normalization, and visualization of proteomic and metabolomic data. The package simplifies preprocessing through automated imputation, scaling, and principal component analysis (PCA)-based exploratory analysis, enabling researchers to prepare omics datasets efficiently for downstream statistical and machine learning analyses.  "
  },
  {
    "id": 5465,
    "package_name": "OpenImageR",
    "title": "An Image Processing Toolkit",
    "description": "Incorporates functions for image preprocessing, filtering and image recognition. The package takes advantage of 'RcppArmadillo' to speed up computationally intensive functions. The histogram of oriented gradients descriptor is a modification of the 'findHOGFeatures' function of the 'SimpleCV' computer vision platform, the average_hash(), dhash() and phash() functions are based on the 'ImageHash' python library. The Gabor Feature Extraction functions are based on 'Matlab' code of the paper, \"CloudID: Trustworthy cloud-based and cross-enterprise biometric identification\" by M. Haghighat, S. Zonouz, M. Abdel-Mottaleb, Expert Systems with Applications, vol. 42, no. 21, pp. 7905-7916, 2015, <doi:10.1016/j.eswa.2015.06.025>. The 'SLIC' and 'SLICO' superpixel algorithms were explained in detail in (i) \"SLIC Superpixels Compared to State-of-the-art Superpixel Methods\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, num. 11, p. 2274-2282, May 2012, <doi:10.1109/TPAMI.2012.120> and (ii) \"SLIC Superpixels\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, EPFL Technical Report no. 149300, June 2010.",
    "version": "1.3.0",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>),\n  Sight Machine [cph] (findHOGFeatures function of the SimpleCV computer\n    vision platform),\n  Johannes Buchner [cph] (average_hash, dhash and phash functions of the\n    ImageHash python library),\n  Mohammad Haghighat [cph] (Gabor Feature Extraction),\n  Radhakrishna Achanta [cph] (Author of the C++ code of the SLIC and\n    SLICO algorithms (for commercial use please contact the author)),\n  Oleh Onyshchak [cph] (Author of the Python code of the WarpAffine\n    function)",
    "url": "https://github.com/mlampros/OpenImageR",
    "bug_reports": "https://github.com/mlampros/OpenImageR/issues",
    "repository": "https://cran.r-project.org/package=OpenImageR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OpenImageR An Image Processing Toolkit Incorporates functions for image preprocessing, filtering and image recognition. The package takes advantage of 'RcppArmadillo' to speed up computationally intensive functions. The histogram of oriented gradients descriptor is a modification of the 'findHOGFeatures' function of the 'SimpleCV' computer vision platform, the average_hash(), dhash() and phash() functions are based on the 'ImageHash' python library. The Gabor Feature Extraction functions are based on 'Matlab' code of the paper, \"CloudID: Trustworthy cloud-based and cross-enterprise biometric identification\" by M. Haghighat, S. Zonouz, M. Abdel-Mottaleb, Expert Systems with Applications, vol. 42, no. 21, pp. 7905-7916, 2015, <doi:10.1016/j.eswa.2015.06.025>. The 'SLIC' and 'SLICO' superpixel algorithms were explained in detail in (i) \"SLIC Superpixels Compared to State-of-the-art Superpixel Methods\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, num. 11, p. 2274-2282, May 2012, <doi:10.1109/TPAMI.2012.120> and (ii) \"SLIC Superpixels\", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, EPFL Technical Report no. 149300, June 2010.  "
  },
  {
    "id": 5477,
    "package_name": "OptCirClust",
    "title": "Circular, Periodic, or Framed Data Clustering: Fast, Optimal,\nand Reproducible",
    "description": "Fast, optimal, and reproducible clustering algorithms for\n circular, periodic, or framed data. The algorithms introduced here\n are based on a core algorithm for optimal framed clustering the authors\n have developed (Debnath & Song 2021) <doi:10.1109/TCBB.2021.3077573>.\n The runtime of these algorithms is O(K N log^2 N), where K is the number\n of clusters and N is the number of circular data points. On a desktop\n computer using a single processor core, millions of data points can be\n grouped into a few clusters within seconds. One can apply the algorithms\n to characterize events along circular DNA molecules, circular RNA\n molecules, and circular genomes of bacteria, chloroplast, and\n mitochondria. One can also cluster climate data along any given\n longitude or latitude. Periodic data clustering can be formulated as\n circular clustering. The algorithms offer a general high-performance\n solution to circular, periodic, or framed data clustering. ",
    "version": "0.0.4",
    "maintainer": "Joe Song <joemsong@cs.nmsu.edu>",
    "author": "Tathagata Debnath [aut] (ORCID:\n    <https://orcid.org/0000-0001-6445-275X>),\n  Joe Song [aut, cre] (ORCID: <https://orcid.org/0000-0002-6883-6547>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OptCirClust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OptCirClust Circular, Periodic, or Framed Data Clustering: Fast, Optimal,\nand Reproducible Fast, optimal, and reproducible clustering algorithms for\n circular, periodic, or framed data. The algorithms introduced here\n are based on a core algorithm for optimal framed clustering the authors\n have developed (Debnath & Song 2021) <doi:10.1109/TCBB.2021.3077573>.\n The runtime of these algorithms is O(K N log^2 N), where K is the number\n of clusters and N is the number of circular data points. On a desktop\n computer using a single processor core, millions of data points can be\n grouped into a few clusters within seconds. One can apply the algorithms\n to characterize events along circular DNA molecules, circular RNA\n molecules, and circular genomes of bacteria, chloroplast, and\n mitochondria. One can also cluster climate data along any given\n longitude or latitude. Periodic data clustering can be formulated as\n circular clustering. The algorithms offer a general high-performance\n solution to circular, periodic, or framed data clustering.   "
  },
  {
    "id": 5561,
    "package_name": "PCRedux",
    "title": "Quantitative Polymerase Chain Reaction (qPCR) Data Mining and\nMachine Learning Toolkit as Described in Burdukiewicz (2022)\n<doi:10.21105/Joss.04407>",
    "description": "Extracts features from amplification curve data of quantitative \n    Polymerase Chain Reactions (qPCR) according to Pabinger et al. 2014 \n    <doi:10.1016/j.bdq.2014.08.002> for machine learning purposes. Helper \n    functions prepare the amplification curve data for processing as functional \n    data (e.g., Hausdorff distance) or enable the plotting of amplification \n    curve classes (negative, ambiguous, positive). The hookreg() and hookregNL() \n    functions of Burdukiewicz et al. (2018) <doi:10.1016/j.bdq.2018.08.001> \n    can be used to predict amplification curves with an hook effect-like \n    curvature. The pcrfit_single() function can be used to extract features \n    from an amplification curve.",
    "version": "1.2-1",
    "maintainer": "Andrej-Nikolai Spiess <draspiess@gmail.com>",
    "author": "Stefan Roediger [aut] (ORCID: <https://orcid.org/0000-0002-1441-6512>),\n  Michal Burdukiewicz [aut] (ORCID:\n    <https://orcid.org/0000-0001-8926-582X>),\n  Andrej-Nikolai Spiess [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9630-4724>),\n  Konstantin A. Blagodatskikh [aut] (ORCID:\n    <https://orcid.org/0000-0002-8732-0300>),\n  Dominik Rafacz [ctb] (ORCID: <https://orcid.org/0000-0003-0925-1909>)",
    "url": "https://CRAN.R-project.org/package=PCRedux",
    "bug_reports": "https://github.com/PCRuniversum/PCRedux/issues",
    "repository": "https://cran.r-project.org/package=PCRedux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCRedux Quantitative Polymerase Chain Reaction (qPCR) Data Mining and\nMachine Learning Toolkit as Described in Burdukiewicz (2022)\n<doi:10.21105/Joss.04407> Extracts features from amplification curve data of quantitative \n    Polymerase Chain Reactions (qPCR) according to Pabinger et al. 2014 \n    <doi:10.1016/j.bdq.2014.08.002> for machine learning purposes. Helper \n    functions prepare the amplification curve data for processing as functional \n    data (e.g., Hausdorff distance) or enable the plotting of amplification \n    curve classes (negative, ambiguous, positive). The hookreg() and hookregNL() \n    functions of Burdukiewicz et al. (2018) <doi:10.1016/j.bdq.2018.08.001> \n    can be used to predict amplification curves with an hook effect-like \n    curvature. The pcrfit_single() function can be used to extract features \n    from an amplification curve.  "
  },
  {
    "id": 5575,
    "package_name": "PDtoolkit",
    "title": "Collection of Tools for PD Rating Model Development and\nValidation",
    "description": "The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.",
    "version": "1.2.0",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "author": "Andrija Djurovic [aut, cre]",
    "url": "https://github.com/andrija-djurovic/PDtoolkit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PDtoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PDtoolkit Collection of Tools for PD Rating Model Development and\nValidation The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.  "
  },
  {
    "id": 5585,
    "package_name": "PEkit",
    "title": "Partition Exchangeability Toolkit",
    "description": "Bayesian supervised predictive classifiers, hypothesis testing, and parametric estimation under Partition Exchangeability are implemented. The two classifiers presented are the marginal classifier (that assumes test data is i.i.d.) next to a more computationally costly but accurate simultaneous classifier (that finds a labelling for the entire test dataset at once based on simultanous use of all the test data to predict each label). We also provide the Maximum Likelihood Estimation (MLE) of the only underlying parameter of the partition exchangeability generative model as well as hypothesis testing statistics for equality of this parameter with a single value, alternative, or multiple samples. We present functions to simulate the sequences from Ewens Sampling Formula as the realisation of the Poisson-Dirichlet distribution and their respective probabilities.",
    "version": "1.0.0.1000",
    "maintainer": "Ali Amiryousefi <ali.amiryousefi@helsinki.fi>",
    "author": "Ville Kinnula [aut],\n  Jing Tang [ctb] (ORCID: <https://orcid.org/0000-0001-7480-7710>),\n  Ali Amiryousefi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6317-3860>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PEkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PEkit Partition Exchangeability Toolkit Bayesian supervised predictive classifiers, hypothesis testing, and parametric estimation under Partition Exchangeability are implemented. The two classifiers presented are the marginal classifier (that assumes test data is i.i.d.) next to a more computationally costly but accurate simultaneous classifier (that finds a labelling for the entire test dataset at once based on simultanous use of all the test data to predict each label). We also provide the Maximum Likelihood Estimation (MLE) of the only underlying parameter of the partition exchangeability generative model as well as hypothesis testing statistics for equality of this parameter with a single value, alternative, or multiple samples. We present functions to simulate the sequences from Ewens Sampling Formula as the realisation of the Poisson-Dirichlet distribution and their respective probabilities.  "
  },
  {
    "id": 5591,
    "package_name": "PGaGEV",
    "title": "Power Garima-Generalized Extreme Value Distribution",
    "description": "Density, distribution function, quantile function, \n    and random generation function based on Kittipong Klinjan,Tipat Sottiwan and Sirinapa Aryuyuen (2024)<DOI:10.28919/cmbn/8833>.",
    "version": "0.1.0",
    "maintainer": "Kittipong Klinjan <kittipong_k@rmutt.ac.th>",
    "author": "Kittipong Klinjan [cre, aut],\n  Tipat Sottiwan [aut],\n  Sirinapa Aryuyuen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PGaGEV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PGaGEV Power Garima-Generalized Extreme Value Distribution Density, distribution function, quantile function, \n    and random generation function based on Kittipong Klinjan,Tipat Sottiwan and Sirinapa Aryuyuen (2024)<DOI:10.28919/cmbn/8833>.  "
  },
  {
    "id": 5770,
    "package_name": "Partiallyoverlapping",
    "title": "Partially Overlapping Samples Tests",
    "description": "Tests for a comparison of two partially overlapping samples.\n  A comparison of means using the partially overlapping samples t-test: \n  See Derrick, Russ, Toher and White (2017), Test \n  statistics for the comparison of means for two samples which include \n  both paired observations and independent observations, Journal of \n  Modern Applied Statistical Methods, 16(1). \n  A comparison of proportions using the partially overlapping samples z-test:\n  See Derrick, Dobson-Mckittrick, Toher and White (2015), Test statistics\n  for comparing two proportions with partially overlapping samples.\n  Journal of Applied Quantitative Methods, 10(3).",
    "version": "2.0",
    "maintainer": "Ben Derrick <ben.derrick@uwe.ac.uk>",
    "author": "Ben Derrick",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Partiallyoverlapping",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Partiallyoverlapping Partially Overlapping Samples Tests Tests for a comparison of two partially overlapping samples.\n  A comparison of means using the partially overlapping samples t-test: \n  See Derrick, Russ, Toher and White (2017), Test \n  statistics for the comparison of means for two samples which include \n  both paired observations and independent observations, Journal of \n  Modern Applied Statistical Methods, 16(1). \n  A comparison of proportions using the partially overlapping samples z-test:\n  See Derrick, Dobson-Mckittrick, Toher and White (2015), Test statistics\n  for comparing two proportions with partially overlapping samples.\n  Journal of Applied Quantitative Methods, 10(3).  "
  },
  {
    "id": 5790,
    "package_name": "PepMapViz",
    "title": "A Versatile Toolkit for Peptide Mapping, Visualization, and\nComparative Exploration",
    "description": "A versatile R visualization package that empowers \n    researchers with comprehensive visualization tools for seamlessly mapping peptides \n    to protein sequences, identifying distinct domains and regions of interest, \n    accentuating mutations, and highlighting post-translational modifications, \n    all while enabling comparisons across diverse experimental conditions. \n    Potential applications of 'PepMapViz' include the visualization of cross-software \n    mass spectrometry results at the peptide level for specific protein and domain \n    details in a linearized format and post-translational modification coverage \n    across different experimental conditions; unraveling insights into disease \n    mechanisms. It also enables visualization of Major histocompatibility \n    complex-presented peptide clusters in different antibody regions predicting \n    immunogenicity in antibody drug development.",
    "version": "1.1.0",
    "maintainer": "Zhenru Zhou <zhou.zhenru@gene.com>",
    "author": "Zhenru Zhou [aut, cre],\n  Qui Phung [aut],\n  Corey Bakalarski [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PepMapViz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PepMapViz A Versatile Toolkit for Peptide Mapping, Visualization, and\nComparative Exploration A versatile R visualization package that empowers \n    researchers with comprehensive visualization tools for seamlessly mapping peptides \n    to protein sequences, identifying distinct domains and regions of interest, \n    accentuating mutations, and highlighting post-translational modifications, \n    all while enabling comparisons across diverse experimental conditions. \n    Potential applications of 'PepMapViz' include the visualization of cross-software \n    mass spectrometry results at the peptide level for specific protein and domain \n    details in a linearized format and post-translational modification coverage \n    across different experimental conditions; unraveling insights into disease \n    mechanisms. It also enables visualization of Major histocompatibility \n    complex-presented peptide clusters in different antibody regions predicting \n    immunogenicity in antibody drug development.  "
  },
  {
    "id": 5995,
    "package_name": "QGA",
    "title": "Quantum Genetic Algorithm",
    "description": "Function that implements the Quantum Genetic Algorithm, first proposed by Han and Kim in 2000. This is an R implementation of the 'python' application developed by Lahoz-Beltra  (<https://github.com/ResearchCodesHub/QuantumGeneticAlgorithms>). Each optimization problem is represented as a maximization one, where each solution is a sequence of (qu)bits. Following the quantum paradigm, these qubits are in a superposition state: when measuring them, they collapse in a 0 or 1 state. After measurement, the fitness of the solution is calculated as in usual genetic algorithms. The evolution at each iteration is oriented by the application of two quantum gates to the amplitudes of the qubits: (1) a rotation gate (always); (2) a Pauli-X gate (optionally). The rotation is based on the theta angle values: higher values allow a quicker evolution, and lower values avoid local maxima. The Pauli-X gate is equivalent to the classical mutation operator and determines the swap between alfa and beta amplitudes of a given qubit. The package has been developed in such a way as to permit a complete separation between the engine, and the particular problem subject to combinatorial optimization. ",
    "version": "1.0",
    "maintainer": "Giulio Barcaroli <gbarcaroli@gmail.com>",
    "author": "Giulio Barcaroli [aut, cre]",
    "url": "https://barcaroli.github.io/QGA/,\nhttps://github.com/barcaroli/QGA/",
    "bug_reports": "https://github.com/barcaroli/QGA/issues",
    "repository": "https://cran.r-project.org/package=QGA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QGA Quantum Genetic Algorithm Function that implements the Quantum Genetic Algorithm, first proposed by Han and Kim in 2000. This is an R implementation of the 'python' application developed by Lahoz-Beltra  (<https://github.com/ResearchCodesHub/QuantumGeneticAlgorithms>). Each optimization problem is represented as a maximization one, where each solution is a sequence of (qu)bits. Following the quantum paradigm, these qubits are in a superposition state: when measuring them, they collapse in a 0 or 1 state. After measurement, the fitness of the solution is calculated as in usual genetic algorithms. The evolution at each iteration is oriented by the application of two quantum gates to the amplitudes of the qubits: (1) a rotation gate (always); (2) a Pauli-X gate (optionally). The rotation is based on the theta angle values: higher values allow a quicker evolution, and lower values avoid local maxima. The Pauli-X gate is equivalent to the classical mutation operator and determines the swap between alfa and beta amplitudes of a given qubit. The package has been developed in such a way as to permit a complete separation between the engine, and the particular problem subject to combinatorial optimization.   "
  },
  {
    "id": 6030,
    "package_name": "QuICSeedR",
    "title": "Analyze Data for Fluorophore-Assisted Seed Amplification Assays",
    "description": "A toolkit for analysis and visualization of data from fluorophore-assisted seed amplification assays, such as Real-Time Quaking-Induced Conversion (RT-QuIC) and Fluorophore-Assisted Protein Misfolding Cyclic Amplification (PMCA). 'QuICSeedR' addresses limitations in existing software by automating data processing, supporting large-scale analysis, and enabling comparative studies of analysis methods. It incorporates methods described in Henderson et al. (2015) <doi:10.1099/vir.0.069906-0>, Li et al. (2020) <doi:10.1038/s41598-021-96127-8>, Rowden et al. (2023) <doi:10.3390/pathogens12020309>, Haley et al. (2013) <doi:10.1371/journal.pone.0081488>, and Mair and Wilcox (2020) <doi:10.3758/s13428-019-01246-w>. Please refer to the original publications for details.",
    "version": "0.1.2",
    "maintainer": "Manci Li <li000021@umn.edu>",
    "author": "Manci Li [aut, cre] (ORCID: <https://orcid.org/0000-0003-3930-7117>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QuICSeedR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QuICSeedR Analyze Data for Fluorophore-Assisted Seed Amplification Assays A toolkit for analysis and visualization of data from fluorophore-assisted seed amplification assays, such as Real-Time Quaking-Induced Conversion (RT-QuIC) and Fluorophore-Assisted Protein Misfolding Cyclic Amplification (PMCA). 'QuICSeedR' addresses limitations in existing software by automating data processing, supporting large-scale analysis, and enabling comparative studies of analysis methods. It incorporates methods described in Henderson et al. (2015) <doi:10.1099/vir.0.069906-0>, Li et al. (2020) <doi:10.1038/s41598-021-96127-8>, Rowden et al. (2023) <doi:10.3390/pathogens12020309>, Haley et al. (2013) <doi:10.1371/journal.pone.0081488>, and Mair and Wilcox (2020) <doi:10.3758/s13428-019-01246-w>. Please refer to the original publications for details.  "
  },
  {
    "id": 6063,
    "package_name": "R2DT",
    "title": "Translation of Base R-Like Functions for 'data.table' Objects",
    "description": "Some heavily used base R functions are reconstructed to also be compliant to data.table objects. Also, some general helper functions that could be of interest for working with data.table objects are included.",
    "version": "0.2",
    "maintainer": "Robin Van Oirbeek <robin.vanoirbeek@gmail.com>",
    "author": "Robin Van Oirbeek",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=R2DT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R2DT Translation of Base R-Like Functions for 'data.table' Objects Some heavily used base R functions are reconstructed to also be compliant to data.table objects. Also, some general helper functions that could be of interest for working with data.table objects are included.  "
  },
  {
    "id": 6106,
    "package_name": "RAthena",
    "title": "Connect to 'AWS Athena' using 'Boto3' ('DBI' Interface)",
    "description": "Designed to be compatible with the R package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this 'Python' 'Boto3' Software Development Kit ('SDK')\n    <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html> is used as a driver.",
    "version": "2.6.3",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "Dyfan Jones [aut, cre]",
    "url": "https://dyfanjones.github.io/RAthena/,\nhttps://github.com/DyfanJones/RAthena",
    "bug_reports": "https://github.com/DyfanJones/RAthena/issues",
    "repository": "https://cran.r-project.org/package=RAthena",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAthena Connect to 'AWS Athena' using 'Boto3' ('DBI' Interface) Designed to be compatible with the R package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this 'Python' 'Boto3' Software Development Kit ('SDK')\n    <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html> is used as a driver.  "
  },
  {
    "id": 6141,
    "package_name": "RClickhouse",
    "title": "'Yandex Clickhouse' Interface for R with Basic 'dplyr' Support",
    "description": "'Yandex Clickhouse' (<https://clickhouse.com/>) is a high-performance relational column-store database to enable\n    big data exploration and 'analytics' scaling to petabytes of data. Methods are\n    provided that enable working with 'Yandex Clickhouse' databases via\n    'DBI' methods and using 'dplyr'/'dbplyr' idioms.",
    "version": "0.6.10",
    "maintainer": "Christian Hotz-Behofsits <christian.hotz-behofsits@wu.ac.at>",
    "author": "Christian Hotz-Behofsits [aut, cre],\n  Daniel Winkler [aut],\n  Luca Rauchenberger [aut],\n  Peter Knaus [aut],\n  Clemens Danninger [aut],\n  Daria Yudaeva [aut],\n  Simon Stiebellehner [aut],\n  Dan Egnor [aut],\n  Vlad Losev [aut],\n  Keith Ray [aut],\n  Zhanyong Wan [aut],\n  Markus Heule [aut],\n  Oliver Flasch [aut],\n  Google [cph],\n  Yann Collet [cph, aut] (Yann Collet is the author and copyright holder\n    of 'lz4')",
    "url": "https://github.com/IMSMWU/RClickhouse",
    "bug_reports": "https://github.com/IMSMWU/RClickhouse/issues",
    "repository": "https://cran.r-project.org/package=RClickhouse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RClickhouse 'Yandex Clickhouse' Interface for R with Basic 'dplyr' Support 'Yandex Clickhouse' (<https://clickhouse.com/>) is a high-performance relational column-store database to enable\n    big data exploration and 'analytics' scaling to petabytes of data. Methods are\n    provided that enable working with 'Yandex Clickhouse' databases via\n    'DBI' methods and using 'dplyr'/'dbplyr' idioms.  "
  },
  {
    "id": 6227,
    "package_name": "RHPCBenchmark",
    "title": "Benchmarks for High-Performance Computing Environments",
    "description": "Microbenchmarks for determining the run time\n  performance of aspects of the R programming environment and packages\n  relevant to high-performance computation.  The benchmarks are divided into\n  three categories: dense matrix linear algebra kernels, sparse matrix linear\n  algebra kernels, and machine learning functionality.",
    "version": "0.1.0",
    "maintainer": "James McCombs <jmccombs@iu.edu>",
    "author": "James McCombs [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RHPCBenchmark",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RHPCBenchmark Benchmarks for High-Performance Computing Environments Microbenchmarks for determining the run time\n  performance of aspects of the R programming environment and packages\n  relevant to high-performance computation.  The benchmarks are divided into\n  three categories: dense matrix linear algebra kernels, sparse matrix linear\n  algebra kernels, and machine learning functionality.  "
  },
  {
    "id": 6245,
    "package_name": "RInside",
    "title": "C++ Classes to Embed R in C++ (and C) Applications",
    "description": "C++ classes to embed R in C++ (and C) applications\n A C++ class providing the R interpreter is offered by this package\n making it easier to have \"R inside\" your C++ application. As R itself\n is embedded into your application, a shared library build of R is\n required. This works on Linux, OS X and even on Windows provided you\n use the same tools used to build R itself. Numerous examples are\n provided in the nine subdirectories of the examples/ directory of\n the installed package: standard, 'mpi' (for parallel computing), 'qt'\n (showing how to embed 'RInside' inside a Qt GUI application), 'wt'\n (showing how to build a \"web-application\" using the Wt toolkit),\n 'armadillo' (for 'RInside' use with 'RcppArmadillo'), 'eigen' (for\n 'RInside' use with 'RcppEigen'), and 'c_interface' for a basic C\n interface and 'Ruby' illustration.  The examples use 'GNUmakefile(s)'\n with GNU extensions, so a GNU make is required (and will use the\n 'GNUmakefile' automatically). 'Doxygen'-generated documentation of\n the C++ classes is available at the 'RInside' website as well.",
    "version": "0.2.19",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Romain Francois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Lance Bachmeier [ctb]",
    "url": "https://github.com/eddelbuettel/rinside/,\nhttps://dirk.eddelbuettel.com/code/rinside.html",
    "bug_reports": "https://github.com/eddelbuettel/rinside/issues",
    "repository": "https://cran.r-project.org/package=RInside",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RInside C++ Classes to Embed R in C++ (and C) Applications C++ classes to embed R in C++ (and C) applications\n A C++ class providing the R interpreter is offered by this package\n making it easier to have \"R inside\" your C++ application. As R itself\n is embedded into your application, a shared library build of R is\n required. This works on Linux, OS X and even on Windows provided you\n use the same tools used to build R itself. Numerous examples are\n provided in the nine subdirectories of the examples/ directory of\n the installed package: standard, 'mpi' (for parallel computing), 'qt'\n (showing how to embed 'RInside' inside a Qt GUI application), 'wt'\n (showing how to build a \"web-application\" using the Wt toolkit),\n 'armadillo' (for 'RInside' use with 'RcppArmadillo'), 'eigen' (for\n 'RInside' use with 'RcppEigen'), and 'c_interface' for a basic C\n interface and 'Ruby' illustration.  The examples use 'GNUmakefile(s)'\n with GNU extensions, so a GNU make is required (and will use the\n 'GNUmakefile' automatically). 'Doxygen'-generated documentation of\n the C++ classes is available at the 'RInside' website as well.  "
  },
  {
    "id": 6273,
    "package_name": "RMAT",
    "title": "Random Matrix Analysis Toolkit",
    "description": "Simulate random matrices and ensembles and compute their eigenvalue spectra and dispersions.",
    "version": "0.2.0",
    "maintainer": "Ali Taqi <alif.taqi00@gmail.com>",
    "author": "Ali Taqi [aut, cre],\n  Jonathan Wells [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RMAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMAT Random Matrix Analysis Toolkit Simulate random matrices and ensembles and compute their eigenvalue spectra and dispersions.  "
  },
  {
    "id": 6325,
    "package_name": "ROCnGO",
    "title": "Fast Analysis of ROC Curves",
    "description": "A toolkit for analyzing classifier performance by using receiver\n    operating characteristic (ROC) curves. Performance may be assessed on\n    a single classifier or multiple ones simultaneously, making it suitable for\n    comparisons. In addition, different metrics allow the evaluation of local\n    performance when working within restricted ranges of sensitivity and\n    specificity. For details on the different implementations, see\n    McClish D. K. (1989) <doi:10.1177/0272989X8900900307>,\n    Vivo J.-M., Franco M. and Vicari D. (2018) <doi:10.1007/S11634-017-0295-9>,\n    Jiang Y., et al (1996) <doi:10.1148/radiology.201.3.8939225>,\n    Franco M. and Vivo J.-M. (2021) <doi:10.3390/math9212826> and\n    Carrington, Andr\u00e9 M., et al (2020) <doi: 10.1186/s12911-019-1014-6>.",
    "version": "0.1.0",
    "maintainer": "Pablo Navarro <pablo.navarrocarpio@gmail.com>",
    "author": "Pablo Navarro [aut, cre, cph],\n  Juana-Mar\u00eda Vivo [aut],\n  Manuel Franco [aut]",
    "url": "https://pablopnc.github.io/ROCnGO/,\nhttps://github.com/pabloPNC/ROCnGO",
    "bug_reports": "https://github.com/pabloPNC/ROCnGO/issues",
    "repository": "https://cran.r-project.org/package=ROCnGO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ROCnGO Fast Analysis of ROC Curves A toolkit for analyzing classifier performance by using receiver\n    operating characteristic (ROC) curves. Performance may be assessed on\n    a single classifier or multiple ones simultaneously, making it suitable for\n    comparisons. In addition, different metrics allow the evaluation of local\n    performance when working within restricted ranges of sensitivity and\n    specificity. For details on the different implementations, see\n    McClish D. K. (1989) <doi:10.1177/0272989X8900900307>,\n    Vivo J.-M., Franco M. and Vicari D. (2018) <doi:10.1007/S11634-017-0295-9>,\n    Jiang Y., et al (1996) <doi:10.1148/radiology.201.3.8939225>,\n    Franco M. and Vivo J.-M. (2021) <doi:10.3390/math9212826> and\n    Carrington, Andr\u00e9 M., et al (2020) <doi: 10.1186/s12911-019-1014-6>.  "
  },
  {
    "id": 6347,
    "package_name": "RPDTest",
    "title": "A New Type of Test Statistic and Method for Multinomial\nGoodness-of-Fit Test",
    "description": "Performs multinomial goodness-of-fit test on multinomially distributed data using the Randomized phi-divergence test statistics. Details of this kind of statistics can be found at Nikita Puchkin, Vladimir Ulyanov (2023) <doi:10.1214/22-AIHP1299>.",
    "version": "0.0.2",
    "maintainer": "Renkang Liu <eG0im@outlook.com>",
    "author": "Renkang Liu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RPDTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPDTest A New Type of Test Statistic and Method for Multinomial\nGoodness-of-Fit Test Performs multinomial goodness-of-fit test on multinomially distributed data using the Randomized phi-divergence test statistics. Details of this kind of statistics can be found at Nikita Puchkin, Vladimir Ulyanov (2023) <doi:10.1214/22-AIHP1299>.  "
  },
  {
    "id": 6426,
    "package_name": "RTL",
    "title": "Risk Tool Library - Trading, Risk, Analytics for Commodities",
    "description": "A toolkit for Commodities 'analytics', risk management and\n    trading professionals. Includes functions for API calls to\n    <https://commodities.morningstar.com/#/>, <https://developer.genscape.com/>,\n    and <https://www.bankofcanada.ca/valet/docs>.",
    "version": "1.3.7",
    "maintainer": "Philippe Cote <pcote@ualberta.ca>",
    "author": "Philippe Cote [aut, cre],\n  Nima Safaian [aut]",
    "url": "https://github.com/risktoollib/RTL",
    "bug_reports": "https://github.com/risktoollib/RTL/issues",
    "repository": "https://cran.r-project.org/package=RTL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTL Risk Tool Library - Trading, Risk, Analytics for Commodities A toolkit for Commodities 'analytics', risk management and\n    trading professionals. Includes functions for API calls to\n    <https://commodities.morningstar.com/#/>, <https://developer.genscape.com/>,\n    and <https://www.bankofcanada.ca/valet/docs>.  "
  },
  {
    "id": 6460,
    "package_name": "Racmacs",
    "title": "Antigenic Cartography Macros",
    "description": "A toolkit for making antigenic maps from immunological assay data,\n    in order to quantify and visualize antigenic differences between different\n    pathogen strains as described in\n    Smith et al. (2004) <doi:10.1126/science.1097211> and used in the World\n    Health Organization influenza vaccine strain selection process. Additional\n    functions allow for the diagnostic evaluation of antigenic maps and an\n    interactive viewer is provided to explore antigenic relationships amongst\n    several strains and incorporate the visualization of associated genetic\n    information.",
    "version": "1.2.9",
    "maintainer": "Sam Wilks <sw463@cam.ac.uk>",
    "author": "Sam Wilks [aut, cre]",
    "url": "https://acorg.github.io/Racmacs/,\nhttps://github.com/acorg/Racmacs/",
    "bug_reports": "https://github.com/acorg/Racmacs/issues",
    "repository": "https://cran.r-project.org/package=Racmacs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Racmacs Antigenic Cartography Macros A toolkit for making antigenic maps from immunological assay data,\n    in order to quantify and visualize antigenic differences between different\n    pathogen strains as described in\n    Smith et al. (2004) <doi:10.1126/science.1097211> and used in the World\n    Health Organization influenza vaccine strain selection process. Additional\n    functions allow for the diagnostic evaluation of antigenic maps and an\n    interactive viewer is provided to explore antigenic relationships amongst\n    several strains and incorporate the visualization of associated genetic\n    information.  "
  },
  {
    "id": 6486,
    "package_name": "RapidFuzz",
    "title": "String Similarity Computation Using 'RapidFuzz'",
    "description": "Provides a high-performance interface for calculating string similarities and distances, leveraging the efficient library 'RapidFuzz' <https://github.com/rapidfuzz/rapidfuzz-cpp>. This package integrates the 'C++' implementation, allowing 'R' users to access cutting-edge algorithms for fuzzy matching and text analysis.",
    "version": "1.0",
    "maintainer": "Andre Leite <leite@castlab.org>",
    "author": "Andre Leite [aut, cre],\n  Hugo Vaconcelos [aut],\n  Max Bachmann [ctb],\n  Adam Cohen [ctb]",
    "url": "<https://github.com/StrategicProjects/RapidFuzz>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RapidFuzz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RapidFuzz String Similarity Computation Using 'RapidFuzz' Provides a high-performance interface for calculating string similarities and distances, leveraging the efficient library 'RapidFuzz' <https://github.com/rapidfuzz/rapidfuzz-cpp>. This package integrates the 'C++' implementation, allowing 'R' users to access cutting-edge algorithms for fuzzy matching and text analysis.  "
  },
  {
    "id": 6497,
    "package_name": "Rato",
    "title": "Resilience Analysis Toolkit (RATO)",
    "description": "Collection of tools for the analysis of the resilience of dynamic networks. Created as a classroom project.",
    "version": "0.1.0",
    "maintainer": "Victor Chavauty <lesserfish@pm.me>",
    "author": "Victor Chavauty [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rato",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rato Resilience Analysis Toolkit (RATO) Collection of tools for the analysis of the resilience of dynamic networks. Created as a classroom project.  "
  },
  {
    "id": 6557,
    "package_name": "RcppBlaze",
    "title": "'Rcpp' Integration for the 'Blaze' High-Performance 'C++' Math\nLibrary",
    "description": "Blaze is an open-source, high-performance 'C++' math library for dense and sparse arithmetic. \n    With its state-of-the-art Smart Expression Template implementation Blaze combines the elegance and \n    ease of use of a domain-specific language with HPC-grade performance, making it one of the most \n    intuitive and fastest 'C++' math libraries available. The 'RcppBlaze' package includes the header files \n    from the 'Blaze' library with disabling some functionalities related to link to the thread and system\n    libraries which make 'RcppBlaze' be a header-only library. Therefore, users do not need to install \n    'Blaze'.",
    "version": "1.0.1",
    "maintainer": "Ching-Chuan Chen <zw12356@gmail.com>",
    "author": "Ching-Chuan Chen [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0007-8273-3206>),\n  Klaus Iglberger [aut] (blaze),\n  Georg Georg [aut] (blaze),\n  Tobias Scharpff [aut] (blaze)",
    "url": "https://github.com/Chingchuan-chen/RcppBlaze,\nhttps://bitbucket.org/blaze-lib/blaze",
    "bug_reports": "https://github.com/Chingchuan-chen/RcppBlaze/issues",
    "repository": "https://cran.r-project.org/package=RcppBlaze",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppBlaze 'Rcpp' Integration for the 'Blaze' High-Performance 'C++' Math\nLibrary Blaze is an open-source, high-performance 'C++' math library for dense and sparse arithmetic. \n    With its state-of-the-art Smart Expression Template implementation Blaze combines the elegance and \n    ease of use of a domain-specific language with HPC-grade performance, making it one of the most \n    intuitive and fastest 'C++' math libraries available. The 'RcppBlaze' package includes the header files \n    from the 'Blaze' library with disabling some functionalities related to link to the thread and system\n    libraries which make 'RcppBlaze' be a header-only library. Therefore, users do not need to install \n    'Blaze'.  "
  },
  {
    "id": 6586,
    "package_name": "RcppLbfgsBlaze",
    "title": "'L-BFGS' Algorithm Based on 'Blaze' for 'R' and 'Rcpp'",
    "description": "The 'L-BFGS' algorithm is a popular optimization algorithm for unconstrained optimization problems. \n  'Blaze' is a high-performance 'C++' math library for dense and sparse arithmetic. \n  This package provides a simple interface to the 'L-BFGS' algorithm and allows users to optimize \n  their objective functions with 'Blaze' vectors and matrices in 'R' and 'Rcpp'.",
    "version": "0.1.0",
    "maintainer": "Ching-Chuan Chen <zw12356@gmail.com>",
    "author": "Ching-Chuan Chen [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0007-8273-3206>),\n  Zhepei Wang [aut] (LBFGS-Lite),\n  Naoaki Okazaki [aut] (liblbfgs)",
    "url": "https://github.com/ChingChuan-Chen/RcppLbfgsBlaze,\nhttps://github.com/ChingChuan-Chen/LBFGS-blaze,\nhttps://github.com/ZJU-FAST-Lab/LBFGS-Lite,\nhttps://bitbucket.org/blaze-lib/blaze/src/master/",
    "bug_reports": "https://github.com/Chingchuan-chen/RcppLbfgsBlaze/issues",
    "repository": "https://cran.r-project.org/package=RcppLbfgsBlaze",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppLbfgsBlaze 'L-BFGS' Algorithm Based on 'Blaze' for 'R' and 'Rcpp' The 'L-BFGS' algorithm is a popular optimization algorithm for unconstrained optimization problems. \n  'Blaze' is a high-performance 'C++' math library for dense and sparse arithmetic. \n  This package provides a simple interface to the 'L-BFGS' algorithm and allows users to optimize \n  their objective functions with 'Blaze' vectors and matrices in 'R' and 'Rcpp'.  "
  },
  {
    "id": 6600,
    "package_name": "RcppSimdJson",
    "title": "'Rcpp' Bindings for the 'simdjson' Header-Only Library for\n'JSON' Parsing",
    "description": "The 'JSON' format is ubiquitous for data interchange, and the\n 'simdjson' library written by Daniel Lemire (and many contributors) provides a\n high-performance parser for these files which by relying on parallel 'SIMD'\n instruction manages to parse these files as faster than disk speed. See the\n <doi:10.48550/arXiv.1902.08318> paper for more details about 'simdjson'.  This\n package parses 'JSON' from string, file, or remote URLs under a variety of\n settings.",
    "version": "0.1.14",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Brendan Knapp [aut] (ORCID: <https://orcid.org/0000-0003-3284-4972>),\n  Daniel Lemire [aut] (ORCID: <https://orcid.org/0000-0003-3306-6922>)",
    "url": "https://github.com/eddelbuettel/rcppsimdjson/",
    "bug_reports": "https://github.com/eddelbuettel/rcppsimdjson/issues",
    "repository": "https://cran.r-project.org/package=RcppSimdJson",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcppSimdJson 'Rcpp' Bindings for the 'simdjson' Header-Only Library for\n'JSON' Parsing The 'JSON' format is ubiquitous for data interchange, and the\n 'simdjson' library written by Daniel Lemire (and many contributors) provides a\n high-performance parser for these files which by relying on parallel 'SIMD'\n instruction manages to parse these files as faster than disk speed. See the\n <doi:10.48550/arXiv.1902.08318> paper for more details about 'simdjson'.  This\n package parses 'JSON' from string, file, or remote URLs under a variety of\n settings.  "
  },
  {
    "id": 6646,
    "package_name": "RedditExtractoR",
    "title": "Reddit Data Extraction Toolkit",
    "description": "A collection of tools for extracting structured data from <https://www.reddit.com/>.",
    "version": "3.0.9",
    "maintainer": "Ivan Rivera <ivan.s.rivera@gmail.com>",
    "author": "Ivan Rivera <ivan.s.rivera@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RedditExtractoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RedditExtractoR Reddit Data Extraction Toolkit A collection of tools for extracting structured data from <https://www.reddit.com/>.  "
  },
  {
    "id": 6672,
    "package_name": "RepeatedHighDim",
    "title": "Methods for High-Dimensional Repeated Measures Data",
    "description": "A toolkit for the analysis of high-dimensional repeated measurements, providing functions \n    for outlier detection, differential expression analysis, gene-set tests, and binary random data generation.",
    "version": "2.4.0",
    "maintainer": "Klaus Jung <klaus.jung@tiho-hannover.de>",
    "author": "Klaus Jung [aut, cre],\n  Jochen Kruppa [aut],\n  Sergej Ruff [aut]",
    "url": "https://software.klausjung-lab.de",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RepeatedHighDim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RepeatedHighDim Methods for High-Dimensional Repeated Measures Data A toolkit for the analysis of high-dimensional repeated measurements, providing functions \n    for outlier detection, differential expression analysis, gene-set tests, and binary random data generation.  "
  },
  {
    "id": 6688,
    "package_name": "RestRserve",
    "title": "A Framework for Building HTTP API",
    "description": "\n  Allows to easily create high-performance full featured HTTP APIs from R\n  functions. Provides high-level classes such as 'Request', 'Response',\n  'Application', 'Middleware' in order to streamline server side\n  application development. Out of the box allows to serve requests using\n  'Rserve' package, but flexible enough to integrate with other HTTP servers\n  such as 'httpuv'.",
    "version": "1.2.4",
    "maintainer": "Dmitry Selivanov <selivanov.dmitriy@gmail.com>",
    "author": "Dmitry Selivanov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5413-1506>),\n  Artem Klevtsov [aut] (ORCID: <https://orcid.org/0000-0003-0492-6647>),\n  David Zimmermann [ctb],\n  rexy.ai [cph, fnd]",
    "url": "https://restrserve.org, https://github.com/rexyai/RestRserve",
    "bug_reports": "https://github.com/rexyai/RestRserve/issues",
    "repository": "https://cran.r-project.org/package=RestRserve",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RestRserve A Framework for Building HTTP API \n  Allows to easily create high-performance full featured HTTP APIs from R\n  functions. Provides high-level classes such as 'Request', 'Response',\n  'Application', 'Middleware' in order to streamline server side\n  application development. Out of the box allows to serve requests using\n  'Rserve' package, but flexible enough to integrate with other HTTP servers\n  such as 'httpuv'.  "
  },
  {
    "id": 6724,
    "package_name": "Rita",
    "title": "Automated Transformations, Normality Testing, and Reporting",
    "description": "\n    Automated performance of common transformations used to fulfill parametric\n    assumptions of normality and identification of the best performing method \n    for the user. Output for various normality tests (Thode, 2002) corresponding \n    to the best performing method and a descriptive statistical report of the \n    input data in its original units (5-number summary and mathematical moments) \n    are also presented. Lastly, the Rankit, an empirical normal quantile transformation \n    (ENQT) (Soloman & Sawilowsky, 2009), is provided to accommodate non-standard \n    use cases and facilitate adoption.  \n    <DOI: 10.1201/9780203910894>.\n    <DOI: 10.22237/jmasm/1257034080>.",
    "version": "1.2.0",
    "maintainer": "Daniel Mattei <DMattei@live.com>",
    "author": "Daniel Mattei [aut, cre],\n  John Ruscio [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rita",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rita Automated Transformations, Normality Testing, and Reporting \n    Automated performance of common transformations used to fulfill parametric\n    assumptions of normality and identification of the best performing method \n    for the user. Output for various normality tests (Thode, 2002) corresponding \n    to the best performing method and a descriptive statistical report of the \n    input data in its original units (5-number summary and mathematical moments) \n    are also presented. Lastly, the Rankit, an empirical normal quantile transformation \n    (ENQT) (Soloman & Sawilowsky, 2009), is provided to accommodate non-standard \n    use cases and facilitate adoption.  \n    <DOI: 10.1201/9780203910894>.\n    <DOI: 10.22237/jmasm/1257034080>.  "
  },
  {
    "id": 6836,
    "package_name": "Rtrack",
    "title": "Spatial Navigation Strategy Analysis",
    "description": "A toolkit for the analysis of paths from spatial tracking experiments and calculation of goal-finding strategies. \n    This package is centered on an approach using machine learning for path classification.",
    "version": "2.0.4",
    "maintainer": "Rupert Overall <rtrack@rupertoverall.net>",
    "author": "Rupert Overall [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3882-6073>)",
    "url": "https://rupertoverall.net/Rtrack/,\nhttps://github.com/rupertoverall/Rtrack",
    "bug_reports": "https://github.com/rupertoverall/Rtrack/issues",
    "repository": "https://cran.r-project.org/package=Rtrack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rtrack Spatial Navigation Strategy Analysis A toolkit for the analysis of paths from spatial tracking experiments and calculation of goal-finding strategies. \n    This package is centered on an approach using machine learning for path classification.  "
  },
  {
    "id": 6873,
    "package_name": "SAMtool",
    "title": "Stock Assessment Methods Toolkit",
    "description": "Simulation tools for closed-loop simulation are provided for the 'MSEtool' operating model to inform data-rich fisheries. \n  'SAMtool' provides a conditioning model, assessment models of varying complexity with standardized reporting, \n  model-based management procedures, and diagnostic tools for evaluating assessments inside closed-loop simulation.",
    "version": "1.9.0",
    "maintainer": "Quang Huynh <quang@bluematterscience.com>",
    "author": "Quang Huynh [aut, cre],\n  Tom Carruthers [aut],\n  Adrian Hordyk [aut]",
    "url": "https://openmse.com, https://samtool.openmse.com,\nhttps://github.com/Blue-Matter/SAMtool",
    "bug_reports": "https://github.com/Blue-Matter/SAMtool/issues",
    "repository": "https://cran.r-project.org/package=SAMtool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SAMtool Stock Assessment Methods Toolkit Simulation tools for closed-loop simulation are provided for the 'MSEtool' operating model to inform data-rich fisheries. \n  'SAMtool' provides a conditioning model, assessment models of varying complexity with standardized reporting, \n  model-based management procedures, and diagnostic tools for evaluating assessments inside closed-loop simulation.  "
  },
  {
    "id": 6977,
    "package_name": "SIAtools",
    "title": "'ShinyItemAnalysis' Modules Development Toolkit",
    "description": "A comprehensive suite of functions designed for constructing\n    and managing 'ShinyItemAnalysis' modules, supplemented with detailed\n    guides, ready-to-use templates, linters, and tests. This package\n    allows developers to seamlessly create and integrate one or more\n    modules into their existing packages or to start a new module project\n    from scratch.",
    "version": "0.1.4",
    "maintainer": "Jan Netik <netik@cs.cas.cz>",
    "author": "Jan Netik [cre, aut] (ORCID: <https://orcid.org/0000-0002-3888-3203>),\n  Patricia Martinkova [aut] (ORCID:\n    <https://orcid.org/0000-0003-4754-8543>)",
    "url": "https://applstat.github.io/SIAtools/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SIAtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SIAtools 'ShinyItemAnalysis' Modules Development Toolkit A comprehensive suite of functions designed for constructing\n    and managing 'ShinyItemAnalysis' modules, supplemented with detailed\n    guides, ready-to-use templates, linters, and tests. This package\n    allows developers to seamlessly create and integrate one or more\n    modules into their existing packages or to start a new module project\n    from scratch.  "
  },
  {
    "id": 7016,
    "package_name": "SLIC",
    "title": "LIC for Distributed Skewed Regression",
    "description": "This comprehensive toolkit for skewed regression is designated as \"SLIC\" (The LIC for Distributed Skewed Regression Analysis). It is predicated on the assumption that the error term follows a skewed distribution, such as the Skew-Normal, Skew-t, or Skew-Laplace. The methodology and theoretical foundation of the package are described in Guo G.(2020) <doi:10.1080/02664763.2022.2053949>.",
    "version": "0.3",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "author": "Guangbao Guo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4115-6218>),\n  Hengxin Gao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SLIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SLIC LIC for Distributed Skewed Regression This comprehensive toolkit for skewed regression is designated as \"SLIC\" (The LIC for Distributed Skewed Regression Analysis). It is predicated on the assumption that the error term follows a skewed distribution, such as the Skew-Normal, Skew-t, or Skew-Laplace. The methodology and theoretical foundation of the package are described in Guo G.(2020) <doi:10.1080/02664763.2022.2053949>.  "
  },
  {
    "id": 7074,
    "package_name": "SP2000",
    "title": "Catalogue of Life Toolkit",
    "description": "A programmatic interface to <http://sp2000.org.cn>, re-written based on an accompanying 'Species 2000' API. Access tables describing catalogue of the Chinese known species of animals, plants, fungi, micro-organisms, and more. This package also supports access to catalogue of life global <http://catalogueoflife.org>, China animal scientific database <http://zoology.especies.cn> and catalogue of life Taiwan <https://taibnet.sinica.edu.tw/home_eng.php>. The development of 'SP2000' package were supported by Biodiversity Survey and Assessment Project of the Ministry of Ecology and Environment, China <2019HJ2096001006>,Yunnan University's \"Double First Class\" Project <C176240405> and Yunnan University's Research Innovation Fund for Graduate Students <2019227>.",
    "version": "0.2.0",
    "maintainer": "Liuyong Ding <ly_ding@126.com>",
    "author": "Liuyong Ding [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5490-182X>),\n  Minrui Huang [ctb],\n  Ke Yang [ctb],\n  Jun Wang [ctb] (ORCID: <https://orcid.org/0000-0003-2481-1409>),\n  Juan Tao [ctb],\n  Chengzhi Ding [ctb] (ORCID: <https://orcid.org/0000-0001-5215-7374>),\n  Daming He [ctb]",
    "url": "https://otoliths.github.io/SP2000/",
    "bug_reports": "https://github.com/Otoliths/SP2000/issues",
    "repository": "https://cran.r-project.org/package=SP2000",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SP2000 Catalogue of Life Toolkit A programmatic interface to <http://sp2000.org.cn>, re-written based on an accompanying 'Species 2000' API. Access tables describing catalogue of the Chinese known species of animals, plants, fungi, micro-organisms, and more. This package also supports access to catalogue of life global <http://catalogueoflife.org>, China animal scientific database <http://zoology.especies.cn> and catalogue of life Taiwan <https://taibnet.sinica.edu.tw/home_eng.php>. The development of 'SP2000' package were supported by Biodiversity Survey and Assessment Project of the Ministry of Ecology and Environment, China <2019HJ2096001006>,Yunnan University's \"Double First Class\" Project <C176240405> and Yunnan University's Research Innovation Fund for Graduate Students <2019227>.  "
  },
  {
    "id": 7166,
    "package_name": "SVDNF",
    "title": "Discrete Nonlinear Filtering for Stochastic Volatility Models",
    "description": "Implements the discrete nonlinear filter (DNF) of Kitagawa (1987) <doi:10.1080/01621459.1987.10478534> to a wide class of stochastic volatility (SV) models with return and volatility jumps following the work of B\u00e9gin and Boudreault (2021) <doi:10.1080/10618600.2020.1840995> to obtain likelihood evaluations and maximum likelihood parameter estimates. Offers several built-in SV models and a flexible framework for users to create customized models by specifying drift and diffusion functions along with an arrival distribution for the return and volatility jumps. Allows for the estimation of factor models with stochastic volatility (e.g., heteroskedastic volatility CAPM) by incorporating expected return predictors. Also includes functions to compute filtering and prediction distribution estimates, to simulate data from built-in and custom SV models with jumps, and to forecast future returns and volatility values using Monte Carlo simulation from a given SV model. ",
    "version": "0.1.11",
    "maintainer": "Louis Arsenault-Mahjoubi <larsenau@sfu.ca>",
    "author": "Louis Arsenault-Mahjoubi [aut, cre],\n  Jean-Fran\u00e7ois B\u00e9gin [aut],\n  Mathieu Boudreault [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SVDNF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SVDNF Discrete Nonlinear Filtering for Stochastic Volatility Models Implements the discrete nonlinear filter (DNF) of Kitagawa (1987) <doi:10.1080/01621459.1987.10478534> to a wide class of stochastic volatility (SV) models with return and volatility jumps following the work of B\u00e9gin and Boudreault (2021) <doi:10.1080/10618600.2020.1840995> to obtain likelihood evaluations and maximum likelihood parameter estimates. Offers several built-in SV models and a flexible framework for users to create customized models by specifying drift and diffusion functions along with an arrival distribution for the return and volatility jumps. Allows for the estimation of factor models with stochastic volatility (e.g., heteroskedastic volatility CAPM) by incorporating expected return predictors. Also includes functions to compute filtering and prediction distribution estimates, to simulate data from built-in and custom SV models with jumps, and to forecast future returns and volatility values using Monte Carlo simulation from a given SV model.   "
  },
  {
    "id": 7297,
    "package_name": "SimNPH",
    "title": "Simulate Non-Proportional Hazards",
    "description": "A toolkit for simulation studies concerning time-to-event endpoints\n    with non-proportional hazards. 'SimNPH' encompasses functions for simulating\n    time-to-event data in various scenarios, simulating different trial designs\n    like fixed-followup, event-driven, and group sequential designs. The package\n    provides functions to calculate the true values of common summary statistics\n    for the implemented scenarios and offers common analysis methods for\n    time-to-event data. Helper functions for running simulations with the\n    'SimDesign' package and for aggregating and presenting the results are also\n    included. Results of the conducted simulation study are available in the \n    paper: \"A Comparison of Statistical Methods for Time-To-Event Analyses in \n    Randomized Controlled Trials Under Non-Proportional Hazards\", \n    Klinglm\u00fcller et al. (2025) <doi:10.1002/sim.70019>.",
    "version": "0.5.8",
    "maintainer": "Tobias Fellinger <tobias.fellinger@ages.at>",
    "author": "Tobias Fellinger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9474-2731>),\n  Florian Klinglmueller [aut] (ORCID:\n    <https://orcid.org/0000-0002-7346-3669>)",
    "url": "https://simnph.github.io/SimNPH/,\nhttps://github.com/SimNPH/SimNPH/",
    "bug_reports": "https://github.com/SimNPH/SimNPH/issues/",
    "repository": "https://cran.r-project.org/package=SimNPH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimNPH Simulate Non-Proportional Hazards A toolkit for simulation studies concerning time-to-event endpoints\n    with non-proportional hazards. 'SimNPH' encompasses functions for simulating\n    time-to-event data in various scenarios, simulating different trial designs\n    like fixed-followup, event-driven, and group sequential designs. The package\n    provides functions to calculate the true values of common summary statistics\n    for the implemented scenarios and offers common analysis methods for\n    time-to-event data. Helper functions for running simulations with the\n    'SimDesign' package and for aggregating and presenting the results are also\n    included. Results of the conducted simulation study are available in the \n    paper: \"A Comparison of Statistical Methods for Time-To-Event Analyses in \n    Randomized Controlled Trials Under Non-Proportional Hazards\", \n    Klinglm\u00fcller et al. (2025) <doi:10.1002/sim.70019>.  "
  },
  {
    "id": 7315,
    "package_name": "SingleCellStat",
    "title": "A Toolkit for Statistical Analysis of Single-Cell Omics Data",
    "description": "A suite of statistical methods for analysis of single-cell omics data including linear model-based methods for differential abundance analysis for individual level single-cell RNA-seq data. For more details see Zhang, et al. (Submitted to Bioinformatics)<https://github.com/Lujun995/DiSC_Replication_Code>.",
    "version": "0.3.1",
    "maintainer": "Jun Chen <chen.jun2@mayo.edu>",
    "author": "Lujun Zhang [aut],\n  Jun Chen [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SingleCellStat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SingleCellStat A Toolkit for Statistical Analysis of Single-Cell Omics Data A suite of statistical methods for analysis of single-cell omics data including linear model-based methods for differential abundance analysis for individual level single-cell RNA-seq data. For more details see Zhang, et al. (Submitted to Bioinformatics)<https://github.com/Lujun995/DiSC_Replication_Code>.  "
  },
  {
    "id": 7372,
    "package_name": "SpaTopic",
    "title": "Topic Inference to Identify Tissue Architecture in Multiplexed\nImages",
    "description": "A novel spatial topic model to integrate both cell type and spatial information to identify the complex spatial tissue architecture on multiplexed tissue images without human intervention. The Package implements a collapsed Gibbs sampling algorithm for inference. 'SpaTopic' is scalable to large-scale image datasets without extracting neighborhood information for every single cell. For more details on the methodology, see <https://xiyupeng.github.io/SpaTopic/>.",
    "version": "1.2.0",
    "maintainer": "Xiyu Peng <pansypeng124@gmail.com>",
    "author": "Xiyu Peng [aut, cre] (ORCID: <https://orcid.org/0000-0003-4232-0910>)",
    "url": "https://github.com/xiyupeng/SpaTopic",
    "bug_reports": "https://github.com/xiyupeng/SpaTopic/issues",
    "repository": "https://cran.r-project.org/package=SpaTopic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpaTopic Topic Inference to Identify Tissue Architecture in Multiplexed\nImages A novel spatial topic model to integrate both cell type and spatial information to identify the complex spatial tissue architecture on multiplexed tissue images without human intervention. The Package implements a collapsed Gibbs sampling algorithm for inference. 'SpaTopic' is scalable to large-scale image datasets without extracting neighborhood information for every single cell. For more details on the methodology, see <https://xiyupeng.github.io/SpaTopic/>.  "
  },
  {
    "id": 7483,
    "package_name": "StratifiedMedicine",
    "title": "Stratified Medicine",
    "description": "A toolkit for stratified medicine, subgroup identification, and precision medicine.\n    Current tools include (1) filtering models (reduce covariate space), (2) patient-level estimate\n    models (counterfactual patient-level quantities, such as the conditional average treatment effect), \n    (3) subgroup identification models (find subsets of patients with similar treatment effects), \n    and (4) treatment effect estimation and inference (for the overall population and discovered \n    subgroups). These tools can be customized and are directly used in PRISM \n    (patient response identifiers for stratified medicine; Jemielita and Mehrotra 2019\n    <arXiv:1912.03337>. This package is in beta and will be continually updated.",
    "version": "1.0.5",
    "maintainer": "Thomas Jemielita <thomasjemielita@gmail.com>",
    "author": "Thomas Jemielita [aut, cre]",
    "url": "https://github.com/thomasjemielita/StratifiedMedicine",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=StratifiedMedicine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StratifiedMedicine Stratified Medicine A toolkit for stratified medicine, subgroup identification, and precision medicine.\n    Current tools include (1) filtering models (reduce covariate space), (2) patient-level estimate\n    models (counterfactual patient-level quantities, such as the conditional average treatment effect), \n    (3) subgroup identification models (find subsets of patients with similar treatment effects), \n    and (4) treatment effect estimation and inference (for the overall population and discovered \n    subgroups). These tools can be customized and are directly used in PRISM \n    (patient response identifiers for stratified medicine; Jemielita and Mehrotra 2019\n    <arXiv:1912.03337>. This package is in beta and will be continually updated.  "
  },
  {
    "id": 7590,
    "package_name": "TDAkit",
    "title": "Toolkit for Topological Data Analysis",
    "description": "Topological data analysis studies structure and shape of the data using topological features. We provide a variety of algorithms to learn with persistent homology of the data based on functional summaries for clustering, hypothesis testing, visualization, and others. We refer to Wasserman (2018) <doi:10.1146/annurev-statistics-031017-100045> for a statistical perspective on the topic. ",
    "version": "0.1.3",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "author": "Kisung You [aut, cre] (ORCID: <https://orcid.org/0000-0002-8584-459X>),\n  Byeongsu Yu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TDAkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TDAkit Toolkit for Topological Data Analysis Topological data analysis studies structure and shape of the data using topological features. We provide a variety of algorithms to learn with persistent homology of the data based on functional summaries for clustering, hypothesis testing, visualization, and others. We refer to Wasserman (2018) <doi:10.1146/annurev-statistics-031017-100045> for a statistical perspective on the topic.   "
  },
  {
    "id": 7616,
    "package_name": "TIGERr",
    "title": "Technical Variation Elimination with Ensemble Learning\nArchitecture",
    "description": "\n    The R implementation of TIGER. \n    TIGER integrates random forest algorithm into an innovative ensemble learning architecture. Benefiting from this advanced architecture, TIGER is resilient to outliers, free from model tuning and less likely to be affected by specific hyperparameters.\n    TIGER supports targeted and untargeted metabolomics data and is competent to perform both intra- and inter-batch technical variation removal. TIGER can also be used for cross-kit adjustment to ensure data obtained from different analytical assays can be effectively combined and compared.\n    Reference: Han S. et al. (2022) <doi:10.1093/bib/bbab535>.",
    "version": "1.0.0",
    "maintainer": "Siyu Han <siyu.han@helmholtz-muenchen.de>",
    "author": "Siyu Han [aut, cre], Jialing Huang [aut], Francesco Foppiano [aut], Cornelia Prehn [aut], Jerzy Adamski [aut], Karsten Suhre [aut], Ying Li [aut], Giuseppe Matullo [aut], Freimut Schliess [aut], Christian Gieger [aut], Annette Peters [aut], Rui Wang-Sattler [aut]",
    "url": "",
    "bug_reports": "https://github.com/HAN-Siyu/TIGER/issues",
    "repository": "https://cran.r-project.org/package=TIGERr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TIGERr Technical Variation Elimination with Ensemble Learning\nArchitecture \n    The R implementation of TIGER. \n    TIGER integrates random forest algorithm into an innovative ensemble learning architecture. Benefiting from this advanced architecture, TIGER is resilient to outliers, free from model tuning and less likely to be affected by specific hyperparameters.\n    TIGER supports targeted and untargeted metabolomics data and is competent to perform both intra- and inter-batch technical variation removal. TIGER can also be used for cross-kit adjustment to ensure data obtained from different analytical assays can be effectively combined and compared.\n    Reference: Han S. et al. (2022) <doi:10.1093/bib/bbab535>.  "
  },
  {
    "id": 7622,
    "package_name": "TLIC",
    "title": "The LIC for T Distribution Regression Analysis",
    "description": "This comprehensive toolkit for T-distributed regression is designated as \"TLIC\" (The LIC for T Distribution Regression Analysis) analysis. It is predicated on the assumption that the error term adheres to a T-distribution. The philosophy of the package is described in Guo G. (2020) <doi:10.1080/02664763.2022.2053949>. ",
    "version": "0.4",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "author": "Guangbao Guo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4115-6218>),\n  Guofu Jing [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TLIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TLIC The LIC for T Distribution Regression Analysis This comprehensive toolkit for T-distributed regression is designated as \"TLIC\" (The LIC for T Distribution Regression Analysis) analysis. It is predicated on the assumption that the error term adheres to a T-distribution. The philosophy of the package is described in Guo G. (2020) <doi:10.1080/02664763.2022.2053949>.   "
  },
  {
    "id": 7642,
    "package_name": "TPMplt",
    "title": "Tool-Kit for Dynamic Materials Model and Thermal Processing Maps",
    "description": "Provides a simple approach for constructing dynamic materials\n modeling suggested by Prasad and Gegel (1984) <doi:10.1007/BF02664902>. It\n can easily generate various processing-maps based on this model as well. The\n calculation result in this package contains full materials constants, information\n about power dissipation efficiency factor, and rheological properties, can\n be exported completely also, through which further analysis and customized\n plots will be applicable as well.",
    "version": "0.1.7",
    "maintainer": "Chen Zhang <chen.zhang_06sept@foxmail.com>",
    "author": "Chen Zhang [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0007-7689-5030>),\n  Huakang Bian [ctb],\n  Kenta Yamanaka [ths] (ORCID: <https://orcid.org/0000-0003-1675-4731>),\n  Akihiko Chiba [ths]",
    "url": "https://github.com/CubicZebra/TPMplt",
    "bug_reports": "https://github.com/CubicZebra/TPMplt/issues",
    "repository": "https://cran.r-project.org/package=TPMplt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TPMplt Tool-Kit for Dynamic Materials Model and Thermal Processing Maps Provides a simple approach for constructing dynamic materials\n modeling suggested by Prasad and Gegel (1984) <doi:10.1007/BF02664902>. It\n can easily generate various processing-maps based on this model as well. The\n calculation result in this package contains full materials constants, information\n about power dissipation efficiency factor, and rheological properties, can\n be exported completely also, through which further analysis and customized\n plots will be applicable as well.  "
  },
  {
    "id": 7677,
    "package_name": "TSSS",
    "title": "Time Series Analysis with State Space Model",
    "description": "Functions for statistical analysis, modeling and simulation of time\n series with state space model, based on the methodology in Kitagawa\n (2020, ISBN: 978-0-367-18733-0).",
    "version": "1.3.4-5",
    "maintainer": "Masami Saga <msaga@mtb.biglobe.ne.jp>",
    "author": "The Institute of Statistical Mathematics, based on the program by\n Genshiro Kitagawa",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSSS Time Series Analysis with State Space Model Functions for statistical analysis, modeling and simulation of time\n series with state space model, based on the methodology in Kitagawa\n (2020, ISBN: 978-0-367-18733-0).  "
  },
  {
    "id": 7682,
    "package_name": "TSdisaggregation",
    "title": "High-Dimensional Temporal Disaggregation",
    "description": "First - Generates (potentially high-dimensional) high-frequency and low-frequency series for simulation studies in temporal disaggregation; Second - a toolkit utilizing temporal disaggregation and benchmarking techniques with a low-dimensional matrix of indicator series previously proposed in Dagum and Cholette (2006, ISBN:978-0-387-35439-2) ; and Third - novel techniques proposed by Mosley, Gibberd and Eckley (2021) <arXiv:2108.05783> for disaggregating low-frequency series in the presence of high-dimensional indicator matrices.",
    "version": "2.0.0",
    "maintainer": "Luke Mosley <l.mosley@lancaster.ac.uk>",
    "author": "Luke Mosley [aut, cre],\n  Kaveh S. Nobari [aut] (ORCID: <https://orcid.org/0000-0002-4053-0781>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSdisaggregation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSdisaggregation High-Dimensional Temporal Disaggregation First - Generates (potentially high-dimensional) high-frequency and low-frequency series for simulation studies in temporal disaggregation; Second - a toolkit utilizing temporal disaggregation and benchmarking techniques with a low-dimensional matrix of indicator series previously proposed in Dagum and Cholette (2006, ISBN:978-0-387-35439-2) ; and Third - novel techniques proposed by Mosley, Gibberd and Eckley (2021) <arXiv:2108.05783> for disaggregating low-frequency series in the presence of high-dimensional indicator matrices.  "
  },
  {
    "id": 7777,
    "package_name": "Tivy",
    "title": "Toolkit for Investigation and Visualization of Young Anchovies",
    "description": "Specialized toolkit for processing biological and fisheries data from Peru's anchovy (Engraulis ringens) fishery. Provides functions to analyze fishing logbooks, calculate biological indicators (length-weight relationships, juvenile percentages), generate spatial fishing indicators, and visualize regulatory measures from Peru's Ministry of Production. Features automated data processing from multiple file formats, coordinate validation, spatial analysis of fishing zones, and tools for analyzing fishing closure announcements and regulatory compliance. Includes built-in datasets of Peruvian coastal coordinates and parallel lines for analyzing fishing activities within regulatory zones.",
    "version": "0.1.1",
    "maintainer": "Hans Ttito <kvttitos@gmail.com>",
    "author": "Hans Ttito [aut, cre] (ORCID: <https://orcid.org/0000-0003-3732-9419>)",
    "url": "https://github.com/HansTtito/Tivy",
    "bug_reports": "https://github.com/HansTtito/Tivy/issues",
    "repository": "https://cran.r-project.org/package=Tivy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Tivy Toolkit for Investigation and Visualization of Young Anchovies Specialized toolkit for processing biological and fisheries data from Peru's anchovy (Engraulis ringens) fishery. Provides functions to analyze fishing logbooks, calculate biological indicators (length-weight relationships, juvenile percentages), generate spatial fishing indicators, and visualize regulatory measures from Peru's Ministry of Production. Features automated data processing from multiple file formats, coordinate validation, spatial analysis of fishing zones, and tools for analyzing fishing closure announcements and regulatory compliance. Includes built-in datasets of Peruvian coastal coordinates and parallel lines for analyzing fishing activities within regulatory zones.  "
  },
  {
    "id": 7856,
    "package_name": "UAHDataScienceSF",
    "title": "Interactive Statistical Learning Functions",
    "description": "An educational toolkit for learning statistical concepts through \n    interactive exploration. Provides functions for basic statistics (mean, \n    variance, etc.) and probability distributions with step-by-step explanations \n    and interactive learning modes. Each function can be used for simple \n    calculations, detailed learning with explanations, or interactive practice \n    with feedback.",
    "version": "1.0.0",
    "maintainer": "Andriy Protsak Protsak <andriy.protsak@edu.uah.es>",
    "author": "Carlos Javier Hellin Asensio [aut],\n  Jose Manuel Gomez Caceres [aut],\n  Dennis Monheimius [aut],\n  Eduardo Benito [aut],\n  Juan Jose Cuadrado [aut],\n  Andriy Protsak Protsak [aut, cre],\n  Universidad de Alcala de Henares [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UAHDataScienceSF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UAHDataScienceSF Interactive Statistical Learning Functions An educational toolkit for learning statistical concepts through \n    interactive exploration. Provides functions for basic statistics (mean, \n    variance, etc.) and probability distributions with step-by-step explanations \n    and interactive learning modes. Each function can be used for simple \n    calculations, detailed learning with explanations, or interactive practice \n    with feedback.  "
  },
  {
    "id": 7857,
    "package_name": "UAHDataScienceUC",
    "title": "Learn Clustering Techniques Through Examples and Code",
    "description": "A comprehensive educational package combining clustering algorithms with \n    detailed step-by-step explanations. Provides implementations of both traditional \n    (hierarchical, k-means) and modern (Density-Based Spatial Clustering of Applications with Noise (DBSCAN), \n    Gaussian Mixture Models (GMM), genetic k-means) clustering methods \n    as described in Ezugwu et. al., (2022) <doi:10.1016/j.engappai.2022.104743>. \n    Includes educational datasets highlighting different clustering challenges, based on \n    'scikit-learn' examples (Pedregosa et al., 2011) \n    <https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html>. Features detailed \n    algorithm explanations, visualizations, and weighted distance calculations for \n    enhanced learning.",
    "version": "1.0.1",
    "maintainer": "Andriy Protsak Protsak <andriy.protsak@edu.uah.es>",
    "author": "Eduardo Ruiz Sabajanes [aut],\n  Roberto Alcantara [aut],\n  Juan Jose Cuadrado Gallego [aut] (ORCID:\n    <https://orcid.org/0000-0001-8178-5556>),\n  Andriy Protsak Protsak [aut, cre],\n  Universidad de Alcala [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UAHDataScienceUC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UAHDataScienceUC Learn Clustering Techniques Through Examples and Code A comprehensive educational package combining clustering algorithms with \n    detailed step-by-step explanations. Provides implementations of both traditional \n    (hierarchical, k-means) and modern (Density-Based Spatial Clustering of Applications with Noise (DBSCAN), \n    Gaussian Mixture Models (GMM), genetic k-means) clustering methods \n    as described in Ezugwu et. al., (2022) <doi:10.1016/j.engappai.2022.104743>. \n    Includes educational datasets highlighting different clustering challenges, based on \n    'scikit-learn' examples (Pedregosa et al., 2011) \n    <https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html>. Features detailed \n    algorithm explanations, visualizations, and weighted distance calculations for \n    enhanced learning.  "
  },
  {
    "id": 8055,
    "package_name": "WebGestaltR",
    "title": "Gene Set Analysis Toolkit WebGestaltR",
    "description": "The web version WebGestalt <https://www.webgestalt.org> supports 12 organisms, 354 gene identifiers and 321,251 function categories. Users can upload the data and functional categories with their own gene identifiers. In addition to the Over-Representation Analysis, WebGestalt also supports Gene Set Enrichment Analysis and Network Topology Analysis. The user-friendly output report allows interactive and efficient exploration of enrichment results. The WebGestaltR package not only supports all above functions but also can be integrated into other pipeline or simultaneously analyze multiple gene lists.",
    "version": "0.4.6",
    "maintainer": "Yuxing Liao <yuxingliao@gmail.com>",
    "author": "Jing Wang [aut],\n  Yuxing Liao [aut, cre],\n  Eric Jaehnig [ctb],\n  Zhiao Shi [ctb],\n  Quanhu Sheng [ctb]",
    "url": "https://github.com/bzhanglab/WebGestaltR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WebGestaltR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WebGestaltR Gene Set Analysis Toolkit WebGestaltR The web version WebGestalt <https://www.webgestalt.org> supports 12 organisms, 354 gene identifiers and 321,251 function categories. Users can upload the data and functional categories with their own gene identifiers. In addition to the Over-Representation Analysis, WebGestalt also supports Gene Set Enrichment Analysis and Network Topology Analysis. The user-friendly output report allows interactive and efficient exploration of enrichment results. The WebGestaltR package not only supports all above functions but also can be integrated into other pipeline or simultaneously analyze multiple gene lists.  "
  },
  {
    "id": 8089,
    "package_name": "WormTensor",
    "title": "A Clustering Method for Time-Series Whole-Brain Activity Data of\n'C. elegans'",
    "description": "A toolkit to detect clusters from distance matrices. \n    The distance matrices are assumed to be calculated between the cells of \n    multiple animals ('Caenorhabditis elegans') from input time-series matrices. \n    Some functions for generating distance matrices, performing clustering, \n    evaluating the clustering, and visualizing the results of clustering and \n    evaluation are available. We're also providing the download function to \n    retrieve the calculated distance matrices from \n    'figshare' <https://figshare.com>.",
    "version": "0.1.2",
    "maintainer": "Kentaro Yamamoto <yamaken37.the.answer@gmail.com>",
    "author": "Kentaro Yamamoto [aut, cre],\n  Koki Tsuyuzaki [aut],\n  Itoshi Nikaido [aut]",
    "url": "https://github.com/rikenbit/WormTensor",
    "bug_reports": "https://github.com/rikenbit/WormTensor/issues",
    "repository": "https://cran.r-project.org/package=WormTensor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WormTensor A Clustering Method for Time-Series Whole-Brain Activity Data of\n'C. elegans' A toolkit to detect clusters from distance matrices. \n    The distance matrices are assumed to be calculated between the cells of \n    multiple animals ('Caenorhabditis elegans') from input time-series matrices. \n    Some functions for generating distance matrices, performing clustering, \n    evaluating the clustering, and visualizing the results of clustering and \n    evaluation are available. We're also providing the download function to \n    retrieve the calculated distance matrices from \n    'figshare' <https://figshare.com>.  "
  },
  {
    "id": 8166,
    "package_name": "abdiv",
    "title": "Alpha and Beta Diversity Measures",
    "description": "A collection of measures for measuring ecological diversity.\n  Ecological diversity comes in two flavors: alpha diversity measures the\n  diversity within a single site or sample, and beta diversity measures the\n  diversity across two sites or samples. This package overlaps considerably\n  with other R packages such as 'vegan', 'gUniFrac', 'betapart', and 'fossil'.\n  We also include a wide range of functions that are implemented in software\n  outside the R ecosystem, such as 'scipy', 'Mothur', and 'scikit-bio'.  The\n  implementations here are designed to be basic and clear to the reader.",
    "version": "0.2.0",
    "maintainer": "Kyle Bittinger <kylebittinger@gmail.com>",
    "author": "Kyle Bittinger [aut, cre]",
    "url": "https://github.com/kylebittinger/abdiv",
    "bug_reports": "https://github.com/kylebittinger/abdiv/issues",
    "repository": "https://cran.r-project.org/package=abdiv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "abdiv Alpha and Beta Diversity Measures A collection of measures for measuring ecological diversity.\n  Ecological diversity comes in two flavors: alpha diversity measures the\n  diversity within a single site or sample, and beta diversity measures the\n  diversity across two sites or samples. This package overlaps considerably\n  with other R packages such as 'vegan', 'gUniFrac', 'betapart', and 'fossil'.\n  We also include a wide range of functions that are implemented in software\n  outside the R ecosystem, such as 'scipy', 'Mothur', and 'scikit-bio'.  The\n  implementations here are designed to be basic and clear to the reader.  "
  },
  {
    "id": 8169,
    "package_name": "abess",
    "title": "Fast Best Subset Selection",
    "description": "Extremely efficient toolkit for solving the best subset selection problem <https://www.jmlr.org/papers/v23/21-1060.html>. This package is its R interface. The package implements and generalizes algorithms designed in <doi:10.1073/pnas.2014241117> that exploits a novel sequencing-and-splicing technique to guarantee exact support recovery and globally optimal solution in polynomial times for linear model. It also supports best subset selection for logistic regression, Poisson regression, Cox proportional hazard model, Gamma regression, multiple-response regression, multinomial logistic regression, ordinal regression, Ising model reconstruction <doi:10.1080/01621459.2025.2571245>, (sequential) principal component analysis, and robust principal component analysis. The other valuable features such as the best subset of group selection <doi:10.1287/ijoc.2022.1241> and sure independence screening <doi:10.1111/j.1467-9868.2008.00674.x> are also provided.  ",
    "version": "0.4.11",
    "maintainer": "Jin Zhu <zhuj1jqx@gmail.com>",
    "author": "Jin Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0001-8550-5822>),\n  Zezhi Wang [aut],\n  Liyuan Hu [aut],\n  Junhao Huang [aut],\n  Kangkang Jiang [aut],\n  Yanhang Zhang [aut],\n  Borui Tang [aut],\n  Shiyun Lin [aut],\n  Junxian Zhu [aut],\n  Canhong Wen [aut],\n  Heping Zhang [aut] (ORCID: <https://orcid.org/0000-0002-0688-4076>),\n  Xueqin Wang [aut] (ORCID: <https://orcid.org/0000-0001-5205-9950>),\n  spectra contributors [cph] (Spectra implementation)",
    "url": "https://github.com/abess-team/abess,\nhttps://abess-team.github.io/abess/,\nhttps://abess.readthedocs.io",
    "bug_reports": "https://github.com/abess-team/abess/issues",
    "repository": "https://cran.r-project.org/package=abess",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "abess Fast Best Subset Selection Extremely efficient toolkit for solving the best subset selection problem <https://www.jmlr.org/papers/v23/21-1060.html>. This package is its R interface. The package implements and generalizes algorithms designed in <doi:10.1073/pnas.2014241117> that exploits a novel sequencing-and-splicing technique to guarantee exact support recovery and globally optimal solution in polynomial times for linear model. It also supports best subset selection for logistic regression, Poisson regression, Cox proportional hazard model, Gamma regression, multiple-response regression, multinomial logistic regression, ordinal regression, Ising model reconstruction <doi:10.1080/01621459.2025.2571245>, (sequential) principal component analysis, and robust principal component analysis. The other valuable features such as the best subset of group selection <doi:10.1287/ijoc.2022.1241> and sure independence screening <doi:10.1111/j.1467-9868.2008.00674.x> are also provided.    "
  },
  {
    "id": 8221,
    "package_name": "act",
    "title": "Aligned Corpus Toolkit",
    "description": "The Aligned Corpus Toolkit (act) is designed for linguists that work with time aligned transcription data. It offers functions to import and export various annotation file formats ('ELAN' .eaf, 'EXMARaLDA .exb and 'Praat' .TextGrid files), create print transcripts in the style of conversation analysis, search transcripts (span searches across multiple annotations, search in normalized annotations, make concordances etc.), export and re-import search results (.csv and 'Excel' .xlsx format), create cuts for the search results (print transcripts, audio/video cuts using 'FFmpeg' and video sub titles in 'Subrib title' .srt format), modify the data in a corpus (search/replace, delete, filter etc.), interact with 'Praat' using 'Praat'-scripts, and exchange data with the 'rPraat' package. The package is itself written in R and may be expanded by other users.",
    "version": "1.3.1",
    "maintainer": "Oliver Ehmer <oliver.ehmer@uos.de>",
    "author": "Oliver Ehmer [aut, cre]",
    "url": "http://www.oliverehmer.de, https://github.com/oliverehmer/act",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=act",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "act Aligned Corpus Toolkit The Aligned Corpus Toolkit (act) is designed for linguists that work with time aligned transcription data. It offers functions to import and export various annotation file formats ('ELAN' .eaf, 'EXMARaLDA .exb and 'Praat' .TextGrid files), create print transcripts in the style of conversation analysis, search transcripts (span searches across multiple annotations, search in normalized annotations, make concordances etc.), export and re-import search results (.csv and 'Excel' .xlsx format), create cuts for the search results (print transcripts, audio/video cuts using 'FFmpeg' and video sub titles in 'Subrib title' .srt format), modify the data in a corpus (search/replace, delete, filter etc.), interact with 'Praat' using 'Praat'-scripts, and exchange data with the 'rPraat' package. The package is itself written in R and may be expanded by other users.  "
  },
  {
    "id": 8394,
    "package_name": "aion",
    "title": "Archaeological Time Series",
    "description": "A toolkit for archaeological time series and time intervals.\n    This package provides a system of classes and methods to represent and\n    work with archaeological time series and time intervals. Dates are\n    represented as \"rata die\" and can be converted to (virtually) any\n    calendar defined by Reingold and Dershowitz (2018)\n    <doi:10.1017/9781107415058>. This packages offers a simple API that\n    can be used by other specialized packages.",
    "version": "1.6.0",
    "maintainer": "Nicolas Frerebeau <nicolas.frerebeau@u-bordeaux-montaigne.fr>",
    "author": "Nicolas Frerebeau [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5759-4944>),\n  Joe Roe [aut] (ORCID: <https://orcid.org/0000-0002-1011-1244>),\n  Brice Lebrun [art] (ORCID: <https://orcid.org/0000-0001-7503-8685>,\n    Logo designer),\n  Universit\u00e9 Bordeaux Montaigne [fnd] (ROR: <https://ror.org/03pbgwk21>),\n  CNRS [fnd] (ROR: <https://ror.org/02feahw73>)",
    "url": "https://codeberg.org/tesselle/aion,\nhttps://tesselle.r-universe.dev/aion,\nhttps://packages.tesselle.org/aion/",
    "bug_reports": "https://codeberg.org/tesselle/aion/issues",
    "repository": "https://cran.r-project.org/package=aion",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aion Archaeological Time Series A toolkit for archaeological time series and time intervals.\n    This package provides a system of classes and methods to represent and\n    work with archaeological time series and time intervals. Dates are\n    represented as \"rata die\" and can be converted to (virtually) any\n    calendar defined by Reingold and Dershowitz (2018)\n    <doi:10.1017/9781107415058>. This packages offers a simple API that\n    can be used by other specialized packages.  "
  },
  {
    "id": 8463,
    "package_name": "amadeus",
    "title": "Accessing and Analyzing Large-Scale Environmental Data",
    "description": "Functions are designed to facilitate access to and utility with large scale, publicly available environmental data in R. The package contains functions for downloading raw data files from web URLs (download_data()), processing the raw data files into clean spatial objects (process_covariates()), and extracting values from the spatial data objects at point and polygon locations (calculate_covariates()). These functions call a series of source-specific functions which are tailored to each data sources/datasets particular URL structure, data format, and spatial/temporal resolution. The functions are tested, versioned, and open source and open access. For sum_edc() method details, see Messier, Akita, and Serre (2012) <doi:10.1021/es203152a>.",
    "version": "1.2.4.9",
    "maintainer": "Kyle Messier <kyle.messier@nih.gov>",
    "author": "Mitchell Manware [aut, ctb] (ORCID:\n    <https://orcid.org/0009-0003-6440-6106>),\n  Insang Song [aut, ctb] (ORCID: <https://orcid.org/0000-0001-8732-3256>),\n  Eva Marques [aut, ctb] (ORCID: <https://orcid.org/0000-0001-9817-6546>),\n  Mariana Alifa Kassien [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-2295-406X>),\n  Elizabeth Scholl [ctb] (ORCID: <https://orcid.org/0000-0003-2727-1954>),\n  Kyle Messier [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9508-9623>),\n  Spatiotemporal Exposures and Toxicology Group [cph]",
    "url": "https://niehs.github.io/amadeus/",
    "bug_reports": "https://github.com/NIEHS/amadeus/issues",
    "repository": "https://cran.r-project.org/package=amadeus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amadeus Accessing and Analyzing Large-Scale Environmental Data Functions are designed to facilitate access to and utility with large scale, publicly available environmental data in R. The package contains functions for downloading raw data files from web URLs (download_data()), processing the raw data files into clean spatial objects (process_covariates()), and extracting values from the spatial data objects at point and polygon locations (calculate_covariates()). These functions call a series of source-specific functions which are tailored to each data sources/datasets particular URL structure, data format, and spatial/temporal resolution. The functions are tested, versioned, and open source and open access. For sum_edc() method details, see Messier, Akita, and Serre (2012) <doi:10.1021/es203152a>.  "
  },
  {
    "id": 8484,
    "package_name": "ampir",
    "title": "Predict Antimicrobial Peptides",
    "description": "A toolkit to predict antimicrobial peptides from protein sequences on a genome-wide scale.\n    It incorporates two support vector machine models (\"precursor\" and \"mature\") trained on publicly available antimicrobial peptide data using calculated\n    physico-chemical and compositional sequence properties described in Meher et al. (2017) <doi:10.1038/srep42362>.\n    In order to support genome-wide analyses, these models are designed to accept any type of protein as input\n    and calculation of compositional properties has been optimised for high-throughput use. For best results it is important to select the model that accurately \n    represents your sequence type: for full length proteins, it is recommended to use the default \"precursor\" model. The alternative, \"mature\", model is best suited\n    for mature peptide sequences that represent the final antimicrobial peptide sequence after post-translational processing. For details see Fingerhut et al. (2020) <doi:10.1093/bioinformatics/btaa653>.\n    The 'ampir' package is also available via a Shiny based GUI at <https://ampir.marine-omics.net/>.",
    "version": "1.1.0",
    "maintainer": "Legana Fingerhut <legana.fingerhut@my.jcu.edu.au>",
    "author": "Legana Fingerhut [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2482-5336>),\n  Ira Cooke [aut] (ORCID: <https://orcid.org/0000-0001-6520-1397>),\n  Jinlong Zhang [ctb] (R/read_faa.R),\n  Nan Xiao [ctb] (R/calc_pseudo_comp.R)",
    "url": "https://github.com/Legana/ampir",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ampir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ampir Predict Antimicrobial Peptides A toolkit to predict antimicrobial peptides from protein sequences on a genome-wide scale.\n    It incorporates two support vector machine models (\"precursor\" and \"mature\") trained on publicly available antimicrobial peptide data using calculated\n    physico-chemical and compositional sequence properties described in Meher et al. (2017) <doi:10.1038/srep42362>.\n    In order to support genome-wide analyses, these models are designed to accept any type of protein as input\n    and calculation of compositional properties has been optimised for high-throughput use. For best results it is important to select the model that accurately \n    represents your sequence type: for full length proteins, it is recommended to use the default \"precursor\" model. The alternative, \"mature\", model is best suited\n    for mature peptide sequences that represent the final antimicrobial peptide sequence after post-translational processing. For details see Fingerhut et al. (2020) <doi:10.1093/bioinformatics/btaa653>.\n    The 'ampir' package is also available via a Shiny based GUI at <https://ampir.marine-omics.net/>.  "
  },
  {
    "id": 8527,
    "package_name": "anticlust",
    "title": "Subset Partitioning via Anticlustering",
    "description": "The method of anticlustering partitions a pool of elements into groups (i.e., anticlusters) with the goal of maximizing between-group similarity or within-group heterogeneity. The anticlustering approach thereby reverses the logic of cluster analysis that strives for high within-group homogeneity and clear separation between groups.  Computationally, anticlustering is accomplished by maximizing instead of minimizing a clustering objective function, such as the intra-cluster variance (used in k-means clustering) or the sum of pairwise distances within clusters. The main function anticlustering() gives access to optimal and heuristic anticlustering methods described in Papenberg and Klau (2021; <doi:10.1037/met0000301>), Brusco et al. (2020; <doi:10.1111/bmsp.12186>), Papenberg (2024;  <doi:10.1111/bmsp.12315>), Papenberg, Wang, et al. (2025; <doi:10.1016/j.crmeth.2025.101137>), Papenberg, Breuer, et al. (2025; <doi:10.1017/psy.2025.10052>), and Yang et al. (2022; <doi:10.1016/j.ejor.2022.02.003>). The optimal algorithms require that an integer linear programming solver is installed. This package will install 'lpSolve' (<https://cran.r-project.org/package=lpSolve>) as a default solver, but it is also possible to use the package 'Rglpk' (<https://cran.r-project.org/package=Rglpk>), which requires the GNU linear programming kit (<https://www.gnu.org/software/glpk/glpk.html>), the package 'Rsymphony' (<https://cran.r-project.org/package=Rsymphony>), which requires the SYMPHONY ILP solver (<https://github.com/coin-or/SYMPHONY>), or the commercial solver Gurobi, which provides its own R package that is not available via CRAN (<https://www.gurobi.com/downloads/>). 'Rglpk', 'Rsymphony', 'gurobi' and their system dependencies have to be manually installed by the user because they are only suggested dependencies. Full access to the bicriterion anticlustering method proposed by Brusco et al. (2020) is given via the function bicriterion_anticlustering(), while kplus_anticlustering() implements the full functionality of the k-plus anticlustering approach proposed by Papenberg (2024). Some other functions are available to solve classical clustering problems. The function balanced_clustering() applies a cluster analysis under size constraints, i.e., creates equal-sized clusters. The function matching() can be used for (unrestricted, bipartite, or K-partite) matching. The function wce() can be used optimally solve the (weighted) cluster editing problem, also known as correlation clustering, clique partitioning problem or transitivity clustering.",
    "version": "0.8.13",
    "maintainer": "Martin Papenberg <martin.papenberg@hhu.de>",
    "author": "Martin Papenberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9900-4268>),\n  Meik Michalke [ctb] (centroid based clustering algorithm),\n  Gunnar W. Klau [ths],\n  Juliane V. Nagel [ctb] (package logo),\n  Martin Breuer [ctb] (Bicriterion algorithm by Brusco et al.),\n  Marie L. Schaper [ctb] (Example data set),\n  Max Diekhoff [ctb] (Optimal maximum dispersion algorithm),\n  Hannah Hengelbrock [ctb] (TPSDP heuristic by Yang et al.)",
    "url": "https://github.com/m-Py/anticlust,\nhttps://m-py.github.io/anticlust/",
    "bug_reports": "https://github.com/m-Py/anticlust/issues",
    "repository": "https://cran.r-project.org/package=anticlust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "anticlust Subset Partitioning via Anticlustering The method of anticlustering partitions a pool of elements into groups (i.e., anticlusters) with the goal of maximizing between-group similarity or within-group heterogeneity. The anticlustering approach thereby reverses the logic of cluster analysis that strives for high within-group homogeneity and clear separation between groups.  Computationally, anticlustering is accomplished by maximizing instead of minimizing a clustering objective function, such as the intra-cluster variance (used in k-means clustering) or the sum of pairwise distances within clusters. The main function anticlustering() gives access to optimal and heuristic anticlustering methods described in Papenberg and Klau (2021; <doi:10.1037/met0000301>), Brusco et al. (2020; <doi:10.1111/bmsp.12186>), Papenberg (2024;  <doi:10.1111/bmsp.12315>), Papenberg, Wang, et al. (2025; <doi:10.1016/j.crmeth.2025.101137>), Papenberg, Breuer, et al. (2025; <doi:10.1017/psy.2025.10052>), and Yang et al. (2022; <doi:10.1016/j.ejor.2022.02.003>). The optimal algorithms require that an integer linear programming solver is installed. This package will install 'lpSolve' (<https://cran.r-project.org/package=lpSolve>) as a default solver, but it is also possible to use the package 'Rglpk' (<https://cran.r-project.org/package=Rglpk>), which requires the GNU linear programming kit (<https://www.gnu.org/software/glpk/glpk.html>), the package 'Rsymphony' (<https://cran.r-project.org/package=Rsymphony>), which requires the SYMPHONY ILP solver (<https://github.com/coin-or/SYMPHONY>), or the commercial solver Gurobi, which provides its own R package that is not available via CRAN (<https://www.gurobi.com/downloads/>). 'Rglpk', 'Rsymphony', 'gurobi' and their system dependencies have to be manually installed by the user because they are only suggested dependencies. Full access to the bicriterion anticlustering method proposed by Brusco et al. (2020) is given via the function bicriterion_anticlustering(), while kplus_anticlustering() implements the full functionality of the k-plus anticlustering approach proposed by Papenberg (2024). Some other functions are available to solve classical clustering problems. The function balanced_clustering() applies a cluster analysis under size constraints, i.e., creates equal-sized clusters. The function matching() can be used for (unrestricted, bipartite, or K-partite) matching. The function wce() can be used optimally solve the (weighted) cluster editing problem, also known as correlation clustering, clique partitioning problem or transitivity clustering.  "
  },
  {
    "id": 8551,
    "package_name": "apex",
    "title": "Phylogenetic Methods for Multiple Gene Data",
    "description": "Toolkit for the analysis of multiple gene data (Jombart et al. 2017) <doi:10.1111/1755-0998.12567>. \n    'apex' implements the new S4 classes 'multidna', 'multiphyDat' and associated methods to handle aligned DNA sequences from multiple genes.",
    "version": "1.0.7",
    "maintainer": "Klaus Schliep <klaus.schliep@gmail.com>",
    "author": "Klaus Schliep [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2941-0161>),\n  Thibaut Jombart [aut],\n  Zhian Namir Kamvar [aut],\n  Eric Archer [aut],\n  Rebecca Harris [aut]",
    "url": "https://github.com/thibautjombart/apex",
    "bug_reports": "https://github.com/thibautjombart/apex/issues",
    "repository": "https://cran.r-project.org/package=apex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "apex Phylogenetic Methods for Multiple Gene Data Toolkit for the analysis of multiple gene data (Jombart et al. 2017) <doi:10.1111/1755-0998.12567>. \n    'apex' implements the new S4 classes 'multidna', 'multiphyDat' and associated methods to handle aligned DNA sequences from multiple genes.  "
  },
  {
    "id": 8684,
    "package_name": "assertable",
    "title": "Verbose Assertions for Tabular Data (Data.frames and\nData.tables)",
    "description": "Simple, flexible, assertions on data.frame or data.table objects with verbose output for vetting. While other assertion packages apply towards more general use-cases, assertable is tailored towards tabular data. It includes functions to check variable names and values, whether the dataset contains all combinations of a given set of unique identifiers, and whether it is a certain length. In addition, assertable includes utility functions to check the existence of target files and to efficiently import multiple tabular data files into one data.table.",
    "version": "0.2.8",
    "maintainer": "Grant Nguyen <grant.nguyen@gmail.com>",
    "author": "Grant Nguyen [aut, cre],\n  Max Czapanskiy [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=assertable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "assertable Verbose Assertions for Tabular Data (Data.frames and\nData.tables) Simple, flexible, assertions on data.frame or data.table objects with verbose output for vetting. While other assertion packages apply towards more general use-cases, assertable is tailored towards tabular data. It includes functions to check variable names and values, whether the dataset contains all combinations of a given set of unique identifiers, and whether it is a certain length. In addition, assertable includes utility functions to check the existence of target files and to efficiently import multiple tabular data files into one data.table.  "
  },
  {
    "id": 8775,
    "package_name": "avidaR",
    "title": "A Computational Biologist\u2019s Toolkit To Get Data From 'avidaDB'",
    "description": "Easy-to-use tools for performing complex queries on 'avidaDB', a\n  semantic database that stores genomic and transcriptomic data of\n  self-replicating computer programs (known as digital organisms) that mutate\n  and evolve within a user-defined computational environment.",
    "version": "1.2.1",
    "maintainer": "Ra\u00fal Ortega <raul.ortega@ebd.csic.es>",
    "author": "Miguel A. Fortuna [aut] (ORCID:\n    <https://orcid.org/0000-0002-8374-1941>),\n  Ra\u00fal Ortega [cre] (ORCID: <https://orcid.org/0000-0002-1306-5378>)",
    "url": "https://gitlab.com/fortunalab/avidaR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=avidaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "avidaR A Computational Biologist\u2019s Toolkit To Get Data From 'avidaDB' Easy-to-use tools for performing complex queries on 'avidaDB', a\n  semantic database that stores genomic and transcriptomic data of\n  self-replicating computer programs (known as digital organisms) that mutate\n  and evolve within a user-defined computational environment.  "
  },
  {
    "id": 8803,
    "package_name": "azuremlsdk",
    "title": "Interface to the 'Azure Machine Learning' 'SDK'",
    "description": "Interface to the 'Azure Machine Learning' Software Development Kit\n    ('SDK'). Data scientists can use the 'SDK' to train, deploy, automate, and\n    manage machine learning models on the 'Azure Machine Learning' service. To\n    learn more about 'Azure Machine Learning' visit the website:\n    <https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml>.",
    "version": "1.10.0",
    "maintainer": "Diondra Peck <Diondra.Peck@microsoft.com>",
    "author": "Diondra Peck [cre, aut],\n  Minna Xiao [aut],\n  AzureML R SDK Team [ctb],\n  Microsoft [cph, fnd],\n  Google Inc. [cph] (Examples and Tutorials),\n  The TensorFlow Authors [cph] (Examples and Tutorials),\n  RStudio Inc. [cph] (Examples and Tutorials)",
    "url": "https://github.com/azure/azureml-sdk-for-r",
    "bug_reports": "https://github.com/azure/azureml-sdk-for-r/issues",
    "repository": "https://cran.r-project.org/package=azuremlsdk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "azuremlsdk Interface to the 'Azure Machine Learning' 'SDK' Interface to the 'Azure Machine Learning' Software Development Kit\n    ('SDK'). Data scientists can use the 'SDK' to train, deploy, automate, and\n    manage machine learning models on the 'Azure Machine Learning' service. To\n    learn more about 'Azure Machine Learning' visit the website:\n    <https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml>.  "
  },
  {
    "id": 8849,
    "package_name": "banditpam",
    "title": "Almost Linear-Time k-Medoids Clustering",
    "description": "Interface to a high-performance implementation of k-medoids clustering described in Tiwari, Zhang, Mayclin, Thrun, Piech and Shomorony (2020) \"BanditPAM: Almost Linear Time k-medoids Clustering via Multi-Armed Bandits\" <https://proceedings.neurips.cc/paper/2020/file/73b817090081cef1bca77232f4532c5d-Paper.pdf>.",
    "version": "1.0-2",
    "maintainer": "Balasubramanian Narasimhan <naras@stanford.edu>",
    "author": "Balasubramanian Narasimhan [aut, cre],\n  Mo Tiwari [aut] (https://motiwari.com)",
    "url": "",
    "bug_reports": "https://github.com/motiwari/BanditPAM/issues",
    "repository": "https://cran.r-project.org/package=banditpam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "banditpam Almost Linear-Time k-Medoids Clustering Interface to a high-performance implementation of k-medoids clustering described in Tiwari, Zhang, Mayclin, Thrun, Piech and Shomorony (2020) \"BanditPAM: Almost Linear Time k-medoids Clustering via Multi-Armed Bandits\" <https://proceedings.neurips.cc/paper/2020/file/73b817090081cef1bca77232f4532c5d-Paper.pdf>.  "
  },
  {
    "id": 8880,
    "package_name": "basemodels",
    "title": "Baseline Models for Classification and Regression",
    "description": "Providing equivalent functions for the dummy\n    classifier and regressor used in 'Python' 'scikit-learn' library. Our goal\n    is to allow R users to easily identify baseline performance for their\n    classification and regression problems. Our baseline models use no\n    predictors, and are useful in cases of class imbalance, multiclass\n    classification, and when users want to quickly identify how much\n    improvement their statistical and machine learning models are over several\n    baseline models. We use a \"better\" default (proportional guessing) for\n    the dummy classifier than the 'Python' implementation (\"prior\", which is\n    the most frequent class in the training set). The functions in the\n    package can be used on their own, or introduce methods named\n    'dummy_regressor' or 'dummy_classifier' that can be used within the\n    caret package pipeline.",
    "version": "1.1.0",
    "maintainer": "Ying-Ju Chen <ychen4@udayton.edu>",
    "author": "Ying-Ju Chen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6444-6859>),\n  Fadel M. Megahed [aut] (ORCID: <https://orcid.org/0000-0003-2194-5110>),\n  L. Allison Jones-Farmer [aut] (ORCID:\n    <https://orcid.org/0000-0002-1529-1133>),\n  Steven E. Rigdon [aut] (ORCID: <https://orcid.org/0000-0001-7668-0899>)",
    "url": "https://github.com/Ying-Ju/basemodels",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=basemodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "basemodels Baseline Models for Classification and Regression Providing equivalent functions for the dummy\n    classifier and regressor used in 'Python' 'scikit-learn' library. Our goal\n    is to allow R users to easily identify baseline performance for their\n    classification and regression problems. Our baseline models use no\n    predictors, and are useful in cases of class imbalance, multiclass\n    classification, and when users want to quickly identify how much\n    improvement their statistical and machine learning models are over several\n    baseline models. We use a \"better\" default (proportional guessing) for\n    the dummy classifier than the 'Python' implementation (\"prior\", which is\n    the most frequent class in the training set). The functions in the\n    package can be used on their own, or introduce methods named\n    'dummy_regressor' or 'dummy_classifier' that can be used within the\n    caret package pipeline.  "
  },
  {
    "id": 8973,
    "package_name": "bbotk",
    "title": "Black-Box Optimization Toolkit",
    "description": "Features highly configurable search spaces via the 'paradox'\n    package and optimizes every user-defined objective function. The\n    package includes several optimization algorithms e.g. Random Search,\n    Iterated Racing, Bayesian Optimization (in 'mlr3mbo') and Hyperband\n    (in 'mlr3hyperband'). bbotk is the base package of 'mlr3tuning',\n    'mlr3fselect' and 'miesmuschel'.",
    "version": "1.8.1",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Marc Becker [cre, aut] (ORCID: <https://orcid.org/0000-0002-8115-0400>),\n  Jakob Richter [aut] (ORCID: <https://orcid.org/0000-0003-4481-5554>),\n  Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>),\n  Bernd Bischl [aut] (ORCID: <https://orcid.org/0000-0001-6002-6980>),\n  Martin Binder [aut],\n  Olaf Mersmann [ctb]",
    "url": "https://bbotk.mlr-org.com, https://github.com/mlr-org/bbotk",
    "bug_reports": "https://github.com/mlr-org/bbotk/issues",
    "repository": "https://cran.r-project.org/package=bbotk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bbotk Black-Box Optimization Toolkit Features highly configurable search spaces via the 'paradox'\n    package and optimizes every user-defined objective function. The\n    package includes several optimization algorithms e.g. Random Search,\n    Iterated Racing, Bayesian Optimization (in 'mlr3mbo') and Hyperband\n    (in 'mlr3hyperband'). bbotk is the base package of 'mlr3tuning',\n    'mlr3fselect' and 'miesmuschel'.  "
  },
  {
    "id": 9028,
    "package_name": "behavr",
    "title": "Canonical Data Structure for Behavioural Data",
    "description": "Implements an S3 class based on 'data.table' to store and process efficiently ethomics (high-throughput behavioural) data.",
    "version": "0.3.3",
    "maintainer": "Quentin Geissmann <qgeissmann@gmail.com>",
    "author": "Quentin Geissmann [aut, cre]",
    "url": "https://github.com/rethomics/behavr",
    "bug_reports": "https://github.com/rethomics/behavr/issues",
    "repository": "https://cran.r-project.org/package=behavr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "behavr Canonical Data Structure for Behavioural Data Implements an S3 class based on 'data.table' to store and process efficiently ethomics (high-throughput behavioural) data.  "
  },
  {
    "id": 9038,
    "package_name": "bennu",
    "title": "Bayesian Estimation of Naloxone Kit Number Under-Reporting",
    "description": "Bayesian model and associated tools for generating estimates of \n    total naloxone kit numbers distributed and used from naloxone kit orders \n    data. Provides functions for generating simulated data of naloxone kit use\n    and functions for generating samples from the posterior.",
    "version": "0.3.2",
    "maintainer": "Mike Irvine <mike.irvine@bccdc.ca>",
    "author": "Mike Irvine [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4785-8998>),\n  Samantha Bardwell [ctb],\n  Andrew Johnson [ctb]",
    "url": "https://sempwn.github.io/bennu/",
    "bug_reports": "https://github.com/sempwn/bennu/issues",
    "repository": "https://cran.r-project.org/package=bennu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bennu Bayesian Estimation of Naloxone Kit Number Under-Reporting Bayesian model and associated tools for generating estimates of \n    total naloxone kit numbers distributed and used from naloxone kit orders \n    data. Provides functions for generating simulated data of naloxone kit use\n    and functions for generating samples from the posterior.  "
  },
  {
    "id": 9065,
    "package_name": "bezier",
    "title": "Toolkit for Bezier Curves and Splines",
    "description": "The bezier package is a toolkit for working with Bezier curves and splines. The package provides functions for point generation, arc length estimation, degree elevation and curve fitting.",
    "version": "1.1.2",
    "maintainer": "Aaron Olsen <aarolsen@gmail.com>",
    "author": "Aaron Olsen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bezier",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bezier Toolkit for Bezier Curves and Splines The bezier package is a toolkit for working with Bezier curves and splines. The package provides functions for point generation, arc length estimation, degree elevation and curve fitting.  "
  },
  {
    "id": 9098,
    "package_name": "bidux",
    "title": "Behavioral Insight Design: A Toolkit for Integrating Behavioral\nScience in UI/UX Design",
    "description": "Provides a framework and toolkit to guide R dashboard developers in\n    implementing the Behavioral Insight Design (BID) framework. The package offers\n    functions for documenting each of the five stages (Interpret, Notice,\n    Anticipate, Structure, and Validate), along with a comprehensive concept\n    dictionary. Works with both 'shiny' applications and 'Quarto' dashboards.",
    "version": "0.3.3",
    "maintainer": "Jeremy Winget <contact@jrwinget.com>",
    "author": "Jeremy Winget [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3783-4354>)",
    "url": "https://jrwinget.github.io/bidux/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bidux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bidux Behavioral Insight Design: A Toolkit for Integrating Behavioral\nScience in UI/UX Design Provides a framework and toolkit to guide R dashboard developers in\n    implementing the Behavioral Insight Design (BID) framework. The package offers\n    functions for documenting each of the five stages (Interpret, Notice,\n    Anticipate, Structure, and Validate), along with a comprehensive concept\n    dictionary. Works with both 'shiny' applications and 'Quarto' dashboards.  "
  },
  {
    "id": 9126,
    "package_name": "bigreadr",
    "title": "Read Large Text Files",
    "description": "Read large text files by splitting them in smaller files.\n    Package 'bigreadr' also provides some convenient wrappers around fread()\n    and fwrite() from package 'data.table'. ",
    "version": "0.2.5",
    "maintainer": "Florian Priv\u00e9 <florian.prive.21@gmail.com>",
    "author": "Florian Priv\u00e9 [aut, cre]",
    "url": "https://github.com/privefl/bigreadr",
    "bug_reports": "https://github.com/privefl/bigreadr/issues",
    "repository": "https://cran.r-project.org/package=bigreadr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigreadr Read Large Text Files Read large text files by splitting them in smaller files.\n    Package 'bigreadr' also provides some convenient wrappers around fread()\n    and fwrite() from package 'data.table'.   "
  },
  {
    "id": 9214,
    "package_name": "birankr",
    "title": "Ranking Nodes in Bipartite and Weighted Networks",
    "description": "Highly efficient functions for estimating various rank (centrality) measures of nodes in bipartite graphs (two-mode networks). Includes methods for estimating HITS, CoHITS, BGRM, and BiRank with implementation primarily inspired by He et al. (2016) <doi:10.1109/TKDE.2016.2611584>. Also provides easy-to-use tools for efficiently estimating PageRank in one-mode graphs, incorporating or removing edge-weights during rank estimation, projecting two-mode graphs to one-mode, and for converting edgelists and matrices to sparseMatrix format. Best of all, the package's rank estimators can work directly with common formats of network data including edgelists (class data.frame, data.table, or tbl_df) and adjacency matrices (class matrix or dgCMatrix).",
    "version": "1.0.1",
    "maintainer": "Brian Aronson <bdaronson@gmail.com>",
    "author": "Brian Aronson [aut, cre],\n  Kai-Cheng Yang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=birankr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "birankr Ranking Nodes in Bipartite and Weighted Networks Highly efficient functions for estimating various rank (centrality) measures of nodes in bipartite graphs (two-mode networks). Includes methods for estimating HITS, CoHITS, BGRM, and BiRank with implementation primarily inspired by He et al. (2016) <doi:10.1109/TKDE.2016.2611584>. Also provides easy-to-use tools for efficiently estimating PageRank in one-mode graphs, incorporating or removing edge-weights during rank estimation, projecting two-mode graphs to one-mode, and for converting edgelists and matrices to sparseMatrix format. Best of all, the package's rank estimators can work directly with common formats of network data including edgelists (class data.frame, data.table, or tbl_df) and adjacency matrices (class matrix or dgCMatrix).  "
  },
  {
    "id": 9254,
    "package_name": "blit",
    "title": "Bioinformatics Library for Integrated Tools",
    "description": "An all-encompassing R toolkit designed to streamline the\n    process of calling various bioinformatics software and then performing data\n    analysis and visualization in R. With 'blit', users can easily integrate a\n    wide array of bioinformatics command line tools into their workflows,\n    leveraging the power of R for sophisticated data manipulation and graphical\n    representation.",
    "version": "0.2.0",
    "maintainer": "Yun Peng <yunyunp96@163.com>",
    "author": "Yun Peng [aut, cre] (ORCID: <https://orcid.org/0000-0003-2801-3332>),\n  Shixiang Wang [aut] (ORCID: <https://orcid.org/0000-0001-9855-7357>),\n  Jennifer Lu [cph] (Author of the included scripts from Kraken2 and\n    KrakenTools libraries),\n  Li Song [cph] (Author of included scripts from TRUST4 library),\n  X. Shirley Liu [cph] (Author of included scripts from TRUST4 library)",
    "url": "https://github.com/WangLabCSU/blit",
    "bug_reports": "https://github.com/WangLabCSU/blit/issues",
    "repository": "https://cran.r-project.org/package=blit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blit Bioinformatics Library for Integrated Tools An all-encompassing R toolkit designed to streamline the\n    process of calling various bioinformatics software and then performing data\n    analysis and visualization in R. With 'blit', users can easily integrate a\n    wide array of bioinformatics command line tools into their workflows,\n    leveraging the power of R for sophisticated data manipulation and graphical\n    representation.  "
  },
  {
    "id": 9395,
    "package_name": "brainR",
    "title": "Helper Functions to 'misc3d' and 'rgl' Packages for Brain\nImaging",
    "description": "This includes functions for creating 3D and 4D images using \n    'WebGL', 'rgl', and 'JavaScript' commands.  \n    This package relies on the X toolkit ('XTK',\n    <https://github.com/xtk/X#readme>).",
    "version": "1.7.0",
    "maintainer": "John Muschelli <muschellij2@gmail.com>",
    "author": "John Muschelli [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=brainR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "brainR Helper Functions to 'misc3d' and 'rgl' Packages for Brain\nImaging This includes functions for creating 3D and 4D images using \n    'WebGL', 'rgl', and 'JavaScript' commands.  \n    This package relies on the X toolkit ('XTK',\n    <https://github.com/xtk/X#readme>).  "
  },
  {
    "id": 9406,
    "package_name": "bread",
    "title": "Analyze Big Files Without Loading Them in Memory",
    "description": "A simple set of wrapper functions for data.table::fread() that allows subsetting or\n    filtering rows and selecting columns of table-formatted files too large for the available RAM.\n    'b stands for 'big files'.\n    bread makes heavy use of Unix commands like 'grep', 'sed', 'wc', 'awk' and 'cut'. They are available \n    by default in all Unix environments.\n    For Windows, you need to install those commands externally in order to simulate a \n    Unix environment and make sure that the executables are in the Windows PATH variable.\n    To my knowledge, the simplest ways are to install 'RTools', 'Git' or 'Cygwin'. If they have been \n    correctly installed (with the expected registry entries), they should be detected on loading\n    the package and the correct directories will be added automatically to the PATH.",
    "version": "0.4.1",
    "maintainer": "Vincent Guegan <vincent.guegan@banque-france.fr>",
    "author": "Vincent Guegan [aut, cre]",
    "url": "https://github.com/MagicHead99/bread/",
    "bug_reports": "https://github.com/MagicHead99/bread/issues",
    "repository": "https://cran.r-project.org/package=bread",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bread Analyze Big Files Without Loading Them in Memory A simple set of wrapper functions for data.table::fread() that allows subsetting or\n    filtering rows and selecting columns of table-formatted files too large for the available RAM.\n    'b stands for 'big files'.\n    bread makes heavy use of Unix commands like 'grep', 'sed', 'wc', 'awk' and 'cut'. They are available \n    by default in all Unix environments.\n    For Windows, you need to install those commands externally in order to simulate a \n    Unix environment and make sure that the executables are in the Windows PATH variable.\n    To my knowledge, the simplest ways are to install 'RTools', 'Git' or 'Cygwin'. If they have been \n    correctly installed (with the expected registry entries), they should be detected on loading\n    the package and the correct directories will be added automatically to the PATH.  "
  },
  {
    "id": 9474,
    "package_name": "btw",
    "title": "A Toolkit for Connecting R and Large Language Models",
    "description": "A complete toolkit for connecting 'R' environments with Large\n    Language Models (LLMs). Provides utilities for describing 'R' objects,\n    package documentation, and workspace state in plain text formats\n    optimized for LLM consumption. Supports multiple workflows:\n    interactive copy-paste to external chat interfaces, programmatic tool\n    registration with 'ellmer' chat clients, batteries-included chat\n    applications via 'shinychat', and exposure to external coding agents\n    through the Model Context Protocol. Project configuration files enable\n    stable, repeatable conversations with project-specific context and\n    preferred LLM settings.",
    "version": "1.0.0",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>),\n  Simon Couch [aut] (ORCID: <https://orcid.org/0000-0001-5676-5107>),\n  Joe Cheng [aut],\n  Posit Software, PBC [cph, fnd],\n  Google [cph] (Material Design Icons),\n  Jamie Perkins [cph] (countUp.js author)",
    "url": "https://github.com/posit-dev/btw, https://posit-dev.github.io/btw/",
    "bug_reports": "https://github.com/posit-dev/btw/issues",
    "repository": "https://cran.r-project.org/package=btw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "btw A Toolkit for Connecting R and Large Language Models A complete toolkit for connecting 'R' environments with Large\n    Language Models (LLMs). Provides utilities for describing 'R' objects,\n    package documentation, and workspace state in plain text formats\n    optimized for LLM consumption. Supports multiple workflows:\n    interactive copy-paste to external chat interfaces, programmatic tool\n    registration with 'ellmer' chat clients, batteries-included chat\n    applications via 'shinychat', and exposure to external coding agents\n    through the Model Context Protocol. Project configuration files enable\n    stable, repeatable conversations with project-specific context and\n    preferred LLM settings.  "
  },
  {
    "id": 9494,
    "package_name": "bupaR",
    "title": "Business Process Analysis in R",
    "description": "Comprehensive Business Process Analysis toolkit. Creates S3-class for event log objects, and related handler functions. Imports related packages for filtering event data, computation of descriptive statistics, handling of 'Petri Net' objects and visualization of process maps. See also packages 'edeaR','processmapR', 'eventdataR' and 'processmonitR'.",
    "version": "1.0.0",
    "maintainer": "Beno\u00eet Depaire <benoit.depaire@uhasselt.be>",
    "author": "Beno\u00eet Depaire [cre],\n  Gert Janssenswillen [aut],\n  Gerard van Hulzen [ctb],\n  Felix Mannhardt [ctb],\n  Niels Martin [ctb],\n  Greg Van Houdt [ctb],\n  Hasselt University [cph]",
    "url": "https://bupar.net/, https://github.com/bupaverse/bupaR/,\nhttps://bupaverse.github.io/bupaR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bupaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bupaR Business Process Analysis in R Comprehensive Business Process Analysis toolkit. Creates S3-class for event log objects, and related handler functions. Imports related packages for filtering event data, computation of descriptive statistics, handling of 'Petri Net' objects and visualization of process maps. See also packages 'edeaR','processmapR', 'eventdataR' and 'processmonitR'.  "
  },
  {
    "id": 9524,
    "package_name": "cNORM",
    "title": "Continuous Norming",
    "description": "A comprehensive toolkit for generating continuous test norms in \n     psychometrics and biometrics, and analyzing model fit. The package offers \n     both distribution-free modeling using Taylor polynomials and parametric \n     modeling using the beta-binomial and the 'Sinh-Arcsinh' distribution. \n     Originally developed for achievement tests, it is applicable to a wide \n     range of mental, physical, or other test scores dependent on continuous or \n     discrete explanatory variables. The package provides several advantages: \n     It minimizes deviations from representativeness in subsamples, interpolates \n     between discrete levels of explanatory variables, and significantly reduces \n     the required sample size compared to conventional norming per age group. \n     cNORM enables graphical and analytical evaluation of model fit, \n     accommodates a wide range of scales including those with negative and \n     descending values, and even supports conventional norming. It generates \n     norm tables including confidence intervals. It also includes methods for \n     addressing representativeness issues through Iterative Proportional Fitting. \n     Based on Lenhard et al. (2016) \n    <doi:10.1177/1073191116656437>, Lenhard et al. (2019) \n    <doi:10.1371/journal.pone.0222279>, Lenhard and Lenhard (2021) \n    <doi:10.1177/0013164420928457> and Gary et al. (2023) \n    <doi:10.1007/s00181-023-02456-0>.",
    "version": "3.5.1",
    "maintainer": "Wolfgang Lenhard <wolfgang.lenhard@uni-wuerzburg.de>",
    "author": "Alexandra Lenhard [aut] (ORCID:\n    <https://orcid.org/0000-0001-8680-4381>),\n  Wolfgang Lenhard [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-8184-6889>),\n  Sebastian Gary [aut],\n  WPS Publisher [fnd] (https://www.wpspublish.com/)",
    "url": "https://www.psychometrica.de/cNorm_en.html,\nhttps://github.com/WLenhard/cNORM",
    "bug_reports": "https://github.com/WLenhard/cNORM/issues",
    "repository": "https://cran.r-project.org/package=cNORM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cNORM Continuous Norming A comprehensive toolkit for generating continuous test norms in \n     psychometrics and biometrics, and analyzing model fit. The package offers \n     both distribution-free modeling using Taylor polynomials and parametric \n     modeling using the beta-binomial and the 'Sinh-Arcsinh' distribution. \n     Originally developed for achievement tests, it is applicable to a wide \n     range of mental, physical, or other test scores dependent on continuous or \n     discrete explanatory variables. The package provides several advantages: \n     It minimizes deviations from representativeness in subsamples, interpolates \n     between discrete levels of explanatory variables, and significantly reduces \n     the required sample size compared to conventional norming per age group. \n     cNORM enables graphical and analytical evaluation of model fit, \n     accommodates a wide range of scales including those with negative and \n     descending values, and even supports conventional norming. It generates \n     norm tables including confidence intervals. It also includes methods for \n     addressing representativeness issues through Iterative Proportional Fitting. \n     Based on Lenhard et al. (2016) \n    <doi:10.1177/1073191116656437>, Lenhard et al. (2019) \n    <doi:10.1371/journal.pone.0222279>, Lenhard and Lenhard (2021) \n    <doi:10.1177/0013164420928457> and Gary et al. (2023) \n    <doi:10.1007/s00181-023-02456-0>.  "
  },
  {
    "id": 9589,
    "package_name": "capl",
    "title": "Compute and Visualize CAPL-2 Scores and Interpretations",
    "description": "A toolkit for computing and visualizing CAPL-2\n    (Canadian Assessment of Physical Literacy, Second Edition;\n    <https://www.capl-eclp.ca>) scores and interpretations from raw data.",
    "version": "1.42",
    "maintainer": "Joel Barnes <j@barnzilla.ca>",
    "author": "Joel Barnes [aut, cre] (ORCID: <https://orcid.org/0000-0002-7621-9020>),\n  Michelle Guerrero [aut] (ORCID:\n    <https://orcid.org/0000-0001-8169-5040>)",
    "url": "https://github.com/barnzilla/capl",
    "bug_reports": "https://github.com/barnzilla/capl/issues",
    "repository": "https://cran.r-project.org/package=capl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "capl Compute and Visualize CAPL-2 Scores and Interpretations A toolkit for computing and visualizing CAPL-2\n    (Canadian Assessment of Physical Literacy, Second Edition;\n    <https://www.capl-eclp.ca>) scores and interpretations from raw data.  "
  },
  {
    "id": 9648,
    "package_name": "catcont",
    "title": "Test, Identify, Select and Mutate Categorical or Continuous\nValues",
    "description": "Methods and utilities for testing, identifying, selecting and  \n    mutating objects as categorical or continous types. These functions work on both \n    atomic vectors as well as recursive objects: data.frames, data.tables, \n    tibbles, lists, etc.. ",
    "version": "0.5.0",
    "maintainer": "Christopher Brown <chris.brown@decisionpatterns.com>",
    "author": "Christopher Brown [aut, cre],\n  Decision Patterns [cph]",
    "url": "https://github.com/decisionpatterns/catcont\nhttp://www.decisionpatterns.com",
    "bug_reports": "https://github.com/decisionpatterns/catcont/issues",
    "repository": "https://cran.r-project.org/package=catcont",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catcont Test, Identify, Select and Mutate Categorical or Continuous\nValues Methods and utilities for testing, identifying, selecting and  \n    mutating objects as categorical or continous types. These functions work on both \n    atomic vectors as well as recursive objects: data.frames, data.tables, \n    tibbles, lists, etc..   "
  },
  {
    "id": 9651,
    "package_name": "categoryEncodings",
    "title": "Category Variable Encodings",
    "description": "Simple, fast, and automatic encodings for category data using \n             a data.table backend. Most of the methods are an implementation \n             of \"Sufficient Representation for Categorical Variables\" by\n             Johannemann, Hadad, Athey, Wager (2019) <arXiv:1908.09874>,\n             particularly their mean, sparse principal component analysis, \n             low rank representation, and multinomial logit encodings.",
    "version": "1.4.3",
    "maintainer": "Juraj Szitas <szitas.juraj13@gmail.com>",
    "author": "Juraj Szitas [aut, cre]",
    "url": "https://github.com/JSzitas/categoryEncodings",
    "bug_reports": "https://github.com/JSzitas/categoryEncodings/issues",
    "repository": "https://cran.r-project.org/package=categoryEncodings",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "categoryEncodings Category Variable Encodings Simple, fast, and automatic encodings for category data using \n             a data.table backend. Most of the methods are an implementation \n             of \"Sufficient Representation for Categorical Variables\" by\n             Johannemann, Hadad, Athey, Wager (2019) <arXiv:1908.09874>,\n             particularly their mean, sparse principal component analysis, \n             low rank representation, and multinomial logit encodings.  "
  },
  {
    "id": 9753,
    "package_name": "cencrne",
    "title": "Consistent Estimation of the Number of Communities via\nRegularized Network Embedding",
    "description": "The network analysis plays an important role in numerous application domains including biomedicine. \n             Estimation of the number of communities is a fundamental and critical issue in network analysis. Most existing studies assume that the number of communities is known a priori, or lack of rigorous theoretical guarantee on the estimation consistency. This method proposes a regularized network embedding model to simultaneously estimate the community structure and the number of communities in a unified formulation. \n\t           The proposed model equips network embedding with a novel composite regularization term, which pushes the embedding vector towards its center and collapses similar community centers with each other. A rigorous theoretical analysis is conducted, establishing asymptotic consistency in terms of community detection and estimation of the number of communities. \n\t           Reference: \n             Ren, M., Zhang S. and Wang J. (2022). \"Consistent Estimation of the Number of Communities via Regularized Network Embedding\". Biometrics, <doi:10.1111/biom.13815>.",
    "version": "1.0.0",
    "maintainer": "Mingyang Ren <renmingyang17@mails.ucas.ac.cn>",
    "author": "Mingyang Ren [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8061-9940>),\n  Sanguo Zhang [aut],\n  Junhui Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cencrne",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cencrne Consistent Estimation of the Number of Communities via\nRegularized Network Embedding The network analysis plays an important role in numerous application domains including biomedicine. \n             Estimation of the number of communities is a fundamental and critical issue in network analysis. Most existing studies assume that the number of communities is known a priori, or lack of rigorous theoretical guarantee on the estimation consistency. This method proposes a regularized network embedding model to simultaneously estimate the community structure and the number of communities in a unified formulation. \n\t           The proposed model equips network embedding with a novel composite regularization term, which pushes the embedding vector towards its center and collapses similar community centers with each other. A rigorous theoretical analysis is conducted, establishing asymptotic consistency in terms of community detection and estimation of the number of communities. \n\t           Reference: \n             Ren, M., Zhang S. and Wang J. (2022). \"Consistent Estimation of the Number of Communities via Regularized Network Embedding\". Biometrics, <doi:10.1111/biom.13815>.  "
  },
  {
    "id": 9795,
    "package_name": "cgmguru",
    "title": "Advanced Continuous Glucose Monitoring Analysis with\nHigh-Performance C++ Backend",
    "description": "Tools for advanced analysis of continuous glucose monitoring (CGM)\n    time-series, implementing GRID (Glucose Rate Increase Detector) and GRID-based\n    algorithms for postprandial peak detection, and detection of hypoglycemic and\n    hyperglycemic episodes (Levels 1/2/Extended) aligned with international consensus\n    CGM metrics. Core algorithms are implemented in optimized C++ using 'Rcpp' to\n    provide accurate and fast analysis on large datasets.",
    "version": "0.1.0",
    "maintainer": "Sang Ho Park <shstat1729@gmail.com>",
    "author": "Sang Ho Park [aut, cre],\n  Rosa Oh [aut, ctb],\n  Sang-Man Jin [aut, ctb]",
    "url": "https://github.com/shstat1729/cgmguru",
    "bug_reports": "https://github.com/shstat1729/cgmguru/issues",
    "repository": "https://cran.r-project.org/package=cgmguru",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cgmguru Advanced Continuous Glucose Monitoring Analysis with\nHigh-Performance C++ Backend Tools for advanced analysis of continuous glucose monitoring (CGM)\n    time-series, implementing GRID (Glucose Rate Increase Detector) and GRID-based\n    algorithms for postprandial peak detection, and detection of hypoglycemic and\n    hyperglycemic episodes (Levels 1/2/Extended) aligned with international consensus\n    CGM metrics. Core algorithms are implemented in optimized C++ using 'Rcpp' to\n    provide accurate and fast analysis on large datasets.  "
  },
  {
    "id": 9834,
    "package_name": "checkthat",
    "title": "Intuitive Unit Testing Tools for Data Manipulation",
    "description": "Provides a lightweight data validation and testing toolkit for R. \n    Its guiding philosophy is that adding code-based data checks to users' \n    existing workflow should be both quick and intuitive. The suite of \n    functions included therefore mirror the common data checks many users \n    already perform by hand or by eye. Additionally, the 'checkthat' package is \n    optimized to work within 'tidyverse' data manipulation pipelines.",
    "version": "0.1.0",
    "maintainer": "Ian Cero <ian_cero@urmc.rochester.edu>",
    "author": "Ian Cero [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2862-0450>)",
    "url": "https://github.com/iancero/checkthat,\nhttps://iancero.github.io/checkthat/",
    "bug_reports": "https://github.com/iancero/checkthat/issues",
    "repository": "https://cran.r-project.org/package=checkthat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "checkthat Intuitive Unit Testing Tools for Data Manipulation Provides a lightweight data validation and testing toolkit for R. \n    Its guiding philosophy is that adding code-based data checks to users' \n    existing workflow should be both quick and intuitive. The suite of \n    functions included therefore mirror the common data checks many users \n    already perform by hand or by eye. Additionally, the 'checkthat' package is \n    optimized to work within 'tidyverse' data manipulation pipelines.  "
  },
  {
    "id": 9838,
    "package_name": "cheetahR",
    "title": "High Performance Tables Using 'Cheetah Grid'",
    "description": "An R interface to 'Cheetah Grid', a high-performance JavaScript table widget. \n    'cheetahR' allows users to render millions of rows in just a few milliseconds, \n    making it an excellent alternative to other R table widgets. The package wraps \n    the 'Cheetah Grid' JavaScript functions and makes them readily available for R users.\n    The underlying grid implementation is based on 'Cheetah Grid'\n    <https://github.com/future-architect/cheetah-grid>.",
    "version": "0.3.0",
    "maintainer": "Olajoke Oladipo <olajoke@cynkra.com>",
    "author": "Olajoke Oladipo [aut, cre],\n  David Granjon [aut],\n  cynkra GmbH [fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cheetahR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cheetahR High Performance Tables Using 'Cheetah Grid' An R interface to 'Cheetah Grid', a high-performance JavaScript table widget. \n    'cheetahR' allows users to render millions of rows in just a few milliseconds, \n    making it an excellent alternative to other R table widgets. The package wraps \n    the 'Cheetah Grid' JavaScript functions and makes them readily available for R users.\n    The underlying grid implementation is based on 'Cheetah Grid'\n    <https://github.com/future-architect/cheetah-grid>.  "
  },
  {
    "id": 9853,
    "package_name": "chicane",
    "title": "Capture Hi-C Analysis Engine",
    "description": "Toolkit for processing and calling interactions in capture Hi-C data. Converts BAM files into counts of reads linking restriction fragments, and identifies pairs of fragments that interact more than expected by chance. Significant interactions are identified by comparing the observed read count to the expected background rate from a count regression model.",
    "version": "0.1.8",
    "maintainer": "Syed Haider <Syed.Haider@icr.ac.uk>",
    "author": "Erle Holgersen [aut],\n  Olivia Leavy [aut],\n  Olivia Fletcher [aut],\n  Frank Dudbridge [aut],\n  Syed Haider [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=chicane",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chicane Capture Hi-C Analysis Engine Toolkit for processing and calling interactions in capture Hi-C data. Converts BAM files into counts of reads linking restriction fragments, and identifies pairs of fragments that interact more than expected by chance. Significant interactions are identified by comparing the observed read count to the expected background rate from a count regression model.  "
  },
  {
    "id": 9861,
    "package_name": "chipPCR",
    "title": "Toolkit of Helper Functions to Pre-Process Amplification Data",
    "description": "A collection of functions to pre-process amplification curve data from polymerase chain reaction (PCR) or isothermal amplification reactions. Contains functions to normalize and baseline amplification curves, to detect both the start and end of an amplification reaction, several smoothers (e.g., LOWESS, moving average, cubic splines, Savitzky-Golay), a function to detect false positive amplification reactions and a function to determine the amplification efficiency. Quantification point (Cq) methods include the first (FDM) and second approximate derivative maximum (SDM) methods (calculated by a 5-point-stencil) and the cycle threshold method. Data sets of experimental nucleic acid amplification systems ('VideoScan HCU', capillary convective PCR (ccPCR)) and commercial systems are included. Amplification curves were generated by helicase dependent amplification (HDA), ccPCR or PCR. As detection system intercalating dyes (EvaGreen, SYBR Green) and hydrolysis probes (TaqMan) were used. For more information see: Roediger et al. (2015) <doi:10.1093/bioinformatics/btv205>. ",
    "version": "1.0-2",
    "maintainer": "Stefan Roediger <stefan.roediger@b-tu.de>",
    "author": "Stefan Roediger [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-1441-6512>),\n  Michal Burdukiewicz [aut] (ORCID:\n    <https://orcid.org/0000-0001-8926-582X>),\n  Konstantin A. Blagodatskikh [ctb],\n  Andrej-Nikolai Spiess [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9630-4724>)",
    "url": "https://github.com/PCRuniversum/chipPCR",
    "bug_reports": "https://github.com/PCRuniversum/chipPCR/issues",
    "repository": "https://cran.r-project.org/package=chipPCR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chipPCR Toolkit of Helper Functions to Pre-Process Amplification Data A collection of functions to pre-process amplification curve data from polymerase chain reaction (PCR) or isothermal amplification reactions. Contains functions to normalize and baseline amplification curves, to detect both the start and end of an amplification reaction, several smoothers (e.g., LOWESS, moving average, cubic splines, Savitzky-Golay), a function to detect false positive amplification reactions and a function to determine the amplification efficiency. Quantification point (Cq) methods include the first (FDM) and second approximate derivative maximum (SDM) methods (calculated by a 5-point-stencil) and the cycle threshold method. Data sets of experimental nucleic acid amplification systems ('VideoScan HCU', capillary convective PCR (ccPCR)) and commercial systems are included. Amplification curves were generated by helicase dependent amplification (HDA), ccPCR or PCR. As detection system intercalating dyes (EvaGreen, SYBR Green) and hydrolysis probes (TaqMan) were used. For more information see: Roediger et al. (2015) <doi:10.1093/bioinformatics/btv205>.   "
  },
  {
    "id": 9986,
    "package_name": "clinPK",
    "title": "Clinical Pharmacokinetics Toolkit",
    "description": "Provides equations commonly used in clinical\n        pharmacokinetics and clinical pharmacology, such as equations\n        for dose individualization, compartmental pharmacokinetics,\n        drug exposure, anthropomorphic calculations, clinical\n        chemistry, and conversion of common clinical parameters. Where\n        possible and relevant, it provides multiple published and\n        peer-reviewed equations within the respective R function.",
    "version": "0.13.0",
    "maintainer": "Ron Keizer <ron@insight-rx.com>",
    "author": "Ron Keizer [aut, cre],\n  Jasmine Hughes [aut],\n  Dominic Tong [aut],\n  Kara Woo [aut],\n  InsightRX [cph, fnd]",
    "url": "https://github.com/InsightRX/clinPK,\nhttps://insightrx.github.io/clinPK/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clinPK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinPK Clinical Pharmacokinetics Toolkit Provides equations commonly used in clinical\n        pharmacokinetics and clinical pharmacology, such as equations\n        for dose individualization, compartmental pharmacokinetics,\n        drug exposure, anthropomorphic calculations, clinical\n        chemistry, and conversion of common clinical parameters. Where\n        possible and relevant, it provides multiple published and\n        peer-reviewed equations within the respective R function.  "
  },
  {
    "id": 10054,
    "package_name": "clustlearn",
    "title": "Learn Clustering Techniques Through Examples and Code",
    "description": "Clustering methods, which (if asked) can provide step-by-step\n    explanations of the algorithms used, as described in Ezugwu et. al., (2022)\n    <doi:10.1016/j.engappai.2022.104743>; and datasets to test them on, which\n    highlight the strengths and weaknesses of each technique, as presented in\n    the clustering section of 'scikit-learn' (Pedregosa et al., 2011)\n    <https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html>.",
    "version": "1.0.0",
    "maintainer": "Eduardo Ruiz Sabajanes <eduardo.ruizs@edu.uah.es>",
    "author": "Eduardo Ruiz Sabajanes [aut, cre],\n  Juan Jose Cuadrado Gallego [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8178-5556>),\n  Universidad de Alcala [cph]",
    "url": "https://github.com/Ediu3095/clustlearn",
    "bug_reports": "https://github.com/Ediu3095/clustlearn/issues",
    "repository": "https://cran.r-project.org/package=clustlearn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clustlearn Learn Clustering Techniques Through Examples and Code Clustering methods, which (if asked) can provide step-by-step\n    explanations of the algorithms used, as described in Ezugwu et. al., (2022)\n    <doi:10.1016/j.engappai.2022.104743>; and datasets to test them on, which\n    highlight the strengths and weaknesses of each technique, as presented in\n    the clustering section of 'scikit-learn' (Pedregosa et al., 2011)\n    <https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html>.  "
  },
  {
    "id": 10166,
    "package_name": "collapse",
    "title": "Advanced and Fast Data Transformation",
    "description": "A large C/C++-based package for advanced data transformation and \n    statistical computing in R that is extremely fast, class-agnostic, robust, and \n    programmer friendly. Core functionality includes a rich set of S3 generic grouped \n    and weighted statistical functions for vectors, matrices and data frames, which \n    provide efficient low-level vectorizations, OpenMP multithreading, and skip missing \n    values by default. These are integrated with fast grouping and ordering algorithms \n    (also callable from C), and efficient data manipulation functions. The package also \n    provides a flexible and rigorous approach to time series and panel data in R, fast \n    functions for data transformation and common statistical procedures, detailed \n    (grouped, weighted) summary statistics, powerful tools to work with nested data, \n    fast data object conversions, functions for memory efficient R programming, and \n    helpers to effectively deal with variable labels, attributes, and missing data. It \n    seamlessly supports base R objects/classes as well as 'units', 'integer64', 'xts'/\n    'zoo', 'tibble', 'grouped_df', 'data.table', 'sf', and 'pseries'/'pdata.frame'.",
    "version": "2.1.5",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6212-5229>),\n  Matt Dowle [ctb],\n  Arun Srinivasan [ctb],\n  Morgan Jacob [ctb],\n  Dirk Eddelbuettel [ctb],\n  Laurent Berge [ctb],\n  Kevin Tappe [ctb],\n  Alina Cherkas [ctb],\n  R Core Team and contributors worldwide [ctb],\n  Martyn Plummer [cph],\n  1999-2016 The R Core Team [cph]",
    "url": "https://sebkrantz.github.io/collapse/,\nhttps://github.com/SebKrantz/collapse",
    "bug_reports": "https://github.com/SebKrantz/collapse/issues",
    "repository": "https://cran.r-project.org/package=collapse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "collapse Advanced and Fast Data Transformation A large C/C++-based package for advanced data transformation and \n    statistical computing in R that is extremely fast, class-agnostic, robust, and \n    programmer friendly. Core functionality includes a rich set of S3 generic grouped \n    and weighted statistical functions for vectors, matrices and data frames, which \n    provide efficient low-level vectorizations, OpenMP multithreading, and skip missing \n    values by default. These are integrated with fast grouping and ordering algorithms \n    (also callable from C), and efficient data manipulation functions. The package also \n    provides a flexible and rigorous approach to time series and panel data in R, fast \n    functions for data transformation and common statistical procedures, detailed \n    (grouped, weighted) summary statistics, powerful tools to work with nested data, \n    fast data object conversions, functions for memory efficient R programming, and \n    helpers to effectively deal with variable labels, attributes, and missing data. It \n    seamlessly supports base R objects/classes as well as 'units', 'integer64', 'xts'/\n    'zoo', 'tibble', 'grouped_df', 'data.table', 'sf', and 'pseries'/'pdata.frame'.  "
  },
  {
    "id": 10167,
    "package_name": "collapsibleTree",
    "title": "Interactive Collapsible Tree Diagrams using 'D3.js'",
    "description": "\n    Interactive Reingold-Tilford tree diagrams created using 'D3.js', where every node can be expanded and collapsed by clicking on it.\n    Tooltips and color gradients can be mapped to nodes using a numeric column in the source data frame.\n    See 'collapsibleTree' website for more information and examples.",
    "version": "0.1.8",
    "maintainer": "Adeel Khan <AdeelK@gwu.edu>",
    "author": "Adeel Khan [aut, cre],\n  Dhrumin Shah [ctb],\n  Mike Bostock [ctb, cph] (D3.js library, http://d3js.org)",
    "url": "https://github.com/AdeelK93/collapsibleTree,\nhttps://AdeelK93.github.io/collapsibleTree/",
    "bug_reports": "https://github.com/AdeelK93/collapsibleTree/issues",
    "repository": "https://cran.r-project.org/package=collapsibleTree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "collapsibleTree Interactive Collapsible Tree Diagrams using 'D3.js' \n    Interactive Reingold-Tilford tree diagrams created using 'D3.js', where every node can be expanded and collapsed by clicking on it.\n    Tooltips and color gradients can be mapped to nodes using a numeric column in the source data frame.\n    See 'collapsibleTree' website for more information and examples.  "
  },
  {
    "id": 10176,
    "package_name": "collpcm",
    "title": "Collapsed Latent Position Cluster Model for Social Networks",
    "description": "Markov chain Monte Carlo based inference routines for collapsed latent position cluster models or social networks, which includes searches over the model space (number of clusters in the latent position cluster model). The label switching algorithm used is that of Nobile and Fearnside (2007) <doi:10.1007/s11222-006-9014-7> which relies on the algorithm of Carpaneto and Toth (1980) <doi:10.1145/355873.355883>. ",
    "version": "1.4",
    "maintainer": "Jason Wyse <wyseja@tcd.ie>",
    "author": "Jason Wyse [aut, cre],\n  Caitriona Ryan [aut],\n  Nial Friel [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=collpcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "collpcm Collapsed Latent Position Cluster Model for Social Networks Markov chain Monte Carlo based inference routines for collapsed latent position cluster models or social networks, which includes searches over the model space (number of clusters in the latent position cluster model). The label switching algorithm used is that of Nobile and Fearnside (2007) <doi:10.1007/s11222-006-9014-7> which relies on the algorithm of Carpaneto and Toth (1980) <doi:10.1145/355873.355883>.   "
  },
  {
    "id": 10321,
    "package_name": "connectcreds",
    "title": "Manage 'OAuth' Credentials from 'Posit Connect'",
    "description": "A toolkit for making use of credentials mediated by 'Posit\n  Connect'. It handles the details of communicating with the Connect API\n  correctly, 'OAuth' token caching, and refresh behaviour.",
    "version": "0.1.0",
    "maintainer": "Aaron Jacobs <aaron.jacobs@posit.co>",
    "author": "Aaron Jacobs [aut, cre],\n  Posit Software, PBC [cph, fnd]",
    "url": "https://github.com/posit-dev/connectcreds",
    "bug_reports": "https://github.com/posit-dev/connectcreds/issues",
    "repository": "https://cran.r-project.org/package=connectcreds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "connectcreds Manage 'OAuth' Credentials from 'Posit Connect' A toolkit for making use of credentials mediated by 'Posit\n  Connect'. It handles the details of communicating with the Connect API\n  correctly, 'OAuth' token caching, and refresh behaviour.  "
  },
  {
    "id": 10345,
    "package_name": "container",
    "title": "Extending Base 'R' Lists",
    "description": "Extends the functionality of base 'R' lists and provides\n    specialized data structures 'deque', 'set', 'dict', and 'dict.table',\n    the latter to extend the 'data.table' package.",
    "version": "1.1.0",
    "maintainer": "Roman Pahl <roman.pahl@gmail.com>",
    "author": "Roman Pahl [aut, cre]",
    "url": "https://rpahl.github.io/container/,\nhttps://github.com/rpahl/container",
    "bug_reports": "https://github.com/rpahl/container/issues",
    "repository": "https://cran.r-project.org/package=container",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "container Extending Base 'R' Lists Extends the functionality of base 'R' lists and provides\n    specialized data structures 'deque', 'set', 'dict', and 'dict.table',\n    the latter to extend the 'data.table' package.  "
  },
  {
    "id": 10553,
    "package_name": "creditmodel",
    "title": "Toolkit for Credit Modeling, Analysis and Visualization",
    "description": "\n  Provides a highly efficient R tool suite for Credit Modeling, Analysis and Visualization.Contains infrastructure functionalities such as data exploration and preparation, missing values treatment, outliers treatment, variable derivation, variable selection, dimensionality reduction, grid search for hyper parameters, data mining and visualization, model evaluation, strategy analysis etc. This package is designed to make the development of binary classification models (machine learning based models as well as credit scorecard) simpler and faster. The references including: 1 Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: Development and Implementation Using SAS; 2 Bezdek, James C.FCM: The fuzzy c-means clustering algorithm. Computers & Geosciences (0098-3004),<DOI:10.1016/0098-3004(84)90020-7>.",
    "version": "1.3.1",
    "maintainer": "Dongping Fan <fdp@pku.edu.cn>",
    "author": "Dongping Fan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=creditmodel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "creditmodel Toolkit for Credit Modeling, Analysis and Visualization \n  Provides a highly efficient R tool suite for Credit Modeling, Analysis and Visualization.Contains infrastructure functionalities such as data exploration and preparation, missing values treatment, outliers treatment, variable derivation, variable selection, dimensionality reduction, grid search for hyper parameters, data mining and visualization, model evaluation, strategy analysis etc. This package is designed to make the development of binary classification models (machine learning based models as well as credit scorecard) simpler and faster. The references including: 1 Refaat, M. (2011, ISBN: 9781447511199). Credit Risk Scorecard: Development and Implementation Using SAS; 2 Bezdek, James C.FCM: The fuzzy c-means clustering algorithm. Computers & Geosciences (0098-3004),<DOI:10.1016/0098-3004(84)90020-7>.  "
  },
  {
    "id": 10560,
    "package_name": "crew.cluster",
    "title": "Crew Launcher Plugins for Traditional High-Performance Computing\nClusters",
    "description": "In computationally demanding analysis projects,\n  statisticians and data scientists asynchronously\n  deploy long-running tasks to distributed systems,\n  ranging from traditional clusters to cloud services.\n  The 'crew.cluster' package extends the 'mirai'-powered\n  'crew' package with worker launcher plugins for traditional\n  high-performance computing systems.\n  Inspiration also comes from packages 'mirai' by Gao (2023)\n  <https://github.com/r-lib/mirai>,\n  'future' by Bengtsson (2021) <doi:10.32614/RJ-2021-048>,\n  'rrq' by FitzJohn and Ashton (2023) <https://github.com/mrc-ide/rrq>,\n  'clustermq' by Schubert (2019) <doi:10.1093/bioinformatics/btz284>),\n  and 'batchtools' by Lang, Bischl, and Surmann (2017).\n  <doi:10.21105/joss.00135>.",
    "version": "0.4.0",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "author": "William Michael Landau [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1878-3253>),\n  Michael Gilbert Levin [aut] (ORCID:\n    <https://orcid.org/0000-0002-9937-9932>),\n  Brendan Furneaux [aut] (ORCID: <https://orcid.org/0000-0003-3522-7363>),\n  Eli Lilly and Company [cph, fnd]",
    "url": "https://wlandau.github.io/crew.cluster/,\nhttps://github.com/wlandau/crew.cluster",
    "bug_reports": "https://github.com/wlandau/crew.cluster/issues",
    "repository": "https://cran.r-project.org/package=crew.cluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crew.cluster Crew Launcher Plugins for Traditional High-Performance Computing\nClusters In computationally demanding analysis projects,\n  statisticians and data scientists asynchronously\n  deploy long-running tasks to distributed systems,\n  ranging from traditional clusters to cloud services.\n  The 'crew.cluster' package extends the 'mirai'-powered\n  'crew' package with worker launcher plugins for traditional\n  high-performance computing systems.\n  Inspiration also comes from packages 'mirai' by Gao (2023)\n  <https://github.com/r-lib/mirai>,\n  'future' by Bengtsson (2021) <doi:10.32614/RJ-2021-048>,\n  'rrq' by FitzJohn and Ashton (2023) <https://github.com/mrc-ide/rrq>,\n  'clustermq' by Schubert (2019) <doi:10.1093/bioinformatics/btz284>),\n  and 'batchtools' by Lang, Bischl, and Surmann (2017).\n  <doi:10.21105/joss.00135>.  "
  },
  {
    "id": 10632,
    "package_name": "csdb",
    "title": "An Abstracted System for Easily Working with Databases with\nLarge Datasets",
    "description": "Provides object-oriented database management tools for working with large datasets across multiple database systems. Features include robust connection management for SQL Server and PostgreSQL databases, advanced table operations with bulk data loading and upsert functionality, comprehensive data validation through customizable field type and content validators, efficient index management, and cross-database compatibility. Designed for high-performance data operations in surveillance systems and large-scale data processing workflows.",
    "version": "2025.7.30",
    "maintainer": "Richard Aubrey White <hello@rwhite.no>",
    "author": "Richard Aubrey White [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6747-1726>),\n  August S\u00f8rli Mathisen [aut],\n  CSIDS [cph]",
    "url": "https://www.csids.no/csdb/, https://github.com/csids/csdb",
    "bug_reports": "https://github.com/csids/csdb/issues",
    "repository": "https://cran.r-project.org/package=csdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csdb An Abstracted System for Easily Working with Databases with\nLarge Datasets Provides object-oriented database management tools for working with large datasets across multiple database systems. Features include robust connection management for SQL Server and PostgreSQL databases, advanced table operations with bulk data loading and upsert functionality, comprehensive data validation through customizable field type and content validators, efficient index management, and cross-database compatibility. Designed for high-performance data operations in surveillance systems and large-scale data processing workflows.  "
  },
  {
    "id": 10714,
    "package_name": "cusumcharter",
    "title": "Easier CUSUM Control Charts",
    "description": "Create CUSUM (cumulative sum) statistics from a vector or dataframe.\n    Also create single or faceted CUSUM control charts, with or without control limits.\n    Accepts vector, dataframe, tibble or data.table inputs.",
    "version": "0.1.0",
    "maintainer": "John MacKintosh <johnmackintosh.jm@gmail.com>",
    "author": "John MacKintosh [aut, cre]",
    "url": "https://github.com/johnmackintosh/cusumcharter,\nhttps://johnmackintosh.github.io/cusumcharter/",
    "bug_reports": "https://github.com/johnmackintosh/cusumcharter/issues",
    "repository": "https://cran.r-project.org/package=cusumcharter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cusumcharter Easier CUSUM Control Charts Create CUSUM (cumulative sum) statistics from a vector or dataframe.\n    Also create single or faceted CUSUM control charts, with or without control limits.\n    Accepts vector, dataframe, tibble or data.table inputs.  "
  },
  {
    "id": 10744,
    "package_name": "cymruservices",
    "title": "Query 'Team Cymru' 'IP' Address, Autonomous System Number\n('ASN'), Border Gateway Protocol ('BGP'), Bogon and 'Malware'\nHash Data Services",
    "description": "A toolkit for querying 'Team Cymru' <http://team-cymru.org> 'IP'\n    address, Autonomous System Number ('ASN'), Border Gateway Protocol ('BGP'), Bogon\n    and 'Malware' Hash Data Services.",
    "version": "0.5.0",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis [aut, cre] (ORCID: <https://orcid.org/0000-0001-5670-2640>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cymruservices",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cymruservices Query 'Team Cymru' 'IP' Address, Autonomous System Number\n('ASN'), Border Gateway Protocol ('BGP'), Bogon and 'Malware'\nHash Data Services A toolkit for querying 'Team Cymru' <http://team-cymru.org> 'IP'\n    address, Autonomous System Number ('ASN'), Border Gateway Protocol ('BGP'), Bogon\n    and 'Malware' Hash Data Services.  "
  },
  {
    "id": 10755,
    "package_name": "d3r",
    "title": "'d3.js' Utilities for R",
    "description": "Provides a suite of functions to help ease the use of 'd3.js' in R.\n              These helpers include 'htmltools::htmlDependency' functions, hierarchy\n              builders, and conversion tools for 'partykit', 'igraph,' 'table',\n              and 'data.frame' R objects into the 'JSON' that 'd3.js' expects.",
    "version": "1.1.0",
    "maintainer": "Kent Russell <kent.russell@timelyportfolio.com>",
    "author": "Mike Bostock [aut, cph] (d3.js library in htmlwidgets/lib,\n    http://d3js.org),\n  Kent Russell [aut, cre, cph] (R interface),\n  Gregor Aisch [aut, cph] (d3-jetpack creator,\n    https://github.com/gka/d3-jetpack),\n  Adam Pearce [aut] (core contributor to d3-jetpack),\n  Ben Ortiz [ctb]",
    "url": "https://github.com/timelyportfolio/d3r",
    "bug_reports": "https://github.com/timelyportfolio/d3r/issues",
    "repository": "https://cran.r-project.org/package=d3r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "d3r 'd3.js' Utilities for R Provides a suite of functions to help ease the use of 'd3.js' in R.\n              These helpers include 'htmltools::htmlDependency' functions, hierarchy\n              builders, and conversion tools for 'partykit', 'igraph,' 'table',\n              and 'data.frame' R objects into the 'JSON' that 'd3.js' expects.  "
  },
  {
    "id": 10784,
    "package_name": "dagHMM",
    "title": "Directed Acyclic Graph HMM with TAN Structured Emissions",
    "description": "Hidden Markov models (HMMs) are a formal foundation for making probabilistic models of linear sequence. They provide a conceptual toolkit for building complex models just by drawing an intuitive picture. They are at the heart of a diverse range of programs, including genefinding, profile searches, multiple sequence alignment and regulatory site identification. HMMs are the Legos of computational sequence analysis. In graph theory, a tree is an undirected graph in which any two vertices are connected by exactly one path, or equivalently a connected acyclic undirected graph. Tree represents the nodes connected by edges. It is a non-linear data structure. A poly-tree is simply a directed acyclic graph whose underlying undirected graph is a tree. The model proposed in this package is the same as an HMM but where the states are linked via a polytree structure rather than a simple path.",
    "version": "0.1.1",
    "maintainer": "Prajwal Bende <prajwal.bende@gmail.com>",
    "author": "Prajwal Bende [aut, cre],\n  Russ Greiner [ths],\n  Pouria Ramazi [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dagHMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dagHMM Directed Acyclic Graph HMM with TAN Structured Emissions Hidden Markov models (HMMs) are a formal foundation for making probabilistic models of linear sequence. They provide a conceptual toolkit for building complex models just by drawing an intuitive picture. They are at the heart of a diverse range of programs, including genefinding, profile searches, multiple sequence alignment and regulatory site identification. HMMs are the Legos of computational sequence analysis. In graph theory, a tree is an undirected graph in which any two vertices are connected by exactly one path, or equivalently a connected acyclic undirected graph. Tree represents the nodes connected by edges. It is a non-linear data structure. A poly-tree is simply a directed acyclic graph whose underlying undirected graph is a tree. The model proposed in this package is the same as an HMM but where the states are linked via a polytree structure rather than a simple path.  "
  },
  {
    "id": 10813,
    "package_name": "dat",
    "title": "Tools for Data Manipulation",
    "description": "An implementation of common higher order functions with syntactic\n    sugar for anonymous function. Provides also a link to 'dplyr' and\n    'data.table' for common transformations on data frames to work around non\n    standard evaluation by default.",
    "version": "0.5.0",
    "maintainer": "Sebastian Warnholz <wahani@gmail.com>",
    "author": "Sebastian Warnholz [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/wahani/dat/issues",
    "repository": "https://cran.r-project.org/package=dat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dat Tools for Data Manipulation An implementation of common higher order functions with syntactic\n    sugar for anonymous function. Provides also a link to 'dplyr' and\n    'data.table' for common transformations on data frames to work around non\n    standard evaluation by default.  "
  },
  {
    "id": 10815,
    "package_name": "data.table.threads",
    "title": "Analyze Multi-Threading Performance for 'data.table' Functions",
    "description": "Assists in finding the most suitable thread count for the various\n             'data.table' routines that support parallel processing.",
    "version": "1.0.1",
    "maintainer": "Anirban Chetia <ac4743@nau.edu>",
    "author": "Anirban Chetia [aut, cre]",
    "url": "https://github.com/Anirban166/data.table.threads",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=data.table.threads",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "data.table.threads Analyze Multi-Threading Performance for 'data.table' Functions Assists in finding the most suitable thread count for the various\n             'data.table' routines that support parallel processing.  "
  },
  {
    "id": 10823,
    "package_name": "dataMojo",
    "title": "Reshape Data Table",
    "description": "A grammar of data manipulation with 'data.table', providing a consistent a series of utility functions that help you solve the most common data manipulation challenges.",
    "version": "1.0.0",
    "maintainer": "Jiena McLellan <jienagu90@gmail.com>",
    "author": "Jiena McLellan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5578-088X>),\n  Michael Condouris [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8862-4250>),\n  Brittney Zykan [ctb],\n  Sai Im [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dataMojo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataMojo Reshape Data Table A grammar of data manipulation with 'data.table', providing a consistent a series of utility functions that help you solve the most common data manipulation challenges.  "
  },
  {
    "id": 10824,
    "package_name": "dataPreparation",
    "title": "Automated Data Preparation",
    "description": "Do most of the painful data preparation for a data science project with a minimum amount of code; Take advantages of 'data.table' efficiency and use some algorithmic trick in order to perform data preparation in a time and RAM efficient way.",
    "version": "1.1.2",
    "maintainer": "Emmanuel-Lin Toulemonde <el.toulemonde@protonmail.com>",
    "author": "Emmanuel-Lin Toulemonde [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/ELToulemonde/dataPreparation/issues",
    "repository": "https://cran.r-project.org/package=dataPreparation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataPreparation Automated Data Preparation Do most of the painful data preparation for a data science project with a minimum amount of code; Take advantages of 'data.table' efficiency and use some algorithmic trick in order to perform data preparation in a time and RAM efficient way.  "
  },
  {
    "id": 10856,
    "package_name": "dataverifyr",
    "title": "A Lightweight, Flexible, and Fast Data Validation Package that\nCan Handle All Sizes of Data",
    "description": "Allows you to define rules which can be used to verify a given\n    dataset.\n    The package acts as a thin wrapper around more powerful data packages such\n    as 'dplyr', 'data.table', 'arrow', and 'DBI' ('SQL'), which do the heavy lifting.",
    "version": "0.1.8",
    "maintainer": "David Zimmermann-Kollenda <david_j_zimmermann@hotmail.com>",
    "author": "David Zimmermann-Kollenda [aut, cre],\n  Beniamino Green [ctb]",
    "url": "https://github.com/DavZim/dataverifyr,\nhttps://davzim.github.io/dataverifyr/",
    "bug_reports": "https://github.com/DavZim/dataverifyr/issues",
    "repository": "https://cran.r-project.org/package=dataverifyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataverifyr A Lightweight, Flexible, and Fast Data Validation Package that\nCan Handle All Sizes of Data Allows you to define rules which can be used to verify a given\n    dataset.\n    The package acts as a thin wrapper around more powerful data packages such\n    as 'dplyr', 'data.table', 'arrow', and 'DBI' ('SQL'), which do the heavy lifting.  "
  },
  {
    "id": 10886,
    "package_name": "dbi.table",
    "title": "Database Queries Using 'data.table' Syntax",
    "description": "\n  Query database tables over a 'DBI' connection using 'data.table' syntax.\n  Attach database schemas to the search path. Automatically merge using foreign\n  key constraints.",
    "version": "1.0.5",
    "maintainer": "Kjell P. Konis <kjellk@gmail.com>",
    "author": "Kjell P. Konis [aut, cre],\n  Luis Rocha [ctb] (Chinook Database - see example_files/LICENSE)",
    "url": "https://github.com/kjellpk/dbi.table",
    "bug_reports": "https://github.com/kjellpk/dbi.table/issues",
    "repository": "https://cran.r-project.org/package=dbi.table",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbi.table Database Queries Using 'data.table' Syntax \n  Query database tables over a 'DBI' connection using 'data.table' syntax.\n  Attach database schemas to the search path. Automatically merge using foreign\n  key constraints.  "
  },
  {
    "id": 10960,
    "package_name": "dedupewider",
    "title": "Deduplication Across Multiple Columns",
    "description": "Duplicated data can exist in different rows and columns and user may need to\n    treat observations (rows) connected by duplicated data as one observation,\n    e.g. companies can belong to one family (and thus: be one company) by sharing\n    some telephone numbers. This package allows to find connected rows\n    based on data on chosen columns and collapse it into one row.",
    "version": "0.1.1",
    "maintainer": "Grzegorz Smoli\u0144ski <g.smolinski1@gmail.com>",
    "author": "Grzegorz Smoli\u0144ski [aut, cre]",
    "url": "https://github.com/gsmolinski/dedupewider",
    "bug_reports": "https://github.com/gsmolinski/dedupewider/issues",
    "repository": "https://cran.r-project.org/package=dedupewider",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dedupewider Deduplication Across Multiple Columns Duplicated data can exist in different rows and columns and user may need to\n    treat observations (rows) connected by duplicated data as one observation,\n    e.g. companies can belong to one family (and thus: be one company) by sharing\n    some telephone numbers. This package allows to find connected rows\n    based on data on chosen columns and collapse it into one row.  "
  },
  {
    "id": 10969,
    "package_name": "deepnet",
    "title": "Deep Learning Toolkit in R",
    "description": "Implement some deep learning architectures and neural network\n    algorithms, including BP,RBM,DBN,Deep autoencoder and so on.",
    "version": "0.2.1",
    "maintainer": "Xiao Rong <runxiao@gmail.com>",
    "author": "Xiao Rong",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=deepnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deepnet Deep Learning Toolkit in R Implement some deep learning architectures and neural network\n    algorithms, including BP,RBM,DBN,Deep autoencoder and so on.  "
  },
  {
    "id": 11057,
    "package_name": "desk",
    "title": "Didactic Econometrics Starter Kit",
    "description": "Written to help undergraduate as well as graduate students to get started\n    with R for basic econometrics without the need to import specific functions\n    and datasets from many different sources. Primarily, the package is meant to \n    accompany the German textbook Auer, L.v., Hoffmann, S., Kranz, T. (2024,\n    ISBN: 978-3-662-68263-0) from which the exercises cover all the topics from the textbook\n    Auer, L.v. (2023, ISBN: 978-3-658-42699-6).",
    "version": "1.1.2",
    "maintainer": "Soenke Hoffmann <sohoffma@ovgu.de>",
    "author": "Soenke Hoffmann [cre, aut],\n  Tobias Kranz [aut]",
    "url": "https://github.com/OvGU-SH/desk",
    "bug_reports": "https://github.com/OvGU-SH/desk/issues",
    "repository": "https://cran.r-project.org/package=desk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "desk Didactic Econometrics Starter Kit Written to help undergraduate as well as graduate students to get started\n    with R for basic econometrics without the need to import specific functions\n    and datasets from many different sources. Primarily, the package is meant to \n    accompany the German textbook Auer, L.v., Hoffmann, S., Kranz, T. (2024,\n    ISBN: 978-3-662-68263-0) from which the exercises cover all the topics from the textbook\n    Auer, L.v. (2023, ISBN: 978-3-658-42699-6).  "
  },
  {
    "id": 11079,
    "package_name": "dexisensitivity",
    "title": "'DEXi' Decision Tree Analysis and Visualization",
    "description": "Provides a versatile toolkit for analyzing and visualizing 'DEXi' (Decision EXpert for education)\n    decision trees, facilitating multi-criteria decision analysis directly within R. Users can\n    read .dxi files, manipulate decision trees, and evaluate various scenarios. It supports sensitivity\n    analysis through Monte Carlo simulations, one-at-a-time approaches, and variance-based\n    methods, helping to discern the impact of input variations. Additionally, it includes functionalities\n    for generating sampling plans and an array of visualization options for decision trees and\n    analysis results. A distinctive feature is the synoptic table plot, aiding in the efficient\n    comparison of scenarios. Whether for in-depth decision modeling or sensitivity analysis, this\n    package stands as a comprehensive solution. Definition of sensitivity analyses available in\n    Carpani, Bergez and Monod (2012) <doi:10.1016/j.envsoft.2011.10.002> and detailed description of the package soon available in \n    Alaphilippe et al. (2025) <doi:10.1016/j.simpa.2024.100729>.",
    "version": "1.0.2",
    "maintainer": "Nicolas Cavan <nicolas.cavan@inrae.fr>",
    "author": "Roland Allart [aut],\n  Jacques-Eric Bergez [aut] (ORCID:\n    <https://orcid.org/0000-0003-3467-2617>),\n  Marta Carpani [aut],\n  Herv\u00e9 Monod [aut] (ORCID: <https://orcid.org/0000-0001-8225-495X>),\n  Aude Alaphilippe [ctb] (ORCID: <https://orcid.org/0000-0002-4504-2728>),\n  Nicolas Cavan [ctb, cre],\n  INRAE [cph] ((National Research Institute for Agriculture, Food and\n    Environment, France))",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dexisensitivity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dexisensitivity 'DEXi' Decision Tree Analysis and Visualization Provides a versatile toolkit for analyzing and visualizing 'DEXi' (Decision EXpert for education)\n    decision trees, facilitating multi-criteria decision analysis directly within R. Users can\n    read .dxi files, manipulate decision trees, and evaluate various scenarios. It supports sensitivity\n    analysis through Monte Carlo simulations, one-at-a-time approaches, and variance-based\n    methods, helping to discern the impact of input variations. Additionally, it includes functionalities\n    for generating sampling plans and an array of visualization options for decision trees and\n    analysis results. A distinctive feature is the synoptic table plot, aiding in the efficient\n    comparison of scenarios. Whether for in-depth decision modeling or sensitivity analysis, this\n    package stands as a comprehensive solution. Definition of sensitivity analyses available in\n    Carpani, Bergez and Monod (2012) <doi:10.1016/j.envsoft.2011.10.002> and detailed description of the package soon available in \n    Alaphilippe et al. (2025) <doi:10.1016/j.simpa.2024.100729>.  "
  },
  {
    "id": 11090,
    "package_name": "dfms",
    "title": "Dynamic Factor Models",
    "description": "Efficient estimation of Dynamic Factor Models using the Expectation Maximization (EM) algorithm \n  or Two-Step (2S) estimation, supporting datasets with missing data. Factors are assumed to follow a stationary VAR \n  process of order p. The estimation options follow advances in the econometric literature: either running the Kalman \n  Filter and Smoother once with initial values from PCA - 2S estimation as in Doz, Giannone and Reichlin (2011) \n  <doi:10.1016/j.jeconom.2011.02.012> - or via iterated Kalman Filtering and Smoothing until EM convergence - following \n  Doz, Giannone and Reichlin (2012) <doi:10.1162/REST_a_00225> - or using the adapted EM algorithm of Banbura and \n  Modugno (2014) <doi:10.1002/jae.2306>, allowing arbitrary patterns of missing data. The implementation makes heavy \n  use of the 'Armadillo' 'C++' library and the 'collapse' package, providing for particularly speedy estimation. \n  A comprehensive set of methods supports interpretation and visualization of the model as well as forecasting. \n  Information criteria to choose the number of factors are also provided - following Bai and Ng (2002) \n  <doi:10.1111/1468-0262.00273>.",
    "version": "0.3.2",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre],\n  Rytis Bagdziunas [aut],\n  Santtu Tikka [rev],\n  Eli Holmes [rev]",
    "url": "https://sebkrantz.github.io/dfms/,\nhttps://github.com/SebKrantz/dfms",
    "bug_reports": "https://github.com/SebKrantz/dfms/issues",
    "repository": "https://cran.r-project.org/package=dfms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dfms Dynamic Factor Models Efficient estimation of Dynamic Factor Models using the Expectation Maximization (EM) algorithm \n  or Two-Step (2S) estimation, supporting datasets with missing data. Factors are assumed to follow a stationary VAR \n  process of order p. The estimation options follow advances in the econometric literature: either running the Kalman \n  Filter and Smoother once with initial values from PCA - 2S estimation as in Doz, Giannone and Reichlin (2011) \n  <doi:10.1016/j.jeconom.2011.02.012> - or via iterated Kalman Filtering and Smoothing until EM convergence - following \n  Doz, Giannone and Reichlin (2012) <doi:10.1162/REST_a_00225> - or using the adapted EM algorithm of Banbura and \n  Modugno (2014) <doi:10.1002/jae.2306>, allowing arbitrary patterns of missing data. The implementation makes heavy \n  use of the 'Armadillo' 'C++' library and the 'collapse' package, providing for particularly speedy estimation. \n  A comprehensive set of methods supports interpretation and visualization of the model as well as forecasting. \n  Information criteria to choose the number of factors are also provided - following Bai and Ng (2002) \n  <doi:10.1111/1468-0262.00273>.  "
  },
  {
    "id": 11125,
    "package_name": "dichromat",
    "title": "Color Schemes for Dichromats",
    "description": "Collapse red-green or green-blue distinctions to simulate\n        the effects of different types of color-blindness.",
    "version": "2.0-0.1",
    "maintainer": "Thomas Lumley <tlumley@u.washington.edu>",
    "author": "Thomas Lumley [aut, cre],\n  Ken Knoblauch [ctb],\n  Scott Waichler [ctb],\n  Achim Zeileis [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dichromat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dichromat Color Schemes for Dichromats Collapse red-green or green-blue distinctions to simulate\n        the effects of different types of color-blindness.  "
  },
  {
    "id": 11166,
    "package_name": "dint",
    "title": "A Toolkit for Year-Quarter, Year-Month and Year-Isoweek Dates",
    "description": "S3 classes and methods to create and work\n    with year-quarter, year-month and year-isoweek vectors. Basic\n    arithmetic operations (such as adding and subtracting) are supported,\n    as well as formatting and converting to and from standard R date\n    types.",
    "version": "2.1.5",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>)",
    "url": "https://github.com/s-fleck/dint",
    "bug_reports": "https://github.com/s-fleck/dint/issues",
    "repository": "https://cran.r-project.org/package=dint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dint A Toolkit for Year-Quarter, Year-Month and Year-Isoweek Dates S3 classes and methods to create and work\n    with year-quarter, year-month and year-isoweek vectors. Basic\n    arithmetic operations (such as adding and subtracting) are supported,\n    as well as formatting and converting to and from standard R date\n    types.  "
  },
  {
    "id": 11168,
    "package_name": "dipsaus",
    "title": "A Dipping Sauce for Data Analysis and Visualizations",
    "description": "Works as an \"add-on\" to packages like 'shiny', 'future', as well as \n    'rlang', and provides utility functions. Just like dipping sauce adding \n    flavors to potato chips or pita bread, 'dipsaus' for data analysis and \n    visualizations adds handy functions and enhancements to popular packages. \n    The goal is to provide simple solutions that are frequently asked for \n    online, such as how to synchronize 'shiny' inputs without freezing the app,\n    or how to get memory size on 'Linux' or 'MacOS' system. The enhancements \n    roughly fall into these four categories: 1. 'shiny' input widgets; 2. \n    high-performance computing using the 'future' package; 3. \n    modify R calls and convert among numbers, strings, and other objects. 4. \n    utility functions to get system information such like CPU chip-set, memory \n    limit, etc.",
    "version": "0.3.2",
    "maintainer": "Zhengjia Wang <dipterix.wang@gmail.com>",
    "author": "Zhengjia Wang [aut, cre],\n  John Magnotti [ctb] (Contributed to `rutabaga.R`),\n  Xiang Zhang [ctb] (Contributed to `rutabaga.R`)",
    "url": "https://github.com/dipterix/dipsaus, https://dipterix.org/dipsaus/",
    "bug_reports": "https://github.com/dipterix/dipsaus/issues",
    "repository": "https://cran.r-project.org/package=dipsaus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dipsaus A Dipping Sauce for Data Analysis and Visualizations Works as an \"add-on\" to packages like 'shiny', 'future', as well as \n    'rlang', and provides utility functions. Just like dipping sauce adding \n    flavors to potato chips or pita bread, 'dipsaus' for data analysis and \n    visualizations adds handy functions and enhancements to popular packages. \n    The goal is to provide simple solutions that are frequently asked for \n    online, such as how to synchronize 'shiny' inputs without freezing the app,\n    or how to get memory size on 'Linux' or 'MacOS' system. The enhancements \n    roughly fall into these four categories: 1. 'shiny' input widgets; 2. \n    high-performance computing using the 'future' package; 3. \n    modify R calls and convert among numbers, strings, and other objects. 4. \n    utility functions to get system information such like CPU chip-set, memory \n    limit, etc.  "
  },
  {
    "id": 11203,
    "package_name": "disk.frame",
    "title": "Larger-than-RAM Disk-Based Data Manipulation Framework",
    "description": "A disk-based data manipulation tool for working with \n  large-than-RAM datasets. Aims to lower the barrier-to-entry for \n  manipulating large datasets by adhering closely to popular and \n  familiar data manipulation paradigms like 'dplyr' verbs and \n  'data.table' syntax.",
    "version": "0.8.3",
    "maintainer": "Dai ZJ <zhuojia.dai@gmail.com>",
    "author": "Dai ZJ [aut, cre],\n  Jacky Poon [ctb]",
    "url": "https://diskframe.com",
    "bug_reports": "https://github.com/DiskFrame/disk.frame/issues",
    "repository": "https://cran.r-project.org/package=disk.frame",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "disk.frame Larger-than-RAM Disk-Based Data Manipulation Framework A disk-based data manipulation tool for working with \n  large-than-RAM datasets. Aims to lower the barrier-to-entry for \n  manipulating large datasets by adhering closely to popular and \n  familiar data manipulation paradigms like 'dplyr' verbs and \n  'data.table' syntax.  "
  },
  {
    "id": 11241,
    "package_name": "dittoViz",
    "title": "User Friendly Data Visualization",
    "description": "A comprehensive visualization toolkit built with coders of all\n    skill levels and color-vision impaired audiences in mind. It allows creation\n    of finely-tuned, publication-quality figures from single function calls.\n    Visualizations include scatter plots, compositional bar plots, violin, box,\n    and ridge plots, and more. Customization ranges from size and title\n    adjustments to discrete-group circling and labeling, hidden data overlay\n    upon cursor hovering via ggplotly() conversion, and many more, all with\n    simple, discrete inputs. Color blindness friendliness is powered by legend\n    adjustments (enlarged keys), and by allowing the use of shapes or\n    letter-overlay in addition to the carefully selected dittoColors().",
    "version": "1.0.5",
    "maintainer": "Daniel Bunis <daniel.bunis@ucsf.edu>",
    "author": "Daniel Bunis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0010-1616>)",
    "url": "https://github.com/dtm2451/dittoViz",
    "bug_reports": "https://github.com/dtm2451/dittoViz/issues",
    "repository": "https://cran.r-project.org/package=dittoViz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dittoViz User Friendly Data Visualization A comprehensive visualization toolkit built with coders of all\n    skill levels and color-vision impaired audiences in mind. It allows creation\n    of finely-tuned, publication-quality figures from single function calls.\n    Visualizations include scatter plots, compositional bar plots, violin, box,\n    and ridge plots, and more. Customization ranges from size and title\n    adjustments to discrete-group circling and labeling, hidden data overlay\n    upon cursor hovering via ggplotly() conversion, and many more, all with\n    simple, discrete inputs. Color blindness friendliness is powered by legend\n    adjustments (enlarged keys), and by allowing the use of shapes or\n    letter-overlay in addition to the carefully selected dittoColors().  "
  },
  {
    "id": 11261,
    "package_name": "dlib",
    "title": "Allow Access to the 'Dlib' C++ Library",
    "description": "Interface for 'Rcpp' users to 'dlib' <http://dlib.net> which is a\n    'C++' toolkit containing machine learning algorithms and computer vision tools.\n    It is used in a wide range of domains including robotics, embedded devices,\n    mobile phones, and large high performance computing environments. This package\n    allows R users to use 'dlib' through 'Rcpp'.",
    "version": "1.0.3.1",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels, BNOSAC, Davis King and dlib package authors (see file AUTHORS)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dlib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dlib Allow Access to the 'Dlib' C++ Library Interface for 'Rcpp' users to 'dlib' <http://dlib.net> which is a\n    'C++' toolkit containing machine learning algorithms and computer vision tools.\n    It is used in a wide range of domains including robotics, embedded devices,\n    mobile phones, and large high performance computing environments. This package\n    allows R users to use 'dlib' through 'Rcpp'.  "
  },
  {
    "id": 11326,
    "package_name": "doolkit",
    "title": "Exploration of Dental Surface Topography",
    "description": "Tools for exploring the topography of 3d triangle meshes.\n    The functions were developed with dental surfaces in mind, but could be \n    applied to any triangle mesh of class 'mesh3d'. More specifically, 'doolkit'\n    allows to isolate the border of a mesh, or a subpart of the mesh using the \n    polygon networks method; crop a mesh; compute basic descriptors (elevation, \n    orientation, footprint area); compute slope, angularity and relief index\n    (Ungar and Williamson (2000) <https://palaeo-electronica.org/2000_1/gorilla/issue1_00.htm>;\n    Boyer (2008) <doi:10.1016/j.jhevol.2008.08.002>), inclination and occlusal\n    relief index or gamma (Guy et al. (2013) <doi:10.1371/journal.pone.0066142>),\n    OPC (Evans et al. (2007) <doi:10.1038/nature05433>), OPCR (Wilson et al. \n    (2012) <doi:10.1038/nature10880>), DNE (Bunn et al. (2011) <doi:10.1002/ajpa.21489>;\n    Pampush et al. (2016) <doi:10.1007/s10914-016-9326-0>), form factor (Horton \n    (1932) <doi:10.1029/TR013i001p00350>), basin elongation (Schum (1956) \n    <doi:10.1130/0016-7606(1956)67[597:EODSAS]2.0.CO;2>), lemniscate ratio \n    (Chorley et al; (1957) <doi:10.2475/ajs.255.2.138>), enamel-dentine distance\n    (Guy et al. (2015) <doi:10.1371/journal.pone.0138802>; Thiery et al. (2017) \n    <doi:10.3389/fphys.2017.00524>), absolute crown strength (Schwartz et al. \n    (2020) <doi:10.1098/rsbl.2019.0671>), relief rate (Thiery et al. (2019)\n    <doi:10.1002/ajpa.23916>) and area-relative curvature; draw cumulative\n    profiles of a topographic variable; and map a variable over a 3d triangle \n    mesh.",
    "version": "1.42.2",
    "maintainer": "Ghislain Thiery <ghislain.thiery@ntymail.com>",
    "author": "Ghislain Thiery [aut, cre],\n  Franck Guy [aut],\n  Vincent Lazzari [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=doolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doolkit Exploration of Dental Surface Topography Tools for exploring the topography of 3d triangle meshes.\n    The functions were developed with dental surfaces in mind, but could be \n    applied to any triangle mesh of class 'mesh3d'. More specifically, 'doolkit'\n    allows to isolate the border of a mesh, or a subpart of the mesh using the \n    polygon networks method; crop a mesh; compute basic descriptors (elevation, \n    orientation, footprint area); compute slope, angularity and relief index\n    (Ungar and Williamson (2000) <https://palaeo-electronica.org/2000_1/gorilla/issue1_00.htm>;\n    Boyer (2008) <doi:10.1016/j.jhevol.2008.08.002>), inclination and occlusal\n    relief index or gamma (Guy et al. (2013) <doi:10.1371/journal.pone.0066142>),\n    OPC (Evans et al. (2007) <doi:10.1038/nature05433>), OPCR (Wilson et al. \n    (2012) <doi:10.1038/nature10880>), DNE (Bunn et al. (2011) <doi:10.1002/ajpa.21489>;\n    Pampush et al. (2016) <doi:10.1007/s10914-016-9326-0>), form factor (Horton \n    (1932) <doi:10.1029/TR013i001p00350>), basin elongation (Schum (1956) \n    <doi:10.1130/0016-7606(1956)67[597:EODSAS]2.0.CO;2>), lemniscate ratio \n    (Chorley et al; (1957) <doi:10.2475/ajs.255.2.138>), enamel-dentine distance\n    (Guy et al. (2015) <doi:10.1371/journal.pone.0138802>; Thiery et al. (2017) \n    <doi:10.3389/fphys.2017.00524>), absolute crown strength (Schwartz et al. \n    (2020) <doi:10.1098/rsbl.2019.0671>), relief rate (Thiery et al. (2019)\n    <doi:10.1002/ajpa.23916>) and area-relative curvature; draw cumulative\n    profiles of a topographic variable; and map a variable over a 3d triangle \n    mesh.  "
  },
  {
    "id": 11351,
    "package_name": "dowser",
    "title": "B Cell Receptor Phylogenetics Toolkit",
    "description": "Provides a set of functions for inferring, visualizing, and analyzing B cell phylogenetic trees.\n    Provides methods to 1) reconstruct unmutated ancestral sequences,\n    2) build B cell phylogenetic trees using multiple methods,\n    3) visualize trees with metadata at the tips,\n    4) reconstruct intermediate sequences,\n    5) detect biased ancestor-descendant relationships among metadata types\n    Workflow examples available at documentation site (see URL).\n    Citations:\n    Hoehn et al (2022) <doi:10.1371/journal.pcbi.1009885>,\n    Hoehn et al (2021) <doi:10.1101/2021.01.06.425648>.",
    "version": "2.4.0",
    "maintainer": "Kenneth Hoehn <kenneth.b.hoehn@dartmouth.edu>",
    "author": "Kenneth Hoehn [aut, cre],\n  Cole Jensen [aut],\n  Jessie Fielding [aut],\n  Hunter Melton [aut],\n  Susanna Marquez [ctb],\n  Jason Vander Heiden [ctb],\n  Steven Kleinstein [aut, cph]",
    "url": "https://dowser.readthedocs.io",
    "bug_reports": "https://github.com/immcantation/dowser/issues",
    "repository": "https://cran.r-project.org/package=dowser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dowser B Cell Receptor Phylogenetics Toolkit Provides a set of functions for inferring, visualizing, and analyzing B cell phylogenetic trees.\n    Provides methods to 1) reconstruct unmutated ancestral sequences,\n    2) build B cell phylogenetic trees using multiple methods,\n    3) visualize trees with metadata at the tips,\n    4) reconstruct intermediate sequences,\n    5) detect biased ancestor-descendant relationships among metadata types\n    Workflow examples available at documentation site (see URL).\n    Citations:\n    Hoehn et al (2022) <doi:10.1371/journal.pcbi.1009885>,\n    Hoehn et al (2021) <doi:10.1101/2021.01.06.425648>.  "
  },
  {
    "id": 11392,
    "package_name": "droll",
    "title": "Analyze Roll Distributions",
    "description": "A toolkit for parsing dice notation, analyzing rolls,\n    calculating success probabilities, and plotting outcome distributions.",
    "version": "0.1.0",
    "maintainer": "C. Lente <clente@curso-r.com>",
    "author": "C. Lente [aut, cre],\n  Curso-R [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=droll",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "droll Analyze Roll Distributions A toolkit for parsing dice notation, analyzing rolls,\n    calculating success probabilities, and plotting outcome distributions.  "
  },
  {
    "id": 11403,
    "package_name": "drugsens",
    "title": "Automated Analysis of 'QuPath' Output Data and Metadata\nExtraction",
    "description": "A comprehensive toolkit for analyzing microscopy data output from 'QuPath' software. \n    Provides functionality for automated data processing, metadata extraction, and statistical \n    analysis of imaging results. The methodology implemented in this package is based on \n    Labrosse et al. (2024) <doi:10.1016/j.xpro.2024.103274> \"Protocol for quantifying drug \n    sensitivity in 3D patient-derived ovarian cancer models\", which describes the complete \n    workflow for drug sensitivity analysis in patient-derived cancer models.",
    "version": "0.1.0",
    "maintainer": "Flavio Lombardo <flavio.lombardo@unibas.ch>",
    "author": "Flavio Lombardo [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4853-6838>),\n  Ricardo Coelho [cph],\n  Ovarian Cancer Research [cph],\n  University of Basel and University Hospital Basel [cph]",
    "url": "https://git.scicore.unibas.ch/ovca-research/drugsens/",
    "bug_reports": "https://git.scicore.unibas.ch/ovca-research/drugsens/-/issues",
    "repository": "https://cran.r-project.org/package=drugsens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "drugsens Automated Analysis of 'QuPath' Output Data and Metadata\nExtraction A comprehensive toolkit for analyzing microscopy data output from 'QuPath' software. \n    Provides functionality for automated data processing, metadata extraction, and statistical \n    analysis of imaging results. The methodology implemented in this package is based on \n    Labrosse et al. (2024) <doi:10.1016/j.xpro.2024.103274> \"Protocol for quantifying drug \n    sensitivity in 3D patient-derived ovarian cancer models\", which describes the complete \n    workflow for drug sensitivity analysis in patient-derived cancer models.  "
  },
  {
    "id": 11404,
    "package_name": "drumr",
    "title": "Turn R into a Drum Machine",
    "description": "Includes various functions for playing drum sounds. beat() plays a drum sound\n  from one of the six included drum kits. tempo() sets spacing between calls to beat()\n  in bpm. Together the two functions can be used to create many different drum patterns. ",
    "version": "0.1.0",
    "maintainer": "James Martherus <james@martherus.com>",
    "author": "James Martherus [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=drumr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "drumr Turn R into a Drum Machine Includes various functions for playing drum sounds. beat() plays a drum sound\n  from one of the six included drum kits. tempo() sets spacing between calls to beat()\n  in bpm. Together the two functions can be used to create many different drum patterns.   "
  },
  {
    "id": 11436,
    "package_name": "dtlg",
    "title": "A Performance-Focused Package for Clinical Trial Tables",
    "description": "Create high-performance clinical reporting tables (TLGs) from\n    ADaM-like inputs. The package provides a consistent, programmatic API\n    to generate common tables such as demographics, adverse event incidence,\n    and laboratory summaries, using 'data.table' for fast aggregation over\n    large populations. Functions support flexible target-variable selection,\n    stratification by treatment, and customizable summary statistics, and\n    return tidy, machine-readable results ready to render with downstream\n    table/formatting packages in analysis pipelines.",
    "version": "0.0.2",
    "maintainer": "Ramiro Magno <ramiro.morgado@ascent.io>",
    "author": "Max Ebenezer-Brown [aut],\n  Max Norman [aut],\n  Xinye Li [aut],\n  Anja Peebles-Brown [aut],\n  Ramiro Magno [aut, cre]",
    "url": "https://AscentSoftware.github.io/dtlg/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dtlg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dtlg A Performance-Focused Package for Clinical Trial Tables Create high-performance clinical reporting tables (TLGs) from\n    ADaM-like inputs. The package provides a consistent, programmatic API\n    to generate common tables such as demographics, adverse event incidence,\n    and laboratory summaries, using 'data.table' for fast aggregation over\n    large populations. Functions support flexible target-variable selection,\n    stratification by treatment, and customizable summary statistics, and\n    return tidy, machine-readable results ready to render with downstream\n    table/formatting packages in analysis pipelines.  "
  },
  {
    "id": 11445,
    "package_name": "dtts",
    "title": "'data.table' Time-Series",
    "description": "High-frequency time-series support via 'nanotime' and 'data.table'.",
    "version": "0.1.3",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel and Leonardo Silvestri",
    "url": "",
    "bug_reports": "https://github.com/eddelbuettel/dtts/issues",
    "repository": "https://cran.r-project.org/package=dtts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dtts 'data.table' Time-Series High-frequency time-series support via 'nanotime' and 'data.table'.  "
  },
  {
    "id": 11556,
    "package_name": "easybio",
    "title": "Comprehensive Single-Cell Annotation and Transcriptomic Analysis\nToolkit",
    "description": "Provides a comprehensive toolkit for single-cell annotation with the 'CellMarker2.0' database (see Xia Li, Peng Wang, Yunpeng Zhang (2023) <doi: 10.1093/nar/gkac947>). Streamlines biological label assignment in single-cell RNA-seq data and facilitates transcriptomic analysis, including preparation of TCGA<https://portal.gdc.cancer.gov/> and GEO<https://www.ncbi.nlm.nih.gov/geo/> datasets, differential expression analysis and visualization of enrichment analysis results. Additional utility functions support various bioinformatics workflows. See Wei Cui (2024) <doi: 10.1101/2024.09.14.609619> for more details.",
    "version": "1.2.3",
    "maintainer": "Wei Cui <m2c.w@outlook.com>",
    "author": "Wei Cui [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-8315-5899>)",
    "url": "https://github.com/person-c/easybio",
    "bug_reports": "https://github.com/person-c/easybio/issues",
    "repository": "https://cran.r-project.org/package=easybio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easybio Comprehensive Single-Cell Annotation and Transcriptomic Analysis\nToolkit Provides a comprehensive toolkit for single-cell annotation with the 'CellMarker2.0' database (see Xia Li, Peng Wang, Yunpeng Zhang (2023) <doi: 10.1093/nar/gkac947>). Streamlines biological label assignment in single-cell RNA-seq data and facilitates transcriptomic analysis, including preparation of TCGA<https://portal.gdc.cancer.gov/> and GEO<https://www.ncbi.nlm.nih.gov/geo/> datasets, differential expression analysis and visualization of enrichment analysis results. Additional utility functions support various bioinformatics workflows. See Wei Cui (2024) <doi: 10.1101/2024.09.14.609619> for more details.  "
  },
  {
    "id": 11682,
    "package_name": "eegkit",
    "title": "Toolkit for Electroencephalography Data",
    "description": "Analysis and visualization tools for electroencephalography (EEG) data. Includes functions for (i) plotting EEG data, (ii) filtering EEG data, (iii) smoothing EEG data; (iv) frequency domain (Fourier) analysis of EEG data, (v) Independent Component Analysis of EEG data, and (vi) simulating event-related potential EEG data.",
    "version": "1.0-5",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "author": "Nathaniel E. Helwig [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eegkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eegkit Toolkit for Electroencephalography Data Analysis and visualization tools for electroencephalography (EEG) data. Includes functions for (i) plotting EEG data, (ii) filtering EEG data, (iii) smoothing EEG data; (iv) frequency domain (Fourier) analysis of EEG data, (v) Independent Component Analysis of EEG data, and (vi) simulating event-related potential EEG data.  "
  },
  {
    "id": 11683,
    "package_name": "eegkitdata",
    "title": "Electroencephalography Toolkit Datasets",
    "description": "Contains the example EEG data used in the package eegkit. Also contains code for easily creating larger EEG datasets from the EEG Database on the UCI Machine Learning Repository.",
    "version": "1.1",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "author": "Nathaniel E. Helwig <helwig@umn.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eegkitdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eegkitdata Electroencephalography Toolkit Datasets Contains the example EEG data used in the package eegkit. Also contains code for easily creating larger EEG datasets from the EEG Database on the UCI Machine Learning Repository.  "
  },
  {
    "id": 11693,
    "package_name": "effectplots",
    "title": "Effect Plots",
    "description": "High-performance implementation of various effect plots\n    useful for regression and probabilistic classification tasks.  The\n    package includes partial dependence plots (Friedman, 2021,\n    <doi:10.1214/aos/1013203451>), accumulated local effect plots and\n    M-plots (both from Apley and Zhu, 2016, <doi:10.1111/rssb.12377>), as\n    well as plots that describe the statistical associations between model\n    response and features.  It supports visualizations with either\n    'ggplot2' or 'plotly', and is compatible with most models, including\n    'Tidymodels', models wrapped in 'DALEX' explainers, or models with\n    case weights.",
    "version": "0.2.2",
    "maintainer": "Michael Mayer <mayermichael79@gmail.com>",
    "author": "Michael Mayer [aut, cre]",
    "url": "https://github.com/mayer79/effectplots",
    "bug_reports": "https://github.com/mayer79/effectplots/issues",
    "repository": "https://cran.r-project.org/package=effectplots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "effectplots Effect Plots High-performance implementation of various effect plots\n    useful for regression and probabilistic classification tasks.  The\n    package includes partial dependence plots (Friedman, 2021,\n    <doi:10.1214/aos/1013203451>), accumulated local effect plots and\n    M-plots (both from Apley and Zhu, 2016, <doi:10.1111/rssb.12377>), as\n    well as plots that describe the statistical associations between model\n    response and features.  It supports visualizations with either\n    'ggplot2' or 'plotly', and is compatible with most models, including\n    'Tidymodels', models wrapped in 'DALEX' explainers, or models with\n    case weights.  "
  },
  {
    "id": 11740,
    "package_name": "elfgen",
    "title": "Ecological Limit Function Model Generation and Analysis Toolkit",
    "description": "\n\tA toolset for generating Ecological Limit Function (ELF) models and evaluating potential species loss resulting from flow change, based on the 'elfgen' framework. ELFs describe the relation between aquatic species richness (fish or benthic macroinvertebrates) and stream size characteristics (streamflow or drainage area). Journal publications are available outlining framework methodology (Kleiner et al. (2020) <doi:10.1111/1752-1688.12876>) and application (Rapp et al. (2020) <doi:10.1111/1752-1688.12877>).",
    "version": "2.3.4",
    "maintainer": "Connor Brogan <connor.brogan@deq.virginia.gov>",
    "author": "Joseph Kleiner [aut] (ORCID: <https://orcid.org/0000-0003-4837-7678>),\n  Robert Burgholzer [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7290-4928>),\n  Connor Brogan [cre] (ORCID: <https://orcid.org/0000-0001-9377-1805>)",
    "url": "https://github.com/HARPgroup/elfgen",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=elfgen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "elfgen Ecological Limit Function Model Generation and Analysis Toolkit \n\tA toolset for generating Ecological Limit Function (ELF) models and evaluating potential species loss resulting from flow change, based on the 'elfgen' framework. ELFs describe the relation between aquatic species richness (fish or benthic macroinvertebrates) and stream size characteristics (streamflow or drainage area). Journal publications are available outlining framework methodology (Kleiner et al. (2020) <doi:10.1111/1752-1688.12876>) and application (Rapp et al. (2020) <doi:10.1111/1752-1688.12877>).  "
  },
  {
    "id": 11833,
    "package_name": "epanet2toolkit",
    "title": "Call 'EPANET' Functions to Simulate Pipe Networks",
    "description": "Enables simulation of water piping networks using 'EPANET'.\n    The package provides functions from the 'EPANET' programmer's toolkit as R\n    functions so that basic or customized simulations can be carried out from R.\n    The package uses 'EPANET' version 2.2 from Open Water Analytics\n    <https://github.com/OpenWaterAnalytics/EPANET/releases/tag/v2.2>.  ",
    "version": "1.0.8",
    "maintainer": "Bradley Eck <brad@bradeck.net>",
    "author": "Ernesto Arandia [aut],\n  Bradley Eck [aut, cre],\n  Lew Rossman [aut],\n  Michael Tryby [ctb],\n  Sam Hatchett [ctb],\n  Feng Shang [ctb],\n  James Uber [ctb],\n  Tom Taxon [ctb],\n  Hyoungmin Woo [ctb],\n  Jinduan Chen [ctb],\n  Yunier Soad [ctb],\n  Mike Kane [ctb],\n  Demetrios Eliades [ctb],\n  Will Furnass [ctb],\n  Steffen Macke [ctb],\n  Marios Kyriakou [ctb],\n  Elad Salomons [ctb],\n  Maurizio Cingi [ctb],\n  Bryant McDonnell [ctb],\n  Angela Marchi [ctb],\n  Markus Sunela [ctb],\n  Milad Ghiami [ctb],\n  IBM Corp. [cph],\n  Open Water Analytics [cph]",
    "url": "https://github.com/bradleyjeck/epanet2toolkit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=epanet2toolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epanet2toolkit Call 'EPANET' Functions to Simulate Pipe Networks Enables simulation of water piping networks using 'EPANET'.\n    The package provides functions from the 'EPANET' programmer's toolkit as R\n    functions so that basic or customized simulations can be carried out from R.\n    The package uses 'EPANET' version 2.2 from Open Water Analytics\n    <https://github.com/OpenWaterAnalytics/EPANET/releases/tag/v2.2>.    "
  },
  {
    "id": 11843,
    "package_name": "epidict",
    "title": "Epidemiology Data Dictionaries and Random Data Generators",
    "description": "The 'R4EPIs' project <https://r4epi.github.io/sitrep/> seeks\n    to provide a set of standardized tools for analysis of outbreak and\n    survey data in humanitarian aid settings. This package currently\n    provides standardized data dictionaries from Medecins Sans Frontieres\n    Operational Centre Amsterdam for outbreak scenarios (Acute Jaundice\n    Syndrome, Cholera, Diphtheria, Measles, Meningitis) and surveys\n    (Retrospective mortality and access to care, Malnutrition, Vaccination\n    coverage and Event Based Surveillance) - as described in the following\n    <https://scienceportal.msf.org/assets/standardised-mortality-surveys?utm_source=chatgpt.com>.\n    In addition, a data generator from these dictionaries is provided. It\n    is also possible to read in any Open Data Kit format data dictionary.",
    "version": "0.1.0",
    "maintainer": "Alexander Spina <aspina@appliedepi.org>",
    "author": "Alexander Spina [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8425-1867>),\n  Zhian N. Kamvar [aut] (ORCID: <https://orcid.org/0000-0003-1458-7108>),\n  Lukas Richter [aut],\n  Patrick Keating [aut],\n  Annick Lenglet [ctb],\n  Applied Epi Incorporated [cph],\n  Medecins Sans Frontieres Operational Centre Amsterdam [fnd]",
    "url": "https://github.com/R4EPI/epidict/,\nhttps://r4epi.github.io/epidict/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=epidict",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epidict Epidemiology Data Dictionaries and Random Data Generators The 'R4EPIs' project <https://r4epi.github.io/sitrep/> seeks\n    to provide a set of standardized tools for analysis of outbreak and\n    survey data in humanitarian aid settings. This package currently\n    provides standardized data dictionaries from Medecins Sans Frontieres\n    Operational Centre Amsterdam for outbreak scenarios (Acute Jaundice\n    Syndrome, Cholera, Diphtheria, Measles, Meningitis) and surveys\n    (Retrospective mortality and access to care, Malnutrition, Vaccination\n    coverage and Event Based Surveillance) - as described in the following\n    <https://scienceportal.msf.org/assets/standardised-mortality-surveys?utm_source=chatgpt.com>.\n    In addition, a data generator from these dictionaries is provided. It\n    is also possible to read in any Open Data Kit format data dictionary.  "
  },
  {
    "id": 11864,
    "package_name": "eplusr",
    "title": "A Toolkit for Using Whole Building Simulation Program\n'EnergyPlus'",
    "description": "A rich toolkit of using the whole building\n    simulation program 'EnergyPlus'(<https://energyplus.net>), which\n    enables programmatic navigation, modification of 'EnergyPlus' models\n    and makes it less painful to do parametric simulations and analysis.",
    "version": "0.16.3",
    "maintainer": "Hongyuan Jia <hongyuanjia@cqust.edu.cn>",
    "author": "Hongyuan Jia [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0075-8183>),\n  Adrian Chong [aut] (ORCID: <https://orcid.org/0000-0002-9486-4728>)",
    "url": "https://hongyuanjia.github.io/eplusr/,\nhttps://github.com/hongyuanjia/eplusr",
    "bug_reports": "https://github.com/hongyuanjia/eplusr/issues",
    "repository": "https://cran.r-project.org/package=eplusr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eplusr A Toolkit for Using Whole Building Simulation Program\n'EnergyPlus' A rich toolkit of using the whole building\n    simulation program 'EnergyPlus'(<https://energyplus.net>), which\n    enables programmatic navigation, modification of 'EnergyPlus' models\n    and makes it less painful to do parametric simulations and analysis.  "
  },
  {
    "id": 11950,
    "package_name": "ethiodate",
    "title": "Working with Ethiopian Dates",
    "description": "A robust and efficient solution for working with Ethiopian dates. It can seamlessly convert to and from Gregorian dates. \n    It is designed to be compatible with the 'tidyverse' data workflow, including plotting with 'ggplot2'.\n    It ensures lightning-fast computations by integrating high-performance 'C++' code through 'Rcpp' package.",
    "version": "0.2.0",
    "maintainer": "Gutama Girja Urago <girjagutama@gmail.com>",
    "author": "Gutama Girja Urago [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5588-2301>)",
    "url": "https://guturago.github.io/ethiodate/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ethiodate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ethiodate Working with Ethiopian Dates A robust and efficient solution for working with Ethiopian dates. It can seamlessly convert to and from Gregorian dates. \n    It is designed to be compatible with the 'tidyverse' data workflow, including plotting with 'ggplot2'.\n    It ensures lightning-fast computations by integrating high-performance 'C++' code through 'Rcpp' package.  "
  },
  {
    "id": 11955,
    "package_name": "etree",
    "title": "Classification and Regression with Structured and Mixed-Type\nData",
    "description": "Implementation of Energy Trees, a statistical model to perform \n    classification and regression with structured and mixed-type data. The\n    model has a similar structure to Conditional Trees, but brings in Energy\n    Statistics to test independence between variables that are possibly \n    structured and of different nature. Currently, the package covers functions\n    and graphs as structured covariates. It builds upon 'partykit' to\n    provide functionalities for fitting, printing, plotting, and predicting with\n    Energy Trees. Energy Trees are described in Giubilei et al. (2022) \n    <arXiv:2207.04430>. ",
    "version": "0.1.0",
    "maintainer": "Riccardo Giubilei <riccardogbl@gmail.com>",
    "author": "Riccardo Giubilei [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1674-4886>),\n  Tullia Padellini [aut],\n  Pierpaolo Brutti [aut],\n  Marco Brandi [ctb],\n  Gabriel Nespoli [ctb],\n  Torsten Hothorn [ctb] (ORCID: <https://orcid.org/0000-0001-8301-0471>,\n    (partykit author)),\n  Achim Zeileis [ctb] (ORCID: <https://orcid.org/0000-0003-0918-3766>,\n    (partykit author))",
    "url": "https://github.com/ricgbl/etree",
    "bug_reports": "https://github.com/ricgbl/etree/issues",
    "repository": "https://cran.r-project.org/package=etree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "etree Classification and Regression with Structured and Mixed-Type\nData Implementation of Energy Trees, a statistical model to perform \n    classification and regression with structured and mixed-type data. The\n    model has a similar structure to Conditional Trees, but brings in Energy\n    Statistics to test independence between variables that are possibly \n    structured and of different nature. Currently, the package covers functions\n    and graphs as structured covariates. It builds upon 'partykit' to\n    provide functionalities for fitting, printing, plotting, and predicting with\n    Energy Trees. Energy Trees are described in Giubilei et al. (2022) \n    <arXiv:2207.04430>.   "
  },
  {
    "id": 11967,
    "package_name": "eurodata",
    "title": "Fast and Easy Eurostat Data Import and Search",
    "description": "Interface to Eurostat\u2019s API (SDMX 2.1) with fast data.table-based import of\n    data, labels, and metadata. On top of the core functionality, data search and data\n    description/comparison functions are also provided.\n    Use <https://github.com/alekrutkowski/eurodata_codegen> \u2014 a point-and-click app for rapid and\n    easy generation of richly-commented R code \u2014 to import a Eurostat dataset or its subset\n    (based on the eurodata::importData() function).",
    "version": "1.7.0",
    "maintainer": "Aleksander Rutkowski <alek.rutkowski@gmail.com>",
    "author": "Aleksander Rutkowski [aut, cre]",
    "url": "https://github.com/alekrutkowski/eurodata/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eurodata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eurodata Fast and Easy Eurostat Data Import and Search Interface to Eurostat\u2019s API (SDMX 2.1) with fast data.table-based import of\n    data, labels, and metadata. On top of the core functionality, data search and data\n    description/comparison functions are also provided.\n    Use <https://github.com/alekrutkowski/eurodata_codegen> \u2014 a point-and-click app for rapid and\n    easy generation of richly-commented R code \u2014 to import a Eurostat dataset or its subset\n    (based on the eurodata::importData() function).  "
  },
  {
    "id": 12015,
    "package_name": "evtree",
    "title": "Evolutionary Learning of Globally Optimal Trees",
    "description": "Commonly used classification and regression tree methods like the CART algorithm\n             are recursive partitioning methods that build the model in a forward stepwise search.\n\t     Although this approach is known to be an efficient heuristic, the results of recursive\n\t     tree methods are only locally optimal, as splits are chosen to maximize homogeneity at\n\t     the next step only. An alternative way to search over the parameter space of trees is\n\t     to use global optimization methods like evolutionary algorithms. The 'evtree' package\n\t     implements an evolutionary algorithm for learning globally optimal classification and\n\t     regression trees in R. CPU and memory-intensive tasks are fully computed in C++ while\n\t     the 'partykit' package is leveraged to represent the resulting trees in R, providing\n\t     unified infrastructure for summaries, visualizations, and predictions.",
    "version": "1.0-8",
    "maintainer": "Thomas Grubinger <ThomasGrubinger@gmail.com>",
    "author": "Thomas Grubinger [aut, cre],\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>),\n  Karl-Peter Pfeiffer [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=evtree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evtree Evolutionary Learning of Globally Optimal Trees Commonly used classification and regression tree methods like the CART algorithm\n             are recursive partitioning methods that build the model in a forward stepwise search.\n\t     Although this approach is known to be an efficient heuristic, the results of recursive\n\t     tree methods are only locally optimal, as splits are chosen to maximize homogeneity at\n\t     the next step only. An alternative way to search over the parameter space of trees is\n\t     to use global optimization methods like evolutionary algorithms. The 'evtree' package\n\t     implements an evolutionary algorithm for learning globally optimal classification and\n\t     regression trees in R. CPU and memory-intensive tasks are fully computed in C++ while\n\t     the 'partykit' package is leveraged to represent the resulting trees in R, providing\n\t     unified infrastructure for summaries, visualizations, and predictions.  "
  },
  {
    "id": 12060,
    "package_name": "exploratory",
    "title": "A Tool for Large-Scale Exploratory Analyses",
    "description": "Conduct numerous exploratory analyses in an instant with a \n    point-and-click interface. With one simple command, this tool \n    launches a Shiny App on the local machine. Drag and drop variables \n    in a data set to categorize them as possible independent, \n    dependent, moderating, or mediating variables. Then run dozens \n    (or hundreds) of analyses instantly to uncover any statistically \n    significant relationships among variables. Any relationship \n    thus uncovered should be tested in follow-up studies. \n    This tool is designed only to facilitate exploratory \n    analyses and should NEVER be used for p-hacking. Many of \n    the functions used in this package are previous versions of functions\n    in the R Packages 'kim' and 'ezr'.\n    Selected References:\n    Chang et al. (2021) <https://CRAN.R-project.org/package=shiny>.\n    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.\n    Kim (2023) <https://jinkim.science/docs/kim.pdf>.\n    Kim (2021) <doi:10.5281/zenodo.4619237>.\n    Kim (2020) <https://CRAN.R-project.org/package=ezr>.\n    Simmons et al. (2011) <doi:10.1177/0956797611417632>\n    Tingley et al. (2019) <https://CRAN.R-project.org/package=mediation>.\n    Wickham et al. (2020) <https://CRAN.R-project.org/package=ggplot2>.",
    "version": "0.3.31",
    "maintainer": "Jin Kim <jin.m.kim@yale.edu>",
    "author": "Jin Kim [aut, cre] (ORCID: <https://orcid.org/0000-0002-5013-3958>)",
    "url": "https://exploratoryonly.com",
    "bug_reports": "https://github.com/jinkim3/exploratory/issues",
    "repository": "https://cran.r-project.org/package=exploratory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exploratory A Tool for Large-Scale Exploratory Analyses Conduct numerous exploratory analyses in an instant with a \n    point-and-click interface. With one simple command, this tool \n    launches a Shiny App on the local machine. Drag and drop variables \n    in a data set to categorize them as possible independent, \n    dependent, moderating, or mediating variables. Then run dozens \n    (or hundreds) of analyses instantly to uncover any statistically \n    significant relationships among variables. Any relationship \n    thus uncovered should be tested in follow-up studies. \n    This tool is designed only to facilitate exploratory \n    analyses and should NEVER be used for p-hacking. Many of \n    the functions used in this package are previous versions of functions\n    in the R Packages 'kim' and 'ezr'.\n    Selected References:\n    Chang et al. (2021) <https://CRAN.R-project.org/package=shiny>.\n    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.\n    Kim (2023) <https://jinkim.science/docs/kim.pdf>.\n    Kim (2021) <doi:10.5281/zenodo.4619237>.\n    Kim (2020) <https://CRAN.R-project.org/package=ezr>.\n    Simmons et al. (2011) <doi:10.1177/0956797611417632>\n    Tingley et al. (2019) <https://CRAN.R-project.org/package=mediation>.\n    Wickham et al. (2020) <https://CRAN.R-project.org/package=ggplot2>.  "
  },
  {
    "id": 12143,
    "package_name": "fabricatr",
    "title": "Imagine Your Data Before You Collect It",
    "description": "Helps you imagine your data before you collect it. Hierarchical data structures\n   and correlated data can be easily simulated, either from random number generators or\n   by resampling from existing data sources. This package is faster with 'data.table' and\n   'mvnfast' installed.",
    "version": "1.0.2",
    "maintainer": "Graeme Blair <graeme.blair@gmail.com>",
    "author": "Graeme Blair [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9164-2102>),\n  Jasper Cooper [aut] (ORCID: <https://orcid.org/0000-0002-8639-3188>),\n  Alexander Coppock [aut] (ORCID:\n    <https://orcid.org/0000-0002-5733-2386>),\n  Macartan Humphreys [aut] (ORCID:\n    <https://orcid.org/0000-0001-7029-2326>),\n  Aaron Rudkin [aut],\n  Neal Fultz [aut],\n  David C. Hall [ctb] (ORCID: <https://orcid.org/0000-0002-2193-0480>)",
    "url": "https://declaredesign.org/r/fabricatr/,\nhttps://github.com/DeclareDesign/fabricatr",
    "bug_reports": "https://github.com/DeclareDesign/fabricatr/issues",
    "repository": "https://cran.r-project.org/package=fabricatr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fabricatr Imagine Your Data Before You Collect It Helps you imagine your data before you collect it. Hierarchical data structures\n   and correlated data can be easily simulated, either from random number generators or\n   by resampling from existing data sources. This package is faster with 'data.table' and\n   'mvnfast' installed.  "
  },
  {
    "id": 12151,
    "package_name": "facmodCS",
    "title": "Cross-Section Factor Models",
    "description": "Linear cross-section factor model fitting with least-squares \n    and robust fitting the 'lmrobdetMM()' function from 'RobStatTM'; related\n    volatility, Value at Risk and Expected Shortfall risk and performance \n    attribution (factor-contributed vs idiosyncratic returns);\n    tabular displays of risk and performance reports;\n    factor model Monte Carlo. The package authors would like to thank Chicago\n    Research on Security Prices,LLC for the cross-section of about 300\n    CRSP stocks data (in the data.table object 'stocksCRSP', and S&P GLOBAL MARKET\n    INTELLIGENCE for contributing 14 factor scores (a.k.a \"alpha factors\".and\n    \"factor exposures\") fundamental data on the 300 companies in the data.table\n    object 'factorSPGMI'.  The 'stocksCRSP' and 'factorsSPGMI' data are not covered by\n    the GPL-2 license, are not provided as open source of any kind, and they are\n    not to be redistributed in any form.",
    "version": "1.0",
    "maintainer": "Mido Shammaa <midoshammaa@yahoo.com>",
    "author": "Mido Shammaa [aut, cre],\n  Doug Martin [ctb, aut],\n  Kirk Li [aut, ctb],\n  Avinash Acharya [ctb],\n  Lingjie Yi [ctb]",
    "url": "https://github.com/robustport/facmodCS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=facmodCS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "facmodCS Cross-Section Factor Models Linear cross-section factor model fitting with least-squares \n    and robust fitting the 'lmrobdetMM()' function from 'RobStatTM'; related\n    volatility, Value at Risk and Expected Shortfall risk and performance \n    attribution (factor-contributed vs idiosyncratic returns);\n    tabular displays of risk and performance reports;\n    factor model Monte Carlo. The package authors would like to thank Chicago\n    Research on Security Prices,LLC for the cross-section of about 300\n    CRSP stocks data (in the data.table object 'stocksCRSP', and S&P GLOBAL MARKET\n    INTELLIGENCE for contributing 14 factor scores (a.k.a \"alpha factors\".and\n    \"factor exposures\") fundamental data on the 300 companies in the data.table\n    object 'factorSPGMI'.  The 'stocksCRSP' and 'factorsSPGMI' data are not covered by\n    the GPL-2 license, are not provided as open source of any kind, and they are\n    not to be redistributed in any form.  "
  },
  {
    "id": 12232,
    "package_name": "fastbioclim",
    "title": "Scalable and Efficient Derivation of Bioclimatic Variables",
    "description": "Provides a high-performance framework for deriving bioclimatic and custom summary variables from \n    large-scale climate raster data. The package features a dual-backend architecture that intelligently switches \n    between fast in-memory processing for smaller datasets (via the 'terra' package) and a memory-safe tiled approach \n    for massive datasets that do not fit in RAM (via 'exactextractr' and 'Rfast'). The main functions, \n    derive_bioclim() and derive_statistics(), offer a unified interface with advanced options for \n    custom time periods and static indices, making it suitable for a wide range of ecological and \n    environmental modeling applications. A software note is in preparation. In the meantime, you can visit \n    the package website <https://gepinillab.github.io/fastbioclim/> to find tutorials in English and Spanish.",
    "version": "0.3.0",
    "maintainer": "Gonzalo E. Pinilla-Buitrago <gepinillab@gmail.com>",
    "author": "Gonzalo E. Pinilla-Buitrago [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0065-945X>),\n  Luis Osorio-Olvera [aut] (ORCID:\n    <https://orcid.org/0000-0003-0701-5398>)",
    "url": "https://gepinillab.github.io/fastbioclim/",
    "bug_reports": "https://github.com/gepinillab/fastbioclim/issues",
    "repository": "https://cran.r-project.org/package=fastbioclim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastbioclim Scalable and Efficient Derivation of Bioclimatic Variables Provides a high-performance framework for deriving bioclimatic and custom summary variables from \n    large-scale climate raster data. The package features a dual-backend architecture that intelligently switches \n    between fast in-memory processing for smaller datasets (via the 'terra' package) and a memory-safe tiled approach \n    for massive datasets that do not fit in RAM (via 'exactextractr' and 'Rfast'). The main functions, \n    derive_bioclim() and derive_statistics(), offer a unified interface with advanced options for \n    custom time periods and static indices, making it suitable for a wide range of ecological and \n    environmental modeling applications. A software note is in preparation. In the meantime, you can visit \n    the package website <https://gepinillab.github.io/fastbioclim/> to find tutorials in English and Spanish.  "
  },
  {
    "id": 12254,
    "package_name": "fastplyr",
    "title": "Fast Alternatives to 'tidyverse' Functions",
    "description": "A full set of fast data manipulation tools with a tidy\n    front-end and a fast back-end using 'collapse' and 'cheapr'.",
    "version": "0.9.91",
    "maintainer": "Nick Christofides <nick.christofides.r@gmail.com>",
    "author": "Nick Christofides [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9743-7342>)",
    "url": "",
    "bug_reports": "https://github.com/NicChr/fastplyr/issues",
    "repository": "https://cran.r-project.org/package=fastplyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastplyr Fast Alternatives to 'tidyverse' Functions A full set of fast data manipulation tools with a tidy\n    front-end and a fast back-end using 'collapse' and 'cheapr'.  "
  },
  {
    "id": 12267,
    "package_name": "fastverse",
    "title": "A Suite of High-Performance Packages for Statistics and Data\nManipulation",
    "description": "Easy installation, loading and management, of high-performance packages \n             for statistical computing and data manipulation in R. \n             The core 'fastverse' consists of 4 packages: 'data.table', 'collapse', \n             'kit' and 'magrittr', that jointly only depend on 'Rcpp'. \n             The 'fastverse' can be freely and permanently extended with \n             additional packages, both globally or for individual projects. \n             Separate package verses can also be created. Fast packages \n             for many common tasks such as time series, dates and times, strings, \n             spatial data, statistics, data serialization, larger-than-memory \n             processing, and compilation of R code are listed in the README file: \n             <https://github.com/fastverse/fastverse#suggested-extensions>.",
    "version": "0.3.4",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre],\n  Hadley Wickham [ctb]",
    "url": "https://fastverse.github.io/fastverse/,\nhttps://fastverse.r-universe.dev/, https://github.com/fastverse",
    "bug_reports": "https://github.com/fastverse/fastverse/issues",
    "repository": "https://cran.r-project.org/package=fastverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastverse A Suite of High-Performance Packages for Statistics and Data\nManipulation Easy installation, loading and management, of high-performance packages \n             for statistical computing and data manipulation in R. \n             The core 'fastverse' consists of 4 packages: 'data.table', 'collapse', \n             'kit' and 'magrittr', that jointly only depend on 'Rcpp'. \n             The 'fastverse' can be freely and permanently extended with \n             additional packages, both globally or for individual projects. \n             Separate package verses can also be created. Fast packages \n             for many common tasks such as time series, dates and times, strings, \n             spatial data, statistics, data serialization, larger-than-memory \n             processing, and compilation of R code are listed in the README file: \n             <https://github.com/fastverse/fastverse#suggested-extensions>.  "
  },
  {
    "id": 12278,
    "package_name": "fbar",
    "title": "An Extensible Approach to Flux Balance Analysis",
    "description": "A toolkit for Flux Balance Analysis and related\n    metabolic modeling techniques. Functions are provided for: parsing\n    models in tabular format, converting parsed metabolic models to input\n    formats for common linear programming solvers, and\n    evaluating and applying gene-protein-reaction mappings. In addition, there\n    are wrappers to parse a model, select a solver, find the metabolic fluxes,\n    and return the results applied to the original model. Compared to other\n    packages in this field, this package puts a much heavier focus on\n    providing reusable components that can be used in the design of new\n    implementation of new techniques, in particular those that involve large\n    parameter sweeps. For a background on the theory, see What is Flux Balance \n    Analysis <doi:10.1038/nbt.1614>.",
    "version": "0.6.0",
    "maintainer": "Max Conway <conway.max1@gmail.com>",
    "author": "Max Conway [aut, cre]",
    "url": "http://maxconway.github.io/fbar/,\nhttps://github.com/maxconway/fbar",
    "bug_reports": "https://github.com/maxconway/fbar/issues",
    "repository": "https://cran.r-project.org/package=fbar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fbar An Extensible Approach to Flux Balance Analysis A toolkit for Flux Balance Analysis and related\n    metabolic modeling techniques. Functions are provided for: parsing\n    models in tabular format, converting parsed metabolic models to input\n    formats for common linear programming solvers, and\n    evaluating and applying gene-protein-reaction mappings. In addition, there\n    are wrappers to parse a model, select a solver, find the metabolic fluxes,\n    and return the results applied to the original model. Compared to other\n    packages in this field, this package puts a much heavier focus on\n    providing reusable components that can be used in the design of new\n    implementation of new techniques, in particular those that involve large\n    parameter sweeps. For a background on the theory, see What is Flux Balance \n    Analysis <doi:10.1038/nbt.1614>.  "
  },
  {
    "id": 12354,
    "package_name": "ff",
    "title": "Memory-Efficient Storage of Large Data on Disk and Fast Access\nFunctions",
    "description": "The ff package provides data structures that are stored on\n\tdisk but behave (almost) as if they were in RAM by transparently \n\tmapping only a section (pagesize) in main memory - the effective \n\tvirtual memory consumption per ff object. ff supports R's standard \n\tatomic data types 'double', 'logical', 'raw' and 'integer' and \n\tnon-standard atomic types boolean (1 bit), quad (2 bit unsigned), \n\tnibble (4 bit unsigned), byte (1 byte signed with NAs), ubyte (1 byte \n\tunsigned), short (2 byte signed with NAs), ushort (2 byte unsigned), \n\tsingle (4 byte float with NAs). For example 'quad' allows efficient \n\tstorage of genomic data as an 'A','T','G','C' factor. The unsigned \n\ttypes support 'circular' arithmetic. There is also support for \n\tclose-to-atomic types 'factor', 'ordered', 'POSIXct', 'Date' and \n\tcustom close-to-atomic types. \n\tff not only has native C-support for vectors, matrices and arrays \n\twith flexible dimorder (major column-order, major row-order and \n\tgeneralizations for arrays). There is also a ffdf class not unlike \n\tdata.frames and import/export filters for csv files.\n\tff objects store raw data in binary flat files in native encoding,\n\tand complement this with metadata stored in R as physical and virtual\n\tattributes. ff objects have well-defined hybrid copying semantics, \n\twhich gives rise to certain performance improvements through \n\tvirtualization. ff objects can be stored and reopened across R \n\tsessions. ff files can be shared by multiple ff R objects \n\t(using different data en/de-coding schemes) in the same process \n\tor from multiple R processes to exploit parallelism. A wide choice of \n\tfinalizer options allows to work with 'permanent' files as well as \n\tcreating/removing 'temporary' ff files completely transparent to the \n\tuser. On certain OS/Filesystem combinations, creating the ff files\n\tworks without notable delay thanks to using sparse file allocation.\n\tSeveral access optimization techniques such as Hybrid Index \n\tPreprocessing and Virtualization are implemented to achieve good \n\tperformance even with large datasets, for example virtual matrix \n\ttranspose without touching a single byte on disk. Further, to reduce \n\tdisk I/O, 'logicals' and non-standard data types get stored native and \n\tcompact on binary flat files i.e. logicals take up exactly 2 bits to \n\trepresent TRUE, FALSE and NA. \n\tBeyond basic access functions, the ff package also provides \n\tcompatibility functions that facilitate writing code for ff and ram \n\tobjects and support for batch processing on ff objects (e.g. as.ram, \n\tas.ff, ffapply). ff interfaces closely with functionality from package \n\t'bit': chunked looping, fast bit operations and coercions between \n\tdifferent objects that can store subscript information ('bit', \n\t'bitwhich', ff 'boolean', ri range index, hi hybrid index). This allows\n\tto work interactively with selections of large datasets and quickly \n\tmodify selection criteria. \n\tFurther high-performance enhancements can be made available upon request. ",
    "version": "4.5.2",
    "maintainer": "Jens Oehlschl\u00e4gel <Jens.Oehlschlaegel@truecluster.com>",
    "author": "Daniel Adler [aut],\n  Christian Gl\u00e4ser [ctb],\n  Oleg Nenadic [ctb],\n  Jens Oehlschl\u00e4gel [aut, cre],\n  Martijn Schuemie [ctb],\n  Walter Zucchini [ctb]",
    "url": "https://github.com/truecluster/ff",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ff Memory-Efficient Storage of Large Data on Disk and Fast Access\nFunctions The ff package provides data structures that are stored on\n\tdisk but behave (almost) as if they were in RAM by transparently \n\tmapping only a section (pagesize) in main memory - the effective \n\tvirtual memory consumption per ff object. ff supports R's standard \n\tatomic data types 'double', 'logical', 'raw' and 'integer' and \n\tnon-standard atomic types boolean (1 bit), quad (2 bit unsigned), \n\tnibble (4 bit unsigned), byte (1 byte signed with NAs), ubyte (1 byte \n\tunsigned), short (2 byte signed with NAs), ushort (2 byte unsigned), \n\tsingle (4 byte float with NAs). For example 'quad' allows efficient \n\tstorage of genomic data as an 'A','T','G','C' factor. The unsigned \n\ttypes support 'circular' arithmetic. There is also support for \n\tclose-to-atomic types 'factor', 'ordered', 'POSIXct', 'Date' and \n\tcustom close-to-atomic types. \n\tff not only has native C-support for vectors, matrices and arrays \n\twith flexible dimorder (major column-order, major row-order and \n\tgeneralizations for arrays). There is also a ffdf class not unlike \n\tdata.frames and import/export filters for csv files.\n\tff objects store raw data in binary flat files in native encoding,\n\tand complement this with metadata stored in R as physical and virtual\n\tattributes. ff objects have well-defined hybrid copying semantics, \n\twhich gives rise to certain performance improvements through \n\tvirtualization. ff objects can be stored and reopened across R \n\tsessions. ff files can be shared by multiple ff R objects \n\t(using different data en/de-coding schemes) in the same process \n\tor from multiple R processes to exploit parallelism. A wide choice of \n\tfinalizer options allows to work with 'permanent' files as well as \n\tcreating/removing 'temporary' ff files completely transparent to the \n\tuser. On certain OS/Filesystem combinations, creating the ff files\n\tworks without notable delay thanks to using sparse file allocation.\n\tSeveral access optimization techniques such as Hybrid Index \n\tPreprocessing and Virtualization are implemented to achieve good \n\tperformance even with large datasets, for example virtual matrix \n\ttranspose without touching a single byte on disk. Further, to reduce \n\tdisk I/O, 'logicals' and non-standard data types get stored native and \n\tcompact on binary flat files i.e. logicals take up exactly 2 bits to \n\trepresent TRUE, FALSE and NA. \n\tBeyond basic access functions, the ff package also provides \n\tcompatibility functions that facilitate writing code for ff and ram \n\tobjects and support for batch processing on ff objects (e.g. as.ram, \n\tas.ff, ffapply). ff interfaces closely with functionality from package \n\t'bit': chunked looping, fast bit operations and coercions between \n\tdifferent objects that can store subscript information ('bit', \n\t'bitwhich', ff 'boolean', ri range index, hi hybrid index). This allows\n\tto work interactively with selections of large datasets and quickly \n\tmodify selection criteria. \n\tFurther high-performance enhancements can be made available upon request.   "
  },
  {
    "id": 12547,
    "package_name": "fmds",
    "title": "Multidimensional Scaling Development Kit",
    "description": "Multidimensional scaling (MDS) functions for various tasks that are beyond the beta stage and way past the alpha stage.\n             Currently, options are available for weights, restrictions, classical scaling or principal coordinate analysis, transformations (linear, power, Box-Cox, spline, ordinal), outlier mitigation (rdop), out-of-sample estimation (predict), negative dissimilarities, fast and faster executions with low memory footprints, penalized restrictions, cross-validation-based penalty selection, supplementary variable estimation (explain), additive constant estimation, mixed measurement level distance calculation, restricted classical scaling, etc. More will come in the future.\n             References. Busing (2024) \"A Simple Population Size Estimator for Local Minima Applied to Multidimensional Scaling\". Manuscript submitted for publication.\n             Busing (2025) \"Node Localization by Multidimensional Scaling with Iterative Majorization\". Manuscript submitted for publication.\n             Busing (2025) \"Faster Multidimensional Scaling\". Manuscript in preparation.\n             Barroso and Busing (2025) \"e-RDOP, Relative Density-Based Outlier Probabilities, Extended to Proximity Mapping\". Manuscript submitted for publication.",
    "version": "0.1.5",
    "maintainer": "Frank M.T.A. Busing <busing@fsw.leidenuniv.nl>",
    "author": "Frank M.T.A. Busing [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8062-538X>),\n  Juan Claramunt Gonzalez [aut, com] (ORCID:\n    <https://orcid.org/0009-0009-5387-6341>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fmds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fmds Multidimensional Scaling Development Kit Multidimensional scaling (MDS) functions for various tasks that are beyond the beta stage and way past the alpha stage.\n             Currently, options are available for weights, restrictions, classical scaling or principal coordinate analysis, transformations (linear, power, Box-Cox, spline, ordinal), outlier mitigation (rdop), out-of-sample estimation (predict), negative dissimilarities, fast and faster executions with low memory footprints, penalized restrictions, cross-validation-based penalty selection, supplementary variable estimation (explain), additive constant estimation, mixed measurement level distance calculation, restricted classical scaling, etc. More will come in the future.\n             References. Busing (2024) \"A Simple Population Size Estimator for Local Minima Applied to Multidimensional Scaling\". Manuscript submitted for publication.\n             Busing (2025) \"Node Localization by Multidimensional Scaling with Iterative Majorization\". Manuscript submitted for publication.\n             Busing (2025) \"Faster Multidimensional Scaling\". Manuscript in preparation.\n             Barroso and Busing (2025) \"e-RDOP, Relative Density-Based Outlier Probabilities, Extended to Proximity Mapping\". Manuscript submitted for publication.  "
  },
  {
    "id": 12580,
    "package_name": "foqat",
    "title": "Field Observation Quick Analysis Toolkit",
    "description": "Tools for quickly processing and analyzing \n\tfield observation data and air quality data. This \n\ttools contain functions that facilitate analysis \n\tin atmospheric chemistry (especially in ozone \n\tpollution). Some functions of time series are also \n\tapplicable to other fields. For detail please view \n\thomepage<https://github.com/tianshu129/foqat>.\n\tScientific Reference:\n\t1. The Hydroxyl Radical (OH) Reactivity: Roger Atkinson and Janet Arey (2003) <doi:10.1021/cr0206420>.\n\t2. Ozone Formation Potential (OFP): <http://ww2.arb.ca.gov/sites/default/files/barcu/regact/2009/mir2009/mir10.pdf>, Zhang et al.(2021) <doi:10.5194/acp-21-11053-2021>.\n\t3. Aerosol Formation Potential (AFP): Wenjing Wu et al. (2016) <doi:10.1016/j.jes.2016.03.025>.\n\t4. TUV model: <https://www2.acom.ucar.edu/modeling/tropospheric-ultraviolet-and-visible-tuv-radiation-model>.",
    "version": "2.0.8.2",
    "maintainer": "Tianshu Chen <tianshu129@163.com>",
    "author": "Tianshu Chen",
    "url": "https://github.com/tianshu129/foqat,\nhttps://tianshu129.github.io/foqat/",
    "bug_reports": "https://github.com/tianshu129/foqat/issues",
    "repository": "https://cran.r-project.org/package=foqat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "foqat Field Observation Quick Analysis Toolkit Tools for quickly processing and analyzing \n\tfield observation data and air quality data. This \n\ttools contain functions that facilitate analysis \n\tin atmospheric chemistry (especially in ozone \n\tpollution). Some functions of time series are also \n\tapplicable to other fields. For detail please view \n\thomepage<https://github.com/tianshu129/foqat>.\n\tScientific Reference:\n\t1. The Hydroxyl Radical (OH) Reactivity: Roger Atkinson and Janet Arey (2003) <doi:10.1021/cr0206420>.\n\t2. Ozone Formation Potential (OFP): <http://ww2.arb.ca.gov/sites/default/files/barcu/regact/2009/mir2009/mir10.pdf>, Zhang et al.(2021) <doi:10.5194/acp-21-11053-2021>.\n\t3. Aerosol Formation Potential (AFP): Wenjing Wu et al. (2016) <doi:10.1016/j.jes.2016.03.025>.\n\t4. TUV model: <https://www2.acom.ucar.edu/modeling/tropospheric-ultraviolet-and-visible-tuv-radiation-model>.  "
  },
  {
    "id": 12611,
    "package_name": "forestr",
    "title": "Ecosystem and Canopy Structural Complexity Metrics from LiDAR",
    "description": "Provides a toolkit for calculating forest and canopy structural complexity metrics from\n    terrestrial LiDAR (light detection and ranging). References:  Atkins et al. 2018 <doi:10.1111/2041-210X.13061>; Hardiman et al. 2013 <doi:10.3390/f4030537>;\n    Parker et al. 2004 <doi:10.1111/j.0021-8901.2004.00925.x>.",
    "version": "2.0.2",
    "maintainer": "Jeff Atkins <jwatkins6@vcu.edu>",
    "author": "Jeff Atkins [aut, cre],\n  Gil Bohrer [aut],\n  Robert Fahey [aut],\n  Brady Hardiman [aut],\n  Chrisopher Gough [aut],\n  Timothy Morin [aut],\n  Atticus Stovall [aut],\n  Naupaka Zimmerman [ctb, aut],\n  Chris Black [ctb]",
    "url": "https://github.com/atkinsjeff/forestr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=forestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "forestr Ecosystem and Canopy Structural Complexity Metrics from LiDAR Provides a toolkit for calculating forest and canopy structural complexity metrics from\n    terrestrial LiDAR (light detection and ranging). References:  Atkins et al. 2018 <doi:10.1111/2041-210X.13061>; Hardiman et al. 2013 <doi:10.3390/f4030537>;\n    Parker et al. 2004 <doi:10.1111/j.0021-8901.2004.00925.x>.  "
  },
  {
    "id": 12636,
    "package_name": "foundry",
    "title": "'Palantir Foundry' Software Development Kit",
    "description": "Interface to 'Palantir Foundry', including\n    reading and writing structured or unstructured datasets, and more\n    <https://www.palantir.com/platforms/foundry/>.",
    "version": "0.13.0",
    "maintainer": "Alexandre Guinaudeau <aguinaudeau@palantir.com>",
    "author": "Alexandre Guinaudeau [aut, cre],\n  Palantir Technologies [aut, cph]",
    "url": "https://github.com/palantir/palantir-r-sdk",
    "bug_reports": "https://github.com/palantir/palantir-r-sdk/issues",
    "repository": "https://cran.r-project.org/package=foundry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "foundry 'Palantir Foundry' Software Development Kit Interface to 'Palantir Foundry', including\n    reading and writing structured or unstructured datasets, and more\n    <https://www.palantir.com/platforms/foundry/>.  "
  },
  {
    "id": 12647,
    "package_name": "fplyr",
    "title": "Apply Functions to Blocks of Files",
    "description": "Read and process a large delimited file block by\n    block. A block consists of all the contiguous rows that have the same value\n    in the first field. The result can be returned as a list or a data.table,\n    or even directly printed to an output file.",
    "version": "1.3.0",
    "maintainer": "Federico Marotta <federico.marotta96@gmail.com>",
    "author": "Federico Marotta [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0174-3901>)",
    "url": "https://github.com/fmarotta/fplyr",
    "bug_reports": "https://github.com/fmarotta/fplyr/issues",
    "repository": "https://cran.r-project.org/package=fplyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fplyr Apply Functions to Blocks of Files Read and process a large delimited file block by\n    block. A block consists of all the contiguous rows that have the same value\n    in the first field. The result can be returned as a list or a data.table,\n    or even directly printed to an output file.  "
  },
  {
    "id": 12793,
    "package_name": "future.batchtools",
    "title": "A Future API for Parallel and Distributed Processing using\n'batchtools'",
    "description": "Implementation of the Future API <doi:10.32614/RJ-2021-048> on top of the 'batchtools' package.\n    This allows you to process futures, as defined by the 'future' package,\n    in parallel out of the box, not only on your local machine or ad-hoc\n    cluster of machines, but also via high-performance compute ('HPC') job\n    schedulers such as 'LSF', 'OpenLava', 'Slurm', 'SGE', and 'TORQUE' / 'PBS',\n    e.g. 'y <- future.apply::future_lapply(files, FUN = process)'.",
    "version": "0.21.0",
    "maintainer": "Henrik Bengtsson <henrikb@braju.com>",
    "author": "Henrik Bengtsson [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7579-5165>)",
    "url": "https://future.batchtools.futureverse.org,\nhttps://github.com/futureverse/future.batchtools",
    "bug_reports": "https://github.com/futureverse/future.batchtools/issues",
    "repository": "https://cran.r-project.org/package=future.batchtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "future.batchtools A Future API for Parallel and Distributed Processing using\n'batchtools' Implementation of the Future API <doi:10.32614/RJ-2021-048> on top of the 'batchtools' package.\n    This allows you to process futures, as defined by the 'future' package,\n    in parallel out of the box, not only on your local machine or ad-hoc\n    cluster of machines, but also via high-performance compute ('HPC') job\n    schedulers such as 'LSF', 'OpenLava', 'Slurm', 'SGE', and 'TORQUE' / 'PBS',\n    e.g. 'y <- future.apply::future_lapply(files, FUN = process)'.  "
  },
  {
    "id": 12814,
    "package_name": "g6R",
    "title": "Graph Visualisation Engine Widget for R and 'shiny' Apps",
    "description": "Create stunning network experiences powered by the 'G6' graph visualisation engine\n    'JavaScript' library <https://g6.antv.antgroup.com/en>. In 'shiny' mode, modify your\n    graph directly from the server function to dynamically interact with nodes and edges.\n    Select your favorite layout among 20 choices. 15 behaviors are available such as\n    interactive edge creation, collapse-expand and brush select.\n    17 plugins designed to improve the user experience such as a mini-map,\n    toolbars and grid lines. Customise the look and feel of your graph with comprehensive\n    options for nodes, edges and more.",
    "version": "0.5.0",
    "maintainer": "David Granjon <dgranjon@ymail.com>",
    "author": "David Granjon [aut, cre],\n  David Schoch [aut],\n  cynkra GmbH [fnd],\n  Bristol Myers Squibb [fnd]",
    "url": "https://github.com/cynkra/g6R, https://cynkra.github.io/g6R/",
    "bug_reports": "https://github.com/cynkra/g6R/issues",
    "repository": "https://cran.r-project.org/package=g6R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "g6R Graph Visualisation Engine Widget for R and 'shiny' Apps Create stunning network experiences powered by the 'G6' graph visualisation engine\n    'JavaScript' library <https://g6.antv.antgroup.com/en>. In 'shiny' mode, modify your\n    graph directly from the server function to dynamically interact with nodes and edges.\n    Select your favorite layout among 20 choices. 15 behaviors are available such as\n    interactive edge creation, collapse-expand and brush select.\n    17 plugins designed to improve the user experience such as a mini-map,\n    toolbars and grid lines. Customise the look and feel of your graph with comprehensive\n    options for nodes, edges and more.  "
  },
  {
    "id": 12825,
    "package_name": "gProfileR",
    "title": "Interface to the 'g:Profiler' Toolkit",
    "description": "This package has been deprecated and will not be updated. \n    New users should use the package 'gprofiler2' (<https://CRAN.R-project.org/package=gprofiler2>)\n    for up-to-date data and improved functionality.\n    Functional enrichment analysis, gene identifier conversion and\n    mapping homologous genes across related organisms via the 'g:Profiler' toolkit\n    (<https://biit.cs.ut.ee/gprofiler/>).",
    "version": "0.7.0",
    "maintainer": "Ivan Kuzmin <ivan.kuzmin@ut.ee>",
    "author": "Juri Reimand <juri.reimand@ut.ee>, Raivo Kolde\n    <rkolde@gmail.com>, Tambet Arak <tambet.arak@gmail.com>",
    "url": "",
    "bug_reports": "http://biit.cs.ut.ee/gprofiler/contact.cgi",
    "repository": "https://cran.r-project.org/package=gProfileR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gProfileR Interface to the 'g:Profiler' Toolkit This package has been deprecated and will not be updated. \n    New users should use the package 'gprofiler2' (<https://CRAN.R-project.org/package=gprofiler2>)\n    for up-to-date data and improved functionality.\n    Functional enrichment analysis, gene identifier conversion and\n    mapping homologous genes across related organisms via the 'g:Profiler' toolkit\n    (<https://biit.cs.ut.ee/gprofiler/>).  "
  },
  {
    "id": 12837,
    "package_name": "gWidgets2",
    "title": "Rewrite of gWidgets API for Simplified GUI Construction",
    "description": "Re-implementation of the 'gWidgets' API. The API is defined in this\n    package. A second, toolkit-specific package is required to use it. At this point only 'gWidgets2tcltk' is viable.",
    "version": "1.0-10",
    "maintainer": "John Verzani <jverzani@gmail.com>",
    "author": "John Verzani [aut, cre]",
    "url": "https://github.com/gWidgets3/gWidgets2",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gWidgets2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gWidgets2 Rewrite of gWidgets API for Simplified GUI Construction Re-implementation of the 'gWidgets' API. The API is defined in this\n    package. A second, toolkit-specific package is required to use it. At this point only 'gWidgets2tcltk' is viable.  "
  },
  {
    "id": 12838,
    "package_name": "gWidgets2tcltk",
    "title": "Toolkit Implementation of gWidgets2 for tcltk",
    "description": "Port of the 'gWidgets2' API for the 'tcltk' package.",
    "version": "1.0-9",
    "maintainer": "John Verzani <jverzani@gmail.com>",
    "author": "John Verzani [aut, cre]",
    "url": "https://github.com/gwidgets3/gWidgets2tcltk/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gWidgets2tcltk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gWidgets2tcltk Toolkit Implementation of gWidgets2 for tcltk Port of the 'gWidgets2' API for the 'tcltk' package.  "
  },
  {
    "id": 12954,
    "package_name": "gdm",
    "title": "Generalized Dissimilarity Modeling",
    "description": "A toolkit with functions to fit, plot, summarize, and apply Generalized Dissimilarity Models. Mokany K, Ware C, Woolley SNC, Ferrier S, Fitzpatrick MC (2022) <doi:10.1111/geb.13459> Ferrier S, Manion G, Elith J, Richardson K (2007) <doi:10.1111/j.1472-4642.2007.00341.x>.",
    "version": "1.6.0-7",
    "maintainer": "Matt Fitzpatrick <mfitzpatrick@umces.edu>",
    "author": "Matt Fitzpatrick [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1911-8407>),\n  Karel Mokany [aut] (ORCID: <https://orcid.org/0000-0003-4199-3697>),\n  Glenn Manion [aut],\n  Diego Nieto-Lugilde [aut] (ORCID:\n    <https://orcid.org/0000-0003-4135-2881>),\n  Simon Ferrier [aut] (ORCID: <https://orcid.org/0000-0001-7884-2388>),\n  Roozbeh Valavi [ctb],\n  Matthew Lisk [ctb],\n  Chris Ware [ctb],\n  Skip Woolley [ctb],\n  Tom Harwood [ctb]",
    "url": "https://mfitzpatrick.al.umces.edu/gdm/,\nhttps://github.com/fitzLab-AL/gdm/",
    "bug_reports": "https://github.com/fitzLab-AL/gdm/issues/",
    "repository": "https://cran.r-project.org/package=gdm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdm Generalized Dissimilarity Modeling A toolkit with functions to fit, plot, summarize, and apply Generalized Dissimilarity Models. Mokany K, Ware C, Woolley SNC, Ferrier S, Fitzpatrick MC (2022) <doi:10.1111/geb.13459> Ferrier S, Manion G, Elith J, Richardson K (2007) <doi:10.1111/j.1472-4642.2007.00341.x>.  "
  },
  {
    "id": 12959,
    "package_name": "gdxdt",
    "title": "IO for GAMS GDX Files using 'data.table'",
    "description": "Interfaces GAMS data (*.gdx) files with 'data.table's using the GAMS R package 'gdxrrw'. The 'gdxrrw' package is available on the GAMS wiki: <https://support.gams.com/doku.php?id=gdxrrw:interfacing_gams_and_r>.",
    "version": "0.1.0",
    "maintainer": "Alois Dirnaichner <alodi@directbox.com>",
    "author": "Alois Dirnaichner [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gdxdt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdxdt IO for GAMS GDX Files using 'data.table' Interfaces GAMS data (*.gdx) files with 'data.table's using the GAMS R package 'gdxrrw'. The 'gdxrrw' package is available on the GAMS wiki: <https://support.gams.com/doku.php?id=gdxrrw:interfacing_gams_and_r>.  "
  },
  {
    "id": 13005,
    "package_name": "genekitr",
    "title": "Gene Analysis Toolkit",
    "description": "Provides features for searching, converting, analyzing, plotting, and exporting data effortlessly by inputting feature IDs. Enables easy retrieval of feature information, conversion of ID types, gene enrichment analysis, publication-level figures, group interaction plotting, and result export in one Excel file for seamless sharing and communication.",
    "version": "1.2.8",
    "maintainer": "Yunze Liu <jieandze1314@gmail.com>",
    "author": "Yunze Liu [aut, cre]",
    "url": "https://www.genekitr.fun/",
    "bug_reports": "https://github.com/GangLiLab/genekitr/issues",
    "repository": "https://cran.r-project.org/package=genekitr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genekitr Gene Analysis Toolkit Provides features for searching, converting, analyzing, plotting, and exporting data effortlessly by inputting feature IDs. Enables easy retrieval of feature information, conversion of ID types, gene enrichment analysis, publication-level figures, group interaction plotting, and result export in one Excel file for seamless sharing and communication.  "
  },
  {
    "id": 13072,
    "package_name": "geojsonR",
    "title": "A GeoJson Processing Toolkit",
    "description": "Includes functions for processing GeoJson objects <https://en.wikipedia.org/wiki/GeoJSON> relying on 'RFC 7946' <https://datatracker.ietf.org/doc/html/rfc7946>. The geojson encoding is based on 'json11', a tiny JSON library for 'C++11' <https://github.com/dropbox/json11>. Furthermore, the source code is exported in R through the 'Rcpp' and 'RcppArmadillo' packages.",
    "version": "1.1.2",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>),\n  Dropbox Inc [cph]",
    "url": "https://github.com/mlampros/geojsonR",
    "bug_reports": "https://github.com/mlampros/geojsonR/issues",
    "repository": "https://cran.r-project.org/package=geojsonR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geojsonR A GeoJson Processing Toolkit Includes functions for processing GeoJson objects <https://en.wikipedia.org/wiki/GeoJSON> relying on 'RFC 7946' <https://datatracker.ietf.org/doc/html/rfc7946>. The geojson encoding is based on 'json11', a tiny JSON library for 'C++11' <https://github.com/dropbox/json11>. Furthermore, the source code is exported in R through the 'Rcpp' and 'RcppArmadillo' packages.  "
  },
  {
    "id": 13092,
    "package_name": "geos",
    "title": "Open Source Geometry Engine ('GEOS') R API",
    "description": "Provides an R API to the Open Source Geometry Engine\n  ('GEOS') library (<https://libgeos.org/>) and a vector format \n  with which to efficiently store 'GEOS' geometries. High-performance functions \n  to extract information from, calculate relationships between, and\n  transform geometries are provided. Finally, facilities to import \n  and export geometry vectors to other spatial formats are provided.",
    "version": "0.2.4",
    "maintainer": "Dewey Dunnington <dewey@fishandwhistle.net>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Edzer Pebesma [aut] (ORCID: <https://orcid.org/0000-0001-8049-7069>)",
    "url": "https://paleolimbot.github.io/geos/,\nhttps://github.com/paleolimbot/geos/",
    "bug_reports": "https://github.com/paleolimbot/geos/issues",
    "repository": "https://cran.r-project.org/package=geos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geos Open Source Geometry Engine ('GEOS') R API Provides an R API to the Open Source Geometry Engine\n  ('GEOS') library (<https://libgeos.org/>) and a vector format \n  with which to efficiently store 'GEOS' geometries. High-performance functions \n  to extract information from, calculate relationships between, and\n  transform geometries are provided. Finally, facilities to import \n  and export geometry vectors to other spatial formats are provided.  "
  },
  {
    "id": 13097,
    "package_name": "geospatialsuite",
    "title": "Comprehensive Geospatiotemporal Analysis and Multimodal\nIntegration Toolkit",
    "description": "A comprehensive toolkit for geospatiotemporal analysis\n    featuring 60+ vegetation indices, advanced raster visualization,\n    universal spatial mapping, water quality analysis, CDL crop analysis,\n    spatial interpolation, temporal analysis, and terrain analysis.\n    Designed for agricultural research, environmental monitoring, remote\n    sensing applications, and publication-quality mapping with support for\n    any geographic region and robust error handling. Methods include\n    vegetation indices calculations (Rouse et al. 1974), NDVI and enhanced\n    vegetation indices (Huete et al. 1997)\n    <doi:10.1016/S0034-4257(97)00104-1>, (Akanbi et al. 2024) \n    <doi:10.1007/s41651-023-00164-y>, spatial interpolation techniques\n    (Cressie 1993, ISBN:9780471002556), water quality indices (McFeeters\n    1996) <doi:10.1080/01431169608948714>, and crop data layer analysis\n    (USDA NASS 2024)\n    <https://www.nass.usda.gov/Research_and_Science/Cropland/>.  Funding:\n    This material is based upon financial support by the National Science\n    Foundation, EEC Division of Engineering Education and Centers, NSF\n    Engineering Research Center for Advancing Sustainable and Distributed\n    Fertilizer production (CASFER), NSF 20-553 Gen-4 Engineering Research\n    Centers award 2133576.",
    "version": "0.1.1",
    "maintainer": "Olatunde D. Akanbi <olatunde.akanbi@case.edu>",
    "author": "Olatunde D. Akanbi [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-7719-2619>),\n  Vibha Mandayam [aut] (ORCID: <https://orcid.org/0009-0008-8628-9904>),\n  Yinghui Wu [aut] (ORCID: <https://orcid.org/0000-0003-3991-5155>),\n  Jeffrey Yarus [aut] (ORCID: <https://orcid.org/0000-0002-9331-9568>),\n  Erika I. Barcelos [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-9273-8488>),\n  Roger H. French [aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6162-0532>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geospatialsuite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geospatialsuite Comprehensive Geospatiotemporal Analysis and Multimodal\nIntegration Toolkit A comprehensive toolkit for geospatiotemporal analysis\n    featuring 60+ vegetation indices, advanced raster visualization,\n    universal spatial mapping, water quality analysis, CDL crop analysis,\n    spatial interpolation, temporal analysis, and terrain analysis.\n    Designed for agricultural research, environmental monitoring, remote\n    sensing applications, and publication-quality mapping with support for\n    any geographic region and robust error handling. Methods include\n    vegetation indices calculations (Rouse et al. 1974), NDVI and enhanced\n    vegetation indices (Huete et al. 1997)\n    <doi:10.1016/S0034-4257(97)00104-1>, (Akanbi et al. 2024) \n    <doi:10.1007/s41651-023-00164-y>, spatial interpolation techniques\n    (Cressie 1993, ISBN:9780471002556), water quality indices (McFeeters\n    1996) <doi:10.1080/01431169608948714>, and crop data layer analysis\n    (USDA NASS 2024)\n    <https://www.nass.usda.gov/Research_and_Science/Cropland/>.  Funding:\n    This material is based upon financial support by the National Science\n    Foundation, EEC Division of Engineering Education and Centers, NSF\n    Engineering Research Center for Advancing Sustainable and Distributed\n    Fertilizer production (CASFER), NSF 20-553 Gen-4 Engineering Research\n    Centers award 2133576.  "
  },
  {
    "id": 13268,
    "package_name": "ggparty",
    "title": "'ggplot' Visualizations for the 'partykit' Package",
    "description": "Extends 'ggplot2' functionality to the 'partykit' package. 'ggparty' provides the necessary tools to create clearly structured and highly customizable visualizations for tree-objects of the class 'party'.",
    "version": "1.0.0.1",
    "maintainer": "Martin Borkovec <martin.borkovec@skyforge.at>",
    "author": "Martin Borkovec [aut, cre],\n  Niyaz Madin [aut],\n  Hadley Wickham [ctb],\n  Winston Chang [ctb],\n  Lionel Henry [ctb],\n  Thomas Lin Pedersen [ctb],\n  Kohske Takahashi [ctb],\n  Claus Wilke [ctb],\n  Kara Woo [ctb],\n  Hiroaki Yutani [ctb]",
    "url": "https://github.com/martin-borkovec/ggparty",
    "bug_reports": "https://github.com/martin-borkovec/ggparty/issues",
    "repository": "https://cran.r-project.org/package=ggparty",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggparty 'ggplot' Visualizations for the 'partykit' Package Extends 'ggplot2' functionality to the 'partykit' package. 'ggparty' provides the necessary tools to create clearly structured and highly customizable visualizations for tree-objects of the class 'party'.  "
  },
  {
    "id": 13422,
    "package_name": "glmertree",
    "title": "Generalized Linear Mixed Model Trees",
    "description": "Recursive partitioning based on (generalized) linear mixed models\n    (GLMMs) combining lmer()/glmer() from 'lme4' and lmtree()/glmtree() from \n    'partykit'. The fitting algorithm is described in more detail in Fokkema,\n    Smits, Zeileis, Hothorn & Kelderman (2018; <DOI:10.3758/s13428-017-0971-x>).\n    For detecting and modeling subgroups in growth curves with GLMM trees see\n    Fokkema & Zeileis (2024; <DOI:10.3758/s13428-024-02389-1>).",
    "version": "0.2-6",
    "maintainer": "Marjolein Fokkema <M.Fokkema@fsw.leidenuniv.nl>",
    "author": "Marjolein Fokkema [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9252-8325>),\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glmertree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glmertree Generalized Linear Mixed Model Trees Recursive partitioning based on (generalized) linear mixed models\n    (GLMMs) combining lmer()/glmer() from 'lme4' and lmtree()/glmtree() from \n    'partykit'. The fitting algorithm is described in more detail in Fokkema,\n    Smits, Zeileis, Hothorn & Kelderman (2018; <DOI:10.3758/s13428-017-0971-x>).\n    For detecting and modeling subgroups in growth curves with GLMM trees see\n    Fokkema & Zeileis (2024; <DOI:10.3758/s13428-024-02389-1>).  "
  },
  {
    "id": 13482,
    "package_name": "gmm",
    "title": "Generalized Method of Moments and Generalized Empirical\nLikelihood",
    "description": "It is a complete suite to estimate models based on moment conditions. It includes the two step Generalized method of moments (Hansen 1982; <doi:10.2307/1912775>), the iterated GMM and continuous updated estimator (Hansen, Eaton and Yaron 1996; <doi:10.2307/1392442>) and several methods that belong to the Generalized Empirical Likelihood family of estimators (Smith 1997; <doi:10.1111/j.0013-0133.1997.174.x>, Kitamura 1997; <doi:10.1214/aos/1069362388>, Newey and Smith 2004; <doi:10.1111/j.1468-0262.2004.00482.x>, and Anatolyev 2005 <doi:10.1111/j.1468-0262.2005.00601.x>).\t",
    "version": "1.9-1",
    "maintainer": "Pierre Chausse <pchausse@uwaterloo.ca>",
    "author": "Pierre Chausse [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gmm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gmm Generalized Method of Moments and Generalized Empirical\nLikelihood It is a complete suite to estimate models based on moment conditions. It includes the two step Generalized method of moments (Hansen 1982; <doi:10.2307/1912775>), the iterated GMM and continuous updated estimator (Hansen, Eaton and Yaron 1996; <doi:10.2307/1392442>) and several methods that belong to the Generalized Empirical Likelihood family of estimators (Smith 1997; <doi:10.1111/j.0013-0133.1997.174.x>, Kitamura 1997; <doi:10.1214/aos/1069362388>, Newey and Smith 2004; <doi:10.1111/j.1468-0262.2004.00482.x>, and Anatolyev 2005 <doi:10.1111/j.1468-0262.2005.00601.x>).\t  "
  },
  {
    "id": 13618,
    "package_name": "grattan",
    "title": "Australian Tax Policy Analysis",
    "description": "Utilities to cost and evaluate Australian tax policy, including fast\n    projections of personal income tax collections, high-performance tax and \n    transfer calculators, and an interface to common indices from the Australian\n    Bureau of Statistics.  Written to support Grattan Institute's Australian \n    Perspectives program, and related projects. Access to the Australian Taxation\n    Office's sample files of personal income tax returns is assumed. ",
    "version": "2025.5.0",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre],\n  Tim Cameron [aut],\n  Brendan Coates [aut],\n  Matthew Katzen [aut],\n  William Young [aut],\n  Ittima Cherastidtham [dtc],\n  W. Karsten [ctb],\n  M. Enrique Garcia [ctb],\n  Matt Cowgill [aut]",
    "url": "https://github.com/HughParsonage/grattan,\nhttps://hughparsonage.github.io/grattan/",
    "bug_reports": "https://github.com/HughParsonage/grattan/issues",
    "repository": "https://cran.r-project.org/package=grattan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grattan Australian Tax Policy Analysis Utilities to cost and evaluate Australian tax policy, including fast\n    projections of personal income tax collections, high-performance tax and \n    transfer calculators, and an interface to common indices from the Australian\n    Bureau of Statistics.  Written to support Grattan Institute's Australian \n    Perspectives program, and related projects. Access to the Australian Taxation\n    Office's sample files of personal income tax returns is assumed.   "
  },
  {
    "id": 13666,
    "package_name": "groupdata2",
    "title": "Creating Groups from Data",
    "description": "Methods for dividing data into groups. \n    Create balanced partitions and cross-validation folds. \n    Perform time series windowing and general grouping and splitting of data. \n    Balance existing groups with up- and downsampling or collapse them to fewer groups.",
    "version": "2.0.5",
    "maintainer": "Ludvig Renbo Olsen <r-pkgs@ludvigolsen.dk>",
    "author": "Ludvig Renbo Olsen [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-6798-7454>)",
    "url": "https://github.com/ludvigolsen/groupdata2",
    "bug_reports": "https://github.com/ludvigolsen/groupdata2/issues",
    "repository": "https://cran.r-project.org/package=groupdata2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "groupdata2 Creating Groups from Data Methods for dividing data into groups. \n    Create balanced partitions and cross-validation folds. \n    Perform time series windowing and general grouping and splitting of data. \n    Balance existing groups with up- and downsampling or collapse them to fewer groups.  "
  },
  {
    "id": 13692,
    "package_name": "grwat",
    "title": "River Hydrograph Separation and Analysis",
    "description": "River hydrograph separation and daily runoff time series analysis. Provides\n  various filters to separate baseflow and quickflow. Implements advanced separation \n  technique by Rets et al. (2022) <doi:10.1134/S0097807822010146> which involves \n  meteorological data to reveal genetic components of the runoff: ground, rain, thaw \n  and spring (seasonal thaw). High-performance C++17 computation, annually aggregated \n  variables, statistical testing and numerous plotting functions for high-quality \n  visualization.",
    "version": "0.1",
    "maintainer": "Timofey Samsonov <tsamsonov@geogr.msu.ru>",
    "author": "Timofey Samsonov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5994-0302>),\n  Ekaterina Rets [ctb] (ORCID: <https://orcid.org/0000-0002-4505-1173>),\n  Maria Kireeva [ctb] (ORCID: <https://orcid.org/0000-0002-8285-9761>)",
    "url": "https://github.com/tsamsonov/grwat,\nhttps://tsamsonov.github.io/grwat/",
    "bug_reports": "https://github.com/tsamsonov/grwat/issues",
    "repository": "https://cran.r-project.org/package=grwat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grwat River Hydrograph Separation and Analysis River hydrograph separation and daily runoff time series analysis. Provides\n  various filters to separate baseflow and quickflow. Implements advanced separation \n  technique by Rets et al. (2022) <doi:10.1134/S0097807822010146> which involves \n  meteorological data to reveal genetic components of the runoff: ground, rain, thaw \n  and spring (seasonal thaw). High-performance C++17 computation, annually aggregated \n  variables, statistical testing and numerous plotting functions for high-quality \n  visualization.  "
  },
  {
    "id": 13697,
    "package_name": "gsalib",
    "title": "Utility Functions for 'GATK'",
    "description": "Provides utility functions used by the Genome Analysis Toolkit ('GATK') to load tables and plot data. The 'GATK' is a toolkit for variant discovery in high-throughput sequencing data.",
    "version": "2.2.1",
    "maintainer": "Louis Bergelson <louisb@broadinstitute.org>",
    "author": "Kiran Garimella",
    "url": "https://gatk.broadinstitute.org/hc/en-us,\nhttps://github.com/broadinstitute/gatk,\nhttps://github.com/broadinstitute/gsalib/",
    "bug_reports": "https://github.com/broadinstitute/gsalib/issues",
    "repository": "https://cran.r-project.org/package=gsalib",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gsalib Utility Functions for 'GATK' Provides utility functions used by the Genome Analysis Toolkit ('GATK') to load tables and plot data. The 'GATK' is a toolkit for variant discovery in high-throughput sequencing data.  "
  },
  {
    "id": 13728,
    "package_name": "gtfs2gps",
    "title": "Converting Transport Data from GTFS Format to GPS-Like Records",
    "description": "Convert general transit feed specification (GTFS) data to global positioning system (GPS) records in 'data.table' format. It also has some functions to subset GTFS data in time and space and to convert both representations to simple feature format.",
    "version": "2.1-3",
    "maintainer": "Pedro R. Andrade <pedro.andrade@inpe.br>",
    "author": "Rafael H. M. Pereira [aut] (ORCID:\n    <https://orcid.org/0000-0003-2125-7465>),\n  Pedro R. Andrade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8675-4046>),\n  Joao Bazzo [aut] (ORCID: <https://orcid.org/0000-0003-4536-5006>),\n  Daniel Herszenhut [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8066-1105>),\n  Marcin Stepniak [ctb],\n  Marcus Saraiva [ctb] (ORCID: <https://orcid.org/0000-0001-6218-2338>),\n  Ipea - Institue for Applied Economic Research [cph, fnd]",
    "url": "https://github.com/ipeaGIT/gtfs2gps,\nhttps://ipeagit.github.io/gtfs2gps/",
    "bug_reports": "https://github.com/ipeaGIT/gtfs2gps/issues",
    "repository": "https://cran.r-project.org/package=gtfs2gps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gtfs2gps Converting Transport Data from GTFS Format to GPS-Like Records Convert general transit feed specification (GTFS) data to global positioning system (GPS) records in 'data.table' format. It also has some functions to subset GTFS data in time and space and to convert both representations to simple feature format.  "
  },
  {
    "id": 13752,
    "package_name": "gustave",
    "title": "A User-Oriented Statistical Toolkit for Analytical Variance\nEstimation",
    "description": "Provides a toolkit for analytical variance estimation in survey sampling. Apart from the implementation of standard variance estimators, its main feature is to help the sampling expert produce easy-to-use variance estimation \"wrappers\", where systematic operations (linearization, domain estimation) are handled in a consistent and transparent way.",
    "version": "1.0.0",
    "maintainer": "Khaled Larbi <khaled.larbi@insee.fr>",
    "author": "Martin Chevalier [aut] (Creator),\n  Khaled Larbi [cre],\n  Institut national de la statistique et des \u00e9tudes \u00e9conomiques [cph]",
    "url": "https://github.com/InseeFr/gustave",
    "bug_reports": "https://github.com/InseeFr/gustave/issues",
    "repository": "https://cran.r-project.org/package=gustave",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gustave A User-Oriented Statistical Toolkit for Analytical Variance\nEstimation Provides a toolkit for analytical variance estimation in survey sampling. Apart from the implementation of standard variance estimators, its main feature is to help the sampling expert produce easy-to-use variance estimation \"wrappers\", where systematic operations (linearization, domain estimation) are handled in a consistent and transparent way.  "
  },
  {
    "id": 13767,
    "package_name": "gym",
    "title": "Provides Access to the OpenAI Gym API",
    "description": "OpenAI Gym is a open-source Python toolkit for developing and comparing\n    reinforcement learning algorithms. This is a wrapper for the OpenAI Gym API,\n    and enables access to an ever-growing variety of environments.\n    For more details on OpenAI Gym, please see here: <https://github.com/openai/gym>.\n    For more details on the OpenAI Gym API specification, please see here:\n    <https://github.com/openai/gym-http-api>.",
    "version": "0.1.0",
    "maintainer": "Paul Hendricks <paul.hendricks.2013@owu.edu>",
    "author": "Paul Hendricks [aut, cre]",
    "url": "https://github.com/paulhendricks/gym-R",
    "bug_reports": "https://github.com/paulhendricks/gym-R/issues",
    "repository": "https://cran.r-project.org/package=gym",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gym Provides Access to the OpenAI Gym API OpenAI Gym is a open-source Python toolkit for developing and comparing\n    reinforcement learning algorithms. This is a wrapper for the OpenAI Gym API,\n    and enables access to an ever-growing variety of environments.\n    For more details on OpenAI Gym, please see here: <https://github.com/openai/gym>.\n    For more details on the OpenAI Gym API specification, please see here:\n    <https://github.com/openai/gym-http-api>.  "
  },
  {
    "id": 13823,
    "package_name": "hawkes",
    "title": "Hawkes process simulation and calibration toolkit",
    "description": "The package allows to simulate Hawkes process both in univariate and multivariate settings. It gives functions to compute different moments of the number of jumps of the process on a given interval, such as mean, variance or autocorrelation of process jumps on time intervals separated by a lag.",
    "version": "0.0-4",
    "maintainer": "Riadh Zaatour <zaatour_riadh@yahoo.fr>",
    "author": "Riadh Zaatour <zaatour_riadh@yahoo.fr>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hawkes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hawkes Hawkes process simulation and calibration toolkit The package allows to simulate Hawkes process both in univariate and multivariate settings. It gives functions to compute different moments of the number of jumps of the process on a given interval, such as mean, variance or autocorrelation of process jumps on time intervals separated by a lag.  "
  },
  {
    "id": 13852,
    "package_name": "hdd",
    "title": "Easy Manipulation of Out of Memory Data Sets",
    "description": "Hard drive data: Class of data allowing the easy importation/manipulation of out of memory data sets. The data sets are located on disk but look like in-memory, the syntax for manipulation is similar to 'data.table'. Operations are performed \"chunk-wise\" behind the scene.",
    "version": "0.1.1",
    "maintainer": "Laurent Berge <laurent.berge@u-bordeaux.fr>",
    "author": "Laurent Berge [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hdd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdd Easy Manipulation of Out of Memory Data Sets Hard drive data: Class of data allowing the easy importation/manipulation of out of memory data sets. The data sets are located on disk but look like in-memory, the syntax for manipulation is similar to 'data.table'. Operations are performed \"chunk-wise\" behind the scene.  "
  },
  {
    "id": 13862,
    "package_name": "hdnom",
    "title": "Benchmarking and Visualization Toolkit for Penalized Cox Models",
    "description": "Creates nomogram visualizations for penalized Cox regression\n    models, with the support of reproducible survival model building,\n    validation, calibration, and comparison for high-dimensional data.",
    "version": "6.1.0",
    "maintainer": "Nan Xiao <me@nanx.me>",
    "author": "Nan Xiao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0250-5673>),\n  Qing-Song Xu [aut],\n  Miao-Zhu Li [aut],\n  Frank Harrell [ctb] (rms author),\n  Sergej Potapov [ctb] (survAUC author),\n  Werner Adler [ctb] (survAUC author),\n  Matthias Schmid [ctb] (survAUC author)",
    "url": "https://nanx.me/hdnom/, https://github.com/nanxstats/hdnom",
    "bug_reports": "https://github.com/nanxstats/hdnom/issues",
    "repository": "https://cran.r-project.org/package=hdnom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdnom Benchmarking and Visualization Toolkit for Penalized Cox Models Creates nomogram visualizations for penalized Cox regression\n    models, with the support of reproducible survival model building,\n    validation, calibration, and comparison for high-dimensional data.  "
  },
  {
    "id": 13916,
    "package_name": "hesim",
    "title": "Health Economic Simulation Modeling and Decision Analysis",
    "description": "A modular and computationally efficient R package for  \n  parameterizing, simulating, and analyzing health economic simulation \n  models. The package supports cohort discrete time state transition models \n  (Briggs et al. 1998) <doi:10.2165/00019053-199813040-00003>,\n  N-state partitioned survival models (Glasziou et al. 1990)\n  <doi:10.1002/sim.4780091106>, and individual-level continuous \n  time state transition models (Siebert et al. 2012) <doi:10.1016/j.jval.2012.06.014>,\n  encompassing both Markov (time-homogeneous and time-inhomogeneous) and \n  semi-Markov processes. Decision uncertainty from a cost-effectiveness analysis is \n  quantified with standard graphical and tabular summaries of a probabilistic \n  sensitivity analysis (Claxton et al. 2005, Barton et al. 2008) <doi:10.1002/hec.985>, \n  <doi:10.1111/j.1524-4733.2008.00358.x>. Use of C++ and data.table\n  make individual-patient simulation, probabilistic sensitivity analysis, \n  and incorporation of patient heterogeneity fast.",
    "version": "0.5.7",
    "maintainer": "Devin Incerti <devin.incerti@gmail.com>",
    "author": "Devin Incerti [aut, cre],\n  Jeroen P. Jansen [aut],\n  Mark Clements [aut],\n  R Core Team [ctb] (hesim uses some slightly modified C functions from\n    base R)",
    "url": "https://hesim-dev.github.io/hesim/,\nhttps://github.com/hesim-dev/hesim",
    "bug_reports": "https://github.com/hesim-dev/hesim/issues",
    "repository": "https://cran.r-project.org/package=hesim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hesim Health Economic Simulation Modeling and Decision Analysis A modular and computationally efficient R package for  \n  parameterizing, simulating, and analyzing health economic simulation \n  models. The package supports cohort discrete time state transition models \n  (Briggs et al. 1998) <doi:10.2165/00019053-199813040-00003>,\n  N-state partitioned survival models (Glasziou et al. 1990)\n  <doi:10.1002/sim.4780091106>, and individual-level continuous \n  time state transition models (Siebert et al. 2012) <doi:10.1016/j.jval.2012.06.014>,\n  encompassing both Markov (time-homogeneous and time-inhomogeneous) and \n  semi-Markov processes. Decision uncertainty from a cost-effectiveness analysis is \n  quantified with standard graphical and tabular summaries of a probabilistic \n  sensitivity analysis (Claxton et al. 2005, Barton et al. 2008) <doi:10.1002/hec.985>, \n  <doi:10.1111/j.1524-4733.2008.00358.x>. Use of C++ and data.table\n  make individual-patient simulation, probabilistic sensitivity analysis, \n  and incorporation of patient heterogeneity fast.  "
  },
  {
    "id": 14108,
    "package_name": "hutils",
    "title": "Miscellaneous R Functions and Aliases",
    "description": "Provides utility functions for, and drawing on, the 'data.table' package. The package also collates useful miscellaneous functions extending base R not available elsewhere. The name is a portmanteau of 'utils' and the author.",
    "version": "1.8.1",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre],\n  Michael Frasco [ctb],\n  Ben Hamner [ctb]",
    "url": "https://github.com/hughparsonage/hutils,\nhttps://hughparsonage.github.io/hutils/",
    "bug_reports": "https://github.com/hughparsonage/hutils/issues",
    "repository": "https://cran.r-project.org/package=hutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hutils Miscellaneous R Functions and Aliases Provides utility functions for, and drawing on, the 'data.table' package. The package also collates useful miscellaneous functions extending base R not available elsewhere. The name is a portmanteau of 'utils' and the author.  "
  },
  {
    "id": 14172,
    "package_name": "iCellR",
    "title": "Analyzing High-Throughput Single Cell Sequencing Data",
    "description": "A toolkit that allows scientists to work with data from single cell sequencing technologies such as scRNA-seq, scVDJ-seq, scATAC-seq, CITE-Seq and Spatial Transcriptomics (ST). Single (i) Cell R package ('iCellR') provides unprecedented flexibility at every step of the analysis pipeline, including normalization, clustering, dimensionality reduction, imputation, visualization, and so on. Users can design both unsupervised and supervised models to best suit their research. In addition, the toolkit provides 2D and 3D interactive visualizations, differential expression analysis, filters based on cells, genes and clusters, data merging, normalizing for dropouts, data imputation methods, correcting for batch differences, pathway analysis, tools to find marker genes for clusters and conditions, predict cell types and pseudotime analysis. See Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.05.05.078550>  and Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.03.31.019109> for more details.",
    "version": "1.7.0",
    "maintainer": "Alireza Khodadadi-Jamayran <alireza.khodadadi.j@gmail.com>",
    "author": "Alireza Khodadadi-Jamayran [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2495-7504>),\n  Joseph Pucella [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-0875-8046>),\n  Hua Zhou [aut, ctb] (ORCID: <https://orcid.org/0000-0003-1822-1306>),\n  Nicole Doudican [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-3827-9644>),\n  John Carucci [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-6817-9439>),\n  Adriana Heguy [aut, ctb],\n  Boris Reizis [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-1140-7853>),\n  Aristotelis Tsirigos [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-7512-8477>)",
    "url": "https://github.com/rezakj/iCellR",
    "bug_reports": "https://github.com/rezakj/iCellR/issues",
    "repository": "https://cran.r-project.org/package=iCellR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iCellR Analyzing High-Throughput Single Cell Sequencing Data A toolkit that allows scientists to work with data from single cell sequencing technologies such as scRNA-seq, scVDJ-seq, scATAC-seq, CITE-Seq and Spatial Transcriptomics (ST). Single (i) Cell R package ('iCellR') provides unprecedented flexibility at every step of the analysis pipeline, including normalization, clustering, dimensionality reduction, imputation, visualization, and so on. Users can design both unsupervised and supervised models to best suit their research. In addition, the toolkit provides 2D and 3D interactive visualizations, differential expression analysis, filters based on cells, genes and clusters, data merging, normalizing for dropouts, data imputation methods, correcting for batch differences, pathway analysis, tools to find marker genes for clusters and conditions, predict cell types and pseudotime analysis. See Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.05.05.078550>  and Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.03.31.019109> for more details.  "
  },
  {
    "id": 14218,
    "package_name": "ibdfindr",
    "title": "HMM Toolkit for Inferring IBD Segments from SNP Genotypes",
    "description": "Implements continuous-time hidden Markov models (HMMs) to\n    infer identity-by-descent (IBD) segments shared by two individuals\n    from their single-nucleotide polymorphism (SNP) genotypes. Provides\n    posterior probabilities at each marker (forward-backward algorithm),\n    prediction of IBD segments (Viterbi algorithm), and functions for\n    visualising results. Supports both autosomal data and X-chromosomal\n    data.",
    "version": "0.3.1",
    "maintainer": "Magnus Dehli Vigeland <m.d.vigeland@medisin.uio.no>",
    "author": "Magnus Dehli Vigeland [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9134-4962>)",
    "url": "https://github.com/magnusdv/ibdfindr",
    "bug_reports": "https://github.com/magnusdv/ibdfindr/issues",
    "repository": "https://cran.r-project.org/package=ibdfindr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ibdfindr HMM Toolkit for Inferring IBD Segments from SNP Genotypes Implements continuous-time hidden Markov models (HMMs) to\n    infer identity-by-descent (IBD) segments shared by two individuals\n    from their single-nucleotide polymorphism (SNP) genotypes. Provides\n    posterior probabilities at each marker (forward-backward algorithm),\n    prediction of IBD segments (Viterbi algorithm), and functions for\n    visualising results. Supports both autosomal data and X-chromosomal\n    data.  "
  },
  {
    "id": 14224,
    "package_name": "ibmcraftr",
    "title": "Toolkits to Develop Individual-Based Models in Infectious\nDisease",
    "description": "It provides a generic set of tools for initializing a synthetic\n         population with each individual in specific disease states, and\n         making transitions between those disease states according to the rates\n         calculated on each timestep. The new version 1.0.0 has C++ code \n         integration to make the functions run faster. It has also a higher level\n         function to actually run the transitions for the number of timesteps\n         that users specify. Additional functions will follow for changing\n         attributes on demographic, health belief and movement.",
    "version": "1.0.0",
    "maintainer": "Sai Thein Than Tun <theinthantun.sai@gmail.com>",
    "author": "Sai Thein Than Tun [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ibmcraftr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ibmcraftr Toolkits to Develop Individual-Based Models in Infectious\nDisease It provides a generic set of tools for initializing a synthetic\n         population with each individual in specific disease states, and\n         making transitions between those disease states according to the rates\n         calculated on each timestep. The new version 1.0.0 has C++ code \n         integration to make the functions run faster. It has also a higher level\n         function to actually run the transitions for the number of timesteps\n         that users specify. Additional functions will follow for changing\n         attributes on demographic, health belief and movement.  "
  },
  {
    "id": 14293,
    "package_name": "ifaTools",
    "title": "Toolkit for Item Factor Analysis with 'OpenMx'",
    "description": "Tools, tutorials, and demos of Item Factor Analysis using 'OpenMx'.\n    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>.",
    "version": "0.23",
    "maintainer": "Joshua N. Pritikin <jpritikin@pobox.com>",
    "author": "Joshua N. Pritikin [cre, aut]",
    "url": "https://github.com/jpritikin/ifaTools",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ifaTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ifaTools Toolkit for Item Factor Analysis with 'OpenMx' Tools, tutorials, and demos of Item Factor Analysis using 'OpenMx'.\n    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>.  "
  },
  {
    "id": 14316,
    "package_name": "iheiddown",
    "title": "For Writing Geneva Graduate Institute Documents",
    "description": "A set of tools for writing documents\n    according to Geneva Graduate Institute conventions and regulations.\n    The most common use is for writing and compiling theses or thesis\n    chapters, as drafts or for examination with correct preamble formatting. \n    However, the package also offers users to create HTML presentation\n    slides with 'xaringan', complete problem sets, format posters, and, \n    for course instructors, prepare a syllabus.\n    The package includes additional functions for institutional color palettes,\n    an institutional 'ggplot' theme, a function for counting manuscript words,\n    and a bibliographical analysis toolkit.",
    "version": "0.9.7",
    "maintainer": "James Hollway <james.hollway@graduateinstitute.ch>",
    "author": "James Hollway [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8361-9647>),\n  Bernhard Bieri [ctb] (ORCID: <https://orcid.org/0000-0001-5943-9059>),\n  Henrique Sposito [ctb] (ORCID: <https://orcid.org/0000-0003-3420-6085>)",
    "url": "https://github.com/jhollway/iheiddown",
    "bug_reports": "https://github.com/jhollway/iheiddown/issues",
    "repository": "https://cran.r-project.org/package=iheiddown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iheiddown For Writing Geneva Graduate Institute Documents A set of tools for writing documents\n    according to Geneva Graduate Institute conventions and regulations.\n    The most common use is for writing and compiling theses or thesis\n    chapters, as drafts or for examination with correct preamble formatting. \n    However, the package also offers users to create HTML presentation\n    slides with 'xaringan', complete problem sets, format posters, and, \n    for course instructors, prepare a syllabus.\n    The package includes additional functions for institutional color palettes,\n    an institutional 'ggplot' theme, a function for counting manuscript words,\n    and a bibliographical analysis toolkit.  "
  },
  {
    "id": 14356,
    "package_name": "immunogenetr",
    "title": "A Comprehensive Toolkit for Clinical HLA Informatics",
    "description": "A comprehensive toolkit for clinical Human Leukocyte Antigen (HLA) informatics, built on 'tidyverse' <https://tidyverse.tidyverse.org/> principles and making use of genotype list string (GL string, Mack et al. (2023) <doi:10.1111/tan.15126>) for storing and computing HLA genotype data.  Specific functionalities include: coercion of HLA data in tabular format to and from GL string; calculation of matching and mismatching in all directions, with multiple output formats; automatic formatting of HLA data for searching within a GL string; truncation of molecular HLA data to a specific number of fields; and reading HLA genotypes in HML files and extracting the GL string. This library is intended for research use. Any application making use of this package in a clinical setting will need to be independently validated according to local regulations.",
    "version": "1.0.1",
    "maintainer": "Nicholas Brown <nicholas.brown@pennmedicine.upenn.edu>",
    "author": "Nicholas Brown [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-0046-2315>),\n  Busra Coskun [aut] (ORCID: <https://orcid.org/0009-0008-6828-3453>)",
    "url": "https://github.com/k96nb01/immunogenetr_package",
    "bug_reports": "https://github.com/k96nb01/immunogenetr_package/issues",
    "repository": "https://cran.r-project.org/package=immunogenetr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "immunogenetr A Comprehensive Toolkit for Clinical HLA Informatics A comprehensive toolkit for clinical Human Leukocyte Antigen (HLA) informatics, built on 'tidyverse' <https://tidyverse.tidyverse.org/> principles and making use of genotype list string (GL string, Mack et al. (2023) <doi:10.1111/tan.15126>) for storing and computing HLA genotype data.  Specific functionalities include: coercion of HLA data in tabular format to and from GL string; calculation of matching and mismatching in all directions, with multiple output formats; automatic formatting of HLA data for searching within a GL string; truncation of molecular HLA data to a specific number of fields; and reading HLA genotypes in HML files and extracting the GL string. This library is intended for research use. Any application making use of this package in a clinical setting will need to be independently validated according to local regulations.  "
  },
  {
    "id": 14381,
    "package_name": "imt",
    "title": "Impact Measurement Toolkit",
    "description": "A toolkit for causal inference in experimental and observational \n    studies. Implements various simple Bayesian models including linear, \n    negative binomial, and logistic regression for impact estimation. \n    Provides functionality for randomization and checking baseline equivalence \n    in experimental designs. The package aims to simplify the process of \n    impact measurement for researchers and analysts across different fields. \n    Examples and detailed usage instructions are available at \n    <https://book.martinez.fyi>.",
    "version": "1.0.0",
    "maintainer": "Ignacio Martinez <martinezig@google.com>",
    "author": "Ignacio Martinez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3721-8172>)",
    "url": "https://github.com/google/imt",
    "bug_reports": "https://github.com/google/imt/issues",
    "repository": "https://cran.r-project.org/package=imt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imt Impact Measurement Toolkit A toolkit for causal inference in experimental and observational \n    studies. Implements various simple Bayesian models including linear, \n    negative binomial, and logistic regression for impact estimation. \n    Provides functionality for randomization and checking baseline equivalence \n    in experimental designs. The package aims to simplify the process of \n    impact measurement for researchers and analysts across different fields. \n    Examples and detailed usage instructions are available at \n    <https://book.martinez.fyi>.  "
  },
  {
    "id": 14392,
    "package_name": "incidence",
    "title": "Compute, Handle, Plot and Model Incidence of Dated Events",
    "description": "Provides functions and classes to compute, handle and visualise\n  incidence from dated events for a defined time interval. Dates can be provided\n  in various standard formats. The class 'incidence' is used to store computed\n  incidence and can be easily manipulated, subsetted, and plotted. In addition,\n  log-linear models can be fitted to 'incidence' objects using 'fit'. This\n  package is part of the RECON (<https://www.repidemicsconsortium.org/>) toolkit\n  for outbreak analysis.",
    "version": "1.7.6",
    "maintainer": "Tim Taylor <tim.taylor@hiddenelephants.co.uk>",
    "author": "Thibaut Jombart [aut],\n  Zhian N. Kamvar [aut] (ORCID: <https://orcid.org/0000-0003-1458-7108>),\n  Rich FitzJohn [aut],\n  Tim Taylor [cre] (ORCID: <https://orcid.org/0000-0002-8587-7113>),\n  Jun Cai [ctb] (ORCID: <https://orcid.org/0000-0001-9495-1226>),\n  Sangeeta Bhatia [ctb],\n  Jakob Schumacher [ctb],\n  Juliet R.C. Pulliam [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3314-8223>)",
    "url": "https://www.repidemicsconsortium.org/incidence/",
    "bug_reports": "https://github.com/reconhub/incidence/issues",
    "repository": "https://cran.r-project.org/package=incidence",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "incidence Compute, Handle, Plot and Model Incidence of Dated Events Provides functions and classes to compute, handle and visualise\n  incidence from dated events for a defined time interval. Dates can be provided\n  in various standard formats. The class 'incidence' is used to store computed\n  incidence and can be easily manipulated, subsetted, and plotted. In addition,\n  log-linear models can be fitted to 'incidence' objects using 'fit'. This\n  package is part of the RECON (<https://www.repidemicsconsortium.org/>) toolkit\n  for outbreak analysis.  "
  },
  {
    "id": 14440,
    "package_name": "injurytools",
    "title": "A Toolkit for Sports Injury and Illness Data Analysis",
    "description": "Sports Injury Data analysis aims to identify and describe the\n    magnitude of the injury problem, and to gain more insights (e.g.\n    determine potential risk factors) by statistical modelling approaches.\n    The 'injurytools' package provides standardized routines and utilities\n    that simplify such analyses. It offers functions for data preparation,\n    informative visualizations and descriptive and model-based analyses.",
    "version": "2.0.0",
    "maintainer": "Lore Zumeta Olaskoaga <lorezumeta@gmail.com>",
    "author": "Lore Zumeta Olaskoaga [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6141-1469>),\n  Dae-Jin Lee [ctb] (ORCID: <https://orcid.org/0000-0002-8995-8535>)",
    "url": "https://github.com/lzumeta/injurytools,\nhttps://lzumeta.github.io/injurytools/",
    "bug_reports": "https://github.com/lzumeta/injurytools/issues",
    "repository": "https://cran.r-project.org/package=injurytools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "injurytools A Toolkit for Sports Injury and Illness Data Analysis Sports Injury Data analysis aims to identify and describe the\n    magnitude of the injury problem, and to gain more insights (e.g.\n    determine potential risk factors) by statistical modelling approaches.\n    The 'injurytools' package provides standardized routines and utilities\n    that simplify such analyses. It offers functions for data preparation,\n    informative visualizations and descriptive and model-based analyses.  "
  },
  {
    "id": 14484,
    "package_name": "interactions",
    "title": "Comprehensive, User-Friendly Toolkit for Probing Interactions",
    "description": "A suite of functions for conducting and interpreting analysis \n  of statistical interaction in regression models that was formerly part of the \n  'jtools' package. Functionality includes visualization of two- and three-way\n  interactions among continuous and/or categorical variables as well as \n  calculation of \"simple slopes\" and Johnson-Neyman intervals (see e.g., \n  Bauer & Curran, 2005 <doi:10.1207/s15327906mbr4003_5>). These\n  capabilities are implemented for generalized linear models in addition to the \n  standard linear regression context.",
    "version": "1.2.0",
    "maintainer": "Jacob A. Long <jacob.long@sc.edu>",
    "author": "Jacob A. Long [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1582-6214>)",
    "url": "https://interactions.jacob-long.com",
    "bug_reports": "https://github.com/jacob-long/interactions/issues",
    "repository": "https://cran.r-project.org/package=interactions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "interactions Comprehensive, User-Friendly Toolkit for Probing Interactions A suite of functions for conducting and interpreting analysis \n  of statistical interaction in regression models that was formerly part of the \n  'jtools' package. Functionality includes visualization of two- and three-way\n  interactions among continuous and/or categorical variables as well as \n  calculation of \"simple slopes\" and Johnson-Neyman intervals (see e.g., \n  Bauer & Curran, 2005 <doi:10.1207/s15327906mbr4003_5>). These\n  capabilities are implemented for generalized linear models in addition to the \n  standard linear regression context.  "
  },
  {
    "id": 14487,
    "package_name": "interface",
    "title": "Runtime Type System",
    "description": "Provides a runtime type system, allowing users to define and implement interfaces, enums, typed data.frame/data.table, as well as typed functions. This package enables stricter type checking and validation, improving code structure, robustness and reliability.",
    "version": "0.1.2",
    "maintainer": "Dereck Mezquita <dereck@mezquita.io>",
    "author": "Dereck Mezquita [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9307-6762>)",
    "url": "https://github.com/dereckmezquita/interface",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=interface",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "interface Runtime Type System Provides a runtime type system, allowing users to define and implement interfaces, enums, typed data.frame/data.table, as well as typed functions. This package enables stricter type checking and validation, improving code structure, robustness and reliability.  "
  },
  {
    "id": 14645,
    "package_name": "itol.toolkit",
    "title": "Helper Functions for 'Interactive Tree Of Life'",
    "description": "\n    The 'Interactive Tree Of Life' <https://itol.embl.de/> online server can \n    edit and annotate trees interactively. The 'itol.toolkit' package can \n    support all types of annotation templates.",
    "version": "1.1.12",
    "maintainer": "Tong Zhou <tongzhou2017@gmail.com>",
    "author": "Tong Zhou [aut, cre]",
    "url": "https://tongzhou2017.github.io/itol.toolkit/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=itol.toolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "itol.toolkit Helper Functions for 'Interactive Tree Of Life' \n    The 'Interactive Tree Of Life' <https://itol.embl.de/> online server can \n    edit and annotate trees interactively. The 'itol.toolkit' package can \n    support all types of annotation templates.  "
  },
  {
    "id": 14724,
    "package_name": "jmotif",
    "title": "Time Series Analysis Toolkit Based on Symbolic Aggregate\nDiscretization, i.e. SAX",
    "description": "Implements time series z-normalization, SAX, HOT-SAX, VSM, SAX-VSM, RePair, and RRA\n    algorithms facilitating time series motif (i.e., recurrent pattern), discord (i.e., anomaly),\n    and characteristic pattern discovery along with interpretable time series classification.",
    "version": "1.2.0",
    "maintainer": "Pavel Senin <seninp@gmail.com>",
    "author": "Pavel Senin [aut, cre]",
    "url": "https://github.com/jMotif/jmotif-R",
    "bug_reports": "https://github.com/jMotif/jmotif-R/issues",
    "repository": "https://cran.r-project.org/package=jmotif",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jmotif Time Series Analysis Toolkit Based on Symbolic Aggregate\nDiscretization, i.e. SAX Implements time series z-normalization, SAX, HOT-SAX, VSM, SAX-VSM, RePair, and RRA\n    algorithms facilitating time series motif (i.e., recurrent pattern), discord (i.e., anomaly),\n    and characteristic pattern discovery along with interpretable time series classification.  "
  },
  {
    "id": 14753,
    "package_name": "joyn",
    "title": "Tool for Diagnosis of Tables Joins and Complementary Join\nFeatures",
    "description": "Tool for diagnosing table joins. It combines the speed of `collapse`\n     and `data.table`, the flexibility of `dplyr`, and the diagnosis and features \n     of the `merge` command in `Stata`.",
    "version": "0.3.0",
    "maintainer": "R.Andres Castaneda <acastanedaa@worldbank.org>",
    "author": "R.Andres Castaneda [aut, cre],\n  Zander Prinsloo [aut],\n  Rossana Tatulli [aut]",
    "url": "https://github.com/randrescastaneda/joyn,\nhttps://randrescastaneda.github.io/joyn/",
    "bug_reports": "https://github.com/randrescastaneda/joyn/issues",
    "repository": "https://cran.r-project.org/package=joyn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "joyn Tool for Diagnosis of Tables Joins and Complementary Join\nFeatures Tool for diagnosing table joins. It combines the speed of `collapse`\n     and `data.table`, the flexibility of `dplyr`, and the diagnosis and features \n     of the `merge` command in `Stata`.  "
  },
  {
    "id": 14799,
    "package_name": "kairos",
    "title": "Analysis of Chronological Patterns from Archaeological Count\nData",
    "description": "A toolkit for absolute and relative dating and analysis of\n    chronological patterns. This package includes functions for\n    chronological modeling and dating of archaeological assemblages from\n    count data. It provides methods for matrix seriation. It also allows\n    to compute time point estimates and density estimates of the\n    occupation and duration of an archaeological site.",
    "version": "2.3.0",
    "maintainer": "Nicolas Frerebeau <nicolas.frerebeau@u-bordeaux-montaigne.fr>",
    "author": "Nicolas Frerebeau [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5759-4944>),\n  Brice Lebrun [art] (ORCID: <https://orcid.org/0000-0001-7503-8685>,\n    Logo designer),\n  Ben Marwick [ctb] (ORCID: <https://orcid.org/0000-0001-7879-4531>),\n  Anne Philippe [ctb] (ORCID: <https://orcid.org/0000-0002-5331-5087>),\n  Universit\u00e9 Bordeaux Montaigne [fnd] (ROR: <https://ror.org/03pbgwk21>),\n  CNRS [fnd] (ROR: <https://ror.org/02feahw73>)",
    "url": "https://codeberg.org/tesselle/kairos,\nhttps://packages.tesselle.org/kairos/,\nhttps://tesselle.r-universe.dev/kairos",
    "bug_reports": "https://codeberg.org/tesselle/kairos/issues",
    "repository": "https://cran.r-project.org/package=kairos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kairos Analysis of Chronological Patterns from Archaeological Count\nData A toolkit for absolute and relative dating and analysis of\n    chronological patterns. This package includes functions for\n    chronological modeling and dating of archaeological assemblages from\n    count data. It provides methods for matrix seriation. It also allows\n    to compute time point estimates and density estimates of the\n    occupation and duration of an archaeological site.  "
  },
  {
    "id": 14855,
    "package_name": "keyATM",
    "title": "Keyword Assisted Topic Models",
    "description": "Fits keyword assisted topic models (keyATM) using collapsed Gibbs samplers. The keyATM combines the latent dirichlet allocation (LDA) models with a small number of keywords selected by researchers in order to improve the interpretability and topic classification of the LDA. The keyATM can also incorporate covariates and directly model time trends. The keyATM is proposed in Eshima, Imai, and Sasaki (2024) <doi:10.1111/ajps.12779>.",
    "version": "0.5.4",
    "maintainer": "Shusei Eshima <shuseieshima@gmail.com>",
    "author": "Shusei Eshima [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3613-4046>),\n  Tomoya Sasaki [aut],\n  Kosuke Imai [aut],\n  Chung-hong Chan [ctb] (ORCID: <https://orcid.org/0000-0002-6232-7530>),\n  Romain Fran\u00e7ois [ctb] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Martin Feldkircher [ctb] (ORCID:\n    <https://orcid.org/0000-0002-5511-9215>),\n  William Lowe [ctb],\n  Seo-young Silvia Kim [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8801-9210>)",
    "url": "https://keyatm.github.io/keyATM/",
    "bug_reports": "https://github.com/keyATM/keyATM/issues",
    "repository": "https://cran.r-project.org/package=keyATM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "keyATM Keyword Assisted Topic Models Fits keyword assisted topic models (keyATM) using collapsed Gibbs samplers. The keyATM combines the latent dirichlet allocation (LDA) models with a small number of keywords selected by researchers in order to improve the interpretability and topic classification of the LDA. The keyATM can also incorporate covariates and directly model time trends. The keyATM is proposed in Eshima, Imai, and Sasaki (2024) <doi:10.1111/ajps.12779>.  "
  },
  {
    "id": 14879,
    "package_name": "kim",
    "title": "A Toolkit for Behavioral Scientists",
    "description": "A collection of functions for analyzing data typically collected \n    or used by behavioral scientists. Examples of the functions include\n    a function that compares groups in a factorial experimental design,\n    a function that conducts two-way analysis of variance (ANOVA),\n    and a function that cleans a data set generated by Qualtrics surveys.\n    Some of the functions will require installing additional package(s).\n    Such packages and other references are cited within the section\n    describing the relevant functions. Many functions in this package\n    rely heavily on these two popular R packages:\n    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.\n    Wickham et al. (2021) <https://CRAN.R-project.org/package=ggplot2>.",
    "version": "0.6.1",
    "maintainer": "Jin Kim <jinkim@aya.yale.edu>",
    "author": "Jin Kim [aut, cre] (ORCID: <https://orcid.org/0000-0002-5013-3958>)",
    "url": "https://github.com/jinkim3/kim, https://jinkim.science",
    "bug_reports": "https://github.com/jinkim3/kim/issues",
    "repository": "https://cran.r-project.org/package=kim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kim A Toolkit for Behavioral Scientists A collection of functions for analyzing data typically collected \n    or used by behavioral scientists. Examples of the functions include\n    a function that compares groups in a factorial experimental design,\n    a function that conducts two-way analysis of variance (ANOVA),\n    and a function that cleans a data set generated by Qualtrics surveys.\n    Some of the functions will require installing additional package(s).\n    Such packages and other references are cited within the section\n    describing the relevant functions. Many functions in this package\n    rely heavily on these two popular R packages:\n    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.\n    Wickham et al. (2021) <https://CRAN.R-project.org/package=ggplot2>.  "
  },
  {
    "id": 14893,
    "package_name": "kitagawa",
    "title": "Spectral Response of Water Wells to Harmonic Strain and Pressure\nSignals",
    "description": "Provides tools to calculate the theoretical hydrodynamic response\n    of an aquifer undergoing harmonic straining or pressurization, or analyze\n    measured responses. There are\n    two classes of models here, designed for use with confined\n    aquifers: (1) for sealed wells, based on the model of \n    Kitagawa et al (2011, <doi:10.1029/2010JB007794>), \n    and (2) for open wells, based on the models of\n    Cooper et al (1965, <doi:10.1029/JZ070i016p03915>), \n    Hsieh et al (1987, <doi:10.1029/WR023i010p01824>), \n    Rojstaczer (1988, <doi:10.1029/JB093iB11p13619>), \n    Liu et al (1989, <doi:10.1029/JB094iB07p09453>), and\n    Wang et al (2018, <doi:10.1029/2018WR022793>). Wang's \n    solution is a special exception which\n    allows for leakage out of the aquifer \n    (semi-confined); it is equivalent to Hsieh's model\n    when there is no leakage (the confined case).\n    These models treat strain (or aquifer head) as an input to the\n    physical system, and fluid-pressure (or water height) as the output. The\n    applicable frequency band of these models is characteristic of seismic\n    waves, atmospheric pressure fluctuations, and solid earth tides.",
    "version": "3.1.3",
    "maintainer": "Andrew J. Barbour <andy.barbour@gmail.com>",
    "author": "Andrew J. Barbour [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6890-2452>),\n  Jonathan Kennel [ctb] (ORCID: <https://orcid.org/0000-0003-4474-6886>)",
    "url": "https://github.com/abarbour/kitagawa",
    "bug_reports": "https://github.com/abarbour/kitagawa/issues",
    "repository": "https://cran.r-project.org/package=kitagawa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kitagawa Spectral Response of Water Wells to Harmonic Strain and Pressure\nSignals Provides tools to calculate the theoretical hydrodynamic response\n    of an aquifer undergoing harmonic straining or pressurization, or analyze\n    measured responses. There are\n    two classes of models here, designed for use with confined\n    aquifers: (1) for sealed wells, based on the model of \n    Kitagawa et al (2011, <doi:10.1029/2010JB007794>), \n    and (2) for open wells, based on the models of\n    Cooper et al (1965, <doi:10.1029/JZ070i016p03915>), \n    Hsieh et al (1987, <doi:10.1029/WR023i010p01824>), \n    Rojstaczer (1988, <doi:10.1029/JB093iB11p13619>), \n    Liu et al (1989, <doi:10.1029/JB094iB07p09453>), and\n    Wang et al (2018, <doi:10.1029/2018WR022793>). Wang's \n    solution is a special exception which\n    allows for leakage out of the aquifer \n    (semi-confined); it is equivalent to Hsieh's model\n    when there is no leakage (the confined case).\n    These models treat strain (or aquifer head) as an input to the\n    physical system, and fluid-pressure (or water height) as the output. The\n    applicable frequency band of these models is characteristic of seismic\n    waves, atmospheric pressure fluctuations, and solid earth tides.  "
  },
  {
    "id": 14894,
    "package_name": "kitesquare",
    "title": "Visualize Contingency Tables Using Kite-Square Plots",
    "description": "Create a kite-square plot for contingency tables using 'ggplot2', to display their relevant quantities in a single figure (marginal, conditional, expected, observed, chi-squared). The plot resembles a flying kite inside a square if the variables are independent, and deviates from this the more dependence exists.",
    "version": "0.0.2",
    "maintainer": "John Wiedenh\u00f6ft <john.wiedenhoeft@medizin.uni-leipzig.de>",
    "author": "John Wiedenh\u00f6ft [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6935-1517>)",
    "url": "https://github.com/HUGLeipzig/kitesquare",
    "bug_reports": "https://github.com/HUGLeipzig/kitesquare/issues",
    "repository": "https://cran.r-project.org/package=kitesquare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kitesquare Visualize Contingency Tables Using Kite-Square Plots Create a kite-square plot for contingency tables using 'ggplot2', to display their relevant quantities in a single figure (marginal, conditional, expected, observed, chi-squared). The plot resembles a flying kite inside a square if the variables are independent, and deviates from this the more dependence exists.  "
  },
  {
    "id": 14951,
    "package_name": "kronos",
    "title": "Microbiome Oriented Circadian Rhythm Analysis Toolkit",
    "description": "The goal of 'kronos' is to provide an easy-to-use framework to analyse circadian or otherwise rhythmic data using the familiar R linear modelling syntax, while taking care of the trigonometry under the hood. ",
    "version": "1.0.0",
    "maintainer": "Thomaz Bastiaanssen <thomazbastiaanssen@gmail.com>",
    "author": "Thomaz Bastiaanssen [aut, cre],\n  Sarah-Jane Leigh [aut]",
    "url": "https://github.com/thomazbastiaanssen/kronos",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kronos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kronos Microbiome Oriented Circadian Rhythm Analysis Toolkit The goal of 'kronos' is to provide an easy-to-use framework to analyse circadian or otherwise rhythmic data using the familiar R linear modelling syntax, while taking care of the trigonometry under the hood.   "
  },
  {
    "id": 14967,
    "package_name": "kuzuR",
    "title": "Interface to 'kuzu' Graph Database",
    "description": "Provides a high-performance 'R' interface to the 'kuzu' graph database.\n    It uses the 'reticulate' package to wrap the official 'Python' client\n    ('kuzu', 'pandas', and 'networkx'), allowing users to interact with 'kuzu'\n    seamlessly from within 'R'. Key features include managing database\n    connections, executing 'Cypher' queries, and efficiently loading data from\n    'R' data frames. It also provides seamless integration with the 'R' ecosystem\n    by converting query results directly into popular 'R' data structures,\n    including 'tibble', 'igraph', 'tidygraph', and 'g6R' objects, making\n    'kuzu's powerful graph computation capabilities readily available for data\n    analysis and visualization workflows in 'R'.\n    The 'kuzu' documentation can be found at <https://kuzudb.github.io/docs/>.",
    "version": "0.2.3",
    "maintainer": "Manuel Wick-Eckl <manuel.wick@gmail.com>",
    "author": "Manuel Wick-Eckl [aut, cre]",
    "url": "https://github.com/WickM/kuzuR, https://wickm.github.io/kuzuR/",
    "bug_reports": "https://github.com/WickM/kuzuR/issues",
    "repository": "https://cran.r-project.org/package=kuzuR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kuzuR Interface to 'kuzu' Graph Database Provides a high-performance 'R' interface to the 'kuzu' graph database.\n    It uses the 'reticulate' package to wrap the official 'Python' client\n    ('kuzu', 'pandas', and 'networkx'), allowing users to interact with 'kuzu'\n    seamlessly from within 'R'. Key features include managing database\n    connections, executing 'Cypher' queries, and efficiently loading data from\n    'R' data frames. It also provides seamless integration with the 'R' ecosystem\n    by converting query results directly into popular 'R' data structures,\n    including 'tibble', 'igraph', 'tidygraph', and 'g6R' objects, making\n    'kuzu's powerful graph computation capabilities readily available for data\n    analysis and visualization workflows in 'R'.\n    The 'kuzu' documentation can be found at <https://kuzudb.github.io/docs/>.  "
  },
  {
    "id": 15090,
    "package_name": "lda",
    "title": "Collapsed Gibbs Sampling Methods for Topic Models",
    "description": "Implements latent Dirichlet allocation (LDA)\n\t     and related models.  This includes (but is not limited\n\t     to) sLDA, corrLDA, and the mixed-membership stochastic\n\t     blockmodel.  Inference for all of these models is\n\t     implemented via a fast collapsed Gibbs sampler written\n\t     in C.  Utility functions for reading/writing data\n\t     typically used in topic models, as well as tools for\n\t     examining posterior distributions are also included.",
    "version": "1.5.2",
    "maintainer": "Santiago Olivella <olivella@unc.edu>",
    "author": "Jonathan Chang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lda Collapsed Gibbs Sampling Methods for Topic Models Implements latent Dirichlet allocation (LDA)\n\t     and related models.  This includes (but is not limited\n\t     to) sLDA, corrLDA, and the mixed-membership stochastic\n\t     blockmodel.  Inference for all of these models is\n\t     implemented via a fast collapsed Gibbs sampler written\n\t     in C.  Utility functions for reading/writing data\n\t     typically used in topic models, as well as tools for\n\t     examining posterior distributions are also included.  "
  },
  {
    "id": 15177,
    "package_name": "libgeos",
    "title": "Open Source Geometry Engine ('GEOS') C API",
    "description": "Provides the Open Source Geometry Engine ('GEOS') as a\n  C API that can be used to write high-performance C and C++\n  geometry operations using R as an interface. Headers are provided\n  to make linking to and using these functions from C++ code as\n  easy and as safe as possible. This package contains an internal\n  copy of the 'GEOS' library to guarantee the best possible\n  consistency on multiple platforms.",
    "version": "3.11.1-3",
    "maintainer": "Dewey Dunnington <dewey@fishandwhistle.net>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  GEOS authors [aut],\n  Martin Davis [ctb, cph],\n  Benjamin Campbell [ctb, cph],\n  Tomasz Sowa [ctb, cph],\n  Christian Kaiser [ctb, cph],\n  David Skea [ctb, cph],\n  Daniel Baston [ctb, cph],\n  Sandro Santilli [ctb, cph],\n  Mateusz Loskot [ctb, cph],\n  Paul Ramsey [ctb, cph],\n  Olivier Devillers [ctb, cph],\n  Sean Gillies [ctb, cph],\n  Mika Heiskanen [ctb, cph],\n  Safe Software Inc. [cph],\n  Refractions Research Inc. [cph],\n  Vivid Solutions Inc. [cph]",
    "url": "https://paleolimbot.github.io/libgeos/,\nhttps://github.com/paleolimbot/libgeos",
    "bug_reports": "https://github.com/paleolimbot/libgeos/issues",
    "repository": "https://cran.r-project.org/package=libgeos",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "libgeos Open Source Geometry Engine ('GEOS') C API Provides the Open Source Geometry Engine ('GEOS') as a\n  C API that can be used to write high-performance C and C++\n  geometry operations using R as an interface. Headers are provided\n  to make linking to and using these functions from C++ code as\n  easy and as safe as possible. This package contains an internal\n  copy of the 'GEOS' library to guarantee the best possible\n  consistency on multiple platforms.  "
  },
  {
    "id": 15223,
    "package_name": "lineupjs",
    "title": "'HTMLWidget' Wrapper of 'LineUp' for Visual Analysis of\nMulti-Attribute Rankings",
    "description": "'LineUp' is an interactive technique designed to create, visualize and explore rankings of items based on a set of heterogeneous attributes.\n  This is a 'htmlwidget' wrapper around the JavaScript library 'LineUp.js'.\n  It is designed to be used in 'R Shiny' apps and 'R Markddown' files.\n  Due to an outdated 'webkit' version of 'RStudio' it won't work in the integrated viewer.",
    "version": "4.6.0",
    "maintainer": "Samuel Gratzl <sam@sgratzl.com>",
    "author": "Samuel Gratzl [aut, cre]",
    "url": "https://github.com/lineupjs/lineup_htmlwidget/",
    "bug_reports": "https://github.com/lineupjs/lineup_htmlwidget/issues",
    "repository": "https://cran.r-project.org/package=lineupjs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lineupjs 'HTMLWidget' Wrapper of 'LineUp' for Visual Analysis of\nMulti-Attribute Rankings 'LineUp' is an interactive technique designed to create, visualize and explore rankings of items based on a set of heterogeneous attributes.\n  This is a 'htmlwidget' wrapper around the JavaScript library 'LineUp.js'.\n  It is designed to be used in 'R Shiny' apps and 'R Markddown' files.\n  Due to an outdated 'webkit' version of 'RStudio' it won't work in the integrated viewer.  "
  },
  {
    "id": 15237,
    "package_name": "lintools",
    "title": "Manipulation of Linear Systems of (in)Equalities",
    "description": "Variable elimination (Gaussian elimination, Fourier-Motzkin elimination), \n    Moore-Penrose pseudoinverse, reduction to reduced row echelon form, value substitution,  \n    projecting a vector on the convex polytope described by a system of (in)equations, \n    simplify systems by removing spurious columns and rows and collapse implied equalities, \n    test if a matrix is totally unimodular, compute variable ranges implied by linear\n    (in)equalities.",
    "version": "0.1.7",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [aut, cre],\n  Edwin de Jonge [aut]",
    "url": "https://github.com/data-cleaning/lintools",
    "bug_reports": "https://github.com/data-cleaning/lintools/issues",
    "repository": "https://cran.r-project.org/package=lintools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lintools Manipulation of Linear Systems of (in)Equalities Variable elimination (Gaussian elimination, Fourier-Motzkin elimination), \n    Moore-Penrose pseudoinverse, reduction to reduced row echelon form, value substitution,  \n    projecting a vector on the convex polytope described by a system of (in)equations, \n    simplify systems by removing spurious columns and rows and collapse implied equalities, \n    test if a matrix is totally unimodular, compute variable ranges implied by linear\n    (in)equalities.  "
  },
  {
    "id": 15264,
    "package_name": "liver",
    "title": "Toolkit and Datasets for Data Science",
    "description": "Provides a collection of helper functions and illustrative datasets to support learning and teaching of data science with R. The package is designed as a companion to the book <https://book-data-science-r.netlify.app>, making key data science techniques accessible to individuals with minimal coding experience. Functions include tools for data partitioning, performance evaluation, and data transformations (e.g., z-score and min-max scaling). The included datasets are curated to highlight practical applications in data exploration, modeling, and multivariate analysis. An early inspiration for the package came from an ancient Persian idiom about \"eating the liveR,\" symbolizing deep and immersive engagement with knowledge.",
    "version": "1.26",
    "maintainer": "Reza Mohammadi <a.mohammadi@uva.nl>",
    "author": "Reza Mohammadi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9538-0648>),\n  Kevin Burke [aut]",
    "url": "https://book-data-science-r.netlify.app",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=liver",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "liver Toolkit and Datasets for Data Science Provides a collection of helper functions and illustrative datasets to support learning and teaching of data science with R. The package is designed as a companion to the book <https://book-data-science-r.netlify.app>, making key data science techniques accessible to individuals with minimal coding experience. Functions include tools for data partitioning, performance evaluation, and data transformations (e.g., z-score and min-max scaling). The included datasets are curated to highlight practical applications in data exploration, modeling, and multivariate analysis. An early inspiration for the package came from an ancient Persian idiom about \"eating the liveR,\" symbolizing deep and immersive engagement with knowledge.  "
  },
  {
    "id": 15349,
    "package_name": "logiBin",
    "title": "Binning Variables to Use in Logistic Regression",
    "description": "Fast binning of multiple variables using parallel processing. A summary of all the variables binned is generated which provides the information value, entropy, an indicator of whether the variable follows a monotonic trend or not, etc. It supports rebinning of variables to force a monotonic trend as well as manual binning based on pre specified cuts. The cut points of the bins are based on conditional inference trees as implemented in the partykit package. The conditional inference framework is described by Hothorn T, Hornik K, Zeileis A (2006) <doi:10.1198/106186006X133933>.",
    "version": "0.3",
    "maintainer": "Sneha Tody <sn.tody1@gmail.com>",
    "author": "Sneha Tody",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=logiBin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "logiBin Binning Variables to Use in Logistic Regression Fast binning of multiple variables using parallel processing. A summary of all the variables binned is generated which provides the information value, entropy, an indicator of whether the variable follows a monotonic trend or not, etc. It supports rebinning of variables to force a monotonic trend as well as manual binning based on pre specified cuts. The cut points of the bins are based on conditional inference trees as implemented in the partykit package. The conditional inference framework is described by Hothorn T, Hornik K, Zeileis A (2006) <doi:10.1198/106186006X133933>.  "
  },
  {
    "id": 15394,
    "package_name": "loon",
    "title": "Interactive Statistical Data Visualization",
    "description": "An extendable toolkit for interactive data visualization and exploration.",
    "version": "1.4.3",
    "maintainer": "R. Wayne Oldford <rwoldford@uwaterloo.ca>",
    "author": "Adrian Waddell [aut],\n  R. Wayne Oldford [aut, cre, ths],\n  Zehao Xu [ctb],\n  Martin Gauch [ctb]",
    "url": "https://great-northern-diver.github.io/loon/",
    "bug_reports": "https://github.com/great-northern-diver/loon/issues",
    "repository": "https://cran.r-project.org/package=loon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "loon Interactive Statistical Data Visualization An extendable toolkit for interactive data visualization and exploration.  "
  },
  {
    "id": 15397,
    "package_name": "loon.shiny",
    "title": "Automatically Create a 'Shiny' App Based on Interactive 'Loon'\nWidgets",
    "description": "Package 'shiny' provides interactive web applications in R. Package 'loon' is an interactive toolkit engaged in open-ended, creative and unscripted data exploration. The 'loon.shiny' package can take 'loon' widgets and display a selfsame 'shiny' app. ",
    "version": "1.0.4",
    "maintainer": "Zehao Xu <z267xu@gmail.com>",
    "author": "Zehao Xu [aut, cre],\n  R. Wayne Oldford [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=loon.shiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "loon.shiny Automatically Create a 'Shiny' App Based on Interactive 'Loon'\nWidgets Package 'shiny' provides interactive web applications in R. Package 'loon' is an interactive toolkit engaged in open-ended, creative and unscripted data exploration. The 'loon.shiny' package can take 'loon' widgets and display a selfsame 'shiny' app.   "
  },
  {
    "id": 15490,
    "package_name": "mKBO",
    "title": "Multi-Group Kitagawa-Blinder-Oaxaca Decomposition",
    "description": "Provides multigroup Kitagawa-Blinder-Oaxaca ('mKBO') decompositions, that allow for more than two groups. Each group is compared to the sample average. For more details see Thaning and Nieuwenhuis (2025) <doi:10.31235/osf.io/6twvj_v1>.  ",
    "version": "0.1.0",
    "maintainer": "Rense Nieuwenhuis <rense.nieuwenhuis@sofi.su.se>",
    "author": "Rense Nieuwenhuis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-6138-0463>),\n  Max Thaning [aut] (ORCID: <https://orcid.org/0000-0002-1859-4703>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mKBO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mKBO Multi-Group Kitagawa-Blinder-Oaxaca Decomposition Provides multigroup Kitagawa-Blinder-Oaxaca ('mKBO') decompositions, that allow for more than two groups. Each group is compared to the sample average. For more details see Thaning and Nieuwenhuis (2025) <doi:10.31235/osf.io/6twvj_v1>.    "
  },
  {
    "id": 15516,
    "package_name": "maditr",
    "title": "Fast Data Aggregation, Modification, and Filtering with Pipes\nand 'data.table'",
    "description": "Provides pipe-style interface for 'data.table'. Package preserves all 'data.table' features without\n              significant impact on performance. 'let' and 'take' functions are simplified interfaces for most common data\n              manipulation tasks. For example, you can write 'take(mtcars, mean(mpg), by = am)' for aggregation or \n              'let(mtcars, hp_wt = hp/wt, hp_wt_mpg = hp_wt/mpg)' for modification. Use 'take_if/let_if' for conditional\n              aggregation/modification. Additionally there are some conveniences such as automatic 'data.frame' \n              conversion to 'data.table'.",
    "version": "0.8.6",
    "maintainer": "Gregory Demin <gdemin@gmail.com>",
    "author": "Gregory Demin [aut, cre]",
    "url": "https://github.com/gdemin/maditr",
    "bug_reports": "https://github.com/gdemin/maditr/issues",
    "repository": "https://cran.r-project.org/package=maditr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maditr Fast Data Aggregation, Modification, and Filtering with Pipes\nand 'data.table' Provides pipe-style interface for 'data.table'. Package preserves all 'data.table' features without\n              significant impact on performance. 'let' and 'take' functions are simplified interfaces for most common data\n              manipulation tasks. For example, you can write 'take(mtcars, mean(mpg), by = am)' for aggregation or \n              'let(mtcars, hp_wt = hp/wt, hp_wt_mpg = hp_wt/mpg)' for modification. Use 'take_if/let_if' for conditional\n              aggregation/modification. Additionally there are some conveniences such as automatic 'data.frame' \n              conversion to 'data.table'.  "
  },
  {
    "id": 15554,
    "package_name": "mallet",
    "title": "An R Wrapper for the Java Mallet Topic Modeling Toolkit",
    "description": "\n  An R interface for the Java Machine Learning for Language Toolkit (mallet)\n  <http://mallet.cs.umass.edu/> to estimate probabilistic topic models, such\n  as Latent Dirichlet Allocation. We can use the R package to read textual \n  data into mallet from R objects, run the Java implementation of mallet \n  directly in R, and extract results as R objects. The Mallet toolkit \n  has many functions, this wrapper focuses on the topic modeling sub-package \n  written by David Mimno. The package uses the rJava package to connect to a \n  JVM.",
    "version": "1.3.0",
    "maintainer": "M\u00e5ns Magnusson <mons.magnusson@gmail.com>",
    "author": "M\u00e5ns Magnusson [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-0296-2719>),\n  David Mimno [aut, cph] (ORCID: <https://orcid.org/0000-0001-7510-9404>)",
    "url": "https://github.com/mimno/RMallet",
    "bug_reports": "https://github.com/mimno/RMallet/issues",
    "repository": "https://cran.r-project.org/package=mallet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mallet An R Wrapper for the Java Mallet Topic Modeling Toolkit \n  An R interface for the Java Machine Learning for Language Toolkit (mallet)\n  <http://mallet.cs.umass.edu/> to estimate probabilistic topic models, such\n  as Latent Dirichlet Allocation. We can use the R package to read textual \n  data into mallet from R objects, run the Java implementation of mallet \n  directly in R, and extract results as R objects. The Mallet toolkit \n  has many functions, this wrapper focuses on the topic modeling sub-package \n  written by David Mimno. The package uses the rJava package to connect to a \n  JVM.  "
  },
  {
    "id": 15656,
    "package_name": "matchmaker",
    "title": "Flexible Dictionary-Based Cleaning",
    "description": "Provides flexible dictionary-based\n    cleaning that allows users to specify implicit and explicit missing data,\n    regular expressions for both data and columns, and global matches, while\n    respecting ordering of factors. This package is part of the 'RECON'\n    (<https://www.repidemicsconsortium.org/>) toolkit for outbreak analysis.",
    "version": "0.1.1",
    "maintainer": "Zhian N. Kamvar <zkamvar@gmail.com>",
    "author": "Zhian N. Kamvar [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1458-7108>),\n  Thibaut Jombart [ctb],\n  Patrick Barks [ctb]",
    "url": "https://www.repidemicsconsortium.org/matchmaker,\nhttps://github.com/reconhub/matchmaker",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=matchmaker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matchmaker Flexible Dictionary-Based Cleaning Provides flexible dictionary-based\n    cleaning that allows users to specify implicit and explicit missing data,\n    regular expressions for both data and columns, and global matches, while\n    respecting ordering of factors. This package is part of the 'RECON'\n    (<https://www.repidemicsconsortium.org/>) toolkit for outbreak analysis.  "
  },
  {
    "id": 15690,
    "package_name": "matsindf",
    "title": "Matrices in Data Frames",
    "description": "Provides functions to collapse a tidy data frame into matrices in a data frame\n    and expand a data frame of matrices into a tidy data frame.",
    "version": "0.4.11",
    "maintainer": "Matthew Heun <matthew.heun@me.com>",
    "author": "Matthew Heun [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7438-214X>)",
    "url": "https://github.com/MatthewHeun/matsindf,\nhttps://matthewheun.github.io/matsindf/",
    "bug_reports": "https://github.com/MatthewHeun/matsindf/issues",
    "repository": "https://cran.r-project.org/package=matsindf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matsindf Matrices in Data Frames Provides functions to collapse a tidy data frame into matrices in a data frame\n    and expand a data frame of matrices into a tidy data frame.  "
  },
  {
    "id": 15890,
    "package_name": "metaconfoundr",
    "title": "Visualize 'Confounder' Control in Meta-Analyses",
    "description": "Visualize 'confounder' control in meta-analysis.\n    'metaconfoundr' is an approach to evaluating bias in studies used in\n    meta-analyses based on the causal inference framework. Study groups\n    create a causal diagram displaying their assumptions about the\n    scientific question. From this, they develop a list of important\n    'confounders'. Then, they evaluate whether studies controlled for\n    these variables well. 'metaconfoundr' is a toolkit to facilitate this\n    process and visualize the results as heat maps, traffic light plots,\n    and more.",
    "version": "0.1.2",
    "maintainer": "Malcolm Barrett <malcolmbarrett@gmail.com>",
    "author": "Malcolm Barrett [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0299-5825>),\n  Julie M. Petersen [aut] (ORCID:\n    <https://orcid.org/0000-0001-7845-4545>),\n  Ludovic Trinquart [aut] (ORCID:\n    <https://orcid.org/0000-0002-3028-4900>)",
    "url": "https://github.com/malcolmbarrett/metaconfoundr",
    "bug_reports": "https://github.com/malcolmbarrett/metaconfoundr/issues",
    "repository": "https://cran.r-project.org/package=metaconfoundr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metaconfoundr Visualize 'Confounder' Control in Meta-Analyses Visualize 'confounder' control in meta-analysis.\n    'metaconfoundr' is an approach to evaluating bias in studies used in\n    meta-analyses based on the causal inference framework. Study groups\n    create a causal diagram displaying their assumptions about the\n    scientific question. From this, they develop a list of important\n    'confounders'. Then, they evaluate whether studies controlled for\n    these variables well. 'metaconfoundr' is a toolkit to facilitate this\n    process and visualize the results as heat maps, traffic light plots,\n    and more.  "
  },
  {
    "id": 15945,
    "package_name": "metevalue",
    "title": "E-Value in the Omics Data Association Studies",
    "description": "In the omics data association studies, it is common to conduct the p-value corrections to control the false significance. Beyond the P-value corrections, E-value is recently studied to facilitate multiple testing correction based on V. Vovk and R. Wang (2021) <doi:10.1214/20-AOS2020>. This package provides E-value calculation for DNA methylation data and RNA-seq data. Currently, five data formats are supported: DNA methylation levels using DMR detection tools (BiSeq, DMRfinder, MethylKit, Metilene and other DNA methylation tools) and RNA-seq data. The relevant references are listed below: Katja Hebestreit and Hans-Ulrich Klein (2022) <doi:10.18129/B9.bioc.BiSeq>; Altuna Akalin et.al (2012) <doi:10.18129/B9.bioc.methylKit>.",
    "version": "0.2.4",
    "maintainer": "Yifan Yang <yfyang.86@hotmail.com>",
    "author": "Yifan Yang [aut, cre, cph],\n  Xiaoqing Pan [aut],\n  Haoyuan Liu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metevalue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metevalue E-Value in the Omics Data Association Studies In the omics data association studies, it is common to conduct the p-value corrections to control the false significance. Beyond the P-value corrections, E-value is recently studied to facilitate multiple testing correction based on V. Vovk and R. Wang (2021) <doi:10.1214/20-AOS2020>. This package provides E-value calculation for DNA methylation data and RNA-seq data. Currently, five data formats are supported: DNA methylation levels using DMR detection tools (BiSeq, DMRfinder, MethylKit, Metilene and other DNA methylation tools) and RNA-seq data. The relevant references are listed below: Katja Hebestreit and Hans-Ulrich Klein (2022) <doi:10.18129/B9.bioc.BiSeq>; Altuna Akalin et.al (2012) <doi:10.18129/B9.bioc.methylKit>.  "
  },
  {
    "id": 16003,
    "package_name": "miceFast",
    "title": "Fast Imputations Using 'Rcpp' and 'Armadillo'",
    "description": "\n  Fast imputations under the object-oriented programming paradigm. \t\n  Moreover there are offered a few functions built to work with popular R packages such as 'data.table' or 'dplyr'.\n  The biggest improvement in time performance could be achieve for a calculation where a grouping variable have to be used.\n  A single evaluation of a quantitative model for the multiple imputations is another major enhancement.\n  A new major improvement is one of the fastest predictive mean matching in the R world because of presorting and binary search.",
    "version": "0.8.5",
    "maintainer": "Maciej Nasinski <nasinski.maciej@gmail.com>",
    "author": "Maciej Nasinski [aut, cre]",
    "url": "https://github.com/Polkas/miceFast",
    "bug_reports": "https://github.com/Polkas/miceFast/issues",
    "repository": "https://cran.r-project.org/package=miceFast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "miceFast Fast Imputations Using 'Rcpp' and 'Armadillo' \n  Fast imputations under the object-oriented programming paradigm. \t\n  Moreover there are offered a few functions built to work with popular R packages such as 'data.table' or 'dplyr'.\n  The biggest improvement in time performance could be achieve for a calculation where a grouping variable have to be used.\n  A single evaluation of a quantitative model for the multiple imputations is another major enhancement.\n  A new major improvement is one of the fastest predictive mean matching in the R world because of presorting and binary search.  "
  },
  {
    "id": 16088,
    "package_name": "minioclient",
    "title": "Interface to the 'MinIO' Client",
    "description": "An R interface to the 'MinIO' Client. The 'MinIO' Client ('mc')\n  provides a modern alternative to UNIX commands like 'ls', 'cat', 'cp',\n  'mirror', 'diff', 'find' etc. It supports 'filesystems'  and Amazon \"S3\"\n  compatible cloud storage service (\"AWS\" Signature v2 and v4).\n  This package provides convenience functions for installing the 'MinIO'\n  client and running any operations, as described in the official \n  documentation, <https://min.io/docs/minio/linux/reference/minio-mc.html?ref=docs-redirect>.\n  This package provides a flexible and high-performance alternative to 'aws.s3'.",
    "version": "0.0.6",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>),\n  Markus Skyttner [ctb]",
    "url": "https://github.com/cboettig/minioclient,\nhttps://cboettig.github.io/minioclient/",
    "bug_reports": "https://github.com/cboettig/minioclient/issues",
    "repository": "https://cran.r-project.org/package=minioclient",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minioclient Interface to the 'MinIO' Client An R interface to the 'MinIO' Client. The 'MinIO' Client ('mc')\n  provides a modern alternative to UNIX commands like 'ls', 'cat', 'cp',\n  'mirror', 'diff', 'find' etc. It supports 'filesystems'  and Amazon \"S3\"\n  compatible cloud storage service (\"AWS\" Signature v2 and v4).\n  This package provides convenience functions for installing the 'MinIO'\n  client and running any operations, as described in the official \n  documentation, <https://min.io/docs/minio/linux/reference/minio-mc.html?ref=docs-redirect>.\n  This package provides a flexible and high-performance alternative to 'aws.s3'.  "
  },
  {
    "id": 16098,
    "package_name": "mintyr",
    "title": "Streamlined Data Processing Tools for Genomic Selection",
    "description": "A toolkit for genomic selection in animal breeding with\n    emphasis on multi-breed and multi-trait nested grouping operations.\n    Streamlines iterative analysis workflows when working with 'ASReml-R'\n    package.  Includes utility functions for phenotypic data processing\n    commonly used by animal breeders.",
    "version": "0.1.2",
    "maintainer": "Guo Meng <tony2015116@163.com>",
    "author": "Guo Meng [aut, cre],\n  Guo Meng [cph]",
    "url": "https://tony2015116.github.io/mintyr/,\nhttps://github.com/tony2015116/mintyr",
    "bug_reports": "https://github.com/tony2015116/mintyr/issues",
    "repository": "https://cran.r-project.org/package=mintyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mintyr Streamlined Data Processing Tools for Genomic Selection A toolkit for genomic selection in animal breeding with\n    emphasis on multi-breed and multi-trait nested grouping operations.\n    Streamlines iterative analysis workflows when working with 'ASReml-R'\n    package.  Includes utility functions for phenotypic data processing\n    commonly used by animal breeders.  "
  },
  {
    "id": 16161,
    "package_name": "mixedfact",
    "title": "Generate and Analyze Mixed-Level Blocked Factorial Designs",
    "description": "Generates blocked designs for mixed-level factorial experiments for a \n    given block size. Internally, it uses finite-field based, collapsed, and heuristic \n    methods to construct block structures that minimize confounding between block \n    effects and factorial effects. The package creates the full treatment combination \n    table, partitions runs into blocks, and computes detailed confounding diagnostics \n    for main effects and two-factor interactions. It also checks orthogonal factorial \n    structure (OFS) and computes efficiencies of factorial effects using the methods \n    of Nair and Rao (1948) <doi:10.1111/j.2517-6161.1948.tb00005.x>. When OFS is not satisfied but \n    the design has equal treatment replications and equal block sizes, a general method \n    based on the C-matrix and custom contrast vectors is used to compute efficiencies. \n    The output includes the generated design, finite-field metadata, confounding \n    summaries, OFS diagnostics, and efficiency results.",
    "version": "0.1.1",
    "maintainer": "Sukanta Dash <sukanta.iasri@gmail.com>",
    "author": "Archana A [aut],\n  Sukanta Dash [aut, cre],\n  Anil Kumar [aut],\n  Medram Verma [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mixedfact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixedfact Generate and Analyze Mixed-Level Blocked Factorial Designs Generates blocked designs for mixed-level factorial experiments for a \n    given block size. Internally, it uses finite-field based, collapsed, and heuristic \n    methods to construct block structures that minimize confounding between block \n    effects and factorial effects. The package creates the full treatment combination \n    table, partitions runs into blocks, and computes detailed confounding diagnostics \n    for main effects and two-factor interactions. It also checks orthogonal factorial \n    structure (OFS) and computes efficiencies of factorial effects using the methods \n    of Nair and Rao (1948) <doi:10.1111/j.2517-6161.1948.tb00005.x>. When OFS is not satisfied but \n    the design has equal treatment replications and equal block sizes, a general method \n    based on the C-matrix and custom contrast vectors is used to compute efficiencies. \n    The output includes the generated design, finite-field metadata, confounding \n    summaries, OFS diagnostics, and efficiency results.  "
  },
  {
    "id": 16186,
    "package_name": "mlapi",
    "title": "Abstract Classes for Building 'scikit-learn' Like API",
    "description": "Provides 'R6' abstract classes for building machine learning models \n    with 'scikit-learn' like API. <https://scikit-learn.org/> is a popular module \n    for 'Python' programming language which design became de facto a standard \n    in industry for machine learning tasks.",
    "version": "0.1.1",
    "maintainer": "Dmitriy Selivanov <selivanov.dmitriy@gmail.com>",
    "author": "Dmitriy Selivanov",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlapi Abstract Classes for Building 'scikit-learn' Like API Provides 'R6' abstract classes for building machine learning models \n    with 'scikit-learn' like API. <https://scikit-learn.org/> is a popular module \n    for 'Python' programming language which design became de facto a standard \n    in industry for machine learning tasks.  "
  },
  {
    "id": 16211,
    "package_name": "mlmtools",
    "title": "Multi-Level Model Assessment Kit",
    "description": "Multilevel models (mixed effects models) are the statistical tool of choice for analyzing multilevel data (Searle et al, 2009). These models account for the correlated nature of observations within higher level units by adding group-level error terms that augment the singular residual error of a standard OLS regression. Multilevel and mixed effects models often require specialized data pre-processing and further post-estimation derivations and graphics to gain insight into model results. The package presented here, 'mlmtools', is a suite of pre- and post-estimation tools for multilevel models in 'R'. Package implements post-estimation tools designed to work with models estimated using 'lme4''s (Bates et al., 2014) lmer() function, which fits linear mixed effects regression models. Searle, S. R., Casella, G., & McCulloch, C. E. (2009, ISBN:978-0470009598). Bates, D., M\u00e4chler, M., Bolker, B., & Walker, S. (2014) <doi:10.18637/jss.v067.i01>.",
    "version": "1.0.2",
    "maintainer": "Laura Jamison <lj5yn@virginia.edu>",
    "author": "Laura Jamison [aut, cre],\n  Jessica Mazen [aut],\n  Erik Ruzek [aut],\n  Gus Sjobeck [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlmtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlmtools Multi-Level Model Assessment Kit Multilevel models (mixed effects models) are the statistical tool of choice for analyzing multilevel data (Searle et al, 2009). These models account for the correlated nature of observations within higher level units by adding group-level error terms that augment the singular residual error of a standard OLS regression. Multilevel and mixed effects models often require specialized data pre-processing and further post-estimation derivations and graphics to gain insight into model results. The package presented here, 'mlmtools', is a suite of pre- and post-estimation tools for multilevel models in 'R'. Package implements post-estimation tools designed to work with models estimated using 'lme4''s (Bates et al., 2014) lmer() function, which fits linear mixed effects regression models. Searle, S. R., Casella, G., & McCulloch, C. E. (2009, ISBN:978-0470009598). Bates, D., M\u00e4chler, M., Bolker, B., & Walker, S. (2014) <doi:10.18637/jss.v067.i01>.  "
  },
  {
    "id": 16220,
    "package_name": "mlr3batchmark",
    "title": "Batch Experiments for 'mlr3'",
    "description": "Extends the 'mlr3' package with a connector to the package\n    'batchtools'. This allows to run large-scale benchmark experiments on\n    scheduled high-performance computing clusters.",
    "version": "0.2.2",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Marc Becker [cre, aut] (ORCID: <https://orcid.org/0000-0002-8115-0400>),\n  Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>),\n  Toby Hocking [ctb] (ORCID: <https://orcid.org/0000-0002-3146-0865>)",
    "url": "https://mlr3batchmark.mlr-org.com,\nhttps://github.com/mlr-org/mlr3batchmark",
    "bug_reports": "https://github.com/mlr-org/mlr3batchmark/issues",
    "repository": "https://cran.r-project.org/package=mlr3batchmark",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3batchmark Batch Experiments for 'mlr3' Extends the 'mlr3' package with a connector to the package\n    'batchtools'. This allows to run large-scale benchmark experiments on\n    scheduled high-performance computing clusters.  "
  },
  {
    "id": 16233,
    "package_name": "mlr3misc",
    "title": "Helper Functions for 'mlr3'",
    "description": "Frequently used helper functions and assertions used in\n    'mlr3' and its companion packages. Comes with helper functions for\n    functional programming, for printing, to work with 'data.table', as\n    well as some generally useful 'R6' classes. This package also\n    supersedes the package 'BBmisc'.",
    "version": "0.19.0",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Marc Becker [cre, aut] (ORCID: <https://orcid.org/0000-0002-8115-0400>),\n  Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>),\n  Patrick Schratz [aut] (ORCID: <https://orcid.org/0000-0003-0748-6624>)",
    "url": "https://mlr3misc.mlr-org.com, https://github.com/mlr-org/mlr3misc",
    "bug_reports": "https://github.com/mlr-org/mlr3misc/issues",
    "repository": "https://cran.r-project.org/package=mlr3misc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3misc Helper Functions for 'mlr3' Frequently used helper functions and assertions used in\n    'mlr3' and its companion packages. Comes with helper functions for\n    functional programming, for printing, to work with 'data.table', as\n    well as some generally useful 'R6' classes. This package also\n    supersedes the package 'BBmisc'.  "
  },
  {
    "id": 16235,
    "package_name": "mlr3pipelines",
    "title": "Preprocessing Operators and Pipelines for 'mlr3'",
    "description": "Dataflow programming toolkit that enriches 'mlr3' with a diverse\n  set of pipelining operators ('PipeOps') that can be composed into graphs.\n  Operations exist for data preprocessing, model fitting, and ensemble\n  learning. Graphs can themselves be treated as 'mlr3' 'Learners' and can\n  therefore be resampled, benchmarked, and tuned.",
    "version": "0.10.0",
    "maintainer": "Martin Binder <mlr.developer@mb706.com>",
    "author": "Martin Binder [aut, cre],\n  Florian Pfisterer [aut] (ORCID:\n    <https://orcid.org/0000-0001-8867-762X>),\n  Lennart Schneider [aut] (ORCID:\n    <https://orcid.org/0000-0003-4152-5308>),\n  Bernd Bischl [aut] (ORCID: <https://orcid.org/0000-0001-6002-6980>),\n  Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>),\n  Sebastian Fischer [aut] (ORCID:\n    <https://orcid.org/0000-0002-9609-3197>),\n  Susanne Dandl [aut],\n  Keno Mersmann [ctb],\n  Maximilian M\u00fccke [ctb] (ORCID: <https://orcid.org/0009-0000-9432-9795>),\n  Lona Koers [ctb],\n  Alexander Winterstetter [ctb]",
    "url": "https://mlr3pipelines.mlr-org.com,\nhttps://github.com/mlr-org/mlr3pipelines",
    "bug_reports": "https://github.com/mlr-org/mlr3pipelines/issues",
    "repository": "https://cran.r-project.org/package=mlr3pipelines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3pipelines Preprocessing Operators and Pipelines for 'mlr3' Dataflow programming toolkit that enriches 'mlr3' with a diverse\n  set of pipelining operators ('PipeOps') that can be composed into graphs.\n  Operations exist for data preprocessing, model fitting, and ensemble\n  learning. Graphs can themselves be treated as 'mlr3' 'Learners' and can\n  therefore be resampled, benchmarked, and tuned.  "
  },
  {
    "id": 16257,
    "package_name": "mltools",
    "title": "Machine Learning Tools",
    "description": "A collection of machine learning helper functions, particularly assisting in the Exploratory Data Analysis phase. Makes heavy use of the 'data.table' package for optimal speed and memory efficiency. Highlights include a versatile bin_data() function, sparsify() for converting a data.table to sparse matrix format with one-hot encoding, fast evaluation metrics, and empirical_cdf() for calculating empirical Multivariate Cumulative Distribution Functions.",
    "version": "0.3.5",
    "maintainer": "Ben Gorman <bgorman@GormAnalysis.com>",
    "author": "Ben Gorman",
    "url": "https://github.com/ben519/mltools",
    "bug_reports": "https://github.com/ben519/mltools/issues",
    "repository": "https://cran.r-project.org/package=mltools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mltools Machine Learning Tools A collection of machine learning helper functions, particularly assisting in the Exploratory Data Analysis phase. Makes heavy use of the 'data.table' package for optimal speed and memory efficiency. Highlights include a versatile bin_data() function, sparsify() for converting a data.table to sparse matrix format with one-hot encoding, fast evaluation metrics, and empirical_cdf() for calculating empirical Multivariate Cumulative Distribution Functions.  "
  },
  {
    "id": 16317,
    "package_name": "modelSSE",
    "title": "Modelling Infectious Disease Superspreading from Contact Tracing\nData",
    "description": "Comprehensive analytical tools are provided to characterize infectious disease superspreading from contact tracing surveillance data. The underlying theoretical frameworks of this toolkit include branching process with transmission heterogeneity (Lloyd-Smith et al. (2005) <doi:10.1038/nature04153>), case cluster size distribution (Nishiura et al. (2012) <doi:10.1016/j.jtbi.2011.10.039>, Blumberg et al. (2014) <doi:10.1371/journal.ppat.1004452>, and Kucharski and Althaus (2015) <doi:10.2807/1560-7917.ES2015.20.25.21167>), and decomposition of reproduction number (Zhao et al. (2022) <doi:10.1371/journal.pcbi.1010281>).",
    "version": "0.1-3",
    "maintainer": "Shi Zhao <zhaoshi.cmsa@gmail.com>",
    "author": "Shi Zhao [aut, cre] (ORCID: <https://orcid.org/0000-0001-8722-6149>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=modelSSE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modelSSE Modelling Infectious Disease Superspreading from Contact Tracing\nData Comprehensive analytical tools are provided to characterize infectious disease superspreading from contact tracing surveillance data. The underlying theoretical frameworks of this toolkit include branching process with transmission heterogeneity (Lloyd-Smith et al. (2005) <doi:10.1038/nature04153>), case cluster size distribution (Nishiura et al. (2012) <doi:10.1016/j.jtbi.2011.10.039>, Blumberg et al. (2014) <doi:10.1371/journal.ppat.1004452>, and Kucharski and Althaus (2015) <doi:10.2807/1560-7917.ES2015.20.25.21167>), and decomposition of reproduction number (Zhao et al. (2022) <doi:10.1371/journal.pcbi.1010281>).  "
  },
  {
    "id": 16352,
    "package_name": "momentfit",
    "title": "Methods of Moments",
    "description": "Several classes for moment-based models are defined. The classes are defined for moment conditions derived from a single equation or a system of equations. The conditions can also be expressed as functions or formulas. Several methods are also offered to facilitate the development of different estimation techniques. The methods that are currently provided are the Generalized method of moments (Hansen 1982; <doi:10.2307/1912775>), for single equations and systems of equation, and  the Generalized Empirical Likelihood (Smith 1997; <doi:10.1111/j.0013-0133.1997.174.x>, Kitamura 1997; <doi:10.1214/aos/1069362388>, Newey and Smith 2004; <doi:10.1111/j.1468-0262.2004.00482.x>, and Anatolyev 2005 <doi:10.1111/j.1468-0262.2005.00601.x>). Some work is being done to add tools to deal with weak and/or many instruments. This includes K-Class estimators (Limited Information Maximum Likelihood and Fuller), Anderson and Rubin statistic test, etc. ",
    "version": "1.0",
    "maintainer": "Pierre Chausse <pchausse@uwaterloo.ca>",
    "author": "Pierre Chausse [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=momentfit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "momentfit Methods of Moments Several classes for moment-based models are defined. The classes are defined for moment conditions derived from a single equation or a system of equations. The conditions can also be expressed as functions or formulas. Several methods are also offered to facilitate the development of different estimation techniques. The methods that are currently provided are the Generalized method of moments (Hansen 1982; <doi:10.2307/1912775>), for single equations and systems of equation, and  the Generalized Empirical Likelihood (Smith 1997; <doi:10.1111/j.0013-0133.1997.174.x>, Kitamura 1997; <doi:10.1214/aos/1069362388>, Newey and Smith 2004; <doi:10.1111/j.1468-0262.2004.00482.x>, and Anatolyev 2005 <doi:10.1111/j.1468-0262.2005.00601.x>). Some work is being done to add tools to deal with weak and/or many instruments. This includes K-Class estimators (Limited Information Maximum Likelihood and Fuller), Anderson and Rubin statistic test, etc.   "
  },
  {
    "id": 16359,
    "package_name": "mongolite",
    "title": "Fast and Simple 'MongoDB' Client for R",
    "description": "High-performance MongoDB client based on 'mongo-c-driver' and 'jsonlite'.\n    Includes support for aggregation, indexing, map-reduce, streaming, encryption,\n    enterprise authentication, and GridFS. The online user manual provides an overview \n    of the available methods in the package: <https://jeroen.github.io/mongolite/>.",
    "version": "4.0.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  MongoDB, Inc [cph] (Bundled mongo-c-driver, see AUTHORS file)",
    "url": "https://jeroen.r-universe.dev/mongolite",
    "bug_reports": "https://github.com/jeroen/mongolite/issues",
    "repository": "https://cran.r-project.org/package=mongolite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mongolite Fast and Simple 'MongoDB' Client for R High-performance MongoDB client based on 'mongo-c-driver' and 'jsonlite'.\n    Includes support for aggregation, indexing, map-reduce, streaming, encryption,\n    enterprise authentication, and GridFS. The online user manual provides an overview \n    of the available methods in the package: <https://jeroen.github.io/mongolite/>.  "
  },
  {
    "id": 16387,
    "package_name": "morestopwords",
    "title": "All Stop Words in One Place",
    "description": "A standalone package combining several stop-word lists for 65 languages with a median of 329 stop words for language and over 1,000 entries for English, Breton, Latin, Slovenian, and Ancient Greek! The user automatically gets access to all the unique stop words contained in: the 'StopwordISO' repository; the 'Natural Language Toolkit' for 'python'; the 'Snowball' stop-word list; the R package 'quanteda'; the 'marimo' repository; the 'Perseus' project; and A. Berra's list of stop words for Ancient Greek and Latin.",
    "version": "0.2.0",
    "maintainer": "Fabio Ashtar Telarico <Fabio-Ashtar.Telarico@fdv.uni-lj.si>",
    "author": "Fabio Ashtar Telarico [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8740-7078>),\n  Kohei Watanabe [aut]",
    "url": "https://fatelarico.github.io/morestopwords.html",
    "bug_reports": "https://github.com/FATelarico/morestopwords/issues",
    "repository": "https://cran.r-project.org/package=morestopwords",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "morestopwords All Stop Words in One Place A standalone package combining several stop-word lists for 65 languages with a median of 329 stop words for language and over 1,000 entries for English, Breton, Latin, Slovenian, and Ancient Greek! The user automatically gets access to all the unique stop words contained in: the 'StopwordISO' repository; the 'Natural Language Toolkit' for 'python'; the 'Snowball' stop-word list; the R package 'quanteda'; the 'marimo' repository; the 'Perseus' project; and A. Berra's list of stop words for Ancient Greek and Latin.  "
  },
  {
    "id": 16396,
    "package_name": "mort",
    "title": "Identifying Potential Mortalities and Expelled Tags in Aquatic\nAcoustic Telemetry Arrays",
    "description": "A toolkit for identifying potential mortalities and expelled tags in aquatic acoustic telemetry arrays. \n    Designed for arrays with non-overlapping receivers. ",
    "version": "0.0.1",
    "maintainer": "Rosie Smith <rosieluain@gmail.com>",
    "author": "Rosie Smith [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2675-4452>),\n  Heidi Swanson [fnd]",
    "url": "https://github.com/rosieluain/mort,\nhttps://rosieluain.github.io/mort/",
    "bug_reports": "https://github.com/rosieluain/mort/issues",
    "repository": "https://cran.r-project.org/package=mort",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mort Identifying Potential Mortalities and Expelled Tags in Aquatic\nAcoustic Telemetry Arrays A toolkit for identifying potential mortalities and expelled tags in aquatic acoustic telemetry arrays. \n    Designed for arrays with non-overlapping receivers.   "
  },
  {
    "id": 16507,
    "package_name": "mtb",
    "title": "My Toolbox for Assisting Document Editing and Data Presenting",
    "description": "\n    The purpose of this package is to share a collection of functions the author wrote during weekends for managing\n    kitchen and garden tasks, e.g. making plant growth charts or Thanksgiving kitchen schedule charts, etc. \n    Functions might include but not limited to:\n    (1) aiding summarizing time related data; \n    (2) generating axis transformation from data; and\n    (3) aiding Markdown (with html output) and Shiny file editing.",
    "version": "0.1.9",
    "maintainer": "Y Hsu <yh202109@gmail.com>",
    "author": "Y Hsu [aut, cre]",
    "url": "https://github.com/yh202109/mtb",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mtb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mtb My Toolbox for Assisting Document Editing and Data Presenting \n    The purpose of this package is to share a collection of functions the author wrote during weekends for managing\n    kitchen and garden tasks, e.g. making plant growth charts or Thanksgiving kitchen schedule charts, etc. \n    Functions might include but not limited to:\n    (1) aiding summarizing time related data; \n    (2) generating axis transformation from data; and\n    (3) aiding Markdown (with html output) and Shiny file editing.  "
  },
  {
    "id": 16524,
    "package_name": "mulea",
    "title": "Enrichment Analysis Using Multiple Ontologies and False\nDiscovery Rate",
    "description": "Background - Traditional gene set enrichment analyses are \n    typically limited to a few ontologies and do not account for the \n    interdependence of gene sets or terms, resulting in overcorrected p-values. \n    To address these challenges, we introduce mulea, an R package offering \n    comprehensive overrepresentation and functional enrichment analysis. \n    Results - mulea employs a progressive empirical false discovery rate \n    (eFDR) method, specifically designed for interconnected biological data, \n    to accurately identify significant terms within diverse ontologies. mulea \n    expands beyond traditional tools by incorporating a wide range of \n    ontologies, encompassing Gene Ontology, pathways, regulatory elements, \n    genomic locations, and protein domains. This flexibility enables \n    researchers to tailor enrichment analysis to their specific questions, \n    such as identifying enriched transcriptional regulators in gene expression \n    data or overrepresented protein domains in protein sets. To facilitate \n    seamless analysis, mulea provides gene sets (in standardised GMT format) \n    for 27 model organisms, covering 22 ontology types from 16 databases and \n    various identifiers resulting in almost 900 files. Additionally, the \n    muleaData ExperimentData Bioconductor package simplifies access to these \n    pre-defined ontologies. Finally, mulea's architecture allows for easy \n    integration of user-defined ontologies, or GMT files from external \n    sources (e.g., MSigDB or Enrichr), expanding its applicability across \n    diverse research areas. Conclusions - mulea is distributed as a CRAN R \n    package. It offers researchers a powerful and flexible toolkit for \n    functional enrichment analysis, addressing limitations of traditional \n    tools with its progressive eFDR and by supporting a variety of ontologies. \n    Overall, mulea fosters the exploration of diverse biological questions \n    across various model organisms.",
    "version": "1.1.1",
    "maintainer": "Tamas Stirling <stirling.tamas@gmail.com>",
    "author": "Cezary Turek [aut] (ORCID: <https://orcid.org/0000-0002-1445-5378>),\n  Marton Olbei [aut] (ORCID: <https://orcid.org/0000-0002-4903-6237>),\n  Tamas Stirling [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8964-6443>),\n  Gergely Fekete [aut] (ORCID: <https://orcid.org/0000-0001-9939-4860>),\n  Ervin Tasnadi [aut] (ORCID: <https://orcid.org/0000-0002-4713-5397>),\n  Leila Gul [aut],\n  Balazs Bohar [aut] (ORCID: <https://orcid.org/0000-0002-3033-5448>),\n  Balazs Papp [aut] (ORCID: <https://orcid.org/0000-0003-3093-8852>),\n  Wiktor Jurkowski [aut] (ORCID: <https://orcid.org/0000-0002-7820-1991>),\n  Eszter Ari [aut, cph] (ORCID: <https://orcid.org/0000-0001-7774-1067>)",
    "url": "https://github.com/ELTEbioinformatics/mulea",
    "bug_reports": "https://github.com/ELTEbioinformatics/mulea/issues",
    "repository": "https://cran.r-project.org/package=mulea",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mulea Enrichment Analysis Using Multiple Ontologies and False\nDiscovery Rate Background - Traditional gene set enrichment analyses are \n    typically limited to a few ontologies and do not account for the \n    interdependence of gene sets or terms, resulting in overcorrected p-values. \n    To address these challenges, we introduce mulea, an R package offering \n    comprehensive overrepresentation and functional enrichment analysis. \n    Results - mulea employs a progressive empirical false discovery rate \n    (eFDR) method, specifically designed for interconnected biological data, \n    to accurately identify significant terms within diverse ontologies. mulea \n    expands beyond traditional tools by incorporating a wide range of \n    ontologies, encompassing Gene Ontology, pathways, regulatory elements, \n    genomic locations, and protein domains. This flexibility enables \n    researchers to tailor enrichment analysis to their specific questions, \n    such as identifying enriched transcriptional regulators in gene expression \n    data or overrepresented protein domains in protein sets. To facilitate \n    seamless analysis, mulea provides gene sets (in standardised GMT format) \n    for 27 model organisms, covering 22 ontology types from 16 databases and \n    various identifiers resulting in almost 900 files. Additionally, the \n    muleaData ExperimentData Bioconductor package simplifies access to these \n    pre-defined ontologies. Finally, mulea's architecture allows for easy \n    integration of user-defined ontologies, or GMT files from external \n    sources (e.g., MSigDB or Enrichr), expanding its applicability across \n    diverse research areas. Conclusions - mulea is distributed as a CRAN R \n    package. It offers researchers a powerful and flexible toolkit for \n    functional enrichment analysis, addressing limitations of traditional \n    tools with its progressive eFDR and by supporting a variety of ontologies. \n    Overall, mulea fosters the exploration of diverse biological questions \n    across various model organisms.  "
  },
  {
    "id": 16547,
    "package_name": "multiROC",
    "title": "Calculating and Visualizing ROC and PR Curves Across Multi-Class\nClassifications",
    "description": "Tools to solve real-world problems with multiple classes classifications by computing the areas under ROC and PR curve via micro-averaging and macro-averaging. The vignettes of this package can be found via <https://github.com/WandeRum/multiROC>. The methodology is described in V. Van Asch (2013) <https://www.clips.uantwerpen.be/~vincent/pdf/microaverage.pdf> and Pedregosa et al. (2011) <http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html>.",
    "version": "1.1.1",
    "maintainer": "Runmin Wei <runmin@hawaii.edu>",
    "author": "Runmin Wei [aut, cre],\n  Jingye Wang [aut],\n  Wei Jia [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multiROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multiROC Calculating and Visualizing ROC and PR Curves Across Multi-Class\nClassifications Tools to solve real-world problems with multiple classes classifications by computing the areas under ROC and PR curve via micro-averaging and macro-averaging. The vignettes of this package can be found via <https://github.com/WandeRum/multiROC>. The methodology is described in V. Van Asch (2013) <https://www.clips.uantwerpen.be/~vincent/pdf/microaverage.pdf> and Pedregosa et al. (2011) <http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html>.  "
  },
  {
    "id": 16559,
    "package_name": "multicmp",
    "title": "Flexible Modeling of Multivariate Count Data via the\nMultivariate Conway-Maxwell-Poisson Distribution",
    "description": "A toolkit containing statistical analysis models motivated by multivariate forms of the Conway-Maxwell-Poisson (COM-Poisson) distribution for flexible modeling of multivariate count data, especially in the presence of data dispersion. Currently the package only supports bivariate data, via the bivariate COM-Poisson distribution described in Sellers et al. (2016) <doi:10.1016/j.jmva.2016.04.007>. Future development will extend the package to higher-dimensional data.",
    "version": "1.1",
    "maintainer": "Diag Davenport <diag.davenport@gmail.com>",
    "author": "Kimberly Sellers [aut],\n  Darcy Steeg Morris [aut],\n  Narayanaswamy Balakrishnan [aut],\n  Diag Davenport [aut, cre]",
    "url": "http://dx.doi.org/10.1016/j.jmva.2016.04.007",
    "bug_reports": "https://github.com/diagdavenport/multicmp/issues",
    "repository": "https://cran.r-project.org/package=multicmp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multicmp Flexible Modeling of Multivariate Count Data via the\nMultivariate Conway-Maxwell-Poisson Distribution A toolkit containing statistical analysis models motivated by multivariate forms of the Conway-Maxwell-Poisson (COM-Poisson) distribution for flexible modeling of multivariate count data, especially in the presence of data dispersion. Currently the package only supports bivariate data, via the bivariate COM-Poisson distribution described in Sellers et al. (2016) <doi:10.1016/j.jmva.2016.04.007>. Future development will extend the package to higher-dimensional data.  "
  },
  {
    "id": 16710,
    "package_name": "na.tools",
    "title": "Comprehensive Library for Working with Missing (NA) Values in\nVectors",
    "description": "\n    This comprehensive toolkit provide a consistent and \n    extensible framework for working with missing values in vectors. The \n    companion package 'tidyimpute' provides similar functionality for list-like \n    and table-like structures).\n    Functions exist for detection, removal, replacement, imputation, \n    recollection, etc. of 'NAs'.",
    "version": "0.3.1",
    "maintainer": "Christopher Brown <chris.brown@decisionpatterns.com>",
    "author": "Christopher Brown [aut, cre],\n  Decision Patterns [cph]",
    "url": "https://github.com/decisionpatterns/na.tools",
    "bug_reports": "https://github.com/decisionpatterns/na.tools/issues",
    "repository": "https://cran.r-project.org/package=na.tools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "na.tools Comprehensive Library for Working with Missing (NA) Values in\nVectors \n    This comprehensive toolkit provide a consistent and \n    extensible framework for working with missing values in vectors. The \n    companion package 'tidyimpute' provides similar functionality for list-like \n    and table-like structures).\n    Functions exist for detection, removal, replacement, imputation, \n    recollection, etc. of 'NAs'.  "
  },
  {
    "id": 16733,
    "package_name": "naryn",
    "title": "Native Access Medical Record Retriever for High Yield Analytics",
    "description": "A toolkit for medical records data analysis. The 'naryn'\n    package implements an efficient data structure for storing medical\n    records, and provides a set of functions for data extraction,\n    manipulation and analysis.",
    "version": "2.6.31",
    "maintainer": "Aviezer Lifshitz <aviezer.lifshitz@weizmann.ac.il>",
    "author": "Misha Hoichman [aut],\n  Aviezer Lifshitz [aut, cre],\n  Ben Gilat [aut],\n  Netta Mendelson-Cohen [ctb],\n  Rami Jaschek [ctb],\n  Weizmann Institute of Science [cph]",
    "url": "https://tanaylab.github.io/naryn/",
    "bug_reports": "https://github.com/tanaylab/naryn/issues",
    "repository": "https://cran.r-project.org/package=naryn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "naryn Native Access Medical Record Retriever for High Yield Analytics A toolkit for medical records data analysis. The 'naryn'\n    package implements an efficient data structure for storing medical\n    records, and provides a set of functions for data extraction,\n    manipulation and analysis.  "
  },
  {
    "id": 16777,
    "package_name": "ndjson",
    "title": "Wicked-Fast Streaming 'JSON' ('ndjson') Reader",
    "description": "Streaming 'JSON' ('ndjson') has one 'JSON' record per-line\n        and many modern 'ndjson' files contain large numbers of records.\n        These constructs may not be columnar in nature, but it is often\n        useful to read in these files and \"flatten\" the structure out to\n        enable working with the data in an R 'data.frame'-like context.\n        Functions are provided that make it possible to read in plain\n        'ndjson' files or compressed ('gz') 'ndjson' files and either\n        validate the format of the records or create \"flat\" 'data.table'\n        structures from them.",
    "version": "0.9.1",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis [aut, cre] (ORCID: <https://orcid.org/0000-0001-5670-2640>),\n  Niels Lohmann [aut] (C++ json parser),\n  Deepak Bandyopadhyay [aut] (C++ gzstream),\n  Lutz Kettner [aut] (C++ gzstream),\n  Neal Fultz [ctb] (Rcpp integration),\n  Maarten Demeyer [ctb] (dtplyr cleanup)",
    "url": "https://github.com/hrbrmstr/ndjson",
    "bug_reports": "https://github.com/hrbrmstr/ndjson/issues",
    "repository": "https://cran.r-project.org/package=ndjson",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ndjson Wicked-Fast Streaming 'JSON' ('ndjson') Reader Streaming 'JSON' ('ndjson') has one 'JSON' record per-line\n        and many modern 'ndjson' files contain large numbers of records.\n        These constructs may not be columnar in nature, but it is often\n        useful to read in these files and \"flatten\" the structure out to\n        enable working with the data in an R 'data.frame'-like context.\n        Functions are provided that make it possible to read in plain\n        'ndjson' files or compressed ('gz') 'ndjson' files and either\n        validate the format of the records or create \"flat\" 'data.table'\n        structures from them.  "
  },
  {
    "id": 16783,
    "package_name": "neatRanges",
    "title": "Tidy Up Date/Time Ranges",
    "description": "Collapse, partition, combine, fill gaps in and expand date/time ranges.",
    "version": "0.1.4",
    "maintainer": "Aljaz Jelenko <aljaz.jelenko@amis.net>",
    "author": "Aljaz Jelenko [aut, cre],\n  Patrik Punco [aut]",
    "url": "https://github.com/arg0naut91/neatRanges",
    "bug_reports": "https://github.com/arg0naut91/neatRanges/issues",
    "repository": "https://cran.r-project.org/package=neatRanges",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neatRanges Tidy Up Date/Time Ranges Collapse, partition, combine, fill gaps in and expand date/time ranges.  "
  },
  {
    "id": 16926,
    "package_name": "nipnTK",
    "title": "National Information Platforms for Nutrition Anthropometric Data\nToolkit",
    "description": "An implementation of the National Information Platforms for \n    Nutrition or NiPN's analytic methods for assessing quality of anthropometric \n    datasets that include measurements of weight, height or length, middle upper \n    arm circumference, sex and age. The focus is on anthropometric status but \n    many of the presented methods could be applied to other variables.",
    "version": "0.2.0",
    "maintainer": "Ernest Guevarra <ernest@guevarra.io>",
    "author": "Mark Myatt [aut] (ORCID: <https://orcid.org/0000-0003-1119-1474>),\n  Ernest Guevarra [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4887-4415>)",
    "url": "https://nutriverse.io/nipnTK/,\nhttps://github.com/nutriverse/nipnTK",
    "bug_reports": "https://github.com/nutriverse/nipnTK/issues",
    "repository": "https://cran.r-project.org/package=nipnTK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nipnTK National Information Platforms for Nutrition Anthropometric Data\nToolkit An implementation of the National Information Platforms for \n    Nutrition or NiPN's analytic methods for assessing quality of anthropometric \n    datasets that include measurements of weight, height or length, middle upper \n    arm circumference, sex and age. The focus is on anthropometric status but \n    many of the presented methods could be applied to other variables.  "
  },
  {
    "id": 16930,
    "package_name": "nixtlar",
    "title": "A Software Development Kit for 'Nixtla''s 'TimeGPT'",
    "description": "A Software Development Kit for working with 'Nixtla''s 'TimeGPT', a foundation\n    model for time series forecasting. 'API' is an acronym for 'application\n    programming interface'; this package allows users to interact with\n    'TimeGPT' via the 'API'. You can set and validate 'API' keys and generate forecasts\n    via 'API' calls. It is compatible with 'tsibble' and base R. For more details \n    visit <https://docs.nixtla.io/>.",
    "version": "0.6.2",
    "maintainer": "Mariana Menchero <mariana@nixtla.io>",
    "author": "Mariana Menchero [aut, cre] (First author and maintainer),\n  Nixtla [cph] (Copyright held by 'Nixtla')",
    "url": "https://nixtla.github.io/nixtlar/, https://docs.nixtla.io/,\nhttps://github.com/Nixtla/nixtlar",
    "bug_reports": "https://github.com/Nixtla/nixtlar/issues",
    "repository": "https://cran.r-project.org/package=nixtlar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nixtlar A Software Development Kit for 'Nixtla''s 'TimeGPT' A Software Development Kit for working with 'Nixtla''s 'TimeGPT', a foundation\n    model for time series forecasting. 'API' is an acronym for 'application\n    programming interface'; this package allows users to interact with\n    'TimeGPT' via the 'API'. You can set and validate 'API' keys and generate forecasts\n    via 'API' calls. It is compatible with 'tsibble' and base R. For more details \n    visit <https://docs.nixtla.io/>.  "
  },
  {
    "id": 16980,
    "package_name": "nmslibR",
    "title": "Non Metric Space (Approximate) Library",
    "description": "A Non-Metric Space Library ('NMSLIB' <https://github.com/nmslib/nmslib>) wrapper, which according to the authors \"is an efficient cross-platform similarity search library and a toolkit for evaluation of similarity search methods. The goal of the 'NMSLIB' <https://github.com/nmslib/nmslib> Library is to create an effective and comprehensive toolkit for searching in generic non-metric spaces. Being comprehensive is important, because no single method is likely to be sufficient in all cases. Also note that exact solutions are hardly efficient in high dimensions and/or non-metric spaces. Hence, the main focus is on approximate methods\". The wrapper also includes Approximate Kernel k-Nearest-Neighbor functions based on the 'NMSLIB' <https://github.com/nmslib/nmslib> 'Python' Library.",
    "version": "1.0.7",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>),\n  B. Naidan [cph] (Author of the Non-Metric Space Library (NMSLIB)),\n  L. Boytsov [cph] (Author of the Non-Metric Space Library (NMSLIB)),\n  Yu. Malkov [cph] (Author of the Non-Metric Space Library (NMSLIB)),\n  B. Frederickson [cph] (Author of the Non-Metric Space Library (NMSLIB)),\n  D. Novak [cph] (Author of the Non-Metric Space Library (NMSLIB))",
    "url": "https://github.com/mlampros/nmslibR",
    "bug_reports": "https://github.com/mlampros/nmslibR/issues",
    "repository": "https://cran.r-project.org/package=nmslibR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nmslibR Non Metric Space (Approximate) Library A Non-Metric Space Library ('NMSLIB' <https://github.com/nmslib/nmslib>) wrapper, which according to the authors \"is an efficient cross-platform similarity search library and a toolkit for evaluation of similarity search methods. The goal of the 'NMSLIB' <https://github.com/nmslib/nmslib> Library is to create an effective and comprehensive toolkit for searching in generic non-metric spaces. Being comprehensive is important, because no single method is likely to be sufficient in all cases. Also note that exact solutions are hardly efficient in high dimensions and/or non-metric spaces. Hence, the main focus is on approximate methods\". The wrapper also includes Approximate Kernel k-Nearest-Neighbor functions based on the 'NMSLIB' <https://github.com/nmslib/nmslib> 'Python' Library.  "
  },
  {
    "id": 16999,
    "package_name": "noctua",
    "title": "Connect to 'AWS Athena' using R 'AWS SDK' 'paws' ('DBI'\nInterface)",
    "description": "Designed to be compatible with the 'R' package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this the 'R' 'AWS' Software Development Kit ('SDK') 'paws' \n    <https://github.com/paws-r/paws> is used as a driver.",
    "version": "2.6.3",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "Dyfan Jones [aut, cre]",
    "url": "https://dyfanjones.github.io/noctua/,\nhttps://github.com/DyfanJones/noctua",
    "bug_reports": "https://github.com/DyfanJones/noctua/issues",
    "repository": "https://cran.r-project.org/package=noctua",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "noctua Connect to 'AWS Athena' using R 'AWS SDK' 'paws' ('DBI'\nInterface) Designed to be compatible with the 'R' package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this the 'R' 'AWS' Software Development Kit ('SDK') 'paws' \n    <https://github.com/paws-r/paws> is used as a driver.  "
  },
  {
    "id": 17206,
    "package_name": "odk",
    "title": "Convert 'ODK' or 'XLSForm' to 'SPSS' Data Frame",
    "description": "After develop a 'ODK' <https://opendatakit.org/> frame, we can link the frame to 'Google Sheets' <https://www.google.com/sheets/about/> and collect data through 'Android' <https://www.android.com/>. This data uploaded to a 'Google sheets'. odk2spss() function help to convert the 'odk' frame into 'SPSS' <https://www.ibm.com/analytics/us/en/technology/spss/> frame. Also able to add downloaded 'Google sheets' data or read data from 'Google sheets' by using 'ODK' frame 'submission_url'.",
    "version": "1.5",
    "maintainer": "Muntashir-Al-Arefin <muntashir.a2i@gmail.com>",
    "author": "Muntashir-Al-Arefin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=odk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "odk Convert 'ODK' or 'XLSForm' to 'SPSS' Data Frame After develop a 'ODK' <https://opendatakit.org/> frame, we can link the frame to 'Google Sheets' <https://www.google.com/sheets/about/> and collect data through 'Android' <https://www.android.com/>. This data uploaded to a 'Google sheets'. odk2spss() function help to convert the 'odk' frame into 'SPSS' <https://www.ibm.com/analytics/us/en/technology/spss/> frame. Also able to add downloaded 'Google sheets' data or read data from 'Google sheets' by using 'ODK' frame 'submission_url'.  "
  },
  {
    "id": 17237,
    "package_name": "omixVizR",
    "title": "A Toolkit for Omics Data Visualization",
    "description": "Provides a suite of tools for the comprehensive visualization of multi-omics data, including genomics, transcriptomics, and proteomics. Offers user-friendly functions to generate publication-quality plots, thereby facilitating the exploration and interpretation of complex biological datasets. Supports seamless integration with popular R visualization frameworks and is well-suited for both exploratory data analysis and the presentation of final results. Key formats and methods are presented in Huang, S., et al. (2024) \"The Born in Guangzhou Cohort Study enables generational genetic discoveries\" <doi:10.1038/s41586-023-06988-4>.",
    "version": "1.4.0",
    "maintainer": "Zhen Lu <luzh29@mail2.sysu.edu.cn>",
    "author": "Zhen Lu [aut, cre] (ORCID: <https://orcid.org/0000-0002-3481-6310>)",
    "url": "https://github.com/Leslie-Lu/omixVizR",
    "bug_reports": "https://github.com/Leslie-Lu/omixVizR/issues",
    "repository": "https://cran.r-project.org/package=omixVizR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "omixVizR A Toolkit for Omics Data Visualization Provides a suite of tools for the comprehensive visualization of multi-omics data, including genomics, transcriptomics, and proteomics. Offers user-friendly functions to generate publication-quality plots, thereby facilitating the exploration and interpretation of complex biological datasets. Supports seamless integration with popular R visualization frameworks and is well-suited for both exploratory data analysis and the presentation of final results. Key formats and methods are presented in Huang, S., et al. (2024) \"The Born in Guangzhou Cohort Study enables generational genetic discoveries\" <doi:10.1038/s41586-023-06988-4>.  "
  },
  {
    "id": 17291,
    "package_name": "openMSE",
    "title": "Easily Install and Load the 'openMSE' Packages",
    "description": "The 'openMSE' package is designed for building operating models, \n    doing simulation modelling and management strategy evaluation for fisheries.\n    'openMSE' is an umbrella package for the 'MSEtool' (Management Strategy Evaluation\n    toolkit), 'DLMtool' (Data-Limited Methods toolkit), and \n    SAMtool (Stock Assessment Methods toolkit) packages. By loading and installing\n    'openMSE', users have access to the full functionality contained within\n    these packages. Learn more about 'openMSE' at <https://openmse.com/>.",
    "version": "1.0.1",
    "maintainer": "Adrian Hordyk <adrian@bluematterscience.com>",
    "author": "Adrian Hordyk [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5620-3446>),\n  Quang Huynh [aut],\n  Tom Carruthers [aut]",
    "url": "https://openmse.com/, https://github.com/Blue-Matter/openMSE,\nhttps://openMSE.openMSE.com",
    "bug_reports": "https://github.com/Blue-Matter/openMSE/issues",
    "repository": "https://cran.r-project.org/package=openMSE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openMSE Easily Install and Load the 'openMSE' Packages The 'openMSE' package is designed for building operating models, \n    doing simulation modelling and management strategy evaluation for fisheries.\n    'openMSE' is an umbrella package for the 'MSEtool' (Management Strategy Evaluation\n    toolkit), 'DLMtool' (Data-Limited Methods toolkit), and \n    SAMtool (Stock Assessment Methods toolkit) packages. By loading and installing\n    'openMSE', users have access to the full functionality contained within\n    these packages. Learn more about 'openMSE' at <https://openmse.com/>.  "
  },
  {
    "id": 17292,
    "package_name": "openNLP",
    "title": "Apache OpenNLP Tools Interface",
    "description": "An interface to the Apache OpenNLP tools (version 1.5.3).\n  The Apache OpenNLP library is a machine learning based toolkit for the\n  processing of natural language text written in Java.\n  It supports the most common NLP tasks, such as tokenization, sentence\n  segmentation, part-of-speech tagging, named entity extraction, chunking,\n  parsing, and coreference resolution.\n  See <https://opennlp.apache.org/> for more information.",
    "version": "0.2-7",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=openNLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openNLP Apache OpenNLP Tools Interface An interface to the Apache OpenNLP tools (version 1.5.3).\n  The Apache OpenNLP library is a machine learning based toolkit for the\n  processing of natural language text written in Java.\n  It supports the most common NLP tasks, such as tokenization, sentence\n  segmentation, part-of-speech tagging, named entity extraction, chunking,\n  parsing, and coreference resolution.\n  See <https://opennlp.apache.org/> for more information.  "
  },
  {
    "id": 17312,
    "package_name": "openmetrics",
    "title": "A 'Prometheus' Client for R Using the 'OpenMetrics' Format",
    "description": "Provides a client for the open-source monitoring and alerting\n  toolkit, 'Prometheus', that emits metrics in the 'OpenMetrics' format. Allows\n  users to automatically instrument 'Plumber' and 'Shiny' applications, collect\n  standard process metrics, as well as define custom counter, gauge, and\n  histogram metrics of their own.",
    "version": "0.3.0",
    "maintainer": "Aaron Jacobs <atheriel@gmail.com>",
    "author": "Aaron Jacobs [aut, cre],\n  Crescendo Technology Ltd. [cph]",
    "url": "https://github.com/atheriel/openmetrics",
    "bug_reports": "https://github.com/atheriel/openmetrics/issues",
    "repository": "https://cran.r-project.org/package=openmetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openmetrics A 'Prometheus' Client for R Using the 'OpenMetrics' Format Provides a client for the open-source monitoring and alerting\n  toolkit, 'Prometheus', that emits metrics in the 'OpenMetrics' format. Allows\n  users to automatically instrument 'Plumber' and 'Shiny' applications, collect\n  standard process metrics, as well as define custom counter, gauge, and\n  histogram metrics of their own.  "
  },
  {
    "id": 17316,
    "package_name": "openssl",
    "title": "Toolkit for Encryption, Signatures and Certificates Based on\nOpenSSL",
    "description": "Bindings to OpenSSL libssl and libcrypto, plus custom SSH key parsers.\n    Supports RSA, DSA and EC curves P-256, P-384, P-521, and curve25519. Cryptographic\n    signatures can either be created and verified manually or via x509 certificates. \n    AES can be used in cbc, ctr or gcm mode for symmetric encryption; RSA for asymmetric\n    (public key) encryption or EC for Diffie Hellman. High-level envelope functions \n    combine RSA and AES for encrypting arbitrary sized data. Other utilities include key\n    generators, hash functions (md5, sha1, sha256, etc), base64 encoder, a secure random\n    number generator, and 'bignum' math methods for manually performing crypto \n    calculations on large multibyte integers.",
    "version": "2.3.4",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  Oliver Keyes [ctb]",
    "url": "https://jeroen.r-universe.dev/openssl",
    "bug_reports": "https://github.com/jeroen/openssl/issues",
    "repository": "https://cran.r-project.org/package=openssl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openssl Toolkit for Encryption, Signatures and Certificates Based on\nOpenSSL Bindings to OpenSSL libssl and libcrypto, plus custom SSH key parsers.\n    Supports RSA, DSA and EC curves P-256, P-384, P-521, and curve25519. Cryptographic\n    signatures can either be created and verified manually or via x509 certificates. \n    AES can be used in cbc, ctr or gcm mode for symmetric encryption; RSA for asymmetric\n    (public key) encryption or EC for Diffie Hellman. High-level envelope functions \n    combine RSA and AES for encrypting arbitrary sized data. Other utilities include key\n    generators, hash functions (md5, sha1, sha256, etc), base64 encoder, a secure random\n    number generator, and 'bignum' math methods for manually performing crypto \n    calculations on large multibyte integers.  "
  },
  {
    "id": 17369,
    "package_name": "orbweaver",
    "title": "Fast and Efficient Graph Data Structures",
    "description": "Seamlessly build and manipulate graph structures, leveraging\n    its high-performance methods for filtering, joining, and mutating\n    data. Ensures that mutations and changes to the graph are performed in\n    place, streamlining your workflow for optimal productivity.",
    "version": "0.18.2",
    "maintainer": "Andres Quintero <andres@ixpantia.com>",
    "author": "ixpantia, SRL [cph],\n  Andres Quintero [aut, cre],\n  The authors of the dependency Rust crates [ctb] (see inst/AUTHORS file\n    for details)",
    "url": "https://github.com/ixpantia/orbweaver-r",
    "bug_reports": "https://github.com/ixpantia/orbweaver-r/issues",
    "repository": "https://cran.r-project.org/package=orbweaver",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "orbweaver Fast and Efficient Graph Data Structures Seamlessly build and manipulate graph structures, leveraging\n    its high-performance methods for filtering, joining, and mutating\n    data. Ensures that mutations and changes to the graph are performed in\n    place, streamlining your workflow for optimal productivity.  "
  },
  {
    "id": 17541,
    "package_name": "palmtree",
    "title": "Partially Additive (Generalized) Linear Model Trees",
    "description": "This is an implementation of model-based trees with global model\n  parameters (PALM trees). The PALM tree algorithm is an extension to the MOB\n  algorithm (implemented in the 'partykit' package), where some parameters are\n  fixed across all groups. Details about the method can be found in Seibold,\n  Hothorn, Zeileis (2016) <arXiv:1612.07498>. The package offers coef(),\n  logLik(), plot(), and predict() functions for PALM trees.",
    "version": "0.9-1",
    "maintainer": "Heidi Seibold <heidi@seibold.co>",
    "author": "Heidi Seibold [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8960-9642>),\n  Torsten Hothorn [aut] (ORCID: <https://orcid.org/0000-0001-8301-0471>),\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=palmtree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "palmtree Partially Additive (Generalized) Linear Model Trees This is an implementation of model-based trees with global model\n  parameters (PALM trees). The PALM tree algorithm is an extension to the MOB\n  algorithm (implemented in the 'partykit' package), where some parameters are\n  fixed across all groups. Details about the method can be found in Seibold,\n  Hothorn, Zeileis (2016) <arXiv:1612.07498>. The package offers coef(),\n  logLik(), plot(), and predict() functions for PALM trees.  "
  },
  {
    "id": 17557,
    "package_name": "panelWranglR",
    "title": "Panel Data Wrangling Tools",
    "description": "Leading/lagging a panel, creating dummy variables,\n             taking panel differences, looking for panel autocorrelations,\n             and more. Implemented via a 'data.table' back end. ",
    "version": "1.2.13",
    "maintainer": "Juraj Szit\u00e1s <szitas.juraj13@gmail.com>",
    "author": "Juraj Szit\u00e1s [aut, cre]",
    "url": "https://github.com/JSzitas/panelWranglR",
    "bug_reports": "https://github.com/JSzitas/panelWranglR/issues",
    "repository": "https://cran.r-project.org/package=panelWranglR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "panelWranglR Panel Data Wrangling Tools Leading/lagging a panel, creating dummy variables,\n             taking panel differences, looking for panel autocorrelations,\n             and more. Implemented via a 'data.table' back end.   "
  },
  {
    "id": 17567,
    "package_name": "parSim",
    "title": "Parallel Simulation Studies",
    "description": "Perform flexible simulation studies using one or multiple computer cores.\n          The package is set up to be usable on high-performance clusters in addition\n          to being run locally, see examples on <https://github.com/SachaEpskamp/parSim>.",
    "version": "0.1.5",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "author": "Sacha Epskamp",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=parSim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parSim Parallel Simulation Studies Perform flexible simulation studies using one or multiple computer cores.\n          The package is set up to be usable on high-performance clusters in addition\n          to being run locally, see examples on <https://github.com/SachaEpskamp/parSim>.  "
  },
  {
    "id": 17617,
    "package_name": "parttree",
    "title": "Visualize Simple 2-D Decision Tree Partitions",
    "description": "Visualize the partitions of simple decision trees, involving one or\n    two predictors, on the scale of the original data. Provides an intuitive\n    alternative to traditional tree diagrams, by visualizing how a decision tree\n    divides the predictor space in a simple 2D plot alongside the original data.\n    The 'parttree' package supports both classification and regression trees\n    from 'rpart' and 'partykit', as well as trees produced by popular frontend\n    systems like 'tidymodels' and 'mlr3'. Visualization methods are provided for\n    both base R graphics and 'ggplot2'.",
    "version": "0.1.1",
    "maintainer": "Grant McDermott <gmcd@amazon.com>",
    "author": "Grant McDermott [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7883-8573>),\n  Achim Zeileis [ctb] (ORCID: <https://orcid.org/0000-0003-0918-3766>),\n  Brian Heseung Kim [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7988-1802>),\n  Julia Silge [ctb] (ORCID: <https://orcid.org/0000-0002-3671-836X>)",
    "url": "https://grantmcdermott.com/parttree/",
    "bug_reports": "https://github.com/grantmcdermott/parttree/issues",
    "repository": "https://cran.r-project.org/package=parttree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parttree Visualize Simple 2-D Decision Tree Partitions Visualize the partitions of simple decision trees, involving one or\n    two predictors, on the scale of the original data. Provides an intuitive\n    alternative to traditional tree diagrams, by visualizing how a decision tree\n    divides the predictor space in a simple 2D plot alongside the original data.\n    The 'parttree' package supports both classification and regression trees\n    from 'rpart' and 'partykit', as well as trees produced by popular frontend\n    systems like 'tidymodels' and 'mlr3'. Visualization methods are provided for\n    both base R graphics and 'ggplot2'.  "
  },
  {
    "id": 17649,
    "package_name": "paws",
    "title": "Amazon Web Services Software Development Kit",
    "description": "Interface to Amazon Web Services <https://aws.amazon.com>,\n    including storage, database, and compute services, such as 'Simple\n    Storage Service' ('S3'), 'DynamoDB' 'NoSQL' database, and 'Lambda'\n    functions-as-a-service.",
    "version": "0.9.0",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "David Kretch [aut],\n  Adam Banker [aut],\n  Dyfan Jones [cre],\n  Amazon.com, Inc. [cph]",
    "url": "https://github.com/paws-r/paws, https://paws-r.r-universe.dev/paws",
    "bug_reports": "https://github.com/paws-r/paws/issues",
    "repository": "https://cran.r-project.org/package=paws",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paws Amazon Web Services Software Development Kit Interface to Amazon Web Services <https://aws.amazon.com>,\n    including storage, database, and compute services, such as 'Simple\n    Storage Service' ('S3'), 'DynamoDB' 'NoSQL' database, and 'Lambda'\n    functions-as-a-service.  "
  },
  {
    "id": 17672,
    "package_name": "pbdZMQ",
    "title": "Programming with Big Data -- Interface to 'ZeroMQ'",
    "description": "'ZeroMQ' is a well-known library for high-performance\n    asynchronous messaging in scalable, distributed applications.  This\n    package provides high level R wrapper functions to easily utilize\n    'ZeroMQ'. We mainly focus on interactive client/server programming\n    frameworks. For convenience, a minimal 'ZeroMQ' library (4.2.2)\n    is shipped with 'pbdZMQ', which can be used if no system installation\n    of 'ZeroMQ' is available.  A few wrapper functions compatible with\n    'rzmq' are also provided.",
    "version": "0.3-14",
    "maintainer": "Wei-Chen Chen <wccsnow@gmail.com>",
    "author": "Wei-Chen Chen [aut, cre],\n  Drew Schmidt [aut],\n  Christian Heckendorf [aut] (file transfer),\n  George Ostrouchov [aut] (Mac OSX),\n  Whit Armstrong [ctb] (some functions are modified from the rzmq package\n    for backwards compatibility),\n  Brian Ripley [ctb] (C code of shellexec, and Solaris),\n  R Core team [ctb] (some functions and headers are copied or modified\n    from the R source code),\n  Philipp A. [ctb] (Fedora),\n  Elliott Sales de Andrade [ctb] (sprintf),\n  Spencer Aiello [ctb] (windows conf),\n  Paul Andrey [ctb] (Mac OSX conf),\n  Panagiotis Cheilaris [ctb] (add serialize version),\n  Jeroen Ooms [ctb] (clang++ on MacOS ARM64),\n  ZeroMQ authors [aut, cph] (source files in 'src/zmq_src/')",
    "url": "https://pbdr.org/",
    "bug_reports": "https://github.com/snoweye/pbdZMQ/issues",
    "repository": "https://cran.r-project.org/package=pbdZMQ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pbdZMQ Programming with Big Data -- Interface to 'ZeroMQ' 'ZeroMQ' is a well-known library for high-performance\n    asynchronous messaging in scalable, distributed applications.  This\n    package provides high level R wrapper functions to easily utilize\n    'ZeroMQ'. We mainly focus on interactive client/server programming\n    frameworks. For convenience, a minimal 'ZeroMQ' library (4.2.2)\n    is shipped with 'pbdZMQ', which can be used if no system installation\n    of 'ZeroMQ' is available.  A few wrapper functions compatible with\n    'rzmq' are also provided.  "
  },
  {
    "id": 17726,
    "package_name": "pdi",
    "title": "Phenotypic Index Measures for Oak Decline Severity",
    "description": "Oak declines are complex disease syndromes and consist of many visual indicators that include aspects of tree size, crown condition and trunk condition. This can cause difficulty in the manual classification of symptomatic and non-symptomatic trees from what is in reality a broad spectrum of oak tree health condition. Two phenotypic oak decline indexes have been developed to quantitatively describe and differentiate oak decline syndromes in Quercus robur. This package provides a toolkit to generate these decline indexes from phenotypic descriptors using the machine learning algorithm random forest. The methodology for generating these indexes is outlined in Finch et al. (2121) <doi:10.1016/j.foreco.2021.118948>.",
    "version": "0.4.2",
    "maintainer": "Jasen Finch <jsf9@aber.ac.uk>",
    "author": "Jasen Finch [aut, cre] (ORCID: <https://orcid.org/0000-0002-6070-7476>)",
    "url": "https://jasenfinch.github.io/pdi",
    "bug_reports": "https://github.com/jasenfinch/pdi/issues",
    "repository": "https://cran.r-project.org/package=pdi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdi Phenotypic Index Measures for Oak Decline Severity Oak declines are complex disease syndromes and consist of many visual indicators that include aspects of tree size, crown condition and trunk condition. This can cause difficulty in the manual classification of symptomatic and non-symptomatic trees from what is in reality a broad spectrum of oak tree health condition. Two phenotypic oak decline indexes have been developed to quantitatively describe and differentiate oak decline syndromes in Quercus robur. This package provides a toolkit to generate these decline indexes from phenotypic descriptors using the machine learning algorithm random forest. The methodology for generating these indexes is outlined in Finch et al. (2121) <doi:10.1016/j.foreco.2021.118948>.  "
  },
  {
    "id": 17753,
    "package_name": "pedquant",
    "title": "Public Economic Data and Quantitative Analysis",
    "description": "\n    Provides an interface to access public economic and financial data for \n    economic research and quantitative analysis. The data sources including \n    NBS, FRED, Sina, Eastmoney and etc. It also provides quantitative \n    functions for trading strategies based on the 'data.table', 'TTR', \n    'PerformanceAnalytics' and etc packages.",
    "version": "0.2.6",
    "maintainer": "Shichen Xie <xie@shichen.name>",
    "author": "Shichen Xie [aut, cre]",
    "url": "https://github.com/ShichenXie/pedquant, https://pedquant.com/",
    "bug_reports": "https://github.com/ShichenXie/pedquant/issues",
    "repository": "https://cran.r-project.org/package=pedquant",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pedquant Public Economic Data and Quantitative Analysis \n    Provides an interface to access public economic and financial data for \n    economic research and quantitative analysis. The data sources including \n    NBS, FRED, Sina, Eastmoney and etc. It also provides quantitative \n    functions for trading strategies based on the 'data.table', 'TTR', \n    'PerformanceAnalytics' and etc packages.  "
  },
  {
    "id": 17786,
    "package_name": "peptoolkit",
    "title": "A Toolkit for Using Peptide Sequences in Machine Learning",
    "description": "This toolkit is designed for manipulation and analysis of peptides. It provides functionalities to assist researchers in peptide engineering and proteomics. Users can manipulate peptides by adding amino acids at every position, count occurrences of each amino acid at each position, and transform amino acid counts based on probabilities. The package offers functionalities to select the best versus the worst peptides and analyze these peptides, which includes counting specific residues, reducing peptide sequences, extracting features through One Hot Encoding (OHE), and utilizing Quantitative Structure-Activity Relationship (QSAR) properties (based in the package 'Peptides' by Osorio et al. (2015) <doi:10.32614/RJ-2015-001>). This package is intended for both researchers and bioinformatics enthusiasts working on peptide-based projects, especially for their use with machine learning.",
    "version": "0.0.1",
    "maintainer": "Josep-Ramon Codina <jrc356@miami.edu>",
    "author": "Josep-Ramon Codina [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4391-450X>)",
    "url": "https://github.com/jrcodina/peptoolkit",
    "bug_reports": "https://github.com/jrcodina/peptoolkit/issues",
    "repository": "https://cran.r-project.org/package=peptoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "peptoolkit A Toolkit for Using Peptide Sequences in Machine Learning This toolkit is designed for manipulation and analysis of peptides. It provides functionalities to assist researchers in peptide engineering and proteomics. Users can manipulate peptides by adding amino acids at every position, count occurrences of each amino acid at each position, and transform amino acid counts based on probabilities. The package offers functionalities to select the best versus the worst peptides and analyze these peptides, which includes counting specific residues, reducing peptide sequences, extracting features through One Hot Encoding (OHE), and utilizing Quantitative Structure-Activity Relationship (QSAR) properties (based in the package 'Peptides' by Osorio et al. (2015) <doi:10.32614/RJ-2015-001>). This package is intended for both researchers and bioinformatics enthusiasts working on peptide-based projects, especially for their use with machine learning.  "
  },
  {
    "id": 17862,
    "package_name": "phdcocktail",
    "title": "Enhance the Ease of R Experience as an Emerging Researcher",
    "description": "A toolkit of functions to help: i) effortlessly transform collected data into a \n  publication ready format, ii) generate insightful visualizations from clinical data, iii) report\n  summary statistics in a publication-ready format, iv) efficiently export, save and reload R objects\n  within the framework of R projects.",
    "version": "0.1.0",
    "maintainer": "Dahham Alsoud <dahhamalsoud@gmail.com>",
    "author": "Dahham Alsoud [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-6795-6328>)",
    "url": "https://dahhamalsoud.github.io/phdcocktail/,\nhttps://github.com/DahhamAlsoud/phdcocktail",
    "bug_reports": "https://github.com/DahhamAlsoud/phdcocktail/issues",
    "repository": "https://cran.r-project.org/package=phdcocktail",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phdcocktail Enhance the Ease of R Experience as an Emerging Researcher A toolkit of functions to help: i) effortlessly transform collected data into a \n  publication ready format, ii) generate insightful visualizations from clinical data, iii) report\n  summary statistics in a publication-ready format, iv) efficiently export, save and reload R objects\n  within the framework of R projects.  "
  },
  {
    "id": 17897,
    "package_name": "photon",
    "title": "High-Performance Geocoding using 'photon'",
    "description": "Features unstructured, structured and reverse geocoding using the\n    'photon' geocoding API <https://photon.komoot.io/>.\n    Facilitates the setup of local 'photon' instances to enable offline\n    geocoding.",
    "version": "0.7.4",
    "maintainer": "Jonas Lieth <jonas.lieth@gesis.org>",
    "author": "Jonas Lieth [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3451-3176>)",
    "url": "https://github.com/jslth/photon/, https://jslth.github.io/photon/",
    "bug_reports": "https://github.com/jslth/photon/issues",
    "repository": "https://cran.r-project.org/package=photon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "photon High-Performance Geocoding using 'photon' Features unstructured, structured and reverse geocoding using the\n    'photon' geocoding API <https://photon.komoot.io/>.\n    Facilitates the setup of local 'photon' instances to enable offline\n    geocoding.  "
  },
  {
    "id": 17983,
    "package_name": "pkgKitten",
    "title": "Create Simple Packages Which Do not Upset R Package Checks",
    "description": "Provides a function kitten() which creates cute little \n packages which pass R package checks. This sets it apart from \n package.skeleton() which it calls, and which leaves imperfect files \n behind. As this is not exactly helpful for beginners, kitten() offers \n an alternative. Unit test support can be added via the 'tinytest'\n package (if present), and documentation-creation support can be\n added via 'roxygen2' (if present).",
    "version": "0.2.4",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>)",
    "url": "https://github.com/eddelbuettel/pkgkitten,\nhttps://eddelbuettel.github.io/pkgkitten/",
    "bug_reports": "https://github.com/eddelbuettel/pkgkitten/issues",
    "repository": "https://cran.r-project.org/package=pkgKitten",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pkgKitten Create Simple Packages Which Do not Upset R Package Checks Provides a function kitten() which creates cute little \n packages which pass R package checks. This sets it apart from \n package.skeleton() which it calls, and which leaves imperfect files \n behind. As this is not exactly helpful for beginners, kitten() offers \n an alternative. Unit test support can be added via the 'tinytest'\n package (if present), and documentation-creation support can be\n added via 'roxygen2' (if present).  "
  },
  {
    "id": 18047,
    "package_name": "plotlsirm",
    "title": "Plot Toolkit for Latent Space Item Response Models",
    "description": "Provides publication\u2011quality and interactive plots for exploring \n             the posterior output of Latent Space Item Response Models, including \n             Posterior Interaction Profiles, radar charts, 2\u2011D latent maps, and \n             item\u2011similarity heat maps. The methods implemented in this package \n             are based on work by Jeon, M., Jin, I. H., Schweinberger, M., Baugh, S. (2021) <doi:10.1007/s11336-021-09762-5>.",
    "version": "0.1.3",
    "maintainer": "Jinwen Luo <jevanluo@ucla.edu>",
    "author": "Jinwen Luo [aut, cre] (ORCID: <https://orcid.org/0000-0002-8511-7165>),\n  Minjeong Jeon [aut] (ORCID: <https://orcid.org/0000-0002-5880-4146>)",
    "url": "https://github.com/jevanluo/plotlsirm",
    "bug_reports": "https://github.com/jevanluo/plotlsirm/issues",
    "repository": "https://cran.r-project.org/package=plotlsirm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plotlsirm Plot Toolkit for Latent Space Item Response Models Provides publication\u2011quality and interactive plots for exploring \n             the posterior output of Latent Space Item Response Models, including \n             Posterior Interaction Profiles, radar charts, 2\u2011D latent maps, and \n             item\u2011similarity heat maps. The methods implemented in this package \n             are based on work by Jeon, M., Jin, I. H., Schweinberger, M., Baugh, S. (2021) <doi:10.1007/s11336-021-09762-5>.  "
  },
  {
    "id": 18081,
    "package_name": "plyr",
    "title": "Tools for Splitting, Applying and Combining Data",
    "description": "A set of tools that solves a common set of problems: you need\n    to break a big problem down into manageable pieces, operate on each\n    piece and then put all the pieces back together.  For example, you\n    might want to fit a model to each spatial location or time point in\n    your study, summarise data by panels or collapse high-dimensional\n    arrays to simpler summary statistics. The development of 'plyr' has\n    been generously supported by 'Becton Dickinson'.",
    "version": "1.8.9",
    "maintainer": "Hadley Wickham <hadley@rstudio.com>",
    "author": "Hadley Wickham [aut, cre]",
    "url": "http://had.co.nz/plyr, https://github.com/hadley/plyr",
    "bug_reports": "https://github.com/hadley/plyr/issues",
    "repository": "https://cran.r-project.org/package=plyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "plyr Tools for Splitting, Applying and Combining Data A set of tools that solves a common set of problems: you need\n    to break a big problem down into manageable pieces, operate on each\n    piece and then put all the pieces back together.  For example, you\n    might want to fit a model to each spatial location or time point in\n    your study, summarise data by panels or collapse high-dimensional\n    arrays to simpler summary statistics. The development of 'plyr' has\n    been generously supported by 'Becton Dickinson'.  "
  },
  {
    "id": 18103,
    "package_name": "pmxTools",
    "title": "Pharmacometric and Pharmacokinetic Toolkit",
    "description": "Pharmacometric tools for common data analytical tasks; closed-form solutions for calculating concentrations at given \n    times after dosing based on compartmental PK models (1-compartment, 2-compartment and 3-compartment, covering infusions, zero- \n    and first-order absorption, and lag times, after single doses and at steady state, per Bertrand & Mentre (2008) \n    <https://www.facm.ucl.ac.be/cooperation/Vietnam/WBI-Vietnam-October-2011/Modelling/Monolix32_PKPD_library.pdf>); parametric simulation from NONMEM-generated parameter estimates \n    and other output; and parsing, tabulating and plotting results generated by Perl-speaks-NONMEM (PsN).",
    "version": "1.5",
    "maintainer": "Justin Wilkins <justin.wilkins@occams.com>",
    "author": "Justin Wilkins [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7099-9396>),\n  Bill Denney [aut] (ORCID: <https://orcid.org/0000-0002-5759-428X>),\n  Rik Schoemaker [aut],\n  Satyaprakash Nayak [ctb],\n  Leonid Gibiansky [ctb],\n  Andrew Hooker [ctb],\n  E. Niclas Jonsson [ctb],\n  Mats O. Karlsson [ctb],\n  John Johnson [ctb]",
    "url": "https://github.com/kestrel99/pmxTools,\nhttps://kestrel99.github.io/pmxTools/",
    "bug_reports": "https://github.com/kestrel99/pmxTools/issues",
    "repository": "https://cran.r-project.org/package=pmxTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmxTools Pharmacometric and Pharmacokinetic Toolkit Pharmacometric tools for common data analytical tasks; closed-form solutions for calculating concentrations at given \n    times after dosing based on compartmental PK models (1-compartment, 2-compartment and 3-compartment, covering infusions, zero- \n    and first-order absorption, and lag times, after single doses and at steady state, per Bertrand & Mentre (2008) \n    <https://www.facm.ucl.ac.be/cooperation/Vietnam/WBI-Vietnam-October-2011/Modelling/Monolix32_PKPD_library.pdf>); parametric simulation from NONMEM-generated parameter estimates \n    and other output; and parsing, tabulating and plotting results generated by Perl-speaks-NONMEM (PsN).  "
  },
  {
    "id": 18182,
    "package_name": "popEpi",
    "title": "Functions for Epidemiological Analysis using Population Data",
    "description": "Enables computation of epidemiological statistics, including\n    those where counts or mortality rates of the reference population are\n    used.  Currently supported: excess hazard models (Dickman, Sloggett,\n    Hills, and Hakulinen (2012) <doi:10.1002/sim.1597>), rates, mean\n    survival times, relative/net survival (in particular the Ederer II\n    (Ederer and Heise (1959)) and Pohar Perme (Pohar Perme, Stare, and\n    Esteve (2012) <doi:10.1111/j.1541-0420.2011.01640.x>) estimators), and\n    standardized incidence and mortality ratios, all of which can be\n    easily adjusted for by covariates such as age. Fast splitting and\n    aggregation of 'Lexis' objects (from package 'Epi') and other\n    computations achieved using 'data.table'.",
    "version": "0.4.13",
    "maintainer": "Joonas Miettinen <joonas.miettinen@cancer.fi>",
    "author": "Joonas Miettinen [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-8624-6754>),\n  Matti Rantanen [aut],\n  Karri Seppa [ctb]",
    "url": "https://github.com/FinnishCancerRegistry/popEpi",
    "bug_reports": "https://github.com/FinnishCancerRegistry/popEpi/issues",
    "repository": "https://cran.r-project.org/package=popEpi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "popEpi Functions for Epidemiological Analysis using Population Data Enables computation of epidemiological statistics, including\n    those where counts or mortality rates of the reference population are\n    used.  Currently supported: excess hazard models (Dickman, Sloggett,\n    Hills, and Hakulinen (2012) <doi:10.1002/sim.1597>), rates, mean\n    survival times, relative/net survival (in particular the Ederer II\n    (Ederer and Heise (1959)) and Pohar Perme (Pohar Perme, Stare, and\n    Esteve (2012) <doi:10.1111/j.1541-0420.2011.01640.x>) estimators), and\n    standardized incidence and mortality ratios, all of which can be\n    easily adjusted for by covariates such as age. Fast splitting and\n    aggregation of 'Lexis' objects (from package 'Epi') and other\n    computations achieved using 'data.table'.  "
  },
  {
    "id": 18271,
    "package_name": "ppmlasso",
    "title": "Point Process Models with LASSO-Type Penalties",
    "description": "Toolkit for fitting point process models with sequences of LASSO penalties (\"regularisation paths\"), as described in Renner, I.W. and Warton, D.I. (2013) <doi:10.1111/j.1541-0420.2012.01824.x>. Regularisation paths of Poisson point process models or area-interaction models can be fitted with LASSO, adaptive LASSO or elastic net penalties. A number of criteria are available to judge the bias-variance tradeoff.",
    "version": "1.5",
    "maintainer": "Ian Renner <Ian.Renner@newcastle.edu.au>",
    "author": "Ian Renner [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ppmlasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ppmlasso Point Process Models with LASSO-Type Penalties Toolkit for fitting point process models with sequences of LASSO penalties (\"regularisation paths\"), as described in Renner, I.W. and Warton, D.I. (2013) <doi:10.1111/j.1541-0420.2012.01824.x>. Regularisation paths of Poisson point process models or area-interaction models can be fitted with LASSO, adaptive LASSO or elastic net penalties. A number of criteria are available to judge the bias-variance tradeoff.  "
  },
  {
    "id": 18416,
    "package_name": "projectLSA",
    "title": "Latent Structure Analysis Toolkit",
    "description": "Provides an interactive Shiny-based toolkit for conducting latent structure analyses, including Latent Profile Analysis (LPA), Latent Class Analysis (LCA), Latent Trait Analysis (LTA/IRT), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM). The implementation is grounded in established methodological frameworks: LPA is supported through 'tidyLPA' (Rosenberg et al., 2018) <doi:10.21105/joss.00978>, LCA through 'poLCA' (Linzer & Lewis, 2011), LTA/IRT via 'mirt' (Chalmers, 2012) <doi:10.18637/jss.v048.i06>, and EFA via 'psych' (Revelle, 2025). SEM and CFA functionalities build upon the 'lavaan' framework (Rosseel, 2012) <doi:10.18637/jss.v048.i02>. Users can upload datasets or use built-in examples, fit models, compare fit indices, visualize results, and export outputs without programming.",
    "version": "0.0.3",
    "maintainer": "Hasan Djidu <hasandjidu@gmail.com>",
    "author": "Hasan Djidu [aut, cre] (ORCID: <https://orcid.org/0000-0003-1110-6815>),\n  Heri Retnawati [ctb] (ORCID: <https://orcid.org/0000-0002-1792-5873>),\n  Samsul Hadi [ctb] (ORCID: <https://orcid.org/0000-0003-3437-2542>),\n  Haryanto [ctb] (ORCID: <https://orcid.org/0000-0003-3322-904X>)",
    "url": "https://github.com/hasandjidu/projectLSA",
    "bug_reports": "https://github.com/hasandjidu/projectLSA/issues",
    "repository": "https://cran.r-project.org/package=projectLSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "projectLSA Latent Structure Analysis Toolkit Provides an interactive Shiny-based toolkit for conducting latent structure analyses, including Latent Profile Analysis (LPA), Latent Class Analysis (LCA), Latent Trait Analysis (LTA/IRT), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Structural Equation Modeling (SEM). The implementation is grounded in established methodological frameworks: LPA is supported through 'tidyLPA' (Rosenberg et al., 2018) <doi:10.21105/joss.00978>, LCA through 'poLCA' (Linzer & Lewis, 2011), LTA/IRT via 'mirt' (Chalmers, 2012) <doi:10.18637/jss.v048.i06>, and EFA via 'psych' (Revelle, 2025). SEM and CFA functionalities build upon the 'lavaan' framework (Rosseel, 2012) <doi:10.18637/jss.v048.i02>. Users can upload datasets or use built-in examples, fit models, compare fit indices, visualize results, and export outputs without programming.  "
  },
  {
    "id": 18448,
    "package_name": "protolite",
    "title": "Highly Optimized Protocol Buffer Serializers",
    "description": "Pure C++ implementations for reading and writing several common data \n    formats based on Google protocol-buffers. Currently supports 'rexp.proto' for \n    serialized R objects, 'geobuf.proto' for binary geojson, and 'mvt.proto' for \n    vector tiles. This package uses the auto-generated C++ code by protobuf-compiler, \n    hence the entire serialization is optimized at compile time. The 'RProtoBuf' \n    package on the other hand uses the protobuf runtime library to provide a general-\n    purpose toolkit for reading and writing arbitrary protocol-buffer data in R.",
    "version": "2.3.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://github.com/jeroen/protolite\nhttps://jeroen.r-universe.dev/protolite",
    "bug_reports": "https://github.com/jeroen/protolite/issues",
    "repository": "https://cran.r-project.org/package=protolite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "protolite Highly Optimized Protocol Buffer Serializers Pure C++ implementations for reading and writing several common data \n    formats based on Google protocol-buffers. Currently supports 'rexp.proto' for \n    serialized R objects, 'geobuf.proto' for binary geojson, and 'mvt.proto' for \n    vector tiles. This package uses the auto-generated C++ code by protobuf-compiler, \n    hence the entire serialization is optimized at compile time. The 'RProtoBuf' \n    package on the other hand uses the protobuf runtime library to provide a general-\n    purpose toolkit for reading and writing arbitrary protocol-buffer data in R.  "
  },
  {
    "id": 18452,
    "package_name": "protr",
    "title": "Generating Various Numerical Representation Schemes for Protein\nSequences",
    "description": "Comprehensive toolkit for generating various numerical\n    features of protein sequences described in Xiao et al. (2015)\n    <DOI:10.1093/bioinformatics/btv042>. For full functionality,\n    the software 'ncbi-blast+' is needed, see\n    <https://blast.ncbi.nlm.nih.gov/doc/blast-help/downloadblastdata.html>\n    for more information.",
    "version": "1.7-5",
    "maintainer": "Nan Xiao <me@nanx.me>",
    "author": "Nan Xiao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0250-5673>),\n  Qing-Song Xu [aut],\n  Dong-Sheng Cao [aut],\n  Sebastian Mueller [ctb] (Alva Genomics)",
    "url": "https://nanx.me/protr/, https://github.com/nanxstats/protr,\nhttp://protr.org",
    "bug_reports": "https://github.com/nanxstats/protr/issues",
    "repository": "https://cran.r-project.org/package=protr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "protr Generating Various Numerical Representation Schemes for Protein\nSequences Comprehensive toolkit for generating various numerical\n    features of protein sequences described in Xiao et al. (2015)\n    <DOI:10.1093/bioinformatics/btv042>. For full functionality,\n    the software 'ncbi-blast+' is needed, see\n    <https://blast.ncbi.nlm.nih.gov/doc/blast-help/downloadblastdata.html>\n    for more information.  "
  },
  {
    "id": 18469,
    "package_name": "prt",
    "title": "Tabular Data Backed by Partitioned 'fst' Files",
    "description": "Intended for larger-than-memory tabular data, 'prt' objects provide an interface to read row and/or column subsets into memory as data.table objects. Data queries, constructed as 'R' expressions, are evaluated using the non-standard evaluation framework provided by 'rlang' and file-backing is powered by the fast and efficient 'fst' package.",
    "version": "0.2.1",
    "maintainer": "Nicolas Bennett <r@nbenn.ch>",
    "author": "Nicolas Bennett [aut, cre],\n  Drago Plecko [ctb]",
    "url": "https://nbenn.github.io/prt/",
    "bug_reports": "https://github.com/nbenn/prt/issues",
    "repository": "https://cran.r-project.org/package=prt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prt Tabular Data Backed by Partitioned 'fst' Files Intended for larger-than-memory tabular data, 'prt' objects provide an interface to read row and/or column subsets into memory as data.table objects. Data queries, constructed as 'R' expressions, are evaluated using the non-standard evaluation framework provided by 'rlang' and file-backing is powered by the fast and efficient 'fst' package.  "
  },
  {
    "id": 18514,
    "package_name": "psychmeta",
    "title": "Psychometric Meta-Analysis Toolkit",
    "description": "Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.",
    "version": "2.7.0",
    "maintainer": "Jeffrey A. Dahlke <jeff.dahlke.phd@gmail.com>",
    "author": "Jeffrey A. Dahlke [aut, cre],\n  Brenton M. Wiernik [aut],\n  Wesley Gardiner [ctb] (Unit tests),\n  Michael T. Brannick [ctb] (Testing),\n  Jack Kostal [ctb] (Code for reshape_mat2dat function),\n  Sean Potter [ctb] (Testing; Code for cumulative and leave1out plots),\n  John Sakaluk [ctb] (Code for funnel and forest plots),\n  Yuejia (Mandy) Teng [ctb] (Testing)",
    "url": "",
    "bug_reports": "https://github.com/psychmeta/psychmeta/issues",
    "repository": "https://cran.r-project.org/package=psychmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychmeta Psychometric Meta-Analysis Toolkit Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.  "
  },
  {
    "id": 18520,
    "package_name": "psychotree",
    "title": "Recursive Partitioning Based on Psychometric Models",
    "description": "Recursive partitioning based on psychometric models,\n  employing the general MOB algorithm (from package partykit) to obtain\n  Bradley-Terry trees, Rasch trees, rating scale and partial credit trees, and\n  MPT trees, trees for 1PL, 2PL, 3PL and 4PL models and generalized partial \n  credit models.",
    "version": "0.16-2",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Carolin Strobl [aut] (ORCID: <https://orcid.org/0000-0003-0952-3230>),\n  Florian Wickelmaier [aut],\n  Basil Komboz [aut],\n  Julia Kopf [aut],\n  Lennart Schneider [aut] (ORCID:\n    <https://orcid.org/0000-0003-4152-5308>),\n  David Dreifuss [aut],\n  Rudolf Debelak [aut] (ORCID: <https://orcid.org/0000-0001-8900-2106>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psychotree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychotree Recursive Partitioning Based on Psychometric Models Recursive partitioning based on psychometric models,\n  employing the general MOB algorithm (from package partykit) to obtain\n  Bradley-Terry trees, Rasch trees, rating scale and partial credit trees, and\n  MPT trees, trees for 1PL, 2PL, 3PL and 4PL models and generalized partial \n  credit models.  "
  },
  {
    "id": 18545,
    "package_name": "pubmedtk",
    "title": "'Pubmed' Toolkit",
    "description": "Provides various functions for retrieving and\n    interpreting information from 'Pubmed' via the API,\n    <https://www.ncbi.nlm.nih.gov/home/develop/api/>.",
    "version": "1.0.4",
    "maintainer": "Benjamin Gregory Carlisle <murph@bgcarlisle.com>",
    "author": "Benjamin Gregory Carlisle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8975-0649>)",
    "url": "https://github.com/bgcarlisle/pubmedtk",
    "bug_reports": "https://github.com/bgcarlisle/pubmedtk/issues",
    "repository": "https://cran.r-project.org/package=pubmedtk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pubmedtk 'Pubmed' Toolkit Provides various functions for retrieving and\n    interpreting information from 'Pubmed' via the API,\n    <https://www.ncbi.nlm.nih.gov/home/develop/api/>.  "
  },
  {
    "id": 18583,
    "package_name": "pwranova",
    "title": "Power Analysis of Flexible ANOVA Designs and Related Tests",
    "description": "Provides functions for conducting power analysis in ANOVA designs, including between-, within-, and mixed-factor designs, with full support for both main effects and interactions. The package allows calculation of statistical power, required total sample size, significance level, and minimal detectable effect sizes expressed as partial eta squared or Cohen's f for ANOVA terms and planned contrasts. In addition, complementary functions are included for common related tests such as t-tests and correlation tests, making the package a convenient toolkit for power analysis in experimental psychology and related fields.",
    "version": "1.0.2",
    "maintainer": "Hiroyuki Muto <mutopsy@omu.ac.jp>",
    "author": "Hiroyuki Muto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0007-6019>)",
    "url": "https://github.com/mutopsy/pwranova,\nhttps://mutopsy.github.io/pwranova/",
    "bug_reports": "https://github.com/mutopsy/pwranova/issues",
    "repository": "https://cran.r-project.org/package=pwranova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pwranova Power Analysis of Flexible ANOVA Designs and Related Tests Provides functions for conducting power analysis in ANOVA designs, including between-, within-, and mixed-factor designs, with full support for both main effects and interactions. The package allows calculation of statistical power, required total sample size, significance level, and minimal detectable effect sizes expressed as partial eta squared or Cohen's f for ANOVA terms and planned contrasts. In addition, complementary functions are included for common related tests such as t-tests and correlation tests, making the package a convenient toolkit for power analysis in experimental psychology and related fields.  "
  },
  {
    "id": 18685,
    "package_name": "qsimulatR",
    "title": "A Quantum Computer Simulator",
    "description": "A quantum computer simulator framework with up to 24 qubits. It allows to\n    define general single qubit gates and general controlled single\n    qubit gates. For convenience, it currently provides the\n    most common gates (X, Y, Z, H, Z, S, T, Rx, Ry, Rz, CNOT, SWAP, Toffoli or\n    CCNOT, Fredkin or CSWAP). 'qsimulatR' also implements noise models.\n    'qsimulatR' supports plotting of circuits and is able to\n    export circuits to 'Qiskit' <https://qiskit.org/>, a python package\n    which can be used to run on IBM's hardware <https://quantum-computing.ibm.com/>.",
    "version": "1.1.1",
    "maintainer": "Carsten Urbach <urbach@hiskp.uni-bonn.de>",
    "author": "Johann Ostmeyer [aut],\n  Carsten Urbach [aut, cre]",
    "url": "https://github.com/HISKP-LQCD/qsimulatR",
    "bug_reports": "https://github.com/HISKP-LQCD/qsimulatR/issues",
    "repository": "https://cran.r-project.org/package=qsimulatR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qsimulatR A Quantum Computer Simulator A quantum computer simulator framework with up to 24 qubits. It allows to\n    define general single qubit gates and general controlled single\n    qubit gates. For convenience, it currently provides the\n    most common gates (X, Y, Z, H, Z, S, T, Rx, Ry, Rz, CNOT, SWAP, Toffoli or\n    CCNOT, Fredkin or CSWAP). 'qsimulatR' also implements noise models.\n    'qsimulatR' supports plotting of circuits and is able to\n    export circuits to 'Qiskit' <https://qiskit.org/>, a python package\n    which can be used to run on IBM's hardware <https://quantum-computing.ibm.com/>.  "
  },
  {
    "id": 18692,
    "package_name": "qtkit",
    "title": "Quantitative Text Kit",
    "description": "Support package for the textbook \"An Introduction to\n    Quantitative Text Analysis for Linguists: Reproducible Research Using\n    R\" (Francom, 2024) <doi:10.4324/9781003393764>. Includes functions to\n    acquire, clean, and analyze text data as well as functions to document\n    and share the results of text analysis. The package is designed to be\n    used in conjunction with the book, but can also be used as a standalone\n    package for text analysis.",
    "version": "1.1.1",
    "maintainer": "Jerid Francom <francojc@wfu.edu>",
    "author": "Jerid Francom [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5972-6330>)",
    "url": "https://cran.r-project.org/package=qtkit",
    "bug_reports": "https://github.com/qtalr/qtkit/issues",
    "repository": "https://cran.r-project.org/package=qtkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qtkit Quantitative Text Kit Support package for the textbook \"An Introduction to\n    Quantitative Text Analysis for Linguists: Reproducible Research Using\n    R\" (Francom, 2024) <doi:10.4324/9781003393764>. Includes functions to\n    acquire, clean, and analyze text data as well as functions to document\n    and share the results of text analysis. The package is designed to be\n    used in conjunction with the book, but can also be used as a standalone\n    package for text analysis.  "
  },
  {
    "id": 18790,
    "package_name": "r2fireworks",
    "title": "Enhance Your 'Rmarkdown' and 'shiny' Apps with Dazzling\nFireworks Celebrations",
    "description": "Implementation of 'JQuery' <https://jquery.com> and 'CSS' styles to allow the display of fireworks on a document. Toolkit to easily incorporate celebratory splashes in 'Rmarkdown' and 'shiny' apps.",
    "version": "0.1.0",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://r2fireworks.obi.obianom.com/",
    "bug_reports": "https://github.com/oobianom/r2fireworks/issues",
    "repository": "https://cran.r-project.org/package=r2fireworks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2fireworks Enhance Your 'Rmarkdown' and 'shiny' Apps with Dazzling\nFireworks Celebrations Implementation of 'JQuery' <https://jquery.com> and 'CSS' styles to allow the display of fireworks on a document. Toolkit to easily incorporate celebratory splashes in 'Rmarkdown' and 'shiny' apps.  "
  },
  {
    "id": 18845,
    "package_name": "rFIA",
    "title": "Estimation of Forest Variables using the FIA Database",
    "description": "The goal of 'rFIA' is to increase the accessibility and use of the United States Forest Services (USFS) Forest Inventory and Analysis (FIA) Database by providing a user-friendly, open source toolkit to easily query and analyze FIA Data. Designed to accommodate a wide range of potential user objectives, 'rFIA' simplifies the estimation of forest variables from the FIA Database and allows all R users (experts and newcomers alike) to unlock the flexibility inherent to the Enhanced FIA design. Specifically, 'rFIA' improves accessibility to the spatial-temporal estimation capacity of the FIA Database by producing space-time indexed summaries of forest variables within user-defined population boundaries. Direct integration with other popular R packages (e.g., 'dplyr', 'tidyr', and 'sf') facilitates efficient space-time query and data summary, and supports common data representations and API design. The package implements design-based estimation procedures outlined by Bechtold & Patterson (2005) <doi:10.2737/SRS-GTR-80>, and has been validated against estimates and sampling errors produced by FIA 'EVALIDator'. Current development is focused on the implementation of spatially-enabled model-assisted and model-based estimators to improve population, change, and ratio estimates.",
    "version": "1.1.2",
    "maintainer": "Jeffrey Doser <jwdoser@ncsu.edu>",
    "author": "Jeffrey Doser [aut, cre],\n  Hunter Stanke [aut],\n  Andrew Finley [aut]",
    "url": "https://github.com/doserjef/rFIA,\nhttps://www.doserlab.com/files/rFIA",
    "bug_reports": "https://github.com/doserjef/rFIA/issues",
    "repository": "https://cran.r-project.org/package=rFIA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rFIA Estimation of Forest Variables using the FIA Database The goal of 'rFIA' is to increase the accessibility and use of the United States Forest Services (USFS) Forest Inventory and Analysis (FIA) Database by providing a user-friendly, open source toolkit to easily query and analyze FIA Data. Designed to accommodate a wide range of potential user objectives, 'rFIA' simplifies the estimation of forest variables from the FIA Database and allows all R users (experts and newcomers alike) to unlock the flexibility inherent to the Enhanced FIA design. Specifically, 'rFIA' improves accessibility to the spatial-temporal estimation capacity of the FIA Database by producing space-time indexed summaries of forest variables within user-defined population boundaries. Direct integration with other popular R packages (e.g., 'dplyr', 'tidyr', and 'sf') facilitates efficient space-time query and data summary, and supports common data representations and API design. The package implements design-based estimation procedures outlined by Bechtold & Patterson (2005) <doi:10.2737/SRS-GTR-80>, and has been validated against estimates and sampling errors produced by FIA 'EVALIDator'. Current development is focused on the implementation of spatially-enabled model-assisted and model-based estimators to improve population, change, and ratio estimates.  "
  },
  {
    "id": 18857,
    "package_name": "rJavaEnv",
    "title": "'Java' Environments for R Projects",
    "description": "Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.",
    "version": "0.3.0",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6690-5345>),\n  Chung-hong Chan [aut] (ORCID: <https://orcid.org/0000-0002-6232-7530>),\n  Mauricio Vargas [ctb] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\n  Hadley Wickham [ctb] (use_java feature suggestion and PR review),\n  Enrique Mondragon-Estrada [ctb] (ORCID:\n    <https://orcid.org/0009-0004-5592-1728>),\n  Jonas Lieth [ctb] (ORCID: <https://orcid.org/0000-0002-3451-3176>)",
    "url": "https://github.com/e-kotov/rJavaEnv,\nhttps://www.ekotov.pro/rJavaEnv/",
    "bug_reports": "https://github.com/e-kotov/rJavaEnv/issues",
    "repository": "https://cran.r-project.org/package=rJavaEnv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rJavaEnv 'Java' Environments for R Projects Quickly install 'Java Development Kit (JDK)' without\n    administrative privileges and set environment variables in current R\n    session or project to solve common issues with 'Java' environment\n    management in 'R'. Recommended to users of 'Java'/'rJava'-dependent\n    'R' packages such as 'r5r', 'opentripplanner', 'xlsx', 'openNLP',\n    'rWeka', 'RJDBC', 'tabulapdf', and many more. 'rJavaEnv' prevents\n    common problems like 'Java' not found, 'Java' version conflicts,\n    missing 'Java' installations, and the inability to install 'Java' due\n    to lack of administrative privileges.  'rJavaEnv' automates the\n    download, installation, and setup of the 'Java' on a per-project basis\n    by setting the relevant 'JAVA_HOME' in the current 'R' session or the\n    current working directory (via '.Rprofile', with the user's consent).\n    Similar to what 'renv' does for 'R' packages, 'rJavaEnv' allows\n    different 'Java' versions to be used across different projects, but\n    can also be configured to allow multiple versions within the same\n    project (e.g.  with the help of 'targets' package). Note: there are a\n    few extra steps for 'Linux' users, who don't have any 'Java'\n    previously installed in their system, and who prefer package\n    installation from source, rather then installing binaries from 'Posit\n    Package Manager'. See documentation for details.  "
  },
  {
    "id": 18859,
    "package_name": "rKOMICS",
    "title": "Minicircle Sequence Classes (MSC) Analyses",
    "description": "This is a analysis toolkit to streamline the analyses of minicircle sequence diversity in population-scale genome projects. rKOMICS is a user-friendly R package that has simple installation requirements and that is applicable to all 27 trypanosomatid genera. Once minicircle sequence alignments are generated, rKOMICS allows to examine, summarize and visualize minicircle sequence diversity within and between samples through the analyses of minicircle sequence clusters. We showcase the functionalities of the (r)KOMICS tool suite using a whole-genome sequencing dataset from a recently published study on the history of diversification of the Leishmania braziliensis species complex in Peru. Analyses of population diversity and structure highlighted differences in minicircle sequence richness and composition between Leishmania subspecies, and between subpopulations within subspecies. The rKOMICS package establishes a critical framework to manipulate, explore and extract biologically relevant information from mitochondrial minicircle assemblies in tens to hundreds of samples simultaneously and efficiently. This should facilitate research that aims to develop new molecular markers for identifying species-specific minicircles, or to study the ancestry of parasites for complementary insights into their evolutionary history. ***** !! WARNING: this package relies on dependencies from Bioconductor. For Mac users, this can generate errors when installing rKOMICS. Install Bioconductor and ComplexHeatmap at advance: install.packages(\"BiocManager\"); BiocManager::install(\"ComplexHeatmap\") *****.",
    "version": "1.3",
    "maintainer": "Manon Geerts <geertsmanon@gmail.com>",
    "author": "Frederik Van den Broeck [aut],\n  Manon Geerts [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rKOMICS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rKOMICS Minicircle Sequence Classes (MSC) Analyses This is a analysis toolkit to streamline the analyses of minicircle sequence diversity in population-scale genome projects. rKOMICS is a user-friendly R package that has simple installation requirements and that is applicable to all 27 trypanosomatid genera. Once minicircle sequence alignments are generated, rKOMICS allows to examine, summarize and visualize minicircle sequence diversity within and between samples through the analyses of minicircle sequence clusters. We showcase the functionalities of the (r)KOMICS tool suite using a whole-genome sequencing dataset from a recently published study on the history of diversification of the Leishmania braziliensis species complex in Peru. Analyses of population diversity and structure highlighted differences in minicircle sequence richness and composition between Leishmania subspecies, and between subpopulations within subspecies. The rKOMICS package establishes a critical framework to manipulate, explore and extract biologically relevant information from mitochondrial minicircle assemblies in tens to hundreds of samples simultaneously and efficiently. This should facilitate research that aims to develop new molecular markers for identifying species-specific minicircles, or to study the ancestry of parasites for complementary insights into their evolutionary history. ***** !! WARNING: this package relies on dependencies from Bioconductor. For Mac users, this can generate errors when installing rKOMICS. Install Bioconductor and ComplexHeatmap at advance: install.packages(\"BiocManager\"); BiocManager::install(\"ComplexHeatmap\") *****.  "
  },
  {
    "id": 18893,
    "package_name": "rSFA",
    "title": "Slow Feature Analysis",
    "description": "Slow Feature Analysis (SFA), ported to R based on\n    'matlab' implementations of SFA: 'SFA toolkit' 1.0 by Pietro Berkes and 'SFA toolkit'\n    2.8 by Wolfgang Konen.",
    "version": "1.5",
    "maintainer": "Martin Zaefferer <martin.zaefferer@gmx.de>",
    "author": "Wolfgang Konen <wolfgang.konen@fh-koeln.de>, Martin Zaefferer,\n    Patrick Koch; Bug hunting and testing by Ayodele Fasika, Ashwin\n    Kumar, Prawyn Jebakumar",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rSFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rSFA Slow Feature Analysis Slow Feature Analysis (SFA), ported to R based on\n    'matlab' implementations of SFA: 'SFA toolkit' 1.0 by Pietro Berkes and 'SFA toolkit'\n    2.8 by Wolfgang Konen.  "
  },
  {
    "id": 19007,
    "package_name": "raptr",
    "title": "Representative and Adequate Prioritization Toolkit in R",
    "description": "Biodiversity is in crisis. The overarching aim of conservation\n    is to preserve biodiversity patterns and processes. To this end, protected\n    areas are established to buffer species and preserve biodiversity processes.\n    But resources are limited and so protected areas must be cost-effective.\n    This package contains tools to generate plans for protected areas\n    (prioritizations), using spatially explicit targets for biodiversity\n    patterns and processes. To obtain solutions in a feasible amount  of time,\n    this package uses the commercial 'Gurobi' software (obtained from\n    <https://www.gurobi.com/>). For more information on using\n    this package, see Hanson et al. (2018) <doi:10.1111/2041-210X.12862>.",
    "version": "1.0.1",
    "maintainer": "Jeffrey O Hanson <jeffrey.hanson@uqconnect.edu.au>",
    "author": "Jeffrey O Hanson [aut, cre],\n  Jonathan R Rhodes [aut],\n  Hugh P Possingham [aut],\n  Richard A Fuller [aut]",
    "url": "https://jeffrey-hanson.com/raptr/,\nhttps://github.com/jeffreyhanson/raptr",
    "bug_reports": "https://github.com/jeffreyhanson/raptr/issues",
    "repository": "https://cran.r-project.org/package=raptr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raptr Representative and Adequate Prioritization Toolkit in R Biodiversity is in crisis. The overarching aim of conservation\n    is to preserve biodiversity patterns and processes. To this end, protected\n    areas are established to buffer species and preserve biodiversity processes.\n    But resources are limited and so protected areas must be cost-effective.\n    This package contains tools to generate plans for protected areas\n    (prioritizations), using spatially explicit targets for biodiversity\n    patterns and processes. To obtain solutions in a feasible amount  of time,\n    this package uses the commercial 'Gurobi' software (obtained from\n    <https://www.gurobi.com/>). For more information on using\n    this package, see Hanson et al. (2018) <doi:10.1111/2041-210X.12862>.  "
  },
  {
    "id": 19066,
    "package_name": "rbiom",
    "title": "Read/Write, Analyze, and Visualize 'BIOM' Data",
    "description": "\n    A toolkit for working with Biological Observation Matrix ('BIOM') files.\n    Read/write all 'BIOM' formats. Compute rarefaction, alpha diversity, and \n    beta diversity (including 'UniFrac'). Summarize counts by taxonomic level. \n    Subset based on metadata. Generate visualizations and statistical analyses. \n    CPU intensive operations are coded in C for speed.",
    "version": "2.2.1",
    "maintainer": "Daniel P. Smith <dansmith01@gmail.com>",
    "author": "Daniel P. Smith [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2479-2044>),\n  Alkek Center for Metagenomics and Microbiome Research [cph, fnd]",
    "url": "https://cmmr.github.io/rbiom/, https://github.com/cmmr/rbiom",
    "bug_reports": "https://github.com/cmmr/rbiom/issues",
    "repository": "https://cran.r-project.org/package=rbiom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbiom Read/Write, Analyze, and Visualize 'BIOM' Data \n    A toolkit for working with Biological Observation Matrix ('BIOM') files.\n    Read/write all 'BIOM' formats. Compute rarefaction, alpha diversity, and \n    beta diversity (including 'UniFrac'). Summarize counts by taxonomic level. \n    Subset based on metadata. Generate visualizations and statistical analyses. \n    CPU intensive operations are coded in C for speed.  "
  },
  {
    "id": 19094,
    "package_name": "rcdklibs",
    "title": "The CDK Libraries Packaged for R",
    "description": "An R interface to the Chemistry Development Kit, a Java library\n    for chemoinformatics. Given the size of the library itself, this package is\n    not expected to change very frequently. To make use of the CDK within R, it is\n    suggested that you use the 'rcdk' package. Note that it is possible to directly\n    interact with the CDK using 'rJava'. However 'rcdk' exposes functionality in a more\n    idiomatic way. The CDK library itself is released as LGPL and the sources can be\n    obtained from <https://github.com/cdk/cdk>.",
    "version": "2.9",
    "maintainer": "Zachary Charlop-Powers <zach.charlop.powers@gmail.com>",
    "author": "Rajarshi Guha [aut] (ORCID: <https://orcid.org/0000-0001-7403-8819>),\n  Zachary Charlop-Powers [cre] (ORCID:\n    <https://orcid.org/0000-0001-8816-4680>),\n  CDK Project [ctb, cph] (CDK Java library contained in multiple jar\n    files)",
    "url": "https://cdk-r.github.io/rcdklibs/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rcdklibs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcdklibs The CDK Libraries Packaged for R An R interface to the Chemistry Development Kit, a Java library\n    for chemoinformatics. Given the size of the library itself, this package is\n    not expected to change very frequently. To make use of the CDK within R, it is\n    suggested that you use the 'rcdk' package. Note that it is possible to directly\n    interact with the CDK using 'rJava'. However 'rcdk' exposes functionality in a more\n    idiomatic way. The CDK library itself is released as LGPL and the sources can be\n    obtained from <https://github.com/cdk/cdk>.  "
  },
  {
    "id": 19106,
    "package_name": "rcoder",
    "title": "Lightweight Data Structure for Recoding Categorical Data without\nFactors",
    "description": "A data structure and toolkit for documenting and recoding\n    categorical data that can be shared in other statistical software.",
    "version": "0.3.0",
    "maintainer": "Patrick Anker <psanker@nyu.edu>",
    "author": "Patrick Anker [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2302-0445>),\n  Global TIES for Children [cph]\n    (https://steinhardt.nyu.edu/ihdsc/global-ties)",
    "url": "https://github.com/nyuglobalties/rcoder",
    "bug_reports": "https://github.com/nyuglobalties/rcoder/issues",
    "repository": "https://cran.r-project.org/package=rcoder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcoder Lightweight Data Structure for Recoding Categorical Data without\nFactors A data structure and toolkit for documenting and recoding\n    categorical data that can be shared in other statistical software.  "
  },
  {
    "id": 19220,
    "package_name": "reclin2",
    "title": "Record Linkage Toolkit",
    "description": "Functions to assist in performing probabilistic record linkage and\n    deduplication: generating pairs, comparing records, em-algorithm for\n    estimating m- and u-probabilities\n    (I. Fellegi & A. Sunter (1969) <doi:10.1080/01621459.1969.10501049>, \n    T.N. Herzog, F.J. Scheuren, & W.E. Winkler (2007), \n    \"Data Quality and Record Linkage Techniques\", ISBN:978-0-387-69502-0),\n    forcing one-to-one matching. Can also be\n    used for pre- and post-processing for machine learning methods for record\n    linkage. Focus is on memory, CPU performance and flexibility. ",
    "version": "0.6.0",
    "maintainer": "Jan van der Laan <r@eoos.dds.nl>",
    "author": "Jan van der Laan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0693-1514>)",
    "url": "https://github.com/djvanderlaan/reclin2",
    "bug_reports": "https://github.com/djvanderlaan/reclin2/issues",
    "repository": "https://cran.r-project.org/package=reclin2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reclin2 Record Linkage Toolkit Functions to assist in performing probabilistic record linkage and\n    deduplication: generating pairs, comparing records, em-algorithm for\n    estimating m- and u-probabilities\n    (I. Fellegi & A. Sunter (1969) <doi:10.1080/01621459.1969.10501049>, \n    T.N. Herzog, F.J. Scheuren, & W.E. Winkler (2007), \n    \"Data Quality and Record Linkage Techniques\", ISBN:978-0-387-69502-0),\n    forcing one-to-one matching. Can also be\n    used for pre- and post-processing for machine learning methods for record\n    linkage. Focus is on memory, CPU performance and flexibility.   "
  },
  {
    "id": 19234,
    "package_name": "reconstructr",
    "title": "Session Reconstruction and Analysis",
    "description": "Functions to reconstruct sessions from web log or other user trace data\n             and calculate various metrics around them, producing tabular,\n             output that is compatible with 'dplyr' or 'data.table' centered processes.",
    "version": "2.0.4",
    "maintainer": "Os Keyes <ironholds@gmail.com>",
    "author": "Os Keyes",
    "url": "https://github.com/Ironholds/reconstructr",
    "bug_reports": "https://github.com/Ironholds/reconstructr/issues",
    "repository": "https://cran.r-project.org/package=reconstructr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reconstructr Session Reconstruction and Analysis Functions to reconstruct sessions from web log or other user trace data\n             and calculate various metrics around them, producing tabular,\n             output that is compatible with 'dplyr' or 'data.table' centered processes.  "
  },
  {
    "id": 19235,
    "package_name": "recorder",
    "title": "Toolkit to Validate New Data for a Predictive Model",
    "description": "A lightweight toolkit to validate new observations when computing\n    their predictions with a predictive model. The validation process \n    consists of two steps: (1) record relevant statistics and meta data of the\n    variables in the original training data for the predictive model and\n    (2) use these data to run a set of basic validation tests on the new set of \n    observations.",
    "version": "0.8.2",
    "maintainer": "Lars Kjeldgaard <lars_kjeldgaard@hotmail.com>",
    "author": "Lars Kjeldgaard [aut, cre]",
    "url": "https://github.com/smaakage85/recorder",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=recorder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "recorder Toolkit to Validate New Data for a Predictive Model A lightweight toolkit to validate new observations when computing\n    their predictions with a predictive model. The validation process \n    consists of two steps: (1) record relevant statistics and meta data of the\n    variables in the original training data for the predictive model and\n    (2) use these data to run a set of basic validation tests on the new set of \n    observations.  "
  },
  {
    "id": 19298,
    "package_name": "regrrr",
    "title": "Toolkit for Compiling, (Post-Hoc) Testing, and Plotting\nRegression Results",
    "description": "Compiling regression results into a publishable format, conducting post-hoc hypothesis testing, and plotting moderating effects (the effect of X on Y becomes stronger/weaker as Z increases).",
    "version": "0.1.3",
    "maintainer": "Rui K. Yang <rkzyang@gmail.com>",
    "author": "Rui K. Yang [aut, cre],\n  Luyao Peng [aut]",
    "url": "",
    "bug_reports": "https://github.com/RkzYang/regrrr/issues",
    "repository": "https://cran.r-project.org/package=regrrr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "regrrr Toolkit for Compiling, (Post-Hoc) Testing, and Plotting\nRegression Results Compiling regression results into a publishable format, conducting post-hoc hypothesis testing, and plotting moderating effects (the effect of X on Y becomes stronger/weaker as Z increases).  "
  },
  {
    "id": 19349,
    "package_name": "replacer",
    "title": "A Value Replacement Utility",
    "description": "Updates values within csv format data files using a custom, User-built\n    csv format lookup file. Based on 'data.table' package.",
    "version": "1.0.2",
    "maintainer": "Bandur Dragos <dbandur@sympatico.ca>",
    "author": "Bandur Dragos [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=replacer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "replacer A Value Replacement Utility Updates values within csv format data files using a custom, User-built\n    csv format lookup file. Based on 'data.table' package.  "
  },
  {
    "id": 19437,
    "package_name": "rexpokit",
    "title": "R Wrappers for EXPOKIT; Other Matrix Functions",
    "description": "Wraps some of the matrix exponentiation \n        utilities from EXPOKIT (<http://www.maths.uq.edu.au/expokit/>), \n        a FORTRAN library that is widely recommended for matrix \n        exponentiation (Sidje RB, 1998. \"Expokit: A Software Package\n        for Computing Matrix Exponentials.\" ACM Trans. Math. Softw.\n        24(1): 130-156).  EXPOKIT includes functions for \n        exponentiating both small, dense matrices, and large, sparse\n        matrices (in sparse matrices, most of the cells have value 0).\n        Rapid matrix exponentiation is useful in phylogenetics when we \n        have a large number of states (as we do when we are inferring \n        the history of transitions between the possible geographic \n        ranges of a species), but is probably useful in other ways as \n        well. NOTE: In case FORTRAN checks temporarily get rexpokit \n        archived on CRAN, see archived binaries at GitHub in: \n        nmatzke/Matzke_R_binaries (binaries install without compilation \n        of source code).",
    "version": "0.26.6.14",
    "maintainer": "Nicholas J. Matzke <nickmatzke.ncse@gmail.com>",
    "author": "Nicholas J. Matzke [aut, cre, cph],\n  Roger B. Sidje [aut, cph],\n  Drew Schmidt [aut]",
    "url": "http://phylo.wikidot.com/rexpokit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rexpokit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rexpokit R Wrappers for EXPOKIT; Other Matrix Functions Wraps some of the matrix exponentiation \n        utilities from EXPOKIT (<http://www.maths.uq.edu.au/expokit/>), \n        a FORTRAN library that is widely recommended for matrix \n        exponentiation (Sidje RB, 1998. \"Expokit: A Software Package\n        for Computing Matrix Exponentials.\" ACM Trans. Math. Softw.\n        24(1): 130-156).  EXPOKIT includes functions for \n        exponentiating both small, dense matrices, and large, sparse\n        matrices (in sparse matrices, most of the cells have value 0).\n        Rapid matrix exponentiation is useful in phylogenetics when we \n        have a large number of states (as we do when we are inferring \n        the history of transitions between the possible geographic \n        ranges of a species), but is probably useful in other ways as \n        well. NOTE: In case FORTRAN checks temporarily get rexpokit \n        archived on CRAN, see archived binaries at GitHub in: \n        nmatzke/Matzke_R_binaries (binaries install without compilation \n        of source code).  "
  },
  {
    "id": 19456,
    "package_name": "rfviz",
    "title": "Interactive Visualization Tool for Random Forests",
    "description": "An interactive data visualization and exploration toolkit\n    that implements Breiman and Cutler's original random forest Java based        \n    visualization tools in R, for supervised and unsupervised classification and \n    regression within the algorithm random forest. ",
    "version": "1.0.1",
    "maintainer": "Chris Kuchar <chrisjkuchar@gmail.com>",
    "author": "Chris Kuchar [aut, cre]",
    "url": "https://www.stat.berkeley.edu/~breiman/RandomForests/cc_graphics.htm",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rfviz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rfviz Interactive Visualization Tool for Random Forests An interactive data visualization and exploration toolkit\n    that implements Breiman and Cutler's original random forest Java based        \n    visualization tools in R, for supervised and unsupervised classification and \n    regression within the algorithm random forest.   "
  },
  {
    "id": 19629,
    "package_name": "rmutil",
    "title": "Utilities for Nonlinear Regression and Repeated Measurements\nModels",
    "description": "A toolkit of functions for nonlinear regression and repeated\n    measurements not to be used by itself but called by other Lindsey packages such\n    as 'gnlm', 'stable', 'growth', 'repeated', and 'event' \n    (available at <https://www.commanster.eu/rcode.html>).",
    "version": "1.1.10",
    "maintainer": "Bruce Swihart <bruce.swihart@gmail.com>",
    "author": "Bruce Swihart [cre, aut],\n  Jim Lindsey [aut] (Jim created this package, Bruce is maintaining the\n    CRAN version),\n  K. Sikorski [ctb, cph] (Wrote TOMS614/INTHP, https://calgo.acm.org/),\n  F. Stenger [ctb, cph] (Wrote TOMS614/INTHP, https://calgo.acm.org/),\n  J. Schwing [ctb, cph] (Wrote TOMS614/INTHP, https://calgo.acm.org/)",
    "url": "https://www.commanster.eu/rcode.html",
    "bug_reports": "https://github.com/swihart/rmutil/issues",
    "repository": "https://cran.r-project.org/package=rmutil",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmutil Utilities for Nonlinear Regression and Repeated Measurements\nModels A toolkit of functions for nonlinear regression and repeated\n    measurements not to be used by itself but called by other Lindsey packages such\n    as 'gnlm', 'stable', 'growth', 'repeated', and 'event' \n    (available at <https://www.commanster.eu/rcode.html>).  "
  },
  {
    "id": 19641,
    "package_name": "rnmamod",
    "title": "Bayesian Network Meta-Analysis with Missing Participants",
    "description": "A comprehensive suite of functions to perform and visualise \n    pairwise and network meta-analysis with aggregate binary or continuous\n    missing participant outcome data. The package covers core Bayesian one-stage\n    models implemented in a systematic review with multiple interventions, \n    including fixed-effect and random-effects network meta-analysis, \n    meta-regression, evaluation of the consistency assumption via the \n    node-splitting approach and the unrelated mean effects model (original and\n    revised model proposed by Spineli, (2022) <doi:10.1177/0272989X211068005>), and \n    sensitivity analysis (see Spineli et al., (2021) <doi:10.1186/s12916-021-02195-y>). \n    Missing participant outcome data are addressed in all models of the package \n    (see Spineli, (2019) <doi:10.1186/s12874-019-0731-y>, Spineli et al., (2019) \n    <doi:10.1002/sim.8207>, Spineli, (2019) <doi:10.1016/j.jclinepi.2018.09.002>,\n    and Spineli et al., (2021) <doi:10.1002/jrsm.1478>). \n    The robustness to primary analysis results can also be investigated using a \n    novel intuitive index (see Spineli et al., (2021) <doi:10.1177/0962280220983544>). \n    Methods to evaluate the transitivity assumption using trial dissimilarities\n    and hierarchical clustering are provided \n    (see Spineli, (2024) <doi:10.1186/s12874-024-02436-7>, and \n    Spineli et al., (2025) <doi:10.1002/sim.70068>). A novel index to \n    facilitate interpretation of local inconsistency is also available \n    (see Spineli, (2024) <doi:10.1186/s13643-024-02680-4>)\n    The package also offers a rich, user-friendly visualisation toolkit that aids \n    in appraising and interpreting the results thoroughly and preparing the \n    manuscript for journal submission. The visualisation tools comprise the \n    network plot, forest plots, panel of diagnostic plots, heatmaps on the extent \n    of missing participant outcome data in the network, league heatmaps on \n    estimation and prediction, rankograms, Bland-Altman plot, leverage plot, \n    deviance scatterplot, heatmap of robustness, barplot of Kullback-Leibler \n    divergence, heatmap of comparison dissimilarities and dendrogram of comparison \n    clustering. The package also allows the user to export the results to an Excel \n    file at the working directory.",
    "version": "0.5.0",
    "maintainer": "Loukia Spineli <Spineli.Loukia@mh-hannover.de>",
    "author": "Loukia Spineli [aut, cre],\n  Chrysostomos Kalyvas [ctb],\n  Katerina Papadimitropoulou [ctb]",
    "url": "https://CRAN.R-project.org/package=rnmamod,\nhttps://github.com/LoukiaSpin/rnmamod,\nhttps://loukiaspin.github.io/rnmamod/",
    "bug_reports": "https://github.com/LoukiaSpin/rnmamod/issues",
    "repository": "https://cran.r-project.org/package=rnmamod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rnmamod Bayesian Network Meta-Analysis with Missing Participants A comprehensive suite of functions to perform and visualise \n    pairwise and network meta-analysis with aggregate binary or continuous\n    missing participant outcome data. The package covers core Bayesian one-stage\n    models implemented in a systematic review with multiple interventions, \n    including fixed-effect and random-effects network meta-analysis, \n    meta-regression, evaluation of the consistency assumption via the \n    node-splitting approach and the unrelated mean effects model (original and\n    revised model proposed by Spineli, (2022) <doi:10.1177/0272989X211068005>), and \n    sensitivity analysis (see Spineli et al., (2021) <doi:10.1186/s12916-021-02195-y>). \n    Missing participant outcome data are addressed in all models of the package \n    (see Spineli, (2019) <doi:10.1186/s12874-019-0731-y>, Spineli et al., (2019) \n    <doi:10.1002/sim.8207>, Spineli, (2019) <doi:10.1016/j.jclinepi.2018.09.002>,\n    and Spineli et al., (2021) <doi:10.1002/jrsm.1478>). \n    The robustness to primary analysis results can also be investigated using a \n    novel intuitive index (see Spineli et al., (2021) <doi:10.1177/0962280220983544>). \n    Methods to evaluate the transitivity assumption using trial dissimilarities\n    and hierarchical clustering are provided \n    (see Spineli, (2024) <doi:10.1186/s12874-024-02436-7>, and \n    Spineli et al., (2025) <doi:10.1002/sim.70068>). A novel index to \n    facilitate interpretation of local inconsistency is also available \n    (see Spineli, (2024) <doi:10.1186/s13643-024-02680-4>)\n    The package also offers a rich, user-friendly visualisation toolkit that aids \n    in appraising and interpreting the results thoroughly and preparing the \n    manuscript for journal submission. The visualisation tools comprise the \n    network plot, forest plots, panel of diagnostic plots, heatmaps on the extent \n    of missing participant outcome data in the network, league heatmaps on \n    estimation and prediction, rankograms, Bland-Altman plot, leverage plot, \n    deviance scatterplot, heatmap of robustness, barplot of Kullback-Leibler \n    divergence, heatmap of comparison dissimilarities and dendrogram of comparison \n    clustering. The package also allows the user to export the results to an Excel \n    file at the working directory.  "
  },
  {
    "id": 19706,
    "package_name": "rock",
    "title": "Reproducible Open Coding Kit",
    "description": "The Reproducible Open Coding Kit ('ROCK', and this package, 'rock')\n  was developed to facilitate reproducible and open coding, specifically\n  geared towards qualitative research methods. It was developed to be both\n  human- and machine-readable, in the spirit of MarkDown and 'YAML'. The idea is\n  that this makes it relatively easy to write other functions and packages\n  to process 'ROCK' files. The 'rock' package contains functions for basic\n  coding and analysis, such as collecting and showing coded fragments and\n  prettifying sources, as well as a number of advanced analyses such as the\n  Qualitative Network Approach and Qualitative/Unified Exploration of State\n  Transitions. The 'ROCK' and this 'rock' package are described in the ROCK\n  book (Z\u00f6rg\u0151 & Peters, 2022; <https://rockbook.org>), in Z\u00f6rg\u0151 & Peters\n  (2024) <doi:10.1080/21642850.2022.2119144> and Peters, Z\u00f6rg\u0151 and van der\n  Maas (2022) <doi:10.31234/osf.io/cvf52>, and more information and\n  tutorials are available at <https://rock.science>.",
    "version": "0.9.6",
    "maintainer": "Gjalt-Jorn Peters <rock@opens.science>",
    "author": "Gjalt-Jorn Peters [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0336-9589>),\n  Szilvia Z\u00f6rg\u0151 [aut] (ORCID: <https://orcid.org/0000-0002-6916-2097>)",
    "url": "https://rock.opens.science",
    "bug_reports": "https://codeberg.org/R-packages/rock/issues",
    "repository": "https://cran.r-project.org/package=rock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rock Reproducible Open Coding Kit The Reproducible Open Coding Kit ('ROCK', and this package, 'rock')\n  was developed to facilitate reproducible and open coding, specifically\n  geared towards qualitative research methods. It was developed to be both\n  human- and machine-readable, in the spirit of MarkDown and 'YAML'. The idea is\n  that this makes it relatively easy to write other functions and packages\n  to process 'ROCK' files. The 'rock' package contains functions for basic\n  coding and analysis, such as collecting and showing coded fragments and\n  prettifying sources, as well as a number of advanced analyses such as the\n  Qualitative Network Approach and Qualitative/Unified Exploration of State\n  Transitions. The 'ROCK' and this 'rock' package are described in the ROCK\n  book (Z\u00f6rg\u0151 & Peters, 2022; <https://rockbook.org>), in Z\u00f6rg\u0151 & Peters\n  (2024) <doi:10.1080/21642850.2022.2119144> and Peters, Z\u00f6rg\u0151 and van der\n  Maas (2022) <doi:10.31234/osf.io/cvf52>, and more information and\n  tutorials are available at <https://rock.science>.  "
  },
  {
    "id": 19805,
    "package_name": "rqdatatable",
    "title": "'rquery' for 'data.table'",
    "description": "Implements the 'rquery' piped Codd-style query algebra using 'data.table'.  This allows\n   for a high-speed in memory implementation of Codd-style data manipulation tools.",
    "version": "1.3.3",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/rqdatatable/,\nhttps://winvector.github.io/rqdatatable/",
    "bug_reports": "https://github.com/WinVector/rqdatatable/issues",
    "repository": "https://cran.r-project.org/package=rqdatatable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rqdatatable 'rquery' for 'data.table' Implements the 'rquery' piped Codd-style query algebra using 'data.table'.  This allows\n   for a high-speed in memory implementation of Codd-style data manipulation tools.  "
  },
  {
    "id": 19807,
    "package_name": "rquery",
    "title": "Relational Query Generator for Data Manipulation at Scale",
    "description": "A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.",
    "version": "1.4.99",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/rquery/,\nhttps://winvector.github.io/rquery/",
    "bug_reports": "https://github.com/WinVector/rquery/issues",
    "repository": "https://cran.r-project.org/package=rquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rquery Relational Query Generator for Data Manipulation at Scale A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.  "
  },
  {
    "id": 19885,
    "package_name": "rsurveycto",
    "title": "Interact with Data on 'SurveyCTO'",
    "description": "'SurveyCTO' is a platform for mobile data collection in offline settings.\n  The 'rsurveycto' R package uses the 'SurveyCTO' REST API\n  <https://docs.surveycto.com/05-exporting-and-publishing-data/05-api-access/01.api-access.html>\n  to read datasets and forms from a 'SurveyCTO' server into R as 'data.table's\n  and to download file attachments. The package also has limited support to\n  write datasets to a server.",
    "version": "0.2.2",
    "maintainer": "Jake Hughey <jake@agency.fund>",
    "author": "Jake Hughey [aut, cre],\n  Robert On [aut]",
    "url": "https://agency-fund.github.io/rsurveycto/,\nhttps://github.com/agency-fund/rsurveycto",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rsurveycto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsurveycto Interact with Data on 'SurveyCTO' 'SurveyCTO' is a platform for mobile data collection in offline settings.\n  The 'rsurveycto' R package uses the 'SurveyCTO' REST API\n  <https://docs.surveycto.com/05-exporting-and-publishing-data/05-api-access/01.api-access.html>\n  to read datasets and forms from a 'SurveyCTO' server into R as 'data.table's\n  and to download file attachments. The package also has limited support to\n  write datasets to a server.  "
  },
  {
    "id": 19913,
    "package_name": "rtk",
    "title": "Rarefaction Tool Kit",
    "description": "Rarefy data, calculate diversity and plot the results.",
    "version": "0.2.6.1",
    "maintainer": "Paul Saary <rtk@paulsaary.de>",
    "author": "Paul Saary, Falk Hildebrand",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rtk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtk Rarefaction Tool Kit Rarefy data, calculate diversity and plot the results.  "
  },
  {
    "id": 19918,
    "package_name": "rtms",
    "title": "R Toolkit for Mass Spectrometry",
    "description": "Quickly imports, processes, analyzes, and visualizes mass-spectrometric \n\tdata.  Includes functions for easily extracting specific data and measurements\n\tfrom large (multi-gigabyte) raw Bruker data files, as well as a set of S3 object\n\tclasses for manipulating and measuring mass spectrometric peaks and plotting\n\tpeaks and spectra using the 'ggplot2' package.",
    "version": "0.2.0",
    "maintainer": "Nathaniel Twarog <nathaniel.twarog@stjude.org>",
    "author": "Mary Ashley Rimmer [aut],\n  Nathaniel Twarog [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rtms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtms R Toolkit for Mass Spectrometry Quickly imports, processes, analyzes, and visualizes mass-spectrometric \n\tdata.  Includes functions for easily extracting specific data and measurements\n\tfrom large (multi-gigabyte) raw Bruker data files, as well as a set of S3 object\n\tclasses for manipulating and measuring mass spectrometric peaks and plotting\n\tpeaks and spectra using the 'ggplot2' package.  "
  },
  {
    "id": 19942,
    "package_name": "runcharter",
    "title": "Automatically Plot, Analyse and Revises Limits of Multiple Run\nCharts",
    "description": "Plots multiple run charts, finds successive signals of \n    improvement, and revises medians when each signal occurs. Finds runs\n    above, below, or on both sides of the median, and returns a plot and\n    a data.table summarising original medians and any revisions, for all\n    groups within the supplied data.",
    "version": "0.2.0",
    "maintainer": "John MacKintosh <johnmackintosh.jm@gmail.com>",
    "author": "John MacKintosh [aut, cre]",
    "url": "https://github.com/johnmackintosh/runcharter",
    "bug_reports": "https://github.com/johnmackintosh/runcharter/issues",
    "repository": "https://cran.r-project.org/package=runcharter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "runcharter Automatically Plot, Analyse and Revises Limits of Multiple Run\nCharts Plots multiple run charts, finds successive signals of \n    improvement, and revises medians when each signal occurs. Finds runs\n    above, below, or on both sides of the median, and returns a plot and\n    a data.table summarising original medians and any revisions, for all\n    groups within the supplied data.  "
  },
  {
    "id": 20119,
    "package_name": "sarsop",
    "title": "Approximate POMDP Planning Software",
    "description": "A toolkit for Partially Observed Markov Decision Processes (POMDP). Provides\n    bindings to C++ libraries implementing the algorithm SARSOP (Successive Approximations\n    of the Reachable Space under Optimal Policies) and described in Kurniawati et al (2008),\n    <doi:10.15607/RSS.2008.IV.009>.  This package also provides a high-level interface\n    for generating, solving and simulating POMDP problems and their solutions.",
    "version": "0.6.16",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>),\n  Jeroen Ooms [aut],\n  Milad Memarzadeh [aut],\n  Hanna Kurniawati [ctb, cph],\n  David Hsu [ctb, cph],\n  Hanna Kurniawati [ctb, cph],\n  Wee Sun Lee [ctb, cph],\n  Yanzhu Du [ctb],\n  Xan Huang [ctb],\n  Trey Smith [ctb, cph],\n  Tony Cassandra [ctb, cph],\n  Lee Thomason [ctb, cph],\n  Carl Kindman [ctb, cph],\n  Le Trong Dao [ctb, cph],\n  Amit Jain [ctb, cph],\n  Rong Nan [ctb, cph],\n  Ulrich Drepper [ctb],\n  Free Software Foundation [cph],\n  Tyge Lovset [ctb, cph],\n  Yves Berquin [ctb, cph],\n  Benjamin Gr\u00fcdelbach [ctb],\n  RSA Data Security, Inc. [cph]",
    "url": "https://github.com/boettiger-lab/sarsop",
    "bug_reports": "https://github.com/boettiger-lab/sarsop/issues",
    "repository": "https://cran.r-project.org/package=sarsop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sarsop Approximate POMDP Planning Software A toolkit for Partially Observed Markov Decision Processes (POMDP). Provides\n    bindings to C++ libraries implementing the algorithm SARSOP (Successive Approximations\n    of the Reachable Space under Optimal Policies) and described in Kurniawati et al (2008),\n    <doi:10.15607/RSS.2008.IV.009>.  This package also provides a high-level interface\n    for generating, solving and simulating POMDP problems and their solutions.  "
  },
  {
    "id": 20150,
    "package_name": "scCATCH",
    "title": "Single Cell Cluster-Based Annotation Toolkit for Cellular\nHeterogeneity",
    "description": "An automatic cluster-based annotation pipeline based on evidence-based score by matching the marker genes with known cell markers in tissue-specific cell taxonomy reference database for single-cell RNA-seq data. See Shao X, et al (2020) <doi:10.1016/j.isci.2020.100882> for more details.",
    "version": "3.2.2",
    "maintainer": "Xin Shao<xin_shao@zju.edu.cn>",
    "author": "Xin Shao",
    "url": "https://github.com/ZJUFanLab/scCATCH",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scCATCH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scCATCH Single Cell Cluster-Based Annotation Toolkit for Cellular\nHeterogeneity An automatic cluster-based annotation pipeline based on evidence-based score by matching the marker genes with known cell markers in tissue-specific cell taxonomy reference database for single-cell RNA-seq data. See Shao X, et al (2020) <doi:10.1016/j.isci.2020.100882> for more details.  "
  },
  {
    "id": 20177,
    "package_name": "scaRabee",
    "title": "Optimization Toolkit for Pharmacokinetic-Pharmacodynamic Models",
    "description": "A port of the Scarabee toolkit originally written as a \n  Matlab-based application. scaRabee provides a framework for simulation and optimization \n  of pharmacokinetic-pharmacodynamic models at the individual and population level.\n  It is built on top of the neldermead package, which provides the direct search \n  algorithm proposed by Nelder and Mead for model optimization.",
    "version": "1.1-4",
    "maintainer": "Sebastien Bihorel <sb.pmlab@gmail.com>",
    "author": "Sebastien Bihorel [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scaRabee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scaRabee Optimization Toolkit for Pharmacokinetic-Pharmacodynamic Models A port of the Scarabee toolkit originally written as a \n  Matlab-based application. scaRabee provides a framework for simulation and optimization \n  of pharmacokinetic-pharmacodynamic models at the individual and population level.\n  It is built on top of the neldermead package, which provides the direct search \n  algorithm proposed by Nelder and Mead for model optimization.  "
  },
  {
    "id": 20212,
    "package_name": "schoRsch",
    "title": "Tools for Analyzing Factorial Experiments",
    "description": "Offers a helping hand to psychologists and other behavioral scientists who routinely deal with experimental data from factorial experiments. It includes several functions to format output from other R functions according to the style guidelines of the APA (American Psychological Association). This formatted output can be copied directly into manuscripts to facilitate data reporting. These features are backed up by a toolkit of several small helper functions, e.g., offering out-of-the-box outlier removal. The package lends its name to Georg \"Schorsch\" Schuessler, ingenious technician at the Department of Psychology III, University of Wuerzburg. For details on the implemented methods, see Roland Pfister and Markus Janczyk (2016) <doi: 10.20982/tqmp.12.2.p147>.",
    "version": "1.11",
    "maintainer": "Roland Pfister <mail@roland-pfister.net>",
    "author": "Roland Pfister [aut, cre],\n  Markus Janczyk [aut]",
    "url": "https://www.tqmp.org/RegularArticles/vol12-2/p147/index.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=schoRsch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "schoRsch Tools for Analyzing Factorial Experiments Offers a helping hand to psychologists and other behavioral scientists who routinely deal with experimental data from factorial experiments. It includes several functions to format output from other R functions according to the style guidelines of the APA (American Psychological Association). This formatted output can be copied directly into manuscripts to facilitate data reporting. These features are backed up by a toolkit of several small helper functions, e.g., offering out-of-the-box outlier removal. The package lends its name to Georg \"Schorsch\" Schuessler, ingenious technician at the Department of Psychology III, University of Wuerzburg. For details on the implemented methods, see Roland Pfister and Markus Janczyk (2016) <doi: 10.20982/tqmp.12.2.p147>.  "
  },
  {
    "id": 20524,
    "package_name": "sharpshootR",
    "title": "A Soil Survey Toolkit",
    "description": "A collection of data processing, visualization, and export functions to support soil survey operations. Many of the functions build on the `SoilProfileCollection` S4 class provided by the aqp package, extending baseline visualization to more elaborate depictions in the context of spatial and taxonomic data. While this package is primarily developed by and for the USDA-NRCS, in support of the National Cooperative Soil Survey, the authors strive for generalization sufficient to support any soil survey operation. Many of the included functions are used by the SoilWeb suite of websites and movile applications. These functions are provided here, with additional documentation, to enable others to replicate high quality versions of these figures for their own purposes.",
    "version": "2.4",
    "maintainer": "Dylan Beaudette <dylan.beaudette@usda.gov>",
    "author": "Dylan Beaudette [cre, aut],\n  Jay Skovlin [aut],\n  Stephen Roecker [aut],\n  Andrew Brown [aut],\n  USDA-NRCS Soil Survey Staff [ctb]",
    "url": "https://github.com/ncss-tech/sharpshootR",
    "bug_reports": "https://github.com/ncss-tech/sharpshootR/issues",
    "repository": "https://cran.r-project.org/package=sharpshootR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sharpshootR A Soil Survey Toolkit A collection of data processing, visualization, and export functions to support soil survey operations. Many of the functions build on the `SoilProfileCollection` S4 class provided by the aqp package, extending baseline visualization to more elaborate depictions in the context of spatial and taxonomic data. While this package is primarily developed by and for the USDA-NRCS, in support of the National Cooperative Soil Survey, the authors strive for generalization sufficient to support any soil survey operation. Many of the included functions are used by the SoilWeb suite of websites and movile applications. These functions are provided here, with additional documentation, to enable others to replicate high quality versions of these figures for their own purposes.  "
  },
  {
    "id": 20631,
    "package_name": "shinypanels",
    "title": "Shiny Layout with Collapsible Panels",
    "description": "Create 'Shiny Apps' with collapsible vertical panels. \n    This package provides a new visual arrangement for elements on top of 'Shiny'. \n    Use the expand and collapse capabilities to leverage web applications with\n    many elements to focus the user attention on the panel of interest.",
    "version": "0.5.0",
    "maintainer": "Juan Pablo Marin Diaz <jpmarindiaz@gmail.com>",
    "author": "Juan Pablo Marin Diaz [aut, cre]",
    "url": "http://github.com/datasketch/shinypanels",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shinypanels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinypanels Shiny Layout with Collapsible Panels Create 'Shiny Apps' with collapsible vertical panels. \n    This package provides a new visual arrangement for elements on top of 'Shiny'. \n    Use the expand and collapse capabilities to leverage web applications with\n    many elements to focus the user attention on the panel of interest.  "
  },
  {
    "id": 20857,
    "package_name": "slendr",
    "title": "A Simulation Framework for Spatiotemporal Population Genetics",
    "description": "A framework for simulating spatially explicit genomic data which\n    leverages real cartographic information for programmatic and visual encoding\n    of spatiotemporal population dynamics on real geographic landscapes. Population\n    genetic models are then automatically executed by the 'SLiM' software by Haller\n    et al. (2019) <doi:10.1093/molbev/msy228> behind the scenes, using a custom\n    built-in simulation 'SLiM' script. Additionally, fully abstract spatial models\n    not tied to a specific geographic location are supported, and users can also\n    simulate data from standard, non-spatial, random-mating models. These can be\n    simulated either with the 'SLiM' built-in back-end script, or using an efficient\n    coalescent population genetics simulator 'msprime' by Baumdicker et al. (2022)\n    <doi:10.1093/genetics/iyab229> with a custom-built 'Python' script bundled with the\n    R package. Simulated genomic data is saved in a tree-sequence format and can be\n    loaded, manipulated, and summarised using tree-sequence functionality via an R\n    interface to the 'Python' module 'tskit' by Kelleher et al. (2019)\n    <doi:10.1038/s41588-019-0483-y>. Complete model configuration, simulation and\n    analysis pipelines can be therefore constructed without a need to leave the R\n    environment, eliminating friction between disparate tools for population genetic\n    simulations and data analysis.",
    "version": "1.3.0",
    "maintainer": "Martin Petr <contact@bodkan.net>",
    "author": "Martin Petr [aut, cre] (ORCID: <https://orcid.org/0000-0003-4879-8421>)",
    "url": "https://github.com/bodkan/slendr",
    "bug_reports": "https://github.com/bodkan/slendr/issues",
    "repository": "https://cran.r-project.org/package=slendr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "slendr A Simulation Framework for Spatiotemporal Population Genetics A framework for simulating spatially explicit genomic data which\n    leverages real cartographic information for programmatic and visual encoding\n    of spatiotemporal population dynamics on real geographic landscapes. Population\n    genetic models are then automatically executed by the 'SLiM' software by Haller\n    et al. (2019) <doi:10.1093/molbev/msy228> behind the scenes, using a custom\n    built-in simulation 'SLiM' script. Additionally, fully abstract spatial models\n    not tied to a specific geographic location are supported, and users can also\n    simulate data from standard, non-spatial, random-mating models. These can be\n    simulated either with the 'SLiM' built-in back-end script, or using an efficient\n    coalescent population genetics simulator 'msprime' by Baumdicker et al. (2022)\n    <doi:10.1093/genetics/iyab229> with a custom-built 'Python' script bundled with the\n    R package. Simulated genomic data is saved in a tree-sequence format and can be\n    loaded, manipulated, and summarised using tree-sequence functionality via an R\n    interface to the 'Python' module 'tskit' by Kelleher et al. (2019)\n    <doi:10.1038/s41588-019-0483-y>. Complete model configuration, simulation and\n    analysis pipelines can be therefore constructed without a need to leave the R\n    environment, eliminating friction between disparate tools for population genetic\n    simulations and data analysis.  "
  },
  {
    "id": 20898,
    "package_name": "smidm",
    "title": "Statistical Modelling for Infectious Disease Management",
    "description": "Statistical models for specific coronavirus disease 2019 use cases at German local health authorities. All models of Statistical modelling for infectious disease management 'smidm' are part of the decision support toolkit in the 'EsteR' project. More information is published in Sonja J\u00e4ckle, Rieke Alpers, Lisa K\u00fchne, Jakob Schumacher, Benjamin Geisler, Max Westphal \"'EsteR' \u2013 A Digital Toolkit for COVID-19 Decision Support in Local Health Authorities\" (2022) <doi:10.3233/SHTI220799> and Sonja J\u00e4ckle, Elias R\u00f6ger, Volker Dicken, Benjamin Geisler, Jakob Schumacher, Max Westphal \"A Statistical Model to Assess Risk for Supporting COVID-19 Quarantine Decisions\" (2021) <doi:10.3390/ijerph18179166>.",
    "version": "1.0",
    "maintainer": "Sonja J\u00e4ckle <sonja.jaeckle@mevis.fraunhofer.de>",
    "author": "Max Westphal [aut] (ORCID: <https://orcid.org/0000-0002-8488-758X>),\n  Stefanie Grimm [aut],\n  Sonja J\u00e4ckle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2908-299X>),\n  Rieke Alpers [aut] (ORCID: <https://orcid.org/0000-0001-8317-1435>),\n  Hong Phuc Truong [aut],\n  Amelie Lucker [ctb],\n  Fraunhofer MEVIS [cph],\n  Fraunhofer ITWM [cph]",
    "url": "https://gitlab.cc-asp.fraunhofer.de/ester/smidm",
    "bug_reports": "https://gitlab.cc-asp.fraunhofer.de/ester/smidm/-/issues",
    "repository": "https://cran.r-project.org/package=smidm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smidm Statistical Modelling for Infectious Disease Management Statistical models for specific coronavirus disease 2019 use cases at German local health authorities. All models of Statistical modelling for infectious disease management 'smidm' are part of the decision support toolkit in the 'EsteR' project. More information is published in Sonja J\u00e4ckle, Rieke Alpers, Lisa K\u00fchne, Jakob Schumacher, Benjamin Geisler, Max Westphal \"'EsteR' \u2013 A Digital Toolkit for COVID-19 Decision Support in Local Health Authorities\" (2022) <doi:10.3233/SHTI220799> and Sonja J\u00e4ckle, Elias R\u00f6ger, Volker Dicken, Benjamin Geisler, Jakob Schumacher, Max Westphal \"A Statistical Model to Assess Risk for Supporting COVID-19 Quarantine Decisions\" (2021) <doi:10.3390/ijerph18179166>.  "
  },
  {
    "id": 20912,
    "package_name": "smoothemplik",
    "title": "Smoothed Empirical Likelihood",
    "description": "Empirical likelihood methods for asymptotically efficient\n    estimation of models based on conditional or unconditional moment\n    restrictions; see Kitamura, Tripathi & Ahn (2004)\n    <doi:10.1111/j.1468-0262.2004.00550.x> and Owen (2013)\n    <doi:10.1002/cjs.11183>.\n    Kernel-based non-parametric methods for density/regression estimation and\n    numerical routines for empirical likelihood maximisation are implemented in\n    'Rcpp' for speed.",
    "version": "0.0.17",
    "maintainer": "Andre\u00ef Victorovitch Kostyrka <andrei.kostyrka@gmail.com>",
    "author": "Andre\u00ef Victorovitch Kostyrka [aut, cre]",
    "url": "https://github.com/Fifis/smoothemplik",
    "bug_reports": "https://github.com/Fifis/smoothemplik/issues",
    "repository": "https://cran.r-project.org/package=smoothemplik",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smoothemplik Smoothed Empirical Likelihood Empirical likelihood methods for asymptotically efficient\n    estimation of models based on conditional or unconditional moment\n    restrictions; see Kitamura, Tripathi & Ahn (2004)\n    <doi:10.1111/j.1468-0262.2004.00550.x> and Owen (2013)\n    <doi:10.1002/cjs.11183>.\n    Kernel-based non-parametric methods for density/regression estimation and\n    numerical routines for empirical likelihood maximisation are implemented in\n    'Rcpp' for speed.  "
  },
  {
    "id": 21025,
    "package_name": "spBPS",
    "title": "Bayesian Predictive Stacking for Scalable Geospatial Transfer\nLearning",
    "description": "Provides functions for Bayesian Predictive Stacking within the Bayesian transfer learning framework for geospatial artificial systems, as introduced in \"Bayesian Transfer Learning for Artificially Intelligent Geospatial Systems: A Predictive Stacking Approach\" (Presicce and Banerjee, 2024) <doi:10.48550/arXiv.2410.09504>. This methodology enables efficient Bayesian geostatistical modeling, utilizing predictive stacking to improve inference across spatial datasets. The core functions leverage 'C++' for high-performance computation, making the framework well-suited for large-scale spatial data analysis in parallel and distributed computing environments. Designed for scalability, it allows seamless application in computationally demanding scenarios.",
    "version": "0.0-4",
    "maintainer": "Luca Presicce <l.presicce@campus.unimib.it>",
    "author": "Luca Presicce [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-7062-3523>),\n  Sudipto Banerjee [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spBPS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spBPS Bayesian Predictive Stacking for Scalable Geospatial Transfer\nLearning Provides functions for Bayesian Predictive Stacking within the Bayesian transfer learning framework for geospatial artificial systems, as introduced in \"Bayesian Transfer Learning for Artificially Intelligent Geospatial Systems: A Predictive Stacking Approach\" (Presicce and Banerjee, 2024) <doi:10.48550/arXiv.2410.09504>. This methodology enables efficient Bayesian geostatistical modeling, utilizing predictive stacking to improve inference across spatial datasets. The core functions leverage 'C++' for high-performance computation, making the framework well-suited for large-scale spatial data analysis in parallel and distributed computing environments. Designed for scalability, it allows seamless application in computationally demanding scenarios.  "
  },
  {
    "id": 21228,
    "package_name": "splitTools",
    "title": "Tools for Data Splitting",
    "description": "Fast, lightweight toolkit for data splitting. Data sets can\n    be partitioned into disjoint groups (e.g. into training, validation,\n    and test) or into (repeated) k-folds for subsequent cross-validation.\n    Besides basic splits, the package supports stratified, grouped as well\n    as blocked splitting. Furthermore, cross-validation folds for time\n    series data can be created. See e.g. Hastie et al. (2001)\n    <doi:10.1007/978-0-387-84858-7> for the basic background on data\n    partitioning and cross-validation.",
    "version": "1.0.1",
    "maintainer": "Michael Mayer <mayermichael79@gmail.com>",
    "author": "Michael Mayer [aut, cre]",
    "url": "https://github.com/mayer79/splitTools",
    "bug_reports": "https://github.com/mayer79/splitTools/issues",
    "repository": "https://cran.r-project.org/package=splitTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "splitTools Tools for Data Splitting Fast, lightweight toolkit for data splitting. Data sets can\n    be partitioned into disjoint groups (e.g. into training, validation,\n    and test) or into (repeated) k-folds for subsequent cross-validation.\n    Besides basic splits, the package supports stratified, grouped as well\n    as blocked splitting. Furthermore, cross-validation folds for time\n    series data can be created. See e.g. Hastie et al. (2001)\n    <doi:10.1007/978-0-387-84858-7> for the basic background on data\n    partitioning and cross-validation.  "
  },
  {
    "id": 21338,
    "package_name": "sstvars",
    "title": "Toolkit for Reduced Form and Structural Smooth Transition Vector\nAutoregressive Models",
    "description": "Penalized and non-penalized maximum likelihood estimation of smooth\n  transition vector autoregressive models with various types of transition weight\n  functions, conditional distributions, and identification methods. Constrained\n  estimation with various types of constraints is available. Residual based\n  model diagnostics, forecasting, simulations, counterfactual analysis, and\n  computation of impulse response functions, generalized impulse response functions,\n  generalized forecast error variance decompositions, as well as historical\n  decompositions. See\n  Heather Anderson, Farshid Vahid (1998) <doi:10.1016/S0304-4076(97)00076-6>,\n  Helmut L\u00fctkepohl, Aleksei Net\u0161unajev (2017) <doi:10.1016/j.jedc.2017.09.001>,\n  Markku Lanne, Savi Virolainen (2025) <doi:10.1016/j.jedc.2025.105162>,\n  Savi Virolainen (2025) <doi:10.48550/arXiv.2404.19707>.",
    "version": "1.2.2",
    "maintainer": "Savi Virolainen <savi.virolainen@helsinki.fi>",
    "author": "Savi Virolainen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5075-6821>)",
    "url": "https://github.com/saviviro/sstvars",
    "bug_reports": "https://github.com/saviviro/sstvars/issues",
    "repository": "https://cran.r-project.org/package=sstvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sstvars Toolkit for Reduced Form and Structural Smooth Transition Vector\nAutoregressive Models Penalized and non-penalized maximum likelihood estimation of smooth\n  transition vector autoregressive models with various types of transition weight\n  functions, conditional distributions, and identification methods. Constrained\n  estimation with various types of constraints is available. Residual based\n  model diagnostics, forecasting, simulations, counterfactual analysis, and\n  computation of impulse response functions, generalized impulse response functions,\n  generalized forecast error variance decompositions, as well as historical\n  decompositions. See\n  Heather Anderson, Farshid Vahid (1998) <doi:10.1016/S0304-4076(97)00076-6>,\n  Helmut L\u00fctkepohl, Aleksei Net\u0161unajev (2017) <doi:10.1016/j.jedc.2017.09.001>,\n  Markku Lanne, Savi Virolainen (2025) <doi:10.1016/j.jedc.2025.105162>,\n  Savi Virolainen (2025) <doi:10.48550/arXiv.2404.19707>.  "
  },
  {
    "id": 21373,
    "package_name": "staplr",
    "title": "A Toolkit for PDF Files",
    "description": "Provides functions to manipulate PDF files: \n    fill out PDF forms;\n    merge multiple PDF files into one; \n    remove selected pages from a file;\n    rename multiple files in a directory;\n    rotate entire pdf document; \n    rotate selected pages of a pdf file;\n    Select pages from a file;\n    splits single input PDF document into individual pages;\n    splits single input PDF document into parts from given points.",
    "version": "3.2.2",
    "maintainer": "Priyanga Dilini Talagala <pritalagala@gmail.com>",
    "author": "Priyanga Dilini Talagala [aut, cre],\n  Ogan Mancarci [aut],\n  Daniel Padfield [aut],\n  Granville Matheson [aut],\n  Pedro Rafael D. Marinho [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1591-8300>),\n  Marc Vinyals [cph, aut] (Author and copyright holder of included\n    pdftk-java package)",
    "url": "",
    "bug_reports": "https://github.com/pridiltal/staplr/issues",
    "repository": "https://cran.r-project.org/package=staplr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "staplr A Toolkit for PDF Files Provides functions to manipulate PDF files: \n    fill out PDF forms;\n    merge multiple PDF files into one; \n    remove selected pages from a file;\n    rename multiple files in a directory;\n    rotate entire pdf document; \n    rotate selected pages of a pdf file;\n    Select pages from a file;\n    splits single input PDF document into individual pages;\n    splits single input PDF document into parts from given points.  "
  },
  {
    "id": 21383,
    "package_name": "starter",
    "title": "Starter Kit for New Projects",
    "description": "Get started with new projects by dropping a skeleton of a new\n    project into a new or existing directory, initialise git repositories,\n    and create reproducible environments with the 'renv' package. The\n    package allows for dynamically named files, folders, file content, as\n    well as the functionality to drop individual template files into\n    existing projects.",
    "version": "0.1.16",
    "maintainer": "Daniel D. Sjoberg <danield.sjoberg@gmail.com>",
    "author": "Daniel D. Sjoberg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0862-2018>),\n  Emily Vertosick [ctb]",
    "url": "https://github.com/ddsjoberg/starter,\nhttps://www.danieldsjoberg.com/starter/index.html",
    "bug_reports": "https://github.com/ddsjoberg/starter/issues",
    "repository": "https://cran.r-project.org/package=starter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "starter Starter Kit for New Projects Get started with new projects by dropping a skeleton of a new\n    project into a new or existing directory, initialise git repositories,\n    and create reproducible environments with the 'renv' package. The\n    package allows for dynamically named files, folders, file content, as\n    well as the functionality to drop individual template files into\n    existing projects.  "
  },
  {
    "id": 21387,
    "package_name": "starvz",
    "title": "R-Based Visualization Techniques for Task-Based Applications",
    "description": "Performance analysis workflow that combines the power of the R\n    language (and the tidyverse realm) and many auxiliary tools to\n    provide a consistent, flexible, extensible, fast, and versatile\n    framework for the performance analysis of task-based applications\n    that run on top of the StarPU runtime (with its MPI (Message\n    Passing Interface) layer for multi-node support).  Its goal is to\n    provide a fruitful prototypical environment to conduct performance\n    analysis hypothesis-checking for task-based applications that run\n    on heterogeneous (multi-GPU, multi-core) multi-node HPC\n    (High-performance computing) platforms.",
    "version": "0.8.3",
    "maintainer": "Vinicius Garcia Pinto <vinicius.pinto@furg.br>",
    "author": "Lucas Mello Schnorr [aut, ths] (ORCID:\n    <https://orcid.org/0000-0003-4828-9942>),\n  Vinicius Garcia Pinto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6845-9358>),\n  Lucas Leandro Nesi [aut] (ORCID:\n    <https://orcid.org/0000-0001-8874-1839>),\n  Marcelo Cogo Miletto [aut] (ORCID:\n    <https://orcid.org/0000-0002-1191-3863>),\n  Guilherme Alles [ctb],\n  Arnaud Legrand [ctb],\n  Luka Stanisic [ctb],\n  R\u00e9my Drouilhet [ctb]",
    "url": "https://github.com/schnorr/starvz",
    "bug_reports": "https://github.com/schnorr/starvz/issues",
    "repository": "https://cran.r-project.org/package=starvz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "starvz R-Based Visualization Techniques for Task-Based Applications Performance analysis workflow that combines the power of the R\n    language (and the tidyverse realm) and many auxiliary tools to\n    provide a consistent, flexible, extensible, fast, and versatile\n    framework for the performance analysis of task-based applications\n    that run on top of the StarPU runtime (with its MPI (Message\n    Passing Interface) layer for multi-node support).  Its goal is to\n    provide a fruitful prototypical environment to conduct performance\n    analysis hypothesis-checking for task-based applications that run\n    on heterogeneous (multi-GPU, multi-core) multi-node HPC\n    (High-performance computing) platforms.  "
  },
  {
    "id": 21453,
    "package_name": "stepmixr",
    "title": "Interface to 'Python' Package 'StepMix'",
    "description": "This is an interface for the 'Python' package\n  'StepMix'. It is a 'Python' package following the scikit-learn API for\n  model-based clustering and generalized mixture modeling (latent class/profile\n  analysis) of continuous and categorical data. 'StepMix' handles missing values\n  through Full Information Maximum Likelihood (FIML) and provides multiple stepwise\n  Expectation-Maximization (EM) estimation methods based on pseudolikelihood\n  theory. Additional features include support for covariates and distal outcomes,\n  various simulation utilities, and non-parametric bootstrapping, which allows\n  inference in semi-supervised and unsupervised settings. Software paper available\n  at <doi:10.18637/jss.v113.i08>.",
    "version": "0.1.3",
    "maintainer": "Charles-\u00c9douard Gigu\u00e8re <ce.giguere@gmail.com>",
    "author": "\u00c9ric Lacourse [aut],\n  Roxane de la Sablonni\u00e8re [aut],\n  Charles-\u00c9douard Gigu\u00e8re [aut, cre],\n  Sacha Morin [aut],\n  Robin Legault [aut],\n  F\u00e9lix Lalibert\u00e9 [aut],\n  Zsusza Bakk [ctb]",
    "url": "https://github.com/Labo-Lacourse/StepMixr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stepmixr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stepmixr Interface to 'Python' Package 'StepMix' This is an interface for the 'Python' package\n  'StepMix'. It is a 'Python' package following the scikit-learn API for\n  model-based clustering and generalized mixture modeling (latent class/profile\n  analysis) of continuous and categorical data. 'StepMix' handles missing values\n  through Full Information Maximum Likelihood (FIML) and provides multiple stepwise\n  Expectation-Maximization (EM) estimation methods based on pseudolikelihood\n  theory. Additional features include support for covariates and distal outcomes,\n  various simulation utilities, and non-parametric bootstrapping, which allows\n  inference in semi-supervised and unsupervised settings. Software paper available\n  at <doi:10.18637/jss.v113.i08>.  "
  },
  {
    "id": 21500,
    "package_name": "stosim",
    "title": "Stochastic Simulator for Reliability Modeling of Repairable\nSystems",
    "description": "A toolkit for Reliability Availability and Maintainability (RAM) modeling of industrial process systems.",
    "version": "0.0.15",
    "maintainer": "Jacob Ormerod <jake@openreliability.org>",
    "author": "David Silkworth [aut],\n  Jacob Ormerod [cre],\n  OpenReliability.org [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stosim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stosim Stochastic Simulator for Reliability Modeling of Repairable\nSystems A toolkit for Reliability Availability and Maintainability (RAM) modeling of industrial process systems.  "
  },
  {
    "id": 21555,
    "package_name": "strvalidator",
    "title": "Process Control and Validation of Forensic STR Kits",
    "description": "An open source platform for validation and process control.\n    Tools to analyze data from internal validation of forensic short tandem\n    repeat (STR) kits are provided. The tools are developed to provide\n    the necessary data to conform with guidelines for internal validation\n    issued by the European Network of Forensic Science Institutes (ENFSI)\n    DNA Working Group, and the Scientific Working Group on DNA Analysis Methods\n    (SWGDAM). A front-end graphical user interface is provided.\n    More information about each function can be found in the\n    respective help documentation.",
    "version": "2.4.2",
    "maintainer": "Oskar Hansson <oskhan@ous-hf.no>",
    "author": "Oskar Hansson [aut, cre]",
    "url": "https://sites.google.com/site/forensicapps/strvalidator",
    "bug_reports": "https://github.com/OskarHansson/strvalidator/issues",
    "repository": "https://cran.r-project.org/package=strvalidator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strvalidator Process Control and Validation of Forensic STR Kits An open source platform for validation and process control.\n    Tools to analyze data from internal validation of forensic short tandem\n    repeat (STR) kits are provided. The tools are developed to provide\n    the necessary data to conform with guidelines for internal validation\n    issued by the European Network of Forensic Science Institutes (ENFSI)\n    DNA Working Group, and the Scientific Working Group on DNA Analysis Methods\n    (SWGDAM). A front-end graphical user interface is provided.\n    More information about each function can be found in the\n    respective help documentation.  "
  },
  {
    "id": 21694,
    "package_name": "svGUI",
    "title": "'SciViews::R' - Manage GUIs in R",
    "description": "Manage Graphical User Interfaces (GUI) in R. It is independent from\n  any particular GUI widgets ('Tk', 'Gtk2', native, ...). It centralizes info\n  about GUI elements currently used, and it dispatches GUI calls to the\n  particular toolkits in use in function of the context (is R run at the\n  terminal, within a 'Tk' application, a HTML page?).",
    "version": "1.0.2",
    "maintainer": "Philippe Grosjean <phgrosjean@sciviews.org>",
    "author": "Philippe Grosjean [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2694-9471>)",
    "url": "https://github.com/SciViews/svGUI,\nhttps://www.sciviews.org/svGUI/,\nhttps://sciviews.r-universe.dev/svGUI",
    "bug_reports": "https://github.com/SciViews/svGUI/issues",
    "repository": "https://cran.r-project.org/package=svGUI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svGUI 'SciViews::R' - Manage GUIs in R Manage Graphical User Interfaces (GUI) in R. It is independent from\n  any particular GUI widgets ('Tk', 'Gtk2', native, ...). It centralizes info\n  about GUI elements currently used, and it dispatches GUI calls to the\n  particular toolkits in use in function of the context (is R run at the\n  terminal, within a 'Tk' application, a HTML page?).  "
  },
  {
    "id": 21800,
    "package_name": "table.express",
    "title": "Build 'data.table' Expressions with Data Manipulation Verbs",
    "description": "A specialization of 'dplyr' data manipulation verbs that parse and build expressions\n    which are ultimately evaluated by 'data.table', letting it handle all optimizations. A set of\n    additional verbs is also provided to facilitate some common operations on a subset of the data.",
    "version": "0.4.2",
    "maintainer": "Alexis Sarda-Espinosa <alexis.sarda@gmail.com>",
    "author": "Alexis Sarda-Espinosa [cre, aut]",
    "url": "https://asardaes.github.io/table.express/,\nhttps://github.com/asardaes/table.express",
    "bug_reports": "https://github.com/asardaes/table.express/issues",
    "repository": "https://cran.r-project.org/package=table.express",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "table.express Build 'data.table' Expressions with Data Manipulation Verbs A specialization of 'dplyr' data manipulation verbs that parse and build expressions\n    which are ultimately evaluated by 'data.table', letting it handle all optimizations. A set of\n    additional verbs is also provided to facilitate some common operations on a subset of the data.  "
  },
  {
    "id": 21804,
    "package_name": "tableMatrix",
    "title": "Combines 'data.table' and 'matrix' Classes",
    "description": "Provides two classes extending 'data.table' class. Simple\n    'tableList' class wraps 'data.table' and any additional structures together.\n    More complex 'tableMatrix' class combines 'data.table' and\n    'matrix'. See <http://github.com/InferenceTechnologies/tableMatrix> for more\n    information and examples.",
    "version": "0.82.0",
    "maintainer": "Petr Lenhard <petr.lenhard@inferencetech.com>",
    "author": "Petr Lenhard [aut, cre],\n  Inference Technologies [cph],\n  Petra Hudeckova [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tableMatrix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tableMatrix Combines 'data.table' and 'matrix' Classes Provides two classes extending 'data.table' class. Simple\n    'tableList' class wraps 'data.table' and any additional structures together.\n    More complex 'tableMatrix' class combines 'data.table' and\n    'matrix'. See <http://github.com/InferenceTechnologies/tableMatrix> for more\n    information and examples.  "
  },
  {
    "id": 21809,
    "package_name": "tabler",
    "title": "Create Dashboards with 'Tabler' and 'Shiny'",
    "description": "Provides functions to build interactive dashboards combining the 'Tabler UI Kit' with 'Shiny', making it\n    easy to create professional-looking web applications. 'Tabler' is fully responsive and compatible with all modern\n    browsers. Offers customizable layouts and components built with 'HTML5' and 'CSS3'. The underlying 'Tabler'\n    (<https://github.com/tabler/tabler>) and 'Tabler Icons' (<https://github.com/tabler/tabler-icons>)\n    were pre-built from source to eliminate the need for 'Node.js' and 'NPM' on package installation.",
    "version": "0.1.0",
    "maintainer": "Mauricio Vargas Sepulveda <m.vargas.sepulveda@gmail.com>",
    "author": "Mauricio Vargas Sepulveda [aut, cre],\n  Tabler [ctb, cph] (Tabler Dashboard)",
    "url": "https://github.com/pachadotdev/tabler",
    "bug_reports": "https://github.com/pachadotdev/tabler/issues",
    "repository": "https://cran.r-project.org/package=tabler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabler Create Dashboards with 'Tabler' and 'Shiny' Provides functions to build interactive dashboards combining the 'Tabler UI Kit' with 'Shiny', making it\n    easy to create professional-looking web applications. 'Tabler' is fully responsive and compatible with all modern\n    browsers. Offers customizable layouts and components built with 'HTML5' and 'CSS3'. The underlying 'Tabler'\n    (<https://github.com/tabler/tabler>) and 'Tabler Icons' (<https://github.com/tabler/tabler-icons>)\n    were pre-built from source to eliminate the need for 'Node.js' and 'NPM' on package installation.  "
  },
  {
    "id": 21826,
    "package_name": "tabulator",
    "title": "Efficient Tabulation with Stata-Like Output",
    "description": "Efficient tabulation with Stata-like output.\n\tFor each unique value of the variable, it shows the number of \n\tobservations with that value, proportion of observations with that\n\tvalue, and cumulative proportion, in descending order of frequency.\n\tAccepts data.table, tibble, or data.frame as input. \n\tEfficient with big data: if you give it a data.table, \n\ttab() uses data.table syntax.",
    "version": "1.0.0",
    "maintainer": "Sean Higgins <sean.higgins@kellogg.northwestern.edu>",
    "author": "Sean Higgins [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tabulator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabulator Efficient Tabulation with Stata-Like Output Efficient tabulation with Stata-like output.\n\tFor each unique value of the variable, it shows the number of \n\tobservations with that value, proportion of observations with that\n\tvalue, and cumulative proportion, in descending order of frequency.\n\tAccepts data.table, tibble, or data.frame as input. \n\tEfficient with big data: if you give it a data.table, \n\ttab() uses data.table syntax.  "
  },
  {
    "id": 21828,
    "package_name": "tabxplor",
    "title": "User-Friendly Tables with Color Helpers for Data Exploration",
    "description": "Make it easy to deal with multiple cross-tables in data exploration, by\n  creating them, manipulating them, and adding color helpers to highlight\n  important informations (differences from totals, comparisons between lines or\n  columns, contributions to variance, confidence intervals, odds ratios, etc.). \n  All functions are pipe-friendly and render data frames which can be easily \n  manipulated. In the same time, time-taking operations are done with 'data.table'\n  to go faster with big dataframes. Tables can be exported with formats and colors \n  to 'Excel', plot and html.",
    "version": "1.3.1",
    "maintainer": "Brice Nocenti <brice.nocenti@gmail.com>",
    "author": "Brice Nocenti [aut, cre]",
    "url": "https://github.com/BriceNocenti/tabxplor",
    "bug_reports": "https://github.com/BriceNocenti/tabxplor/issues",
    "repository": "https://cran.r-project.org/package=tabxplor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabxplor User-Friendly Tables with Color Helpers for Data Exploration Make it easy to deal with multiple cross-tables in data exploration, by\n  creating them, manipulating them, and adding color helpers to highlight\n  important informations (differences from totals, comparisons between lines or\n  columns, contributions to variance, confidence intervals, odds ratios, etc.). \n  All functions are pipe-friendly and render data frames which can be easily \n  manipulated. In the same time, time-taking operations are done with 'data.table'\n  to go faster with big dataframes. Tables can be exported with formats and colors \n  to 'Excel', plot and html.  "
  },
  {
    "id": 21851,
    "package_name": "taskqueue",
    "title": "Task Queue for Parallel Computing Based on PostgreSQL",
    "description": "Implements a task queue system for asynchronous parallel computing \n    using 'PostgreSQL' <https://www.postgresql.org/> as a backend. Designed for \n    embarrassingly parallel problems where tasks do not communicate with each other.\n    Dynamically distributes tasks to workers, handles uneven load balancing, and \n    allows new workers to join at any time. Particularly useful for running large \n    numbers of independent tasks on high-performance computing (HPC) clusters with \n    'SLURM' <https://slurm.schedmd.com/> job schedulers.",
    "version": "0.2.0",
    "maintainer": "Bangyou Zheng <bangyou.zheng@csiro.au>",
    "author": "Bangyou Zheng [aut, cre]",
    "url": "https://taskqueue.bangyou.me/,\nhttps://github.com/byzheng/taskqueue",
    "bug_reports": "https://github.com/byzheng/taskqueue/issues",
    "repository": "https://cran.r-project.org/package=taskqueue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "taskqueue Task Queue for Parallel Computing Based on PostgreSQL Implements a task queue system for asynchronous parallel computing \n    using 'PostgreSQL' <https://www.postgresql.org/> as a backend. Designed for \n    embarrassingly parallel problems where tasks do not communicate with each other.\n    Dynamically distributes tasks to workers, handles uneven load balancing, and \n    allows new workers to join at any time. Particularly useful for running large \n    numbers of independent tasks on high-performance computing (HPC) clusters with \n    'SLURM' <https://slurm.schedmd.com/> job schedulers.  "
  },
  {
    "id": 21879,
    "package_name": "tcv",
    "title": "Determining the Number of Factors in Poisson Factor Models via\nThinning Cross-Validation",
    "description": "Implements methods for selecting the number of factors in Poisson\n  factor models, with a primary focus on Thinning Cross-Validation (TCV). The\n  TCV method is based on the 'data thinning' technique, which probabilistically\n  partitions each count observation into training and test sets while preserving\n  the underlying factor structure. The Poisson factor model is then fit on the\n  training set, and model selection is performed by comparing predictive\n  performance on the test set. This toolkit is designed for researchers working\n  with high-dimensional count data in fields such as genomics, text mining, and\n  social sciences. The data thinning methodology is detailed in Dharamshi et al.\n  (2025) <doi:10.1080/01621459.2024.2353948> and Wang et al. (2025)\n  <doi:10.1080/01621459.2025.2546577>.",
    "version": "0.1.0",
    "maintainer": "Zhijing Wang <wangzhijing@sjtu.edu.cn>",
    "author": "Zhijing Wang [aut, cre],\n  Heng Peng [aut],\n  Peirong Xu [aut]",
    "url": "https://github.com/Wangzhijingwzj/tcv",
    "bug_reports": "https://github.com/Wangzhijingwzj/tcv/issues",
    "repository": "https://cran.r-project.org/package=tcv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tcv Determining the Number of Factors in Poisson Factor Models via\nThinning Cross-Validation Implements methods for selecting the number of factors in Poisson\n  factor models, with a primary focus on Thinning Cross-Validation (TCV). The\n  TCV method is based on the 'data thinning' technique, which probabilistically\n  partitions each count observation into training and test sets while preserving\n  the underlying factor structure. The Poisson factor model is then fit on the\n  training set, and model selection is performed by comparing predictive\n  performance on the test set. This toolkit is designed for researchers working\n  with high-dimensional count data in fields such as genomics, text mining, and\n  social sciences. The data thinning methodology is detailed in Dharamshi et al.\n  (2025) <doi:10.1080/01621459.2024.2353948> and Wang et al. (2025)\n  <doi:10.1080/01621459.2025.2546577>.  "
  },
  {
    "id": 21924,
    "package_name": "tensor",
    "title": "Tensor Product of Arrays",
    "description": "The tensor product of two arrays is notionally an outer\n        product of the arrays collapsed in specific extents by summing\n        along the appropriate diagonals.",
    "version": "1.5.1",
    "maintainer": "Jonathan Rougier <j.c.rougier@bristol.ac.uk>",
    "author": "Jonathan Rougier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tensor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tensor Tensor Product of Arrays The tensor product of two arrays is notionally an outer\n        product of the arrays collapsed in specific extents by summing\n        along the appropriate diagonals.  "
  },
  {
    "id": 21939,
    "package_name": "terminalgraphics",
    "title": "Graphical Output in Terminals",
    "description": "Defines a graphics device and functions for graphical output in\n    terminal emulators that support graphical output. Currently terminals that\n    support the Terminal Graphics Protocol\n    (<https://sw.kovidgoyal.net/kitty/graphics-protocol/>) and terminal supporting Sixel \n    (<https://en.wikipedia.org/wiki/Sixel>) are supported.",
    "version": "0.2.1",
    "maintainer": "Jan van der Laan <r@eoos.dds.nl>",
    "author": "Jan van der Laan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0693-1514>),\n  Doug Kelkhoff [ctb] (ORCID: <https://orcid.org/0009-0003-7845-4061>)",
    "url": "https://codeberg.org/djvanderlaan/terminalgraphics",
    "bug_reports": "https://codeberg.org/djvanderlaan/terminalgraphics/issues",
    "repository": "https://cran.r-project.org/package=terminalgraphics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "terminalgraphics Graphical Output in Terminals Defines a graphics device and functions for graphical output in\n    terminal emulators that support graphical output. Currently terminals that\n    support the Terminal Graphics Protocol\n    (<https://sw.kovidgoyal.net/kitty/graphics-protocol/>) and terminal supporting Sixel \n    (<https://en.wikipedia.org/wiki/Sixel>) are supported.  "
  },
  {
    "id": 21980,
    "package_name": "textTools",
    "title": "Functions for Text Cleansing and Text Analysis",
    "description": "A framework for text cleansing and analysis. Conveniently prepare and process large amounts of text for analysis. \n  Includes various metrics for word counts/frequencies that scale efficiently. Quickly \n  analyze large amounts of text data using a text.table (a data.table created with one word (or unit of text analysis) per row, similar to the tidytext format). \n  Offers flexibility to efficiently work with text data stored in vectors as well as text data formatted as a text.table.",
    "version": "0.1.0",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=textTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textTools Functions for Text Cleansing and Text Analysis A framework for text cleansing and analysis. Conveniently prepare and process large amounts of text for analysis. \n  Includes various metrics for word counts/frequencies that scale efficiently. Quickly \n  analyze large amounts of text data using a text.table (a data.table created with one word (or unit of text analysis) per row, similar to the tidytext format). \n  Offers flexibility to efficiently work with text data stored in vectors as well as text data formatted as a text.table.  "
  },
  {
    "id": 21992,
    "package_name": "textpress",
    "title": "A Lightweight and Versatile NLP Toolkit",
    "description": "A simple Natural Language Processing (NLP) toolkit focused on search-centric workflows with minimal dependencies. The package offers key features for web scraping, text processing, corpus search, and text embedding generation via the 'HuggingFace API' <https://huggingface.co/docs/api-inference/index>.",
    "version": "1.0.0",
    "maintainer": "Jason Timm <JaTimm@salud.unm.edu>",
    "author": "Jason Timm [aut, cre]",
    "url": "https://github.com/jaytimm/textpress,\nhttps://jaytimm.github.io/textpress/",
    "bug_reports": "https://github.com/jaytimm/textpress/issues",
    "repository": "https://cran.r-project.org/package=textpress",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textpress A Lightweight and Versatile NLP Toolkit A simple Natural Language Processing (NLP) toolkit focused on search-centric workflows with minimal dependencies. The package offers key features for web scraping, text processing, corpus search, and text embedding generation via the 'HuggingFace API' <https://huggingface.co/docs/api-inference/index>.  "
  },
  {
    "id": 22085,
    "package_name": "tidyfast",
    "title": "Fast Tidying of Data",
    "description": "Tidying functions built on 'data.table'\n    to provide quick and efficient data manipulation with\n    minimal overhead.",
    "version": "0.4.0",
    "maintainer": "Tyson Barrett <t.barrett88@gmail.com>",
    "author": "Tyson Barrett [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2137-1391>),\n  Mark Fairbanks [ctb],\n  Ivan Leung [ctb],\n  Indrajeet Patil [ctb] (ORCID: <https://orcid.org/0000-0003-1995-6531>,\n    Twitter: @patilindrajeets)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidyfast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyfast Fast Tidying of Data Tidying functions built on 'data.table'\n    to provide quick and efficient data manipulation with\n    minimal overhead.  "
  },
  {
    "id": 22088,
    "package_name": "tidyft",
    "title": "Fast and Memory Efficient Data Operations in Tidy Syntax",
    "description": "Tidy syntax for 'data.table', using modification by reference whenever possible.\n This toolkit is designed for big data analysis in high-performance desktop or laptop computers.\n The syntax of the package is similar or identical to 'tidyverse'.\n It is user friendly, memory efficient and time saving. For more information,\n check its ancestor package 'tidyfst'.",
    "version": "0.9.20",
    "maintainer": "Tian-Yuan Huang <huang.tian-yuan@qq.com>",
    "author": "Tian-Yuan Huang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4151-3764>)",
    "url": "https://github.com/hope-data-science/tidyft,\nhttps://hope-data-science.github.io/tidyft/",
    "bug_reports": "https://github.com/hope-data-science/tidyft/issues",
    "repository": "https://cran.r-project.org/package=tidyft",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyft Fast and Memory Efficient Data Operations in Tidy Syntax Tidy syntax for 'data.table', using modification by reference whenever possible.\n This toolkit is designed for big data analysis in high-performance desktop or laptop computers.\n The syntax of the package is similar or identical to 'tidyverse'.\n It is user friendly, memory efficient and time saving. For more information,\n check its ancestor package 'tidyfst'.  "
  },
  {
    "id": 22101,
    "package_name": "tidylda",
    "title": "Latent Dirichlet Allocation Using 'tidyverse' Conventions",
    "description": "Implements an algorithm for Latent Dirichlet\n    Allocation (LDA), Blei et at. (2003) <https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>,\n    using style conventions from the 'tidyverse',\n    Wickham et al. (2019)<doi:10.21105/joss.01686>,\n    and 'tidymodels', Kuhn et al.<https://tidymodels.github.io/model-implementation-principles/>.\n    Fitting is done via collapsed Gibbs sampling.\n    Also implements several novel features for LDA such as guided models and\n    transfer learning.",
    "version": "0.0.7",
    "maintainer": "Tommy Jones <jones.thos.w@gmail.com>",
    "author": "Tommy Jones [aut, cre] (ORCID: <https://orcid.org/0000-0001-6457-2452>),\n  Brendan Knapp [ctb] (ORCID: <https://orcid.org/0000-0003-3284-4972>),\n  Barum Park [ctb]",
    "url": "https://github.com/TommyJones/tidylda/",
    "bug_reports": "https://github.com/TommyJones/tidylda/issues",
    "repository": "https://cran.r-project.org/package=tidylda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidylda Latent Dirichlet Allocation Using 'tidyverse' Conventions Implements an algorithm for Latent Dirichlet\n    Allocation (LDA), Blei et at. (2003) <https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>,\n    using style conventions from the 'tidyverse',\n    Wickham et al. (2019)<doi:10.21105/joss.01686>,\n    and 'tidymodels', Kuhn et al.<https://tidymodels.github.io/model-implementation-principles/>.\n    Fitting is done via collapsed Gibbs sampling.\n    Also implements several novel features for LDA such as guided models and\n    transfer learning.  "
  },
  {
    "id": 22128,
    "package_name": "tidytable",
    "title": "Tidy Interface to 'data.table'",
    "description": "A tidy interface to 'data.table',\n  giving users the speed of 'data.table' while using tidyverse-like syntax.",
    "version": "0.11.2",
    "maintainer": "Mark Fairbanks <mark.t.fairbanks@gmail.com>",
    "author": "Mark Fairbanks [aut, cre],\n  Abdessabour Moutik [ctb],\n  Matt Carlson [ctb],\n  Ivan Leung [ctb],\n  Ross Kennedy [ctb],\n  Robert On [ctb],\n  Alexander Sevostianov [ctb],\n  Koen ter Berg [ctb]",
    "url": "https://markfairbanks.github.io/tidytable/,\nhttps://github.com/markfairbanks/tidytable",
    "bug_reports": "https://github.com/markfairbanks/tidytable/issues",
    "repository": "https://cran.r-project.org/package=tidytable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidytable Tidy Interface to 'data.table' A tidy interface to 'data.table',\n  giving users the speed of 'data.table' while using tidyverse-like syntax.  "
  },
  {
    "id": 22157,
    "package_name": "timbeR",
    "title": "Calculate Wood Volumes from Taper Functions",
    "description": "Functions for estimation of wood volumes, number of logs, diameters along the stem and heights at which certain diameters occur, based on taper functions and other parameters. References: McTague, J. P., & Weiskittel, A. (2021). <doi:10.1139/cjfr-2020-0326>.",
    "version": "2.0.1",
    "maintainer": "Sergio Costa <sergio.vscf@gmail.com>",
    "author": "Sergio Costa [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-5432-317X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=timbeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timbeR Calculate Wood Volumes from Taper Functions Functions for estimation of wood volumes, number of logs, diameters along the stem and heights at which certain diameters occur, based on taper functions and other parameters. References: McTague, J. P., & Weiskittel, A. (2021). <doi:10.1139/cjfr-2020-0326>.  "
  },
  {
    "id": 22174,
    "package_name": "timeplyr",
    "title": "Fast Tidy Tools for Date and Date-Time Manipulation",
    "description": "A set of fast tidy functions for wrangling, completing and\n    summarising date and date-time data. It combines 'tidyverse' syntax\n    with the efficiency of 'data.table' and speed of 'collapse'.",
    "version": "1.1.1",
    "maintainer": "Nick Christofides <nick.christofides.r@gmail.com>",
    "author": "Nick Christofides [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9743-7342>)",
    "url": "",
    "bug_reports": "https://github.com/NicChr/timeplyr/issues",
    "repository": "https://cran.r-project.org/package=timeplyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeplyr Fast Tidy Tools for Date and Date-Time Manipulation A set of fast tidy functions for wrangling, completing and\n    summarising date and date-time data. It combines 'tidyverse' syntax\n    with the efficiency of 'data.table' and speed of 'collapse'.  "
  },
  {
    "id": 22235,
    "package_name": "tmcn",
    "title": "A Text Mining Toolkit for Chinese",
    "description": "A Text mining toolkit for Chinese, which includes facilities for \n    Chinese string processing, Chinese NLP supporting, encoding detecting and \n    converting. Moreover, it provides some functions to support 'tm' package \n    in Chinese.",
    "version": "0.2-13",
    "maintainer": "Jian Li <rweibo@sina.com>",
    "author": "Jian Li",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tmcn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tmcn A Text Mining Toolkit for Chinese A Text mining toolkit for Chinese, which includes facilities for \n    Chinese string processing, Chinese NLP supporting, encoding detecting and \n    converting. Moreover, it provides some functions to support 'tm' package \n    in Chinese.  "
  },
  {
    "id": 22257,
    "package_name": "tomledit",
    "title": "Parse, Read, and Edit 'TOML'",
    "description": "A toolkit for working with 'TOML' files in R while preserving\n    formatting, comments, and structure. 'tomledit' enables serialization of R\n    objects such as lists, data.frames, numeric, logical, and date vectors.",
    "version": "0.1.1",
    "maintainer": "Josiah Parry <josiah.parry@gmail.com>",
    "author": "Josiah Parry [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9910-865X>)",
    "url": "https://extendr.github.io/tomledit/,\nhttps://github.com/extendr/tomledit",
    "bug_reports": "https://github.com/extendr/tomledit/issues",
    "repository": "https://cran.r-project.org/package=tomledit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tomledit Parse, Read, and Edit 'TOML' A toolkit for working with 'TOML' files in R while preserving\n    formatting, comments, and structure. 'tomledit' enables serialization of R\n    objects such as lists, data.frames, numeric, logical, and date vectors.  "
  },
  {
    "id": 22271,
    "package_name": "topoDistance",
    "title": "Calculating Topographic Paths and Distances",
    "description": "A toolkit for calculating topographic distances and identifying and plotting topographic paths. Topographic distances can be calculated along shortest topographic paths (Wang (2009) <doi:10.1111/j.1365-294X.2009.04338.x>), weighted topographic paths (Zhan et al. (1993) <doi:10.1007/3-540-57207-4_29>), and topographic least cost paths (Wang and Summers (2010) <doi:10.1111/j.1365-294X.2009.04465.x>). Functions can map topographic paths on colored or hill shade maps and plot topographic cross sections (elevation profiles) for the paths.",
    "version": "1.0.2",
    "maintainer": "Ian Wang <ianwang@berkeley.edu>",
    "author": "Ian Wang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=topoDistance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topoDistance Calculating Topographic Paths and Distances A toolkit for calculating topographic distances and identifying and plotting topographic paths. Topographic distances can be calculated along shortest topographic paths (Wang (2009) <doi:10.1111/j.1365-294X.2009.04338.x>), weighted topographic paths (Zhan et al. (1993) <doi:10.1007/3-540-57207-4_29>), and topographic least cost paths (Wang and Summers (2010) <doi:10.1111/j.1365-294X.2009.04465.x>). Functions can map topographic paths on colored or hill shade maps and plot topographic cross sections (elevation profiles) for the paths.  "
  },
  {
    "id": 22328,
    "package_name": "trainsplit",
    "title": "Split a Dataframe, Tibble, or Data.table into Training and Test\nSets",
    "description": "Split a dataframe, tibble, or data.table into training and test sets. Return either a list, an index, or directly assign training and test sets into memory.",
    "version": "1.2",
    "maintainer": "Zhaochen He <eastnileuc@gmail.com>",
    "author": "Zhaochen He [aut, cre] (ORCID: <https://orcid.org/0000-0002-6579-5073>)",
    "url": "https://github.com/eastnile/trainsplit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=trainsplit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trainsplit Split a Dataframe, Tibble, or Data.table into Training and Test\nSets Split a dataframe, tibble, or data.table into training and test sets. Return either a list, an index, or directly assign training and test sets into memory.  "
  },
  {
    "id": 22402,
    "package_name": "triangulr",
    "title": "High-Performance Triangular Distribution Functions",
    "description": "A collection of high-performance functions for the triangular\n    distribution that consists of the probability density function, cumulative\n    distribution function, quantile function, random variate generator, moment\n    generating function, characteristic function, and expected shortfall\n    function. References: Samuel Kotz, Johan Ren Van Dorp (2004)\n    <doi:10.1142/5720> and Acerbi, Carlo and Tasche, Dirk. (2002)\n    <doi:10.1111/1468-0300.00091>.",
    "version": "1.2.1",
    "maintainer": "Alvin Nursalim <irkaalv@gmail.com>",
    "author": "Alvin Nursalim [aut, cre]",
    "url": "https://github.com/irkaal/triangulr/\nhttps://irkaal.github.io/triangulr/",
    "bug_reports": "https://github.com/irkaal/triangulr/issues",
    "repository": "https://cran.r-project.org/package=triangulr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "triangulr High-Performance Triangular Distribution Functions A collection of high-performance functions for the triangular\n    distribution that consists of the probability density function, cumulative\n    distribution function, quantile function, random variate generator, moment\n    generating function, characteristic function, and expected shortfall\n    function. References: Samuel Kotz, Johan Ren Van Dorp (2004)\n    <doi:10.1142/5720> and Acerbi, Carlo and Tasche, Dirk. (2002)\n    <doi:10.1111/1468-0300.00091>.  "
  },
  {
    "id": 22409,
    "package_name": "trimmer",
    "title": "Trim an Object",
    "description": "A lightweight toolkit to reduce the size of a list object. The\n    object is minimized by recursively removing elements from the object \n    one-by-one. The process is constrained by a reference function call\n    specified by the user, where the target object is given as an argument. \n    The procedure will not allow elements to be removed from the object, that \n    will cause results from the function call to diverge from the function \n    call with the original object.",
    "version": "0.8.1",
    "maintainer": "Lars Kjeldgaard <lars_kjeldgaard@hotmail.com>",
    "author": "Lars Kjeldgaard [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=trimmer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trimmer Trim an Object A lightweight toolkit to reduce the size of a list object. The\n    object is minimized by recursively removing elements from the object \n    one-by-one. The process is constrained by a reference function call\n    specified by the user, where the target object is given as an argument. \n    The procedure will not allow elements to be removed from the object, that \n    will cause results from the function call to diverge from the function \n    call with the original object.  "
  },
  {
    "id": 22478,
    "package_name": "tsmp",
    "title": "Time Series with Matrix Profile",
    "description": "A toolkit implementing the Matrix Profile concept\n    that was created by CS-UCR\n    <http://www.cs.ucr.edu/~eamonn/MatrixProfile.html>.",
    "version": "0.4.16",
    "maintainer": "Francisco Bischoff <fbischoff@med.up.pt>",
    "author": "Francisco Bischoff [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5301-8672>),\n  Michael Yeh [res, ccp, ctb] (ORCID:\n    <https://orcid.org/0000-0002-9807-2963>),\n  Diego Silva [res, ccp, ctb] (ORCID:\n    <https://orcid.org/0000-0002-5184-9413>),\n  Yan Zhu [res, ccp, ctb] (ORCID:\n    <https://orcid.org/0000-0002-5952-2108>),\n  Hoang Dau [res, ccp, ctb] (ORCID:\n    <https://orcid.org/0000-0003-2439-5185>),\n  Michele Linardi [res, ccp, ctb] (ORCID:\n    <https://orcid.org/0000-0002-3249-2068>)",
    "url": "https://github.com/matrix-profile-foundation/tsmp",
    "bug_reports": "https://github.com/matrix-profile-foundation/tsmp/issues",
    "repository": "https://cran.r-project.org/package=tsmp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsmp Time Series with Matrix Profile A toolkit implementing the Matrix Profile concept\n    that was created by CS-UCR\n    <http://www.cs.ucr.edu/~eamonn/MatrixProfile.html>.  "
  },
  {
    "id": 22490,
    "package_name": "tstools",
    "title": "A Time Series Toolbox for Official Statistics",
    "description": "Plot official statistics' time series conveniently: automatic\n    legends, highlight windows, stacked bar chars with positive and\n    negative contributions, sum-as-line option, two y-axes with automatic\n    horizontal grids that fit both axes and other popular chart types.\n    'tstools' comes with a plethora of defaults to let you plot without\n    setting an abundance of parameters first, but gives you the\n    flexibility to tweak the defaults. In addition to charts, 'tstools'\n    provides a super fast, 'data.table' backed time series I/O that allows\n    the user to export / import long format, wide format and transposed\n    wide format data to various file types.",
    "version": "0.4.4",
    "maintainer": "St\u00e9phane Bisinger <bisinger@kof.ethz.ch>",
    "author": "Matthias Bannert [aut],\n  Severin Thoeni [aut],\n  St\u00e9phane Bisinger [aut, cre]",
    "url": "https://kof-ch.github.io/tstools/,\nhttps://github.com/KOF-ch/tstools",
    "bug_reports": "https://github.com/KOF-ch/tstools/issues",
    "repository": "https://cran.r-project.org/package=tstools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tstools A Time Series Toolbox for Official Statistics Plot official statistics' time series conveniently: automatic\n    legends, highlight windows, stacked bar chars with positive and\n    negative contributions, sum-as-line option, two y-axes with automatic\n    horizontal grids that fit both axes and other popular chart types.\n    'tstools' comes with a plethora of defaults to let you plot without\n    setting an abundance of parameters first, but gives you the\n    flexibility to tweak the defaults. In addition to charts, 'tstools'\n    provides a super fast, 'data.table' backed time series I/O that allows\n    the user to export / import long format, wide format and transposed\n    wide format data to various file types.  "
  },
  {
    "id": 22534,
    "package_name": "twang",
    "title": "Toolkit for Weighting and Analysis of Nonequivalent Groups",
    "description": "Provides functions for propensity score\n        estimating and weighting, nonresponse weighting, and diagnosis\n        of the weights.",
    "version": "2.6.1",
    "maintainer": "Lane Burgette <burgette@rand.org>",
    "author": "Matthew Cefalu, Greg Ridgeway, Dan McCaffrey, Andrew Morral, Beth Ann Griffin, and Lane Burgette",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=twang",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twang Toolkit for Weighting and Analysis of Nonequivalent Groups Provides functions for propensity score\n        estimating and weighting, nonresponse weighting, and diagnosis\n        of the weights.  "
  },
  {
    "id": 22535,
    "package_name": "twangContinuous",
    "title": "Toolkit for Weighting and Analysis of Nonequivalent Groups -\nContinuous Exposures",
    "description": "Provides functions for propensity score\n        estimation and weighting for continuous exposures as described in Zhu, Y., \n        Coffman, D. L., & Ghosh, D. (2015). A boosting algorithm for\n        estimating generalized propensity scores with continuous treatments.\n        Journal of Causal Inference, 3(1), 25-40. <doi:10.1515/jci-2014-0022>.",
    "version": "1.0.0",
    "maintainer": "Donna Coffman <donna.coffman@gmail.com>",
    "author": "Donna Coffman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6305-6579>),\n  Brian Vegetabile [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=twangContinuous",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twangContinuous Toolkit for Weighting and Analysis of Nonequivalent Groups -\nContinuous Exposures Provides functions for propensity score\n        estimation and weighting for continuous exposures as described in Zhu, Y., \n        Coffman, D. L., & Ghosh, D. (2015). A boosting algorithm for\n        estimating generalized propensity scores with continuous treatments.\n        Journal of Causal Inference, 3(1), 25-40. <doi:10.1515/jci-2014-0022>.  "
  },
  {
    "id": 22537,
    "package_name": "twbparser",
    "title": "Parse 'Tableau' Workbooks into Functional Data",
    "description": "High-performance parsing of 'Tableau' workbook files into\n    tidy data frames and dependency graphs for other visualization tools\n    like R 'Shiny' or 'Power BI' replication.",
    "version": "0.3.1",
    "maintainer": "George Arthur <prigasgenthian48@gmail.com>",
    "author": "George Arthur [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1975-1459>)",
    "url": "https://prigasg.github.io/twbparser/,\nhttps://github.com/PrigasG/twbparser",
    "bug_reports": "https://github.com/PrigasG/twbparser/issues",
    "repository": "https://cran.r-project.org/package=twbparser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twbparser Parse 'Tableau' Workbooks into Functional Data High-performance parsing of 'Tableau' workbook files into\n    tidy data frames and dependency graphs for other visualization tools\n    like R 'Shiny' or 'Power BI' replication.  "
  },
  {
    "id": 22587,
    "package_name": "udpipe",
    "title": "Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit",
    "description": "This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.",
    "version": "0.8.15",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  Institute of Formal and Applied Linguistics, Faculty of Mathematics and\n    Physics, Charles University in Prague, Czech Republic [cph]\n    (src/udpipe.cpp & src/udpipe.h),\n  Milan Straka [aut, cph] (src/udpipe.cpp & src/udpipe.h),\n  Jana Strakov\u00e1 [ctb, cph] (src/udpipe.cpp & src/udpipe.h)",
    "url": "https://bnosac.github.io/udpipe/en/index.html,\nhttps://github.com/bnosac/udpipe",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=udpipe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "udpipe Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.  "
  },
  {
    "id": 22679,
    "package_name": "urltools",
    "title": "Vectorised Tools for URL Handling and Parsing",
    "description": "A toolkit for all URL-handling needs, including encoding and decoding,\n    parsing, parameter extraction and modification. All functions are\n    designed to be both fast and entirely vectorised. It is intended to be\n    useful for people dealing with web-related datasets, such as server-side\n    logs, although may be useful for other situations involving large sets of\n    URLs.",
    "version": "1.7.3.1",
    "maintainer": "Os Keyes <ironholds@gmail.com>",
    "author": "Os Keyes [aut, cre],\n  Jay Jacobs [aut],\n  Drew Schmidt [aut],\n  Mark Greenaway [ctb],\n  Bob Rudis [ctb],\n  Alex Pinto [ctb],\n  Maryam Khezrzadeh [ctb],\n  Peter Meilstrup [ctb],\n  Adam M. Costello [cph],\n  Jeff Bezanson [cph],\n  Peter Meilstrup [ctb],\n  Xueyuan Jiang [ctb]",
    "url": "https://github.com/Ironholds/urltools/",
    "bug_reports": "https://github.com/Ironholds/urltools/issues",
    "repository": "https://cran.r-project.org/package=urltools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "urltools Vectorised Tools for URL Handling and Parsing A toolkit for all URL-handling needs, including encoding and decoding,\n    parsing, parameter extraction and modification. All functions are\n    designed to be both fast and entirely vectorised. It is intended to be\n    useful for people dealing with web-related datasets, such as server-side\n    logs, although may be useful for other situations involving large sets of\n    URLs.  "
  },
  {
    "id": 22809,
    "package_name": "vegIndexCalc",
    "title": "Vegetation Indices (VIs) Calculation for Remote Sensing Analysis",
    "description": "It provides a comprehensive toolkit for calculating a suite of common vegetation indices (VIs) derived from remote sensing imagery. VIs are essential tools used to quantify vegetation characteristics, such as biomass, leaf area index (LAI) and photosynthetic activity, which are essential parameters in various ecological, agricultural, and environmental studies. Applications of this package include biomass estimation, crop monitoring, forest management, land use and land cover change analysis and climate change studies. For method details see, Deb,D.,Deb,S.,Chakraborty,D.,Singh,J.P.,Singh,A.K.,Dutta,P.and Choudhury,A.(2020)<doi:10.1080/10106049.2020.1756461>. Utilizing this R package, users can effectively extract and analyze critical information from remote sensing imagery, enhancing their comprehension of vegetation dynamics and their importance in global ecosystems. The package includes the function vegetation_indices().",
    "version": "0.1.0",
    "maintainer": "Bijoy Chanda <bijoychanda08@gmail.com>",
    "author": "Dibyendu Deb [aut, ctb],\n  Arpan Bhowmik [aut, ctb],\n  Bijoy Chanda [aut, cre, ctb],\n  J.P. Singh [aut],\n  Sunil Mandi [aut],\n  Alemwati Pongener [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vegIndexCalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vegIndexCalc Vegetation Indices (VIs) Calculation for Remote Sensing Analysis It provides a comprehensive toolkit for calculating a suite of common vegetation indices (VIs) derived from remote sensing imagery. VIs are essential tools used to quantify vegetation characteristics, such as biomass, leaf area index (LAI) and photosynthetic activity, which are essential parameters in various ecological, agricultural, and environmental studies. Applications of this package include biomass estimation, crop monitoring, forest management, land use and land cover change analysis and climate change studies. For method details see, Deb,D.,Deb,S.,Chakraborty,D.,Singh,J.P.,Singh,A.K.,Dutta,P.and Choudhury,A.(2020)<doi:10.1080/10106049.2020.1756461>. Utilizing this R package, users can effectively extract and analyze critical information from remote sensing imagery, enhancing their comprehension of vegetation dynamics and their importance in global ecosystems. The package includes the function vegetation_indices().  "
  },
  {
    "id": 22903,
    "package_name": "vmTools",
    "title": "Version Management Tools on the File System",
    "description": "Data version management on the file system for smaller projects.\n    Manage data pipeline outputs with symbolic folder links, structured logging\n    and reports, using 'R6' classes for encapsulation and 'data.table' for\n    speed. Directory-specific logs used as source of truth to allow portability\n    of versioned data folders.",
    "version": "1.0.1",
    "maintainer": "Sam Byrne <ssbyrne@uw.edu>",
    "author": "Sam Byrne [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0008-1067-307X>)",
    "url": "https://github.com/epi-sam/vmTools",
    "bug_reports": "https://github.com/epi-sam/vmTools/issues",
    "repository": "https://cran.r-project.org/package=vmTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vmTools Version Management Tools on the File System Data version management on the file system for smaller projects.\n    Manage data pipeline outputs with symbolic folder links, structured logging\n    and reports, using 'R6' classes for encapsulation and 'data.table' for\n    speed. Directory-specific logs used as source of truth to allow portability\n    of versioned data folders.  "
  },
  {
    "id": 22975,
    "package_name": "walkboutr",
    "title": "Generate Walk Bouts from GPS and Accelerometry Data",
    "description": "Process GPS and accelerometry data to generate walk bouts. A walk bout is a period of activity with accelerometer movement matching the patterns of walking with corresponding GPS measurements that confirm travel. The inputs of the 'walkboutr' package are individual-level accelerometry and GPS data. The outputs of the model are walk bouts with corresponding times, duration, and summary statistics on the sample population, which collapse all personally identifying information. These bouts can be used to measure walking both as an outcome of a change to the built environment or as a predictor of health outcomes such as a cardioprotective behavior. Kang B, Moudon AV, Hurvitz PM, Saelens BE (2017) <doi:10.1016/j.trd.2017.09.026>.",
    "version": "0.6.0",
    "maintainer": "Lauren Blair Wilner <wilnerl@uw.edu>",
    "author": "Lauren Blair Wilner [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4439-3734>),\n  Stephen J Mooney [aut]",
    "url": "https://github.com/rwalkbout/walkboutr,\nhttps://rwalkbout.github.io/walkboutr/",
    "bug_reports": "https://github.com/rwalkbout/walkboutr/issues",
    "repository": "https://cran.r-project.org/package=walkboutr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "walkboutr Generate Walk Bouts from GPS and Accelerometry Data Process GPS and accelerometry data to generate walk bouts. A walk bout is a period of activity with accelerometer movement matching the patterns of walking with corresponding GPS measurements that confirm travel. The inputs of the 'walkboutr' package are individual-level accelerometry and GPS data. The outputs of the model are walk bouts with corresponding times, duration, and summary statistics on the sample population, which collapse all personally identifying information. These bouts can be used to measure walking both as an outcome of a change to the built environment or as a predictor of health outcomes such as a cardioprotective behavior. Kang B, Moudon AV, Hurvitz PM, Saelens BE (2017) <doi:10.1016/j.trd.2017.09.026>.  "
  },
  {
    "id": 22993,
    "package_name": "washr",
    "title": "Publication Toolkit for Water, Sanitation and Hygiene (WASH)\nData",
    "description": "A toolkit to set up an R data package in a consistent structure. Automates tasks like tidy data export, data dictionary documentation, README and website creation, and citation management.",
    "version": "1.0.1",
    "maintainer": "Colin Walder <cwalder@ethz.ch>",
    "author": "Mian Zhong [aut] (ORCID: <https://orcid.org/0009-0009-4546-7214>),\n  Margaux G\u00f6tschmann [aut] (ORCID:\n    <https://orcid.org/0009-0002-2567-3343>),\n  Colin Walder [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-0969-1954>),\n  Lars Sch\u00f6bitz [aut] (ORCID: <https://orcid.org/0000-0003-2196-5015>),\n  Global Health Engineering, ETH Zurich [cph]",
    "url": "https://openwashdata.github.io/washr/",
    "bug_reports": "https://github.com/openwashdata/washr/issues",
    "repository": "https://cran.r-project.org/package=washr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "washr Publication Toolkit for Water, Sanitation and Hygiene (WASH)\nData A toolkit to set up an R data package in a consistent structure. Automates tasks like tidy data export, data dictionary documentation, README and website creation, and citation management.  "
  },
  {
    "id": 22994,
    "package_name": "waspasR",
    "title": "Tool Kit to Implement a W.A.S.P.A.S. Based Multi-Criteria\nDecision Analysis Solution",
    "description": "Provides a set of functions to implement decision-making systems\n    based on the W.A.S.P.A.S. method (Weighted Aggregated Sum Product Assessment),\n    Chakraborty and Zavadskas (2012) <doi:10.5755/j01.eee.122.6.1810>.\n    So this package offers functions that analyze and validate the\n    raw data, which must be entered in a determined format;\n    extract specific vectors and matrices from this raw database;\n    normalize the input data; calculate rankings by intermediate methods;\n    apply the lambda parameter for the main method; and a function that does\n    everything at once. The package has an example database called choppers,\n    with which the user can see how the input data should be organized so that\n    everything works as recommended by the decision methods based on multiple\n    criteria that this package solves. Basically, the data are composed of a set\n    of alternatives, which will be ranked, a set of choice criteria, a matrix\n    of values for each Alternative-Criterion relationship, a vector of weights\n    associated with the criteria, since certain criteria are considered more\n    important than others, as well as a vector that defines each criterion as\n    cost or benefit, this determines the calculation formula, as there are those\n    criteria that we want the highest possible value (e.g. durability)\n    and others that we want the lowest possible value (e.g. price).",
    "version": "0.1.5",
    "maintainer": "Flavio Barbara <flavio.barbara@gmail.com>",
    "author": "Flavio Barbara [cre, aut],\n  Marcos Santos [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=waspasR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "waspasR Tool Kit to Implement a W.A.S.P.A.S. Based Multi-Criteria\nDecision Analysis Solution Provides a set of functions to implement decision-making systems\n    based on the W.A.S.P.A.S. method (Weighted Aggregated Sum Product Assessment),\n    Chakraborty and Zavadskas (2012) <doi:10.5755/j01.eee.122.6.1810>.\n    So this package offers functions that analyze and validate the\n    raw data, which must be entered in a determined format;\n    extract specific vectors and matrices from this raw database;\n    normalize the input data; calculate rankings by intermediate methods;\n    apply the lambda parameter for the main method; and a function that does\n    everything at once. The package has an example database called choppers,\n    with which the user can see how the input data should be organized so that\n    everything works as recommended by the decision methods based on multiple\n    criteria that this package solves. Basically, the data are composed of a set\n    of alternatives, which will be ranked, a set of choice criteria, a matrix\n    of values for each Alternative-Criterion relationship, a vector of weights\n    associated with the criteria, since certain criteria are considered more\n    important than others, as well as a vector that defines each criterion as\n    cost or benefit, this determines the calculation formula, as there are those\n    criteria that we want the highest possible value (e.g. durability)\n    and others that we want the lowest possible value (e.g. price).  "
  },
  {
    "id": 23079,
    "package_name": "where",
    "title": "Vectorised Substitution and Evaluation",
    "description": "Provides a clean syntax for vectorising the use of\n    Non-Standard Evaluation (NSE), for example in 'ggplot2', 'dplyr', or\n    'data.table'.",
    "version": "1.0.0",
    "maintainer": "Matt Hendtlass <m.hendtlass@gmail.com>",
    "author": "Matt Hendtlass [aut, cre]",
    "url": "https://github.com/KiwiMateo/where",
    "bug_reports": "https://github.com/KiwiMateo/where/issues",
    "repository": "https://cran.r-project.org/package=where",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "where Vectorised Substitution and Evaluation Provides a clean syntax for vectorising the use of\n    Non-Standard Evaluation (NSE), for example in 'ggplot2', 'dplyr', or\n    'data.table'.  "
  },
  {
    "id": 23122,
    "package_name": "wk",
    "title": "Lightweight Well-Known Geometry Parsing",
    "description": "Provides a minimal R and C++ API for parsing\n  well-known binary and well-known text representation of\n  geometries to and from R-native formats.\n  Well-known binary is compact\n  and fast to parse; well-known text is human-readable\n  and is useful for writing tests. These formats are\n  useful in R only if the information they contain can be\n  accessed in R, for which high-performance functions\n  are provided here.",
    "version": "0.9.5",
    "maintainer": "Dewey Dunnington <dewey@fishandwhistle.net>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Edzer Pebesma [aut] (ORCID: <https://orcid.org/0000-0001-8049-7069>),\n  Anthony North [ctb]",
    "url": "https://paleolimbot.github.io/wk/,\nhttps://github.com/paleolimbot/wk",
    "bug_reports": "https://github.com/paleolimbot/wk/issues",
    "repository": "https://cran.r-project.org/package=wk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wk Lightweight Well-Known Geometry Parsing Provides a minimal R and C++ API for parsing\n  well-known binary and well-known text representation of\n  geometries to and from R-native formats.\n  Well-known binary is compact\n  and fast to parse; well-known text is human-readable\n  and is useful for writing tests. These formats are\n  useful in R only if the information they contain can be\n  accessed in R, for which high-performance functions\n  are provided here.  "
  },
  {
    "id": 23208,
    "package_name": "xaringanExtra",
    "title": "Extras and Extensions for 'xaringan' Slides",
    "description": "Extras and extensions for 'xaringan' slides. Navigate your\n    slides with tile view. Make your slides editable, live! Announce slide\n    changes with subtle tones. Animate slide transitions with\n    'animate.css'. Add tabbed panels to slides with 'panelset'. Use the\n    'Tachyons CSS' utility toolkit for rapid slide development. Scribble\n    on your slides. Add a copy button to your code chunks with\n    'clipboard'. Add a logo or top or bottom banner to every slide.\n    Broadcast slides to stay in sync with remote viewers. Include yourself\n    in your slides with 'webcam'.  Plus a whole lot more!",
    "version": "0.8.0",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>),\n  Matthew T. Warkentin [aut] (Contributed scribble, ORCID:\n    <https://orcid.org/0000-0001-8730-3511>),\n  Yotam Mann [cph] (tone.js),\n  Daniel Eden [cph] (animate.css),\n  Tachyons authors [cph],\n  Klaus Hartl, Fagner Brack, GitHub Contributors [cph] (js-cookie),\n  Chris Andrejewski [cph] (himalaya),\n  Eric Londaits [cph] (text-poster.js),\n  Zeno Rocha [cph] (clipboard.js),\n  Nikita Karamov [cph] (shareon.js),\n  Ross Zurowski [cph] (fitvids.js),\n  Michelle Bu and Eric Zhang [cph] (peerjs),\n  Kiril Vatev [cph] (tiny.toast),\n  Andr\u00e9 Restive [cph] (remark.search),\n  Printio (Juriy Zaytsev, Maxim Chernyak) [cph] (fabric.js),\n  Christpher Antonellis [cph] (freezeframe.js)",
    "url": "https://pkg.garrickadenbuie.com/xaringanExtra/,\nhttps://github.com/gadenbuie/xaringanExtra",
    "bug_reports": "https://github.com/gadenbuie/xaringanExtra/issues",
    "repository": "https://cran.r-project.org/package=xaringanExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xaringanExtra Extras and Extensions for 'xaringan' Slides Extras and extensions for 'xaringan' slides. Navigate your\n    slides with tile view. Make your slides editable, live! Announce slide\n    changes with subtle tones. Animate slide transitions with\n    'animate.css'. Add tabbed panels to slides with 'panelset'. Use the\n    'Tachyons CSS' utility toolkit for rapid slide development. Scribble\n    on your slides. Add a copy button to your code chunks with\n    'clipboard'. Add a logo or top or bottom banner to every slide.\n    Broadcast slides to stay in sync with remote viewers. Include yourself\n    in your slides with 'webcam'.  Plus a whole lot more!  "
  },
  {
    "id": 23213,
    "package_name": "xega",
    "title": "Extended Evolutionary and Genetic Algorithms",
    "description": "\n        Implementation of a scalable, highly configurable, and \n        e(x)tended architecture for (e)volutionary and (g)enetic (a)lgorithms.\n        Multiple representations (binary, real-coded, permutation, and \n        derivation-tree), a rich collection of genetic operators, \n        as well as an extended processing pipeline are provided \n        for genetic algorithms (Goldberg, D. E. (1989, ISBN:0-201-15767-5)),\n        differential evolution (Price, Kenneth V., Storn, Rainer M. and Lampinen, Jouni A. (2005)\n        <doi:10.1007/3-540-31306-0>), simulated annealing (Aarts, E., and Korst, J.\n        (1989, ISBN:0-471-92146-7)), grammar-based genetic programming \n        (Geyer-Schulz (1997, ISBN:978-3-7908-0830-X)), grammatical evolution \n        (Ryan, C., O'Neill, M., and Collins, J. J. (2018) <doi:10.1007/978-3-319-78717-6>), \n        and grammatical differential evolution (O'Neill, M. and Brabazon, A. (2006) in  \n        Arabinia, H. (2006, ISBN:978-193-241596-3).\n        All algorithms reuse basic adaptive mechanisms for performance optimization.\n        For xega's architecture, see Geyer-Schulz, A. (2025) <doi:10.5445/IR/1000187255>.\n        Sequential or parallel execution (on multi-core machines, \n        local clusters, and high-performance computing environments) \n        is available for all algorithms. See \n        <https://github.com/ageyerschulz/xega/tree/main/examples/executionModel>.",
    "version": "0.9.0.18",
    "maintainer": "Andreas Geyer-Schulz <Andreas.Geyer-Schulz@kit.edu>",
    "author": "Andreas Geyer-Schulz [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-5237-3579>)",
    "url": "https://github.com/ageyerschulz/xega",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=xega",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xega Extended Evolutionary and Genetic Algorithms \n        Implementation of a scalable, highly configurable, and \n        e(x)tended architecture for (e)volutionary and (g)enetic (a)lgorithms.\n        Multiple representations (binary, real-coded, permutation, and \n        derivation-tree), a rich collection of genetic operators, \n        as well as an extended processing pipeline are provided \n        for genetic algorithms (Goldberg, D. E. (1989, ISBN:0-201-15767-5)),\n        differential evolution (Price, Kenneth V., Storn, Rainer M. and Lampinen, Jouni A. (2005)\n        <doi:10.1007/3-540-31306-0>), simulated annealing (Aarts, E., and Korst, J.\n        (1989, ISBN:0-471-92146-7)), grammar-based genetic programming \n        (Geyer-Schulz (1997, ISBN:978-3-7908-0830-X)), grammatical evolution \n        (Ryan, C., O'Neill, M., and Collins, J. J. (2018) <doi:10.1007/978-3-319-78717-6>), \n        and grammatical differential evolution (O'Neill, M. and Brabazon, A. (2006) in  \n        Arabinia, H. (2006, ISBN:978-193-241596-3).\n        All algorithms reuse basic adaptive mechanisms for performance optimization.\n        For xega's architecture, see Geyer-Schulz, A. (2025) <doi:10.5445/IR/1000187255>.\n        Sequential or parallel execution (on multi-core machines, \n        local clusters, and high-performance computing environments) \n        is available for all algorithms. See \n        <https://github.com/ageyerschulz/xega/tree/main/examples/executionModel>.  "
  },
  {
    "id": 23279,
    "package_name": "yasp",
    "title": "String Functions for Compact R Code",
    "description": "A collection of string functions designed for writing \n    compact and expressive R code. 'yasp' (Yet Another String Package) is simple,\n    fast, dependency-free, and written in pure R. The package provides: a\n    coherent set of abbreviations for paste() from package 'base' with a\n    variety of defaults, such as p() for \"paste\" and pcc() for \"paste and\n    collapse with commas\"; wrap(), bracket(), and others for wrapping a\n    string in flanking characters; unwrap() for removing pairs of characters\n    (at any position in a string); and sentence() for cleaning whitespace\n    around punctuation and capitalization appropriate for prose sentences.",
    "version": "0.2.0",
    "maintainer": "Tomasz Kalinowski <tkalinow@asu.edu>",
    "author": "Tomasz Kalinowski [aut, cre]",
    "url": "https://github.com/t-kalinowski/yasp",
    "bug_reports": "https://github.com/t-kalinowski/yasp/issues",
    "repository": "https://cran.r-project.org/package=yasp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "yasp String Functions for Compact R Code A collection of string functions designed for writing \n    compact and expressive R code. 'yasp' (Yet Another String Package) is simple,\n    fast, dependency-free, and written in pure R. The package provides: a\n    coherent set of abbreviations for paste() from package 'base' with a\n    variety of defaults, such as p() for \"paste\" and pcc() for \"paste and\n    collapse with commas\"; wrap(), bracket(), and others for wrapping a\n    string in flanking characters; unwrap() for removing pairs of characters\n    (at any position in a string); and sentence() for cleaning whitespace\n    around punctuation and capitalization appropriate for prose sentences.  "
  }
]