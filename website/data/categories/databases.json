[
  {
    "id": 449,
    "package_name": "dbplyr",
    "title": "A 'dplyr' Back End for Databases",
    "description": "A 'dplyr' back end for databases that allows you to work\nwith remote database tables as if they are in-memory data\nframes.  Basic features works with any database that has a\n'DBI' back end; more advanced features require 'SQL'\ntranslation to be provided by the package author.",
    "version": "2.5.1.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre],\nMaximilian Girlich [aut],\nEdgar Ruiz [aut],\nPosit Software, PBC [cph, fnd]",
    "url": "https://dbplyr.tidyverse.org/, https://github.com/tidyverse/dbplyr",
    "bug_reports": "https://github.com/tidyverse/dbplyr/issues",
    "repository": "",
    "exports": [
      [
        ".sql"
      ],
      [
        "as_table_path"
      ],
      [
        "as.sql"
      ],
      [
        "base_agg"
      ],
      [
        "base_no_win"
      ],
      [
        "base_odbc_agg"
      ],
      [
        "base_odbc_scalar"
      ],
      [
        "base_odbc_win"
      ],
      [
        "base_scalar"
      ],
      [
        "base_win"
      ],
      [
        "build_sql"
      ],
      [
        "check_table_path"
      ],
      [
        "copy_inline"
      ],
      [
        "copy_lahman"
      ],
      [
        "copy_nycflights13"
      ],
      [
        "db_col_types"
      ],
      [
        "db_collect"
      ],
      [
        "db_compute"
      ],
      [
        "db_connection_describe"
      ],
      [
        "db_copy_to"
      ],
      [
        "db_sql_render"
      ],
      [
        "db_supports_table_alias_with_as"
      ],
      [
        "db_table_temporary"
      ],
      [
        "dbplyr_edition"
      ],
      [
        "dbplyr_pivot_wider_spec"
      ],
      [
        "dbplyr_uncount"
      ],
      [
        "escape"
      ],
      [
        "escape_ansi"
      ],
      [
        "get_returned_rows"
      ],
      [
        "has_lahman"
      ],
      [
        "has_nycflights13"
      ],
      [
        "has_returned_rows"
      ],
      [
        "ident"
      ],
      [
        "ident_q"
      ],
      [
        "in_catalog"
      ],
      [
        "in_schema"
      ],
      [
        "is_table_path"
      ],
      [
        "is.ident"
      ],
      [
        "is.sql"
      ],
      [
        "join_query"
      ],
      [
        "lahman_mysql"
      ],
      [
        "lahman_postgres"
      ],
      [
        "lahman_sqlite"
      ],
      [
        "lahman_srcs"
      ],
      [
        "last_sql"
      ],
      [
        "lazy_base_query"
      ],
      [
        "lazy_frame"
      ],
      [
        "lazy_multi_join_query"
      ],
      [
        "lazy_query"
      ],
      [
        "lazy_rf_join_query"
      ],
      [
        "lazy_select_query"
      ],
      [
        "lazy_semi_join_query"
      ],
      [
        "lazy_set_op_query"
      ],
      [
        "lazy_union_query"
      ],
      [
        "memdb_frame"
      ],
      [
        "named_commas"
      ],
      [
        "nycflights13_postgres"
      ],
      [
        "nycflights13_sqlite"
      ],
      [
        "op_frame"
      ],
      [
        "op_grps"
      ],
      [
        "op_sort"
      ],
      [
        "op_vars"
      ],
      [
        "partial_eval"
      ],
      [
        "remote_con"
      ],
      [
        "remote_name"
      ],
      [
        "remote_query"
      ],
      [
        "remote_query_plan"
      ],
      [
        "remote_src"
      ],
      [
        "remote_table"
      ],
      [
        "select_query"
      ],
      [
        "semi_join_query"
      ],
      [
        "set_op_query"
      ],
      [
        "simulate_access"
      ],
      [
        "simulate_dbi"
      ],
      [
        "simulate_hana"
      ],
      [
        "simulate_hive"
      ],
      [
        "simulate_impala"
      ],
      [
        "simulate_mariadb"
      ],
      [
        "simulate_mssql"
      ],
      [
        "simulate_mysql"
      ],
      [
        "simulate_odbc"
      ],
      [
        "simulate_oracle"
      ],
      [
        "simulate_postgres"
      ],
      [
        "simulate_redshift"
      ],
      [
        "simulate_snowflake"
      ],
      [
        "simulate_spark_sql"
      ],
      [
        "simulate_sqlite"
      ],
      [
        "simulate_teradata"
      ],
      [
        "sql"
      ],
      [
        "sql_aggregate"
      ],
      [
        "sql_aggregate_2"
      ],
      [
        "sql_aggregate_n"
      ],
      [
        "sql_build"
      ],
      [
        "sql_call2"
      ],
      [
        "sql_cast"
      ],
      [
        "sql_check_na_rm"
      ],
      [
        "sql_cot"
      ],
      [
        "sql_escape_date"
      ],
      [
        "sql_escape_datetime"
      ],
      [
        "sql_escape_logical"
      ],
      [
        "sql_escape_raw"
      ],
      [
        "sql_expr"
      ],
      [
        "sql_expr_matches"
      ],
      [
        "sql_glue"
      ],
      [
        "sql_glue2"
      ],
      [
        "sql_indent_subquery"
      ],
      [
        "sql_infix"
      ],
      [
        "sql_join_suffix"
      ],
      [
        "sql_log"
      ],
      [
        "sql_not_supported"
      ],
      [
        "sql_optimise"
      ],
      [
        "sql_options"
      ],
      [
        "sql_paste"
      ],
      [
        "sql_paste_infix"
      ],
      [
        "sql_prefix"
      ],
      [
        "sql_query_append"
      ],
      [
        "sql_query_delete"
      ],
      [
        "sql_query_explain"
      ],
      [
        "sql_query_fields"
      ],
      [
        "sql_query_insert"
      ],
      [
        "sql_query_join"
      ],
      [
        "sql_query_multi_join"
      ],
      [
        "sql_query_rows"
      ],
      [
        "sql_query_save"
      ],
      [
        "sql_query_select"
      ],
      [
        "sql_query_semi_join"
      ],
      [
        "sql_query_set_op"
      ],
      [
        "sql_query_union"
      ],
      [
        "sql_query_update_from"
      ],
      [
        "sql_query_upsert"
      ],
      [
        "sql_query_wrap"
      ],
      [
        "sql_quote"
      ],
      [
        "sql_random"
      ],
      [
        "sql_render"
      ],
      [
        "sql_returning_cols"
      ],
      [
        "sql_runif"
      ],
      [
        "sql_str_sub"
      ],
      [
        "sql_substr"
      ],
      [
        "sql_table_analyze"
      ],
      [
        "sql_table_index"
      ],
      [
        "sql_translation"
      ],
      [
        "sql_translator"
      ],
      [
        "sql_try_cast"
      ],
      [
        "sql_variant"
      ],
      [
        "sql_vector"
      ],
      [
        "src_dbi"
      ],
      [
        "src_memdb"
      ],
      [
        "src_test"
      ],
      [
        "supports_window_clause"
      ],
      [
        "table_path_components"
      ],
      [
        "table_path_name"
      ],
      [
        "tbl_lazy"
      ],
      [
        "tbl_memdb"
      ],
      [
        "tbl_sql"
      ],
      [
        "test_frame"
      ],
      [
        "test_load"
      ],
      [
        "test_register_con"
      ],
      [
        "test_register_src"
      ],
      [
        "translate_sql"
      ],
      [
        "translate_sql_"
      ],
      [
        "union_query"
      ],
      [
        "win_absent"
      ],
      [
        "win_aggregate"
      ],
      [
        "win_aggregate_2"
      ],
      [
        "win_cumulative"
      ],
      [
        "win_current_frame"
      ],
      [
        "win_current_group"
      ],
      [
        "win_current_order"
      ],
      [
        "win_over"
      ],
      [
        "win_rank"
      ],
      [
        "win_recycled"
      ],
      [
        "window_frame"
      ],
      [
        "window_order"
      ]
    ],
    "topics": [
      [
        "database"
      ]
    ],
    "score": 19.6458,
    "stars": 491,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "dbplyr A 'dplyr' Back End for Databases A 'dplyr' back end for databases that allows you to work\nwith remote database tables as if they are in-memory data\nframes.  Basic features works with any database that has a\n'DBI' back end; more advanced features require 'SQL'\ntranslation to be provided by the package author. .sql as_table_path as.sql base_agg base_no_win base_odbc_agg base_odbc_scalar base_odbc_win base_scalar base_win build_sql check_table_path copy_inline copy_lahman copy_nycflights13 db_col_types db_collect db_compute db_connection_describe db_copy_to db_sql_render db_supports_table_alias_with_as db_table_temporary dbplyr_edition dbplyr_pivot_wider_spec dbplyr_uncount escape escape_ansi get_returned_rows has_lahman has_nycflights13 has_returned_rows ident ident_q in_catalog in_schema is_table_path is.ident is.sql join_query lahman_mysql lahman_postgres lahman_sqlite lahman_srcs last_sql lazy_base_query lazy_frame lazy_multi_join_query lazy_query lazy_rf_join_query lazy_select_query lazy_semi_join_query lazy_set_op_query lazy_union_query memdb_frame named_commas nycflights13_postgres nycflights13_sqlite op_frame op_grps op_sort op_vars partial_eval remote_con remote_name remote_query remote_query_plan remote_src remote_table select_query semi_join_query set_op_query simulate_access simulate_dbi simulate_hana simulate_hive simulate_impala simulate_mariadb simulate_mssql simulate_mysql simulate_odbc simulate_oracle simulate_postgres simulate_redshift simulate_snowflake simulate_spark_sql simulate_sqlite simulate_teradata sql sql_aggregate sql_aggregate_2 sql_aggregate_n sql_build sql_call2 sql_cast sql_check_na_rm sql_cot sql_escape_date sql_escape_datetime sql_escape_logical sql_escape_raw sql_expr sql_expr_matches sql_glue sql_glue2 sql_indent_subquery sql_infix sql_join_suffix sql_log sql_not_supported sql_optimise sql_options sql_paste sql_paste_infix sql_prefix sql_query_append sql_query_delete sql_query_explain sql_query_fields sql_query_insert sql_query_join sql_query_multi_join sql_query_rows sql_query_save sql_query_select sql_query_semi_join sql_query_set_op sql_query_union sql_query_update_from sql_query_upsert sql_query_wrap sql_quote sql_random sql_render sql_returning_cols sql_runif sql_str_sub sql_substr sql_table_analyze sql_table_index sql_translation sql_translator sql_try_cast sql_variant sql_vector src_dbi src_memdb src_test supports_window_clause table_path_components table_path_name tbl_lazy tbl_memdb tbl_sql test_frame test_load test_register_con test_register_src translate_sql translate_sql_ union_query win_absent win_aggregate win_aggregate_2 win_cumulative win_current_frame win_current_group win_current_order win_over win_rank win_recycled window_frame window_order database"
  },
  {
    "id": 300,
    "package_name": "bit64",
    "title": "A S3 Class for Vectors of 64bit Integers",
    "description": "Package 'bit64' provides serializable S3 atomic 64bit\n(signed) integers. These are useful for handling database keys\nand exact counting in +-2^63. WARNING: do not use them as\nreplacement for 32bit integers, integer64 are not supported for\nsubscripting by R-core and they have different semantics when\ncombined with double, e.g. integer64 + double => integer64.\nClass integer64 can be used in vectors, matrices, arrays and\ndata.frames. Methods are available for coercion from and to\nlogicals, integers, doubles, characters and factors as well as\nmany elementwise and summary functions. Many fast algorithmic\noperations such as 'match' and 'order' support inter- active\ndata exploration and manipulation and optionally leverage\ncaching.",
    "version": "4.7.99",
    "maintainer": "Michael Chirico <michaelchirico4@gmail.com>",
    "author": "Michael Chirico [aut, cre],\nJens Oehlschl\u00e4gel [aut],\nLeonardo Silvestri [ctb],\nOfek Shilon [ctb]",
    "url": "https://github.com/r-lib/bit64, https://bit64.r-lib.org",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        ":"
      ],
      [
        ":.default"
      ],
      [
        ":.integer64"
      ],
      [
        "[.integer64"
      ],
      [
        "[[.integer64"
      ],
      [
        "[[<-.integer64"
      ],
      [
        "[<-.integer64"
      ],
      [
        "%in%"
      ],
      [
        "%in%.default"
      ],
      [
        "%in%.integer64"
      ],
      [
        "abs.integer64"
      ],
      [
        "all.equal.integer64"
      ],
      [
        "all.integer64"
      ],
      [
        "any.integer64"
      ],
      [
        "as.bitstring"
      ],
      [
        "as.bitstring.integer64"
      ],
      [
        "as.character.integer64"
      ],
      [
        "as.data.frame.integer64"
      ],
      [
        "as.double.integer64"
      ],
      [
        "as.integer.integer64"
      ],
      [
        "as.integer64"
      ],
      [
        "as.integer64.bitstring"
      ],
      [
        "as.integer64.character"
      ],
      [
        "as.integer64.double"
      ],
      [
        "as.integer64.factor"
      ],
      [
        "as.integer64.integer"
      ],
      [
        "as.integer64.integer64"
      ],
      [
        "as.integer64.logical"
      ],
      [
        "as.integer64.NULL"
      ],
      [
        "as.list.integer64"
      ],
      [
        "as.logical.integer64"
      ],
      [
        "benchmark64"
      ],
      [
        "binattr"
      ],
      [
        "c.integer64"
      ],
      [
        "cache"
      ],
      [
        "cbind.integer64"
      ],
      [
        "colSums"
      ],
      [
        "diff.integer64"
      ],
      [
        "duplicated.integer64"
      ],
      [
        "format.integer64"
      ],
      [
        "getcache"
      ],
      [
        "hashcache"
      ],
      [
        "hashdup"
      ],
      [
        "hashdup.cache_integer64"
      ],
      [
        "hashfin"
      ],
      [
        "hashfin.cache_integer64"
      ],
      [
        "hashfun"
      ],
      [
        "hashfun.integer64"
      ],
      [
        "hashmap"
      ],
      [
        "hashmap.integer64"
      ],
      [
        "hashmaptab"
      ],
      [
        "hashmaptab.integer64"
      ],
      [
        "hashmapuni"
      ],
      [
        "hashmapuni.integer64"
      ],
      [
        "hashmapupo"
      ],
      [
        "hashmapupo.integer64"
      ],
      [
        "hashpos"
      ],
      [
        "hashpos.cache_integer64"
      ],
      [
        "hashrev"
      ],
      [
        "hashrev.cache_integer64"
      ],
      [
        "hashrin"
      ],
      [
        "hashrin.cache_integer64"
      ],
      [
        "hashtab"
      ],
      [
        "hashtab.cache_integer64"
      ],
      [
        "hashuni"
      ],
      [
        "hashuni.cache_integer64"
      ],
      [
        "hashupo"
      ],
      [
        "hashupo.cache_integer64"
      ],
      [
        "identical.integer64"
      ],
      [
        "integer64"
      ],
      [
        "is.double"
      ],
      [
        "is.double.default"
      ],
      [
        "is.double.integer64"
      ],
      [
        "is.finite.integer64"
      ],
      [
        "is.infinite.integer64"
      ],
      [
        "is.integer64"
      ],
      [
        "is.na.integer64"
      ],
      [
        "is.nan.integer64"
      ],
      [
        "is.sorted.integer64"
      ],
      [
        "is.vector.integer64"
      ],
      [
        "jamcache"
      ],
      [
        "keypos"
      ],
      [
        "keypos.integer64"
      ],
      [
        "length<-.integer64"
      ],
      [
        "lim.integer64"
      ],
      [
        "log.integer64"
      ],
      [
        "match"
      ],
      [
        "match.default"
      ],
      [
        "match.integer64"
      ],
      [
        "max.integer64"
      ],
      [
        "mean.integer64"
      ],
      [
        "median.integer64"
      ],
      [
        "mergeorder.integer64"
      ],
      [
        "mergesort.integer64"
      ],
      [
        "mergesortorder.integer64"
      ],
      [
        "min.integer64"
      ],
      [
        "minusclass"
      ],
      [
        "NA_integer64_"
      ],
      [
        "na.count.integer64"
      ],
      [
        "newcache"
      ],
      [
        "nties.integer64"
      ],
      [
        "nunique.integer64"
      ],
      [
        "nvalid.integer64"
      ],
      [
        "optimizer64"
      ],
      [
        "order"
      ],
      [
        "order.default"
      ],
      [
        "order.integer64"
      ],
      [
        "ordercache"
      ],
      [
        "orderdup"
      ],
      [
        "orderdup.integer64"
      ],
      [
        "orderfin"
      ],
      [
        "orderfin.integer64"
      ],
      [
        "orderkey"
      ],
      [
        "orderkey.integer64"
      ],
      [
        "ordernut"
      ],
      [
        "ordernut.integer64"
      ],
      [
        "orderpos"
      ],
      [
        "orderpos.integer64"
      ],
      [
        "orderqtl"
      ],
      [
        "orderqtl.integer64"
      ],
      [
        "orderrnk"
      ],
      [
        "orderrnk.integer64"
      ],
      [
        "ordertab"
      ],
      [
        "ordertab.integer64"
      ],
      [
        "ordertie"
      ],
      [
        "ordertie.integer64"
      ],
      [
        "orderuni"
      ],
      [
        "orderuni.integer64"
      ],
      [
        "orderupo"
      ],
      [
        "orderupo.integer64"
      ],
      [
        "plusclass"
      ],
      [
        "prank"
      ],
      [
        "prank.integer64"
      ],
      [
        "print.bitstring"
      ],
      [
        "print.cache"
      ],
      [
        "print.integer64"
      ],
      [
        "qtile"
      ],
      [
        "qtile.integer64"
      ],
      [
        "quantile.integer64"
      ],
      [
        "quickorder.integer64"
      ],
      [
        "quicksort.integer64"
      ],
      [
        "quicksortorder.integer64"
      ],
      [
        "radixorder.integer64"
      ],
      [
        "radixsort.integer64"
      ],
      [
        "radixsortorder.integer64"
      ],
      [
        "ramorder.integer64"
      ],
      [
        "ramsort.integer64"
      ],
      [
        "ramsortorder.integer64"
      ],
      [
        "rank"
      ],
      [
        "rank.default"
      ],
      [
        "rank.integer64"
      ],
      [
        "rbind.integer64"
      ],
      [
        "remcache"
      ],
      [
        "rep.integer64"
      ],
      [
        "rowSums"
      ],
      [
        "runif64"
      ],
      [
        "scale.integer64"
      ],
      [
        "seq.integer64"
      ],
      [
        "setcache"
      ],
      [
        "shellorder.integer64"
      ],
      [
        "shellsort.integer64"
      ],
      [
        "shellsortorder.integer64"
      ],
      [
        "sort.integer64"
      ],
      [
        "sortcache"
      ],
      [
        "sortfin"
      ],
      [
        "sortfin.integer64"
      ],
      [
        "sortnut"
      ],
      [
        "sortnut.integer64"
      ],
      [
        "sortordercache"
      ],
      [
        "sortorderdup"
      ],
      [
        "sortorderdup.integer64"
      ],
      [
        "sortorderkey"
      ],
      [
        "sortorderkey.integer64"
      ],
      [
        "sortorderpos"
      ],
      [
        "sortorderpos.integer64"
      ],
      [
        "sortorderrnk"
      ],
      [
        "sortorderrnk.integer64"
      ],
      [
        "sortordertab"
      ],
      [
        "sortordertab.integer64"
      ],
      [
        "sortordertie"
      ],
      [
        "sortordertie.integer64"
      ],
      [
        "sortorderuni"
      ],
      [
        "sortorderuni.integer64"
      ],
      [
        "sortorderupo"
      ],
      [
        "sortorderupo.integer64"
      ],
      [
        "sortqtl"
      ],
      [
        "sortqtl.integer64"
      ],
      [
        "sorttab"
      ],
      [
        "sorttab.integer64"
      ],
      [
        "sortuni"
      ],
      [
        "sortuni.integer64"
      ],
      [
        "str.integer64"
      ],
      [
        "sum.integer64"
      ],
      [
        "summary.integer64"
      ],
      [
        "table.integer64"
      ],
      [
        "tiepos"
      ],
      [
        "tiepos.integer64"
      ],
      [
        "unipos"
      ],
      [
        "unipos.integer64"
      ],
      [
        "unique.integer64"
      ],
      [
        "xor.integer64"
      ]
    ],
    "topics": [],
    "score": 15.3512,
    "stars": 38,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "bit64 A S3 Class for Vectors of 64bit Integers Package 'bit64' provides serializable S3 atomic 64bit\n(signed) integers. These are useful for handling database keys\nand exact counting in +-2^63. WARNING: do not use them as\nreplacement for 32bit integers, integer64 are not supported for\nsubscripting by R-core and they have different semantics when\ncombined with double, e.g. integer64 + double => integer64.\nClass integer64 can be used in vectors, matrices, arrays and\ndata.frames. Methods are available for coercion from and to\nlogicals, integers, doubles, characters and factors as well as\nmany elementwise and summary functions. Many fast algorithmic\noperations such as 'match' and 'order' support inter- active\ndata exploration and manipulation and optionally leverage\ncaching. : :.default :.integer64 [.integer64 [[.integer64 [[<-.integer64 [<-.integer64 %in% %in%.default %in%.integer64 abs.integer64 all.equal.integer64 all.integer64 any.integer64 as.bitstring as.bitstring.integer64 as.character.integer64 as.data.frame.integer64 as.double.integer64 as.integer.integer64 as.integer64 as.integer64.bitstring as.integer64.character as.integer64.double as.integer64.factor as.integer64.integer as.integer64.integer64 as.integer64.logical as.integer64.NULL as.list.integer64 as.logical.integer64 benchmark64 binattr c.integer64 cache cbind.integer64 colSums diff.integer64 duplicated.integer64 format.integer64 getcache hashcache hashdup hashdup.cache_integer64 hashfin hashfin.cache_integer64 hashfun hashfun.integer64 hashmap hashmap.integer64 hashmaptab hashmaptab.integer64 hashmapuni hashmapuni.integer64 hashmapupo hashmapupo.integer64 hashpos hashpos.cache_integer64 hashrev hashrev.cache_integer64 hashrin hashrin.cache_integer64 hashtab hashtab.cache_integer64 hashuni hashuni.cache_integer64 hashupo hashupo.cache_integer64 identical.integer64 integer64 is.double is.double.default is.double.integer64 is.finite.integer64 is.infinite.integer64 is.integer64 is.na.integer64 is.nan.integer64 is.sorted.integer64 is.vector.integer64 jamcache keypos keypos.integer64 length<-.integer64 lim.integer64 log.integer64 match match.default match.integer64 max.integer64 mean.integer64 median.integer64 mergeorder.integer64 mergesort.integer64 mergesortorder.integer64 min.integer64 minusclass NA_integer64_ na.count.integer64 newcache nties.integer64 nunique.integer64 nvalid.integer64 optimizer64 order order.default order.integer64 ordercache orderdup orderdup.integer64 orderfin orderfin.integer64 orderkey orderkey.integer64 ordernut ordernut.integer64 orderpos orderpos.integer64 orderqtl orderqtl.integer64 orderrnk orderrnk.integer64 ordertab ordertab.integer64 ordertie ordertie.integer64 orderuni orderuni.integer64 orderupo orderupo.integer64 plusclass prank prank.integer64 print.bitstring print.cache print.integer64 qtile qtile.integer64 quantile.integer64 quickorder.integer64 quicksort.integer64 quicksortorder.integer64 radixorder.integer64 radixsort.integer64 radixsortorder.integer64 ramorder.integer64 ramsort.integer64 ramsortorder.integer64 rank rank.default rank.integer64 rbind.integer64 remcache rep.integer64 rowSums runif64 scale.integer64 seq.integer64 setcache shellorder.integer64 shellsort.integer64 shellsortorder.integer64 sort.integer64 sortcache sortfin sortfin.integer64 sortnut sortnut.integer64 sortordercache sortorderdup sortorderdup.integer64 sortorderkey sortorderkey.integer64 sortorderpos sortorderpos.integer64 sortorderrnk sortorderrnk.integer64 sortordertab sortordertab.integer64 sortordertie sortordertie.integer64 sortorderuni sortorderuni.integer64 sortorderupo sortorderupo.integer64 sortqtl sortqtl.integer64 sorttab sorttab.integer64 sortuni sortuni.integer64 str.integer64 sum.integer64 summary.integer64 table.integer64 tiepos tiepos.integer64 unipos unipos.integer64 unique.integer64 xor.integer64 "
  },
  {
    "id": 753,
    "package_name": "learnr",
    "title": "Interactive Tutorials for R",
    "description": "Create interactive tutorials using R Markdown. Use a\ncombination of narrative, figures, videos, exercises, and\nquizzes to create self-paced tutorials for learning about R and\nR packages.",
    "version": "0.11.6.9000",
    "maintainer": "Garrick Aden-Buie <garrick@posit.co>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7111-0077>),\nBarret Schloerke [aut] (ORCID: <https://orcid.org/0000-0001-9986-114X>),\nJJ Allaire [aut, ccp],\nAlexander Rossell Hayes [aut] (ORCID:\n<https://orcid.org/0000-0001-9412-0457>),\nNischal Shrestha [ctb] (ORCID: <https://orcid.org/0000-0003-3321-1712>),\nAngela Li [ctb] (vignette),\nPosit, PBC [cph, fnd],\nAjax.org B.V. [ctb, cph] (Ace library),\nZeno Rocha [ctb, cph] (clipboard.js library),\nNick Payne [ctb, cph] (Bootbox library),\nJake Archibald [ctb, cph] (idb-keyval library),\ni18next authors [ctb, cph] (i18next library)",
    "url": "https://rstudio.github.io/learnr/,\nhttps://github.com/rstudio/learnr",
    "bug_reports": "https://github.com/rstudio/learnr/issues",
    "repository": "",
    "exports": [
      [
        "answer"
      ],
      [
        "answer_fn"
      ],
      [
        "available_tutorials"
      ],
      [
        "correct"
      ],
      [
        "disable_all_tags"
      ],
      [
        "duplicate_env"
      ],
      [
        "event_register_handler"
      ],
      [
        "external_evaluator"
      ],
      [
        "filesystem_storage"
      ],
      [
        "finalize_question"
      ],
      [
        "get_tutorial_info"
      ],
      [
        "get_tutorial_state"
      ],
      [
        "incorrect"
      ],
      [
        "initialize_tutorial"
      ],
      [
        "mark_as"
      ],
      [
        "mock_chunk"
      ],
      [
        "mock_exercise"
      ],
      [
        "one_time"
      ],
      [
        "question"
      ],
      [
        "question_checkbox"
      ],
      [
        "question_is_correct"
      ],
      [
        "question_is_valid"
      ],
      [
        "question_numeric"
      ],
      [
        "question_radio"
      ],
      [
        "question_text"
      ],
      [
        "question_ui_completed"
      ],
      [
        "question_ui_initialize"
      ],
      [
        "question_ui_try_again"
      ],
      [
        "quiz"
      ],
      [
        "random_encouragement"
      ],
      [
        "random_phrases_add"
      ],
      [
        "random_praise"
      ],
      [
        "run_tutorial"
      ],
      [
        "safe"
      ],
      [
        "safe_env"
      ],
      [
        "tutorial"
      ],
      [
        "tutorial_html_dependency"
      ],
      [
        "tutorial_options"
      ],
      [
        "tutorial_package_dependencies"
      ]
    ],
    "topics": [
      [
        "interactive"
      ],
      [
        "python"
      ],
      [
        "rmarkdown"
      ],
      [
        "shiny"
      ],
      [
        "sql"
      ],
      [
        "teaching"
      ],
      [
        "tutorial"
      ]
    ],
    "score": 15.0078,
    "stars": 726,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "learnr Interactive Tutorials for R Create interactive tutorials using R Markdown. Use a\ncombination of narrative, figures, videos, exercises, and\nquizzes to create self-paced tutorials for learning about R and\nR packages. answer answer_fn available_tutorials correct disable_all_tags duplicate_env event_register_handler external_evaluator filesystem_storage finalize_question get_tutorial_info get_tutorial_state incorrect initialize_tutorial mark_as mock_chunk mock_exercise one_time question question_checkbox question_is_correct question_is_valid question_numeric question_radio question_text question_ui_completed question_ui_initialize question_ui_try_again quiz random_encouragement random_phrases_add random_praise run_tutorial safe safe_env tutorial tutorial_html_dependency tutorial_options tutorial_package_dependencies interactive python rmarkdown shiny sql teaching tutorial"
  },
  {
    "id": 1085,
    "package_name": "rentrez",
    "title": "'Entrez' in R",
    "description": "Provides an R interface to the NCBI's 'EUtils' API,\nallowing users to search databases like 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and 'PubMed'\n<https://pubmed.ncbi.nlm.nih.gov/>, process the results of\nthose searches and pull data into their R sessions.",
    "version": "1.2.4",
    "maintainer": "Indraneel Chakraborty <hello.indraneel@gmail.com>",
    "author": "David Winter [aut] (ORCID: <https://orcid.org/0000-0002-6165-0029>),\nScott Chamberlain [ctb] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nHan Guangchun [ctb] (ORCID: <https://orcid.org/0000-0001-9277-2507>),\nIndraneel Chakraborty [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6958-8269>)",
    "url": "https://github.com/ropensci/rentrez/",
    "bug_reports": "https://github.com/ropensci/rentrez/issues/",
    "repository": "",
    "exports": [
      [
        "entrez_citmatch"
      ],
      [
        "entrez_db_links"
      ],
      [
        "entrez_db_searchable"
      ],
      [
        "entrez_db_summary"
      ],
      [
        "entrez_dbs"
      ],
      [
        "entrez_fetch"
      ],
      [
        "entrez_global_query"
      ],
      [
        "entrez_info"
      ],
      [
        "entrez_link"
      ],
      [
        "entrez_post"
      ],
      [
        "entrez_search"
      ],
      [
        "entrez_summary"
      ],
      [
        "extract_from_esummary"
      ],
      [
        "linkout_urls"
      ],
      [
        "parse_pubmed_xml"
      ],
      [
        "set_entrez_key"
      ]
    ],
    "topics": [],
    "score": 14.6938,
    "stars": 212,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rentrez 'Entrez' in R Provides an R interface to the NCBI's 'EUtils' API,\nallowing users to search databases like 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and 'PubMed'\n<https://pubmed.ncbi.nlm.nih.gov/>, process the results of\nthose searches and pull data into their R sessions. entrez_citmatch entrez_db_links entrez_db_searchable entrez_db_summary entrez_dbs entrez_fetch entrez_global_query entrez_info entrez_link entrez_post entrez_search entrez_summary extract_from_esummary linkout_urls parse_pubmed_xml set_entrez_key "
  },
  {
    "id": 499,
    "package_name": "duckdb",
    "title": "DBI Package for the DuckDB Database Management System",
    "description": "The DuckDB project is an embedded analytical data\nmanagement system with support for the Structured Query\nLanguage (SQL). This package includes all of DuckDB and an R\nDatabase Interface (DBI) connector.",
    "version": "1.4.2.9006",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Hannes M\u00fchleisen [aut] (ORCID: <https://orcid.org/0000-0001-8552-0029>),\nMark Raasveldt [aut] (ORCID: <https://orcid.org/0000-0001-5005-6844>),\nKirill M\u00fcller [cre] (ORCID: <https://orcid.org/0000-0002-1416-3412>),\nStichting DuckDB Foundation [cph],\nApache Software Foundation [cph],\nPostgreSQL Global Development Group [cph],\nThe Regents of the University of California [cph],\nCameron Desrochers [cph],\nVictor Zverovich [cph],\nRAD Game Tools [cph],\nValve Software [cph],\nRich Geldreich [cph],\nTenacious Software LLC [cph],\nThe RE2 Authors [cph],\nGoogle Inc. [cph],\nFacebook Inc. [cph],\nSteven G. Johnson [cph],\nJiahao Chen [cph],\nTony Kelman [cph],\nJonas Fonseca [cph],\nLukas Fittl [cph],\nSalvatore Sanfilippo [cph],\nArt.sy, Inc. [cph],\nOran Agra [cph],\nRedis Labs, Inc. [cph],\nMelissa O'Neill [cph],\nPCG Project contributors [cph]",
    "url": "https://r.duckdb.org/, https://github.com/duckdb/duckdb-r",
    "bug_reports": "https://github.com/duckdb/duckdb-r/issues",
    "repository": "",
    "exports": [
      [
        "dbAppendTable"
      ],
      [
        "dbBegin"
      ],
      [
        "dbBind"
      ],
      [
        "dbClearResult"
      ],
      [
        "dbColumnInfo"
      ],
      [
        "dbCommit"
      ],
      [
        "dbConnect"
      ],
      [
        "dbDataType"
      ],
      [
        "dbDisconnect"
      ],
      [
        "dbExistsTable"
      ],
      [
        "dbFetch"
      ],
      [
        "dbGetInfo"
      ],
      [
        "dbGetRowCount"
      ],
      [
        "dbGetRowsAffected"
      ],
      [
        "dbGetStatement"
      ],
      [
        "dbHasCompleted"
      ],
      [
        "dbIsValid"
      ],
      [
        "dbListFields"
      ],
      [
        "dbListTables"
      ],
      [
        "dbQuoteIdentifier"
      ],
      [
        "dbQuoteLiteral"
      ],
      [
        "dbRemoveTable"
      ],
      [
        "dbRollback"
      ],
      [
        "dbSendQuery"
      ],
      [
        "dbWriteTable"
      ],
      [
        "default_conn"
      ],
      [
        "duckdb"
      ],
      [
        "duckdb_adbc"
      ],
      [
        "duckdb_fetch_arrow"
      ],
      [
        "duckdb_fetch_record_batch"
      ],
      [
        "duckdb_list_arrow"
      ],
      [
        "duckdb_read_csv"
      ],
      [
        "duckdb_register"
      ],
      [
        "duckdb_register_arrow"
      ],
      [
        "duckdb_shutdown"
      ],
      [
        "duckdb_unregister"
      ],
      [
        "duckdb_unregister_arrow"
      ],
      [
        "read_csv_duckdb"
      ],
      [
        "show"
      ],
      [
        "simulate_duckdb"
      ],
      [
        "sql_exec"
      ],
      [
        "sql_query"
      ],
      [
        "tbl_file"
      ],
      [
        "tbl_function"
      ],
      [
        "tbl_query"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "duckdb"
      ],
      [
        "olap"
      ],
      [
        "cpp"
      ]
    ],
    "score": 14.2642,
    "stars": 197,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "duckdb DBI Package for the DuckDB Database Management System The DuckDB project is an embedded analytical data\nmanagement system with support for the Structured Query\nLanguage (SQL). This package includes all of DuckDB and an R\nDatabase Interface (DBI) connector. dbAppendTable dbBegin dbBind dbClearResult dbColumnInfo dbCommit dbConnect dbDataType dbDisconnect dbExistsTable dbFetch dbGetInfo dbGetRowCount dbGetRowsAffected dbGetStatement dbHasCompleted dbIsValid dbListFields dbListTables dbQuoteIdentifier dbQuoteLiteral dbRemoveTable dbRollback dbSendQuery dbWriteTable default_conn duckdb duckdb_adbc duckdb_fetch_arrow duckdb_fetch_record_batch duckdb_list_arrow duckdb_read_csv duckdb_register duckdb_register_arrow duckdb_shutdown duckdb_unregister duckdb_unregister_arrow read_csv_duckdb show simulate_duckdb sql_exec sql_query tbl_file tbl_function tbl_query database duckdb olap cpp"
  },
  {
    "id": 302,
    "package_name": "blob",
    "title": "A Simple S3 Class for Representing Vectors of Binary Data\n('BLOBS')",
    "description": "R's raw vector is useful for storing a single binary\nobject. What if you want to put a vector of them in a data\nframe? The 'blob' package provides the blob object, a list of\nraw vectors, suitable for use as a column in data frame.",
    "version": "1.2.4.9019",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Hadley Wickham [aut],\nKirill M\u00fcller [cre],\nRStudio [cph, fnd]",
    "url": "https://blob.tidyverse.org, https://github.com/tidyverse/blob",
    "bug_reports": "https://github.com/tidyverse/blob/issues",
    "repository": "",
    "exports": [
      [
        "as_blob"
      ],
      [
        "as.blob"
      ],
      [
        "blob"
      ],
      [
        "is_blob"
      ],
      [
        "new_blob"
      ],
      [
        "validate_blob"
      ],
      [
        "vec_cast.blob"
      ],
      [
        "vec_ptype2.blob"
      ]
    ],
    "topics": [
      [
        "database"
      ]
    ],
    "score": 13.6862,
    "stars": 47,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "blob A Simple S3 Class for Representing Vectors of Binary Data\n('BLOBS') R's raw vector is useful for storing a single binary\nobject. What if you want to put a vector of them in a data\nframe? The 'blob' package provides the blob object, a list of\nraw vectors, suitable for use as a column in data frame. as_blob as.blob blob is_blob new_blob validate_blob vec_cast.blob vec_ptype2.blob database"
  },
  {
    "id": 1103,
    "package_name": "rgbif",
    "title": "Interface to the Global Biodiversity Information Facility API",
    "description": "A programmatic interface to the Web Service methods\nprovided by the Global Biodiversity Information Facility (GBIF;\n<https://www.gbif.org/developer/summary>). GBIF is a database\nof species occurrence records from sources all over the globe.\nrgbif includes functions for searching for taxonomic names,\nretrieving information on data providers, getting species\noccurrence records, getting counts of occurrence records, and\nusing the GBIF tile map service to make rasters summarizing\nhuge amounts of data.",
    "version": "3.8.4.2",
    "maintainer": "John Waller <jwaller@gbif.org>",
    "author": "Scott Chamberlain [aut] (0000-0003-1444-9135),\nDamiano Oldoni [aut] (0000-0003-3445-7562),\nVijay Barve [ctb] (0000-0002-4852-2567),\nPeter Desmet [ctb] (0000-0002-8442-8025),\nLaurens Geffert [ctb],\nDan Mcglinn [ctb] (0000-0003-2359-3526),\nKarthik Ram [ctb] (0000-0002-0233-1757),\nrOpenSci [fnd] (019jywm96),\nJohn Waller [aut, cre] (0000-0002-7302-5976)",
    "url": "https://github.com/ropensci/rgbif (devel),\nhttps://docs.ropensci.org/rgbif/ (documentation)",
    "bug_reports": "https://github.com/ropensci/rgbif/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "as.download"
      ],
      [
        "blanktheme"
      ],
      [
        "check_wkt"
      ],
      [
        "collection_export"
      ],
      [
        "collection_search"
      ],
      [
        "count_facet"
      ],
      [
        "dataset"
      ],
      [
        "dataset_comment"
      ],
      [
        "dataset_constituents"
      ],
      [
        "dataset_contact"
      ],
      [
        "dataset_doi"
      ],
      [
        "dataset_duplicate"
      ],
      [
        "dataset_endpoint"
      ],
      [
        "dataset_export"
      ],
      [
        "dataset_get"
      ],
      [
        "dataset_gridded"
      ],
      [
        "dataset_identifier"
      ],
      [
        "dataset_machinetag"
      ],
      [
        "dataset_metrics"
      ],
      [
        "dataset_networks"
      ],
      [
        "dataset_noendpoint"
      ],
      [
        "dataset_process"
      ],
      [
        "dataset_search"
      ],
      [
        "dataset_suggest"
      ],
      [
        "dataset_tag"
      ],
      [
        "datasets"
      ],
      [
        "derived_dataset"
      ],
      [
        "derived_dataset_prep"
      ],
      [
        "DownReq"
      ],
      [
        "elevation"
      ],
      [
        "enumeration"
      ],
      [
        "enumeration_country"
      ],
      [
        "gbif_bbox2wkt"
      ],
      [
        "gbif_citation"
      ],
      [
        "gbif_geocode"
      ],
      [
        "gbif_issues"
      ],
      [
        "gbif_issues_lookup"
      ],
      [
        "gbif_names"
      ],
      [
        "gbif_oai_get_records"
      ],
      [
        "gbif_oai_identify"
      ],
      [
        "gbif_oai_list_identifiers"
      ],
      [
        "gbif_oai_list_metadataformats"
      ],
      [
        "gbif_oai_list_records"
      ],
      [
        "gbif_oai_list_sets"
      ],
      [
        "gbif_photos"
      ],
      [
        "gbif_wkt2bbox"
      ],
      [
        "GbifQueue"
      ],
      [
        "installation_comment"
      ],
      [
        "installation_contact"
      ],
      [
        "installation_dataset"
      ],
      [
        "installation_endpoint"
      ],
      [
        "installation_identifier"
      ],
      [
        "installation_machinetag"
      ],
      [
        "installation_search"
      ],
      [
        "installation_tag"
      ],
      [
        "installations"
      ],
      [
        "institution_export"
      ],
      [
        "institution_search"
      ],
      [
        "lit_count"
      ],
      [
        "lit_export"
      ],
      [
        "lit_search"
      ],
      [
        "map_fetch"
      ],
      [
        "mvt_fetch"
      ],
      [
        "name_backbone"
      ],
      [
        "name_backbone_checklist"
      ],
      [
        "name_backbone_verbose"
      ],
      [
        "name_issues"
      ],
      [
        "name_lookup"
      ],
      [
        "name_parse"
      ],
      [
        "name_suggest"
      ],
      [
        "name_usage"
      ],
      [
        "network"
      ],
      [
        "network_constituents"
      ],
      [
        "networks"
      ],
      [
        "nodes"
      ],
      [
        "occ_count"
      ],
      [
        "occ_count_basis_of_record"
      ],
      [
        "occ_count_country"
      ],
      [
        "occ_count_pub_country"
      ],
      [
        "occ_count_year"
      ],
      [
        "occ_data"
      ],
      [
        "occ_download"
      ],
      [
        "occ_download_cached"
      ],
      [
        "occ_download_cancel"
      ],
      [
        "occ_download_cancel_staged"
      ],
      [
        "occ_download_dataset_activity"
      ],
      [
        "occ_download_datasets"
      ],
      [
        "occ_download_describe"
      ],
      [
        "occ_download_doi"
      ],
      [
        "occ_download_get"
      ],
      [
        "occ_download_import"
      ],
      [
        "occ_download_list"
      ],
      [
        "occ_download_meta"
      ],
      [
        "occ_download_prep"
      ],
      [
        "occ_download_queue"
      ],
      [
        "occ_download_sql"
      ],
      [
        "occ_download_sql_prep"
      ],
      [
        "occ_download_sql_validate"
      ],
      [
        "occ_download_wait"
      ],
      [
        "occ_facet"
      ],
      [
        "occ_get"
      ],
      [
        "occ_get_verbatim"
      ],
      [
        "occ_issues"
      ],
      [
        "occ_metadata"
      ],
      [
        "occ_search"
      ],
      [
        "occ_spellcheck"
      ],
      [
        "organizations"
      ],
      [
        "parsenames"
      ],
      [
        "pred"
      ],
      [
        "pred_and"
      ],
      [
        "pred_default"
      ],
      [
        "pred_gt"
      ],
      [
        "pred_gte"
      ],
      [
        "pred_in"
      ],
      [
        "pred_isnull"
      ],
      [
        "pred_like"
      ],
      [
        "pred_lt"
      ],
      [
        "pred_lte"
      ],
      [
        "pred_not"
      ],
      [
        "pred_notnull"
      ],
      [
        "pred_or"
      ],
      [
        "pred_within"
      ],
      [
        "rgb_country_codes"
      ],
      [
        "suggestfields"
      ],
      [
        "taxrank"
      ],
      [
        "wkt_parse"
      ]
    ],
    "topics": [
      [
        "gbif"
      ],
      [
        "specimens"
      ],
      [
        "api"
      ],
      [
        "web-services"
      ],
      [
        "occurrences"
      ],
      [
        "species"
      ],
      [
        "taxonomy"
      ],
      [
        "biodiversity"
      ],
      [
        "data"
      ],
      [
        "lifewatch"
      ],
      [
        "oscibio"
      ],
      [
        "spocc"
      ]
    ],
    "score": 13.6363,
    "stars": 171,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rgbif Interface to the Global Biodiversity Information Facility API A programmatic interface to the Web Service methods\nprovided by the Global Biodiversity Information Facility (GBIF;\n<https://www.gbif.org/developer/summary>). GBIF is a database\nof species occurrence records from sources all over the globe.\nrgbif includes functions for searching for taxonomic names,\nretrieving information on data providers, getting species\noccurrence records, getting counts of occurrence records, and\nusing the GBIF tile map service to make rasters summarizing\nhuge amounts of data. %>% as.download blanktheme check_wkt collection_export collection_search count_facet dataset dataset_comment dataset_constituents dataset_contact dataset_doi dataset_duplicate dataset_endpoint dataset_export dataset_get dataset_gridded dataset_identifier dataset_machinetag dataset_metrics dataset_networks dataset_noendpoint dataset_process dataset_search dataset_suggest dataset_tag datasets derived_dataset derived_dataset_prep DownReq elevation enumeration enumeration_country gbif_bbox2wkt gbif_citation gbif_geocode gbif_issues gbif_issues_lookup gbif_names gbif_oai_get_records gbif_oai_identify gbif_oai_list_identifiers gbif_oai_list_metadataformats gbif_oai_list_records gbif_oai_list_sets gbif_photos gbif_wkt2bbox GbifQueue installation_comment installation_contact installation_dataset installation_endpoint installation_identifier installation_machinetag installation_search installation_tag installations institution_export institution_search lit_count lit_export lit_search map_fetch mvt_fetch name_backbone name_backbone_checklist name_backbone_verbose name_issues name_lookup name_parse name_suggest name_usage network network_constituents networks nodes occ_count occ_count_basis_of_record occ_count_country occ_count_pub_country occ_count_year occ_data occ_download occ_download_cached occ_download_cancel occ_download_cancel_staged occ_download_dataset_activity occ_download_datasets occ_download_describe occ_download_doi occ_download_get occ_download_import occ_download_list occ_download_meta occ_download_prep occ_download_queue occ_download_sql occ_download_sql_prep occ_download_sql_validate occ_download_wait occ_facet occ_get occ_get_verbatim occ_issues occ_metadata occ_search occ_spellcheck organizations parsenames pred pred_and pred_default pred_gt pred_gte pred_in pred_isnull pred_like pred_lt pred_lte pred_not pred_notnull pred_or pred_within rgb_country_codes suggestfields taxrank wkt_parse gbif specimens api web-services occurrences species taxonomy biodiversity data lifewatch oscibio spocc"
  },
  {
    "id": 1306,
    "package_name": "taxize",
    "title": "Taxonomic Information from Around the Web",
    "description": "Interacts with a suite of web application programming\ninterfaces (API) for taxonomic tasks, such as getting database\nspecific taxonomic identifiers, verifying species names,\ngetting taxonomic hierarchies, fetching downstream and upstream\ntaxonomic names, getting taxonomic synonyms, converting\nscientific to common names and vice versa, and more. Some of\nthe services supported include 'NCBI E-utilities'\n(<https://www.ncbi.nlm.nih.gov/books/NBK25501/>), 'Encyclopedia\nof Life' (<https://eol.org/docs/what-is-eol/data-services>),\n'Global Biodiversity Information Facility'\n(<https://techdocs.gbif.org/en/openapi/>), and many more. Links\nto the API documentation for other supported services are\navailable in the documentation for their respective functions\nin this package.",
    "version": "0.10.0",
    "maintainer": "Zachary Foster <zacharyfoster1989@gmail.com>",
    "author": "Scott Chamberlain [aut],\nEduard Szoecs [aut],\nZachary Foster [aut, cre],\nZebulun Arendsee [aut],\nCarl Boettiger [ctb],\nKarthik Ram [ctb],\nIgnasi Bartomeus [ctb],\nJohn Baumgartner [ctb],\nJames O'Donnell [ctb],\nJari Oksanen [ctb],\nBastian Greshake Tzovaras [ctb],\nPhilippe Marchand [ctb],\nVinh Tran [ctb],\nMa\u00eblle Salmon [ctb],\nGaopeng Li [ctb],\nMatthias Greni\u00e9 [ctb],\nrOpenSci [fnd]",
    "url": "https://docs.ropensci.org/taxize/ (website),\nhttps://github.com/ropensci/taxize (devel)",
    "bug_reports": "https://github.com/ropensci/taxize/issues",
    "repository": "",
    "exports": [
      [
        "apg_lookup"
      ],
      [
        "apgFamilies"
      ],
      [
        "apgOrders"
      ],
      [
        "as.boldid"
      ],
      [
        "as.colid"
      ],
      [
        "as.eolid"
      ],
      [
        "as.gbifid"
      ],
      [
        "as.iucn"
      ],
      [
        "as.natservid"
      ],
      [
        "as.nbnid"
      ],
      [
        "as.pow"
      ],
      [
        "as.tolid"
      ],
      [
        "as.tpsid"
      ],
      [
        "as.tsn"
      ],
      [
        "as.ubioid"
      ],
      [
        "as.uid"
      ],
      [
        "as.wiki"
      ],
      [
        "as.wormsid"
      ],
      [
        "bold_children"
      ],
      [
        "bold_downstream"
      ],
      [
        "bold_ping"
      ],
      [
        "bold_search"
      ],
      [
        "children"
      ],
      [
        "class2tree"
      ],
      [
        "classification"
      ],
      [
        "col_children"
      ],
      [
        "col_classification"
      ],
      [
        "col_downstream"
      ],
      [
        "col_ping"
      ],
      [
        "col_search"
      ],
      [
        "comm2sci"
      ],
      [
        "downstream"
      ],
      [
        "eol_dataobjects"
      ],
      [
        "eol_hierarchy"
      ],
      [
        "eol_invasive"
      ],
      [
        "eol_pages"
      ],
      [
        "eol_ping"
      ],
      [
        "eol_search"
      ],
      [
        "eubon"
      ],
      [
        "eubon_capabilities"
      ],
      [
        "eubon_children"
      ],
      [
        "eubon_hierarchy"
      ],
      [
        "eubon_search"
      ],
      [
        "fg_all_updated_names"
      ],
      [
        "fg_author_search"
      ],
      [
        "fg_deprecated_names"
      ],
      [
        "fg_epithet_search"
      ],
      [
        "fg_name_by_key"
      ],
      [
        "fg_name_full_by_lsid"
      ],
      [
        "fg_name_search"
      ],
      [
        "fg_ping"
      ],
      [
        "gbif_downstream"
      ],
      [
        "gbif_name_usage"
      ],
      [
        "gbif_parse"
      ],
      [
        "gbif_ping"
      ],
      [
        "genbank2uid"
      ],
      [
        "get_boldid"
      ],
      [
        "get_boldid_"
      ],
      [
        "get_colid"
      ],
      [
        "get_colid_"
      ],
      [
        "get_eolid"
      ],
      [
        "get_eolid_"
      ],
      [
        "get_gbifid"
      ],
      [
        "get_gbifid_"
      ],
      [
        "get_genes"
      ],
      [
        "get_genes_avail"
      ],
      [
        "get_ids"
      ],
      [
        "get_ids_"
      ],
      [
        "get_iucn"
      ],
      [
        "get_natservid"
      ],
      [
        "get_natservid_"
      ],
      [
        "get_nbnid"
      ],
      [
        "get_nbnid_"
      ],
      [
        "get_pow"
      ],
      [
        "get_pow_"
      ],
      [
        "get_seqs"
      ],
      [
        "get_tolid"
      ],
      [
        "get_tolid_"
      ],
      [
        "get_tpsid"
      ],
      [
        "get_tpsid_"
      ],
      [
        "get_tsn"
      ],
      [
        "get_tsn_"
      ],
      [
        "get_ubioid"
      ],
      [
        "get_ubioid_"
      ],
      [
        "get_uid"
      ],
      [
        "get_uid_"
      ],
      [
        "get_wiki"
      ],
      [
        "get_wiki_"
      ],
      [
        "get_wormsid"
      ],
      [
        "get_wormsid_"
      ],
      [
        "getkey"
      ],
      [
        "gisd_isinvasive"
      ],
      [
        "gna_data_sources"
      ],
      [
        "gna_parse"
      ],
      [
        "gna_search"
      ],
      [
        "gna_verifier"
      ],
      [
        "gni_details"
      ],
      [
        "gni_parse"
      ],
      [
        "gni_seach"
      ],
      [
        "gnr_datasources"
      ],
      [
        "gnr_resolve"
      ],
      [
        "id2name"
      ],
      [
        "ion"
      ],
      [
        "iplant_resolve"
      ],
      [
        "ipni_ping"
      ],
      [
        "ipni_search"
      ],
      [
        "itis_acceptname"
      ],
      [
        "itis_downstream"
      ],
      [
        "itis_getrecord"
      ],
      [
        "itis_hierarchy"
      ],
      [
        "itis_kingdomnames"
      ],
      [
        "itis_lsid"
      ],
      [
        "itis_name"
      ],
      [
        "itis_native"
      ],
      [
        "itis_ping"
      ],
      [
        "itis_refs"
      ],
      [
        "itis_taxrank"
      ],
      [
        "itis_terms"
      ],
      [
        "iucn_getname"
      ],
      [
        "iucn_id"
      ],
      [
        "iucn_status"
      ],
      [
        "iucn_summary"
      ],
      [
        "lowest_common"
      ],
      [
        "names_list"
      ],
      [
        "nbn_classification"
      ],
      [
        "nbn_ping"
      ],
      [
        "nbn_search"
      ],
      [
        "nbn_synonyms"
      ],
      [
        "ncbi_children"
      ],
      [
        "ncbi_downstream"
      ],
      [
        "ncbi_get_taxon_summary"
      ],
      [
        "ncbi_getbyid"
      ],
      [
        "ncbi_getbyname"
      ],
      [
        "ncbi_ping"
      ],
      [
        "ncbi_search"
      ],
      [
        "phylomatic_format"
      ],
      [
        "phylomatic_tree"
      ],
      [
        "plantminer"
      ],
      [
        "pow_lookup"
      ],
      [
        "pow_search"
      ],
      [
        "pow_synonyms"
      ],
      [
        "rankagg"
      ],
      [
        "resolve"
      ],
      [
        "sci2comm"
      ],
      [
        "scrapenames"
      ],
      [
        "status_codes"
      ],
      [
        "synonyms"
      ],
      [
        "synonyms_df"
      ],
      [
        "tax_agg"
      ],
      [
        "tax_name"
      ],
      [
        "tax_rank"
      ],
      [
        "taxize_capwords"
      ],
      [
        "taxize_cite"
      ],
      [
        "taxize_ldfast"
      ],
      [
        "taxize_options"
      ],
      [
        "taxon_clear"
      ],
      [
        "taxon_last"
      ],
      [
        "tnrs"
      ],
      [
        "tnrs_sources"
      ],
      [
        "tol_resolve"
      ],
      [
        "tp_acceptednames"
      ],
      [
        "tp_accnames"
      ],
      [
        "tp_classification"
      ],
      [
        "tp_dist"
      ],
      [
        "tp_namedistributions"
      ],
      [
        "tp_namereferences"
      ],
      [
        "tp_refs"
      ],
      [
        "tp_search"
      ],
      [
        "tp_summary"
      ],
      [
        "tp_synonyms"
      ],
      [
        "tpl_families"
      ],
      [
        "tpl_get"
      ],
      [
        "tpl_search"
      ],
      [
        "tropicos_ping"
      ],
      [
        "ubio_classification"
      ],
      [
        "ubio_classification_search"
      ],
      [
        "ubio_id"
      ],
      [
        "ubio_ping"
      ],
      [
        "ubio_search"
      ],
      [
        "ubio_synonyms"
      ],
      [
        "upstream"
      ],
      [
        "use_entrez"
      ],
      [
        "use_eol"
      ],
      [
        "use_iucn"
      ],
      [
        "use_tropicos"
      ],
      [
        "vascan_ping"
      ],
      [
        "vascan_search"
      ],
      [
        "worms_downstream"
      ]
    ],
    "topics": [
      [
        "taxonomy"
      ],
      [
        "biology"
      ],
      [
        "nomenclature"
      ],
      [
        "json"
      ],
      [
        "api"
      ],
      [
        "web"
      ],
      [
        "api-client"
      ],
      [
        "identifiers"
      ],
      [
        "species"
      ],
      [
        "names"
      ],
      [
        "api-wrapper"
      ],
      [
        "biodiversity"
      ],
      [
        "darwincore"
      ],
      [
        "data"
      ],
      [
        "taxize"
      ]
    ],
    "score": 13.5332,
    "stars": 293,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxize Taxonomic Information from Around the Web Interacts with a suite of web application programming\ninterfaces (API) for taxonomic tasks, such as getting database\nspecific taxonomic identifiers, verifying species names,\ngetting taxonomic hierarchies, fetching downstream and upstream\ntaxonomic names, getting taxonomic synonyms, converting\nscientific to common names and vice versa, and more. Some of\nthe services supported include 'NCBI E-utilities'\n(<https://www.ncbi.nlm.nih.gov/books/NBK25501/>), 'Encyclopedia\nof Life' (<https://eol.org/docs/what-is-eol/data-services>),\n'Global Biodiversity Information Facility'\n(<https://techdocs.gbif.org/en/openapi/>), and many more. Links\nto the API documentation for other supported services are\navailable in the documentation for their respective functions\nin this package. apg_lookup apgFamilies apgOrders as.boldid as.colid as.eolid as.gbifid as.iucn as.natservid as.nbnid as.pow as.tolid as.tpsid as.tsn as.ubioid as.uid as.wiki as.wormsid bold_children bold_downstream bold_ping bold_search children class2tree classification col_children col_classification col_downstream col_ping col_search comm2sci downstream eol_dataobjects eol_hierarchy eol_invasive eol_pages eol_ping eol_search eubon eubon_capabilities eubon_children eubon_hierarchy eubon_search fg_all_updated_names fg_author_search fg_deprecated_names fg_epithet_search fg_name_by_key fg_name_full_by_lsid fg_name_search fg_ping gbif_downstream gbif_name_usage gbif_parse gbif_ping genbank2uid get_boldid get_boldid_ get_colid get_colid_ get_eolid get_eolid_ get_gbifid get_gbifid_ get_genes get_genes_avail get_ids get_ids_ get_iucn get_natservid get_natservid_ get_nbnid get_nbnid_ get_pow get_pow_ get_seqs get_tolid get_tolid_ get_tpsid get_tpsid_ get_tsn get_tsn_ get_ubioid get_ubioid_ get_uid get_uid_ get_wiki get_wiki_ get_wormsid get_wormsid_ getkey gisd_isinvasive gna_data_sources gna_parse gna_search gna_verifier gni_details gni_parse gni_seach gnr_datasources gnr_resolve id2name ion iplant_resolve ipni_ping ipni_search itis_acceptname itis_downstream itis_getrecord itis_hierarchy itis_kingdomnames itis_lsid itis_name itis_native itis_ping itis_refs itis_taxrank itis_terms iucn_getname iucn_id iucn_status iucn_summary lowest_common names_list nbn_classification nbn_ping nbn_search nbn_synonyms ncbi_children ncbi_downstream ncbi_get_taxon_summary ncbi_getbyid ncbi_getbyname ncbi_ping ncbi_search phylomatic_format phylomatic_tree plantminer pow_lookup pow_search pow_synonyms rankagg resolve sci2comm scrapenames status_codes synonyms synonyms_df tax_agg tax_name tax_rank taxize_capwords taxize_cite taxize_ldfast taxize_options taxon_clear taxon_last tnrs tnrs_sources tol_resolve tp_acceptednames tp_accnames tp_classification tp_dist tp_namedistributions tp_namereferences tp_refs tp_search tp_summary tp_synonyms tpl_families tpl_get tpl_search tropicos_ping ubio_classification ubio_classification_search ubio_id ubio_ping ubio_search ubio_synonyms upstream use_entrez use_eol use_iucn use_tropicos vascan_ping vascan_search worms_downstream taxonomy biology nomenclature json api web api-client identifiers species names api-wrapper biodiversity darwincore data taxize"
  },
  {
    "id": 1001,
    "package_name": "pool",
    "title": "Object Pooling",
    "description": "Enables the creation of object pools, which make it less\ncomputationally expensive to fetch a new object. Currently the\nonly supported pooled objects are 'DBI' connections.",
    "version": "1.0.4.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Joe Cheng [aut],\nBarbara Borges [aut],\nHadley Wickham [aut, cre],\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/pool, https://rstudio.github.io/pool/",
    "bug_reports": "https://github.com/rstudio/pool/issues",
    "repository": "",
    "exports": [
      [
        "dbAppendTable"
      ],
      [
        "dbAppendTableArrow"
      ],
      [
        "dbBegin"
      ],
      [
        "dbBreak"
      ],
      [
        "dbCommit"
      ],
      [
        "dbCreateTable"
      ],
      [
        "dbCreateTableArrow"
      ],
      [
        "dbDataType"
      ],
      [
        "dbDisconnect"
      ],
      [
        "dbExecute"
      ],
      [
        "dbExistsTable"
      ],
      [
        "dbGetInfo"
      ],
      [
        "dbGetQuery"
      ],
      [
        "dbGetQueryArrow"
      ],
      [
        "dbIsReadOnly"
      ],
      [
        "dbIsValid"
      ],
      [
        "dbListFields"
      ],
      [
        "dbListObjects"
      ],
      [
        "dbListTables"
      ],
      [
        "dbPool"
      ],
      [
        "dbQuoteIdentifier"
      ],
      [
        "dbQuoteLiteral"
      ],
      [
        "dbQuoteString"
      ],
      [
        "dbReadTable"
      ],
      [
        "dbReadTableArrow"
      ],
      [
        "dbRemoveTable"
      ],
      [
        "dbRollback"
      ],
      [
        "dbSendQuery"
      ],
      [
        "dbSendQueryArrow"
      ],
      [
        "dbSendStatement"
      ],
      [
        "dbUnquoteIdentifier"
      ],
      [
        "dbWithTransaction"
      ],
      [
        "dbWriteTable"
      ],
      [
        "dbWriteTableArrow"
      ],
      [
        "demoDb"
      ],
      [
        "localCheckout"
      ],
      [
        "onActivate"
      ],
      [
        "onDestroy"
      ],
      [
        "onPassivate"
      ],
      [
        "onValidate"
      ],
      [
        "Pool"
      ],
      [
        "poolCheckout"
      ],
      [
        "poolClose"
      ],
      [
        "poolCreate"
      ],
      [
        "poolReturn"
      ],
      [
        "poolWithTransaction"
      ],
      [
        "sqlAppendTable"
      ],
      [
        "sqlCreateTable"
      ],
      [
        "sqlData"
      ],
      [
        "sqlInterpolate"
      ],
      [
        "sqlParseVariables"
      ]
    ],
    "topics": [],
    "score": 12.7796,
    "stars": 255,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "pool Object Pooling Enables the creation of object pools, which make it less\ncomputationally expensive to fetch a new object. Currently the\nonly supported pooled objects are 'DBI' connections. dbAppendTable dbAppendTableArrow dbBegin dbBreak dbCommit dbCreateTable dbCreateTableArrow dbDataType dbDisconnect dbExecute dbExistsTable dbGetInfo dbGetQuery dbGetQueryArrow dbIsReadOnly dbIsValid dbListFields dbListObjects dbListTables dbPool dbQuoteIdentifier dbQuoteLiteral dbQuoteString dbReadTable dbReadTableArrow dbRemoveTable dbRollback dbSendQuery dbSendQueryArrow dbSendStatement dbUnquoteIdentifier dbWithTransaction dbWriteTable dbWriteTableArrow demoDb localCheckout onActivate onDestroy onPassivate onValidate Pool poolCheckout poolClose poolCreate poolReturn poolWithTransaction sqlAppendTable sqlCreateTable sqlData sqlInterpolate sqlParseVariables "
  },
  {
    "id": 1346,
    "package_name": "tidypredict",
    "title": "Run Predictions Inside the Database",
    "description": "It parses a fitted 'R' model object, and returns a formula\nin 'Tidy Eval' code that calculates the predictions.  It works\nwith several databases back-ends because it leverages 'dplyr'\nand 'dbplyr' for the final 'SQL' translation of the algorithm.\nIt currently supports lm(), glm(), randomForest(), ranger(),\nearth(), xgb.Booster.complete(), cubist(), and ctree() models.",
    "version": "1.0.1.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Emil Hvitfeldt [aut, cre],\nEdgar Ruiz [aut],\nMax Kuhn [aut]",
    "url": "https://tidypredict.tidymodels.org,\nhttps://github.com/tidymodels/tidypredict",
    "bug_reports": "https://github.com/tidymodels/tidypredict/issues",
    "repository": "",
    "exports": [
      [
        ".extract_partykit_classprob"
      ],
      [
        ".extract_xgb_trees"
      ],
      [
        "acceptable_formula"
      ],
      [
        "as_parsed_model"
      ],
      [
        "parse_model"
      ],
      [
        "tidy"
      ],
      [
        "tidypredict_fit"
      ],
      [
        "tidypredict_interval"
      ],
      [
        "tidypredict_sql"
      ],
      [
        "tidypredict_sql_interval"
      ],
      [
        "tidypredict_test"
      ],
      [
        "tidypredict_to_column"
      ]
    ],
    "topics": [
      [
        "dbplyr"
      ],
      [
        "dplyr"
      ],
      [
        "purrr"
      ],
      [
        "rlang"
      ]
    ],
    "score": 12.1646,
    "stars": 261,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "tidypredict Run Predictions Inside the Database It parses a fitted 'R' model object, and returns a formula\nin 'Tidy Eval' code that calculates the predictions.  It works\nwith several databases back-ends because it leverages 'dplyr'\nand 'dbplyr' for the final 'SQL' translation of the algorithm.\nIt currently supports lm(), glm(), randomForest(), ranger(),\nearth(), xgb.Booster.complete(), cubist(), and ctree() models. .extract_partykit_classprob .extract_xgb_trees acceptable_formula as_parsed_model parse_model tidy tidypredict_fit tidypredict_interval tidypredict_sql tidypredict_sql_interval tidypredict_test tidypredict_to_column dbplyr dplyr purrr rlang"
  },
  {
    "id": 1397,
    "package_name": "tzdb",
    "title": "Time Zone Database Information",
    "description": "Provides an up-to-date copy of the Internet Assigned\nNumbers Authority (IANA) Time Zone Database. It is updated\nperiodically to reflect changes made by political bodies to\ntime zone boundaries, UTC offsets, and daylight saving time\nrules. Additionally, this package provides a C++ interface for\nworking with the 'date' library. 'date' provides comprehensive\nsupport for working with dates and date-times, which this\npackage exposes to make it easier for other R packages to\nutilize. Headers are provided for calendar specific\ncalculations, along with a limited interface for time zone\nmanipulations.",
    "version": "0.5.0.9000",
    "maintainer": "Davis Vaughan <davis@posit.co>",
    "author": "Davis Vaughan [aut, cre],\nHoward Hinnant [cph] (Author of the included date library),\nPosit Software, PBC [cph, fnd]",
    "url": "https://tzdb.r-lib.org, https://github.com/r-lib/tzdb",
    "bug_reports": "https://github.com/r-lib/tzdb/issues",
    "repository": "",
    "exports": [
      [
        "tzdb_initialize"
      ],
      [
        "tzdb_names"
      ],
      [
        "tzdb_path"
      ],
      [
        "tzdb_version"
      ]
    ],
    "topics": [
      [
        "cpp"
      ]
    ],
    "score": 11.8088,
    "stars": 7,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "tzdb Time Zone Database Information Provides an up-to-date copy of the Internet Assigned\nNumbers Authority (IANA) Time Zone Database. It is updated\nperiodically to reflect changes made by political bodies to\ntime zone boundaries, UTC offsets, and daylight saving time\nrules. Additionally, this package provides a C++ interface for\nworking with the 'date' library. 'date' provides comprehensive\nsupport for working with dates and date-times, which this\npackage exposes to make it easier for other R packages to\nutilize. Headers are provided for calendar specific\ncalculations, along with a limited interface for time zone\nmanipulations. tzdb_initialize tzdb_names tzdb_path tzdb_version cpp"
  },
  {
    "id": 296,
    "package_name": "biomartr",
    "title": "Genomic Data Retrieval",
    "description": "Perform large scale genomic data retrieval and functional\nannotation retrieval. This package aims to provide users with a\nstandardized way to automate genome, proteome, 'RNA', coding\nsequence ('CDS'), 'GFF', and metagenome retrieval from 'NCBI\nRefSeq', 'NCBI Genbank', 'ENSEMBL', and 'UniProt' databases.\nFurthermore, an interface to the 'BioMart' database (Smedley et\nal. (2009) <doi:10.1186/1471-2164-10-22>) allows users to\nretrieve functional annotation for genomic loci. In addition,\nusers can download entire databases such as 'NCBI RefSeq'\n(Pruitt et al. (2007) <doi:10.1093/nar/gkl842>), 'NCBI nr',\n'NCBI nt', 'NCBI Genbank' (Benson et al. (2013)\n<doi:10.1093/nar/gks1195>), etc. with only one command.",
    "version": "1.0.11",
    "maintainer": "Hajk-Georg Drost <hajk-georg.drost@tuebingen.mpg.de>",
    "author": "Hajk-Georg Drost [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-1567-306X>),\nHaakon Tjeldnes [aut, ctb]",
    "url": "https://docs.ropensci.org/biomartr/,\nhttps://github.com/ropensci/biomartr",
    "bug_reports": "https://github.com/ropensci/biomartr/issues",
    "repository": "",
    "exports": [
      [
        "biomart"
      ],
      [
        "cachedir"
      ],
      [
        "cachedir_set"
      ],
      [
        "check_annotation_biomartr"
      ],
      [
        "clean.retrieval"
      ],
      [
        "download.database"
      ],
      [
        "download.database.all"
      ],
      [
        "ensembl_divisions"
      ],
      [
        "get.ensembl.info"
      ],
      [
        "getAssemblyStats"
      ],
      [
        "getAttributes"
      ],
      [
        "getBioSet"
      ],
      [
        "getCDS"
      ],
      [
        "getCDSSet"
      ],
      [
        "getCollection"
      ],
      [
        "getCollectionSet"
      ],
      [
        "getDatasets"
      ],
      [
        "getENSEMBLGENOMESInfo"
      ],
      [
        "getENSEMBLInfo"
      ],
      [
        "getFilters"
      ],
      [
        "getGenome"
      ],
      [
        "getGENOMEREPORT"
      ],
      [
        "getGenomeSet"
      ],
      [
        "getGFF"
      ],
      [
        "getGFFSet"
      ],
      [
        "getGO"
      ],
      [
        "getGroups"
      ],
      [
        "getGTF"
      ],
      [
        "getKingdomAssemblySummary"
      ],
      [
        "getKingdoms"
      ],
      [
        "getMarts"
      ],
      [
        "getMetaGenomeAnnotations"
      ],
      [
        "getMetaGenomes"
      ],
      [
        "getMetaGenomeSummary"
      ],
      [
        "getProteome"
      ],
      [
        "getProteomeSet"
      ],
      [
        "getReleases"
      ],
      [
        "getRepeatMasker"
      ],
      [
        "getRNA"
      ],
      [
        "getRNASet"
      ],
      [
        "getSummaryFile"
      ],
      [
        "getUniProtInfo"
      ],
      [
        "getUniProtSTATS"
      ],
      [
        "is.genome.available"
      ],
      [
        "listDatabases"
      ],
      [
        "listGenomes"
      ],
      [
        "listGroups"
      ],
      [
        "listKingdoms"
      ],
      [
        "listMetaGenomes"
      ],
      [
        "listNCBIDatabases"
      ],
      [
        "meta.retrieval"
      ],
      [
        "meta.retrieval.all"
      ],
      [
        "organismAttributes"
      ],
      [
        "organismBM"
      ],
      [
        "organismFilters"
      ],
      [
        "read_assemblystats"
      ],
      [
        "read_cds"
      ],
      [
        "read_genome"
      ],
      [
        "read_gff"
      ],
      [
        "read_proteome"
      ],
      [
        "read_rm"
      ],
      [
        "read_rna"
      ],
      [
        "refseqOrganisms"
      ],
      [
        "summary_cds"
      ],
      [
        "summary_genome"
      ]
    ],
    "topics": [
      [
        "biomart"
      ],
      [
        "genomic-data-retrieval"
      ],
      [
        "annotation-retrieval"
      ],
      [
        "database-retrieval"
      ],
      [
        "ncbi"
      ],
      [
        "ensembl"
      ],
      [
        "biological-data-retrieval"
      ],
      [
        "ensembl-servers"
      ],
      [
        "genome"
      ],
      [
        "genome-annotation"
      ],
      [
        "genome-retrieval"
      ],
      [
        "genomics"
      ],
      [
        "meta-analysis"
      ],
      [
        "metagenomics"
      ],
      [
        "ncbi-genbank"
      ],
      [
        "peer-reviewed"
      ],
      [
        "proteome"
      ],
      [
        "sequenced-genomes"
      ]
    ],
    "score": 11.7795,
    "stars": 222,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "biomartr Genomic Data Retrieval Perform large scale genomic data retrieval and functional\nannotation retrieval. This package aims to provide users with a\nstandardized way to automate genome, proteome, 'RNA', coding\nsequence ('CDS'), 'GFF', and metagenome retrieval from 'NCBI\nRefSeq', 'NCBI Genbank', 'ENSEMBL', and 'UniProt' databases.\nFurthermore, an interface to the 'BioMart' database (Smedley et\nal. (2009) <doi:10.1186/1471-2164-10-22>) allows users to\nretrieve functional annotation for genomic loci. In addition,\nusers can download entire databases such as 'NCBI RefSeq'\n(Pruitt et al. (2007) <doi:10.1093/nar/gkl842>), 'NCBI nr',\n'NCBI nt', 'NCBI Genbank' (Benson et al. (2013)\n<doi:10.1093/nar/gks1195>), etc. with only one command. biomart cachedir cachedir_set check_annotation_biomartr clean.retrieval download.database download.database.all ensembl_divisions get.ensembl.info getAssemblyStats getAttributes getBioSet getCDS getCDSSet getCollection getCollectionSet getDatasets getENSEMBLGENOMESInfo getENSEMBLInfo getFilters getGenome getGENOMEREPORT getGenomeSet getGFF getGFFSet getGO getGroups getGTF getKingdomAssemblySummary getKingdoms getMarts getMetaGenomeAnnotations getMetaGenomes getMetaGenomeSummary getProteome getProteomeSet getReleases getRepeatMasker getRNA getRNASet getSummaryFile getUniProtInfo getUniProtSTATS is.genome.available listDatabases listGenomes listGroups listKingdoms listMetaGenomes listNCBIDatabases meta.retrieval meta.retrieval.all organismAttributes organismBM organismFilters read_assemblystats read_cds read_genome read_gff read_proteome read_rm read_rna refseqOrganisms summary_cds summary_genome biomart genomic-data-retrieval annotation-retrieval database-retrieval ncbi ensembl biological-data-retrieval ensembl-servers genome genome-annotation genome-retrieval genomics meta-analysis metagenomics ncbi-genbank peer-reviewed proteome sequenced-genomes"
  },
  {
    "id": 998,
    "package_name": "pointblank",
    "title": "Data Validation and Organization of Metadata for Local and\nRemote Tables",
    "description": "Validate data in data frames, 'tibble' objects, 'Spark'\n'DataFrames', and database tables. Validation pipelines can be\nmade using easily-readable, consecutive validation steps. Upon\nexecution of the validation plan, several reporting options are\navailable. User-defined thresholds for failure rates allow for\nthe determination of appropriate reporting actions. Many other\nworkflows are available including an information management\nworkflow, where the aim is to record, collect, and generate\nuseful information on data tables.",
    "version": "0.12.3.9000",
    "maintainer": "Richard Iannone <rich@posit.co>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3925-190X>),\nMauricio Vargas [aut] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\nJune Choe [aut] (ORCID: <https://orcid.org/0000-0002-0701-921X>),\nOlivier Roy [ctb]",
    "url": "https://rstudio.github.io/pointblank/,\nhttps://github.com/rstudio/pointblank",
    "bug_reports": "https://github.com/rstudio/pointblank/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "action_levels"
      ],
      [
        "activate_steps"
      ],
      [
        "affix_date"
      ],
      [
        "affix_datetime"
      ],
      [
        "all_passed"
      ],
      [
        "between"
      ],
      [
        "case_when"
      ],
      [
        "col_count_match"
      ],
      [
        "col_exists"
      ],
      [
        "col_is_character"
      ],
      [
        "col_is_date"
      ],
      [
        "col_is_factor"
      ],
      [
        "col_is_integer"
      ],
      [
        "col_is_logical"
      ],
      [
        "col_is_numeric"
      ],
      [
        "col_is_posix"
      ],
      [
        "col_schema"
      ],
      [
        "col_schema_match"
      ],
      [
        "col_vals_between"
      ],
      [
        "col_vals_decreasing"
      ],
      [
        "col_vals_equal"
      ],
      [
        "col_vals_expr"
      ],
      [
        "col_vals_gt"
      ],
      [
        "col_vals_gte"
      ],
      [
        "col_vals_in_set"
      ],
      [
        "col_vals_increasing"
      ],
      [
        "col_vals_lt"
      ],
      [
        "col_vals_lte"
      ],
      [
        "col_vals_make_set"
      ],
      [
        "col_vals_make_subset"
      ],
      [
        "col_vals_not_between"
      ],
      [
        "col_vals_not_equal"
      ],
      [
        "col_vals_not_in_set"
      ],
      [
        "col_vals_not_null"
      ],
      [
        "col_vals_null"
      ],
      [
        "col_vals_regex"
      ],
      [
        "col_vals_within_spec"
      ],
      [
        "conjointly"
      ],
      [
        "create_agent"
      ],
      [
        "create_informant"
      ],
      [
        "create_multiagent"
      ],
      [
        "creds"
      ],
      [
        "creds_anonymous"
      ],
      [
        "creds_file"
      ],
      [
        "creds_key"
      ],
      [
        "db_tbl"
      ],
      [
        "deactivate_steps"
      ],
      [
        "draft_validation"
      ],
      [
        "email_blast"
      ],
      [
        "email_create"
      ],
      [
        "expect_col_count_match"
      ],
      [
        "expect_col_exists"
      ],
      [
        "expect_col_is_character"
      ],
      [
        "expect_col_is_date"
      ],
      [
        "expect_col_is_factor"
      ],
      [
        "expect_col_is_integer"
      ],
      [
        "expect_col_is_logical"
      ],
      [
        "expect_col_is_numeric"
      ],
      [
        "expect_col_is_posix"
      ],
      [
        "expect_col_schema_match"
      ],
      [
        "expect_col_vals_between"
      ],
      [
        "expect_col_vals_decreasing"
      ],
      [
        "expect_col_vals_equal"
      ],
      [
        "expect_col_vals_expr"
      ],
      [
        "expect_col_vals_gt"
      ],
      [
        "expect_col_vals_gte"
      ],
      [
        "expect_col_vals_in_set"
      ],
      [
        "expect_col_vals_increasing"
      ],
      [
        "expect_col_vals_lt"
      ],
      [
        "expect_col_vals_lte"
      ],
      [
        "expect_col_vals_make_set"
      ],
      [
        "expect_col_vals_make_subset"
      ],
      [
        "expect_col_vals_not_between"
      ],
      [
        "expect_col_vals_not_equal"
      ],
      [
        "expect_col_vals_not_in_set"
      ],
      [
        "expect_col_vals_not_null"
      ],
      [
        "expect_col_vals_null"
      ],
      [
        "expect_col_vals_regex"
      ],
      [
        "expect_col_vals_within_spec"
      ],
      [
        "expect_conjointly"
      ],
      [
        "expect_row_count_match"
      ],
      [
        "expect_rows_complete"
      ],
      [
        "expect_rows_distinct"
      ],
      [
        "expect_serially"
      ],
      [
        "expect_specially"
      ],
      [
        "expect_tbl_match"
      ],
      [
        "export_report"
      ],
      [
        "expr"
      ],
      [
        "file_tbl"
      ],
      [
        "from_github"
      ],
      [
        "get_agent_report"
      ],
      [
        "get_agent_x_list"
      ],
      [
        "get_data_extracts"
      ],
      [
        "get_informant_report"
      ],
      [
        "get_multiagent_report"
      ],
      [
        "get_sundered_data"
      ],
      [
        "get_tt_param"
      ],
      [
        "has_columns"
      ],
      [
        "incorporate"
      ],
      [
        "info_columns"
      ],
      [
        "info_columns_from_tbl"
      ],
      [
        "info_section"
      ],
      [
        "info_snippet"
      ],
      [
        "info_tabular"
      ],
      [
        "interrogate"
      ],
      [
        "log4r_step"
      ],
      [
        "read_disk_multiagent"
      ],
      [
        "remove_steps"
      ],
      [
        "row_count_match"
      ],
      [
        "rows_complete"
      ],
      [
        "rows_distinct"
      ],
      [
        "scan_data"
      ],
      [
        "serially"
      ],
      [
        "set_tbl"
      ],
      [
        "small_table_sqlite"
      ],
      [
        "snip_highest"
      ],
      [
        "snip_list"
      ],
      [
        "snip_lowest"
      ],
      [
        "snip_stats"
      ],
      [
        "specially"
      ],
      [
        "stock_msg_body"
      ],
      [
        "stock_msg_footer"
      ],
      [
        "stop_if_not"
      ],
      [
        "stop_on_fail"
      ],
      [
        "tbl_get"
      ],
      [
        "tbl_match"
      ],
      [
        "tbl_source"
      ],
      [
        "tbl_store"
      ],
      [
        "test_col_count_match"
      ],
      [
        "test_col_exists"
      ],
      [
        "test_col_is_character"
      ],
      [
        "test_col_is_date"
      ],
      [
        "test_col_is_factor"
      ],
      [
        "test_col_is_integer"
      ],
      [
        "test_col_is_logical"
      ],
      [
        "test_col_is_numeric"
      ],
      [
        "test_col_is_posix"
      ],
      [
        "test_col_schema_match"
      ],
      [
        "test_col_vals_between"
      ],
      [
        "test_col_vals_decreasing"
      ],
      [
        "test_col_vals_equal"
      ],
      [
        "test_col_vals_expr"
      ],
      [
        "test_col_vals_gt"
      ],
      [
        "test_col_vals_gte"
      ],
      [
        "test_col_vals_in_set"
      ],
      [
        "test_col_vals_increasing"
      ],
      [
        "test_col_vals_lt"
      ],
      [
        "test_col_vals_lte"
      ],
      [
        "test_col_vals_make_set"
      ],
      [
        "test_col_vals_make_subset"
      ],
      [
        "test_col_vals_not_between"
      ],
      [
        "test_col_vals_not_equal"
      ],
      [
        "test_col_vals_not_in_set"
      ],
      [
        "test_col_vals_not_null"
      ],
      [
        "test_col_vals_null"
      ],
      [
        "test_col_vals_regex"
      ],
      [
        "test_col_vals_within_spec"
      ],
      [
        "test_conjointly"
      ],
      [
        "test_row_count_match"
      ],
      [
        "test_rows_complete"
      ],
      [
        "test_rows_distinct"
      ],
      [
        "test_serially"
      ],
      [
        "test_specially"
      ],
      [
        "test_tbl_match"
      ],
      [
        "tt_string_info"
      ],
      [
        "tt_summary_stats"
      ],
      [
        "tt_tbl_colnames"
      ],
      [
        "tt_tbl_dims"
      ],
      [
        "tt_time_shift"
      ],
      [
        "tt_time_slice"
      ],
      [
        "validate_rmd"
      ],
      [
        "vars"
      ],
      [
        "warn_on_fail"
      ],
      [
        "write_testthat_file"
      ],
      [
        "x_read_disk"
      ],
      [
        "x_write_disk"
      ],
      [
        "yaml_agent_interrogate"
      ],
      [
        "yaml_agent_show_exprs"
      ],
      [
        "yaml_agent_string"
      ],
      [
        "yaml_exec"
      ],
      [
        "yaml_informant_incorporate"
      ],
      [
        "yaml_read_agent"
      ],
      [
        "yaml_read_informant"
      ],
      [
        "yaml_write"
      ]
    ],
    "topics": [
      [
        "data-assertions"
      ],
      [
        "data-checker"
      ],
      [
        "data-dictionaries"
      ],
      [
        "data-frames"
      ],
      [
        "data-inference"
      ],
      [
        "data-management"
      ],
      [
        "data-profiler"
      ],
      [
        "data-quality"
      ],
      [
        "data-validation"
      ],
      [
        "data-verification"
      ],
      [
        "database-tables"
      ],
      [
        "easy-to-understand"
      ],
      [
        "reporting-tool"
      ],
      [
        "schema-validation"
      ],
      [
        "testing-tools"
      ],
      [
        "yaml-configuration"
      ]
    ],
    "score": 11.5175,
    "stars": 1016,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "pointblank Data Validation and Organization of Metadata for Local and\nRemote Tables Validate data in data frames, 'tibble' objects, 'Spark'\n'DataFrames', and database tables. Validation pipelines can be\nmade using easily-readable, consecutive validation steps. Upon\nexecution of the validation plan, several reporting options are\navailable. User-defined thresholds for failure rates allow for\nthe determination of appropriate reporting actions. Many other\nworkflows are available including an information management\nworkflow, where the aim is to record, collect, and generate\nuseful information on data tables. %>% action_levels activate_steps affix_date affix_datetime all_passed between case_when col_count_match col_exists col_is_character col_is_date col_is_factor col_is_integer col_is_logical col_is_numeric col_is_posix col_schema col_schema_match col_vals_between col_vals_decreasing col_vals_equal col_vals_expr col_vals_gt col_vals_gte col_vals_in_set col_vals_increasing col_vals_lt col_vals_lte col_vals_make_set col_vals_make_subset col_vals_not_between col_vals_not_equal col_vals_not_in_set col_vals_not_null col_vals_null col_vals_regex col_vals_within_spec conjointly create_agent create_informant create_multiagent creds creds_anonymous creds_file creds_key db_tbl deactivate_steps draft_validation email_blast email_create expect_col_count_match expect_col_exists expect_col_is_character expect_col_is_date expect_col_is_factor expect_col_is_integer expect_col_is_logical expect_col_is_numeric expect_col_is_posix expect_col_schema_match expect_col_vals_between expect_col_vals_decreasing expect_col_vals_equal expect_col_vals_expr expect_col_vals_gt expect_col_vals_gte expect_col_vals_in_set expect_col_vals_increasing expect_col_vals_lt expect_col_vals_lte expect_col_vals_make_set expect_col_vals_make_subset expect_col_vals_not_between expect_col_vals_not_equal expect_col_vals_not_in_set expect_col_vals_not_null expect_col_vals_null expect_col_vals_regex expect_col_vals_within_spec expect_conjointly expect_row_count_match expect_rows_complete expect_rows_distinct expect_serially expect_specially expect_tbl_match export_report expr file_tbl from_github get_agent_report get_agent_x_list get_data_extracts get_informant_report get_multiagent_report get_sundered_data get_tt_param has_columns incorporate info_columns info_columns_from_tbl info_section info_snippet info_tabular interrogate log4r_step read_disk_multiagent remove_steps row_count_match rows_complete rows_distinct scan_data serially set_tbl small_table_sqlite snip_highest snip_list snip_lowest snip_stats specially stock_msg_body stock_msg_footer stop_if_not stop_on_fail tbl_get tbl_match tbl_source tbl_store test_col_count_match test_col_exists test_col_is_character test_col_is_date test_col_is_factor test_col_is_integer test_col_is_logical test_col_is_numeric test_col_is_posix test_col_schema_match test_col_vals_between test_col_vals_decreasing test_col_vals_equal test_col_vals_expr test_col_vals_gt test_col_vals_gte test_col_vals_in_set test_col_vals_increasing test_col_vals_lt test_col_vals_lte test_col_vals_make_set test_col_vals_make_subset test_col_vals_not_between test_col_vals_not_equal test_col_vals_not_in_set test_col_vals_not_null test_col_vals_null test_col_vals_regex test_col_vals_within_spec test_conjointly test_row_count_match test_rows_complete test_rows_distinct test_serially test_specially test_tbl_match tt_string_info tt_summary_stats tt_tbl_colnames tt_tbl_dims tt_time_shift tt_time_slice validate_rmd vars warn_on_fail write_testthat_file x_read_disk x_write_disk yaml_agent_interrogate yaml_agent_show_exprs yaml_agent_string yaml_exec yaml_informant_incorporate yaml_read_agent yaml_read_informant yaml_write data-assertions data-checker data-dictionaries data-frames data-inference data-management data-profiler data-quality data-validation data-verification database-tables easy-to-understand reporting-tool schema-validation testing-tools yaml-configuration"
  },
  {
    "id": 910,
    "package_name": "openalexR",
    "title": "Getting Bibliographic Records from 'OpenAlex' Database Using\n'DSL' API",
    "description": "A set of tools to extract bibliographic content from\n'OpenAlex' database using API <https://docs.openalex.org>.",
    "version": "2.0.2.9000",
    "maintainer": "Massimo Aria <aria@unina.it>",
    "author": "Massimo Aria [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-8517-9411>),\nCorrado Cuccurullo [ctb] (ORCID:\n<https://orcid.org/0000-0002-7401-8575>),\nTrang Le [aut] (ORCID: <https://orcid.org/0000-0003-3737-6565>),\nJune Choe [aut] (ORCID: <https://orcid.org/0000-0002-0701-921X>)",
    "url": "https://github.com/ropensci/openalexR,\nhttps://docs.ropensci.org/openalexR/",
    "bug_reports": "https://github.com/ropensci/openalexR/issues",
    "repository": "",
    "exports": [
      [
        "authors2df"
      ],
      [
        "concepts2df"
      ],
      [
        "funders2df"
      ],
      [
        "get_coverage"
      ],
      [
        "institutions2df"
      ],
      [
        "keywords2df"
      ],
      [
        "oa_apikey"
      ],
      [
        "oa_email"
      ],
      [
        "oa_entities"
      ],
      [
        "oa_fetch"
      ],
      [
        "oa_generate"
      ],
      [
        "oa_ngrams"
      ],
      [
        "oa_query"
      ],
      [
        "oa_random"
      ],
      [
        "oa_request"
      ],
      [
        "oa_snowball"
      ],
      [
        "oa2bibliometrix"
      ],
      [
        "oa2df"
      ],
      [
        "publishers2df"
      ],
      [
        "show_authors"
      ],
      [
        "show_works"
      ],
      [
        "snowball2df"
      ],
      [
        "sources2df"
      ],
      [
        "topics2df"
      ],
      [
        "works2df"
      ]
    ],
    "topics": [
      [
        "bibliographic-data"
      ],
      [
        "bibliographic-database"
      ],
      [
        "bibliometrics"
      ],
      [
        "bibliometrix"
      ],
      [
        "science-mapping"
      ]
    ],
    "score": 10.882,
    "stars": 128,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "openalexR Getting Bibliographic Records from 'OpenAlex' Database Using\n'DSL' API A set of tools to extract bibliographic content from\n'OpenAlex' database using API <https://docs.openalex.org>. authors2df concepts2df funders2df get_coverage institutions2df keywords2df oa_apikey oa_email oa_entities oa_fetch oa_generate oa_ngrams oa_query oa_random oa_request oa_snowball oa2bibliometrix oa2df publishers2df show_authors show_works snowball2df sources2df topics2df works2df bibliographic-data bibliographic-database bibliometrics bibliometrix science-mapping"
  },
  {
    "id": 1066,
    "package_name": "rdflib",
    "title": "Tools to Manipulate and Query Semantic Data",
    "description": "The Resource Description Framework, or 'RDF' is a widely\nused data representation model that forms the cornerstone of\nthe Semantic Web. 'RDF' represents data as a graph rather than\nthe familiar data table or rectangle of relational databases.\nThe 'rdflib' package provides a friendly and concise user\ninterface for performing common tasks on 'RDF' data, such as\nreading, writing and converting between the various\nserializations of 'RDF' data, including 'rdfxml', 'turtle',\n'nquads', 'ntriples', and 'json-ld'; creating new 'RDF' graphs,\nand performing graph queries using 'SPARQL'. This package wraps\nthe low level 'redland' R package which provides direct\nbindings to the 'redland' C library.  Additionally, the package\nsupports the newer and more developer friendly 'JSON-LD' format\nthrough the 'jsonld' package. The package interface takes\ninspiration from the Python 'rdflib' library.",
    "version": "0.2.9",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nBryce Mecum [rev] (ORCID: <https://orcid.org/0000-0002-0381-3766>),\nAnna Krystalli [rev] (ORCID: <https://orcid.org/0000-0002-2378-4915>),\nViktor Senderov [ctb] (ORCID: <https://orcid.org/0000-0003-3340-5963>)",
    "url": "https://docs.ropensci.org/rdflib/,\nhttps://github.com/ropensci/rdflib",
    "bug_reports": "https://github.com/ropensci/rdflib/issues",
    "repository": "",
    "exports": [
      [
        "as_rdf"
      ],
      [
        "rdf"
      ],
      [
        "rdf_add"
      ],
      [
        "rdf_free"
      ],
      [
        "rdf_has_bdb"
      ],
      [
        "rdf_parse"
      ],
      [
        "rdf_query"
      ],
      [
        "rdf_serialize"
      ],
      [
        "read_nquads"
      ],
      [
        "write_nquads"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ]
    ],
    "score": 10.3875,
    "stars": 60,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rdflib Tools to Manipulate and Query Semantic Data The Resource Description Framework, or 'RDF' is a widely\nused data representation model that forms the cornerstone of\nthe Semantic Web. 'RDF' represents data as a graph rather than\nthe familiar data table or rectangle of relational databases.\nThe 'rdflib' package provides a friendly and concise user\ninterface for performing common tasks on 'RDF' data, such as\nreading, writing and converting between the various\nserializations of 'RDF' data, including 'rdfxml', 'turtle',\n'nquads', 'ntriples', and 'json-ld'; creating new 'RDF' graphs,\nand performing graph queries using 'SPARQL'. This package wraps\nthe low level 'redland' R package which provides direct\nbindings to the 'redland' C library.  Additionally, the package\nsupports the newer and more developer friendly 'JSON-LD' format\nthrough the 'jsonld' package. The package interface takes\ninspiration from the Python 'rdflib' library. as_rdf rdf rdf_add rdf_free rdf_has_bdb rdf_parse rdf_query rdf_serialize read_nquads write_nquads peer-reviewed"
  },
  {
    "id": 1075,
    "package_name": "rebird",
    "title": "R Client for the eBird Database of Bird Observations",
    "description": "A programmatic client for the eBird database\n(<https://ebird.org/home>), including functions for searching\nfor bird observations by geographic location (latitude,\nlongitude), eBird hotspots, location identifiers, by notable\nsightings, by region, and by taxonomic name.",
    "version": "1.3.9007",
    "maintainer": "Sebastian Pardo <sebpardo@gmail.com>",
    "author": "Rafael Maia [aut],\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nAndy Teucher [aut],\nGuy Babineau [ctb],\nMarianna Foos [ctb],\nDavid Bradnum [ctb],\nDave Slager [ctb] (ORCID: <https://orcid.org/0000-0003-2525-2039>),\nSebastian Pardo [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-4147-5796>),\nRichard Littauer [ctb] (ORCID: <https://orcid.org/0000-0001-5428-7535>)",
    "url": "https://docs.ropensci.org/rebird/,\nhttps://github.com/ropensci/rebird",
    "bug_reports": "https://github.com/ropensci/rebird/issues",
    "repository": "",
    "exports": [
      [
        "ebirdchecklist"
      ],
      [
        "ebirdchecklistfeed"
      ],
      [
        "ebirdfreq"
      ],
      [
        "ebirdgeo"
      ],
      [
        "ebirdhistorical"
      ],
      [
        "ebirdhotspot"
      ],
      [
        "ebirdhotspotlist"
      ],
      [
        "ebirdloc"
      ],
      [
        "ebirdnotable"
      ],
      [
        "ebirdregion"
      ],
      [
        "ebirdregioncheck"
      ],
      [
        "ebirdregioninfo"
      ],
      [
        "ebirdregionspecies"
      ],
      [
        "ebirdsubregionlist"
      ],
      [
        "ebirdtaxonomy"
      ],
      [
        "ebirdtaxonomyversion"
      ],
      [
        "getlatlng"
      ],
      [
        "nearestobs"
      ],
      [
        "species_code"
      ]
    ],
    "topics": [
      [
        "birds"
      ],
      [
        "birding"
      ],
      [
        "ebird"
      ],
      [
        "database"
      ],
      [
        "data"
      ],
      [
        "biology"
      ],
      [
        "observations"
      ],
      [
        "sightings"
      ],
      [
        "ornithology"
      ],
      [
        "ebird-api"
      ],
      [
        "ebird-webservices"
      ],
      [
        "spocc"
      ]
    ],
    "score": 10.3442,
    "stars": 91,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rebird R Client for the eBird Database of Bird Observations A programmatic client for the eBird database\n(<https://ebird.org/home>), including functions for searching\nfor bird observations by geographic location (latitude,\nlongitude), eBird hotspots, location identifiers, by notable\nsightings, by region, and by taxonomic name. ebirdchecklist ebirdchecklistfeed ebirdfreq ebirdgeo ebirdhistorical ebirdhotspot ebirdhotspotlist ebirdloc ebirdnotable ebirdregion ebirdregioncheck ebirdregioninfo ebirdregionspecies ebirdsubregionlist ebirdtaxonomy ebirdtaxonomyversion getlatlng nearestobs species_code birds birding ebird database data biology observations sightings ornithology ebird-api ebird-webservices spocc"
  },
  {
    "id": 261,
    "package_name": "auk",
    "title": "eBird Data Extraction and Processing in R",
    "description": "Extract and process bird sightings records from eBird\n(<http://ebird.org>), an online tool for recording bird\nobservations.  Public access to the full eBird database is via\nthe eBird Basic Dataset (EBD; see\n<http://ebird.org/ebird/data/download> for access), a\ndownloadable text file. This package is an interface to AWK for\nextracting data from the EBD based on taxonomic, spatial, or\ntemporal filters, to produce a manageable file size that can be\nimported into R.",
    "version": "0.9.0",
    "maintainer": "Matthew Strimas-Mackey <mes335@cornell.edu>",
    "author": "Matthew Strimas-Mackey [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-8929-7776>),\nEliot Miller [aut],\nWesley Hochachka [aut],\nCornell Lab of Ornithology [cph]",
    "url": "https://cornelllabofornithology.github.io/auk/",
    "bug_reports": "https://github.com/CornellLabofOrnithology/auk/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "auk_bbox"
      ],
      [
        "auk_bcr"
      ],
      [
        "auk_breeding"
      ],
      [
        "auk_clean"
      ],
      [
        "auk_complete"
      ],
      [
        "auk_country"
      ],
      [
        "auk_county"
      ],
      [
        "auk_date"
      ],
      [
        "auk_distance"
      ],
      [
        "auk_duration"
      ],
      [
        "auk_ebd"
      ],
      [
        "auk_ebd_version"
      ],
      [
        "auk_exotic"
      ],
      [
        "auk_extent"
      ],
      [
        "auk_filter"
      ],
      [
        "auk_get_awk_path"
      ],
      [
        "auk_get_ebd_path"
      ],
      [
        "auk_last_edited"
      ],
      [
        "auk_observer"
      ],
      [
        "auk_project"
      ],
      [
        "auk_protocol"
      ],
      [
        "auk_rollup"
      ],
      [
        "auk_sampling"
      ],
      [
        "auk_select"
      ],
      [
        "auk_set_awk_path"
      ],
      [
        "auk_set_ebd_path"
      ],
      [
        "auk_species"
      ],
      [
        "auk_split"
      ],
      [
        "auk_state"
      ],
      [
        "auk_time"
      ],
      [
        "auk_unique"
      ],
      [
        "auk_version"
      ],
      [
        "auk_year"
      ],
      [
        "auk_zerofill"
      ],
      [
        "collapse_zerofill"
      ],
      [
        "ebird_species"
      ],
      [
        "filter_repeat_visits"
      ],
      [
        "format_unmarked_occu"
      ],
      [
        "get_ebird_taxonomy"
      ],
      [
        "process_barcharts"
      ],
      [
        "read_ebd"
      ],
      [
        "read_sampling"
      ]
    ],
    "topics": [
      [
        "dataset"
      ],
      [
        "ebird"
      ]
    ],
    "score": 9.9999,
    "stars": 152,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "auk eBird Data Extraction and Processing in R Extract and process bird sightings records from eBird\n(<http://ebird.org>), an online tool for recording bird\nobservations.  Public access to the full eBird database is via\nthe eBird Basic Dataset (EBD; see\n<http://ebird.org/ebird/data/download> for access), a\ndownloadable text file. This package is an interface to AWK for\nextracting data from the EBD based on taxonomic, spatial, or\ntemporal filters, to produce a manageable file size that can be\nimported into R. %>% auk_bbox auk_bcr auk_breeding auk_clean auk_complete auk_country auk_county auk_date auk_distance auk_duration auk_ebd auk_ebd_version auk_exotic auk_extent auk_filter auk_get_awk_path auk_get_ebd_path auk_last_edited auk_observer auk_project auk_protocol auk_rollup auk_sampling auk_select auk_set_awk_path auk_set_ebd_path auk_species auk_split auk_state auk_time auk_unique auk_version auk_year auk_zerofill collapse_zerofill ebird_species filter_repeat_visits format_unmarked_occu get_ebird_taxonomy process_barcharts read_ebd read_sampling dataset ebird"
  },
  {
    "id": 367,
    "package_name": "ckanr",
    "title": "Client for the Comprehensive Knowledge Archive Network ('CKAN')\nAPI",
    "description": "Client for 'CKAN' API (<https://ckan.org/>). Includes\ninterface to 'CKAN' 'APIs' for search, list, show for packages,\norganizations, and resources. In addition, provides an\ninterface to the 'datastore' API.",
    "version": "0.8.1",
    "maintainer": "Florian Mayer <Florian.Mayer@dpc.wa.gov.au>",
    "author": "Scott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nImanuel Costigan [aut],\nWush Wu [aut] (ORCID: <https://orcid.org/0000-0001-5180-0567>),\nFlorian Mayer [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4269-4242>),\nSharla Gelfand [aut],\nFrancisco Alves [aut] (ORCID: <https://orcid.org/0000-0003-3390-0534>),\nHanna B\u00f6hner [aut] (ORCID: <https://orcid.org/0000-0001-7356-5457>)",
    "url": "https://docs.ropensci.org/ckanr/ (website)\nhttps://github.com/ropensci/ckanr (devel)",
    "bug_reports": "https://github.com/ropensci/ckanr/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "activity_create"
      ],
      [
        "activity_data_show"
      ],
      [
        "activity_diff"
      ],
      [
        "activity_show"
      ],
      [
        "am_following_user"
      ],
      [
        "api_token_create"
      ],
      [
        "api_token_list"
      ],
      [
        "api_token_revoke"
      ],
      [
        "as.ckan_group"
      ],
      [
        "as.ckan_organization"
      ],
      [
        "as.ckan_package"
      ],
      [
        "as.ckan_related"
      ],
      [
        "as.ckan_resource"
      ],
      [
        "as.ckan_resource_view"
      ],
      [
        "as.ckan_tag"
      ],
      [
        "as.ckan_user"
      ],
      [
        "as.ckan_vocabulary"
      ],
      [
        "changes"
      ],
      [
        "ckan_action"
      ],
      [
        "ckan_fetch"
      ],
      [
        "ckan_info"
      ],
      [
        "ckan_version"
      ],
      [
        "ckanr_settings"
      ],
      [
        "ckanr_setup"
      ],
      [
        "config_option_list"
      ],
      [
        "config_option_show"
      ],
      [
        "config_option_update"
      ],
      [
        "dashboard_activity_list"
      ],
      [
        "dashboard_count"
      ],
      [
        "dashboard_mark_activities_old"
      ],
      [
        "dashboard_new_activities_count"
      ],
      [
        "dataset_am_following"
      ],
      [
        "dataset_follow"
      ],
      [
        "dataset_followee_count"
      ],
      [
        "dataset_followee_list"
      ],
      [
        "dataset_follower_count"
      ],
      [
        "dataset_follower_list"
      ],
      [
        "dataset_purge"
      ],
      [
        "dataset_unfollow"
      ],
      [
        "ds_create"
      ],
      [
        "ds_create_dataset"
      ],
      [
        "ds_search"
      ],
      [
        "ds_search_sql"
      ],
      [
        "follow_user"
      ],
      [
        "followee_count"
      ],
      [
        "followee_list"
      ],
      [
        "get_default_key"
      ],
      [
        "get_default_url"
      ],
      [
        "get_test_behaviour"
      ],
      [
        "get_test_did"
      ],
      [
        "get_test_gid"
      ],
      [
        "get_test_key"
      ],
      [
        "get_test_oid"
      ],
      [
        "get_test_rid"
      ],
      [
        "get_test_url"
      ],
      [
        "group_activity_list"
      ],
      [
        "group_am_following"
      ],
      [
        "group_create"
      ],
      [
        "group_delete"
      ],
      [
        "group_follow"
      ],
      [
        "group_followee_count"
      ],
      [
        "group_followee_list"
      ],
      [
        "group_follower_count"
      ],
      [
        "group_follower_list"
      ],
      [
        "group_list"
      ],
      [
        "group_list_authz"
      ],
      [
        "group_member_create"
      ],
      [
        "group_member_delete"
      ],
      [
        "group_patch"
      ],
      [
        "group_show"
      ],
      [
        "group_unfollow"
      ],
      [
        "group_update"
      ],
      [
        "help_show"
      ],
      [
        "is.ckan_group"
      ],
      [
        "is.ckan_organization"
      ],
      [
        "is.ckan_package"
      ],
      [
        "is.ckan_related"
      ],
      [
        "is.ckan_resource"
      ],
      [
        "is.ckan_resource_view"
      ],
      [
        "is.ckan_tag"
      ],
      [
        "is.ckan_user"
      ],
      [
        "is.ckan_vocabulary"
      ],
      [
        "job_cancel"
      ],
      [
        "job_clear"
      ],
      [
        "job_list"
      ],
      [
        "job_show"
      ],
      [
        "license_list"
      ],
      [
        "member_create"
      ],
      [
        "member_delete"
      ],
      [
        "member_list"
      ],
      [
        "member_roles_list"
      ],
      [
        "organization_activity_list"
      ],
      [
        "organization_create"
      ],
      [
        "organization_delete"
      ],
      [
        "organization_list"
      ],
      [
        "organization_list_for_user"
      ],
      [
        "organization_member_create"
      ],
      [
        "organization_member_delete"
      ],
      [
        "organization_purge"
      ],
      [
        "organization_show"
      ],
      [
        "package_activity_list"
      ],
      [
        "package_collaborator_create"
      ],
      [
        "package_collaborator_delete"
      ],
      [
        "package_collaborator_list"
      ],
      [
        "package_collaborator_list_for_user"
      ],
      [
        "package_create"
      ],
      [
        "package_create_default_resource_views"
      ],
      [
        "package_delete"
      ],
      [
        "package_list"
      ],
      [
        "package_list_current"
      ],
      [
        "package_owner_org_update"
      ],
      [
        "package_patch"
      ],
      [
        "package_relationship_create"
      ],
      [
        "package_relationship_delete"
      ],
      [
        "package_relationship_update"
      ],
      [
        "package_relationships_list"
      ],
      [
        "package_resource_reorder"
      ],
      [
        "package_revise"
      ],
      [
        "package_revision_list"
      ],
      [
        "package_search"
      ],
      [
        "package_show"
      ],
      [
        "package_update"
      ],
      [
        "ping"
      ],
      [
        "recently_changed_packages_activity_list"
      ],
      [
        "related_create"
      ],
      [
        "related_delete"
      ],
      [
        "related_list"
      ],
      [
        "related_show"
      ],
      [
        "resource_create"
      ],
      [
        "resource_create_default_resource_views"
      ],
      [
        "resource_delete"
      ],
      [
        "resource_patch"
      ],
      [
        "resource_search"
      ],
      [
        "resource_show"
      ],
      [
        "resource_update"
      ],
      [
        "resource_view_clear"
      ],
      [
        "resource_view_create"
      ],
      [
        "resource_view_delete"
      ],
      [
        "resource_view_list"
      ],
      [
        "resource_view_reorder"
      ],
      [
        "resource_view_show"
      ],
      [
        "resource_view_update"
      ],
      [
        "revision_list"
      ],
      [
        "send_email_notifications"
      ],
      [
        "servers"
      ],
      [
        "src_ckan"
      ],
      [
        "status_show"
      ],
      [
        "tag_autocomplete"
      ],
      [
        "tag_create"
      ],
      [
        "tag_list"
      ],
      [
        "tag_search"
      ],
      [
        "tag_show"
      ],
      [
        "task_status_delete"
      ],
      [
        "task_status_show"
      ],
      [
        "task_status_update"
      ],
      [
        "task_status_update_many"
      ],
      [
        "term_translation_show"
      ],
      [
        "term_translation_update"
      ],
      [
        "term_translation_update_many"
      ],
      [
        "unfollow_user"
      ],
      [
        "user_activity_list"
      ],
      [
        "user_create"
      ],
      [
        "user_delete"
      ],
      [
        "user_followee_count"
      ],
      [
        "user_followee_list"
      ],
      [
        "user_follower_count"
      ],
      [
        "user_follower_list"
      ],
      [
        "user_invite"
      ],
      [
        "user_list"
      ],
      [
        "user_show"
      ],
      [
        "vocabulary_create"
      ],
      [
        "vocabulary_delete"
      ],
      [
        "vocabulary_list"
      ],
      [
        "vocabulary_show"
      ],
      [
        "vocabulary_update"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "open-data"
      ],
      [
        "ckan"
      ],
      [
        "api"
      ],
      [
        "data"
      ],
      [
        "dataset"
      ],
      [
        "api-wrapper"
      ],
      [
        "ckan-api"
      ]
    ],
    "score": 9.9287,
    "stars": 101,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "ckanr Client for the Comprehensive Knowledge Archive Network ('CKAN')\nAPI Client for 'CKAN' API (<https://ckan.org/>). Includes\ninterface to 'CKAN' 'APIs' for search, list, show for packages,\norganizations, and resources. In addition, provides an\ninterface to the 'datastore' API. %>% activity_create activity_data_show activity_diff activity_show am_following_user api_token_create api_token_list api_token_revoke as.ckan_group as.ckan_organization as.ckan_package as.ckan_related as.ckan_resource as.ckan_resource_view as.ckan_tag as.ckan_user as.ckan_vocabulary changes ckan_action ckan_fetch ckan_info ckan_version ckanr_settings ckanr_setup config_option_list config_option_show config_option_update dashboard_activity_list dashboard_count dashboard_mark_activities_old dashboard_new_activities_count dataset_am_following dataset_follow dataset_followee_count dataset_followee_list dataset_follower_count dataset_follower_list dataset_purge dataset_unfollow ds_create ds_create_dataset ds_search ds_search_sql follow_user followee_count followee_list get_default_key get_default_url get_test_behaviour get_test_did get_test_gid get_test_key get_test_oid get_test_rid get_test_url group_activity_list group_am_following group_create group_delete group_follow group_followee_count group_followee_list group_follower_count group_follower_list group_list group_list_authz group_member_create group_member_delete group_patch group_show group_unfollow group_update help_show is.ckan_group is.ckan_organization is.ckan_package is.ckan_related is.ckan_resource is.ckan_resource_view is.ckan_tag is.ckan_user is.ckan_vocabulary job_cancel job_clear job_list job_show license_list member_create member_delete member_list member_roles_list organization_activity_list organization_create organization_delete organization_list organization_list_for_user organization_member_create organization_member_delete organization_purge organization_show package_activity_list package_collaborator_create package_collaborator_delete package_collaborator_list package_collaborator_list_for_user package_create package_create_default_resource_views package_delete package_list package_list_current package_owner_org_update package_patch package_relationship_create package_relationship_delete package_relationship_update package_relationships_list package_resource_reorder package_revise package_revision_list package_search package_show package_update ping recently_changed_packages_activity_list related_create related_delete related_list related_show resource_create resource_create_default_resource_views resource_delete resource_patch resource_search resource_show resource_update resource_view_clear resource_view_create resource_view_delete resource_view_list resource_view_reorder resource_view_show resource_view_update revision_list send_email_notifications servers src_ckan status_show tag_autocomplete tag_create tag_list tag_search tag_show task_status_delete task_status_show task_status_update task_status_update_many term_translation_show term_translation_update term_translation_update_many unfollow_user user_activity_list user_create user_delete user_followee_count user_followee_list user_follower_count user_follower_list user_invite user_list user_show vocabulary_create vocabulary_delete vocabulary_list vocabulary_show vocabulary_update database open-data ckan api data dataset api-wrapper ckan-api"
  },
  {
    "id": 217,
    "package_name": "WDI",
    "title": "World Development Indicators and Other World Bank Data",
    "description": "Search and download data from over 40 databases hosted by\nthe World Bank, including the World Development Indicators\n('WDI'), International Debt Statistics, Doing Business, Human\nCapital Index, and Sub-national Poverty indicators.",
    "version": "2.7.9",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "author": "Vincent Arel-Bundock [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>),\nEtienne Bacher [ctb]",
    "url": "https://vincentarelbundock.github.io/WDI/",
    "bug_reports": "https://github.com/vincentarelbundock/WDI/issues",
    "repository": "",
    "exports": [
      [
        "languages_supported"
      ],
      [
        "WDI"
      ],
      [
        "WDIbulk"
      ],
      [
        "WDIcache"
      ],
      [
        "WDIsearch"
      ]
    ],
    "topics": [],
    "score": 9.6753,
    "stars": 231,
    "primary_category": "statistics",
    "source_universe": "vincentarelbundock",
    "search_text": "WDI World Development Indicators and Other World Bank Data Search and download data from over 40 databases hosted by\nthe World Bank, including the World Development Indicators\n('WDI'), International Debt Statistics, Doing Business, Human\nCapital Index, and Sub-national Poverty indicators. languages_supported WDI WDIbulk WDIcache WDIsearch "
  },
  {
    "id": 762,
    "package_name": "lingtypology",
    "title": "Linguistic Typology and Mapping",
    "description": "Provides R with the Glottolog database\n<https://glottolog.org/> and some more abilities for purposes\nof linguistic mapping. The Glottolog database contains the\ncatalogue of languages of the world. This package helps\nresearchers to make a linguistic maps, using philosophy of the\nCross-Linguistic Linked Data project <https://clld.org/>, which\nallows for while at the same time facilitating uniform access\nto the data across publications. A tutorial for this package is\navailable on GitHub pages\n<https://docs.ropensci.org/lingtypology/> and package vignette.\nMaps created by this package can be used both for the\ninvestigation and linguistic teaching. In addition, package\nprovides an ability to download data from typological databases\nsuch as WALS, AUTOTYP and some others and to create your own\ndatabase website.",
    "version": "1.1.23",
    "maintainer": "George Moroz <agricolamz@gmail.com>",
    "author": "George Moroz [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1990-6083>),\nKirill Koncha [ctb] (ORCID: <https://orcid.org/0000-0003-0676-2658>),\nMikhail Leonov [ctb],\nAnna Smirnova [ctb],\nEkaterina Zalivina [ctb]",
    "url": "https://CRAN.R-project.org/package=lingtypology,\nhttps://github.com/ropensci/lingtypology/,\nhttps://ropensci.github.io/lingtypology/",
    "bug_reports": "https://github.com/ropensci/lingtypology/issues",
    "repository": "",
    "exports": [
      [
        "abvd.feature"
      ],
      [
        "afbo.feature"
      ],
      [
        "aff.lang"
      ],
      [
        "area.lang"
      ],
      [
        "atlas.database"
      ],
      [
        "autotyp.feature"
      ],
      [
        "bantu.feature"
      ],
      [
        "bivaltyp.feature"
      ],
      [
        "country.lang"
      ],
      [
        "eurasianphonology.feature"
      ],
      [
        "frequency_list.feature"
      ],
      [
        "gltc.iso"
      ],
      [
        "gltc.lang"
      ],
      [
        "grambank.feature"
      ],
      [
        "is.glottolog"
      ],
      [
        "iso.gltc"
      ],
      [
        "iso.lang"
      ],
      [
        "iso3.iso1"
      ],
      [
        "lang.aff"
      ],
      [
        "lang.country"
      ],
      [
        "lang.gltc"
      ],
      [
        "lang.iso"
      ],
      [
        "lat.lang"
      ],
      [
        "level.lang"
      ],
      [
        "long.lang"
      ],
      [
        "map.feature"
      ],
      [
        "oto_mangueanIC.feature"
      ],
      [
        "phoible.feature"
      ],
      [
        "sails.feature"
      ],
      [
        "soundcomparisons.feature"
      ],
      [
        "subc.lang"
      ],
      [
        "uralex.feature"
      ],
      [
        "url.lang"
      ],
      [
        "valpal.feature"
      ],
      [
        "vanuatu.feature"
      ],
      [
        "wals.feature"
      ]
    ],
    "topics": [
      [
        "abvd"
      ],
      [
        "afbo"
      ],
      [
        "atlas"
      ],
      [
        "autotype"
      ],
      [
        "bivaltyp"
      ],
      [
        "clld"
      ],
      [
        "glottolog-database"
      ],
      [
        "linguistic-maps"
      ],
      [
        "linguistics"
      ],
      [
        "phoible"
      ],
      [
        "sails"
      ],
      [
        "typology"
      ],
      [
        "wals"
      ]
    ],
    "score": 9.5283,
    "stars": 53,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "lingtypology Linguistic Typology and Mapping Provides R with the Glottolog database\n<https://glottolog.org/> and some more abilities for purposes\nof linguistic mapping. The Glottolog database contains the\ncatalogue of languages of the world. This package helps\nresearchers to make a linguistic maps, using philosophy of the\nCross-Linguistic Linked Data project <https://clld.org/>, which\nallows for while at the same time facilitating uniform access\nto the data across publications. A tutorial for this package is\navailable on GitHub pages\n<https://docs.ropensci.org/lingtypology/> and package vignette.\nMaps created by this package can be used both for the\ninvestigation and linguistic teaching. In addition, package\nprovides an ability to download data from typological databases\nsuch as WALS, AUTOTYP and some others and to create your own\ndatabase website. abvd.feature afbo.feature aff.lang area.lang atlas.database autotyp.feature bantu.feature bivaltyp.feature country.lang eurasianphonology.feature frequency_list.feature gltc.iso gltc.lang grambank.feature is.glottolog iso.gltc iso.lang iso3.iso1 lang.aff lang.country lang.gltc lang.iso lat.lang level.lang long.lang map.feature oto_mangueanIC.feature phoible.feature sails.feature soundcomparisons.feature subc.lang uralex.feature url.lang valpal.feature vanuatu.feature wals.feature abvd afbo atlas autotype bivaltyp clld glottolog-database linguistic-maps linguistics phoible sails typology wals"
  },
  {
    "id": 1343,
    "package_name": "tidync",
    "title": "A Tidy Approach to 'NetCDF' Data Exploration and Extraction",
    "description": "Tidy tools for 'NetCDF' data sources. Explore the contents\nof a 'NetCDF' source (file or URL) presented as variables\norganized by grid with a database-like interface. The\nhyper_filter() interactive function translates the filter value\nor index expressions to array-slicing form. No data is read\nuntil explicitly requested, as a data frame or list of arrays\nvia hyper_tibble() or hyper_array().",
    "version": "0.4.0",
    "maintainer": "Michael Sumner <mdsumner@gmail.com>",
    "author": "Michael Sumner [aut, cre],\nSimon Wotherspoon [ctb],\nTomas Remenyi [ctb],\nBen Raymond [ctb],\nJakub Nowosad [ctb],\nTim Lucas [ctb],\nHadley Wickham [ctb],\nAdrian Odenweller [ctb],\nPatrick Van Laake [ctb],\nFabian Bernhard [ctb]",
    "url": "https://docs.ropensci.org/tidync/",
    "bug_reports": "https://github.com/ropensci/tidync/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "activate"
      ],
      [
        "active"
      ],
      [
        "active<-"
      ],
      [
        "hyper_array"
      ],
      [
        "hyper_dims"
      ],
      [
        "hyper_filter"
      ],
      [
        "hyper_grids"
      ],
      [
        "hyper_slice"
      ],
      [
        "hyper_tbl_cube"
      ],
      [
        "hyper_tibble"
      ],
      [
        "hyper_transforms"
      ],
      [
        "hyper_vars"
      ],
      [
        "nc_get"
      ],
      [
        "tidync"
      ]
    ],
    "topics": [],
    "score": 9.4351,
    "stars": 92,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "tidync A Tidy Approach to 'NetCDF' Data Exploration and Extraction Tidy tools for 'NetCDF' data sources. Explore the contents\nof a 'NetCDF' source (file or URL) presented as variables\norganized by grid with a database-like interface. The\nhyper_filter() interactive function translates the filter value\nor index expressions to array-slicing form. No data is read\nuntil explicitly requested, as a data frame or list of arrays\nvia hyper_tibble() or hyper_array(). %>% activate active active<- hyper_array hyper_dims hyper_filter hyper_grids hyper_slice hyper_tbl_cube hyper_tibble hyper_transforms hyper_vars nc_get tidync "
  },
  {
    "id": 210,
    "package_name": "UCSCXenaTools",
    "title": "Download and Explore Datasets from UCSC Xena Data Hubs",
    "description": "Download and explore datasets from UCSC Xena data hubs,\nwhich are a collection of UCSC-hosted public databases such as\nTCGA, ICGC, TARGET, GTEx, CCLE, and others.  Databases are\nnormalized so they can be combined, linked, filtered, explored\nand downloaded.",
    "version": "1.6.2",
    "maintainer": "Shixiang Wang <w_shixiang@163.com>",
    "author": "Shixiang Wang [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-9855-7357>),\nXue-Song Liu [aut] (ORCID: <https://orcid.org/0000-0002-7736-0077>),\nMartin Morgan [ctb],\nChristine Stawitz [rev] (Christine reviewed the package for ropensci,\nsee <https://github.com/ropensci/software-review/issues/315>),\nCarl Ganz [rev] (Carl reviewed the package for ropensci, see\n<https://github.com/ropensci/software-review/issues/315>)",
    "url": "https://docs.ropensci.org/UCSCXenaTools/,\nhttps://github.com/ropensci/UCSCXenaTools",
    "bug_reports": "https://github.com/ropensci/UCSCXenaTools/issues",
    "repository": "",
    "exports": [
      [
        ".XenaHub"
      ],
      [
        "%>%"
      ],
      [
        "availTCGA"
      ],
      [
        "cohorts"
      ],
      [
        "datasets"
      ],
      [
        "downloadTCGA"
      ],
      [
        "fetch"
      ],
      [
        "fetch_dataset_identifiers"
      ],
      [
        "fetch_dataset_samples"
      ],
      [
        "fetch_dense_values"
      ],
      [
        "fetch_sparse_values"
      ],
      [
        "getTCGAdata"
      ],
      [
        "has_probeMap"
      ],
      [
        "hosts"
      ],
      [
        "samples"
      ],
      [
        "showTCGA"
      ],
      [
        "to_snake"
      ],
      [
        "xena_default_hosts"
      ],
      [
        "XenaBrowse"
      ],
      [
        "XenaDataUpdate"
      ],
      [
        "XenaDownload"
      ],
      [
        "XenaFilter"
      ],
      [
        "XenaGenerate"
      ],
      [
        "XenaHub"
      ],
      [
        "XenaPrepare"
      ],
      [
        "XenaQuery"
      ],
      [
        "XenaQueryProbeMap"
      ],
      [
        "XenaScan"
      ]
    ],
    "topics": [
      [
        "api-client"
      ],
      [
        "bioinformatics"
      ],
      [
        "ccle"
      ],
      [
        "downloader"
      ],
      [
        "icgc"
      ],
      [
        "tcga"
      ],
      [
        "toil"
      ],
      [
        "treehouse"
      ],
      [
        "ucsc"
      ],
      [
        "ucsc-xena"
      ]
    ],
    "score": 9.0182,
    "stars": 111,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "UCSCXenaTools Download and Explore Datasets from UCSC Xena Data Hubs Download and explore datasets from UCSC Xena data hubs,\nwhich are a collection of UCSC-hosted public databases such as\nTCGA, ICGC, TARGET, GTEx, CCLE, and others.  Databases are\nnormalized so they can be combined, linked, filtered, explored\nand downloaded. .XenaHub %>% availTCGA cohorts datasets downloadTCGA fetch fetch_dataset_identifiers fetch_dataset_samples fetch_dense_values fetch_sparse_values getTCGAdata has_probeMap hosts samples showTCGA to_snake xena_default_hosts XenaBrowse XenaDataUpdate XenaDownload XenaFilter XenaGenerate XenaHub XenaPrepare XenaQuery XenaQueryProbeMap XenaScan api-client bioinformatics ccle downloader icgc tcga toil treehouse ucsc ucsc-xena"
  },
  {
    "id": 423,
    "package_name": "cranlogs",
    "title": "Download Logs from the 'RStudio' 'CRAN' Mirror",
    "description": "'API' to the database of 'CRAN' package downloads from the\n'RStudio' 'CRAN mirror'. The database itself is at\n<http://cranlogs.r-pkg.org>, see\n<https://github.com/r-hub/cranlogs.app> for the raw 'API'.",
    "version": "2.1.1.9000",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi [aut, cre],\nR Consortium [fnd]",
    "url": "https://github.com/r-hub/cranlogs,\nhttps://r-hub.github.io/cranlogs",
    "bug_reports": "https://github.com/r-hub/cranlogs/issues",
    "repository": "",
    "exports": [
      [
        "cran_downloads"
      ],
      [
        "cran_top_downloads"
      ],
      [
        "cranlogs_badge"
      ]
    ],
    "topics": [],
    "score": 8.8471,
    "stars": 87,
    "primary_category": "infrastructure",
    "source_universe": "r-hub",
    "search_text": "cranlogs Download Logs from the 'RStudio' 'CRAN' Mirror 'API' to the database of 'CRAN' package downloads from the\n'RStudio' 'CRAN mirror'. The database itself is at\n<http://cranlogs.r-pkg.org>, see\n<https://github.com/r-hub/cranlogs.app> for the raw 'API'. cran_downloads cran_top_downloads cranlogs_badge "
  },
  {
    "id": 982,
    "package_name": "pkgsearch",
    "title": "Search and Query CRAN R Packages",
    "description": "Search CRAN metadata about packages by keyword,\npopularity, recent activity, package name and more. Uses the\n'R-hub' search server, see <https://r-pkg.org> and the CRAN\nmetadata database, that contains information about CRAN\npackages. Note that this is _not_ a CRAN project.",
    "version": "3.1.5.9000",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi [aut, cre],\nMa\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nR Consortium [fnd]",
    "url": "https://github.com/r-hub/pkgsearch,\nhttps://r-hub.github.io/pkgsearch/",
    "bug_reports": "https://github.com/r-hub/pkgsearch/issues",
    "repository": "",
    "exports": [
      [
        "advanced_search"
      ],
      [
        "cran_events"
      ],
      [
        "cran_new"
      ],
      [
        "cran_package"
      ],
      [
        "cran_package_history"
      ],
      [
        "cran_packages"
      ],
      [
        "cran_top_downloaded"
      ],
      [
        "cran_trending"
      ],
      [
        "more"
      ],
      [
        "pkg_search"
      ],
      [
        "pkg_search_addin"
      ],
      [
        "ps"
      ]
    ],
    "topics": [
      [
        "ranking"
      ],
      [
        "search-engine"
      ]
    ],
    "score": 8.6717,
    "stars": 111,
    "primary_category": "infrastructure",
    "source_universe": "r-hub",
    "search_text": "pkgsearch Search and Query CRAN R Packages Search CRAN metadata about packages by keyword,\npopularity, recent activity, package name and more. Uses the\n'R-hub' search server, see <https://r-pkg.org> and the CRAN\nmetadata database, that contains information about CRAN\npackages. Note that this is _not_ a CRAN project. advanced_search cran_events cran_new cran_package cran_package_history cran_packages cran_top_downloaded cran_trending more pkg_search pkg_search_addin ps ranking search-engine"
  },
  {
    "id": 542,
    "package_name": "europepmc",
    "title": "R Interface to the Europe PubMed Central RESTful Web Service",
    "description": "An R Client for the Europe PubMed Central RESTful Web\nService (see <https://europepmc.org/RestfulWebService> for more\ninformation). It gives access to both metadata on life science\nliterature and open access full texts. Europe PMC indexes all\nPubMed content and other literature sources including Agricola,\na bibliographic database of citations to the agricultural\nliterature, or Biological Patents. In addition to bibliographic\nmetadata, the client allows users to fetch citations and\nreference lists. Links between life-science literature and\nother EBI databases, including ENA, PDB or ChEMBL are also\naccessible. No registration or API key is required. See the\nvignettes for usage examples.",
    "version": "0.4.3",
    "maintainer": "Najko Jahn <najko.jahn@gmail.com>",
    "author": "Najko Jahn [aut, cre, cph],\nMa\u00eblle Salmon [ctb]",
    "url": "https://docs.ropensci.org/europepmc/,\nhttps://github.com/ropensci/europepmc/",
    "bug_reports": "https://github.com/ropensci/europepmc/issues",
    "repository": "",
    "exports": [
      [
        "epmc_annotations_by_id"
      ],
      [
        "epmc_citations"
      ],
      [
        "epmc_db"
      ],
      [
        "epmc_db_count"
      ],
      [
        "epmc_details"
      ],
      [
        "epmc_ftxt"
      ],
      [
        "epmc_ftxt_book"
      ],
      [
        "epmc_hits"
      ],
      [
        "epmc_hits_trend"
      ],
      [
        "epmc_lablinks"
      ],
      [
        "epmc_lablinks_count"
      ],
      [
        "epmc_profile"
      ],
      [
        "epmc_refs"
      ],
      [
        "epmc_search"
      ],
      [
        "epmc_search_"
      ],
      [
        "epmc_search_by_doi"
      ],
      [
        "epmc_search_by_doi_"
      ]
    ],
    "topics": [
      [
        "bibliometrics"
      ],
      [
        "europe-pmc"
      ],
      [
        "pubmed"
      ],
      [
        "pubmedcentral"
      ],
      [
        "scientific-literature"
      ],
      [
        "scientific-publications"
      ]
    ],
    "score": 8.6195,
    "stars": 28,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "europepmc R Interface to the Europe PubMed Central RESTful Web Service An R Client for the Europe PubMed Central RESTful Web\nService (see <https://europepmc.org/RestfulWebService> for more\ninformation). It gives access to both metadata on life science\nliterature and open access full texts. Europe PMC indexes all\nPubMed content and other literature sources including Agricola,\na bibliographic database of citations to the agricultural\nliterature, or Biological Patents. In addition to bibliographic\nmetadata, the client allows users to fetch citations and\nreference lists. Links between life-science literature and\nother EBI databases, including ENA, PDB or ChEMBL are also\naccessible. No registration or API key is required. See the\nvignettes for usage examples. epmc_annotations_by_id epmc_citations epmc_db epmc_db_count epmc_details epmc_ftxt epmc_ftxt_book epmc_hits epmc_hits_trend epmc_lablinks epmc_lablinks_count epmc_profile epmc_refs epmc_search epmc_search_ epmc_search_by_doi epmc_search_by_doi_ bibliometrics europe-pmc pubmed pubmedcentral scientific-literature scientific-publications"
  },
  {
    "id": 487,
    "package_name": "dittodb",
    "title": "A Test Environment for Database Requests",
    "description": "Testing and documenting code that communicates with remote\ndatabases can be painful. Although the interaction with R is\nusually relatively simple (e.g. data(frames) passed to and from\na database), because they rely on a separate service and the\ndata there, testing them can be difficult to set up,\nunsustainable in a continuous integration environment, or\nimpossible without replicating an entire production cluster.\nThis package addresses that by allowing you to make recordings\nfrom your database interactions and then play them back while\ntesting (or in other contexts) all without needing to spin up\nor have access to the database your code would typically\nconnect to.",
    "version": "0.1.9.9000",
    "maintainer": "Jonathan Keane <jkeane@gmail.com>",
    "author": "Jonathan Keane [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-7087-9776>),\nMauricio Vargas [aut] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\nHelen Miller [rev] (reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/366),\nEtienne Racine [rev] (reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/366)",
    "url": "https://dittodb.jonkeane.com/, https://github.com/ropensci/dittodb",
    "bug_reports": "https://github.com/ropensci/dittodb/issues",
    "repository": "",
    "exports": [
      [
        ".db_mock_paths"
      ],
      [
        ".dittodb_env"
      ],
      [
        "capture_db_requests"
      ],
      [
        "check_db_path"
      ],
      [
        "check_for_pkg"
      ],
      [
        "clean_statement"
      ],
      [
        "db_mock_paths"
      ],
      [
        "db_path_sanitize"
      ],
      [
        "dbBegin"
      ],
      [
        "dbClearResult"
      ],
      [
        "dbColumnInfo"
      ],
      [
        "dbCommit"
      ],
      [
        "dbDisconnect"
      ],
      [
        "dbExistsTable"
      ],
      [
        "dbFetch"
      ],
      [
        "dbGetInfo"
      ],
      [
        "dbGetQuery"
      ],
      [
        "dbGetRowsAffected"
      ],
      [
        "dbHasCompleted"
      ],
      [
        "dbListFields"
      ],
      [
        "dbListTables"
      ],
      [
        "dbMockConnect"
      ],
      [
        "dbQuoteIdentifier"
      ],
      [
        "dbQuoteString"
      ],
      [
        "dbRemoveTable"
      ],
      [
        "dbRollback"
      ],
      [
        "dbSendQuery"
      ],
      [
        "dbSendStatement"
      ],
      [
        "dbWriteTable"
      ],
      [
        "dittodb_debug_level"
      ],
      [
        "expect_sql"
      ],
      [
        "fetch"
      ],
      [
        "get_dbname"
      ],
      [
        "get_redactor"
      ],
      [
        "get_type"
      ],
      [
        "hash"
      ],
      [
        "hash_db_object"
      ],
      [
        "make_path"
      ],
      [
        "nycflights_sqlite"
      ],
      [
        "nycflights13_create_sql"
      ],
      [
        "nycflights13_create_sqlite"
      ],
      [
        "redact_columns"
      ],
      [
        "sanitize_table_id"
      ],
      [
        "serialize_bit64"
      ],
      [
        "set_dittodb_debug_level"
      ],
      [
        "start_db_capturing"
      ],
      [
        "start_mock_db"
      ],
      [
        "stop_db_capturing"
      ],
      [
        "stop_mock_db"
      ],
      [
        "use_dittodb"
      ],
      [
        "with_mock_db"
      ],
      [
        "with_mock_path"
      ]
    ],
    "topics": [],
    "score": 8.5756,
    "stars": 81,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "dittodb A Test Environment for Database Requests Testing and documenting code that communicates with remote\ndatabases can be painful. Although the interaction with R is\nusually relatively simple (e.g. data(frames) passed to and from\na database), because they rely on a separate service and the\ndata there, testing them can be difficult to set up,\nunsustainable in a continuous integration environment, or\nimpossible without replicating an entire production cluster.\nThis package addresses that by allowing you to make recordings\nfrom your database interactions and then play them back while\ntesting (or in other contexts) all without needing to spin up\nor have access to the database your code would typically\nconnect to. .db_mock_paths .dittodb_env capture_db_requests check_db_path check_for_pkg clean_statement db_mock_paths db_path_sanitize dbBegin dbClearResult dbColumnInfo dbCommit dbDisconnect dbExistsTable dbFetch dbGetInfo dbGetQuery dbGetRowsAffected dbHasCompleted dbListFields dbListTables dbMockConnect dbQuoteIdentifier dbQuoteString dbRemoveTable dbRollback dbSendQuery dbSendStatement dbWriteTable dittodb_debug_level expect_sql fetch get_dbname get_redactor get_type hash hash_db_object make_path nycflights_sqlite nycflights13_create_sql nycflights13_create_sqlite redact_columns sanitize_table_id serialize_bit64 set_dittodb_debug_level start_db_capturing start_mock_db stop_db_capturing stop_mock_db use_dittodb with_mock_db with_mock_path "
  },
  {
    "id": 448,
    "package_name": "dbparser",
    "title": "Drugs Databases Parser",
    "description": "This tool is for parsing public drug databases such as\n'DrugBank' XML database <https://go.drugbank.com/>. The parsed\ndata are then returned in a proper 'R' object called\n'dvobject'.",
    "version": "2.0.3",
    "maintainer": "Mohammed Ali <moh_fcis@yahoo.com>",
    "author": "Mohammed Ali [aut, cre],\nAli Ezzat [aut],\nHao Zhu [rev],\nEmma Mendelsohn [rev]",
    "url": "https://docs.ropensci.org/dbparser/,\nhttps://github.com/ropensci/dbparser",
    "bug_reports": "https://github.com/ropensci/dbparser/issues",
    "repository": "",
    "exports": [
      [
        "cett_nodes_options"
      ],
      [
        "drug_node_options"
      ],
      [
        "parseDrugBank"
      ],
      [
        "references_node_options"
      ],
      [
        "show_dvobject_metadata"
      ]
    ],
    "topics": [],
    "score": 8.3327,
    "stars": 63,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "dbparser Drugs Databases Parser This tool is for parsing public drug databases such as\n'DrugBank' XML database <https://go.drugbank.com/>. The parsed\ndata are then returned in a proper 'R' object called\n'dvobject'. cett_nodes_options drug_node_options parseDrugBank references_node_options show_dvobject_metadata "
  },
  {
    "id": 1170,
    "package_name": "rvertnet",
    "title": "Search 'Vertnet', a 'Database' of Vertebrate Specimen Records",
    "description": "Retrieve, map and summarize data from the 'VertNet.org'\narchives (<https://vertnet.org/>).  Functions allow searching\nby many parameters, including 'taxonomic' names, places, and\ndates. In addition, there is an interface for conducting\nspatially delimited searches, and another for requesting large\n'datasets' via email.",
    "version": "0.8.4.9000",
    "maintainer": "Dave Slager <slager@uw.edu>",
    "author": "Scott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nChris Ray [ctb],\nVijay Barve [ctb],\nDave Slager [ctb, cre] (ORCID: <https://orcid.org/0000-0003-2525-2039>)",
    "url": "https://github.com/ropensci/rvertnet,\nhttps://docs.ropensci.org/rvertnet/",
    "bug_reports": "https://github.com/ropensci/rvertnet/issues",
    "repository": "",
    "exports": [
      [
        "bigsearch"
      ],
      [
        "dump_init"
      ],
      [
        "dump_links"
      ],
      [
        "dump_tbl"
      ],
      [
        "searchbyterm"
      ],
      [
        "spatialsearch"
      ],
      [
        "traitsearch"
      ],
      [
        "vert_id"
      ],
      [
        "vertavailablemaps"
      ],
      [
        "vertlocations"
      ],
      [
        "vertmap"
      ],
      [
        "vertoccurrence"
      ],
      [
        "vertoccurrencecount"
      ],
      [
        "vertproviders"
      ],
      [
        "vertsearch"
      ],
      [
        "vertsummary"
      ],
      [
        "verttaxa"
      ]
    ],
    "topics": [
      [
        "species"
      ],
      [
        "occurrences"
      ],
      [
        "biodiversity"
      ],
      [
        "maps"
      ],
      [
        "vertnet"
      ],
      [
        "mammals"
      ],
      [
        "mammalia"
      ],
      [
        "specimens"
      ],
      [
        "api-wrapper"
      ],
      [
        "specimen"
      ],
      [
        "spocc"
      ]
    ],
    "score": 8.2542,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rvertnet Search 'Vertnet', a 'Database' of Vertebrate Specimen Records Retrieve, map and summarize data from the 'VertNet.org'\narchives (<https://vertnet.org/>).  Functions allow searching\nby many parameters, including 'taxonomic' names, places, and\ndates. In addition, there is an interface for conducting\nspatially delimited searches, and another for requesting large\n'datasets' via email. bigsearch dump_init dump_links dump_tbl searchbyterm spatialsearch traitsearch vert_id vertavailablemaps vertlocations vertmap vertoccurrence vertoccurrencecount vertproviders vertsearch vertsummary verttaxa species occurrences biodiversity maps vertnet mammals mammalia specimens api-wrapper specimen spocc"
  },
  {
    "id": 1305,
    "package_name": "taxadb",
    "title": "A High-Performance Local Taxonomic Database Interface",
    "description": "Creates a local database of many commonly used taxonomic\nauthorities and provides functions that can quickly query this\ndata.",
    "version": "0.2.1.99",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nKari Norman [aut] (ORCID: <https://orcid.org/0000-0002-2029-2325>),\nJorrit Poelen [aut] (ORCID: <https://orcid.org/0000-0003-3138-4118>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nNoam Ross [ctb] (ORCID: <https://orcid.org/0000-0002-2136-0000>),\nMattia Ghilardi [ctb] (ORCID: <https://orcid.org/0000-0001-9592-7252>)",
    "url": "<https://docs.ropensci.org/taxadb/>,\n<https://github.com/ropensci/taxadb>",
    "bug_reports": "https://github.com/ropensci/taxadb/issues",
    "repository": "",
    "exports": [
      [
        "clean_names"
      ],
      [
        "common_contains"
      ],
      [
        "common_starts_with"
      ],
      [
        "filter_by"
      ],
      [
        "filter_common"
      ],
      [
        "filter_id"
      ],
      [
        "filter_name"
      ],
      [
        "filter_rank"
      ],
      [
        "fuzzy_filter"
      ],
      [
        "get_ids"
      ],
      [
        "get_names"
      ],
      [
        "name_contains"
      ],
      [
        "name_starts_with"
      ],
      [
        "taxa_tbl"
      ],
      [
        "taxadb_dir"
      ],
      [
        "td_connect"
      ],
      [
        "td_create"
      ],
      [
        "td_disconnect"
      ],
      [
        "tl_import"
      ]
    ],
    "topics": [],
    "score": 8.2336,
    "stars": 43,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxadb A High-Performance Local Taxonomic Database Interface Creates a local database of many commonly used taxonomic\nauthorities and provides functions that can quickly query this\ndata. clean_names common_contains common_starts_with filter_by filter_common filter_id filter_name filter_rank fuzzy_filter get_ids get_names name_contains name_starts_with taxa_tbl taxadb_dir td_connect td_create td_disconnect tl_import "
  },
  {
    "id": 884,
    "package_name": "nodbi",
    "title": "'NoSQL' Database Connector",
    "description": "Simplified JSON document database access and manipulation,\nproviding a common API across supported 'NoSQL' databases\n'Elasticsearch', 'CouchDB', 'MongoDB' as well as\n'SQLite/JSON1', 'PostgreSQL', and 'DuckDB'.",
    "version": "0.14.0",
    "maintainer": "Ralf Herold <ralf.herold@mailbox.org>",
    "author": "Ralf Herold [aut, cre] (ORCID: <https://orcid.org/0000-0002-8148-6748>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nRich FitzJohn [aut],\nJeroen Ooms [aut],\nIvan Tarbakou [cph] (mongo-to-sql-converter library)",
    "url": "https://docs.ropensci.org/nodbi/,\nhttps://github.com/ropensci/nodbi",
    "bug_reports": "https://github.com/ropensci/nodbi/issues",
    "repository": "",
    "exports": [
      [
        "contacts"
      ],
      [
        "docdb_create"
      ],
      [
        "docdb_delete"
      ],
      [
        "docdb_exists"
      ],
      [
        "docdb_get"
      ],
      [
        "docdb_list"
      ],
      [
        "docdb_query"
      ],
      [
        "docdb_update"
      ],
      [
        "mapdata"
      ],
      [
        "src_couchdb"
      ],
      [
        "src_duckdb"
      ],
      [
        "src_elastic"
      ],
      [
        "src_mongo"
      ],
      [
        "src_postgres"
      ],
      [
        "src_sqlite"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "mongodb"
      ],
      [
        "elasticsearch"
      ],
      [
        "couchdb"
      ],
      [
        "sqlite"
      ],
      [
        "postgresql"
      ],
      [
        "duckdb"
      ],
      [
        "nosql"
      ],
      [
        "json"
      ],
      [
        "documents"
      ]
    ],
    "score": 8.1985,
    "stars": 75,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "nodbi 'NoSQL' Database Connector Simplified JSON document database access and manipulation,\nproviding a common API across supported 'NoSQL' databases\n'Elasticsearch', 'CouchDB', 'MongoDB' as well as\n'SQLite/JSON1', 'PostgreSQL', and 'DuckDB'. contacts docdb_create docdb_delete docdb_exists docdb_get docdb_list docdb_query docdb_update mapdata src_couchdb src_duckdb src_elastic src_mongo src_postgres src_sqlite database mongodb elasticsearch couchdb sqlite postgresql duckdb nosql json documents"
  },
  {
    "id": 983,
    "package_name": "pkgstats",
    "title": "Metrics of R Packages",
    "description": "Static code analyses for R packages using the external\ncode-tagging libraries 'ctags' and 'gtags'. Static analyses\nenable packages to be analysed very quickly, generally a couple\nof seconds at most. The package also provides access to a\ndatabase generating by applying the main function to the full\n'CRAN' archive, enabling the statistical properties of any\npackage to be compared with all other 'CRAN' packages.",
    "version": "0.2.1.005",
    "maintainer": "Mark Padgham <mark.padgham@email.com>",
    "author": "Mark Padgham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2172-5265>),\nMichael Sumner [ctb] (ORCID: <https://orcid.org/0000-0002-2471-7511>),\nJeffrey Hollister [ctb] (ORCID:\n<https://orcid.org/0000-0002-9254-9740>),\nEgor Kotov [ctb] (ORCID: <https://orcid.org/0000-0001-6690-5345>)",
    "url": "https://docs.ropensci.org/pkgstats/,\nhttps://github.com/ropensci-review-tools/pkgstats",
    "bug_reports": "https://github.com/ropensci-review-tools/pkgstats/issues",
    "repository": "",
    "exports": [
      [
        "ctags_install"
      ],
      [
        "ctags_test"
      ],
      [
        "desc_stats"
      ],
      [
        "dl_pkgstats_data"
      ],
      [
        "extract_tarball"
      ],
      [
        "loc_stats"
      ],
      [
        "pkgstats"
      ],
      [
        "pkgstats_cran_current_from_full"
      ],
      [
        "pkgstats_fn_names"
      ],
      [
        "pkgstats_fns_from_archive"
      ],
      [
        "pkgstats_fns_update"
      ],
      [
        "pkgstats_from_archive"
      ],
      [
        "pkgstats_summary"
      ],
      [
        "pkgstats_update"
      ],
      [
        "plot_network"
      ],
      [
        "rd_stats"
      ],
      [
        "tags_data"
      ]
    ],
    "topics": [
      [
        "code-analysis"
      ],
      [
        "code-statistics"
      ],
      [
        "software-analysis"
      ],
      [
        "cpp"
      ]
    ],
    "score": 8.1004,
    "stars": 20,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "pkgstats Metrics of R Packages Static code analyses for R packages using the external\ncode-tagging libraries 'ctags' and 'gtags'. Static analyses\nenable packages to be analysed very quickly, generally a couple\nof seconds at most. The package also provides access to a\ndatabase generating by applying the main function to the full\n'CRAN' archive, enabling the statistical properties of any\npackage to be compared with all other 'CRAN' packages. ctags_install ctags_test desc_stats dl_pkgstats_data extract_tarball loc_stats pkgstats pkgstats_cran_current_from_full pkgstats_fn_names pkgstats_fns_from_archive pkgstats_fns_update pkgstats_from_archive pkgstats_summary pkgstats_update plot_network rd_stats tags_data code-analysis code-statistics software-analysis cpp"
  },
  {
    "id": 60,
    "package_name": "FedData",
    "title": "Download Geospatial Data Available from Several Federated Data\nSources",
    "description": "Download geospatial data available from several federated\ndata sources (mainly sources maintained by the US Federal\ngovernment). Currently, the package enables extraction from\nnine datasets: The National Elevation Dataset digital elevation\nmodels (<https://www.usgs.gov/3d-elevation-program> 1 and 1/3\narc-second; USGS); The National Hydrography Dataset\n(<https://www.usgs.gov/national-hydrography/national-hydrography-dataset>;\nUSGS); The Soil Survey Geographic (SSURGO) database from the\nNational Cooperative Soil Survey\n(<https://websoilsurvey.sc.egov.usda.gov/>; NCSS), which is led\nby the Natural Resources Conservation Service (NRCS) under the\nUSDA; the Global Historical Climatology Network\n(<https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily>;\nGHCN), coordinated by National Climatic Data Center at NOAA;\nthe Daymet gridded estimates of daily weather parameters for\nNorth America, version 4, available from the Oak Ridge National\nLaboratory's Distributed Active Archive Center\n(<https://daymet.ornl.gov/>; DAAC); the International Tree Ring\nData Bank; the National Land Cover Database\n(<https://www.mrlc.gov/>; NLCD); the Cropland Data Layer from\nthe National Agricultural Statistics Service\n(<https://www.nass.usda.gov/Research_and_Science/Cropland/SARS1a.php>;\nNASS); and the PAD-US dataset of protected area boundaries\n(<https://www.usgs.gov/programs/gap-analysis-project/science/pad-us-data-overview>;\nUSGS).",
    "version": "4.3.0",
    "maintainer": "R. Kyle Bocinsky <bocinsky@gmail.com>",
    "author": "R. Kyle Bocinsky [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-1862-3428>),\nDylan Beaudette [ctb],\nScott Chamberlain [ctb, rev],\nJeffrey Hollister [ctb],\nJulia Gustavsen [rev]",
    "url": "https://docs.ropensci.org/FedData/,\nhttps://github.com/ropensci/FedData",
    "bug_reports": "https://github.com/ropensci/FedData/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "agol_filter"
      ],
      [
        "agol_filter_httr"
      ],
      [
        "cdl_colors"
      ],
      [
        "check_service"
      ],
      [
        "download_data"
      ],
      [
        "download_ghcn_daily_station"
      ],
      [
        "download_itrdb"
      ],
      [
        "download_ssurgo_inventory"
      ],
      [
        "download_ssurgo_study_area"
      ],
      [
        "extract_ssurgo_data"
      ],
      [
        "get_cdl"
      ],
      [
        "get_daymet"
      ],
      [
        "get_ghcn_daily"
      ],
      [
        "get_ghcn_daily_station"
      ],
      [
        "get_ghcn_inventory"
      ],
      [
        "get_itrdb"
      ],
      [
        "get_nass"
      ],
      [
        "get_nass_cdl"
      ],
      [
        "get_ned"
      ],
      [
        "get_ned_tile"
      ],
      [
        "get_nhd"
      ],
      [
        "get_nlcd"
      ],
      [
        "get_nlcd_annual"
      ],
      [
        "get_padus"
      ],
      [
        "get_ssurgo"
      ],
      [
        "get_ssurgo_inventory"
      ],
      [
        "get_ssurgo_study_area"
      ],
      [
        "get_wbd"
      ],
      [
        "nlcd_colors"
      ],
      [
        "pal_nlcd"
      ],
      [
        "plot_nhd"
      ],
      [
        "polygon_from_extent"
      ],
      [
        "read_crn"
      ],
      [
        "read_crn_data"
      ],
      [
        "read_crn_metadata"
      ],
      [
        "replace_null"
      ],
      [
        "sequential_duplicated"
      ],
      [
        "split_bbox"
      ],
      [
        "station_to_data_frame"
      ],
      [
        "substr_right"
      ],
      [
        "unwrap_rows"
      ],
      [
        "url_base"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ]
    ],
    "score": 7.8617,
    "stars": 104,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "FedData Download Geospatial Data Available from Several Federated Data\nSources Download geospatial data available from several federated\ndata sources (mainly sources maintained by the US Federal\ngovernment). Currently, the package enables extraction from\nnine datasets: The National Elevation Dataset digital elevation\nmodels (<https://www.usgs.gov/3d-elevation-program> 1 and 1/3\narc-second; USGS); The National Hydrography Dataset\n(<https://www.usgs.gov/national-hydrography/national-hydrography-dataset>;\nUSGS); The Soil Survey Geographic (SSURGO) database from the\nNational Cooperative Soil Survey\n(<https://websoilsurvey.sc.egov.usda.gov/>; NCSS), which is led\nby the Natural Resources Conservation Service (NRCS) under the\nUSDA; the Global Historical Climatology Network\n(<https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily>;\nGHCN), coordinated by National Climatic Data Center at NOAA;\nthe Daymet gridded estimates of daily weather parameters for\nNorth America, version 4, available from the Oak Ridge National\nLaboratory's Distributed Active Archive Center\n(<https://daymet.ornl.gov/>; DAAC); the International Tree Ring\nData Bank; the National Land Cover Database\n(<https://www.mrlc.gov/>; NLCD); the Cropland Data Layer from\nthe National Agricultural Statistics Service\n(<https://www.nass.usda.gov/Research_and_Science/Cropland/SARS1a.php>;\nNASS); and the PAD-US dataset of protected area boundaries\n(<https://www.usgs.gov/programs/gap-analysis-project/science/pad-us-data-overview>;\nUSGS). %>% agol_filter agol_filter_httr cdl_colors check_service download_data download_ghcn_daily_station download_itrdb download_ssurgo_inventory download_ssurgo_study_area extract_ssurgo_data get_cdl get_daymet get_ghcn_daily get_ghcn_daily_station get_ghcn_inventory get_itrdb get_nass get_nass_cdl get_ned get_ned_tile get_nhd get_nlcd get_nlcd_annual get_padus get_ssurgo get_ssurgo_inventory get_ssurgo_study_area get_wbd nlcd_colors pal_nlcd plot_nhd polygon_from_extent read_crn read_crn_data read_crn_metadata replace_null sequential_duplicated split_bbox station_to_data_frame substr_right unwrap_rows url_base peer-reviewed"
  },
  {
    "id": 1072,
    "package_name": "readepi",
    "title": "Read Data from Relational Database Management Systems and Health\nInformation Systems",
    "description": "Import Data from Relational Database Management Systems\n(RDBMS) and Health Information Systems ('HIS'). The current\nversion of the package supports importing data from RDBMS\nincluding 'MS SQL', 'MySQL', 'PostGRESQL', and 'SQLite', as\nwell as from two HIS platforms: 'DHIS2' and 'SORMAS'.",
    "version": "1.0.3.9000",
    "maintainer": "Bubacarr Bah <Bubacarr.Bah1@lshtm.ac.uk>",
    "author": "Karim Man\u00e9 [aut] (ORCID: <https://orcid.org/0000-0002-9892-2999>),\nEmmanuel Kabuga [aut] (ORCID: <https://orcid.org/0009-0006-2291-709X>),\nBubacarr Bah [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3318-6668>),\nBankol\u00e9 Ahadzie [ctb],\nNuredin Mohammed [ctb],\nAbdoelnaser Degoot [ctb],\nHugo Gruson [rev] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nPratik Gupte [rev] (ORCID: <https://orcid.org/0000-0001-5294-7819>),\nAndree Valle-Campos [rev] (ORCID:\n<https://orcid.org/0000-0002-7779-481X>),\nLondon School of Hygiene and Tropical Medicine, LSHTM [cph],\ndata.org [fnd]",
    "url": "https://epiverse-trace.github.io/readepi/,\nhttps://github.com/epiverse-trace/readepi/,\nhttps://github.com/epiverse-trace/readepi",
    "bug_reports": "https://github.com/epiverse-trace/readepi/issues",
    "repository": "",
    "exports": [
      [
        "dhis2_login"
      ],
      [
        "get_api_version"
      ],
      [
        "get_data_elements"
      ],
      [
        "get_organisation_units"
      ],
      [
        "get_program_org_units"
      ],
      [
        "get_program_stages"
      ],
      [
        "get_programs"
      ],
      [
        "get_tracked_entities"
      ],
      [
        "login"
      ],
      [
        "read_dhis2"
      ],
      [
        "read_rdbms"
      ],
      [
        "read_sormas"
      ],
      [
        "show_tables"
      ],
      [
        "sormas_get_api_specification"
      ],
      [
        "sormas_get_data_dictionary"
      ],
      [
        "sormas_get_diseases"
      ]
    ],
    "topics": [
      [
        "data-import"
      ],
      [
        "epidemiology"
      ],
      [
        "epiverse"
      ],
      [
        "health-information-systems"
      ]
    ],
    "score": 7.8485,
    "stars": 6,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "readepi Read Data from Relational Database Management Systems and Health\nInformation Systems Import Data from Relational Database Management Systems\n(RDBMS) and Health Information Systems ('HIS'). The current\nversion of the package supports importing data from RDBMS\nincluding 'MS SQL', 'MySQL', 'PostGRESQL', and 'SQLite', as\nwell as from two HIS platforms: 'DHIS2' and 'SORMAS'. dhis2_login get_api_version get_data_elements get_organisation_units get_program_org_units get_program_stages get_programs get_tracked_entities login read_dhis2 read_rdbms read_sormas show_tables sormas_get_api_specification sormas_get_data_dictionary sormas_get_diseases data-import epidemiology epiverse health-information-systems"
  },
  {
    "id": 831,
    "package_name": "modeldb",
    "title": "Fits Models Inside the Database",
    "description": "Uses 'dplyr' and 'tidyeval' to fit statistical models\ninside the database. It currently supports KMeans and linear\nregression models.",
    "version": "0.3.1.9000",
    "maintainer": "Max Kuhn <max@posit.co>",
    "author": "Edgar Ruiz [aut],\nMax Kuhn [aut, cre]",
    "url": "https://modeldb.tidymodels.org,\nhttps://github.com/tidymodels/modeldb",
    "bug_reports": "https://github.com/tidymodels/modeldb/issues",
    "repository": "",
    "exports": [
      [
        "add_dummy_variables"
      ],
      [
        "as_parsed_model"
      ],
      [
        "db_calculate_squares"
      ],
      [
        "linear_regression_db"
      ],
      [
        "plot_kmeans"
      ],
      [
        "simple_kmeans_db"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "dbplyr"
      ],
      [
        "dplyr"
      ],
      [
        "ggplot2"
      ],
      [
        "modeling"
      ],
      [
        "rlang"
      ],
      [
        "sql"
      ],
      [
        "tidyeval"
      ],
      [
        "visualization"
      ]
    ],
    "score": 7.586,
    "stars": 79,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "modeldb Fits Models Inside the Database Uses 'dplyr' and 'tidyeval' to fit statistical models\ninside the database. It currently supports KMeans and linear\nregression models. add_dummy_variables as_parsed_model db_calculate_squares linear_regression_db plot_kmeans simple_kmeans_db database dbplyr dplyr ggplot2 modeling rlang sql tidyeval visualization"
  },
  {
    "id": 1064,
    "package_name": "rdataretriever",
    "title": "R Interface to the Data Retriever",
    "description": "Provides an R interface to the Data Retriever\n<https://retriever.readthedocs.io/en/latest/> via the Data\nRetriever's command line interface. The Data Retriever\nautomates the tasks of finding, downloading, and cleaning\npublic datasets, and then stores them in a local database.",
    "version": "3.1.1",
    "maintainer": "Henry Senyondo <henrykironde@gmail.com>",
    "author": "Henry Senyondo [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-7105-5808>),\nDaniel McGlinn [aut] (ORCID: <https://orcid.org/0000-0003-2359-3526>),\nPranita Sharma [aut] (ORCID: <https://orcid.org/0000-0002-5871-380X>),\nDavid J Harris [aut] (ORCID: <https://orcid.org/0000-0003-3332-9307>),\nHao Ye [aut] (ORCID: <https://orcid.org/0000-0002-8630-1458>),\nShawn Taylor [aut] (ORCID: <https://orcid.org/0000-0002-6178-6903>),\nJeroen Ooms [aut] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nFrancisco Rodr\u00edguez-S\u00e1nchez Ooms [aut] (ORCID:\n<https://orcid.org/0000-0002-7981-1599>),\nKarthik Ram [aut] (ORCID: <https://orcid.org/0000-0002-0233-1757>),\nApoorva Pandey [aut] (ORCID: <https://orcid.org/0000-0001-9834-4415>),\nHarshit Bansal [aut] (ORCID: <https://orcid.org/0000-0002-3285-812X>),\nMax Pohlman [aut] (ORCID: <https://orcid.org/0000-0003-2325-6984>),\nEthan White [aut] (ORCID: <https://orcid.org/0000-0001-6728-7745>)",
    "url": "https://docs.ropensci.org/rdataretriever/ (website),\nhttps://github.com/ropensci/rdataretriever/",
    "bug_reports": "https://github.com/ropensci/rdataretriever/issues",
    "repository": "",
    "exports": [
      [
        "check_for_updates"
      ],
      [
        "check_retriever_availability"
      ],
      [
        "commit"
      ],
      [
        "commit_log"
      ],
      [
        "data_retriever_version"
      ],
      [
        "dataset_names"
      ],
      [
        "datasets"
      ],
      [
        "display_all_rdataset_names"
      ],
      [
        "download"
      ],
      [
        "fetch"
      ],
      [
        "find_socrata_dataset_by_id"
      ],
      [
        "get_dataset_names_upstream"
      ],
      [
        "get_rdataset_names"
      ],
      [
        "get_retriever_citation"
      ],
      [
        "get_script_citation"
      ],
      [
        "get_script_upstream"
      ],
      [
        "get_updates"
      ],
      [
        "install"
      ],
      [
        "install_csv"
      ],
      [
        "install_json"
      ],
      [
        "install_msaccess"
      ],
      [
        "install_mysql"
      ],
      [
        "install_postgres"
      ],
      [
        "install_retriever"
      ],
      [
        "install_sqlite"
      ],
      [
        "install_xml"
      ],
      [
        "reload_scripts"
      ],
      [
        "reset"
      ],
      [
        "socrata_autocomplete_search"
      ],
      [
        "socrata_dataset_info"
      ],
      [
        "update_rdataset_catalog"
      ],
      [
        "use_RetrieverPath"
      ]
    ],
    "topics": [
      [
        "data"
      ],
      [
        "data-science"
      ],
      [
        "database"
      ],
      [
        "datasets"
      ],
      [
        "science"
      ]
    ],
    "score": 7.4502,
    "stars": 47,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rdataretriever R Interface to the Data Retriever Provides an R interface to the Data Retriever\n<https://retriever.readthedocs.io/en/latest/> via the Data\nRetriever's command line interface. The Data Retriever\nautomates the tasks of finding, downloading, and cleaning\npublic datasets, and then stores them in a local database. check_for_updates check_retriever_availability commit commit_log data_retriever_version dataset_names datasets display_all_rdataset_names download fetch find_socrata_dataset_by_id get_dataset_names_upstream get_rdataset_names get_retriever_citation get_script_citation get_script_upstream get_updates install install_csv install_json install_msaccess install_mysql install_postgres install_retriever install_sqlite install_xml reload_scripts reset socrata_autocomplete_search socrata_dataset_info update_rdataset_catalog use_RetrieverPath data data-science database datasets science"
  },
  {
    "id": 729,
    "package_name": "jsonld",
    "title": "JSON for Linking Data",
    "description": "JSON-LD <https://www.w3.org/TR/json-ld/> is a light-weight\nsyntax for expressing linked data. It is primarily intended for\nweb-based programming environments, interoperable web services\nand for storing linked data in JSON-based databases. This\npackage provides bindings to the JavaScript library for\nconverting, expanding and compacting JSON-LD documents.",
    "version": "2.2.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://docs.ropensci.org/jsonld/,\nhttps://ropensci.r-universe.dev/jsonld",
    "bug_reports": "https://github.com/ropensci/jsonld/issues",
    "repository": "",
    "exports": [
      [
        "jsonld_compact"
      ],
      [
        "jsonld_expand"
      ],
      [
        "jsonld_flatten"
      ],
      [
        "jsonld_frame"
      ],
      [
        "jsonld_from_rdf"
      ],
      [
        "jsonld_normalize"
      ],
      [
        "jsonld_to_rdf"
      ]
    ],
    "topics": [
      [
        "json-ld"
      ]
    ],
    "score": 7.4136,
    "stars": 35,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "jsonld JSON for Linking Data JSON-LD <https://www.w3.org/TR/json-ld/> is a light-weight\nsyntax for expressing linked data. It is primarily intended for\nweb-based programming environments, interoperable web services\nand for storing linked data in JSON-based databases. This\npackage provides bindings to the JavaScript library for\nconverting, expanding and compacting JSON-LD documents. jsonld_compact jsonld_expand jsonld_flatten jsonld_frame jsonld_from_rdf jsonld_normalize jsonld_to_rdf json-ld"
  },
  {
    "id": 251,
    "package_name": "arkdb",
    "title": "Archive and Unarchive Databases Using Flat Files",
    "description": "Flat text files provide a robust, compressible, and\nportable way to store tables from databases.  This package\nprovides convenient functions for exporting tables from\nrelational database connections into compressed text files and\nstreaming those text files back into a database without\nrequiring the whole table to fit in working memory.",
    "version": "0.0.18",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nRichard FitzJohn [ctb],\nBrandon Bertelsen [ctb]",
    "url": "https://github.com/ropensci/arkdb,\nhttps://docs.ropensci.org/arkdb/",
    "bug_reports": "https://github.com/ropensci/arkdb/issues",
    "repository": "",
    "exports": [
      [
        "ark"
      ],
      [
        "arkdb_delete_db"
      ],
      [
        "local_db"
      ],
      [
        "local_db_disconnect"
      ],
      [
        "process_chunks"
      ],
      [
        "streamable_base_csv"
      ],
      [
        "streamable_base_tsv"
      ],
      [
        "streamable_parquet"
      ],
      [
        "streamable_readr_csv"
      ],
      [
        "streamable_readr_tsv"
      ],
      [
        "streamable_table"
      ],
      [
        "streamable_vroom"
      ],
      [
        "unark"
      ]
    ],
    "topics": [
      [
        "archiving"
      ],
      [
        "database"
      ],
      [
        "dbi"
      ],
      [
        "peer-reviewed"
      ]
    ],
    "score": 7.3045,
    "stars": 80,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "arkdb Archive and Unarchive Databases Using Flat Files Flat text files provide a robust, compressible, and\nportable way to store tables from databases.  This package\nprovides convenient functions for exporting tables from\nrelational database connections into compressed text files and\nstreaming those text files back into a database without\nrequiring the whole table to fit in working memory. ark arkdb_delete_db local_db local_db_disconnect process_chunks streamable_base_csv streamable_base_tsv streamable_parquet streamable_readr_csv streamable_readr_tsv streamable_table streamable_vroom unark archiving database dbi peer-reviewed"
  },
  {
    "id": 1186,
    "package_name": "sedonadb",
    "title": "Bindings for Apache SedonaDB",
    "description": "Provides bindings for Apache SedonaDB, a lightweight query\nengine optimized for spatial workflows.",
    "version": "0.2.0.9000",
    "maintainer": "Dewey Dunnington <dewey@dunnington.ca>",
    "author": "Dewey Dunnington [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "as_sedonadb_dataframe"
      ],
      [
        "sd_collect"
      ],
      [
        "sd_compute"
      ],
      [
        "sd_configure_proj"
      ],
      [
        "sd_count"
      ],
      [
        "sd_drop_view"
      ],
      [
        "sd_preview"
      ],
      [
        "sd_read_parquet"
      ],
      [
        "sd_register_udf"
      ],
      [
        "sd_sql"
      ],
      [
        "sd_to_view"
      ],
      [
        "sd_view"
      ],
      [
        "sd_write_parquet"
      ],
      [
        "sedonadb_adbc"
      ]
    ],
    "topics": [
      [
        "c"
      ],
      [
        "database"
      ],
      [
        "geospatial"
      ],
      [
        "hacktoberfest"
      ],
      [
        "python"
      ],
      [
        "rust"
      ],
      [
        "spatial-analysis"
      ],
      [
        "spatial-query"
      ],
      [
        "spatial-sql"
      ],
      [
        "cargo"
      ],
      [
        "geos"
      ]
    ],
    "score": 7.3017,
    "stars": 354,
    "primary_category": "spatial",
    "source_universe": "paleolimbot",
    "search_text": "sedonadb Bindings for Apache SedonaDB Provides bindings for Apache SedonaDB, a lightweight query\nengine optimized for spatial workflows. as_sedonadb_dataframe sd_collect sd_compute sd_configure_proj sd_count sd_drop_view sd_preview sd_read_parquet sd_register_udf sd_sql sd_to_view sd_view sd_write_parquet sedonadb_adbc c database geospatial hacktoberfest python rust spatial-analysis spatial-query spatial-sql cargo geos"
  },
  {
    "id": 924,
    "package_name": "osmapiR",
    "title": "'OpenStreetMap' API",
    "description": "Interface to 'OpenStreetMap API' for fetching and saving\ndata from/to the 'OpenStreetMap' database\n(<https://wiki.openstreetmap.org/wiki/API_v0.6>).",
    "version": "0.2.4.9000",
    "maintainer": "Joan Maspons <joanmaspons@gmail.com>",
    "author": "Joan Maspons [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-2286-8727>),\nJon Harmon [rev] (Jon reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/633, ORCID:\n<https://orcid.org/0000-0003-4781-4346>),\nCarlos C\u00e1mara [rev] (Carles reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/633, ORCID:\n<https://orcid.org/0000-0002-9378-0549>)",
    "url": "https://docs.ropensci.org/osmapiR/,\nhttps://github.com/ropensci/osmapiR",
    "bug_reports": "https://github.com/ropensci/osmapiR/issues",
    "repository": "",
    "exports": [
      [
        "authenticate_osmapi"
      ],
      [
        "get_osmapi_url"
      ],
      [
        "logout_osmapi"
      ],
      [
        "osm_api_versions"
      ],
      [
        "osm_bbox_objects"
      ],
      [
        "osm_capabilities"
      ],
      [
        "osm_close_changeset"
      ],
      [
        "osm_close_note"
      ],
      [
        "osm_comment_changeset_discussion"
      ],
      [
        "osm_create_changeset"
      ],
      [
        "osm_create_comment_note"
      ],
      [
        "osm_create_gpx"
      ],
      [
        "osm_create_note"
      ],
      [
        "osm_create_object"
      ],
      [
        "osm_create_user_block"
      ],
      [
        "osm_delete_gpx"
      ],
      [
        "osm_delete_note"
      ],
      [
        "osm_delete_object"
      ],
      [
        "osm_details_logged_user"
      ],
      [
        "osm_diff_upload_changeset"
      ],
      [
        "osm_download_changeset"
      ],
      [
        "osm_feed_notes"
      ],
      [
        "osm_get_changesets"
      ],
      [
        "osm_get_data_gpx"
      ],
      [
        "osm_get_gpx_metadata"
      ],
      [
        "osm_get_notes"
      ],
      [
        "osm_get_objects"
      ],
      [
        "osm_get_points_gps"
      ],
      [
        "osm_get_preferences_user"
      ],
      [
        "osm_get_user_blocks"
      ],
      [
        "osm_get_user_details"
      ],
      [
        "osm_hide_comment_changeset_discussion"
      ],
      [
        "osm_history_object"
      ],
      [
        "osm_list_active_user_blocks"
      ],
      [
        "osm_list_gpxs"
      ],
      [
        "osm_permissions"
      ],
      [
        "osm_query_changesets"
      ],
      [
        "osm_read_bbox_notes"
      ],
      [
        "osm_redaction_object"
      ],
      [
        "osm_relations_object"
      ],
      [
        "osm_reopen_note"
      ],
      [
        "osm_search_notes"
      ],
      [
        "osm_set_preferences_user"
      ],
      [
        "osm_subscribe_changeset_discussion"
      ],
      [
        "osm_subscribe_note"
      ],
      [
        "osm_unhide_comment_changeset_discussion"
      ],
      [
        "osm_unsubscribe_changeset_discussion"
      ],
      [
        "osm_unsubscribe_note"
      ],
      [
        "osm_update_changeset"
      ],
      [
        "osm_update_gpx"
      ],
      [
        "osm_update_object"
      ],
      [
        "osm_ways_node"
      ],
      [
        "osmapi_objects"
      ],
      [
        "osmchange_create"
      ],
      [
        "osmchange_delete"
      ],
      [
        "osmchange_modify"
      ],
      [
        "set_osmapi_connection"
      ],
      [
        "set_osmapi_url"
      ],
      [
        "st_as_sf.osmapi_changesets"
      ],
      [
        "st_as_sf.osmapi_gps_track"
      ],
      [
        "st_as_sf.osmapi_gpx"
      ],
      [
        "st_as_sf.osmapi_map_notes"
      ],
      [
        "tags_list2wide"
      ],
      [
        "tags_wide2list"
      ]
    ],
    "topics": [
      [
        "open street map"
      ],
      [
        "openstreetmap"
      ],
      [
        "osm"
      ],
      [
        "openstreetmap-api"
      ],
      [
        "osmapi"
      ],
      [
        "api"
      ]
    ],
    "score": 7.1821,
    "stars": 26,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "osmapiR 'OpenStreetMap' API Interface to 'OpenStreetMap API' for fetching and saving\ndata from/to the 'OpenStreetMap' database\n(<https://wiki.openstreetmap.org/wiki/API_v0.6>). authenticate_osmapi get_osmapi_url logout_osmapi osm_api_versions osm_bbox_objects osm_capabilities osm_close_changeset osm_close_note osm_comment_changeset_discussion osm_create_changeset osm_create_comment_note osm_create_gpx osm_create_note osm_create_object osm_create_user_block osm_delete_gpx osm_delete_note osm_delete_object osm_details_logged_user osm_diff_upload_changeset osm_download_changeset osm_feed_notes osm_get_changesets osm_get_data_gpx osm_get_gpx_metadata osm_get_notes osm_get_objects osm_get_points_gps osm_get_preferences_user osm_get_user_blocks osm_get_user_details osm_hide_comment_changeset_discussion osm_history_object osm_list_active_user_blocks osm_list_gpxs osm_permissions osm_query_changesets osm_read_bbox_notes osm_redaction_object osm_relations_object osm_reopen_note osm_search_notes osm_set_preferences_user osm_subscribe_changeset_discussion osm_subscribe_note osm_unhide_comment_changeset_discussion osm_unsubscribe_changeset_discussion osm_unsubscribe_note osm_update_changeset osm_update_gpx osm_update_object osm_ways_node osmapi_objects osmchange_create osmchange_delete osmchange_modify set_osmapi_connection set_osmapi_url st_as_sf.osmapi_changesets st_as_sf.osmapi_gps_track st_as_sf.osmapi_gpx st_as_sf.osmapi_map_notes tags_list2wide tags_wide2list open street map openstreetmap osm openstreetmap-api osmapi api"
  },
  {
    "id": 586,
    "package_name": "forcis",
    "title": "Handle the FORCIS Foraminifera Database",
    "description": "Provides an interface to the 'FORCIS' database (Chaabane\net al. (2024) <doi:10.5281/zenodo.7390791>) on global\nforaminifera distribution. This package allows to download and\nto handle 'FORCIS' data. It is part of the FRB-CESAB working\ngroup FORCIS.\n<https://www.fondationbiodiversite.fr/en/the-frb-in-action/programs-and-projects/le-cesab/forcis/>.",
    "version": "1.0.1",
    "maintainer": "Nicolas Casajus <nicolas.casajus@fondationbiodiversite.fr>",
    "author": "Nicolas Casajus [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-5537-5294>),\nMattia Greco [aut] (ORCID: <https://orcid.org/0000-0003-2416-6235>),\nSonia Chaabane [aut] (ORCID: <https://orcid.org/0000-0002-4653-8610>),\nXavier Giraud [aut] (ORCID: <https://orcid.org/0000-0001-5067-8176>),\nThibault de Garidel-Thoron [aut] (ORCID:\n<https://orcid.org/0000-0001-8983-9571>),\nKhalil Hammami [ctb],\nAir Forbes [rev] (ORCID: <https://orcid.org/0000-0002-9842-7648>),\nFRB-CESAB [fnd]",
    "url": "https://docs.ropensci.org/forcis/,\nhttps://github.com/ropensci/forcis",
    "bug_reports": "https://github.com/ropensci/forcis/issues",
    "repository": "",
    "exports": [
      [
        "compute_abundances"
      ],
      [
        "compute_concentrations"
      ],
      [
        "compute_frequencies"
      ],
      [
        "convert_to_long_format"
      ],
      [
        "data_to_sf"
      ],
      [
        "download_forcis_db"
      ],
      [
        "filter_by_bbox"
      ],
      [
        "filter_by_month"
      ],
      [
        "filter_by_ocean"
      ],
      [
        "filter_by_polygon"
      ],
      [
        "filter_by_species"
      ],
      [
        "filter_by_year"
      ],
      [
        "geom_basemap"
      ],
      [
        "get_available_versions"
      ],
      [
        "get_ocean_names"
      ],
      [
        "get_required_columns"
      ],
      [
        "get_species_names"
      ],
      [
        "get_version_metadata"
      ],
      [
        "ggmap_data"
      ],
      [
        "plot_record_by_depth"
      ],
      [
        "plot_record_by_month"
      ],
      [
        "plot_record_by_season"
      ],
      [
        "plot_record_by_year"
      ],
      [
        "read_cpr_north_data"
      ],
      [
        "read_cpr_south_data"
      ],
      [
        "read_plankton_nets_data"
      ],
      [
        "read_pump_data"
      ],
      [
        "read_sediment_trap_data"
      ],
      [
        "select_forcis_columns"
      ],
      [
        "select_taxonomy"
      ]
    ],
    "topics": [],
    "score": 7.0852,
    "stars": 6,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "forcis Handle the FORCIS Foraminifera Database Provides an interface to the 'FORCIS' database (Chaabane\net al. (2024) <doi:10.5281/zenodo.7390791>) on global\nforaminifera distribution. This package allows to download and\nto handle 'FORCIS' data. It is part of the FRB-CESAB working\ngroup FORCIS.\n<https://www.fondationbiodiversite.fr/en/the-frb-in-action/programs-and-projects/le-cesab/forcis/>. compute_abundances compute_concentrations compute_frequencies convert_to_long_format data_to_sf download_forcis_db filter_by_bbox filter_by_month filter_by_ocean filter_by_polygon filter_by_species filter_by_year geom_basemap get_available_versions get_ocean_names get_required_columns get_species_names get_version_metadata ggmap_data plot_record_by_depth plot_record_by_month plot_record_by_season plot_record_by_year read_cpr_north_data read_cpr_south_data read_plankton_nets_data read_pump_data read_sediment_trap_data select_forcis_columns select_taxonomy "
  },
  {
    "id": 1094,
    "package_name": "restez",
    "title": "Create and Query a Local Copy of 'GenBank' in R",
    "description": "Download large sections of 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and generate a local\nSQL-based database. A user can then query this database using\n'restez' functions or through 'rentrez'\n<https://CRAN.R-project.org/package=rentrez> wrappers.",
    "version": "2.1.5.9000",
    "maintainer": "Joel H. Nitta <joelnitta@gmail.com>",
    "author": "Joel H. Nitta [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4719-7472>),\nDom Bennett [aut] (ORCID: <https://orcid.org/0000-0003-2722-1359>)",
    "url": "https://github.com/ropensci/restez,\nhttps://docs.ropensci.org/restez/",
    "bug_reports": "https://github.com/ropensci/restez/issues",
    "repository": "",
    "exports": [
      [
        "count_db_ids"
      ],
      [
        "db_create"
      ],
      [
        "db_delete"
      ],
      [
        "db_download"
      ],
      [
        "demo_db_create"
      ],
      [
        "entrez_fetch"
      ],
      [
        "gb_definition_get"
      ],
      [
        "gb_extract"
      ],
      [
        "gb_fasta_get"
      ],
      [
        "gb_organism_get"
      ],
      [
        "gb_record_get"
      ],
      [
        "gb_sequence_get"
      ],
      [
        "gb_version_get"
      ],
      [
        "is_in_db"
      ],
      [
        "list_db_ids"
      ],
      [
        "ncbi_acc_get"
      ],
      [
        "restez_connect"
      ],
      [
        "restez_disconnect"
      ],
      [
        "restez_path_get"
      ],
      [
        "restez_path_set"
      ],
      [
        "restez_path_unset"
      ],
      [
        "restez_ready"
      ],
      [
        "restez_status"
      ]
    ],
    "topics": [
      [
        "dna"
      ],
      [
        "entrez"
      ],
      [
        "genbank"
      ],
      [
        "sequence"
      ]
    ],
    "score": 7.0459,
    "stars": 27,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "restez Create and Query a Local Copy of 'GenBank' in R Download large sections of 'GenBank'\n<https://www.ncbi.nlm.nih.gov/genbank/> and generate a local\nSQL-based database. A user can then query this database using\n'restez' functions or through 'rentrez'\n<https://CRAN.R-project.org/package=rentrez> wrappers. count_db_ids db_create db_delete db_download demo_db_create entrez_fetch gb_definition_get gb_extract gb_fasta_get gb_organism_get gb_record_get gb_sequence_get gb_version_get is_in_db list_db_ids ncbi_acc_get restez_connect restez_disconnect restez_path_get restez_path_set restez_path_unset restez_ready restez_status dna entrez genbank sequence"
  },
  {
    "id": 1245,
    "package_name": "sofa",
    "title": "Connector to 'CouchDB'",
    "description": "Provides an interface to the 'NoSQL' database 'CouchDB'\n(<http://couchdb.apache.org>). Methods are provided for\nmanaging databases within 'CouchDB', including\ncreating/deleting/updating/transferring, and managing documents\nwithin databases. One can connect with a local 'CouchDB'\ninstance, or a remote 'CouchDB' databases such as 'Cloudant'.\nDocuments can be inserted directly from vectors, lists,\ndata.frames, and 'JSON'. Targeted at 'CouchDB' v2 or greater.",
    "version": "0.4.1",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-9200-1016>),\nEduard Sz\u00f6cs [aut] (ORCID: <https://orcid.org/0000-0003-1444-9135>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/sofa (devel)\nhttps://docs.ropensci.org/sofa (docs)",
    "bug_reports": "https://github.com/ropensci/sofa/issues",
    "repository": "",
    "exports": [
      [
        "active_tasks"
      ],
      [
        "attach_get"
      ],
      [
        "Cushion"
      ],
      [
        "db_alldocs"
      ],
      [
        "db_bulk_create"
      ],
      [
        "db_bulk_get"
      ],
      [
        "db_bulk_update"
      ],
      [
        "db_changes"
      ],
      [
        "db_compact"
      ],
      [
        "db_create"
      ],
      [
        "db_delete"
      ],
      [
        "db_explain"
      ],
      [
        "db_index"
      ],
      [
        "db_index_create"
      ],
      [
        "db_index_delete"
      ],
      [
        "db_info"
      ],
      [
        "db_list"
      ],
      [
        "db_query"
      ],
      [
        "db_replicate"
      ],
      [
        "db_revisions"
      ],
      [
        "design_create"
      ],
      [
        "design_create_"
      ],
      [
        "design_delete"
      ],
      [
        "design_get"
      ],
      [
        "design_head"
      ],
      [
        "design_info"
      ],
      [
        "design_search"
      ],
      [
        "design_search_many"
      ],
      [
        "doc_attach_create"
      ],
      [
        "doc_attach_delete"
      ],
      [
        "doc_attach_get"
      ],
      [
        "doc_attach_info"
      ],
      [
        "doc_create"
      ],
      [
        "doc_delete"
      ],
      [
        "doc_get"
      ],
      [
        "doc_head"
      ],
      [
        "doc_update"
      ],
      [
        "doc_upsert"
      ],
      [
        "membership"
      ],
      [
        "parse_df"
      ],
      [
        "ping"
      ],
      [
        "restart"
      ],
      [
        "session"
      ],
      [
        "uuids"
      ]
    ],
    "topics": [
      [
        "couchdb"
      ],
      [
        "database"
      ],
      [
        "nosql"
      ],
      [
        "documents"
      ],
      [
        "cloudant"
      ],
      [
        "couchdb-client"
      ]
    ],
    "score": 7.037,
    "stars": 33,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "sofa Connector to 'CouchDB' Provides an interface to the 'NoSQL' database 'CouchDB'\n(<http://couchdb.apache.org>). Methods are provided for\nmanaging databases within 'CouchDB', including\ncreating/deleting/updating/transferring, and managing documents\nwithin databases. One can connect with a local 'CouchDB'\ninstance, or a remote 'CouchDB' databases such as 'Cloudant'.\nDocuments can be inserted directly from vectors, lists,\ndata.frames, and 'JSON'. Targeted at 'CouchDB' v2 or greater. active_tasks attach_get Cushion db_alldocs db_bulk_create db_bulk_get db_bulk_update db_changes db_compact db_create db_delete db_explain db_index db_index_create db_index_delete db_info db_list db_query db_replicate db_revisions design_create design_create_ design_delete design_get design_head design_info design_search design_search_many doc_attach_create doc_attach_delete doc_attach_get doc_attach_info doc_create doc_delete doc_get doc_head doc_update doc_upsert membership parse_df ping restart session uuids couchdb database nosql documents cloudant couchdb-client"
  },
  {
    "id": 1304,
    "package_name": "taxa",
    "title": "Classes for Storing and Manipulating Taxonomic Data",
    "description": "Provides classes for storing and manipulating taxonomic\ndata. Most of the classes can be treated like base R vectors\n(e.g. can be used in tables as columns and can be named).\nVectorized classes can store taxon names and authorities, taxon\nIDs from databases, taxon ranks, and other types of\ninformation. More complex classes are provided to store\ntaxonomic trees and user-defined data associated with them.",
    "version": "0.4.4",
    "maintainer": "Zachary Foster <zacharyfoster1989@gmail.com>",
    "author": "Scott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nZachary Foster [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-5075-0948>)",
    "url": "https://docs.ropensci.org/taxa/, https://github.com/ropensci/taxa",
    "bug_reports": "https://github.com/ropensci/taxa/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "%in%"
      ],
      [
        "as_data_frame"
      ],
      [
        "as_taxon"
      ],
      [
        "classification"
      ],
      [
        "contains"
      ],
      [
        "db_ref"
      ],
      [
        "ends_with"
      ],
      [
        "everything"
      ],
      [
        "internodes"
      ],
      [
        "is_classification"
      ],
      [
        "is_internode"
      ],
      [
        "is_leaf"
      ],
      [
        "is_root"
      ],
      [
        "is_stem"
      ],
      [
        "is_taxon"
      ],
      [
        "is_taxon_authority"
      ],
      [
        "is_taxon_db"
      ],
      [
        "is_taxon_id"
      ],
      [
        "is_taxon_rank"
      ],
      [
        "is_taxonomy"
      ],
      [
        "leaves"
      ],
      [
        "matches"
      ],
      [
        "n_leaves"
      ],
      [
        "n_subtaxa"
      ],
      [
        "n_supertaxa"
      ],
      [
        "num_range"
      ],
      [
        "one_of"
      ],
      [
        "roots"
      ],
      [
        "starts_with"
      ],
      [
        "stems"
      ],
      [
        "subtaxa"
      ],
      [
        "supertaxa"
      ],
      [
        "tax_auth"
      ],
      [
        "tax_auth<-"
      ],
      [
        "tax_author"
      ],
      [
        "tax_author<-"
      ],
      [
        "tax_cite"
      ],
      [
        "tax_cite<-"
      ],
      [
        "tax_date"
      ],
      [
        "tax_date<-"
      ],
      [
        "tax_db"
      ],
      [
        "tax_db<-"
      ],
      [
        "tax_id"
      ],
      [
        "tax_id<-"
      ],
      [
        "tax_name"
      ],
      [
        "tax_name<-"
      ],
      [
        "tax_rank"
      ],
      [
        "tax_rank<-"
      ],
      [
        "taxon"
      ],
      [
        "taxon_authority"
      ],
      [
        "taxon_db"
      ],
      [
        "taxon_id"
      ],
      [
        "taxon_rank"
      ],
      [
        "taxonomy"
      ]
    ],
    "topics": [
      [
        "taxonomy"
      ],
      [
        "biology"
      ],
      [
        "hierarchy"
      ],
      [
        "data-cleaning"
      ],
      [
        "taxon"
      ]
    ],
    "score": 6.9957,
    "stars": 50,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxa Classes for Storing and Manipulating Taxonomic Data Provides classes for storing and manipulating taxonomic\ndata. Most of the classes can be treated like base R vectors\n(e.g. can be used in tables as columns and can be named).\nVectorized classes can store taxon names and authorities, taxon\nIDs from databases, taxon ranks, and other types of\ninformation. More complex classes are provided to store\ntaxonomic trees and user-defined data associated with them. %>% %in% as_data_frame as_taxon classification contains db_ref ends_with everything internodes is_classification is_internode is_leaf is_root is_stem is_taxon is_taxon_authority is_taxon_db is_taxon_id is_taxon_rank is_taxonomy leaves matches n_leaves n_subtaxa n_supertaxa num_range one_of roots starts_with stems subtaxa supertaxa tax_auth tax_auth<- tax_author tax_author<- tax_cite tax_cite<- tax_date tax_date<- tax_db tax_db<- tax_id tax_id<- tax_name tax_name<- tax_rank tax_rank<- taxon taxon_authority taxon_db taxon_id taxon_rank taxonomy taxonomy biology hierarchy data-cleaning taxon"
  },
  {
    "id": 1154,
    "package_name": "rsnps",
    "title": "Get 'SNP' ('Single-Nucleotide' 'Polymorphism') Data on the Web",
    "description": "A programmatic interface to various 'SNP' 'datasets' on\nthe web: 'OpenSNP' (<https://opensnp.org>), and 'NBCIs' 'dbSNP'\ndatabase (<https://www.ncbi.nlm.nih.gov/projects/SNP/>).\nFunctions are included for searching for 'NCBI'. For 'OpenSNP',\nfunctions are included for getting 'SNPs', and data for\n'genotypes', 'phenotypes', annotations, and bulk downloads of\ndata by user.",
    "version": "0.6.1",
    "maintainer": "Julia Gustavsen <j.gustavsen@gmail.com>",
    "author": "Julia Gustavsen [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-4764-4802>),\nSina R\u00fceger [aut] (ORCID: <https://orcid.org/0000-0003-2848-9242>),\nScott Chamberlain [aut] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nKevin Ushey [aut],\nHao Zhu [aut]",
    "url": "https://docs.ropensci.org/rsnps/,\nhttps://github.com/ropensci/rsnps/",
    "bug_reports": "https://github.com/ropensci/rsnps/issues/",
    "repository": "",
    "exports": [
      [
        "allgensnp"
      ],
      [
        "allphenotypes"
      ],
      [
        "annotations"
      ],
      [
        "download_users"
      ],
      [
        "fetch_genotypes"
      ],
      [
        "genotypes"
      ],
      [
        "ld_search"
      ],
      [
        "LDSearch"
      ],
      [
        "ncbi_snp_query"
      ],
      [
        "NCBI_snp_query"
      ],
      [
        "ncbi_snp_query2"
      ],
      [
        "NCBI_snp_query2"
      ],
      [
        "ncbi_snp_summary"
      ],
      [
        "phenotypes"
      ],
      [
        "phenotypes_byid"
      ],
      [
        "read_users"
      ],
      [
        "rsnpsCache"
      ],
      [
        "tryget"
      ],
      [
        "users"
      ]
    ],
    "topics": [
      [
        "gene"
      ],
      [
        "snp"
      ],
      [
        "sequence"
      ],
      [
        "api"
      ],
      [
        "web"
      ],
      [
        "api-client"
      ],
      [
        "species"
      ],
      [
        "dbsnp"
      ],
      [
        "opensnp"
      ],
      [
        "ncbi"
      ],
      [
        "genotype"
      ],
      [
        "data"
      ],
      [
        "snps"
      ],
      [
        "web-api"
      ]
    ],
    "score": 6.7004,
    "stars": 55,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rsnps Get 'SNP' ('Single-Nucleotide' 'Polymorphism') Data on the Web A programmatic interface to various 'SNP' 'datasets' on\nthe web: 'OpenSNP' (<https://opensnp.org>), and 'NBCIs' 'dbSNP'\ndatabase (<https://www.ncbi.nlm.nih.gov/projects/SNP/>).\nFunctions are included for searching for 'NCBI'. For 'OpenSNP',\nfunctions are included for getting 'SNPs', and data for\n'genotypes', 'phenotypes', annotations, and bulk downloads of\ndata by user. allgensnp allphenotypes annotations download_users fetch_genotypes genotypes ld_search LDSearch ncbi_snp_query NCBI_snp_query ncbi_snp_query2 NCBI_snp_query2 ncbi_snp_summary phenotypes phenotypes_byid read_users rsnpsCache tryget users gene snp sequence api web api-client species dbsnp opensnp ncbi genotype data snps web-api"
  },
  {
    "id": 914,
    "package_name": "orbital",
    "title": "Predict with 'tidymodels' Workflows in Databases",
    "description": "Turn 'tidymodels' workflows into objects containing the\nsufficient sequential equations to perform predictions. These\nsmaller objects allow for low dependency prediction locally or\ndirectly in databases.",
    "version": "0.4.1.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Emil Hvitfeldt [aut, cre],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://github.com/tidymodels/orbital,\nhttps://orbital.tidymodels.org",
    "bug_reports": "https://github.com/tidymodels/orbital/issues",
    "repository": "",
    "exports": [
      [
        "augment"
      ],
      [
        "orbital"
      ],
      [
        "orbital_dt"
      ],
      [
        "orbital_inline"
      ],
      [
        "orbital_json_read"
      ],
      [
        "orbital_json_write"
      ],
      [
        "orbital_r_fun"
      ],
      [
        "orbital_sql"
      ]
    ],
    "topics": [],
    "score": 6.6846,
    "stars": 43,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "orbital Predict with 'tidymodels' Workflows in Databases Turn 'tidymodels' workflows into objects containing the\nsufficient sequential equations to perform predictions. These\nsmaller objects allow for low dependency prediction locally or\ndirectly in databases. augment orbital orbital_dt orbital_inline orbital_json_read orbital_json_write orbital_r_fun orbital_sql "
  },
  {
    "id": 1307,
    "package_name": "taxizedb",
    "title": "Offline Access to Taxonomic Databases",
    "description": "Download taxonomic databases, convert them into 'SQLite'\nformat, and query them locally for fast, reliable, and\nreproducible access to taxonomic data.",
    "version": "0.3.2",
    "maintainer": "Tam\u00e1s Stirling <stirling.tamas@gmail.com>",
    "author": "Scott Chamberlain [aut],\nZebulun Arendsee [aut],\nTam\u00e1s Stirling [ctb, cre]",
    "url": "https://docs.ropensci.org/taxizedb/,\nhttps://github.com/ropensci/taxizedb",
    "bug_reports": "https://github.com/ropensci/taxizedb/issues",
    "repository": "",
    "exports": [
      [
        "children"
      ],
      [
        "classification"
      ],
      [
        "db_download_col"
      ],
      [
        "db_download_gbif"
      ],
      [
        "db_download_itis"
      ],
      [
        "db_download_ncbi"
      ],
      [
        "db_download_tpl"
      ],
      [
        "db_download_wfo"
      ],
      [
        "db_download_wikidata"
      ],
      [
        "db_load_col"
      ],
      [
        "db_load_gbif"
      ],
      [
        "db_load_itis"
      ],
      [
        "db_load_ncbi"
      ],
      [
        "db_load_tpl"
      ],
      [
        "db_load_wikidata"
      ],
      [
        "db_path"
      ],
      [
        "downstream"
      ],
      [
        "name2taxid"
      ],
      [
        "sql_collect"
      ],
      [
        "src_col"
      ],
      [
        "src_gbif"
      ],
      [
        "src_itis"
      ],
      [
        "src_ncbi"
      ],
      [
        "src_tpl"
      ],
      [
        "src_wfo"
      ],
      [
        "src_wikidata"
      ],
      [
        "taxa_at"
      ],
      [
        "taxid2name"
      ],
      [
        "taxid2rank"
      ],
      [
        "tdb_cache"
      ]
    ],
    "topics": [
      [
        "itis"
      ],
      [
        "taxize"
      ],
      [
        "taxonomic-databases"
      ],
      [
        "taxonomy"
      ]
    ],
    "score": 6.6805,
    "stars": 33,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "taxizedb Offline Access to Taxonomic Databases Download taxonomic databases, convert them into 'SQLite'\nformat, and query them locally for fast, reliable, and\nreproducible access to taxonomic data. children classification db_download_col db_download_gbif db_download_itis db_download_ncbi db_download_tpl db_download_wfo db_download_wikidata db_load_col db_load_gbif db_load_itis db_load_ncbi db_load_tpl db_load_wikidata db_path downstream name2taxid sql_collect src_col src_gbif src_itis src_ncbi src_tpl src_wfo src_wikidata taxa_at taxid2name taxid2rank tdb_cache itis taxize taxonomic-databases taxonomy"
  },
  {
    "id": 1166,
    "package_name": "ruODK",
    "title": "An R Client for the ODK Central API",
    "description": "Access and tidy up data from the 'ODK Central' API.  'ODK\nCentral' is a clearinghouse for digitally captured data using\nODK <https://docs.getodk.org/central-intro/>.  It manages user\naccounts and permissions, stores form definitions, and allows\ndata collection clients like 'ODK Collect' to connect to it for\nform download and submission upload. The 'ODK Central' API is\ndocumented at <https://docs.getodk.org/central-api/>.",
    "version": "1.5.2",
    "maintainer": "Florian W. Mayer <Florian.Mayer@dpc.wa.gov.au>",
    "author": "Florian W. Mayer [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4269-4242>),\nMa\u00eblle Salmon [rev] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nKarissa Whiting [rev] (ORCID: <https://orcid.org/0000-0002-4683-1868>),\nJason Taylor [rev],\nMarcelo Tyszler [ctb] (ORCID: <https://orcid.org/0000-0002-4573-0002>),\nH\u00e9l\u00e8ne Langet [ctb] (ORCID: <https://orcid.org/0000-0002-6758-2397>),\nDBCA [cph, fnd],\nNWSFTCP [fnd]",
    "url": "https://docs.ropensci.org/ruODK, https://github.com/ropensci/ruODK",
    "bug_reports": "https://github.com/ropensci/ruODK/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "attachment_get"
      ],
      [
        "attachment_link"
      ],
      [
        "attachment_list"
      ],
      [
        "audit_get"
      ],
      [
        "drop_null_coords"
      ],
      [
        "encryption_key_list"
      ],
      [
        "enexpr"
      ],
      [
        "enquo"
      ],
      [
        "ensym"
      ],
      [
        "entity_audits"
      ],
      [
        "entity_changes"
      ],
      [
        "entity_create"
      ],
      [
        "entity_delete"
      ],
      [
        "entity_detail"
      ],
      [
        "entity_list"
      ],
      [
        "entity_update"
      ],
      [
        "entity_versions"
      ],
      [
        "entitylist_detail"
      ],
      [
        "entitylist_download"
      ],
      [
        "entitylist_list"
      ],
      [
        "entitylist_update"
      ],
      [
        "expr"
      ],
      [
        "exprs"
      ],
      [
        "form_detail"
      ],
      [
        "form_list"
      ],
      [
        "form_schema"
      ],
      [
        "form_schema_ext"
      ],
      [
        "form_schema_parse"
      ],
      [
        "form_xml"
      ],
      [
        "get_default_fid"
      ],
      [
        "get_default_odkc_version"
      ],
      [
        "get_default_orders"
      ],
      [
        "get_default_pid"
      ],
      [
        "get_default_pp"
      ],
      [
        "get_default_pw"
      ],
      [
        "get_default_tz"
      ],
      [
        "get_default_un"
      ],
      [
        "get_default_url"
      ],
      [
        "get_one_attachment"
      ],
      [
        "get_one_submission"
      ],
      [
        "get_one_submission_att_list"
      ],
      [
        "get_one_submission_audit"
      ],
      [
        "get_retries"
      ],
      [
        "get_ru_verbose"
      ],
      [
        "get_test_fid"
      ],
      [
        "get_test_fid_att"
      ],
      [
        "get_test_fid_gap"
      ],
      [
        "get_test_fid_wkt"
      ],
      [
        "get_test_fid_zip"
      ],
      [
        "get_test_odkc_version"
      ],
      [
        "get_test_pid"
      ],
      [
        "get_test_pp"
      ],
      [
        "get_test_pw"
      ],
      [
        "get_test_un"
      ],
      [
        "get_test_url"
      ],
      [
        "handle_ru_attachments"
      ],
      [
        "handle_ru_datetimes"
      ],
      [
        "handle_ru_geopoints"
      ],
      [
        "handle_ru_geoshapes"
      ],
      [
        "handle_ru_geotraces"
      ],
      [
        "odata_entitylist_data_get"
      ],
      [
        "odata_entitylist_metadata_get"
      ],
      [
        "odata_entitylist_service_get"
      ],
      [
        "odata_metadata_get"
      ],
      [
        "odata_service_get"
      ],
      [
        "odata_submission_get"
      ],
      [
        "odata_submission_rectangle"
      ],
      [
        "odata_svc_parse"
      ],
      [
        "parse_odkc_version"
      ],
      [
        "project_create"
      ],
      [
        "project_detail"
      ],
      [
        "project_list"
      ],
      [
        "quo"
      ],
      [
        "quo_name"
      ],
      [
        "quos"
      ],
      [
        "ru_msg_abort"
      ],
      [
        "ru_msg_info"
      ],
      [
        "ru_msg_noop"
      ],
      [
        "ru_msg_success"
      ],
      [
        "ru_msg_warn"
      ],
      [
        "ru_settings"
      ],
      [
        "ru_setup"
      ],
      [
        "semver_gt"
      ],
      [
        "semver_lt"
      ],
      [
        "split_geopoint"
      ],
      [
        "split_geoshape"
      ],
      [
        "split_geotrace"
      ],
      [
        "submission_audit_get"
      ],
      [
        "submission_detail"
      ],
      [
        "submission_export"
      ],
      [
        "submission_get"
      ],
      [
        "submission_list"
      ],
      [
        "sym"
      ],
      [
        "syms"
      ],
      [
        "user_list"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "open-data"
      ],
      [
        "odk"
      ],
      [
        "api"
      ],
      [
        "data"
      ],
      [
        "dataset"
      ],
      [
        "odata"
      ],
      [
        "odata-client"
      ],
      [
        "odk-central"
      ],
      [
        "opendatakit"
      ]
    ],
    "score": 6.6257,
    "stars": 44,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "ruODK An R Client for the ODK Central API Access and tidy up data from the 'ODK Central' API.  'ODK\nCentral' is a clearinghouse for digitally captured data using\nODK <https://docs.getodk.org/central-intro/>.  It manages user\naccounts and permissions, stores form definitions, and allows\ndata collection clients like 'ODK Collect' to connect to it for\nform download and submission upload. The 'ODK Central' API is\ndocumented at <https://docs.getodk.org/central-api/>. %>% attachment_get attachment_link attachment_list audit_get drop_null_coords encryption_key_list enexpr enquo ensym entity_audits entity_changes entity_create entity_delete entity_detail entity_list entity_update entity_versions entitylist_detail entitylist_download entitylist_list entitylist_update expr exprs form_detail form_list form_schema form_schema_ext form_schema_parse form_xml get_default_fid get_default_odkc_version get_default_orders get_default_pid get_default_pp get_default_pw get_default_tz get_default_un get_default_url get_one_attachment get_one_submission get_one_submission_att_list get_one_submission_audit get_retries get_ru_verbose get_test_fid get_test_fid_att get_test_fid_gap get_test_fid_wkt get_test_fid_zip get_test_odkc_version get_test_pid get_test_pp get_test_pw get_test_un get_test_url handle_ru_attachments handle_ru_datetimes handle_ru_geopoints handle_ru_geoshapes handle_ru_geotraces odata_entitylist_data_get odata_entitylist_metadata_get odata_entitylist_service_get odata_metadata_get odata_service_get odata_submission_get odata_submission_rectangle odata_svc_parse parse_odkc_version project_create project_detail project_list quo quo_name quos ru_msg_abort ru_msg_info ru_msg_noop ru_msg_success ru_msg_warn ru_settings ru_setup semver_gt semver_lt split_geopoint split_geoshape split_geotrace submission_audit_get submission_detail submission_export submission_get submission_list sym syms user_list database open-data odk api data dataset odata odata-client odk-central opendatakit"
  },
  {
    "id": 21,
    "package_name": "ColOpenData",
    "title": "Download Colombian Demographic, Climate and Geospatial Data",
    "description": "Downloads wrangled Colombian socioeconomic,\ngeospatial,population and climate data from DANE\n<https://www.dane.gov.co/> (National Administrative Department\nof Statistics) and IDEAM <https://ideam.gov.co> (Institute of\nHydrology, Meteorology and Environmental Studies). It solves\nthe problem of Colombian data being issued in different web\npages and sources by using functions that allow the user to\nselect the desired database and download it without having to\ndo the exhausting acquisition process.",
    "version": "0.3.1",
    "maintainer": "Maria Camila Tavera-Cifuentes <mc.tavera@uniandes.edu.co>",
    "author": "Maria Camila Tavera-Cifuentes [aut, cre, cph] (ORCID:\n<https://orcid.org/0009-0007-1610-4583>),\nJulian Otero [aut, cph] (ORCID:\n<https://orcid.org/0009-0006-0429-7747>),\nNatalia Nino-Machado [ctb] (ORCID:\n<https://orcid.org/0000-0001-7887-9439>),\nCatalina Gonzalez-Uribe [ctb] (ORCID:\n<https://orcid.org/0000-0002-3322-5017>),\nJuan Manuel Cordovez [ctb] (ORCID:\n<https://orcid.org/0000-0002-4005-3567>),\nHugo Gruson [rev] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nChris Hartgerink [rev] (ORCID: <https://orcid.org/0000-0003-1050-6809>),\nKarim Mane [rev] (ORCID: <https://orcid.org/0000-0002-9892-2999>),\nJoshua W. Lambert [rev] (ORCID:\n<https://orcid.org/0000-0001-5218-3046>)",
    "url": "https://github.com/epiverse-trace/ColOpenData,\nhttps://epiverse-trace.github.io/ColOpenData/",
    "bug_reports": "https://github.com/epiverse-trace/ColOpenData/issues",
    "repository": "",
    "exports": [
      [
        "aggregate_climate"
      ],
      [
        "code_to_name_dep"
      ],
      [
        "code_to_name_mun"
      ],
      [
        "divipola_table"
      ],
      [
        "download_climate"
      ],
      [
        "download_climate_geom"
      ],
      [
        "download_climate_stations"
      ],
      [
        "download_demographic"
      ],
      [
        "download_geospatial"
      ],
      [
        "download_pop_projections"
      ],
      [
        "geospatial_dictionary"
      ],
      [
        "get_climate_tags"
      ],
      [
        "list_datasets"
      ],
      [
        "look_up"
      ],
      [
        "merge_geo_demographic"
      ],
      [
        "name_to_code_dep"
      ],
      [
        "name_to_code_mun"
      ],
      [
        "name_to_standard_dep"
      ],
      [
        "name_to_standard_mun"
      ],
      [
        "stations_in_roi"
      ]
    ],
    "topics": [
      [
        "climate"
      ],
      [
        "colombia"
      ],
      [
        "data-package"
      ],
      [
        "demographics"
      ],
      [
        "maps"
      ]
    ],
    "score": 6.6189,
    "stars": 11,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "ColOpenData Download Colombian Demographic, Climate and Geospatial Data Downloads wrangled Colombian socioeconomic,\ngeospatial,population and climate data from DANE\n<https://www.dane.gov.co/> (National Administrative Department\nof Statistics) and IDEAM <https://ideam.gov.co> (Institute of\nHydrology, Meteorology and Environmental Studies). It solves\nthe problem of Colombian data being issued in different web\npages and sources by using functions that allow the user to\nselect the desired database and download it without having to\ndo the exhausting acquisition process. aggregate_climate code_to_name_dep code_to_name_mun divipola_table download_climate download_climate_geom download_climate_stations download_demographic download_geospatial download_pop_projections geospatial_dictionary get_climate_tags list_datasets look_up merge_geo_demographic name_to_code_dep name_to_code_mun name_to_standard_dep name_to_standard_mun stations_in_roi climate colombia data-package demographics maps"
  },
  {
    "id": 1060,
    "package_name": "rcites",
    "title": "R Interface to the Species+ Database",
    "description": "A programmatic interface to the Species+\n<https://speciesplus.net/> database via the Species+/CITES\nChecklist API <https://api.speciesplus.net/>.",
    "version": "1.3.0.9000",
    "maintainer": "Kevin Cazelles <kevin.cazelles@gmail.com>",
    "author": "Kevin Cazelles [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6619-9874>),\nJonas Geschke [aut] (ORCID: <https://orcid.org/0000-0002-5654-9313>),\nIgnasi Bartomeus [aut] (ORCID: <https://orcid.org/0000-0001-7893-4389>),\nJonathan Goldenberg [ctb],\nMarie-B\u00e9 Leduc [ctb],\nYasmine Verzelen [ctb],\nNoam Ross [rev],\nMargaret Siple [rev]",
    "url": "https://docs.ropensci.org/rcites/,\nhttps://github.com/ropensci/rcites",
    "bug_reports": "https://github.com/ropensci/rcites/issues",
    "repository": "",
    "exports": [
      [
        "forget_token"
      ],
      [
        "set_token"
      ],
      [
        "spp_cites_legislation"
      ],
      [
        "spp_distributions"
      ],
      [
        "spp_eu_legislation"
      ],
      [
        "spp_references"
      ],
      [
        "spp_taxonconcept"
      ]
    ],
    "topics": [
      [
        "api-client"
      ],
      [
        "cites"
      ],
      [
        "database"
      ],
      [
        "endangered-species"
      ],
      [
        "trade"
      ]
    ],
    "score": 6.6055,
    "stars": 14,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rcites R Interface to the Species+ Database A programmatic interface to the Species+\n<https://speciesplus.net/> database via the Species+/CITES\nChecklist API <https://api.speciesplus.net/>. forget_token set_token spp_cites_legislation spp_distributions spp_eu_legislation spp_references spp_taxonconcept api-client cites database endangered-species trade"
  },
  {
    "id": 397,
    "package_name": "connections",
    "title": "Integrates with the 'RStudio' Connections Pane and 'pins'",
    "description": "Enables 'DBI' compliant packages to integrate with the\n'RStudio' connections pane, and the 'pins' package. It\nautomates the display of schemata, tables, views, as well as\nthe preview of the table's top 1000 records.",
    "version": "0.2.1",
    "maintainer": "Edgar Ruiz <edgar@posit.co>",
    "author": "Edgar Ruiz [aut, cre],\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/rstudio/connections,\nhttps://rstudio.github.io/connections/",
    "bug_reports": "https://github.com/rstudio/connections/issues",
    "repository": "",
    "exports": [
      [
        "connection_close"
      ],
      [
        "connection_code"
      ],
      [
        "connection_open"
      ],
      [
        "connection_pin_read"
      ],
      [
        "connection_pin_write"
      ],
      [
        "connection_update"
      ],
      [
        "connection_view"
      ],
      [
        "dbSendQuery"
      ],
      [
        "dbWriteTable"
      ],
      [
        "read_pin_conn"
      ],
      [
        "write_pin_conn"
      ]
    ],
    "topics": [
      [
        "connection-pane"
      ],
      [
        "database-connection"
      ],
      [
        "pins"
      ],
      [
        "rstudio"
      ]
    ],
    "score": 6.5372,
    "stars": 58,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "connections Integrates with the 'RStudio' Connections Pane and 'pins' Enables 'DBI' compliant packages to integrate with the\n'RStudio' connections pane, and the 'pins' package. It\nautomates the display of schemata, tables, views, as well as\nthe preview of the table's top 1000 records. connection_close connection_code connection_open connection_pin_read connection_pin_write connection_update connection_view dbSendQuery dbWriteTable read_pin_conn write_pin_conn connection-pane database-connection pins rstudio"
  },
  {
    "id": 534,
    "package_name": "epireview",
    "title": "Tools to update and summarise the latest pathogen data from the\nPathogen Epidemiology Review Group (PERG)",
    "description": "Contains the latest open access pathogen data from the\nPathogen Epidemiology Review Group (PERG). Tools are available\nto update pathogen databases with new peer-reviewed data as it\nbecomes available, and to summarise the latest data using\ntables and figures.",
    "version": "1.4.5",
    "maintainer": "Sangeeta Bhatia <s.bhatia@imperial.ac.uk>",
    "author": "Rebecca Nash [aut] (ORCID: <https://orcid.org/0000-0002-5213-4364>),\nChristian Morgenstern [aut] (ORCID:\n<https://orcid.org/0000-0003-3735-1130>),\nCosmo Santoni [aut],\nSangeeta Bhatia [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6525-101X>),\nRichard Sheppard [aut],\nJoseph Hicks [aut],\nGina Cuomo-Dannenburg [aut] (ORCID:\n<https://orcid.org/0000-0001-6821-0352>),\nRuth McCabe [aut] (ORCID: <https://orcid.org/0000-0002-6368-9103>),\nKelly McCain [aut] (ORCID: <https://orcid.org/0000-0003-2393-2217>),\nJoshua Lambert [aut] (ORCID: <https://orcid.org/0000-0001-5218-3046>),\nAnne Cori [aut] (ORCID: <https://orcid.org/0000-0002-8443-9162>),\nThomas Rawson [aut] (ORCID: <https://orcid.org/0000-0001-8182-4279>),\nPatrick Doohan [aut] (ORCID: <https://orcid.org/0000-0001-8076-1106>),\nTristan Naidoo [aut] (ORCID: <https://orcid.org/0000-0001-9970-2421>),\nShazia Ruybal-Pes\u00e1ntez [aut] (ORCID:\n<https://orcid.org/0000-0002-0495-179X>)",
    "url": "https://mrc-ide.github.io/epireview/",
    "bug_reports": "https://github.com/mrc-ide/epireview/issues",
    "repository": "",
    "exports": [
      [
        "article_column_type"
      ],
      [
        "assign_qa_score"
      ],
      [
        "check_column_types"
      ],
      [
        "check_df_for_meta"
      ],
      [
        "check_ulim"
      ],
      [
        "color_palette"
      ],
      [
        "country_palette"
      ],
      [
        "custom_palette"
      ],
      [
        "delays_to_days"
      ],
      [
        "filter_cols"
      ],
      [
        "filter_df_for_metamean"
      ],
      [
        "filter_df_for_metaprop"
      ],
      [
        "forest_plot"
      ],
      [
        "forest_plot_delay_int"
      ],
      [
        "forest_plot_doubling_time"
      ],
      [
        "forest_plot_incubation_period"
      ],
      [
        "forest_plot_infectious_period"
      ],
      [
        "forest_plot_r0"
      ],
      [
        "forest_plot_rt"
      ],
      [
        "forest_plot_serial_interval"
      ],
      [
        "get_incubation_period"
      ],
      [
        "get_key_columns"
      ],
      [
        "get_parameter"
      ],
      [
        "invert_inverse_params"
      ],
      [
        "load_epidata"
      ],
      [
        "load_epidata_raw"
      ],
      [
        "mark_multiple_estimates"
      ],
      [
        "model_column_type"
      ],
      [
        "outbreak_column_type"
      ],
      [
        "param_pm_uncertainty"
      ],
      [
        "parameter_column_type"
      ],
      [
        "parameter_palette"
      ],
      [
        "pretty_article_label"
      ],
      [
        "priority_pathogens"
      ],
      [
        "qa_questions"
      ],
      [
        "reorder_studies"
      ],
      [
        "reparam_gamma"
      ],
      [
        "shape_palette"
      ],
      [
        "short_parameter_type"
      ],
      [
        "theme_epireview"
      ],
      [
        "update_article_info"
      ],
      [
        "value_type_palette"
      ]
    ],
    "topics": [],
    "score": 6.4874,
    "stars": 32,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "epireview Tools to update and summarise the latest pathogen data from the\nPathogen Epidemiology Review Group (PERG) Contains the latest open access pathogen data from the\nPathogen Epidemiology Review Group (PERG). Tools are available\nto update pathogen databases with new peer-reviewed data as it\nbecomes available, and to summarise the latest data using\ntables and figures. article_column_type assign_qa_score check_column_types check_df_for_meta check_ulim color_palette country_palette custom_palette delays_to_days filter_cols filter_df_for_metamean filter_df_for_metaprop forest_plot forest_plot_delay_int forest_plot_doubling_time forest_plot_incubation_period forest_plot_infectious_period forest_plot_r0 forest_plot_rt forest_plot_serial_interval get_incubation_period get_key_columns get_parameter invert_inverse_params load_epidata load_epidata_raw mark_multiple_estimates model_column_type outbreak_column_type param_pm_uncertainty parameter_column_type parameter_palette pretty_article_label priority_pathogens qa_questions reorder_studies reparam_gamma shape_palette short_parameter_type theme_epireview update_article_info value_type_palette "
  },
  {
    "id": 939,
    "package_name": "paleobioDB",
    "title": "Download and Process Data from the Paleobiology Database",
    "description": "Includes functions to wrap most endpoints of the\n'PaleobioDB' API and to visualize and process the obtained\nfossil data. The API documentation for the Paleobiology\nDatabase can be found at <https://paleobiodb.org/data1.2/>.",
    "version": "1.0.1",
    "maintainer": "Adri\u00e1n Castro Insua <adrian.castro.insua@uvigo.gal>",
    "author": "Sara Varela [aut] (ORCID: <https://orcid.org/0000-0002-5756-5737>),\nJavier Gonz\u00e1lez Hern\u00e1ndez [aut],\nLuciano Fabris Sgarbi [aut] (ORCID:\n<https://orcid.org/0000-0002-0416-3275>),\nAdri\u00e1n Castro Insua [cre, ctb] (ORCID:\n<https://orcid.org/0000-0003-4184-8641>)",
    "url": "https://docs.ropensci.org/paleobioDB/,\nhttps://github.com/ropensci/paleobioDB",
    "bug_reports": "https://github.com/ropensci/paleobioDB/issues",
    "repository": "",
    "exports": [
      [
        "pbdb_collection"
      ],
      [
        "pbdb_collections"
      ],
      [
        "pbdb_collections_geo"
      ],
      [
        "pbdb_interval"
      ],
      [
        "pbdb_intervals"
      ],
      [
        "pbdb_map"
      ],
      [
        "pbdb_map_occur"
      ],
      [
        "pbdb_map_richness"
      ],
      [
        "pbdb_measurements"
      ],
      [
        "pbdb_occurrence"
      ],
      [
        "pbdb_occurrences"
      ],
      [
        "pbdb_opinion"
      ],
      [
        "pbdb_opinions"
      ],
      [
        "pbdb_opinions_taxa"
      ],
      [
        "pbdb_orig_ext"
      ],
      [
        "pbdb_ref_collections"
      ],
      [
        "pbdb_ref_occurrences"
      ],
      [
        "pbdb_ref_specimens"
      ],
      [
        "pbdb_ref_taxa"
      ],
      [
        "pbdb_reference"
      ],
      [
        "pbdb_references"
      ],
      [
        "pbdb_richness"
      ],
      [
        "pbdb_scale"
      ],
      [
        "pbdb_scales"
      ],
      [
        "pbdb_specimen"
      ],
      [
        "pbdb_specimens"
      ],
      [
        "pbdb_strata"
      ],
      [
        "pbdb_strata_auto"
      ],
      [
        "pbdb_subtaxa"
      ],
      [
        "pbdb_taxa"
      ],
      [
        "pbdb_taxa_auto"
      ],
      [
        "pbdb_taxon"
      ],
      [
        "pbdb_temp_range"
      ],
      [
        "pbdb_temporal_resolution"
      ]
    ],
    "topics": [],
    "score": 6.3893,
    "stars": 46,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "paleobioDB Download and Process Data from the Paleobiology Database Includes functions to wrap most endpoints of the\n'PaleobioDB' API and to visualize and process the obtained\nfossil data. The API documentation for the Paleobiology\nDatabase can be found at <https://paleobiodb.org/data1.2/>. pbdb_collection pbdb_collections pbdb_collections_geo pbdb_interval pbdb_intervals pbdb_map pbdb_map_occur pbdb_map_richness pbdb_measurements pbdb_occurrence pbdb_occurrences pbdb_opinion pbdb_opinions pbdb_opinions_taxa pbdb_orig_ext pbdb_ref_collections pbdb_ref_occurrences pbdb_ref_specimens pbdb_ref_taxa pbdb_reference pbdb_references pbdb_richness pbdb_scale pbdb_scales pbdb_specimen pbdb_specimens pbdb_strata pbdb_strata_auto pbdb_subtaxa pbdb_taxa pbdb_taxa_auto pbdb_taxon pbdb_temp_range pbdb_temporal_resolution "
  },
  {
    "id": 330,
    "package_name": "c14bazAAR",
    "title": "Download and Prepare C14 Dates from Different Source Databases",
    "description": "Query different C14 date databases and apply basic data\ncleaning, merging and calibration steps. Currently available\ndatabases: 14cpalaeolithic, 14sea, adrac, agrichange, aida,\naustarch, bda, calpal, caribbean, eubar, euroevol, irdd, jomon,\nkatsianis, kiteeastafrica, medafricarbon, mesorad, neonet,\nneonetatl, nerd, p3k14c, pacea, palmisano, rado.nb, rxpand,\nsard, xronos.",
    "version": "5.2.0",
    "maintainer": "Clemens Schmid <clemens@nevrome.de>",
    "author": "Clemens Schmid [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-3448-5715>),\nDirk Seidensticker [aut] (ORCID:\n<https://orcid.org/0000-0002-8155-7702>),\nDaniel Knitter [aut] (ORCID: <https://orcid.org/0000-0003-3014-4497>),\nMartin Hinz [aut] (ORCID: <https://orcid.org/0000-0002-9904-6548>),\nDavid Matzig [aut] (ORCID: <https://orcid.org/0000-0001-7349-5401>),\nWolfgang Hamer [aut] (ORCID: <https://orcid.org/0000-0002-5943-5020>),\nKay Schmuetz [aut],\nThomas Huet [ctb] (ORCID: <https://orcid.org/0000-0002-1112-6122>),\nNils Mueller-Scheessel [ctb] (ORCID:\n<https://orcid.org/0000-0001-7992-8722>),\nJoe Roe [ctb] (ORCID: <https://orcid.org/0000-0002-1011-1244>),\nBen Marwick [rev] (ORCID: <https://orcid.org/0000-0001-7879-4531>),\nEnrico R. Crema [rev] (ORCID: <https://orcid.org/0000-0001-6727-5138>)",
    "url": "https://docs.ropensci.org/c14bazAAR,\nhttps://github.com/ropensci/c14bazAAR",
    "bug_reports": "https://github.com/ropensci/c14bazAAR/issues",
    "repository": "",
    "exports": [
      [
        "as.c14_date_list"
      ],
      [
        "as.sf"
      ],
      [
        "calibrate"
      ],
      [
        "classify_material"
      ],
      [
        "coordinate_precision"
      ],
      [
        "determine_country_by_coordinate"
      ],
      [
        "enforce_types"
      ],
      [
        "finalize_country_name"
      ],
      [
        "fix_database_country_name"
      ],
      [
        "fuse"
      ],
      [
        "get_14cpalaeolithic"
      ],
      [
        "get_14sea"
      ],
      [
        "get_adrac"
      ],
      [
        "get_agrichange"
      ],
      [
        "get_aida"
      ],
      [
        "get_all_dates"
      ],
      [
        "get_austarch"
      ],
      [
        "get_bda"
      ],
      [
        "get_c14data"
      ],
      [
        "get_calpal"
      ],
      [
        "get_caribbean"
      ],
      [
        "get_context"
      ],
      [
        "get_db_url"
      ],
      [
        "get_db_version"
      ],
      [
        "get_db_version_number"
      ],
      [
        "get_emedyd"
      ],
      [
        "get_eubar"
      ],
      [
        "get_euroevol"
      ],
      [
        "get_irdd"
      ],
      [
        "get_jomon"
      ],
      [
        "get_katsianis"
      ],
      [
        "get_kiteeastafrica"
      ],
      [
        "get_medafricarbon"
      ],
      [
        "get_mesorad"
      ],
      [
        "get_neonet"
      ],
      [
        "get_neonetatl"
      ],
      [
        "get_nerd"
      ],
      [
        "get_p3k14c"
      ],
      [
        "get_pacea"
      ],
      [
        "get_palmisano"
      ],
      [
        "get_rado.nb"
      ],
      [
        "get_radon"
      ],
      [
        "get_radonb"
      ],
      [
        "get_rxpand"
      ],
      [
        "get_sard"
      ],
      [
        "get_xronos"
      ],
      [
        "is.c14_date_list"
      ],
      [
        "mark_duplicates"
      ],
      [
        "order_variables"
      ],
      [
        "remove_duplicates"
      ],
      [
        "standardize_country_name"
      ],
      [
        "write_c14"
      ]
    ],
    "topics": [
      [
        "archaeology"
      ],
      [
        "radiocarbon-dates"
      ]
    ],
    "score": 6.2529,
    "stars": 31,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "c14bazAAR Download and Prepare C14 Dates from Different Source Databases Query different C14 date databases and apply basic data\ncleaning, merging and calibration steps. Currently available\ndatabases: 14cpalaeolithic, 14sea, adrac, agrichange, aida,\naustarch, bda, calpal, caribbean, eubar, euroevol, irdd, jomon,\nkatsianis, kiteeastafrica, medafricarbon, mesorad, neonet,\nneonetatl, nerd, p3k14c, pacea, palmisano, rado.nb, rxpand,\nsard, xronos. as.c14_date_list as.sf calibrate classify_material coordinate_precision determine_country_by_coordinate enforce_types finalize_country_name fix_database_country_name fuse get_14cpalaeolithic get_14sea get_adrac get_agrichange get_aida get_all_dates get_austarch get_bda get_c14data get_calpal get_caribbean get_context get_db_url get_db_version get_db_version_number get_emedyd get_eubar get_euroevol get_irdd get_jomon get_katsianis get_kiteeastafrica get_medafricarbon get_mesorad get_neonet get_neonetatl get_nerd get_p3k14c get_pacea get_palmisano get_rado.nb get_radon get_radonb get_rxpand get_sard get_xronos is.c14_date_list mark_duplicates order_variables remove_duplicates standardize_country_name write_c14 archaeology radiocarbon-dates"
  },
  {
    "id": 600,
    "package_name": "gbifdb",
    "title": "High Performance Interface to 'GBIF'",
    "description": "A high performance interface to the Global Biodiversity\nInformation Facility, 'GBIF'.  In contrast to 'rgbif', which\ncan access small subsets of 'GBIF' data through web-based\nqueries to a central server, 'gbifdb' provides enhanced\nperformance for R users performing large-scale analyses on\nservers and cloud computing providers, providing full support\nfor arbitrary 'SQL' or 'dplyr' operations on the complete\n'GBIF' data tables (now over 1 billion records, and over a\nterabyte in size). 'gbifdb' accesses a copy of the 'GBIF' data\nin 'parquet' format, which is already readily available in\ncommercial computing clouds such as the Amazon Open Data portal\nand the Microsoft Planetary Computer, or can be accessed\ndirectly without downloading, or downloaded to any server with\nsuitable bandwidth and storage space. The high-performance\ntechniques for local and remote access are described in\n<https://duckdb.org/why_duckdb> and\n<https://arrow.apache.org/docs/r/articles/fs.html>\nrespectively.",
    "version": "1.0.0.99",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nMarc-Olivier Beausoleil [ctb] (ORCID:\n<https://orcid.org/0000-0003-3717-3223>)",
    "url": "https://docs.ropensci.org/gbifdb/,\nhttps://github.com/ropensci/gbifdb",
    "bug_reports": "https://github.com/ropensci/gbifdb",
    "repository": "",
    "exports": [
      [
        "gbif_dir"
      ],
      [
        "gbif_download"
      ],
      [
        "gbif_example_data"
      ],
      [
        "gbif_local"
      ],
      [
        "gbif_remote"
      ],
      [
        "gbif_version"
      ]
    ],
    "topics": [],
    "score": 6.1072,
    "stars": 40,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "gbifdb High Performance Interface to 'GBIF' A high performance interface to the Global Biodiversity\nInformation Facility, 'GBIF'.  In contrast to 'rgbif', which\ncan access small subsets of 'GBIF' data through web-based\nqueries to a central server, 'gbifdb' provides enhanced\nperformance for R users performing large-scale analyses on\nservers and cloud computing providers, providing full support\nfor arbitrary 'SQL' or 'dplyr' operations on the complete\n'GBIF' data tables (now over 1 billion records, and over a\nterabyte in size). 'gbifdb' accesses a copy of the 'GBIF' data\nin 'parquet' format, which is already readily available in\ncommercial computing clouds such as the Amazon Open Data portal\nand the Microsoft Planetary Computer, or can be accessed\ndirectly without downloading, or downloaded to any server with\nsuitable bandwidth and storage space. The high-performance\ntechniques for local and remote access are described in\n<https://duckdb.org/why_duckdb> and\n<https://arrow.apache.org/docs/r/articles/fs.html>\nrespectively. gbif_dir gbif_download gbif_example_data gbif_local gbif_remote gbif_version "
  },
  {
    "id": 886,
    "package_name": "nomisr",
    "title": "Access 'Nomis' UK Labour Market Data",
    "description": "Access UK official statistics from the 'Nomis' database.\n'Nomis' includes data from the Census, the Labour Force Survey,\nDWP benefit statistics and other economic and demographic data\nfrom the Office for National Statistics, based around\nstatistical geographies. See\n<https://www.nomisweb.co.uk/api/v01/help> for full API\ndocumentation.",
    "version": "0.4.7",
    "maintainer": "Evan Odell <evanodell91@gmail.com>",
    "author": "Evan Odell [aut, cre] (ORCID: <https://orcid.org/0000-0003-1845-808X>),\nPaul Egeler [rev, ctb],\nChristophe Dervieux [rev] (ORCID:\n<https://orcid.org/0000-0003-4474-2498>),\nNina Robery [ctb] (Work and Health Indicators with nomisr vignette)",
    "url": "https://github.com/ropensci/nomisr,\nhttps://docs.evanodell.com/nomisr",
    "bug_reports": "https://github.com/ropensci/nomisr/issues",
    "repository": "",
    "exports": [
      [
        "nomis_api_key"
      ],
      [
        "nomis_codelist"
      ],
      [
        "nomis_content_type"
      ],
      [
        "nomis_data_info"
      ],
      [
        "nomis_get_data"
      ],
      [
        "nomis_get_metadata"
      ],
      [
        "nomis_overview"
      ],
      [
        "nomis_search"
      ]
    ],
    "topics": [
      [
        "api-client"
      ],
      [
        "census-data"
      ],
      [
        "demography"
      ],
      [
        "geographic-data"
      ],
      [
        "national-statistics"
      ],
      [
        "official-statistics"
      ],
      [
        "officialstatistics"
      ],
      [
        "peer-reviewed"
      ],
      [
        "uk"
      ]
    ],
    "score": 6.0617,
    "stars": 51,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "nomisr Access 'Nomis' UK Labour Market Data Access UK official statistics from the 'Nomis' database.\n'Nomis' includes data from the Census, the Labour Force Survey,\nDWP benefit statistics and other economic and demographic data\nfrom the Office for National Statistics, based around\nstatistical geographies. See\n<https://www.nomisweb.co.uk/api/v01/help> for full API\ndocumentation. nomis_api_key nomis_codelist nomis_content_type nomis_data_info nomis_get_data nomis_get_metadata nomis_overview nomis_search api-client census-data demography geographic-data national-statistics official-statistics officialstatistics peer-reviewed uk"
  },
  {
    "id": 533,
    "package_name": "epiparameterDB",
    "title": "Database of Epidemiological Parameters",
    "description": "A data package containing a database of epidemiological\nparameters. It stores the data for the 'epiparameter' R\npackage. Epidemiological parameter estimates are extracted from\nthe literature.",
    "version": "0.1.0.9000",
    "maintainer": "Joshua W. Lambert <joshua.lambert@lshtm.ac.uk>",
    "author": "Joshua W. Lambert [cre, aut, cph] (ORCID:\n<https://orcid.org/0000-0001-5218-3046>),\nAdam Kucharski [aut] (ORCID: <https://orcid.org/0000-0001-8814-9421>),\nCarmen Tamayo Cuartero [aut] (ORCID:\n<https://orcid.org/0000-0003-4184-2864>),\nLondon School of Hygiene and Tropical Medicine, LSHTM [cph] (ROR:\n<https://ror.org/00a0jsq62>)",
    "url": "https://github.com/epiverse-trace/epiparameterDB/,\nhttps://epiverse-trace.github.io/epiparameterDB/",
    "bug_reports": "https://github.com/epiverse-trace/epiparameterDB/issues",
    "repository": "",
    "exports": [],
    "topics": [
      [
        "data-package"
      ],
      [
        "epidemiology"
      ],
      [
        "epiverse"
      ]
    ],
    "score": 6.0334,
    "stars": 2,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "epiparameterDB Database of Epidemiological Parameters A data package containing a database of epidemiological\nparameters. It stores the data for the 'epiparameter' R\npackage. Epidemiological parameter estimates are extracted from\nthe literature.  data-package epidemiology epiverse"
  },
  {
    "id": 1419,
    "package_name": "virtuoso",
    "title": "Interface to 'Virtuoso' using 'ODBC'",
    "description": "Provides users with a simple and convenient mechanism to\nmanage and query a 'Virtuoso' database using the 'DBI'\n(Data-Base Interface) compatible 'ODBC' (Open Database\nConnectivity) interface. 'Virtuoso' is a high-performance\n\"universal server,\" which can act as both a relational\ndatabase, supporting standard Structured Query Language ('SQL')\nqueries, while also supporting data following the Resource\nDescription Framework ('RDF') model for Linked Data. 'RDF' data\ncan be queried using 'SPARQL' ('SPARQL' Protocol and 'RDF'\nQuery Language) queries, a graph-based query that supports\nsemantic reasoning. This allows users to leverage the\nperformance of local or remote 'Virtuoso' servers using popular\n'R' packages such as 'DBI' and 'dplyr', while also providing a\nhigh-performance solution for working with large 'RDF'\n'triplestores' from 'R.' The package also provides helper\nroutines to install, launch, and manage a 'Virtuoso' server\nlocally on 'Mac', 'Windows' and 'Linux' platforms using the\nstandard interactive installers from the 'R' command-line.  By\nautomatically handling these setup steps, the package can make\nusing 'Virtuoso' considerably faster and easier for a most\nusers to deploy in a local environment. Managing the bulk\nimport of triples from common serializations with a single\nintuitive command is another key feature of this package.  Bulk\nimport performance can be tens to hundreds of times faster than\nthe comparable imports using existing 'R' tools, including\n'rdflib' and 'redland' packages.",
    "version": "0.1.8",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nBryce Mecum [ctb] (ORCID: <https://orcid.org/0000-0002-0381-3766>)",
    "url": "https://github.com/ropensci/virtuoso",
    "bug_reports": "https://github.com/ropensci/virtuoso/issues",
    "repository": "",
    "exports": [
      [
        "has_virtuoso"
      ],
      [
        "vos_configure"
      ],
      [
        "vos_connect"
      ],
      [
        "vos_delete_db"
      ],
      [
        "vos_destroy_all"
      ],
      [
        "vos_import"
      ],
      [
        "vos_install"
      ],
      [
        "vos_kill"
      ],
      [
        "vos_list_graphs"
      ],
      [
        "vos_log"
      ],
      [
        "vos_odbcinst"
      ],
      [
        "vos_process"
      ],
      [
        "vos_query"
      ],
      [
        "vos_set_paths"
      ],
      [
        "vos_start"
      ],
      [
        "vos_status"
      ],
      [
        "vos_uninstall"
      ]
    ],
    "topics": [],
    "score": 6.0111,
    "stars": 9,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "virtuoso Interface to 'Virtuoso' using 'ODBC' Provides users with a simple and convenient mechanism to\nmanage and query a 'Virtuoso' database using the 'DBI'\n(Data-Base Interface) compatible 'ODBC' (Open Database\nConnectivity) interface. 'Virtuoso' is a high-performance\n\"universal server,\" which can act as both a relational\ndatabase, supporting standard Structured Query Language ('SQL')\nqueries, while also supporting data following the Resource\nDescription Framework ('RDF') model for Linked Data. 'RDF' data\ncan be queried using 'SPARQL' ('SPARQL' Protocol and 'RDF'\nQuery Language) queries, a graph-based query that supports\nsemantic reasoning. This allows users to leverage the\nperformance of local or remote 'Virtuoso' servers using popular\n'R' packages such as 'DBI' and 'dplyr', while also providing a\nhigh-performance solution for working with large 'RDF'\n'triplestores' from 'R.' The package also provides helper\nroutines to install, launch, and manage a 'Virtuoso' server\nlocally on 'Mac', 'Windows' and 'Linux' platforms using the\nstandard interactive installers from the 'R' command-line.  By\nautomatically handling these setup steps, the package can make\nusing 'Virtuoso' considerably faster and easier for a most\nusers to deploy in a local environment. Managing the bulk\nimport of triples from common serializations with a single\nintuitive command is another key feature of this package.  Bulk\nimport performance can be tens to hundreds of times faster than\nthe comparable imports using existing 'R' tools, including\n'rdflib' and 'redland' packages. has_virtuoso vos_configure vos_connect vos_delete_db vos_destroy_all vos_import vos_install vos_kill vos_list_graphs vos_log vos_odbcinst vos_process vos_query vos_set_paths vos_start vos_status vos_uninstall "
  },
  {
    "id": 292,
    "package_name": "bikedata",
    "title": "Download and Aggregate Data from Public Hire Bicycle Systems",
    "description": "Download and aggregate data from all public hire bicycle\nsystems which provide open data, currently including\n'Santander' Cycles in London, U.K.; from the U.S.A., 'Ford\nGoBike' in San Francisco CA, 'citibike' in New York City NY,\n'Divvy' in Chicago IL, 'Capital Bikeshare' in Washington DC,\n'Hubway' in Boston MA, 'Metro' in Los Angeles LA, 'Indego' in\nPhiladelphia PA, and 'Nice Ride' in Minnesota; 'Bixi' from\nMontreal, Canada; and 'mibici' from Guadalajara, Mexico.",
    "version": "0.2.5.046",
    "maintainer": "Mark Padgham <mark.padgham@email.com>",
    "author": "Mark Padgham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2172-5265>),\nRichard Ellison [aut],\nTom Buckley [aut],\nRyszard Szyma\u0144ski [ctb],\nBea Hern\u00e1ndez [rev] (Bea reviewed the package for ropensci, see\nhttps://github.com/ropensci/onboarding/issues/116),\nElaine McVey [rev] (Elaine reviewed the package for ropensci, see\nhttps://github.com/ropensci/onboarding/issues/116),\nSQLite Consortium [ctb] (Authors of included SQLite code)",
    "url": "https://docs.ropensci.org/bikedata/,\nhttps://github.com/ropensci/bikedata",
    "bug_reports": "https://github.com/ropensci/bikedata/issues",
    "repository": "",
    "exports": [
      [
        "bike_cities"
      ],
      [
        "bike_daily_trips"
      ],
      [
        "bike_datelimits"
      ],
      [
        "bike_db_totals"
      ],
      [
        "bike_demographic_data"
      ],
      [
        "bike_distmat"
      ],
      [
        "bike_latest_files"
      ],
      [
        "bike_match_matrices"
      ],
      [
        "bike_rm_db"
      ],
      [
        "bike_rm_test_data"
      ],
      [
        "bike_stations"
      ],
      [
        "bike_stored_files"
      ],
      [
        "bike_summary_stats"
      ],
      [
        "bike_tripmat"
      ],
      [
        "bike_write_test_data"
      ],
      [
        "dl_bikedata"
      ],
      [
        "download_bikedata"
      ],
      [
        "index_bikedata_db"
      ],
      [
        "store_bikedata"
      ]
    ],
    "topics": [
      [
        "bicycle-hire-systems"
      ],
      [
        "bike-hire-systems"
      ],
      [
        "bike-hire"
      ],
      [
        "bicycle-hire"
      ],
      [
        "database"
      ],
      [
        "bike-data"
      ],
      [
        "peer-reviewed"
      ],
      [
        "cpp"
      ]
    ],
    "score": 5.9877,
    "stars": 81,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "bikedata Download and Aggregate Data from Public Hire Bicycle Systems Download and aggregate data from all public hire bicycle\nsystems which provide open data, currently including\n'Santander' Cycles in London, U.K.; from the U.S.A., 'Ford\nGoBike' in San Francisco CA, 'citibike' in New York City NY,\n'Divvy' in Chicago IL, 'Capital Bikeshare' in Washington DC,\n'Hubway' in Boston MA, 'Metro' in Los Angeles LA, 'Indego' in\nPhiladelphia PA, and 'Nice Ride' in Minnesota; 'Bixi' from\nMontreal, Canada; and 'mibici' from Guadalajara, Mexico. bike_cities bike_daily_trips bike_datelimits bike_db_totals bike_demographic_data bike_distmat bike_latest_files bike_match_matrices bike_rm_db bike_rm_test_data bike_stations bike_stored_files bike_summary_stats bike_tripmat bike_write_test_data dl_bikedata download_bikedata index_bikedata_db store_bikedata bicycle-hire-systems bike-hire-systems bike-hire bicycle-hire database bike-data peer-reviewed cpp"
  },
  {
    "id": 503,
    "package_name": "dwctaxon",
    "title": "Edit and Validate Darwin Core Taxon Data",
    "description": "Edit and validate taxonomic data in compliance with Darwin\nCore standards (Darwin Core 'Taxon' class\n<https://dwc.tdwg.org/terms/#taxon>).",
    "version": "2.0.4",
    "maintainer": "Joel H. Nitta <joelnitta@gmail.com>",
    "author": "Joel H. Nitta [aut, cre, cph] (ORCID:\n<https://orcid.org/0000-0003-4719-7472>),\nWataru Iwasaki [ctb] (ORCID: <https://orcid.org/0000-0002-9169-9245>),\nCollin Schwantes [rev] (Collin reviewed the package (v. 1.0.0.9000) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/574>),\nStephen Formel [rev] (Stephen reviewed the package (v. 1.0.0.9000) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/574>)",
    "url": "https://docs.ropensci.org/dwctaxon/,\nhttps://github.com/ropensci/dwctaxon",
    "bug_reports": "https://github.com/ropensci/dwctaxon/issues",
    "repository": "",
    "exports": [
      [
        "dct_add_row"
      ],
      [
        "dct_check_mapping"
      ],
      [
        "dct_check_sci_name"
      ],
      [
        "dct_check_tax_status"
      ],
      [
        "dct_check_taxon_id"
      ],
      [
        "dct_drop_row"
      ],
      [
        "dct_fill_col"
      ],
      [
        "dct_modify_row"
      ],
      [
        "dct_options"
      ],
      [
        "dct_validate"
      ]
    ],
    "topics": [
      [
        "database"
      ]
    ],
    "score": 5.9786,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "dwctaxon Edit and Validate Darwin Core Taxon Data Edit and validate taxonomic data in compliance with Darwin\nCore standards (Darwin Core 'Taxon' class\n<https://dwc.tdwg.org/terms/#taxon>). dct_add_row dct_check_mapping dct_check_sci_name dct_check_tax_status dct_check_taxon_id dct_drop_row dct_fill_col dct_modify_row dct_options dct_validate database"
  },
  {
    "id": 174,
    "package_name": "Rpolyhedra",
    "title": "Polyhedra Database",
    "description": "A polyhedra database scraped from various sources as R6\nobjects and 'rgl' visualizing capabilities.",
    "version": "0.5.6",
    "maintainer": "Alejandro Baranek <abaranek@dc.uba.ar>",
    "author": "Alejandro Baranek [aut, com, cre, cph],\nLeonardo Belen [aut, com, cph],\nqbotics [cph],\nBarret Schloerke [rev],\nLijia Yu [rev]",
    "url": "https://docs.ropensci.org/Rpolyhedra/,\nhttps://github.com/ropensci/Rpolyhedra",
    "bug_reports": "https://github.com/ropensci/Rpolyhedra/issues",
    "repository": "",
    "exports": [
      [
        "genLogger"
      ],
      [
        "getAvailablePolyhedra"
      ],
      [
        "getAvailableSources"
      ],
      [
        "getLogger"
      ],
      [
        "getPolyhedraObject"
      ],
      [
        "getPolyhedron"
      ],
      [
        "loggerSetupFile"
      ],
      [
        "mutate_cond"
      ],
      [
        "polyhedronToXML"
      ],
      [
        "switchToFullDatabase"
      ]
    ],
    "topics": [
      [
        "geometry"
      ],
      [
        "polyhedra-database"
      ],
      [
        "rgl"
      ]
    ],
    "score": 5.8215,
    "stars": 13,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "Rpolyhedra Polyhedra Database A polyhedra database scraped from various sources as R6\nobjects and 'rgl' visualizing capabilities. genLogger getAvailablePolyhedra getAvailableSources getLogger getPolyhedraObject getPolyhedron loggerSetupFile mutate_cond polyhedronToXML switchToFullDatabase geometry polyhedra-database rgl"
  },
  {
    "id": 1120,
    "package_name": "rmangal",
    "title": "'Mangal' Client",
    "description": "An interface to the 'Mangal' database - a collection of\necological networks. This package includes functions to work\nwith the 'Mangal RESTful API' methods\n(<https://mangal-interactions.github.io/mangal-api/>).",
    "version": "2.2.0",
    "maintainer": "Kevin Cazelles <kevin.cazelles@gmail.com>",
    "author": "Steve Vissault [aut, ctb] (ORCID:\n<https://orcid.org/0000-0002-0866-4376>),\nKevin Cazelles [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-6619-9874>),\nGabriel Bergeron [aut, ctb],\nBenjamin Mercier [aut, ctb],\nCl\u00e9ment Violet [aut, ctb],\nDominique Gravel [aut],\nTimoth\u00e9e Poisot [aut],\nThomas Lin Pedersen [rev] (ORCID:\n<https://orcid.org/0000-0002-5147-4711>),\nAnna Willoughby [rev] (ORCID: <https://orcid.org/0000-0002-0504-0605>)",
    "url": "https://docs.ropensci.org/rmangal/, https://mangal.io,\nhttps://github.com/ropensci/rmangal",
    "bug_reports": "https://github.com/ropensci/rmangal/issues",
    "repository": "",
    "exports": [
      [
        "as.igraph"
      ],
      [
        "combine_mgNetworks"
      ],
      [
        "get_citation"
      ],
      [
        "get_collection"
      ],
      [
        "get_network_by_id"
      ],
      [
        "get_network_by_id_indiv"
      ],
      [
        "rmangal_request"
      ],
      [
        "rmangal_request_singleton"
      ],
      [
        "search_datasets"
      ],
      [
        "search_interactions"
      ],
      [
        "search_networks"
      ],
      [
        "search_networks_sf"
      ],
      [
        "search_nodes"
      ],
      [
        "search_references"
      ],
      [
        "search_taxonomy"
      ]
    ],
    "topics": [
      [
        "ecology"
      ],
      [
        "networks"
      ],
      [
        "food webs"
      ],
      [
        "interactions"
      ],
      [
        "data\npublications"
      ],
      [
        "open access"
      ]
    ],
    "score": 5.5653,
    "stars": 15,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rmangal 'Mangal' Client An interface to the 'Mangal' database - a collection of\necological networks. This package includes functions to work\nwith the 'Mangal RESTful API' methods\n(<https://mangal-interactions.github.io/mangal-api/>). as.igraph combine_mgNetworks get_citation get_collection get_network_by_id get_network_by_id_indiv rmangal_request rmangal_request_singleton search_datasets search_interactions search_networks search_networks_sf search_nodes search_references search_taxonomy ecology networks food webs interactions data\npublications open access"
  },
  {
    "id": 678,
    "package_name": "hddtools",
    "title": "Hydrological Data Discovery Tools",
    "description": "Tools to discover hydrological data, accessing catalogues\nand databases from various data providers. The package is\ndescribed in Vitolo (2017) \"hddtools: Hydrological Data\nDiscovery Tools\" <doi:10.21105/joss.00056>.",
    "version": "0.9.5",
    "maintainer": "Dorothea Hug Peter <dorothea.hug@wsl.ch>",
    "author": "Claudia Vitolo [aut] (ORCID: <https://orcid.org/0000-0002-4252-1176>),\nWouter Buytaert [ctb] (Supervisor),\nErin Le Dell [ctb] (Erin reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/73),\nMichael Sumner [ctb] (Michael reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/73),\nDorothea Hug Peter [aut, cre]",
    "url": "https://docs.ropensci.org/hddtools/,\nhttps://github.com/ropensci/hddtools",
    "bug_reports": "https://github.com/ropensci/hddtools/issues",
    "repository": "",
    "exports": [
      [
        "bboxSpatialPolygon"
      ],
      [
        "catalogueData60UK"
      ],
      [
        "catalogueGRDC"
      ],
      [
        "catalogueMOPEX"
      ],
      [
        "catalogueSEPA"
      ],
      [
        "KGClimateClass"
      ],
      [
        "tsData60UK"
      ],
      [
        "tsMOPEX"
      ],
      [
        "tsSEPA"
      ]
    ],
    "topics": [
      [
        "data60uk"
      ],
      [
        "grdc"
      ],
      [
        "hydrology"
      ],
      [
        "kgclimateclass"
      ],
      [
        "mopex"
      ],
      [
        "peer-reviewed"
      ],
      [
        "precipitation"
      ],
      [
        "sepa"
      ]
    ],
    "score": 5.5563,
    "stars": 48,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "hddtools Hydrological Data Discovery Tools Tools to discover hydrological data, accessing catalogues\nand databases from various data providers. The package is\ndescribed in Vitolo (2017) \"hddtools: Hydrological Data\nDiscovery Tools\" <doi:10.21105/joss.00056>. bboxSpatialPolygon catalogueData60UK catalogueGRDC catalogueMOPEX catalogueSEPA KGClimateClass tsData60UK tsMOPEX tsSEPA data60uk grdc hydrology kgclimateclass mopex peer-reviewed precipitation sepa"
  },
  {
    "id": 1082,
    "package_name": "refsplitr",
    "title": "author name disambiguation, author georeferencing, and mapping\nof coauthorship networks with 'Web of Science' data",
    "description": "Tools to parse and organize reference records downloaded\nfrom the 'Web of Science' citation database into an R-friendly\nformat, disambiguate the names of authors, geocode their\nlocations, and generate/visualize coauthorship networks. This\npackage has been peer-reviewed by rOpenSci (v. 1.0).",
    "version": "1.2.0",
    "maintainer": "Emilio Bruna <embruna@ufl.edu>",
    "author": "Auriel M.V. Fournier [aut],\nMatthew E. Boone [aut],\nForrest R. Stevens [aut],\nEmilio Bruna [aut, cre],\nBianca Kramer [rev] (Kramer reviewed the package (v 1.0) for rOpenSci,\nsee <https://github.com/ropensci/software-review/issues/256>),\nNajko Jahn [rev] (Jahn reviewed the package (v1.0) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/256>)",
    "url": "https://github.com/ropensci/refsplitr,\nhttps://docs.ropensci.org/refsplitr/",
    "bug_reports": "https://github.com/ropensci/refsplitr/issues",
    "repository": "",
    "exports": [
      [
        "authors_clean"
      ],
      [
        "authors_georef"
      ],
      [
        "authors_refine"
      ],
      [
        "plot_addresses_country"
      ],
      [
        "plot_addresses_points"
      ],
      [
        "plot_net_address"
      ],
      [
        "plot_net_coauthor"
      ],
      [
        "plot_net_country"
      ],
      [
        "references_read"
      ]
    ],
    "topics": [
      [
        "name disambiguation"
      ],
      [
        "bibliometrics"
      ],
      [
        "coauthorship"
      ],
      [
        "collaboration"
      ],
      [
        "georeferencing"
      ],
      [
        "metascience"
      ],
      [
        "references"
      ],
      [
        "scientometrics"
      ],
      [
        "science of science"
      ],
      [
        "web of science"
      ]
    ],
    "score": 5.5,
    "stars": 55,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "refsplitr author name disambiguation, author georeferencing, and mapping\nof coauthorship networks with 'Web of Science' data Tools to parse and organize reference records downloaded\nfrom the 'Web of Science' citation database into an R-friendly\nformat, disambiguate the names of authors, geocode their\nlocations, and generate/visualize coauthorship networks. This\npackage has been peer-reviewed by rOpenSci (v. 1.0). authors_clean authors_georef authors_refine plot_addresses_country plot_addresses_points plot_net_address plot_net_coauthor plot_net_country references_read name disambiguation bibliometrics coauthorship collaboration georeferencing metascience references scientometrics science of science web of science"
  },
  {
    "id": 438,
    "package_name": "datamodelr",
    "title": "Define and Plot Data Model Diagrams",
    "description": "Data model definition with YAML file, extract from R data\nframes, reverse-engineering from PostgreSQL and SQL Server and\nrendering with DiagrammeR/graphviz.",
    "version": "0.2.2.9002",
    "maintainer": "Darko Bergant <darko.bergant@gmail.com>",
    "author": "Darko Bergant",
    "url": "https://github.com/bergant/datamodelr",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "as.data_model"
      ],
      [
        "dm_add_colors"
      ],
      [
        "dm_add_reference_"
      ],
      [
        "dm_add_references"
      ],
      [
        "dm_color_scheme"
      ],
      [
        "dm_create_graph"
      ],
      [
        "dm_create_references"
      ],
      [
        "dm_export_graph"
      ],
      [
        "dm_from_data_frames"
      ],
      [
        "dm_get_color_scheme"
      ],
      [
        "dm_get_graph_svg"
      ],
      [
        "dm_list2coltable"
      ],
      [
        "dm_palette"
      ],
      [
        "dm_re_query"
      ],
      [
        "dm_read_yaml"
      ],
      [
        "dm_render_graph"
      ],
      [
        "dm_set_col_attr"
      ],
      [
        "dm_set_color_scheme"
      ],
      [
        "dm_set_display"
      ],
      [
        "dm_set_key"
      ],
      [
        "dm_set_segment"
      ],
      [
        "dot_html_label"
      ],
      [
        "is.data_model"
      ],
      [
        "to_html_table"
      ]
    ],
    "topics": [
      [
        "data-structures"
      ],
      [
        "model-diagram"
      ]
    ],
    "score": 5.3227,
    "stars": 288,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "datamodelr Define and Plot Data Model Diagrams Data model definition with YAML file, extract from R data\nframes, reverse-engineering from PostgreSQL and SQL Server and\nrendering with DiagrammeR/graphviz. as.data_model dm_add_colors dm_add_reference_ dm_add_references dm_color_scheme dm_create_graph dm_create_references dm_export_graph dm_from_data_frames dm_get_color_scheme dm_get_graph_svg dm_list2coltable dm_palette dm_re_query dm_read_yaml dm_render_graph dm_set_col_attr dm_set_color_scheme dm_set_display dm_set_key dm_set_segment dot_html_label is.data_model to_html_table data-structures model-diagram"
  },
  {
    "id": 767,
    "package_name": "liteq",
    "title": "Lightweight Portable Message Queue Using 'SQLite'",
    "description": "Temporary and permanent message queues for R. Built on top\nof 'SQLite' databases. 'SQLite' provides locking, and makes it\npossible to detect crashed consumers. Crashed jobs can be\nautomatically marked as \"failed\", or put in the queue again,\npotentially a limited number of times.",
    "version": "1.1.0",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi [aut, cre],\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/r-lib/liteq#readme",
    "bug_reports": "https://github.com/r-lib/liteq/issues",
    "repository": "",
    "exports": [
      [
        "ack"
      ],
      [
        "consume"
      ],
      [
        "create_queue"
      ],
      [
        "default_db"
      ],
      [
        "delete_queue"
      ],
      [
        "ensure_queue"
      ],
      [
        "is_empty"
      ],
      [
        "list_failed_messages"
      ],
      [
        "list_messages"
      ],
      [
        "list_queues"
      ],
      [
        "message_count"
      ],
      [
        "nack"
      ],
      [
        "publish"
      ],
      [
        "remove_failed_messages"
      ],
      [
        "requeue_failed_messages"
      ],
      [
        "try_consume"
      ]
    ],
    "topics": [],
    "score": 5.274,
    "stars": 58,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "liteq Lightweight Portable Message Queue Using 'SQLite' Temporary and permanent message queues for R. Built on top\nof 'SQLite' databases. 'SQLite' provides locking, and makes it\npossible to detect crashed consumers. Crashed jobs can be\nautomatically marked as \"failed\", or put in the queue again,\npotentially a limited number of times. ack consume create_queue default_db delete_queue ensure_queue is_empty list_failed_messages list_messages list_queues message_count nack publish remove_failed_messages requeue_failed_messages try_consume "
  },
  {
    "id": 1151,
    "package_name": "rscontract",
    "title": "Generic implementation of the 'RStudio' connections contract",
    "description": "Provides a generic implementation of the 'RStudio'\nconnection contract to make it easier for database connections,\nand other type of connections, opened via R packages integrate\nwith the connections pane inside the 'RStudio' interactive\ndevelopment environment (IDE).",
    "version": "0.1.2",
    "maintainer": "Nathan Stephens <nathan@rstudio.com>",
    "author": "Nathan Stephens [aut, cre],\nEdgar Ruiz [aut]",
    "url": "https://github.com/rstudio/rscontract",
    "bug_reports": "https://github.com/rstudio/rscontract/issues",
    "repository": "",
    "exports": [
      [
        "as_rscontract"
      ],
      [
        "rscontract_close"
      ],
      [
        "rscontract_ide"
      ],
      [
        "rscontract_open"
      ],
      [
        "rscontract_spec"
      ],
      [
        "rscontract_update"
      ],
      [
        "sample_catalog"
      ]
    ],
    "topics": [
      [
        "connections-pane"
      ],
      [
        "rstudio"
      ]
    ],
    "score": 5.1206,
    "stars": 22,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "rscontract Generic implementation of the 'RStudio' connections contract Provides a generic implementation of the 'RStudio'\nconnection contract to make it easier for database connections,\nand other type of connections, opened via R packages integrate\nwith the connections pane inside the 'RStudio' interactive\ndevelopment environment (IDE). as_rscontract rscontract_close rscontract_ide rscontract_open rscontract_spec rscontract_update sample_catalog connections-pane rstudio"
  },
  {
    "id": 681,
    "package_name": "helminthR",
    "title": "Access London Natural History Museum Host-Helminth Record\nDatabase",
    "description": "Access to large host-parasite data is often hampered by\nthe availability of data and difficulty in obtaining it in a\nprogrammatic way to encourage analyses. 'helminthR' provides\naccesss to the London Natural History Museum's host-parasite\ndatabase, one of the largest host-parasite databases existing\ncurrently.",
    "version": "1.1.0",
    "maintainer": "Tad Dallas <tad.a.dallas@gmail.com>",
    "author": "Tad Dallas [aut, cre]",
    "url": "https://docs.ropensci.org/helminthR/,\nhttps://github.com/rOpenSci/helminthR/",
    "bug_reports": "https://github.com/rOpenSci/helminthR/issues/",
    "repository": "",
    "exports": [
      [
        "cleanData"
      ],
      [
        "loadData"
      ]
    ],
    "topics": [
      [
        "disease-networks"
      ],
      [
        "helminth"
      ],
      [
        "open-data"
      ],
      [
        "parasites"
      ]
    ],
    "score": 5.1139,
    "stars": 10,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "helminthR Access London Natural History Museum Host-Helminth Record\nDatabase Access to large host-parasite data is often hampered by\nthe availability of data and difficulty in obtaining it in a\nprogrammatic way to encourage analyses. 'helminthR' provides\naccesss to the London Natural History Museum's host-parasite\ndatabase, one of the largest host-parasite databases existing\ncurrently. cleanData loadData disease-networks helminth open-data parasites"
  },
  {
    "id": 352,
    "package_name": "censo2017",
    "title": "Base de Datos de Facil Acceso del Censo 2017 de Chile (2017\nChilean Census Easy Access Database)",
    "description": "Provee un acceso conveniente a mas de 17 millones de\nregistros de la base de datos del Censo 2017. Los datos fueron\nimportados desde el DVD oficial del INE usando el Convertidor\nREDATAM creado por Pablo De Grande. Esta paquete esta\ndocumentado intencionalmente en castellano asciificado para que\nfuncione sin problema en diferentes plataformas. (Provides\nconvenient access to more than 17 million records from the\nChilean Census 2017 database. The datasets were imported from\nthe official DVD provided by the Chilean National Bureau of\nStatistics by using the REDATAM converter created by Pablo De\nGrande and in addition it includes the maps accompanying these\ndatasets.)",
    "version": "0.6.1",
    "maintainer": "Mauricio Vargas <mavargas11@uc.cl>",
    "author": "Mauricio Vargas [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1017-7574>),\nJuan Correa [ctb],\nMaria Paula Caldas [rev] (rOpenSci),\nFrans van Dunn\u00e9 [rev] (rOpenSci),\nMelina Vidoni [rev] (rOpenSci),\nConstanza Manriquez [rev] (revision independiente de las vinietas),\nInstituto Nacional de Estadisticas (INE) [dtc]",
    "url": "https://docs.ropensci.org/censo2017/",
    "bug_reports": "https://github.com/ropensci/censo2017/issues/",
    "repository": "",
    "exports": [
      [
        "censo_conectar"
      ],
      [
        "censo_descargar"
      ],
      [
        "censo_desconectar"
      ],
      [
        "censo_eliminar"
      ],
      [
        "censo_tabla"
      ]
    ],
    "topics": [
      [
        "censo"
      ],
      [
        "census"
      ],
      [
        "chile"
      ],
      [
        "demografia"
      ],
      [
        "demographics"
      ],
      [
        "duckdb"
      ],
      [
        "redatam"
      ]
    ],
    "score": 5.1099,
    "stars": 28,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "censo2017 Base de Datos de Facil Acceso del Censo 2017 de Chile (2017\nChilean Census Easy Access Database) Provee un acceso conveniente a mas de 17 millones de\nregistros de la base de datos del Censo 2017. Los datos fueron\nimportados desde el DVD oficial del INE usando el Convertidor\nREDATAM creado por Pablo De Grande. Esta paquete esta\ndocumentado intencionalmente en castellano asciificado para que\nfuncione sin problema en diferentes plataformas. (Provides\nconvenient access to more than 17 million records from the\nChilean Census 2017 database. The datasets were imported from\nthe official DVD provided by the Chilean National Bureau of\nStatistics by using the REDATAM converter created by Pablo De\nGrande and in addition it includes the maps accompanying these\ndatasets.) censo_conectar censo_descargar censo_desconectar censo_eliminar censo_tabla censo census chile demografia demographics duckdb redatam"
  },
  {
    "id": 873,
    "package_name": "neotoma",
    "title": "Access to the Neotoma Paleoecological Database Through R",
    "description": "NOTE: This package is deprecated. Please use the neotoma2\npackage described at https://github.com/NeotomaDB/neotoma2.\nAccess paleoecological datasets from the Neotoma\nPaleoecological Database using the published API\n(<http://wnapi.neotomadb.org/>), only containing datasets\nuploaded prior to June 2020.  The functions in this package\naccess various pre-built API functions and attempt to return\nthe results from Neotoma in a usable format for researchers and\nthe public.",
    "version": "1.7.7",
    "maintainer": "Simon J. Goring <goring@wisc.edu>",
    "author": "Simon J. Goring [aut, cre], Gavin L. Simpson [aut], Jeremiah P.\nMarsicek [ctb], Karthik Ram [aut], Luke Sosalla [ctb]",
    "url": "https://docs.ropensci.org/neotoma,\nhttps://github.com/ropensci/neotoma",
    "bug_reports": "https://github.com/ropensci/neotoma/issues",
    "repository": "",
    "exports": [
      [
        "ages"
      ],
      [
        "bind"
      ],
      [
        "browse"
      ],
      [
        "compile_downloads"
      ],
      [
        "compile_taxa"
      ],
      [
        "counts"
      ],
      [
        "depths"
      ],
      [
        "get_chroncontrol"
      ],
      [
        "get_closest"
      ],
      [
        "get_contact"
      ],
      [
        "get_dataset"
      ],
      [
        "get_download"
      ],
      [
        "get_geochron"
      ],
      [
        "get_publication"
      ],
      [
        "get_site"
      ],
      [
        "get_table"
      ],
      [
        "get_taxa"
      ],
      [
        "param_check"
      ],
      [
        "plot_leaflet"
      ],
      [
        "read_bacon"
      ],
      [
        "read.tilia"
      ],
      [
        "taxa"
      ],
      [
        "write_agefile"
      ]
    ],
    "topics": [
      [
        "neotoma"
      ],
      [
        "neotoma-apis"
      ],
      [
        "neotoma-database"
      ],
      [
        "nsf"
      ],
      [
        "paleoecology"
      ]
    ],
    "score": 5.0354,
    "stars": 31,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "neotoma Access to the Neotoma Paleoecological Database Through R NOTE: This package is deprecated. Please use the neotoma2\npackage described at https://github.com/NeotomaDB/neotoma2.\nAccess paleoecological datasets from the Neotoma\nPaleoecological Database using the published API\n(<http://wnapi.neotomadb.org/>), only containing datasets\nuploaded prior to June 2020.  The functions in this package\naccess various pre-built API functions and attempt to return\nthe results from Neotoma in a usable format for researchers and\nthe public. ages bind browse compile_downloads compile_taxa counts depths get_chroncontrol get_closest get_contact get_dataset get_download get_geochron get_publication get_site get_table get_taxa param_check plot_leaflet read_bacon read.tilia taxa write_agefile neotoma neotoma-apis neotoma-database nsf paleoecology"
  },
  {
    "id": 1233,
    "package_name": "skynet",
    "title": "Generates Networks from BTS Data",
    "description": "A flexible tool that allows generating bespoke air\ntransport statistics for urban studies based on publicly\navailable data from the Bureau of Transport Statistics (BTS) in\nthe United States\n<https://www.transtats.bts.gov/databases.asp?Z1qr_VQ=E&Z1qr_Qr5p=N8vn6v10&f7owrp6_VQF=D>.",
    "version": "1.4.4",
    "maintainer": "Filipe Teixeira <Filipe.MarquesTeixeira@Ugent.be>",
    "author": "Filipe Teixeira [aut, cre]",
    "url": "https://github.com/ropensci/skynet",
    "bug_reports": "https://github.com/ropensci/skynet/issues",
    "repository": "",
    "exports": [
      [
        "boot_network"
      ],
      [
        "createNodes"
      ],
      [
        "disparity_filter"
      ],
      [
        "download_db1b"
      ],
      [
        "download_ontime"
      ],
      [
        "download_t100"
      ],
      [
        "download_t100_int"
      ],
      [
        "find_airport"
      ],
      [
        "find_carrier"
      ],
      [
        "fit_power"
      ],
      [
        "from_to_stats"
      ],
      [
        "import_db1b"
      ],
      [
        "import_ontime"
      ],
      [
        "import_t100"
      ],
      [
        "make_net_dir"
      ],
      [
        "make_net_path"
      ],
      [
        "make_net_trip"
      ],
      [
        "make_net_und"
      ],
      [
        "make.netInt"
      ],
      [
        "net_map"
      ],
      [
        "netImport"
      ],
      [
        "node_stats"
      ],
      [
        "nodeStatsMetro"
      ],
      [
        "skynet_example"
      ]
    ],
    "topics": [
      [
        "air-transport"
      ],
      [
        "bts"
      ],
      [
        "bureau-of-transport-statistics"
      ],
      [
        "db1b"
      ],
      [
        "peer-reviewed"
      ],
      [
        "rita"
      ],
      [
        "skynet"
      ],
      [
        "t100"
      ],
      [
        "transtats"
      ]
    ],
    "score": 4.7513,
    "stars": 12,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "skynet Generates Networks from BTS Data A flexible tool that allows generating bespoke air\ntransport statistics for urban studies based on publicly\navailable data from the Bureau of Transport Statistics (BTS) in\nthe United States\n<https://www.transtats.bts.gov/databases.asp?Z1qr_VQ=E&Z1qr_Qr5p=N8vn6v10&f7owrp6_VQF=D>. boot_network createNodes disparity_filter download_db1b download_ontime download_t100 download_t100_int find_airport find_carrier fit_power from_to_stats import_db1b import_ontime import_t100 make_net_dir make_net_path make_net_trip make_net_und make.netInt net_map netImport node_stats nodeStatsMetro skynet_example air-transport bts bureau-of-transport-statistics db1b peer-reviewed rita skynet t100 transtats"
  },
  {
    "id": 361,
    "package_name": "chromer",
    "title": "Interface to Chromosome Counts Database API",
    "description": "A programmatic interface to the Chromosome Counts Database\n(<https://ccdb.tau.ac.il/>), Rice et al. (2014)\n<doi:10.1111/nph.13191>. This package is part of the 'ROpenSci'\nsuite (<https://ropensci.org>).",
    "version": "0.10",
    "maintainer": "Karl W Broman <broman@wisc.edu>",
    "author": "Matthew Pennell [aut] (ORCID: <https://orcid.org/0000-0002-2886-3970>),\nPaula Andrea Martinez [aut] (ORCID:\n<https://orcid.org/0000-0002-8990-1985>),\nKarl W Broman [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-4914-6671>)",
    "url": "https://docs.ropensci.org/chromer/,\nhttps://github.com/ropensci/chromer",
    "bug_reports": "https://github.com/ropensci/chromer/issues",
    "repository": "",
    "exports": [
      [
        "chrom_counts"
      ],
      [
        "summarize_counts"
      ]
    ],
    "topics": [],
    "score": 4.5563,
    "stars": 12,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "chromer Interface to Chromosome Counts Database API A programmatic interface to the Chromosome Counts Database\n(<https://ccdb.tau.ac.il/>), Rice et al. (2014)\n<doi:10.1111/nph.13191>. This package is part of the 'ROpenSci'\nsuite (<https://ropensci.org>). chrom_counts summarize_counts "
  },
  {
    "id": 1052,
    "package_name": "ramlegacy",
    "title": "Download and Read RAM Legacy Stock Assessment Database",
    "description": "Contains functions to download, cache and read in 'Excel'\nversion of the RAM Legacy Stock Assessment Data Base, an online\ncompilation of stock assessment results for commercially\nexploited marine populations from around the world. The\ndatabase is named after Dr. Ransom A. Myers whose original\nstock-recruitment database, is no longer being updated. More\ninformation about the database can be found at\n<https://ramlegacy.org/>. Ricard, D., Minto, C., Jensen, O.P.\nand Baum, J.K. (2012) <doi:10.1111/j.1467-2979.2011.00435.x>.",
    "version": "0.2.0",
    "maintainer": "Kshitiz Gupta <kshtzgupta1@berkeley.edu>",
    "author": "Carl Boettiger [aut, cph] (ORCID:\n<https://orcid.org/0000-0002-1642-628X>),\nKshitiz Gupta [aut, cre, cph],\nSam Albers [rev] (ORCID: <https://orcid.org/0000-0002-9270-7884>),\nJamie Afflerbach [rev] (ORCID: <https://orcid.org/0000-0002-5215-9342>),\nRAM Legacy Stock Assessment Database [dtc]",
    "url": "https://docs.ropensci.org/ramlegacy,\nhttps://github.com/ropensci/ramlegacy",
    "bug_reports": "https://github.com/ropensci/ramlegacy/issues",
    "repository": "",
    "exports": [
      [
        "download_ramlegacy"
      ],
      [
        "load_ramlegacy"
      ],
      [
        "ram_dir"
      ]
    ],
    "topics": [
      [
        "fisheries"
      ],
      [
        "marine-biology"
      ],
      [
        "ramlegacy"
      ],
      [
        "ropensci"
      ],
      [
        "stock-assessment"
      ]
    ],
    "score": 4.1761,
    "stars": 5,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "ramlegacy Download and Read RAM Legacy Stock Assessment Database Contains functions to download, cache and read in 'Excel'\nversion of the RAM Legacy Stock Assessment Data Base, an online\ncompilation of stock assessment results for commercially\nexploited marine populations from around the world. The\ndatabase is named after Dr. Ransom A. Myers whose original\nstock-recruitment database, is no longer being updated. More\ninformation about the database can be found at\n<https://ramlegacy.org/>. Ricard, D., Minto, C., Jensen, O.P.\nand Baum, J.K. (2012) <doi:10.1111/j.1467-2979.2011.00435.x>. download_ramlegacy load_ramlegacy ram_dir fisheries marine-biology ramlegacy ropensci stock-assessment"
  },
  {
    "id": 1003,
    "package_name": "popler",
    "title": "Popler R Package",
    "description": "Browse and query the popler database.",
    "version": "0.2.0",
    "maintainer": "Compagnoni Aldo <aldo.compagnoni@aggiemail.usu.edu>",
    "author": "Compagnoni Aldo [cre, aut],\nBibian Andrew [aut],\nOchocki Brad [aut],\nLevin Sam [aut],\nMiller Tom [aut],\nBenjamin Bond-Lamberty [rev] (Ben reviewed the package for ropensci,\nsee <https://github.com/ropensci/software-review/issues/254>),\nCorinna Gries [rev] (Corinna reviewed the package for ropensci, see\n<https://github.com/ropensci/software-review/issues/254>)",
    "url": "https://github.com/ropensci/popler,\nhttps://docs.ropensci.org/popler/",
    "bug_reports": "https://github.com/ropensci/popler/issues",
    "repository": "",
    "exports": [
      [
        "filter"
      ],
      [
        "mutate"
      ],
      [
        "pplr_browse"
      ],
      [
        "pplr_citation"
      ],
      [
        "pplr_cov_unpack"
      ],
      [
        "pplr_dictionary"
      ],
      [
        "pplr_get_data"
      ],
      [
        "pplr_maps"
      ],
      [
        "pplr_metadata_url"
      ],
      [
        "pplr_report_dictionary"
      ],
      [
        "pplr_report_metadata"
      ],
      [
        "pplr_search"
      ],
      [
        "pplr_site_rep"
      ],
      [
        "pplr_site_rep_plot"
      ],
      [
        "pplr_summary"
      ],
      [
        "pplr_summary_table_update"
      ]
    ],
    "topics": [],
    "score": 3.8274,
    "stars": 7,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "popler Popler R Package Browse and query the popler database. filter mutate pplr_browse pplr_citation pplr_cov_unpack pplr_dictionary pplr_get_data pplr_maps pplr_metadata_url pplr_report_dictionary pplr_report_metadata pplr_search pplr_site_rep pplr_site_rep_plot pplr_summary pplr_summary_table_update "
  },
  {
    "id": 1107,
    "package_name": "rgpdd",
    "title": "R Interface to the Global Population Dynamics Database",
    "description": "R Interface to the Global Population Dynamics Database\n(<https://ecologicaldata.org/wiki/global-population-dynamics-database>)",
    "version": "0.1",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre],\nTed Harte [aut],\nScott Chamberlain [aut],\nKarthik Ram [aut]",
    "url": "https://docs.ropensci.org/rgpdd, https://github.com/ropensci/rgpdd",
    "bug_reports": "https://github.com/ropensci/rgpdd/issues",
    "repository": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 8,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rgpdd R Interface to the Global Population Dynamics Database R Interface to the Global Population Dynamics Database\n(<https://ecologicaldata.org/wiki/global-population-dynamics-database>)  "
  },
  {
    "id": 332,
    "package_name": "cRegulome",
    "title": "Obtain and Visualize Regulome-Gene Expression Correlations in\nCancer",
    "description": "Builds a 'SQLite' database file of pre-calculated\ntranscription factor/microRNA-gene correlations (co-expression)\nin cancer from the Cistrome Cancer Liu et al. (2011)\n<doi:10.1186/gb-2011-12-8-r83> and 'miRCancerdb' databases (in\npress). Provides custom classes and functions to query, tidy\nand plot the correlation data.",
    "version": "0.3.2",
    "maintainer": "Mahmoud Ahmed <mahmoud.s.fahmy@students.kasralainy.edu.eg>",
    "author": "Mahmoud Ahmed [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-4377-6541>)",
    "url": "https://docs.ropensci.org/cRegulome,\nhttps://github.com/ropensci/cRegulome",
    "bug_reports": "https://github.com/ropensci/cRegulome/issues",
    "repository": "",
    "exports": [
      [
        "cmicroRNA"
      ],
      [
        "cor_hist"
      ],
      [
        "cor_igraph"
      ],
      [
        "cor_joy"
      ],
      [
        "cor_plot"
      ],
      [
        "cor_prep"
      ],
      [
        "cor_tidy"
      ],
      [
        "cor_upset"
      ],
      [
        "cor_venn_diagram"
      ],
      [
        "cTF"
      ],
      [
        "get_db"
      ],
      [
        "get_mir"
      ],
      [
        "get_tf"
      ],
      [
        "stat_collect"
      ],
      [
        "stat_collect_targets"
      ],
      [
        "stat_make"
      ],
      [
        "stat_make_targets"
      ]
    ],
    "topics": [
      [
        "cancer-genomics"
      ],
      [
        "database"
      ],
      [
        "datascience"
      ],
      [
        "microrna"
      ],
      [
        "peer-reviewed"
      ],
      [
        "tcga-data"
      ],
      [
        "transcription-factors"
      ]
    ],
    "score": 3.6866,
    "stars": 3,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "cRegulome Obtain and Visualize Regulome-Gene Expression Correlations in\nCancer Builds a 'SQLite' database file of pre-calculated\ntranscription factor/microRNA-gene correlations (co-expression)\nin cancer from the Cistrome Cancer Liu et al. (2011)\n<doi:10.1186/gb-2011-12-8-r83> and 'miRCancerdb' databases (in\npress). Provides custom classes and functions to query, tidy\nand plot the correlation data. cmicroRNA cor_hist cor_igraph cor_joy cor_plot cor_prep cor_tidy cor_upset cor_venn_diagram cTF get_db get_mir get_tf stat_collect stat_collect_targets stat_make stat_make_targets cancer-genomics database datascience microrna peer-reviewed tcga-data transcription-factors"
  },
  {
    "id": 1168,
    "package_name": "rusda",
    "title": "Interface to USDA Databases",
    "description": "An interface to the web service methods provided by the\nUnited States Department of Agriculture (USDA). The\nAgricultural Research Service (ARS) provides a large set of\ndatabases. The current version of the package holds interfaces\nto the Systematic Mycology and Microbiology Laboratory (SMML),\nwhich consists of four databases: Fungus-Host Distributions,\nSpecimens, Literature and the Nomenclature database. It\nprovides functions for querying these databases. The main\nfunction is \\code{associations}, which allows searching for\nfungus-host combinations.",
    "version": "1.0.8",
    "maintainer": "Franz-Sebastian Krah <f.krah@mailbox.org>",
    "author": "Franz-Sebastian Krah",
    "url": "https://docs.ropensci.org/rusda,\nhttp://www.usda.gov/wps/portal/usda/usdahome,\nhttp://nt.ars-grin.gov/fungaldatabases/index.cfm,\nhttps://github.com/ropensci/rusda",
    "bug_reports": "https://github.com/ropensci/rusda/issues",
    "repository": "",
    "exports": [
      [
        "associations"
      ],
      [
        "getBPI"
      ],
      [
        "getStudy"
      ],
      [
        "literature"
      ],
      [
        "meta_smml"
      ],
      [
        "substrate"
      ],
      [
        "synonyms_smml"
      ]
    ],
    "topics": [],
    "score": 3.5119,
    "stars": 13,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rusda Interface to USDA Databases An interface to the web service methods provided by the\nUnited States Department of Agriculture (USDA). The\nAgricultural Research Service (ARS) provides a large set of\ndatabases. The current version of the package holds interfaces\nto the Systematic Mycology and Microbiology Laboratory (SMML),\nwhich consists of four databases: Fungus-Host Distributions,\nSpecimens, Literature and the Nomenclature database. It\nprovides functions for querying these databases. The main\nfunction is \\code{associations}, which allows searching for\nfungus-host combinations. associations getBPI getStudy literature meta_smml substrate synonyms_smml "
  },
  {
    "id": 266,
    "package_name": "awardFindR",
    "title": "awardFindR",
    "description": "Queries a number of scientific awards databases. Collects\nrelevant results based on keyword and date parameters, returns\nlist of projects that fit those criteria as a data frame.\nSources include: Arnold Ventures, Carnegie Corp, Federal\nRePORTER, Gates Foundation, MacArthur Foundation, Mellon\nFoundation, NEH, NIH, NSF, Open Philanthropy, Open Society\nFoundations, Rockefeller Foundation, Russell Sage Foundation,\nRobert Wood Johnson Foundation, Sloan Foundation, Social\nScience Research Council, John Templeton Foundation, and\nUSASpending.gov.",
    "version": "1.0.1",
    "maintainer": "Michael McCall <mimccall@syr.edu>",
    "author": "Michael C. McCall",
    "url": "https://github.com/PESData/awardFindR",
    "bug_reports": "https://github.com/PESData/awardFindR/issues",
    "repository": "",
    "exports": [
      [
        "create_templeton_df"
      ],
      [
        "get_arnold"
      ],
      [
        "get_carnegie"
      ],
      [
        "get_gates"
      ],
      [
        "get_macarthur"
      ],
      [
        "get_mellon"
      ],
      [
        "get_nih"
      ],
      [
        "get_nsf"
      ],
      [
        "get_ophil"
      ],
      [
        "get_osociety"
      ],
      [
        "get_rockefeller"
      ],
      [
        "get_rsf"
      ],
      [
        "get_rwjf"
      ],
      [
        "get_sloan"
      ],
      [
        "get_ssrc"
      ],
      [
        "get_templeton"
      ],
      [
        "get_usaspend"
      ],
      [
        "search_awards"
      ]
    ],
    "topics": [],
    "score": 3.4314,
    "stars": 18,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "awardFindR awardFindR Queries a number of scientific awards databases. Collects\nrelevant results based on keyword and date parameters, returns\nlist of projects that fit those criteria as a data frame.\nSources include: Arnold Ventures, Carnegie Corp, Federal\nRePORTER, Gates Foundation, MacArthur Foundation, Mellon\nFoundation, NEH, NIH, NSF, Open Philanthropy, Open Society\nFoundations, Rockefeller Foundation, Russell Sage Foundation,\nRobert Wood Johnson Foundation, Sloan Foundation, Social\nScience Research Council, John Templeton Foundation, and\nUSASpending.gov. create_templeton_df get_arnold get_carnegie get_gates get_macarthur get_mellon get_nih get_nsf get_ophil get_osociety get_rockefeller get_rsf get_rwjf get_sloan get_ssrc get_templeton get_usaspend search_awards "
  },
  {
    "id": 202,
    "package_name": "SymbiotaR2",
    "title": "Downloading Data from Symbiota2 Portals into R",
    "description": "Download data from Symbiota2 portals using Symbiota's API.\nCovers the Checklists, Collections, Crowdsource, Exsiccati,\nGlossary, ImageProcessor, Key, Media, Occurrence, Reference,\nTaxa, Traits, and UserRoles API families. Each Symbiota2 portal\nowner can load their own plugins (and modified code), and so\nthis package may not cover every possible API endpoint from a\ngiven Symbiota2 instance.",
    "version": "0.0-1",
    "maintainer": "Austin Koontz <austinkoontz11@gmail.com>",
    "author": "Austin Koontz [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6103-5896>),\nWilliam D. Pearse [aut] (ORCID:\n<https://orcid.org/0000-0002-6241-3164>),\nBen Bond-Lamberty [rev] (Ben reviewed SymbiotaR2 for rOpenSci and\nprovided valuable feedback, see\nhttps://github.com/ropensci/software-review/issues/373),\nScott Chamberlain [rev] (Scott reviewed SymbiotaR2 for rOpenSci and\nprovided valuable feedback, see\nhttps://github.com/ropensci/software-review/issues/373)",
    "url": "https://github.com/pearselab/SymbiotaR2",
    "bug_reports": "https://github.com/pearselab/SymbiotaR2/issues",
    "repository": "",
    "exports": [
      [
        "AccessStats"
      ],
      [
        "Associations"
      ],
      [
        "Authorities"
      ],
      [
        "Categories"
      ],
      [
        "Central"
      ],
      [
        "CharacterHeading"
      ],
      [
        "Characters"
      ],
      [
        "CharacterStateImages"
      ],
      [
        "CharacterStates"
      ],
      [
        "ChecklistProjects"
      ],
      [
        "Checklists"
      ],
      [
        "Collections"
      ],
      [
        "Comments"
      ],
      [
        "Configurations"
      ],
      [
        "Coordinates"
      ],
      [
        "DatasetLink"
      ],
      [
        "Datasets"
      ],
      [
        "DescriptionBlock"
      ],
      [
        "DescriptionDeletions"
      ],
      [
        "Determinations"
      ],
      [
        "Duplicates"
      ],
      [
        "EditLocks"
      ],
      [
        "Edits"
      ],
      [
        "Exchange"
      ],
      [
        "FullText"
      ],
      [
        "Glossary"
      ],
      [
        "GuidDeterminations"
      ],
      [
        "GuidOccurrences"
      ],
      [
        "Institutions"
      ],
      [
        "Loans"
      ],
      [
        "LookupChronostratigraphy"
      ],
      [
        "LookupCounties"
      ],
      [
        "LookupCountries"
      ],
      [
        "LookupLanguages"
      ],
      [
        "LookupReferenceTypes"
      ],
      [
        "LookupStateProvinces"
      ],
      [
        "Numbers"
      ],
      [
        "Occurrences"
      ],
      [
        "Projects"
      ],
      [
        "Queue"
      ],
      [
        "RawLabels"
      ],
      [
        "SchemaVersion"
      ],
      [
        "Stats"
      ],
      [
        "SymbiotaR2_setup"
      ],
      [
        "Synonymy"
      ],
      [
        "TagKey"
      ],
      [
        "Taxa"
      ],
      [
        "TaxaLink"
      ],
      [
        "TermLink"
      ],
      [
        "Titles"
      ],
      [
        "Traits"
      ],
      [
        "UploadMappings"
      ],
      [
        "UploadParameters"
      ],
      [
        "Verification"
      ]
    ],
    "topics": [
      [
        "database"
      ],
      [
        "library"
      ],
      [
        "specimen-records"
      ],
      [
        "symbiota"
      ],
      [
        "symbiota2"
      ],
      [
        "symbiota2-portal"
      ]
    ],
    "score": 3.301,
    "stars": 2,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "SymbiotaR2 Downloading Data from Symbiota2 Portals into R Download data from Symbiota2 portals using Symbiota's API.\nCovers the Checklists, Collections, Crowdsource, Exsiccati,\nGlossary, ImageProcessor, Key, Media, Occurrence, Reference,\nTaxa, Traits, and UserRoles API families. Each Symbiota2 portal\nowner can load their own plugins (and modified code), and so\nthis package may not cover every possible API endpoint from a\ngiven Symbiota2 instance. AccessStats Associations Authorities Categories Central CharacterHeading Characters CharacterStateImages CharacterStates ChecklistProjects Checklists Collections Comments Configurations Coordinates DatasetLink Datasets DescriptionBlock DescriptionDeletions Determinations Duplicates EditLocks Edits Exchange FullText Glossary GuidDeterminations GuidOccurrences Institutions Loans LookupChronostratigraphy LookupCounties LookupCountries LookupLanguages LookupReferenceTypes LookupStateProvinces Numbers Occurrences Projects Queue RawLabels SchemaVersion Stats SymbiotaR2_setup Synonymy TagKey Taxa TaxaLink TermLink Titles Traits UploadMappings UploadParameters Verification database library specimen-records symbiota symbiota2 symbiota2-portal"
  },
  {
    "id": 916,
    "package_name": "orderly.db",
    "title": "Database Support for 'orderly'",
    "description": "Access databases from 'orderly' while running reports.\nIncludes the basic 'SQL' support originally included in\n'orderly' for establishing connections and setting up data for\nuse within a report.",
    "version": "0.1.5",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "https://github.com/vimc/orderly.db",
    "bug_reports": "https://github.com/vimc/orderly.db/issues",
    "repository": "",
    "exports": [
      [
        "orderly_db_connection"
      ],
      [
        "orderly_db_query"
      ],
      [
        "orderly_db_view"
      ]
    ],
    "topics": [],
    "score": 3.301,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "orderly.db Database Support for 'orderly' Access databases from 'orderly' while running reports.\nIncludes the basic 'SQL' support originally included in\n'orderly' for establishing connections and setting up data for\nuse within a report. orderly_db_connection orderly_db_query orderly_db_view "
  },
  {
    "id": 560,
    "package_name": "fakerbase",
    "title": "Fake Database Tables For Unit Testing",
    "description": "Connect to a Postgres database and generate a suite of\nfunctions that can be used to mock database tables for unit\ntesting.",
    "version": "0.0.1",
    "maintainer": "Alex Hill <alex.hill@gmail.com>",
    "author": "Alex Hill [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "fb_generate"
      ],
      [
        "fb_load"
      ]
    ],
    "topics": [],
    "score": 2,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "fakerbase Fake Database Tables For Unit Testing Connect to a Postgres database and generate a suite of\nfunctions that can be used to mock database tables for unit\ntesting. fb_generate fb_load "
  },
  {
    "id": 85,
    "package_name": "MLwrap",
    "title": "Machine Learning Modelling for Everyone",
    "description": "A minimal library specifically designed to make the estimation of Machine Learning\n             (ML) techniques as easy and accessible as possible, particularly within the framework of\n             the Knowledge Discovery in Databases (KDD) process in data mining. The package provides\n             essential tools to structure and execute each stage of a predictive or classification\n             modeling workflow, aligning closely with the fundamental steps of the KDD methodology,\n             from data selection and preparation, through model building and tuning, to the\n             interpretation and evaluation of results using Sensitivity Analysis. The 'MLwrap' workflow\n             is organized into four core steps; preprocessing(), build_model(), fine_tuning(), and\n             sensitivity_analysis(). It also includes global and pairwise interaction analysis based on\n             Friedman\u2019s H-statistic to support a more detailed interpretation of complex feature\n             relationships.These steps correspond, respectively, to data preparation and transformation,\n             model construction, hyperparameter optimization, and sensitivity analysis. The user can\n             access comprehensive model evaluation results including fit assessment metrics, plots,\n             predictions, and performance diagnostics for ML models implemented through 'Neural\n             Networks', 'Random Forest', 'XGBoost' (Extreme Gradient Boosting), and 'Support Vector\n             Machines' (SVM) algorithms. By streamlining these phases, 'MLwrap' aims to simplify the\n             implementation of ML techniques, allowing analysts and data scientists to focus on\n             extracting actionable insights and meaningful patterns from large datasets, in line with\n             the objectives of the KDD process.",
    "version": "0.3.0",
    "maintainer": "Albert Ses\u00e9 <albert.sese@uib.es>",
    "author": "Javier Mart\u00ednez Garc\u00eda [aut] (ORCID:\n    <https://orcid.org/0009-0007-7861-5274>),\n  Juan Jos\u00e9 Monta\u00f1o Moreno [ctb] (ORCID:\n    <https://orcid.org/0000-0002-1116-1964>),\n  Albert Ses\u00e9 [cre, ctb] (ORCID: <https://orcid.org/0000-0003-3771-1749>)",
    "url": "https://github.com/AlbertSesePsy/MLwrap",
    "bug_reports": "https://github.com/AlbertSesePsy/MLwrap/issues",
    "repository": "https://cran.r-project.org/package=MLwrap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MLwrap Machine Learning Modelling for Everyone A minimal library specifically designed to make the estimation of Machine Learning\n             (ML) techniques as easy and accessible as possible, particularly within the framework of\n             the Knowledge Discovery in Databases (KDD) process in data mining. The package provides\n             essential tools to structure and execute each stage of a predictive or classification\n             modeling workflow, aligning closely with the fundamental steps of the KDD methodology,\n             from data selection and preparation, through model building and tuning, to the\n             interpretation and evaluation of results using Sensitivity Analysis. The 'MLwrap' workflow\n             is organized into four core steps; preprocessing(), build_model(), fine_tuning(), and\n             sensitivity_analysis(). It also includes global and pairwise interaction analysis based on\n             Friedman\u2019s H-statistic to support a more detailed interpretation of complex feature\n             relationships.These steps correspond, respectively, to data preparation and transformation,\n             model construction, hyperparameter optimization, and sensitivity analysis. The user can\n             access comprehensive model evaluation results including fit assessment metrics, plots,\n             predictions, and performance diagnostics for ML models implemented through 'Neural\n             Networks', 'Random Forest', 'XGBoost' (Extreme Gradient Boosting), and 'Support Vector\n             Machines' (SVM) algorithms. By streamlining these phases, 'MLwrap' aims to simplify the\n             implementation of ML techniques, allowing analysts and data scientists to focus on\n             extracting actionable insights and meaningful patterns from large datasets, in line with\n             the objectives of the KDD process.  "
  },
  {
    "id": 98,
    "package_name": "OhdsiReportGenerator",
    "title": "Observational Health Data Sciences and Informatics Report\nGenerator",
    "description": "Extract results into R from the Observational Health Data Sciences and Informatics result database (see <https://ohdsi.github.io/Strategus/results-schema/index.html>) and generate reports/presentations via 'quarto' that summarize results in HTML format. Learn more about 'OhdsiReportGenerator' at <https://ohdsi.github.io/OhdsiReportGenerator/>.",
    "version": "2.0.2",
    "maintainer": "Jenna Reps <jreps@its.jnj.com>",
    "author": "Jenna Reps [aut, cre],\n  Anthony Sena [aut]",
    "url": "https://ohdsi.github.io/OhdsiReportGenerator/,\nhttps://github.com/OHDSI/OhdsiReportGenerator",
    "bug_reports": "https://github.com/OHDSI/OhdsiReportGenerator/issues",
    "repository": "https://cran.r-project.org/package=OhdsiReportGenerator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OhdsiReportGenerator Observational Health Data Sciences and Informatics Report\nGenerator Extract results into R from the Observational Health Data Sciences and Informatics result database (see <https://ohdsi.github.io/Strategus/results-schema/index.html>) and generate reports/presentations via 'quarto' that summarize results in HTML format. Learn more about 'OhdsiReportGenerator' at <https://ohdsi.github.io/OhdsiReportGenerator/>.  "
  },
  {
    "id": 100,
    "package_name": "OralOpioids",
    "title": "Retrieving Oral Opioid Information",
    "description": "Provides details such as Morphine Equivalent Dose (MED), \n    brand name and opioid content which are calculated of all oral opioids \n    authorized for sale by Health Canada and the FDA based on their Drug Identification Number (DIN) or National Drug Code (NDC). \n    MEDs are calculated based on recommendations by Canadian Institute for Health Information (CIHI) and Von Korff et al (2008)\n    and information obtained from Health Canada's Drug Product Database's monthly data dump or FDA Daily database for Canadian and US databases respectively. \n    Please note in no way should output from this package be a substitute for medical advise.  \n    All medications should only be consumed on prescription from a licensed healthcare provider.",
    "version": "2.0.5",
    "maintainer": "Ankona Banerjee <ankonanagpur@gmail.com>",
    "author": "Ankona Banerjee [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4433-5446>),\n  Erik Stricker [aut] (ORCID: <https://orcid.org/0000-0003-2112-2202>)",
    "url": "https://github.com/ankonahouston/OralOpioids",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OralOpioids",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OralOpioids Retrieving Oral Opioid Information Provides details such as Morphine Equivalent Dose (MED), \n    brand name and opioid content which are calculated of all oral opioids \n    authorized for sale by Health Canada and the FDA based on their Drug Identification Number (DIN) or National Drug Code (NDC). \n    MEDs are calculated based on recommendations by Canadian Institute for Health Information (CIHI) and Von Korff et al (2008)\n    and information obtained from Health Canada's Drug Product Database's monthly data dump or FDA Daily database for Canadian and US databases respectively. \n    Please note in no way should output from this package be a substitute for medical advise.  \n    All medications should only be consumed on prescription from a licensed healthcare provider.  "
  },
  {
    "id": 105,
    "package_name": "PGRdup",
    "title": "Discover Probable Duplicates in Plant Genetic Resources\nCollections",
    "description": "Provides functions to aid the identification of probable/possible\n    duplicates in Plant Genetic Resources (PGR) collections using\n    'passport databases' comprising of information records of each constituent\n    sample. These include methods for cleaning the data, creation of a\n    searchable Key Word in Context (KWIC) index of keywords associated with\n    sample records and the identification of nearly identical records with\n    similar information by fuzzy, phonetic and semantic matching of keywords.",
    "version": "0.2.4.0",
    "maintainer": "J. Aravind <j.aravind@icar.org.in>",
    "author": "J. Aravind [aut, cre] (ORCID: <https://orcid.org/0000-0002-4791-442X>),\n  J. Radhamani [aut],\n  Kalyani Srinivasan [aut],\n  B. Ananda Subhash [aut],\n  Rishi Kumar Tyagi [aut],\n  ICAR-NBGPR [cph] (url: https://nbpgr.org.in/),\n  Maurice Aubrey [ctb] (Double Metaphone),\n  Kevin Atkinson [ctb] (Double Metaphone),\n  Lawrence Philips [ctb] (Double Metaphone)",
    "url": "https://cran.r-project.org/package=PGRdup,\nhttps://github.com/aravind-j/PGRdup,\nhttps://doi.org/10.5281/zenodo.841963,\nhttps://aravind-j.github.io/PGRdup/,\nhttps://www.rdocumentation.org/packages/PGRdup",
    "bug_reports": "https://github.com/aravind-j/PGRdup/issues",
    "repository": "https://cran.r-project.org/package=PGRdup",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PGRdup Discover Probable Duplicates in Plant Genetic Resources\nCollections Provides functions to aid the identification of probable/possible\n    duplicates in Plant Genetic Resources (PGR) collections using\n    'passport databases' comprising of information records of each constituent\n    sample. These include methods for cleaning the data, creation of a\n    searchable Key Word in Context (KWIC) index of keywords associated with\n    sample records and the identification of nearly identical records with\n    similar information by fuzzy, phonetic and semantic matching of keywords.  "
  },
  {
    "id": 132,
    "package_name": "RODBC",
    "title": "ODBC Database Access",
    "description": "An ODBC database interface.",
    "version": "1.3-26.1",
    "maintainer": "Brian Ripley <Brian.Ripley@R-project.org>",
    "author": "Brian Ripley [aut, cre],\n  Michael Lapsley [aut] (1999 to Oct 2002)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RODBC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RODBC ODBC Database Access An ODBC database interface.  "
  },
  {
    "id": 185,
    "package_name": "SHARK4R",
    "title": "Accessing and Validating Marine Environmental Data from 'SHARK'\nand Related Databases",
    "description": "Provides functions to retrieve, process, analyze, and\n    quality-control marine physical, chemical, and biological data. The\n    main focus is on Swedish monitoring data available through the 'SHARK'\n    database <https://shark.smhi.se/en/>, with additional API support for 'Nordic\n    Microalgae' <https://nordicmicroalgae.org/>, 'Dyntaxa'\n    <https://artfakta.se/>, World Register of Marine Species ('WoRMS') <https://www.marinespecies.org>,\n    'AlgaeBase' <https://www.algaebase.org>, OBIS 'xylookup' web service \n    <https://iobis.github.io/xylookup/> and Intergovernmental Oceanographic Commission (IOC) - \n    UNESCO databases on harmful algae  <https://www.marinespecies.org/hab/> and toxins\n    <https://toxins.hais.ioc-unesco.org/>.",
    "version": "1.0.2",
    "maintainer": "Anders Torstensson <anders.torstensson@smhi.se>",
    "author": "Markus Lindh [aut] (Swedish Meteorological and Hydrological Institute,\n    ORCID: <https://orcid.org/0000-0002-7120-4145>),\n  Anders Torstensson [aut, cre] (Swedish Meteorological and Hydrological\n    Institute, ORCID: <https://orcid.org/0000-0002-8283-656X>),\n  Mikael Hedblom [ctb] (Swedish Meteorological and Hydrological\n    Institute, ORCID: <https://orcid.org/0009-0007-5124-9956>),\n  Bengt Karlson [ctb] (Swedish Meteorological and Hydrological Institute,\n    ORCID: <https://orcid.org/0000-0002-7524-3504>),\n  SHARK [cph],\n  SBDI [fnd] (Swedish Research Council, 2019-00242)",
    "url": "https://sharksmhi.github.io/SHARK4R/,\nhttps://github.com/sharksmhi/SHARK4R",
    "bug_reports": "https://github.com/sharksmhi/SHARK4R/issues",
    "repository": "https://cran.r-project.org/package=SHARK4R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SHARK4R Accessing and Validating Marine Environmental Data from 'SHARK'\nand Related Databases Provides functions to retrieve, process, analyze, and\n    quality-control marine physical, chemical, and biological data. The\n    main focus is on Swedish monitoring data available through the 'SHARK'\n    database <https://shark.smhi.se/en/>, with additional API support for 'Nordic\n    Microalgae' <https://nordicmicroalgae.org/>, 'Dyntaxa'\n    <https://artfakta.se/>, World Register of Marine Species ('WoRMS') <https://www.marinespecies.org>,\n    'AlgaeBase' <https://www.algaebase.org>, OBIS 'xylookup' web service \n    <https://iobis.github.io/xylookup/> and Intergovernmental Oceanographic Commission (IOC) - \n    UNESCO databases on harmful algae  <https://www.marinespecies.org/hab/> and toxins\n    <https://toxins.hais.ioc-unesco.org/>.  "
  },
  {
    "id": 197,
    "package_name": "SlimR",
    "title": "Machine Learning-Assisted, Marker-Based Tool for Single-Cell and\nSpatial Transcriptomics Annotation",
    "description": "Annotates single-cell and spatial-transcriptomic (ST) data using marker datasets. Supports unified markers list ('Markers_list') creation from built-in databases (e.g., 'Cellmarker2', 'PanglaoDB', 'scIBD', 'TCellSI', 'PCTIT', 'PCTAM'), Seurat objects, or user-supplied Excel files. SlimR can predict calculation parameters by machine learning algorithms (e.g., 'Random Forest', 'Gradient Boosting', 'Support Vector Machine', 'Ensemble Learning'), and based on Markers_list, calculate gene expression of different cell types and predict annotation information, and calculate corresponding AUC and annotate it, then verify it. At the same time, it can calculate gene expression corresponding to the cell type to generate a reference map for manual annotation (e.g., 'Heat Map', 'Feature Plots', 'Combined Plots'). For more details, see Kabacoff (2020, ISBN:9787115420572).",
    "version": "1.0.9",
    "maintainer": "Zhaoqing Wang <zhaoqingwang@mail.sdu.edu.cn>",
    "author": "Zhaoqing Wang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8348-7245>)",
    "url": "https://github.com/Zhaoqing-wang/SlimR",
    "bug_reports": "https://github.com/Zhaoqing-wang/SlimR/issues",
    "repository": "https://cran.r-project.org/package=SlimR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SlimR Machine Learning-Assisted, Marker-Based Tool for Single-Cell and\nSpatial Transcriptomics Annotation Annotates single-cell and spatial-transcriptomic (ST) data using marker datasets. Supports unified markers list ('Markers_list') creation from built-in databases (e.g., 'Cellmarker2', 'PanglaoDB', 'scIBD', 'TCellSI', 'PCTIT', 'PCTAM'), Seurat objects, or user-supplied Excel files. SlimR can predict calculation parameters by machine learning algorithms (e.g., 'Random Forest', 'Gradient Boosting', 'Support Vector Machine', 'Ensemble Learning'), and based on Markers_list, calculate gene expression of different cell types and predict annotation information, and calculate corresponding AUC and annotate it, then verify it. At the same time, it can calculate gene expression corresponding to the cell type to generate a reference map for manual annotation (e.g., 'Heat Map', 'Feature Plots', 'Combined Plots'). For more details, see Kabacoff (2020, ISBN:9787115420572).  "
  },
  {
    "id": 250,
    "package_name": "arcpullr",
    "title": "Pull Data from an 'ArcGIS REST' API",
    "description": "\n  Functions to efficiently query 'ArcGIS REST' APIs \n  <https://developers.arcgis.com/rest/>. \n  Both spatial and SQL queries can be used to retrieve data. \n  Simple Feature (sf) objects are utilized to perform spatial queries. \n  This package was neither produced nor is maintained by Esri.",
    "version": "0.3.2",
    "maintainer": "Paul Frater <paul.frater@wisconsin.gov>",
    "author": "Paul Frater [aut, cre] (ORCID: <https://orcid.org/0000-0002-7237-6563>),\n  Zac Driscoll [aut] (ORCID: <https://orcid.org/0000-0002-8233-0980>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=arcpullr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arcpullr Pull Data from an 'ArcGIS REST' API \n  Functions to efficiently query 'ArcGIS REST' APIs \n  <https://developers.arcgis.com/rest/>. \n  Both spatial and SQL queries can be used to retrieve data. \n  Simple Feature (sf) objects are utilized to perform spatial queries. \n  This package was neither produced nor is maintained by Esri.  "
  },
  {
    "id": 289,
    "package_name": "bibliometrix",
    "title": "Comprehensive Science Mapping Analysis",
    "description": "Tool for quantitative research in scientometrics and bibliometrics.\n    It implements the comprehensive workflow for science mapping analysis proposed in Aria M. and \n    Cuccurullo C. (2017) <doi:10.1016/j.joi.2017.08.007>.\n    'bibliometrix' provides various routines for importing bibliographic data from 'SCOPUS',\n    'Clarivate Analytics Web of Science' (<https://www.webofknowledge.com/>), 'Digital Science Dimensions' \n\t(<https://www.dimensions.ai/>), 'OpenAlex' (<https://openalex.org/>), 'Cochrane Library' (<https://www.cochranelibrary.com/>),  'Lens' (<https://lens.org>), \n\tand 'PubMed' (<https://pubmed.ncbi.nlm.nih.gov/>) databases, performing bibliometric analysis \n    and building networks for co-citation, coupling, scientific collaboration and co-word analysis.",
    "version": "5.2.1",
    "maintainer": "Massimo Aria <aria@unina.it>",
    "author": "Massimo Aria [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-8517-9411>),\n  Corrado Cuccurullo [aut] (ORCID:\n    <https://orcid.org/0000-0002-7401-8575>)",
    "url": "https://www.bibliometrix.org,\nhttps://github.com/massimoaria/bibliometrix,\nhttps://www.k-synth.com",
    "bug_reports": "https://github.com/massimoaria/bibliometrix/issues",
    "repository": "https://cran.r-project.org/package=bibliometrix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bibliometrix Comprehensive Science Mapping Analysis Tool for quantitative research in scientometrics and bibliometrics.\n    It implements the comprehensive workflow for science mapping analysis proposed in Aria M. and \n    Cuccurullo C. (2017) <doi:10.1016/j.joi.2017.08.007>.\n    'bibliometrix' provides various routines for importing bibliographic data from 'SCOPUS',\n    'Clarivate Analytics Web of Science' (<https://www.webofknowledge.com/>), 'Digital Science Dimensions' \n\t(<https://www.dimensions.ai/>), 'OpenAlex' (<https://openalex.org/>), 'Cochrane Library' (<https://www.cochranelibrary.com/>),  'Lens' (<https://lens.org>), \n\tand 'PubMed' (<https://pubmed.ncbi.nlm.nih.gov/>) databases, performing bibliometric analysis \n    and building networks for co-citation, coupling, scientific collaboration and co-word analysis.  "
  },
  {
    "id": 602,
    "package_name": "geeLite",
    "title": "Building and Managing Local Databases from 'Google Earth Engine'",
    "description": "Simplifies the creation, management, and updating of local databases using data extracted from 'Google Earth Engine' ('GEE'). It integrates with 'GEE' to store, aggregate, and process spatio-temporal data, leveraging 'SQLite' for efficient, serverless storage. The 'geeLite' package provides utilities for data transformation and supports real-time monitoring and analysis of geospatial features, making it suitable for researchers and practitioners in geospatial science. For details, see Kurbucz and Andr\u00e9e (2025) \"Building and Managing Local Databases from Google Earth Engine with the geeLite R Package\" <https://hdl.handle.net/10986/43165>.",
    "version": "1.0.6",
    "maintainer": "Marcell T. Kurbucz <m.kurbucz@ucl.ac.uk>",
    "author": "Marcell T. Kurbucz [aut, cre],\n  Bo Pieter Johannes Andr\u00e9e [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geeLite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geeLite Building and Managing Local Databases from 'Google Earth Engine' Simplifies the creation, management, and updating of local databases using data extracted from 'Google Earth Engine' ('GEE'). It integrates with 'GEE' to store, aggregate, and process spatio-temporal data, leveraging 'SQLite' for efficient, serverless storage. The 'geeLite' package provides utilities for data transformation and supports real-time monitoring and analysis of geospatial features, making it suitable for researchers and practitioners in geospatial science. For details, see Kurbucz and Andr\u00e9e (2025) \"Building and Managing Local Databases from Google Earth Engine with the geeLite R Package\" <https://hdl.handle.net/10986/43165>.  "
  },
  {
    "id": 608,
    "package_name": "geoflow",
    "title": "Orchestrate Geospatial (Meta)Data Management Workflows and\nManage FAIR Services",
    "description": "An engine to facilitate the orchestration and execution of metadata-driven data management workflows, in compliance with 'FAIR' \n  (Findable, Accessible, Interoperable and Reusable) data management principles. By means of a pivot metadata model, relying on the 'DublinCore' standard (<https://dublincore.org/>), \n  a unique source of metadata can be used to operate multiple and inter-connected data management actions. Users can also customise their own workflows by creating specific actions \n  but the library comes with a set of native actions targeting common geographic information and data management, in particular actions oriented to the publication on the web of metadata \n  and data resources to provide standard discovery and access services. At first, default actions of the library were meant to focus on providing turn-key actions for geospatial (meta)data: \n  1) by creating manage geospatial (meta)data complying with 'ISO/TC211' (<https://committee.iso.org/home/tc211>) and 'OGC' (<https://www.ogc.org/standards/>) geographic information standards \n  (eg 19115/19119/19110/19139) and related best practices (eg. 'INSPIRE'); and 2) by facilitating extraction, reading and publishing of standard geospatial (meta)data within widely used software \n  that compound a Spatial Data Infrastructure ('SDI'), including spatial databases (eg. 'PostGIS'), metadata catalogues (eg. 'GeoNetwork', 'CSW' servers), data servers (eg. 'GeoServer'). The library was \n  then extended to actions for other domains: 1) biodiversity (meta)data standard management including handling of 'EML' metadata, and their management with 'DataOne' servers, 2) in situ sensors, \n  remote sensing and model outputs  (meta)data standard management by handling part of 'CF' conventions, 'NetCDF' data format and 'OPeNDAP' access protocol, and their management with 'Thredds' servers, \n  3) generic / domain agnostic (meta)data standard managers ('DublinCore', 'DataCite'), to facilitate the publication of data within (meta)data repositories such as 'Zenodo' (<https://zenodo.org>) \n  or DataVerse (<https://dataverse.org/>). The execution of several actions will then allow to cross-reference (meta)data resources in each action performed, offering a way to bind resources \n  between each other (eg. reference 'Zenodo' 'DOI' in 'GeoNetwork'/'GeoServer' metadata, or vice versa reference 'GeoNetwork'/'GeoServer' links in 'Zenodo' or 'EML' metadata). The use of\n  standardized configuration files ('JSON' or 'YAML' formats) allow fully reproducible workflows to facilitate the work of data and information managers.",
    "version": "1.1.0",
    "maintainer": "Emmanuel Blondel <emmanuel.blondel1@gmail.com>",
    "author": "Emmanuel Blondel [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5870-5762>),\n  Julien, Barde [aut] (ORCID: <https://orcid.org/0000-0002-3519-6141>),\n  Wilfried Heintz [aut] (ORCID: <https://orcid.org/0000-0002-9244-9766>),\n  Alexandre Bennici [ctb],\n  Sylvain Poulain [ctb],\n  Bastien Grasset [ctb],\n  Mathias Rouan [ctb],\n  Emilie Lerigoleur [ctb],\n  Yvan Le Bras [ctb],\n  Jeroen Ooms [ctb]",
    "url": "https://github.com/r-geoflow/geoflow",
    "bug_reports": "https://github.com/r-geoflow/geoflow/issues",
    "repository": "https://cran.r-project.org/package=geoflow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoflow Orchestrate Geospatial (Meta)Data Management Workflows and\nManage FAIR Services An engine to facilitate the orchestration and execution of metadata-driven data management workflows, in compliance with 'FAIR' \n  (Findable, Accessible, Interoperable and Reusable) data management principles. By means of a pivot metadata model, relying on the 'DublinCore' standard (<https://dublincore.org/>), \n  a unique source of metadata can be used to operate multiple and inter-connected data management actions. Users can also customise their own workflows by creating specific actions \n  but the library comes with a set of native actions targeting common geographic information and data management, in particular actions oriented to the publication on the web of metadata \n  and data resources to provide standard discovery and access services. At first, default actions of the library were meant to focus on providing turn-key actions for geospatial (meta)data: \n  1) by creating manage geospatial (meta)data complying with 'ISO/TC211' (<https://committee.iso.org/home/tc211>) and 'OGC' (<https://www.ogc.org/standards/>) geographic information standards \n  (eg 19115/19119/19110/19139) and related best practices (eg. 'INSPIRE'); and 2) by facilitating extraction, reading and publishing of standard geospatial (meta)data within widely used software \n  that compound a Spatial Data Infrastructure ('SDI'), including spatial databases (eg. 'PostGIS'), metadata catalogues (eg. 'GeoNetwork', 'CSW' servers), data servers (eg. 'GeoServer'). The library was \n  then extended to actions for other domains: 1) biodiversity (meta)data standard management including handling of 'EML' metadata, and their management with 'DataOne' servers, 2) in situ sensors, \n  remote sensing and model outputs  (meta)data standard management by handling part of 'CF' conventions, 'NetCDF' data format and 'OPeNDAP' access protocol, and their management with 'Thredds' servers, \n  3) generic / domain agnostic (meta)data standard managers ('DublinCore', 'DataCite'), to facilitate the publication of data within (meta)data repositories such as 'Zenodo' (<https://zenodo.org>) \n  or DataVerse (<https://dataverse.org/>). The execution of several actions will then allow to cross-reference (meta)data resources in each action performed, offering a way to bind resources \n  between each other (eg. reference 'Zenodo' 'DOI' in 'GeoNetwork'/'GeoServer' metadata, or vice versa reference 'GeoNetwork'/'GeoServer' links in 'Zenodo' or 'EML' metadata). The use of\n  standardized configuration files ('JSON' or 'YAML' formats) allow fully reproducible workflows to facilitate the work of data and information managers.  "
  },
  {
    "id": 687,
    "package_name": "hitRcovid",
    "title": "Loads, filters, and analyzes the HIT-COVID database",
    "description": "Loads the HIT-COVID database and contains functions to filter and visualize the intervention data and pair it with epidemiological information. The Health Intervention Tracking for COVID-19 (HIT-COVID) project tracks the implementation and relaxation of public health and social measures (PHSMs) taken by governments to slow transmission of SARS-COV-2 globally. Each change in policy and corresponding date is documented at the first-level  administrative unit (e.g., states, districts) and nationally for all countries with more detailed  geographic resolution in some locations (e.g., counties in the US). For a detailed description of the methods see Zheng et al. (2020) <doi:10.1038/s41597-020-00610-2>.",
    "version": "0.0.0.9000",
    "maintainer": "Andrew Azman <azman@jhu.edu>",
    "author": "",
    "url": "https://github.com/HopkinsIDD/hitRcovid",
    "bug_reports": "https://github.com/HopkinsIDD/hitRcovid/issues",
    "repository": "https://github.com/HopkinsIDD/hitRcovid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 2,
    "primary_category": "epidemiology",
    "source_universe": "github:HopkinsIDD",
    "search_text": "hitRcovid Loads, filters, and analyzes the HIT-COVID database Loads the HIT-COVID database and contains functions to filter and visualize the intervention data and pair it with epidemiological information. The Health Intervention Tracking for COVID-19 (HIT-COVID) project tracks the implementation and relaxation of public health and social measures (PHSMs) taken by governments to slow transmission of SARS-COV-2 globally. Each change in policy and corresponding date is documented at the first-level  administrative unit (e.g., states, districts) and nationally for all countries with more detailed  geographic resolution in some locations (e.g., counties in the US). For a detailed description of the methods see Zheng et al. (2020) <doi:10.1038/s41597-020-00610-2>.  "
  },
  {
    "id": 874,
    "package_name": "neotoma2",
    "title": "Working with the Neotoma Paleoecology Database",
    "description": "Access and manipulation of data using the Neotoma Paleoecology Database.\n        <https://api.neotomadb.org/api-docs/>.\n        Examples in functions that require API access are not executed during CRAN checks.\n        Vignettes do not execute as to avoid API calls during CRAN checks.",
    "version": "1.0.11",
    "maintainer": "Dominguez Vidana Socorro <dominguezvid@wisc.edu>",
    "author": "Dominguez Vidana Socorro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7926-4935>),\n  Simon Goring [aut] (ORCID: <https://orcid.org/0000-0002-2700-4605>)",
    "url": "https://github.com/NeotomaDB/neotoma2",
    "bug_reports": "https://github.com/NeotomaDB/neotoma2/issues",
    "repository": "https://cran.r-project.org/package=neotoma2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neotoma2 Working with the Neotoma Paleoecology Database Access and manipulation of data using the Neotoma Paleoecology Database.\n        <https://api.neotomadb.org/api-docs/>.\n        Examples in functions that require API access are not executed during CRAN checks.\n        Vignettes do not execute as to avoid API calls during CRAN checks.  "
  },
  {
    "id": 995,
    "package_name": "pmparser",
    "title": "Create and Maintain a Relational Database of Data from\nPubMed/MEDLINE",
    "description": "Provides a simple interface for extracting various elements from\n  the publicly available PubMed XML files, incorporating PubMed's regular\n  updates, and combining the data with the NIH Open Citation Collection. See\n  Schoenbachler and Hughey (2021) <doi:10.7717/peerj.11071>.",
    "version": "1.0.23",
    "maintainer": "Jake Hughey <jakejhughey@gmail.com>",
    "author": "Jake Hughey [aut, cre],\n  Josh Schoenbachler [aut],\n  Elliot Outland [aut]",
    "url": "https://pmparser.hugheylab.org,\nhttps://github.com/hugheylab/pmparser",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pmparser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmparser Create and Maintain a Relational Database of Data from\nPubMed/MEDLINE Provides a simple interface for extracting various elements from\n  the publicly available PubMed XML files, incorporating PubMed's regular\n  updates, and combining the data with the NIH Open Citation Collection. See\n  Schoenbachler and Hughey (2021) <doi:10.7717/peerj.11071>.  "
  },
  {
    "id": 1026,
    "package_name": "pubchem.bio",
    "title": "Biologically Informed Metabolomic Libraries from 'PubChem'",
    "description": "All 'PubChem' compounds are downloaded to a local computer, but for each compound, only partial records are used.  The data are organized into small files referenced by 'PubChem' CID.  This package also contains functions to parse the biologically relevant compounds from all 'PubChem' compounds, using biological database sources, pathway presence, and taxonomic relationships. Taxonomy is used to generate a lowest common ancestor taxonomy ID (NCBI) for each biological metabolite, which then enables creation of taxonomically specific metabolome databases for any taxon.",
    "version": "1.0.3",
    "maintainer": "Corey Broeckling <cbroeckl@colostate.edu>",
    "author": "Corey Broeckling [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pubchem.bio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pubchem.bio Biologically Informed Metabolomic Libraries from 'PubChem' All 'PubChem' compounds are downloaded to a local computer, but for each compound, only partial records are used.  The data are organized into small files referenced by 'PubChem' CID.  This package also contains functions to parse the biologically relevant compounds from all 'PubChem' compounds, using biological database sources, pathway presence, and taxonomic relationships. Taxonomy is used to generate a lowest common ancestor taxonomy ID (NCBI) for each biological metabolite, which then enables creation of taxonomically specific metabolome databases for any taxon.  "
  },
  {
    "id": 1093,
    "package_name": "resourcecode",
    "title": "Access to the 'RESOURCECODE' Hindcast Database",
    "description": "Utility functions to download data from the 'RESOURCECODE'\n    hindcast database of sea-states, time series of sea-state parameters\n    and time series of 1D and 2D wave spectra.  See\n    <https://resourcecode.ifremer.fr> for more details about the available\n    data.  Also provides facilities to plot and analyse downloaded data,\n    such as computing the sea-state parameters from both the 1D and 2D\n    surface elevation variance spectral density.",
    "version": "0.5.1",
    "maintainer": "Nicolas Raillard <nicolas.raillard@ifremer.fr>",
    "author": "Nicolas Raillard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3385-5104>)",
    "url": "https://github.com/Resourcecode-project/r-resourcecode,\nhttps://resourcecode-project.github.io/r-resourcecode/,\nhttps://resourcecode-project.r-universe.dev/resourcecode",
    "bug_reports": "https://github.com/Resourcecode-project/r-resourcecode/issues",
    "repository": "https://cran.r-project.org/package=resourcecode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "resourcecode Access to the 'RESOURCECODE' Hindcast Database Utility functions to download data from the 'RESOURCECODE'\n    hindcast database of sea-states, time series of sea-state parameters\n    and time series of 1D and 2D wave spectra.  See\n    <https://resourcecode.ifremer.fr> for more details about the available\n    data.  Also provides facilities to plot and analyse downloaded data,\n    such as computing the sea-state parameters from both the 1D and 2D\n    surface elevation variance spectral density.  "
  },
  {
    "id": 1456,
    "package_name": "xgb2sql",
    "title": "Convert Trained 'XGBoost' Model to SQL Query",
    "description": "This tool enables in-database scoring of 'XGBoost' models built in R, by translating trained model objects into SQL query. \n  'XGBoost' <https://github.com/dmlc/xgboost> provides parallel tree boosting (also known as gradient boosting machine, or GBM) algorithms\n  in a highly efficient, flexible and portable way. GBM algorithm is introduced by Friedman (2001) <doi:10.1214/aos/1013203451>, \n  and more details on 'XGBoost' can be found in Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>.",
    "version": "0.1.3",
    "maintainer": "Chengjun Hou <chengjun.hou@gmail.com>",
    "author": "Chengjun Hou [aut, cre],\n  Abhishek Bishoyi [aut]",
    "url": "https://github.com/chengjunhou/xgb2sql",
    "bug_reports": "https://github.com/chengjunhou/xgb2sql/issues",
    "repository": "https://cran.r-project.org/package=xgb2sql",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xgb2sql Convert Trained 'XGBoost' Model to SQL Query This tool enables in-database scoring of 'XGBoost' models built in R, by translating trained model objects into SQL query. \n  'XGBoost' <https://github.com/dmlc/xgboost> provides parallel tree boosting (also known as gradient boosting machine, or GBM) algorithms\n  in a highly efficient, flexible and portable way. GBM algorithm is introduced by Friedman (2001) <doi:10.1214/aos/1013203451>, \n  and more details on 'XGBoost' can be found in Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>.  "
  },
  {
    "id": 1493,
    "package_name": "ACEP",
    "title": "Analisis Computacional de Eventos de Protesta",
    "description": "La libreria 'ACEP' contiene funciones especificas para\n    desarrollar analisis computacional de eventos de protesta. Asimismo,\n    contiene base de datos con colecciones de notas sobre protestas y\n    diccionarios de palabras conflictivas. Coleccion de diccionarios que\n    reune diccionarios de diferentes origenes.  The 'ACEP' library\n    contains specific functions to perform computational analysis of\n    protest events. It also contains a database with collections of notes\n    on protests and dictionaries of conflicting words. Collection of\n    dictionaries that brings together dictionaries from different sources.",
    "version": "0.0.22",
    "maintainer": "Agustin Nieto <agustin.nieto77@gmail.com>",
    "author": "Agustin Nieto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4467-873X>)",
    "url": "https://github.com/agusnieto77/ACEP,\nhttps://agusnieto77.github.io/ACEP/",
    "bug_reports": "https://github.com/agusnieto77/ACEP/issues",
    "repository": "https://cran.r-project.org/package=ACEP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ACEP Analisis Computacional de Eventos de Protesta La libreria 'ACEP' contiene funciones especificas para\n    desarrollar analisis computacional de eventos de protesta. Asimismo,\n    contiene base de datos con colecciones de notas sobre protestas y\n    diccionarios de palabras conflictivas. Coleccion de diccionarios que\n    reune diccionarios de diferentes origenes.  The 'ACEP' library\n    contains specific functions to perform computational analysis of\n    protest events. It also contains a database with collections of notes\n    on protests and dictionaries of conflicting words. Collection of\n    dictionaries that brings together dictionaries from different sources.  "
  },
  {
    "id": 1586,
    "package_name": "ARDECO",
    "title": "Annual Regional Database of the European Commission (ARDECO)",
    "description": "A set of functions to access the 'ARDECO' (Annual Regional Database\n    of the European Commission) data directly from the official ARDECO public\n    repository through the exploitation of the 'ARDECO' APIs.\n    The APIs are completely transparent to the user and the provided functions\n    provide a direct access to the 'ARDECO' data.\n    The 'ARDECO' database is a collection of variables related to demography,\n    employment, labour market, domestic product, capital formation.\n    Each variable can be exposed in one or more units of measure as well as\n    refers to total values plus additional dimensions like economic sectors, gender,\n    age classes. Data can be also aggregated at country level according\n    to the tercet classes as defined by EUROSTAT.\n    The description of the 'ARDECO' database can be found at the following URL\n    <https://territorial.ec.europa.eu/ardeco>.",
    "version": "2.2.3",
    "maintainer": "Carmelo Attardo <carmelo.attardo@ec.europa.eu>",
    "author": "Carmelo Attardo [cre],\n  Giuseppe Bucciarelli [aut],\n  European Commission, JRC [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ARDECO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ARDECO Annual Regional Database of the European Commission (ARDECO) A set of functions to access the 'ARDECO' (Annual Regional Database\n    of the European Commission) data directly from the official ARDECO public\n    repository through the exploitation of the 'ARDECO' APIs.\n    The APIs are completely transparent to the user and the provided functions\n    provide a direct access to the 'ARDECO' data.\n    The 'ARDECO' database is a collection of variables related to demography,\n    employment, labour market, domestic product, capital formation.\n    Each variable can be exposed in one or more units of measure as well as\n    refers to total values plus additional dimensions like economic sectors, gender,\n    age classes. Data can be also aggregated at country level according\n    to the tercet classes as defined by EUROSTAT.\n    The description of the 'ARDECO' database can be found at the following URL\n    <https://territorial.ec.europa.eu/ardeco>.  "
  },
  {
    "id": 1645,
    "package_name": "AdhereR",
    "title": "Adherence to Medications",
    "description": "Computation of adherence to medications from Electronic Health care \n    Data and visualization of individual medication histories and adherence \n    patterns. The package implements a set of S3 classes and\n    functions consistent with current adherence guidelines and definitions. \n    It allows the computation of different measures of\n    adherence (as defined in the literature, but also several original ones), \n    their publication-quality plotting,\n    the estimation of event duration and time to initiation,\n    the interactive exploration of patient medication history and \n    the real-time estimation of adherence given various parameter settings.\n    It scales from very small datasets stored in flat CSV files to very large \n    databases and from single-thread processing on mid-range consumer\n    laptops to parallel processing on large heterogeneous computing clusters.\n    It exposes a standardized interface allowing it to be used from other\n    programming languages and platforms, such as Python.",
    "version": "0.8.1",
    "maintainer": "Dan Dediu <ddediu@gmail.com>",
    "author": "Dan Dediu [aut, cre],\n  Alexandra Dima [aut],\n  Samuel Allemann [aut]",
    "url": "https://github.com/ddediu/AdhereR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AdhereR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AdhereR Adherence to Medications Computation of adherence to medications from Electronic Health care \n    Data and visualization of individual medication histories and adherence \n    patterns. The package implements a set of S3 classes and\n    functions consistent with current adherence guidelines and definitions. \n    It allows the computation of different measures of\n    adherence (as defined in the literature, but also several original ones), \n    their publication-quality plotting,\n    the estimation of event duration and time to initiation,\n    the interactive exploration of patient medication history and \n    the real-time estimation of adherence given various parameter settings.\n    It scales from very small datasets stored in flat CSV files to very large \n    databases and from single-thread processing on mid-range consumer\n    laptops to parallel processing on large heterogeneous computing clusters.\n    It exposes a standardized interface allowing it to be used from other\n    programming languages and platforms, such as Python.  "
  },
  {
    "id": 1652,
    "package_name": "AeRobiology",
    "title": "A Computational Tool for Aerobiological Data",
    "description": "Different tools for managing databases of airborne particles, elaborating the main calculations and visualization of results. In a first step, data are checked using tools for quality control and all missing gaps are completed. Then, the main parameters of the pollen season are calculated and represented graphically. Multiple graphical tools are available: pollen calendars, phenological plots, time series, tendencies, interactive plots, abundance plots...",
    "version": "2.0.1",
    "maintainer": "\"Jose Oteros\" <OterosJose@gmail.com>",
    "author": "Jesus Rojo <Jesus.Rojo@uclm.es>, Antonio Picornell <picornell@uma.es>, Jose Oteros <OterosJose@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AeRobiology",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AeRobiology A Computational Tool for Aerobiological Data Different tools for managing databases of airborne particles, elaborating the main calculations and visualization of results. In a first step, data are checked using tools for quality control and all missing gaps are completed. Then, the main parameters of the pollen season are calculated and represented graphically. Multiple graphical tools are available: pollen calendars, phenological plots, time series, tendencies, interactive plots, abundance plots...  "
  },
  {
    "id": 1724,
    "package_name": "ArctosR",
    "title": "An Interface to the 'Arctos' Database",
    "description": "Performs requests to the 'Arctos' API to download data. Provides a\n    set of builder classes for performing complex requests, as well as a set of\n    simple functions for automating many common requests and workflows. More\n    information about 'Arctos' can be found in \n    Cicero et al. (2024) <doi:10.1371/journal.pone.0296478> or on their website\n    <https://arctosdb.org/>.",
    "version": "0.1.3",
    "maintainer": "Harlan R. Williams <harlanrhwilliams@gmail.com>",
    "author": "Harlan R. Williams [aut, cre],\n  Marlon E. Cobos [aut],\n  Jocelyn P. Colella [aut],\n  Michelle S. Koo [aut],\n  Vijay Barve [aut]",
    "url": "https://github.com/hrhwilliams/arctosr",
    "bug_reports": "https://github.com/hrhwilliams/arctosr/issues",
    "repository": "https://cran.r-project.org/package=ArctosR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ArctosR An Interface to the 'Arctos' Database Performs requests to the 'Arctos' API to download data. Provides a\n    set of builder classes for performing complex requests, as well as a set of\n    simple functions for automating many common requests and workflows. More\n    information about 'Arctos' can be found in \n    Cicero et al. (2024) <doi:10.1371/journal.pone.0296478> or on their website\n    <https://arctosdb.org/>.  "
  },
  {
    "id": 1755,
    "package_name": "Autoseed",
    "title": "Retrieve Disease-Related Genes from Public Sources",
    "description": "For researchers to quickly and comprehensively acquire disease genes, so as to understand the mechanism of disease,\n  we developed this program to acquire disease-related genes.\n  The data is integrated from three public databases. The three databases are 'eDGAR', 'DrugBank'\n  and 'MalaCards'. The 'eDGAR' is a comprehensive database, containing data on the relationship between disease and genes.\n  'DrugBank' contains information on 13443 drugs and 5157 targets. 'MalaCards' integrates human disease information, including disease-related genes.",
    "version": "0.1.0",
    "maintainer": "Jiawei Wu <jared_wood@163.com>",
    "author": "Jiawei Wu [aut, cre],\n  Xu Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Autoseed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Autoseed Retrieve Disease-Related Genes from Public Sources For researchers to quickly and comprehensively acquire disease genes, so as to understand the mechanism of disease,\n  we developed this program to acquire disease-related genes.\n  The data is integrated from three public databases. The three databases are 'eDGAR', 'DrugBank'\n  and 'MalaCards'. The 'eDGAR' is a comprehensive database, containing data on the relationship between disease and genes.\n  'DrugBank' contains information on 13443 drugs and 5157 targets. 'MalaCards' integrates human disease information, including disease-related genes.  "
  },
  {
    "id": 1762,
    "package_name": "AzureCosmosR",
    "title": "Interface to the 'Azure Cosmos DB' 'NoSQL' Database Service",
    "description": "An interface to 'Azure CosmosDB': <https://azure.microsoft.com/en-us/services/cosmos-db/>. On the admin side, 'AzureCosmosR' provides functionality to create and manage 'Cosmos DB' instances in Microsoft's 'Azure' cloud. On the client side, it provides an interface to the 'Cosmos DB' SQL API, letting the user store and query documents and attachments in 'Cosmos DB'. Part of the 'AzureR' family of packages.",
    "version": "1.0.0",
    "maintainer": "Hong Ooi <hongooi73@gmail.com>",
    "author": "Hong Ooi [aut, cre],\n  Andrew Liu [ctb] (Assistance with Cosmos DB),\n  Microsoft [cph]",
    "url": "https://github.com/Azure/AzureCosmosR\nhttps://github.com/Azure/AzureR",
    "bug_reports": "https://github.com/Azure/AzureCosmosR/issues",
    "repository": "https://cran.r-project.org/package=AzureCosmosR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AzureCosmosR Interface to the 'Azure Cosmos DB' 'NoSQL' Database Service An interface to 'Azure CosmosDB': <https://azure.microsoft.com/en-us/services/cosmos-db/>. On the admin side, 'AzureCosmosR' provides functionality to create and manage 'Cosmos DB' instances in Microsoft's 'Azure' cloud. On the client side, it provides an interface to the 'Cosmos DB' SQL API, letting the user store and query documents and attachments in 'Cosmos DB'. Part of the 'AzureR' family of packages.  "
  },
  {
    "id": 1765,
    "package_name": "AzureKusto",
    "title": "Interface to 'Kusto'/'Azure Data Explorer'",
    "description": "An interface to 'Azure Data Explorer', also known as 'Kusto', a fast, distributed data exploration service from Microsoft: <https://azure.microsoft.com/en-us/products/data-explorer/>. Includes 'DBI' and 'dplyr' interfaces, with the latter modelled after the 'dbplyr' package, whereby queries are translated from R into the native 'KQL' query language and executed lazily. On the admin side, the package extends the object framework provided by 'AzureRMR' to support creation and deletion of databases, and management of database principals. Part of the 'AzureR' family of packages.",
    "version": "1.1.4",
    "maintainer": "Alex Kyllo <jekyllo@microsoft.com>",
    "author": "Hong Ooi [aut],\n  Alex Kyllo [aut, cre],\n  dbplyr development team [cph] (Original framework for dplyr/database\n    interface),\n  Microsoft [cph]",
    "url": "https://github.com/Azure/AzureKusto\nhttps://github.com/Azure/AzureR",
    "bug_reports": "https://github.com/Azure/AzureKusto/issues",
    "repository": "https://cran.r-project.org/package=AzureKusto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AzureKusto Interface to 'Kusto'/'Azure Data Explorer' An interface to 'Azure Data Explorer', also known as 'Kusto', a fast, distributed data exploration service from Microsoft: <https://azure.microsoft.com/en-us/products/data-explorer/>. Includes 'DBI' and 'dplyr' interfaces, with the latter modelled after the 'dbplyr' package, whereby queries are translated from R into the native 'KQL' query language and executed lazily. On the admin side, the package extends the object framework provided by 'AzureRMR' to support creation and deletion of databases, and management of database principals. Part of the 'AzureR' family of packages.  "
  },
  {
    "id": 1769,
    "package_name": "AzureTableStor",
    "title": "Interface to the Table Storage Service in 'Azure'",
    "description": "An interface to the table storage service in 'Azure': <https://azure.microsoft.com/en-us/services/storage/tables/>. Supplies functionality for reading and writing data stored in tables, both as part of a storage account and from a 'CosmosDB' database with the table service API. Part of the 'AzureR' family of packages.",
    "version": "1.0.0",
    "maintainer": "Hong Ooi <hongooi73@gmail.com>",
    "author": "Hong Ooi [aut, cre],\n  Microsoft [cph]",
    "url": "https://github.com/Azure/AzureTableStor\nhttps://github.com/Azure/AzureR",
    "bug_reports": "https://github.com/Azure/AzureTableStor/issues",
    "repository": "https://cran.r-project.org/package=AzureTableStor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AzureTableStor Interface to the Table Storage Service in 'Azure' An interface to the table storage service in 'Azure': <https://azure.microsoft.com/en-us/services/storage/tables/>. Supplies functionality for reading and writing data stored in tables, both as part of a storage account and from a 'CosmosDB' database with the table service API. Part of the 'AzureR' family of packages.  "
  },
  {
    "id": 1816,
    "package_name": "BED",
    "title": "Biological Entity Dictionary (BED)",
    "description": "An interface for the 'Neo4j' database providing\n    mapping between different identifiers of biological entities.\n    This Biological Entity Dictionary (BED)\n    has been developed to address three main challenges.\n    The first one is related to the completeness of identifier mappings.\n    Indeed, direct mapping information provided by the different systems\n    are not always complete and can be enriched by mappings provided by other\n    resources.\n    More interestingly, direct mappings not identified by any of these\n    resources can be indirectly inferred by using mappings to a third reference.\n    For example, many human Ensembl gene ID are not directly mapped to any\n    Entrez gene ID but such mappings can be inferred using respective mappings\n    to HGNC ID. The second challenge is related to the mapping of deprecated\n    identifiers. Indeed, entity identifiers can change from one resource\n    release to another. The identifier history is provided by some resources,\n    such as Ensembl or the NCBI, but it is generally not used by mapping tools.\n    The third challenge is related to the automation of the mapping process\n    according to the relationships between the biological entities of interest.\n    Indeed, mapping between gene and protein ID scopes should not be done\n    the same way than between two scopes regarding gene ID.\n    Also, converting identifiers from different organisms should be possible\n    using gene orthologs information.\n    The method has been published by\n    Godard and van Eyll (2018) <doi:10.12688/f1000research.13925.3>.",
    "version": "1.6.2",
    "maintainer": "Patrice Godard <patrice.godard@gmail.com>",
    "author": "Patrice Godard [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6257-9730>)",
    "url": "https://patzaw.github.io/BED/, https://github.com/patzaw/BED",
    "bug_reports": "https://github.com/patzaw/BED/issues",
    "repository": "https://cran.r-project.org/package=BED",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BED Biological Entity Dictionary (BED) An interface for the 'Neo4j' database providing\n    mapping between different identifiers of biological entities.\n    This Biological Entity Dictionary (BED)\n    has been developed to address three main challenges.\n    The first one is related to the completeness of identifier mappings.\n    Indeed, direct mapping information provided by the different systems\n    are not always complete and can be enriched by mappings provided by other\n    resources.\n    More interestingly, direct mappings not identified by any of these\n    resources can be indirectly inferred by using mappings to a third reference.\n    For example, many human Ensembl gene ID are not directly mapped to any\n    Entrez gene ID but such mappings can be inferred using respective mappings\n    to HGNC ID. The second challenge is related to the mapping of deprecated\n    identifiers. Indeed, entity identifiers can change from one resource\n    release to another. The identifier history is provided by some resources,\n    such as Ensembl or the NCBI, but it is generally not used by mapping tools.\n    The third challenge is related to the automation of the mapping process\n    according to the relationships between the biological entities of interest.\n    Indeed, mapping between gene and protein ID scopes should not be done\n    the same way than between two scopes regarding gene ID.\n    Also, converting identifiers from different organisms should be possible\n    using gene orthologs information.\n    The method has been published by\n    Godard and van Eyll (2018) <doi:10.12688/f1000research.13925.3>.  "
  },
  {
    "id": 1844,
    "package_name": "BIEN",
    "title": "Tools for Accessing the Botanical Information and Ecology\nNetwork Database",
    "description": "Provides Tools for Accessing the Botanical Information and Ecology Network Database.  The BIEN database contains cleaned and standardized botanical data including occurrence, trait, plot and taxonomic data (See <https://bien.nceas.ucsb.edu/bien/> for more Information).  This package provides functions that query the BIEN database by constructing and executing optimized SQL queries.",
    "version": "1.2.7",
    "maintainer": "Brian Maitner <bmaitner@gmail.com>",
    "author": "Brian Maitner [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BIEN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BIEN Tools for Accessing the Botanical Information and Ecology\nNetwork Database Provides Tools for Accessing the Botanical Information and Ecology Network Database.  The BIEN database contains cleaned and standardized botanical data including occurrence, trait, plot and taxonomic data (See <https://bien.nceas.ucsb.edu/bien/> for more Information).  This package provides functions that query the BIEN database by constructing and executing optimized SQL queries.  "
  },
  {
    "id": 1874,
    "package_name": "BMRBr",
    "title": "'BMRB' File Downloader",
    "description": "Nuclear magnetic resonance (NMR) is a highly versatile analytical technique for studying molecular configuration, conformation, \n        and dynamics, especially those of biomacromolecules such as proteins. Biological Magnetic Resonance Data Bank ('BMRB') is a repository\n        for Data from NMR Spectroscopy on Proteins, Peptides, Nucleic Acids, and other Biomolecules. Currently, 'BMRB' offers an R package \n        'RBMRB' to fetch data, however, it doesn't easily offer individual data file downloading and storing in a local directory. When using \n        'RBMRB', the data will stored as an R object, which fundamentally hinders the NMR researches to access the rich information from raw \n        data, for example, the metadata. Here, 'BMRBr' File Downloader ('BMRBr') offers a more fundamental, low level downloader, which will \n        download original deposited .str format file. This type of file contains information such as entry title, authors, citation, protein\n        sequences, and so on.\n        Many factors affect NMR experiment outputs, such as temperature, resonance sensitivity and etc., approximately 40% of the entries in the 'BMRB' have \n        chemical shift accuracy problems [1,2] Unfortunately, current reference correction methods are heavily dependent on the availability of\n        assigned protein chemical shifts or protein structure. This is my current research project is going to solve, which will be included\n        in the future release of the package. The current version of the package is sufficient and robust enough for downloading individual \n        'BMRB' data file from the 'BMRB' database <http://www.bmrb.wisc.edu>. The functionalities of this package includes but not limited:\n        * To simplifies NMR researches by combine data downloading and results analysis together.\n        * To allows NMR data reaches a broader audience that could utilize more than just chemical shifts but also metadata.\n        * To offer reference corrected data for entries without assignment or structure information (future release).\n        Reference:\n        [1] E.L. Ulrich, H. Akutsu, J.F. Doreleijers, Y. Harano, Y.E. Ioannidis, J. Lin, et al., BioMagResBank, Nucl. Acids Res. 36 (2008) D402\u20138. <doi:10.1093/nar/gkm957>.\n        [2] L. Wang, H.R. Eghbalnia, A. Bahrami, J.L. Markley, Linear analysis of carbon-13 chemical shift differences and its application to the detection and correction of errors in referencing and spin system identifications, J. Biomol. NMR. 32 (2005) 13\u201322. <doi:10.1007/s10858-005-1717-0>.",
    "version": "0.2.0",
    "maintainer": "Xi Chen <billchenxi@gmail.com>",
    "author": "Xi Chen [aut, cre] (ORCID: <https://orcid.org/0000-0001-7094-6748>)",
    "url": "https://github.com/billchenxi/BMRBr",
    "bug_reports": "https://github.com/billchenxi/BMRBr/issues",
    "repository": "https://cran.r-project.org/package=BMRBr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BMRBr 'BMRB' File Downloader Nuclear magnetic resonance (NMR) is a highly versatile analytical technique for studying molecular configuration, conformation, \n        and dynamics, especially those of biomacromolecules such as proteins. Biological Magnetic Resonance Data Bank ('BMRB') is a repository\n        for Data from NMR Spectroscopy on Proteins, Peptides, Nucleic Acids, and other Biomolecules. Currently, 'BMRB' offers an R package \n        'RBMRB' to fetch data, however, it doesn't easily offer individual data file downloading and storing in a local directory. When using \n        'RBMRB', the data will stored as an R object, which fundamentally hinders the NMR researches to access the rich information from raw \n        data, for example, the metadata. Here, 'BMRBr' File Downloader ('BMRBr') offers a more fundamental, low level downloader, which will \n        download original deposited .str format file. This type of file contains information such as entry title, authors, citation, protein\n        sequences, and so on.\n        Many factors affect NMR experiment outputs, such as temperature, resonance sensitivity and etc., approximately 40% of the entries in the 'BMRB' have \n        chemical shift accuracy problems [1,2] Unfortunately, current reference correction methods are heavily dependent on the availability of\n        assigned protein chemical shifts or protein structure. This is my current research project is going to solve, which will be included\n        in the future release of the package. The current version of the package is sufficient and robust enough for downloading individual \n        'BMRB' data file from the 'BMRB' database <http://www.bmrb.wisc.edu>. The functionalities of this package includes but not limited:\n        * To simplifies NMR researches by combine data downloading and results analysis together.\n        * To allows NMR data reaches a broader audience that could utilize more than just chemical shifts but also metadata.\n        * To offer reference corrected data for entries without assignment or structure information (future release).\n        Reference:\n        [1] E.L. Ulrich, H. Akutsu, J.F. Doreleijers, Y. Harano, Y.E. Ioannidis, J. Lin, et al., BioMagResBank, Nucl. Acids Res. 36 (2008) D402\u20138. <doi:10.1093/nar/gkm957>.\n        [2] L. Wang, H.R. Eghbalnia, A. Bahrami, J.L. Markley, Linear analysis of carbon-13 chemical shift differences and its application to the detection and correction of errors in referencing and spin system identifications, J. Biomol. NMR. 32 (2005) 13\u201322. <doi:10.1007/s10858-005-1717-0>.  "
  },
  {
    "id": 1878,
    "package_name": "BMconcor",
    "title": "CONCOR for Structural- And Regular-Equivalence Blockmodeling",
    "description": "The four functions svdcp() ('cp' for column partitioned), svdbip() or svdbip2() ('bip' for bipartitioned), and svdbips() ('s' for a simultaneous optimization of a set of 'r' solutions), correspond to a singular value decomposition (SVD) by blocks notion, by supposing each block depending on relative subspaces, rather than on two whole spaces as usual SVD does. The other functions, based on this notion, are relative to two column partitioned data matrices x and y defining two sets of subsets x_i and y_j of variables and amount to estimate a link between x_i and y_j for the pair (x_i, y_j) relatively to the links associated to all the other pairs. These methods were first presented in: Lafosse R. & Hanafi M.,(1997) <https://eudml.org/doc/106424> and Hanafi M. & Lafosse, R. (2001) <https://eudml.org/doc/106494>.",
    "version": "2.0.0",
    "maintainer": "Fabio Ashtar Telarico <Fabio-Ashtar.Telarico@fdv.uni-lj.si>",
    "author": "Roger Lafosse [aut],\n  Fabio Ashtar Telarico [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-8740-7078>)",
    "url": "https://fatelarico.github.io/BMconcor/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BMconcor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BMconcor CONCOR for Structural- And Regular-Equivalence Blockmodeling The four functions svdcp() ('cp' for column partitioned), svdbip() or svdbip2() ('bip' for bipartitioned), and svdbips() ('s' for a simultaneous optimization of a set of 'r' solutions), correspond to a singular value decomposition (SVD) by blocks notion, by supposing each block depending on relative subspaces, rather than on two whole spaces as usual SVD does. The other functions, based on this notion, are relative to two column partitioned data matrices x and y defining two sets of subsets x_i and y_j of variables and amount to estimate a link between x_i and y_j for the pair (x_i, y_j) relatively to the links associated to all the other pairs. These methods were first presented in: Lafosse R. & Hanafi M.,(1997) <https://eudml.org/doc/106424> and Hanafi M. & Lafosse, R. (2001) <https://eudml.org/doc/106494>.  "
  },
  {
    "id": 1888,
    "package_name": "BOLDconnectR",
    "title": "Retrieve, Transform and Analyze the Barcode of Life Data Systems\nData",
    "description": "Facilitates retrieval, transformation and analysis of the data\n    from the Barcode of Life Data Systems (BOLD) database <https://boldsystems.org/>. \n    This package allows both public and private user data to be easily downloaded into the R\n    environment using a variety of inputs such as: IDs (processid, sampleid), BINs, dataset codes, \n    project codes, taxonomy, geography etc. It provides frictionless data conversion  \n    into formats compatible with other R-packages and third-party tools, \n    as well as functions for sequence alignment & clustering, biodiversity analysis and spatial mapping.",
    "version": "1.0.0",
    "maintainer": "Sameer Padhye <spadhye@uoguelph.ca>",
    "author": "Sameer Padhye [aut, cre],\n  Liliana Ballesteros-Mejia [aut],\n  Sujeevan Ratnasingham [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BOLDconnectR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BOLDconnectR Retrieve, Transform and Analyze the Barcode of Life Data Systems\nData Facilitates retrieval, transformation and analysis of the data\n    from the Barcode of Life Data Systems (BOLD) database <https://boldsystems.org/>. \n    This package allows both public and private user data to be easily downloaded into the R\n    environment using a variety of inputs such as: IDs (processid, sampleid), BINs, dataset codes, \n    project codes, taxonomy, geography etc. It provides frictionless data conversion  \n    into formats compatible with other R-packages and third-party tools, \n    as well as functions for sequence alignment & clustering, biodiversity analysis and spatial mapping.  "
  },
  {
    "id": 2009,
    "package_name": "BayesSurvive",
    "title": "Bayesian Survival Models for High-Dimensional Data",
    "description": "An implementation of Bayesian survival models with graph-structured selection priors for sparse identification of omics features predictive of survival (Madjar et al., 2021 <doi:10.1186/s12859-021-04483-z>) and its extension to use a fixed graph via a Markov Random Field (MRF) prior for capturing known structure of omics features, e.g. disease-specific pathways from the Kyoto Encyclopedia of Genes and Genomes database (Hermansen et al., 2025 <doi:10.48550/arXiv.2503.13078>).",
    "version": "0.1.0",
    "maintainer": "Zhi Zhao <zhi.zhao@medisin.uio.no>",
    "author": "Zhi Zhao [aut, cre],\n  Waldir Leoncio [aut],\n  Katrin Madjar [aut],\n  Tobias \u00d8stmo Hermansen [aut],\n  Manuela Zucknick [ctb],\n  J\u00f6rg Rahnenf\u00fchrer [ctb]",
    "url": "https://github.com/ocbe-uio/BayesSurvive",
    "bug_reports": "https://github.com/ocbe-uio/BayesSurvive/issues",
    "repository": "https://cran.r-project.org/package=BayesSurvive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesSurvive Bayesian Survival Models for High-Dimensional Data An implementation of Bayesian survival models with graph-structured selection priors for sparse identification of omics features predictive of survival (Madjar et al., 2021 <doi:10.1186/s12859-021-04483-z>) and its extension to use a fixed graph via a Markov Random Field (MRF) prior for capturing known structure of omics features, e.g. disease-specific pathways from the Kyoto Encyclopedia of Genes and Genomes database (Hermansen et al., 2025 <doi:10.48550/arXiv.2503.13078>).  "
  },
  {
    "id": 2045,
    "package_name": "BetaBit",
    "title": "Mini Games from Adventures of Beta and Bit",
    "description": "Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://github.com/BetaAndBit/Charts>.",
    "version": "2.2",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemyslaw Biecek [aut, cre],\n  Witold Chodor [trl],\n  Katarzyna Fak [aut],\n  Tomasz Zoltak [aut],\n  Foundation SmarterPoland.pl [cph]",
    "url": "https://github.com/BetaAndBit/Charts",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BetaBit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BetaBit Mini Games from Adventures of Beta and Bit Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://github.com/BetaAndBit/Charts>.  "
  },
  {
    "id": 2091,
    "package_name": "BioTIMEr",
    "title": "Tools to Use and Explore the 'BioTIME' Database",
    "description": "The 'BioTIME' database was first published in\n    2018 and inspired ideas, questions, project and research\n    article. To make it even more accessible, an R package\n    was created.\n    The 'BioTIMEr' package provides tools designed to interact with the\n    'BioTIME' database. The functions provided include the 'BioTIME' recommended\n    methods for preparing (gridding and rarefaction) time series data, a\n    selection of standard biodiversity metrics (including species richness,\n    numerical abundance and exponential Shannon) alongside examples on how to\n    display change over time. It also includes a sample subset of both the query\n    and meta data, the full versions of which are freely available on the 'BioTIME'\n    website <https://biotime.st-andrews.ac.uk/home.php>.",
    "version": "0.3.0",
    "maintainer": "Alban Sagouis <alban.sagouis@idiv.de>",
    "author": "Alban Sagouis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3827-1063>),\n  Faye Moyes [aut] (ORCID: <https://orcid.org/0000-0001-9687-0593>),\n  In\u00eas S. Martins [aut, rev] (ORCID:\n    <https://orcid.org/0000-0003-4328-7286>),\n  Shane A. Blowes [ctb] (ORCID: <https://orcid.org/0000-0001-6310-3670>),\n  Viviana Brambilla [ctb] (ORCID:\n    <https://orcid.org/0000-0002-0560-4693>),\n  Cher F. Y. Chow [ctb] (ORCID: <https://orcid.org/0000-0002-1020-8409>),\n  Ada Fontrodona-Eslava [ctb] (ORCID:\n    <https://orcid.org/0000-0001-7275-7174>),\n  Laura Ant\u00e3o [ctb, rev] (ORCID: <https://orcid.org/0000-0001-6612-9366>),\n  Jonathan M. Chase [fnd] (ORCID:\n    <https://orcid.org/0000-0001-5580-4303>),\n  Maria Dornelas [fnd, cph] (ORCID:\n    <https://orcid.org/0000-0003-2077-7055>),\n  Anne E. Magurran [fnd] (ORCID: <https://orcid.org/0000-0002-0036-2795>),\n  European Research Council grant AdG BioTIME 250189 [fnd],\n  European Research Council grant PoC BioCHANGE 727440 [fnd],\n  European Research Council grant AdG MetaCHANGE 101098020 [fnd],\n  The Leverhulme Centre for Anthropocene Biodiversity grant RC-2018-021\n    [fnd],\n  German Centre for Integrative Biodiversity Research (iDiv)\n    Halle-Jena-Leipzig [fnd] (ROR: <https://ror.org/01jty7g66>),\n  Martin Luther University Halle-Wittenberg [fnd] (ROR:\n    <https://ror.org/05gqaka33>),\n  University of St Andrews [fnd] (ROR: <https://ror.org/02wn5qz54>)",
    "url": "https://github.com/bioTIMEHub/BioTIMEr",
    "bug_reports": "https://github.com/bioTIMEHub/BioTIMEr/issues",
    "repository": "https://cran.r-project.org/package=BioTIMEr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BioTIMEr Tools to Use and Explore the 'BioTIME' Database The 'BioTIME' database was first published in\n    2018 and inspired ideas, questions, project and research\n    article. To make it even more accessible, an R package\n    was created.\n    The 'BioTIMEr' package provides tools designed to interact with the\n    'BioTIME' database. The functions provided include the 'BioTIME' recommended\n    methods for preparing (gridding and rarefaction) time series data, a\n    selection of standard biodiversity metrics (including species richness,\n    numerical abundance and exponential Shannon) alongside examples on how to\n    display change over time. It also includes a sample subset of both the query\n    and meta data, the full versions of which are freely available on the 'BioTIME'\n    website <https://biotime.st-andrews.ac.uk/home.php>.  "
  },
  {
    "id": 2203,
    "package_name": "CBRT",
    "title": "CBRT Data on Turkish Economy",
    "description": "The Central Bank of the Republic of Turkey (CBRT) provides one of \n  the most comprehensive time series databases on the Turkish economy. The 'CBRT' \n  package provides functions for accessing the CBRT's electronic data delivery \n  system <https://evds2.tcmb.gov.tr/>. It contains the lists of all data \n  categories and data groups for searching the available variables (data series).  \n  As of November 3, 2024, there were 40,826 variables in the dataset. The lists \n  of data categories and data groups can be updated by the user at any time. A \n  specific variable, a group of variables, or all variables in a data group can \n  be downloaded at different frequencies using a variety of aggregation methods.",
    "version": "0.1.1",
    "maintainer": "Erol Taymaz <etaymaz@metu.edu.tr>",
    "author": "Erol Taymaz [aut, cre] (ORCID: <https://orcid.org/0000-0001-7525-6674>)",
    "url": "https://github.com/etaymaz/CBRT",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CBRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CBRT CBRT Data on Turkish Economy The Central Bank of the Republic of Turkey (CBRT) provides one of \n  the most comprehensive time series databases on the Turkish economy. The 'CBRT' \n  package provides functions for accessing the CBRT's electronic data delivery \n  system <https://evds2.tcmb.gov.tr/>. It contains the lists of all data \n  categories and data groups for searching the available variables (data series).  \n  As of November 3, 2024, there were 40,826 variables in the dataset. The lists \n  of data categories and data groups can be updated by the user at any time. A \n  specific variable, a group of variables, or all variables in a data group can \n  be downloaded at different frequencies using a variety of aggregation methods.  "
  },
  {
    "id": 2223,
    "package_name": "CDMConnector",
    "title": "Connect to an OMOP Common Data Model",
    "description": "Provides tools for working with observational health data in the \n  Observational Medical Outcomes Partnership (OMOP) Common Data Model format with a pipe friendly syntax.\n  Common data model database table references are stored in a single compound object along with metadata.",
    "version": "2.2.0",
    "maintainer": "Adam Black <black@ohdsi.org>",
    "author": "Adam Black [aut, cre] (ORCID: <https://orcid.org/0000-0001-5576-8701>),\n  Artem Gorbachev [aut],\n  Edward Burn [aut],\n  Marti Catala Sabate [aut],\n  Ioanna Nika [aut]",
    "url": "https://darwin-eu.github.io/CDMConnector/,\nhttps://github.com/darwin-eu/CDMConnector",
    "bug_reports": "https://github.com/darwin-eu/CDMConnector/issues",
    "repository": "https://cran.r-project.org/package=CDMConnector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CDMConnector Connect to an OMOP Common Data Model Provides tools for working with observational health data in the \n  Observational Medical Outcomes Partnership (OMOP) Common Data Model format with a pipe friendly syntax.\n  Common data model database table references are stored in a single compound object along with metadata.  "
  },
  {
    "id": 2268,
    "package_name": "CITMIC",
    "title": "Estimation of Cell Infiltration Based on Cell Crosstalk",
    "description": "A systematic biology tool was developed to identify cell infiltration via Individualized Cell-Cell interaction network. 'CITMIC' first constructed a weighted cell interaction network through integrating Cell-target interaction information, molecular function data from Gene Ontology (GO) database and gene transcriptomic data in specific sample, and then, it used a network propagation algorithm on the network to identify cell infiltration for the sample. Ultimately, cell infiltration in the patient dataset was obtained by normalizing the centrality scores of the cells.",
    "version": "0.1.3",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han [aut, cre, cph],\n  Xilong Zhao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CITMIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CITMIC Estimation of Cell Infiltration Based on Cell Crosstalk A systematic biology tool was developed to identify cell infiltration via Individualized Cell-Cell interaction network. 'CITMIC' first constructed a weighted cell interaction network through integrating Cell-target interaction information, molecular function data from Gene Ontology (GO) database and gene transcriptomic data in specific sample, and then, it used a network propagation algorithm on the network to identify cell infiltration for the sample. Ultimately, cell infiltration in the patient dataset was obtained by normalizing the centrality scores of the cells.  "
  },
  {
    "id": 2290,
    "package_name": "CMHSU",
    "title": "Mental Health Status, Substance Use Status and their Concurrent\nStatus in North American Healthcare Administrative Databases",
    "description": "Patients' Mental Health (MH) status, Substance Use (SU) status, and concurrent MH/SU status in the American/Canadian Healthcare Administrative Databases can be identified. The detection is based on given parameters of interest by clinicians including the list of  plausible ICD MH/SU codes (3/4/5 characters), the required number of visits of hospital for MH/SU , the required number of visits of service physicians for MH/SU, and the maximum time span within MH visits, within SU visits, and, between MH and SU visits. Methods are described in:  Khan S <https://pubmed.ncbi.nlm.nih.gov/29044442/>, Keen C, et al. (2021) <doi:10.1111/add.15580>,  Lavergne MR, et al. (2022) <doi:10.1186/s12913-022-07759-z>, Casillas, S M, et al. (2022) <doi:10.1016/j.abrep.2022.100464>, CIHI (2022) <https://www.cihi.ca/en>, CDC (2024) <https://www.cdc.gov>, WHO (2019) <https://icd.who.int/en>.",
    "version": "0.0.6.9",
    "maintainer": "Chel Hee Lee <chelhee.lee@ucalgary.ca>",
    "author": "Mohsen Soltanifar [aut] (ORCID:\n    <https://orcid.org/0000-0002-5989-0082>),\n  Chel Hee Lee [cre, aut] (ORCID:\n    <https://orcid.org/0000-0001-8209-8176>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CMHSU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CMHSU Mental Health Status, Substance Use Status and their Concurrent\nStatus in North American Healthcare Administrative Databases Patients' Mental Health (MH) status, Substance Use (SU) status, and concurrent MH/SU status in the American/Canadian Healthcare Administrative Databases can be identified. The detection is based on given parameters of interest by clinicians including the list of  plausible ICD MH/SU codes (3/4/5 characters), the required number of visits of hospital for MH/SU , the required number of visits of service physicians for MH/SU, and the maximum time span within MH visits, within SU visits, and, between MH and SU visits. Methods are described in:  Khan S <https://pubmed.ncbi.nlm.nih.gov/29044442/>, Keen C, et al. (2021) <doi:10.1111/add.15580>,  Lavergne MR, et al. (2022) <doi:10.1186/s12913-022-07759-z>, Casillas, S M, et al. (2022) <doi:10.1016/j.abrep.2022.100464>, CIHI (2022) <https://www.cihi.ca/en>, CDC (2024) <https://www.cdc.gov>, WHO (2019) <https://icd.who.int/en>.  "
  },
  {
    "id": 2340,
    "package_name": "CRANsearcher",
    "title": "RStudio Addin for Searching Packages in CRAN Database Based on\nKeywords",
    "description": "One of the strengths of R is its vast package ecosystem. Indeed, R packages extend from visualization to Bayesian inference and from spatial analyses to pharmacokinetics (<https://cran.r-project.org/web/views/>). There is probably not an area of quantitative research that isn't represented by at least one R package. At the time of this writing, there are more than 10,000 active CRAN packages. Because of this massive ecosystem, it is important to have tools to search and learn about packages related to your personal R needs. For this reason, we developed an RStudio addin capable of searching available CRAN packages directly within RStudio.",
    "version": "1.0.0",
    "maintainer": "Agustin Calatroni <agustin_calatroni@rhoworld.com>",
    "author": "Becca Krouse [aut],\n  Agustin Calatroni [cre, aut]",
    "url": "https://github.com/RhoInc/CRANsearcher",
    "bug_reports": "https://github.com/RhoInc/CRANsearcher/issues",
    "repository": "https://cran.r-project.org/package=CRANsearcher",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CRANsearcher RStudio Addin for Searching Packages in CRAN Database Based on\nKeywords One of the strengths of R is its vast package ecosystem. Indeed, R packages extend from visualization to Bayesian inference and from spatial analyses to pharmacokinetics (<https://cran.r-project.org/web/views/>). There is probably not an area of quantitative research that isn't represented by at least one R package. At the time of this writing, there are more than 10,000 active CRAN packages. Because of this massive ecosystem, it is important to have tools to search and learn about packages related to your personal R needs. For this reason, we developed an RStudio addin capable of searching available CRAN packages directly within RStudio.  "
  },
  {
    "id": 2459,
    "package_name": "ChineseNames",
    "title": "Chinese Name Database 1930-2008",
    "description": "\n    A database of Chinese surnames and given names (1930-2008).\n    This database contains nationwide frequency statistics of\n    1,806 Chinese surnames and 2,614 Chinese characters used in given names,\n    covering about 1.2 billion Han Chinese population\n    (96.8 percent of the Han Chinese household-registered population\n    born from 1930 to 2008 and still alive in 2008).\n    This package also contains a function for computing multiple indices of\n    Chinese surnames and given names for social science research (e.g.,\n    name uniqueness, name gender, name valence, and name warmth/competence).\n    Details are provided at\n    <https://psychbruce.github.io/ChineseNames/>.",
    "version": "2025.8",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "author": "Han Wu Shuang Bao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3043-710X>)",
    "url": "https://psychbruce.github.io/ChineseNames/",
    "bug_reports": "https://github.com/psychbruce/ChineseNames/issues",
    "repository": "https://cran.r-project.org/package=ChineseNames",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ChineseNames Chinese Name Database 1930-2008 \n    A database of Chinese surnames and given names (1930-2008).\n    This database contains nationwide frequency statistics of\n    1,806 Chinese surnames and 2,614 Chinese characters used in given names,\n    covering about 1.2 billion Han Chinese population\n    (96.8 percent of the Han Chinese household-registered population\n    born from 1930 to 2008 and still alive in 2008).\n    This package also contains a function for computing multiple indices of\n    Chinese surnames and given names for social science research (e.g.,\n    name uniqueness, name gender, name valence, and name warmth/competence).\n    Details are provided at\n    <https://psychbruce.github.io/ChineseNames/>.  "
  },
  {
    "id": 2473,
    "package_name": "CirceR",
    "title": "Construct Cohort Inclusion and Restriction Criteria Expressions",
    "description": "Wraps the 'CIRCE' (<https://github.com/ohdsi/circe-be>) 'Java'\n  library allowing cohort definition expressions to be edited and converted to \n  'Markdown' or 'SQL'.",
    "version": "1.3.3",
    "maintainer": "Chris Knoll <cknoll@ohdsi.org>",
    "author": "Chris Knoll [aut, cre],\n  Martijn Schuemie [aut]",
    "url": "https://ohdsi.github.io/CirceR/, https://github.com/OHDSI/CirceR/",
    "bug_reports": "https://github.com/OHDSI/CirceR/issues/",
    "repository": "https://cran.r-project.org/package=CirceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CirceR Construct Cohort Inclusion and Restriction Criteria Expressions Wraps the 'CIRCE' (<https://github.com/ohdsi/circe-be>) 'Java'\n  library allowing cohort definition expressions to be edited and converted to \n  'Markdown' or 'SQL'.  "
  },
  {
    "id": 2487,
    "package_name": "ClickHouseHTTP",
    "title": "A Simple HTTP Database Interface to 'ClickHouse'",
    "description": "'ClickHouse' (<https://clickhouse.com/>)\n   is an open-source, high performance columnar\n   OLAP (online analytical processing of queries) database management system\n   for real-time analytics using SQL. This 'DBI' backend\n   relies on the 'ClickHouse' HTTP interface and support HTTPS protocol.",
    "version": "0.3.4",
    "maintainer": "Patrice Godard <patrice.godard@gmail.com>",
    "author": "Patrice Godard [aut, cre, cph],\n  Eusebiu Marcu [ctb]",
    "url": "https://github.com/patzaw/ClickHouseHTTP",
    "bug_reports": "https://github.com/patzaw/ClickHouseHTTP/issues",
    "repository": "https://cran.r-project.org/package=ClickHouseHTTP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ClickHouseHTTP A Simple HTTP Database Interface to 'ClickHouse' 'ClickHouse' (<https://clickhouse.com/>)\n   is an open-source, high performance columnar\n   OLAP (online analytical processing of queries) database management system\n   for real-time analytics using SQL. This 'DBI' backend\n   relies on the 'ClickHouse' HTTP interface and support HTTPS protocol.  "
  },
  {
    "id": 2539,
    "package_name": "CohortGenerator",
    "title": "Cohort Generation for the OMOP Common Data Model",
    "description": "Generate cohorts and subsets using an Observational \n  Medical Outcomes Partnership (OMOP) Common Data Model (CDM) Database. \n  Cohorts are  defined using 'CIRCE' (<https://github.com/ohdsi/circe-be>) or \n  SQL compatible with 'SqlRender' (<https://github.com/OHDSI/SqlRender>).",
    "version": "1.0.1",
    "maintainer": "Anthony Sena <sena@ohdsi.org>",
    "author": "Anthony Sena [aut, cre],\n  Jamie Gilbert [aut],\n  Gowtham Rao [aut],\n  Freddy Avila Cruz [aut],\n  Martijn Schuemie [aut],\n  Observational Health Data Science and Informatics [cph]",
    "url": "https://ohdsi.github.io/CohortGenerator/,\nhttps://github.com/OHDSI/CohortGenerator",
    "bug_reports": "https://github.com/OHDSI/CohortGenerator/issues",
    "repository": "https://cran.r-project.org/package=CohortGenerator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CohortGenerator Cohort Generation for the OMOP Common Data Model Generate cohorts and subsets using an Observational \n  Medical Outcomes Partnership (OMOP) Common Data Model (CDM) Database. \n  Cohorts are  defined using 'CIRCE' (<https://github.com/ohdsi/circe-be>) or \n  SQL compatible with 'SqlRender' (<https://github.com/OHDSI/SqlRender>).  "
  },
  {
    "id": 2556,
    "package_name": "CommonDataModel",
    "title": "OMOP CDM DDL and Documentation Generator",
    "description": "Generates the scripts required to create an Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database and associated documentation for supported database platforms. Leverages the 'SqlRender' package to convert the Data Definition Language (DDL) script written in parameterized Structured Query Language (SQL) to the other supported dialects.",
    "version": "1.0.1",
    "maintainer": "Clair Blacketer <mblacke@its.jnj.com>",
    "author": "Clair Blacketer [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CommonDataModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CommonDataModel OMOP CDM DDL and Documentation Generator Generates the scripts required to create an Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database and associated documentation for supported database platforms. Leverages the 'SqlRender' package to convert the Data Definition Language (DDL) script written in parameterized Structured Query Language (SQL) to the other supported dialects.  "
  },
  {
    "id": 2599,
    "package_name": "ConfusionTableR",
    "title": "Confusion Matrix Toolset",
    "description": "Takes the outputs of a 'caret' confusion matrix and allows for the quick conversion of these list items to lists.\n    The intended usage is to allow the tool to work with the outputs of machine learning classification models. \n    This tool works with classification problems for binary and multi-classification problems and allows for the record level conversion of the confusion matrix outputs.\n    This is useful, as it allows quick conversion of these objects for storage in database systems and to track ML model performance over time.\n    Traditionally, this approach has been used for highlighting model representation and feature slippage. ",
    "version": "1.0.4",
    "maintainer": "Gary Hutson <hutsons-hacks@outlook.com>",
    "author": "Gary Hutson [aut, cre] (ORCID: <https://orcid.org/0000-0003-3534-6143>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ConfusionTableR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConfusionTableR Confusion Matrix Toolset Takes the outputs of a 'caret' confusion matrix and allows for the quick conversion of these list items to lists.\n    The intended usage is to allow the tool to work with the outputs of machine learning classification models. \n    This tool works with classification problems for binary and multi-classification problems and allows for the record level conversion of the confusion matrix outputs.\n    This is useful, as it allows quick conversion of these objects for storage in database systems and to track ML model performance over time.\n    Traditionally, this approach has been used for highlighting model representation and feature slippage.   "
  },
  {
    "id": 2617,
    "package_name": "ConversationAlign",
    "title": "Process Text and Compute Linguistic Alignment in Conversation\nTranscripts",
    "description": "Imports conversation transcripts into R, concatenates them into a single dataframe appending event identifiers, cleans and formats the text, then yokes user-specified psycholinguistic database values to each word.  'ConversationAlign' then computes alignment indices between two interlocutors across each transcript for >40 possible semantic, lexical, and affective dimensions. In addition to alignment, 'ConversationAlign' also produces a table of analytics (e.g., token count, type-token-ratio) in a summary table describing your particular text corpus.",
    "version": "0.4.0",
    "maintainer": "Jamie Reilly <jamie_reilly@temple.edu>",
    "author": "Jamie Reilly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0891-438X>),\n  Virginia Ulichney [aut],\n  Ben Sacks [aut],\n  Sarah Weinstein [ctb],\n  Chelsea Helion [ctb],\n  Gus Cooney [ctb]",
    "url": "https://github.com/Reilly-ConceptsCognitionLab/ConversationAlign",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ConversationAlign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConversationAlign Process Text and Compute Linguistic Alignment in Conversation\nTranscripts Imports conversation transcripts into R, concatenates them into a single dataframe appending event identifiers, cleans and formats the text, then yokes user-specified psycholinguistic database values to each word.  'ConversationAlign' then computes alignment indices between two interlocutors across each transcript for >40 possible semantic, lexical, and affective dimensions. In addition to alignment, 'ConversationAlign' also produces a table of analytics (e.g., token count, type-token-ratio) in a summary table describing your particular text corpus.  "
  },
  {
    "id": 2711,
    "package_name": "DBI",
    "title": "R Database Interface",
    "description": "A database interface definition for communication between R\n    and relational database management systems.  All classes in this\n    package are virtual and need to be extended by the various R/DBMS\n    implementations.",
    "version": "1.2.3",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "R Special Interest Group on Databases (R-SIG-DB) [aut],\n  Hadley Wickham [aut],\n  Kirill M\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1416-3412>),\n  R Consortium [fnd]",
    "url": "https://dbi.r-dbi.org, https://github.com/r-dbi/DBI",
    "bug_reports": "https://github.com/r-dbi/DBI/issues",
    "repository": "https://cran.r-project.org/package=DBI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DBI R Database Interface A database interface definition for communication between R\n    and relational database management systems.  All classes in this\n    package are virtual and need to be extended by the various R/DBMS\n    implementations.  "
  },
  {
    "id": 2712,
    "package_name": "DBItest",
    "title": "Testing DBI Backends",
    "description": "A helper that tests DBI back ends for conformity to the\n    interface.",
    "version": "1.8.2",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Kirill M\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1416-3412>),\n  RStudio [cph],\n  R Consortium [fnd]",
    "url": "https://dbitest.r-dbi.org, https://github.com/r-dbi/DBItest",
    "bug_reports": "https://github.com/r-dbi/DBItest/issues",
    "repository": "https://cran.r-project.org/package=DBItest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DBItest Testing DBI Backends A helper that tests DBI back ends for conformity to the\n    interface.  "
  },
  {
    "id": 2716,
    "package_name": "DBTC",
    "title": "Dada-BLAST-Taxon Assign-Condense Metabarcode Analysis",
    "description": "First using 'dada2' R tools to analyse metabarcode data, the 'DBTC' package then uses the BLAST algorithm to search unknown sequences against local databases, and then takes reduced matched results and provides best taxonomic assignments.",
    "version": "0.1.0",
    "maintainer": "Robert G Young <rgyoung6@gmail.com>",
    "author": "Robert G Young [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6731-2506>)",
    "url": "<https://github.com/rgyoung6/DBTC>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DBTC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DBTC Dada-BLAST-Taxon Assign-Condense Metabarcode Analysis First using 'dada2' R tools to analyse metabarcode data, the 'DBTC' package then uses the BLAST algorithm to search unknown sequences against local databases, and then takes reduced matched results and provides best taxonomic assignments.  "
  },
  {
    "id": 2719,
    "package_name": "DBmaps",
    "title": "A Metadata-Driven Framework for Streamlining Database Joins",
    "description": "\n    Simplifies and automates the process of exploring and merging \n    data from relational databases. This package allows users to discover \n    table relationships, create a map of all possible joins, and generate \n    executable plans to merge data based on a structured metadata framework.",
    "version": "0.1.0",
    "maintainer": "Akshat Maurya <codingmaster902@gmail.com>",
    "author": "Akshat Maurya [aut, cre],\n  David Shilane [aut]",
    "url": "https://github.com/akshat09867/DBmaps",
    "bug_reports": "https://github.com/akshat09867/DBmaps/issues",
    "repository": "https://cran.r-project.org/package=DBmaps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DBmaps A Metadata-Driven Framework for Streamlining Database Joins \n    Simplifies and automates the process of exploring and merging \n    data from relational databases. This package allows users to discover \n    table relationships, create a map of all possible joins, and generate \n    executable plans to merge data based on a structured metadata framework.  "
  },
  {
    "id": 2722,
    "package_name": "DCEmgmt",
    "title": "DCE Data Reshaping and Processing",
    "description": "Prepare the results of a DCE to be analysed through choice models.'DCEmgmt' reshapes DCE data from wide to long format considering the special characteristics of a DCE. 'DCEmgmt' includes the function 'DCEestm' which estimates choice models once the database has been reshaped with 'DCEmgmt'.",
    "version": "0.0.1",
    "maintainer": "Daniel Perez-Troncoso <danielperez@ugr.es>",
    "author": "Daniel Perez-Troncoso [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DCEmgmt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DCEmgmt DCE Data Reshaping and Processing Prepare the results of a DCE to be analysed through choice models.'DCEmgmt' reshapes DCE data from wide to long format considering the special characteristics of a DCE. 'DCEmgmt' includes the function 'DCEestm' which estimates choice models once the database has been reshaped with 'DCEmgmt'.  "
  },
  {
    "id": 2765,
    "package_name": "DFD",
    "title": "Extract Drugs from Differential Expression Data from LINCS\nDatabase",
    "description": "Get Drug information from given differential expression profile. The package search for the bioactive compounds from reference databases such as LINCS containing the genome-wide gene expression signature (GES) from tens of thousands of drug and genetic perturbations (Subramanian et al. (2017) <DOI:10.1016/j.cell.2017.10.049>).",
    "version": "0.3.0",
    "maintainer": "Mohamed Soudy <MohmedSoudy2009@gmail.com>",
    "author": "Mohamed Soudy [aut, cre]",
    "url": "https://github.com/MohmedSoudy/DFD",
    "bug_reports": "https://github.com/MohmedSoudy/DFD/issues",
    "repository": "https://cran.r-project.org/package=DFD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DFD Extract Drugs from Differential Expression Data from LINCS\nDatabase Get Drug information from given differential expression profile. The package search for the bioactive compounds from reference databases such as LINCS containing the genome-wide gene expression signature (GES) from tens of thousands of drug and genetic perturbations (Subramanian et al. (2017) <DOI:10.1016/j.cell.2017.10.049>).  "
  },
  {
    "id": 2796,
    "package_name": "DIZtools",
    "title": "Lightweight Utilities for 'DIZ' R Package Development",
    "description": "Lightweight utility functions used for the R package\n    development infrastructure inside the data integration centers ('DIZ')\n    to standardize and facilitate repetitive tasks such as setting up a\n    database connection or issuing notification messages and to avoid\n    redundancy.",
    "version": "1.0.3",
    "maintainer": "Jonathan M. Mang <jonathan.mang@uk-erlangen.de>",
    "author": "Jonathan M. Mang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0518-4710>),\n  Lorenz A. Kapsner [aut] (ORCID:\n    <https://orcid.org/0000-0003-1866-860X>),\n  MIRACUM - Medical Informatics in Research and Care in University\n    Medicine [fnd],\n  Universit\u00e4tsklinikum Erlangen, Germany [cph]",
    "url": "https://github.com/miracum/misc-diztools",
    "bug_reports": "https://github.com/miracum/misc-diztools/issues",
    "repository": "https://cran.r-project.org/package=DIZtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DIZtools Lightweight Utilities for 'DIZ' R Package Development Lightweight utility functions used for the R package\n    development infrastructure inside the data integration centers ('DIZ')\n    to standardize and facilitate repetitive tasks such as setting up a\n    database connection or issuing notification messages and to avoid\n    redundancy.  "
  },
  {
    "id": 2797,
    "package_name": "DIZutils",
    "title": "Utilities for 'DIZ' R Package Development",
    "description": "Utility functions used for the R package development\n    infrastructure inside the data integration centers ('DIZ') to\n    standardize and facilitate repetitive tasks such as setting up a\n    database connection or issuing notification messages and to avoid\n    redundancy.",
    "version": "0.1.3",
    "maintainer": "Jonathan M. Mang <jonathan.mang@uk-erlangen.de>",
    "author": "Jonathan M. Mang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0518-4710>),\n  Lorenz A. Kapsner [aut] (ORCID:\n    <https://orcid.org/0000-0003-1866-860X>),\n  MIRACUM - Medical Informatics in Research and Care in University\n    Medicine [fnd],\n  Universit\u00e4tsklinikum Erlangen, Germany [cph]",
    "url": "https://github.com/miracum/misc-dizutils",
    "bug_reports": "https://github.com/miracum/misc-dizutils/issues",
    "repository": "https://cran.r-project.org/package=DIZutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DIZutils Utilities for 'DIZ' R Package Development Utility functions used for the R package development\n    infrastructure inside the data integration centers ('DIZ') to\n    standardize and facilitate repetitive tasks such as setting up a\n    database connection or issuing notification messages and to avoid\n    redundancy.  "
  },
  {
    "id": 2819,
    "package_name": "DNAtools",
    "title": "Tools for Analysing Forensic Genetic DNA Data",
    "description": "Computationally efficient tools for comparing all pairs of profiles\n    in a DNA database. The expectation and covariance of the summary statistic\n    is implemented for fast computing. Routines for estimating proportions of\n    close related individuals are available. The use of wildcards (also called F-\n    designation) is implemented. Dedicated functions ease plotting the results. \n    See Tvedebrink et al. (2012) <doi:10.1016/j.fsigen.2011.08.001>. \n    Compute the distribution of the numbers of alleles in DNA mixtures. \n    See Tvedebrink (2013) <doi:10.1016/j.fsigss.2013.10.142>.",
    "version": "0.2-5",
    "maintainer": "Mikkel Meyer Andersen <mikl@math.aau.dk>",
    "author": "Torben Tvedebrink [aut],\n  James Curran [aut],\n  Mikkel Meyer Andersen [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/mikldk/DNAtools/issues",
    "repository": "https://cran.r-project.org/package=DNAtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DNAtools Tools for Analysing Forensic Genetic DNA Data Computationally efficient tools for comparing all pairs of profiles\n    in a DNA database. The expectation and covariance of the summary statistic\n    is implemented for fast computing. Routines for estimating proportions of\n    close related individuals are available. The use of wildcards (also called F-\n    designation) is implemented. Dedicated functions ease plotting the results. \n    See Tvedebrink et al. (2012) <doi:10.1016/j.fsigen.2011.08.001>. \n    Compute the distribution of the numbers of alleles in DNA mixtures. \n    See Tvedebrink (2013) <doi:10.1016/j.fsigss.2013.10.142>.  "
  },
  {
    "id": 2881,
    "package_name": "DTSEA",
    "title": "Drug Target Set Enrichment Analysis",
    "description": "It is a novel tool used to identify the candidate drugs against a particular disease based on the drug target set enrichment analysis. It assumes the most effective drugs are those with a closer affinity in the protein-protein interaction network to the specified disease. (See G\u00f3mez-Carballa et al. (2022) <doi: 10.1016/j.envres.2022.112890> and Feng et al. (2022) <doi: 10.7150/ijms.67815> for disease expression profiles; see Wishart et al. (2018) <doi: 10.1093/nar/gkx1037> and Gaulton et al. (2017) <doi: 10.1093/nar/gkw1074> for drug target information; see Kanehisa et al. (2021) <doi: 10.1093/nar/gkaa970> for the details of KEGG database.)",
    "version": "0.0.3",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han [aut, cre, cph],\n  Yinchun Su [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DTSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DTSEA Drug Target Set Enrichment Analysis It is a novel tool used to identify the candidate drugs against a particular disease based on the drug target set enrichment analysis. It assumes the most effective drugs are those with a closer affinity in the protein-protein interaction network to the specified disease. (See G\u00f3mez-Carballa et al. (2022) <doi: 10.1016/j.envres.2022.112890> and Feng et al. (2022) <doi: 10.7150/ijms.67815> for disease expression profiles; see Wishart et al. (2018) <doi: 10.1093/nar/gkx1037> and Gaulton et al. (2017) <doi: 10.1093/nar/gkw1074> for drug target information; see Kanehisa et al. (2021) <doi: 10.1093/nar/gkaa970> for the details of KEGG database.)  "
  },
  {
    "id": 2916,
    "package_name": "DatabaseConnector",
    "title": "Connecting to Various Database Platforms",
    "description": "An R 'DataBase Interface' ('DBI') compatible interface to various database platforms ('PostgreSQL', 'Oracle', 'Microsoft SQL Server', \n    'Amazon Redshift', 'Microsoft Parallel Database Warehouse', 'IBM Netezza', 'Apache Impala', 'Google BigQuery', 'Snowflake', 'Spark', 'SQLite', \n    and 'InterSystems IRIS'). Also includes support for fetching data as 'Andromeda' objects. Uses either 'Java Database Connectivity' ('JDBC') or \n    other 'DBI' drivers to connect to databases.",
    "version": "7.0.0",
    "maintainer": "Martijn Schuemie <schuemie@ohdsi.org>",
    "author": "Martijn Schuemie [aut, cre],\n  Marc Suchard [aut],\n  Adam Black [aut],\n  Observational Health Data Science and Informatics [cph],\n  Microsoft Inc. [cph] (SQL Server JDBC driver),\n  PostgreSQL Global Development Group [cph] (PostgreSQL JDBC driver),\n  Oracle Inc. [cph] (Oracle JDBC driver),\n  Amazon Inc. [cph] (RedShift JDBC driver)",
    "url": "https://ohdsi.github.io/DatabaseConnector/,\nhttps://github.com/OHDSI/DatabaseConnector",
    "bug_reports": "https://github.com/OHDSI/DatabaseConnector/issues",
    "repository": "https://cran.r-project.org/package=DatabaseConnector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DatabaseConnector Connecting to Various Database Platforms An R 'DataBase Interface' ('DBI') compatible interface to various database platforms ('PostgreSQL', 'Oracle', 'Microsoft SQL Server', \n    'Amazon Redshift', 'Microsoft Parallel Database Warehouse', 'IBM Netezza', 'Apache Impala', 'Google BigQuery', 'Snowflake', 'Spark', 'SQLite', \n    and 'InterSystems IRIS'). Also includes support for fetching data as 'Andromeda' objects. Uses either 'Java Database Connectivity' ('JDBC') or \n    other 'DBI' drivers to connect to databases.  "
  },
  {
    "id": 2917,
    "package_name": "DatabaseConnectorJars",
    "title": "JAR Dependencies for the 'DatabaseConnector' Package",
    "description": "Provides external JAR dependencies for the 'DatabaseConnector' package.",
    "version": "1.1.0",
    "maintainer": "Martijn Schuemie <schuemie@ohdsi.org>",
    "author": "Martijn Schuemie [aut, cre],\n  Marc Suchard [aut],\n  Observational Health Data Science and Informatics [cph],\n  Microsoft Inc. [cph] (SQL Server JDBC driver),\n  PostgreSQL Global Development Group [cph] (PostgreSQL JDBC driver),\n  Oracle Inc. [cph] (Oracle JDBC driver),\n  Amazon Inc. [cph] (RedShift JDBC driver)",
    "url": "https://github.com/OHDSI/DatabaseConnectorJars",
    "bug_reports": "https://github.com/OHDSI/DatabaseConnectorJars/issues",
    "repository": "https://cran.r-project.org/package=DatabaseConnectorJars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DatabaseConnectorJars JAR Dependencies for the 'DatabaseConnector' Package Provides external JAR dependencies for the 'DatabaseConnector' package.  "
  },
  {
    "id": 2919,
    "package_name": "DatastreamDSWS2R",
    "title": "Provides a Link Between the 'LSEG Datastream' System and R",
    "description": "Provides a set of functions and a class to connect, extract and\n    upload information from the 'LSEG Datastream' database. This\n    package uses the 'DSWS' API and server used by the 'Datastream DFO addin'.  \n    Details of this API are available at <https://www.lseg.com/en/data-analytics>.\n    Please report issues at <https://github.com/CharlesCara/DatastreamDSWS2R/issues>.",
    "version": "1.9.12",
    "maintainer": "Charles Cara <charles.cara@absolute-strategy.com>",
    "author": "Charles Cara [aut, cre]",
    "url": "https://github.com/CharlesCara/DatastreamDSWS2R",
    "bug_reports": "https://github.com/CharlesCara/DatastreamDSWS2R/issues",
    "repository": "https://cran.r-project.org/package=DatastreamDSWS2R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DatastreamDSWS2R Provides a Link Between the 'LSEG Datastream' System and R Provides a set of functions and a class to connect, extract and\n    upload information from the 'LSEG Datastream' database. This\n    package uses the 'DSWS' API and server used by the 'Datastream DFO addin'.  \n    Details of this API are available at <https://www.lseg.com/en/data-analytics>.\n    Please report issues at <https://github.com/CharlesCara/DatastreamDSWS2R/issues>.  "
  },
  {
    "id": 2920,
    "package_name": "DatastreamR",
    "title": "Datastream API",
    "description": "Access Datastream content through <https://product.datastream.com/dswsclient/Docs/Default.aspx>., our historical financial database with over 35 million individual instruments or indicators across all major asset classes, including over 19 million active economic indicators. It features 120 years of data, across 175 countries \u2013 the information you need to interpret market trends, economic cycles, and the impact of world events.    Data spans bond indices, bonds, commodities, convertibles, credit default swaps, derivatives, economics, energy, equities, equity indices, ESG, estimates, exchange rates, fixed income, funds, fundamentals, interest rates, and investment trusts. Unique content includes I/B/E/S Estimates, Worldscope Fundamentals, point-in-time data, and Reuters Polls.    Alongside the content, sit a set of powerful analytical tools for exploring relationships between different asset types, with a library of customizable analytical functions.    In-house timeseries can also be uploaded using the package to comingle with Datastream maintained datasets, use with these analytical tools and displayed in Datastream\u2019s flexible charting facilities in Microsoft Office. ",
    "version": "2.0.4",
    "maintainer": "LSEG (Datastream) <datastreamapi@lseg.com>",
    "author": "LSEG (Datastream) [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DatastreamR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DatastreamR Datastream API Access Datastream content through <https://product.datastream.com/dswsclient/Docs/Default.aspx>., our historical financial database with over 35 million individual instruments or indicators across all major asset classes, including over 19 million active economic indicators. It features 120 years of data, across 175 countries \u2013 the information you need to interpret market trends, economic cycles, and the impact of world events.    Data spans bond indices, bonds, commodities, convertibles, credit default swaps, derivatives, economics, energy, equities, equity indices, ESG, estimates, exchange rates, fixed income, funds, fundamentals, interest rates, and investment trusts. Unique content includes I/B/E/S Estimates, Worldscope Fundamentals, point-in-time data, and Reuters Polls.    Alongside the content, sit a set of powerful analytical tools for exploring relationships between different asset types, with a library of customizable analytical functions.    In-house timeseries can also be uploaded using the package to comingle with Datastream maintained datasets, use with these analytical tools and displayed in Datastream\u2019s flexible charting facilities in Microsoft Office.   "
  },
  {
    "id": 3037,
    "package_name": "DrillR",
    "title": "R Driver for Apache Drill",
    "description": "Provides a R driver for Apache Drill<https://drill.apache.org>, which could connect to the Apache Drill cluster<https://drill.apache.org/docs/installing-drill-on-the-cluster> or drillbit<https://drill.apache.org/docs/embedded-mode-prerequisites> and get result(in data frame) from the SQL query and check the current configuration status. This link <https://drill.apache.org/docs> contains more information about Apache Drill.",
    "version": "0.1",
    "maintainer": "Hanbing Yang <hanbingflying@sina.com>",
    "author": "Hanbing Yang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DrillR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DrillR R Driver for Apache Drill Provides a R driver for Apache Drill<https://drill.apache.org>, which could connect to the Apache Drill cluster<https://drill.apache.org/docs/installing-drill-on-the-cluster> or drillbit<https://drill.apache.org/docs/embedded-mode-prerequisites> and get result(in data frame) from the SQL query and check the current configuration status. This link <https://drill.apache.org/docs> contains more information about Apache Drill.  "
  },
  {
    "id": 3072,
    "package_name": "ECOTOXr",
    "title": "Download and Extract Data from US EPA's ECOTOX Database",
    "description": "The US EPA ECOTOX database is a freely available database\n    with a treasure of aquatic and terrestrial ecotoxicological data.\n    As the online search interface doesn't come with an API, this\n    package provides the means to easily access and search the database\n    in R. To this end, all raw tables are downloaded from the EPA website\n    and stored in a local SQLite database <doi:10.1016/j.chemosphere.2024.143078>.",
    "version": "1.2.4",
    "maintainer": "Pepijn de Vries <pepijn.devries@outlook.com>",
    "author": "Pepijn de Vries [aut, cre, dtc] (ORCID:\n    <https://orcid.org/0000-0002-7961-6646>)",
    "url": "https://github.com/pepijn-devries/ECOTOXr,\nhttps://pepijn-devries.github.io/ECOTOXr/,\nhttps://doi.org/10.1016/j.chemosphere.2024.143078",
    "bug_reports": "https://github.com/pepijn-devries/ECOTOXr/issues",
    "repository": "https://cran.r-project.org/package=ECOTOXr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ECOTOXr Download and Extract Data from US EPA's ECOTOX Database The US EPA ECOTOX database is a freely available database\n    with a treasure of aquatic and terrestrial ecotoxicological data.\n    As the online search interface doesn't come with an API, this\n    package provides the means to easily access and search the database\n    in R. To this end, all raw tables are downloaded from the EPA website\n    and stored in a local SQLite database <doi:10.1016/j.chemosphere.2024.143078>.  "
  },
  {
    "id": 3096,
    "package_name": "EGM",
    "title": "Intracardiac Electrograms",
    "description": "A system for importing electrophysiological signal, based\n    on the 'Waveform Database (WFDB)' software package, written by Moody et al \n    2022 <doi:10.13026/gjvw-1m31>. A R-based system to utilize 'WFDB' functions\n    for reading and writing signal data, as well as functions for visualization\n    and analysis are provided. A stable and broadly compatible class for working\n    with signal data, supporting the reading in of cardiac electrophysiological\n    files such as intracardiac electrograms, is introduced.",
    "version": "0.2.0",
    "maintainer": "Anish S. Shah <shah.in.boots@gmail.com>",
    "author": "Anish S. Shah [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9729-1558>),\n  Darren Seaney [ctb]",
    "url": "https://shah-in-boots.github.io/EGM/",
    "bug_reports": "https://github.com/shah-in-boots/EGM/issues",
    "repository": "https://cran.r-project.org/package=EGM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EGM Intracardiac Electrograms A system for importing electrophysiological signal, based\n    on the 'Waveform Database (WFDB)' software package, written by Moody et al \n    2022 <doi:10.13026/gjvw-1m31>. A R-based system to utilize 'WFDB' functions\n    for reading and writing signal data, as well as functions for visualization\n    and analysis are provided. A stable and broadly compatible class for working\n    with signal data, supporting the reading in of cardiac electrophysiological\n    files such as intracardiac electrograms, is introduced.  "
  },
  {
    "id": 3100,
    "package_name": "EHR",
    "title": "Electronic Health Record (EHR) Data Processing and Analysis Tool",
    "description": "Process and analyze electronic health record (EHR) data. The 'EHR'\n    package provides modules to perform diverse medication-related studies using\n    data from EHR databases. Especially, the package includes modules to perform\n    pharmacokinetic/pharmacodynamic (PK/PD) analyses using EHRs, as outlined in\n    Choi, Beck, McNeer, Weeks, Williams, James, Niu, Abou-Khalil, Birdwell, Roden, \n    Stein, Bejan, Denny, and Van Driest (2020) <doi:10.1002/cpt.1787>. Additional \n    modules will be added in future. In addition, this package provides various \n    functions useful to perform Phenome Wide Association Study (PheWAS) to explore \n    associations between drug exposure and phenotypes obtained from EHR data, as \n    outlined in Choi, Carroll, Beck, Mosley, Roden, Denny, and Van Driest (2018) \n    <doi:10.1093/bioinformatics/bty306>.",
    "version": "0.4-11",
    "maintainer": "Leena Choi <leena.choi@vanderbilt.edu>",
    "author": "Leena Choi [aut, cre] (ORCID: <https://orcid.org/0000-0002-2544-7090>),\n  Cole Beck [aut] (ORCID: <https://orcid.org/0000-0002-6849-6255>),\n  Hannah Weeks [aut] (ORCID: <https://orcid.org/0000-0002-0262-6790>),\n  Elizabeth McNeer [aut],\n  Nathan James [aut] (ORCID: <https://orcid.org/0000-0001-7079-9151>),\n  Michael Williams [aut]",
    "url": "https://choileena.github.io/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EHR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EHR Electronic Health Record (EHR) Data Processing and Analysis Tool Process and analyze electronic health record (EHR) data. The 'EHR'\n    package provides modules to perform diverse medication-related studies using\n    data from EHR databases. Especially, the package includes modules to perform\n    pharmacokinetic/pharmacodynamic (PK/PD) analyses using EHRs, as outlined in\n    Choi, Beck, McNeer, Weeks, Williams, James, Niu, Abou-Khalil, Birdwell, Roden, \n    Stein, Bejan, Denny, and Van Driest (2020) <doi:10.1002/cpt.1787>. Additional \n    modules will be added in future. In addition, this package provides various \n    functions useful to perform Phenome Wide Association Study (PheWAS) to explore \n    associations between drug exposure and phenotypes obtained from EHR data, as \n    outlined in Choi, Carroll, Beck, Mosley, Roden, Denny, and Van Driest (2018) \n    <doi:10.1093/bioinformatics/bty306>.  "
  },
  {
    "id": 3154,
    "package_name": "ERDbuilder",
    "title": "Entity Relationship Diagrams Builder",
    "description": "Build entity relationship diagrams (ERD) to specify the nature of the relationship between tables in a database.",
    "version": "1.0.0",
    "maintainer": "Guillermo Basulto-Elias <basulto@iastate.edu>",
    "author": "Jonathan Wood [aut] (ORCID: <https://orcid.org/0000-0003-0131-6384>),\n  Guillermo Basulto-Elias [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5205-2190>)",
    "url": "https://github.com/gbasulto/ERDbuilder,\nhttps://gbasulto.github.io/ERDbuilder/",
    "bug_reports": "https://github.com/gbasulto/ERDbuilder/issues",
    "repository": "https://cran.r-project.org/package=ERDbuilder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ERDbuilder Entity Relationship Diagrams Builder Build entity relationship diagrams (ERD) to specify the nature of the relationship between tables in a database.  "
  },
  {
    "id": 3167,
    "package_name": "ETLUtils",
    "title": "Utility Functions to Execute Standard Extract/Transform/Load\nOperations (using Package 'ff') on Large Data",
    "description": "Provides functions to facilitate the use of the 'ff' package\n    in interaction with big data in 'SQL' databases (e.g. in 'Oracle', 'MySQL',\n    'PostgreSQL', 'Hive') by allowing easy importing directly into 'ffdf' objects\n    using 'DBI', 'RODBC' and 'RJDBC'. Also contains some basic utility functions to\n    do fast left outer join merging based on 'match', factorisation of data and a\n    basic function for re-coding vectors.",
    "version": "1.6",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre]",
    "url": "https://github.com/jwijffels/ETLUtils",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ETLUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ETLUtils Utility Functions to Execute Standard Extract/Transform/Load\nOperations (using Package 'ff') on Large Data Provides functions to facilitate the use of the 'ff' package\n    in interaction with big data in 'SQL' databases (e.g. in 'Oracle', 'MySQL',\n    'PostgreSQL', 'Hive') by allowing easy importing directly into 'ffdf' objects\n    using 'DBI', 'RODBC' and 'RJDBC'. Also contains some basic utility functions to\n    do fast left outer join merging based on 'match', factorisation of data and a\n    basic function for re-coding vectors.  "
  },
  {
    "id": 3203,
    "package_name": "EdSurvey",
    "title": "Analysis of NCES Education Survey and Assessment Data",
    "description": "Read in and analyze functions for education survey and assessment data from the National Center for Education Statistics (NCES) <https://nces.ed.gov/>, including National Assessment of Educational Progress (NAEP) data <https://nces.ed.gov/nationsreportcard/> and data from the International Assessment Database: Organisation for Economic Co-operation and Development (OECD) <https://www.oecd.org/>, including Programme for International Student Assessment (PISA), Teaching and Learning International Survey (TALIS), Programme for the International Assessment of Adult Competencies (PIAAC), and International Association for the Evaluation of Educational Achievement (IEA) <https://www.iea.nl/>, including Trends in International Mathematics and Science Study (TIMSS), TIMSS Advanced, Progress in International Reading Literacy Study (PIRLS), International Civic and Citizenship Study (ICCS), International Computer and Information Literacy Study (ICILS), and Civic Education Study (CivEd).",
    "version": "4.0.7",
    "maintainer": "Paul Bailey <pbailey@air.org>",
    "author": "Paul Bailey [aut, cre] (ORCID: <https://orcid.org/0000-0003-0989-8729>),\n  Ahmad Emad [aut],\n  Huade Huo [aut] (ORCID: <https://orcid.org/0009-0004-5014-646X>),\n  Michael Lee [aut] (ORCID: <https://orcid.org/0009-0006-0959-787X>),\n  Yuqi Liao [aut] (ORCID: <https://orcid.org/0000-0001-9359-6015>),\n  Alex Lishinski [aut] (ORCID: <https://orcid.org/0000-0003-4506-1600>),\n  Trang Nguyen [aut] (ORCID: <https://orcid.org/0009-0001-0167-8775>),\n  Qingshu Xie [aut],\n  Jiao Yu [aut],\n  Ting Zhang [aut] (ORCID: <https://orcid.org/0009-0001-1724-6141>),\n  Eric Buehler [aut] (ORCID: <https://orcid.org/0009-0004-6354-2015>),\n  Sun-joo Lee [aut],\n  Blue Webb [aut] (ORCID: <https://orcid.org/0009-0004-4080-9864>),\n  Thomas Fink [aut] (ORCID: <https://orcid.org/0009-0003-9308-2833>),\n  Emmanuel Sikali [pdr],\n  Claire Kelley [ctb],\n  Jeppe Bundsgaard [ctb],\n  Ren C'deBaca [ctb],\n  Anders Astrup Christensen [ctb]",
    "url": "https://www.air.org/project/nces-data-r-project-edsurvey",
    "bug_reports": "https://github.com/American-Institutes-for-Research/EdSurvey/issues",
    "repository": "https://cran.r-project.org/package=EdSurvey",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EdSurvey Analysis of NCES Education Survey and Assessment Data Read in and analyze functions for education survey and assessment data from the National Center for Education Statistics (NCES) <https://nces.ed.gov/>, including National Assessment of Educational Progress (NAEP) data <https://nces.ed.gov/nationsreportcard/> and data from the International Assessment Database: Organisation for Economic Co-operation and Development (OECD) <https://www.oecd.org/>, including Programme for International Student Assessment (PISA), Teaching and Learning International Survey (TALIS), Programme for the International Assessment of Adult Competencies (PIAAC), and International Association for the Evaluation of Educational Achievement (IEA) <https://www.iea.nl/>, including Trends in International Mathematics and Science Study (TIMSS), TIMSS Advanced, Progress in International Reading Literacy Study (PIRLS), International Civic and Citizenship Study (ICCS), International Computer and Information Literacy Study (ICILS), and Civic Education Study (CivEd).  "
  },
  {
    "id": 3329,
    "package_name": "FAOSTAT",
    "title": "Download Data from the FAOSTAT Database",
    "description": "Download Data from the FAOSTAT Database of the Food and Agricultural Organization (FAO) of the United Nations.\n    A list of functions to download statistics from FAOSTAT (database of the FAO <https://www.fao.org/faostat/>) \n    and WDI (database of the World Bank <https://data.worldbank.org/>), and to perform some harmonization operations.",
    "version": "2.4.0",
    "maintainer": "Paul Rougieux <paul.rougieux@gmail.com>",
    "author": "Michael C. J., Markus Gesmann, Filippo Gheri, Paul Rougieux <paul.rougieux@gmail.com>, Sebastian Campbell",
    "url": "https://gitlab.com/paulrougieux/faostatpackage",
    "bug_reports": "https://gitlab.com/paulrougieux/faostatpackage/-/issues",
    "repository": "https://cran.r-project.org/package=FAOSTAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FAOSTAT Download Data from the FAOSTAT Database Download Data from the FAOSTAT Database of the Food and Agricultural Organization (FAO) of the United Nations.\n    A list of functions to download statistics from FAOSTAT (database of the FAO <https://www.fao.org/faostat/>) \n    and WDI (database of the World Bank <https://data.worldbank.org/>), and to perform some harmonization operations.  "
  },
  {
    "id": 3439,
    "package_name": "FakeDataR",
    "title": "Privacy-Preserving Synthetic Data for 'LLM' Workflows",
    "description": "Generate privacy-preserving synthetic datasets that mirror structure, types, factor levels, and missingness; export bundles for 'LLM' workflows (data plus 'JSON' schema and guidance); and build fake data directly from 'SQL' database tables without reading real rows. Methods are related to approaches in Nowok, Raab and Dibben (2016) <doi:10.32614/RJ-2016-019> and the foundation-model overview by Bommasani et al. (2021) <doi:10.48550/arXiv.2108.07258>.",
    "version": "0.2.2",
    "maintainer": "Zobaer Ahmed <zunnun09@gmail.com>",
    "author": "Zobaer Ahmed [aut, cre]",
    "url": "https://zobaer09.github.io/FakeDataR/,\nhttps://github.com/zobaer09/FakeDataR",
    "bug_reports": "https://github.com/zobaer09/FakeDataR/issues",
    "repository": "https://cran.r-project.org/package=FakeDataR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FakeDataR Privacy-Preserving Synthetic Data for 'LLM' Workflows Generate privacy-preserving synthetic datasets that mirror structure, types, factor levels, and missingness; export bundles for 'LLM' workflows (data plus 'JSON' schema and guidance); and build fake data directly from 'SQL' database tables without reading real rows. Methods are related to approaches in Nowok, Raab and Dibben (2016) <doi:10.32614/RJ-2016-019> and the foundation-model overview by Bommasani et al. (2021) <doi:10.48550/arXiv.2108.07258>.  "
  },
  {
    "id": 3523,
    "package_name": "FormShare",
    "title": "A Simple Connection Between the 'FormShare App' and 'R' for\nAdvanced Analytics",
    "description": "Provides analytics directly from 'R'. It requires:\n    'FormShare App': <https://github.com/qlands/FormShare >= 2.22.0> .\n    Analytics plugin: <https://github.com/qlands/formshare_analytics_plugin> .\n    Remote SQL plugin: <https://github.com/qlands/formshare_sql_plugin> .",
    "version": "1.0.1",
    "maintainer": "Carlos Quiros <cquiros@qlands.com>",
    "author": "Carlos Quiros [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9485-9961>)",
    "url": "https://github.com/qlands/formshare-R-package",
    "bug_reports": "https://github.com/qlands/formshare-R-package/issues",
    "repository": "https://cran.r-project.org/package=FormShare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FormShare A Simple Connection Between the 'FormShare App' and 'R' for\nAdvanced Analytics Provides analytics directly from 'R'. It requires:\n    'FormShare App': <https://github.com/qlands/FormShare >= 2.22.0> .\n    Analytics plugin: <https://github.com/qlands/formshare_analytics_plugin> .\n    Remote SQL plugin: <https://github.com/qlands/formshare_sql_plugin> .  "
  },
  {
    "id": 3595,
    "package_name": "GCD",
    "title": "Global Charcoal Database",
    "description": "Contains the Global Charcoal database data. Data include charcoal\n    series (age, depth, charcoal quantity, associated units and methods) and\n    information on sedimentary sites (localisation, depositional environment, biome,\n    etc.) as well as publications informations. Since 4.0.0 the GCD mirrors the online SQL database at <http://paleofire.org>.",
    "version": "4.0.7",
    "maintainer": "Olivier Blarquez <blarquez@gmail.com>",
    "author": "Global Paleofire Working Group <paleofire@univ-fcomte.fr>",
    "url": "http://paleofire.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GCD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GCD Global Charcoal Database Contains the Global Charcoal database data. Data include charcoal\n    series (age, depth, charcoal quantity, associated units and methods) and\n    information on sedimentary sites (localisation, depositional environment, biome,\n    etc.) as well as publications informations. Since 4.0.0 the GCD mirrors the online SQL database at <http://paleofire.org>.  "
  },
  {
    "id": 3623,
    "package_name": "GEOmap",
    "title": "Topographic and Geologic Mapping",
    "description": "Set of routines for making map projections (forward and inverse), topographic maps, perspective plots, geological maps, geological map symbols, geological databases, interactive plotting and selection of focus regions.",
    "version": "2.5-11",
    "maintainer": "Jonathan M. Lees <jonathan.lees@unc.edu>",
    "author": "Jonathan M. Lees [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GEOmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GEOmap Topographic and Geologic Mapping Set of routines for making map projections (forward and inverse), topographic maps, perspective plots, geological maps, geological map symbols, geological databases, interactive plotting and selection of focus regions.  "
  },
  {
    "id": 3651,
    "package_name": "GHCNr",
    "title": "Download Weather Station Data from GHCNd",
    "description": "The goal of 'GHCNr' is to provide a fast and friendly interface with the Global Historical Climatology Network daily (GHCNd) database, which contains daily summaries of weather station data worldwide (<https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily>). GHCNd is accessed through the web API <https://www.ncei.noaa.gov/access/services/data/v1>. 'GHCNr' main functionalities consist of downloading data from GHCNd, filter it, and to aggregate it at monthly and annual scales.",
    "version": "1.4.6",
    "maintainer": "Emilio Berti <emilio.berti@idiv.de>",
    "author": "Emilio Berti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9286-011X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GHCNr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GHCNr Download Weather Station Data from GHCNd The goal of 'GHCNr' is to provide a fast and friendly interface with the Global Historical Climatology Network daily (GHCNd) database, which contains daily summaries of weather station data worldwide (<https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily>). GHCNd is accessed through the web API <https://www.ncei.noaa.gov/access/services/data/v1>. 'GHCNr' main functionalities consist of downloading data from GHCNd, filter it, and to aggregate it at monthly and annual scales.  "
  },
  {
    "id": 3656,
    "package_name": "GIFT",
    "title": "Access to the Global Inventory of Floras and Traits (GIFT)",
    "description": "Retrieving regional plant checklists, species traits and\n  distributions, and environmental data from the Global Inventory of Floras and\n  Traits (GIFT). More information about the GIFT database can be found at\n  <https://gift.uni-goettingen.de/about> and the map of available floras can be\n  visualized at <https://gift.uni-goettingen.de/map>. The API and associated\n  queries can be accessed according the following scheme:\n  <https://gift.uni-goettingen.de/api/extended/index2.0.php?query=env_raster>.",
    "version": "1.3.3",
    "maintainer": "Pierre Denelle <pierre.denelle@gmail.com>",
    "author": "Pierre Denelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5037-2281>),\n  Patrick Weigelt [aut] (ORCID: <https://orcid.org/0000-0002-2485-3708>)",
    "url": "https://github.com/BioGeoMacro/GIFT,\nhttps://biogeomacro.github.io/GIFT/",
    "bug_reports": "https://github.com/BioGeoMacro/GIFT/issues",
    "repository": "https://cran.r-project.org/package=GIFT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GIFT Access to the Global Inventory of Floras and Traits (GIFT) Retrieving regional plant checklists, species traits and\n  distributions, and environmental data from the Global Inventory of Floras and\n  Traits (GIFT). More information about the GIFT database can be found at\n  <https://gift.uni-goettingen.de/about> and the map of available floras can be\n  visualized at <https://gift.uni-goettingen.de/map>. The API and associated\n  queries can be accessed according the following scheme:\n  <https://gift.uni-goettingen.de/api/extended/index2.0.php?query=env_raster>.  "
  },
  {
    "id": 3661,
    "package_name": "GISINTEGRATION",
    "title": "GIS Integration",
    "description": "Designed to facilitate the preprocessing and linking of GIS (Geographic Information System) databases\n  <https://www.sciencedirect.com/topics/computer-science/gis-database>,\n  the R package 'GISINTEGRATION' offers a robust solution for efficiently preparing  GIS data for advanced \n  spatial analyses. This package excels in simplifying intrica  procedures like data cleaning, normalization, \n  and format conversion, ensuring that the data are optimally primed for precise and thorough analysis.",
    "version": "1.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Hossein Hassani [aut],\n  Leila Marvian Mashhad [aut, cre],\n  Sara Stewart [aut],\n  Steve Macfeelys [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GISINTEGRATION",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GISINTEGRATION GIS Integration Designed to facilitate the preprocessing and linking of GIS (Geographic Information System) databases\n  <https://www.sciencedirect.com/topics/computer-science/gis-database>,\n  the R package 'GISINTEGRATION' offers a robust solution for efficiently preparing  GIS data for advanced \n  spatial analyses. This package excels in simplifying intrica  procedures like data cleaning, normalization, \n  and format conversion, ensuring that the data are optimally primed for precise and thorough analysis.  "
  },
  {
    "id": 3692,
    "package_name": "GNRS",
    "title": "Access the 'Geographic Name Resolution Service'",
    "description": "Provides tools for interacting with the 'geographic name resolution service' ('GNRS') API <https://github.com/ojalaquellueva/gnrs> and associated functionality. The 'GNRS' is a batch application for resolving & standardizing political division names against standard name in the geonames database <http://www.geonames.org/>. The 'GNRS' resolves political division names at three levels: country, state/province and county/parish. Resolution is performed in a series of steps, beginning with direct matching to standard names, followed by direct matching to alternate names in different languages, followed by direct matching to standard codes (such as ISO and FIPS codes). If direct matching fails, the 'GNRS' attempts to match to standard and then alternate names using fuzzy matching, but does not perform fuzzing matching of political division codes. The 'GNRS' works down the political division hierarchy, stopping at the current level if all matches fail. In other words, if a country cannot be matched, the 'GNRS' does not attempt to match state or county.",
    "version": "0.3.4",
    "maintainer": "Brian Maitner <bmaitner@gmail.com>",
    "author": "Brad Boyle [aut],\n  Brian Maitner [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GNRS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GNRS Access the 'Geographic Name Resolution Service' Provides tools for interacting with the 'geographic name resolution service' ('GNRS') API <https://github.com/ojalaquellueva/gnrs> and associated functionality. The 'GNRS' is a batch application for resolving & standardizing political division names against standard name in the geonames database <http://www.geonames.org/>. The 'GNRS' resolves political division names at three levels: country, state/province and county/parish. Resolution is performed in a series of steps, beginning with direct matching to standard names, followed by direct matching to alternate names in different languages, followed by direct matching to standard codes (such as ISO and FIPS codes). If direct matching fails, the 'GNRS' attempts to match to standard and then alternate names using fuzzy matching, but does not perform fuzzing matching of political division codes. The 'GNRS' works down the political division hierarchy, stopping at the current level if all matches fail. In other words, if a country cannot be matched, the 'GNRS' does not attempt to match state or county.  "
  },
  {
    "id": 3805,
    "package_name": "GencoDymo2",
    "title": "Comprehensive Analysis of 'GENCODE' Annotations and Splice Site\nMotifs",
    "description": "A comprehensive suite of helper functions designed to facilitate the analysis of genomic annotations from the 'GENCODE' database <https://www.gencodegenes.org/>, supporting both human and mouse genomes. This toolkit enables users to extract, filter, and analyze a wide range of annotation features including genes, transcripts, exons, and introns across different 'GENCODE' releases. It provides functionality for cross-version comparisons, allowing researchers to systematically track annotation updates, structural changes, and feature-level differences between releases. In addition, the package can generate high-quality FASTA files containing donor and acceptor splice site motifs, which are formatted for direct input into the 'MaxEntScan' tool (Yeo and Burge, 2004 <doi:10.1089/1066527041410418>), enabling accurate calculation of splice site strength scores.",
    "version": "1.0.3",
    "maintainer": "Monah Abou Alezz <aboualezz.monah@hsr.it>",
    "author": "Monah Abou Alezz [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2006-4250>),\n  Lorenzo Salviati [ctb],\n  Roberta Alfieri [ctb],\n  Silvia Bione [ctb]",
    "url": "https://github.com/monahton/GencoDymo2,\nhttps://monahton.github.io/GencoDymo2/",
    "bug_reports": "https://github.com/monahton/GencoDymo2/issues",
    "repository": "https://cran.r-project.org/package=GencoDymo2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GencoDymo2 Comprehensive Analysis of 'GENCODE' Annotations and Splice Site\nMotifs A comprehensive suite of helper functions designed to facilitate the analysis of genomic annotations from the 'GENCODE' database <https://www.gencodegenes.org/>, supporting both human and mouse genomes. This toolkit enables users to extract, filter, and analyze a wide range of annotation features including genes, transcripts, exons, and introns across different 'GENCODE' releases. It provides functionality for cross-version comparisons, allowing researchers to systematically track annotation updates, structural changes, and feature-level differences between releases. In addition, the package can generate high-quality FASTA files containing donor and acceptor splice site motifs, which are formatted for direct input into the 'MaxEntScan' tool (Yeo and Burge, 2004 <doi:10.1089/1066527041410418>), enabling accurate calculation of splice site strength scores.  "
  },
  {
    "id": 3830,
    "package_name": "GeoMongo",
    "title": "Geospatial Queries Using 'PyMongo'",
    "description": "Utilizes methods of the 'PyMongo' 'Python' library to initialize, insert and query 'GeoJson' data (see <https://github.com/mongodb/mongo-python-driver> for more information on 'PyMongo'). Furthermore, it allows the user to validate 'GeoJson' objects and to use the console for 'MongoDB' (bulk) commands. The 'reticulate' package provides the 'R' interface to 'Python' modules, classes and functions.",
    "version": "1.0.3",
    "maintainer": "Lampros Mouselimis <mouselimislampros@gmail.com>",
    "author": "Lampros Mouselimis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8024-1546>)",
    "url": "https://github.com/mlampros/GeoMongo",
    "bug_reports": "https://github.com/mlampros/GeoMongo/issues",
    "repository": "https://cran.r-project.org/package=GeoMongo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeoMongo Geospatial Queries Using 'PyMongo' Utilizes methods of the 'PyMongo' 'Python' library to initialize, insert and query 'GeoJson' data (see <https://github.com/mongodb/mongo-python-driver> for more information on 'PyMongo'). Furthermore, it allows the user to validate 'GeoJson' objects and to use the console for 'MongoDB' (bulk) commands. The 'reticulate' package provides the 'R' interface to 'Python' modules, classes and functions.  "
  },
  {
    "id": 3844,
    "package_name": "GetQuandlData",
    "title": "Fast and Cached Import of Data from 'Quandl' Using the 'json\nAPI'",
    "description": "Imports time series data from the 'Quandl' database <https://data.nasdaq.com/>. The package uses the  'json api' at <https://data.nasdaq.com/search>, local caching ('memoise' package) and the tidy format by default. \n   Also allows queries of databases, allowing the user to see which time series are available for each database id. In short, it is an alternative to package 'Quandl', with faster data importation in the tidy/long format.",
    "version": "1.0.0",
    "maintainer": "Marcelo S. Perlin <marceloperlin@gmail.com>",
    "author": "Marcelo S. Perlin",
    "url": "https://github.com/msperlin/GetQuandlData/",
    "bug_reports": "https://github.com/msperlin/GetQuandlData/issues",
    "repository": "https://cran.r-project.org/package=GetQuandlData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GetQuandlData Fast and Cached Import of Data from 'Quandl' Using the 'json\nAPI' Imports time series data from the 'Quandl' database <https://data.nasdaq.com/>. The package uses the  'json api' at <https://data.nasdaq.com/search>, local caching ('memoise' package) and the tidy format by default. \n   Also allows queries of databases, allowing the user to see which time series are available for each database id. In short, it is an alternative to package 'Quandl', with faster data importation in the tidy/long format.  "
  },
  {
    "id": 3858,
    "package_name": "GitAI",
    "title": "Extracts Knowledge from 'Git' Repositories",
    "description": "Scan multiple 'Git' repositories, pull specified files content and process it with large language models. You can summarize the content in specific way, extract information and data, or find answers to your questions about the repositories. The output can be stored in vector database and used for semantic search or as a part of a RAG (Retrieval Augmented Generation) prompt.",
    "version": "0.1.3",
    "maintainer": "Kamil Wais <kamil.wais@gmail.com>",
    "author": "Kamil Wais [aut, cre],\n  Krystian Igras [aut],\n  Maciej Banas [aut]",
    "url": "https://github.com/r-world-devs/GitAI",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GitAI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GitAI Extracts Knowledge from 'Git' Repositories Scan multiple 'Git' repositories, pull specified files content and process it with large language models. You can summarize the content in specific way, extract information and data, or find answers to your questions about the repositories. The output can be stored in vector database and used for semantic search or as a part of a RAG (Retrieval Augmented Generation) prompt.  "
  },
  {
    "id": 3914,
    "package_name": "HCT",
    "title": "Calculates Significance Criteria and Power for a Single Arm\nTrial",
    "description": "Given a database of previous treatment/placebo estimates, their standard errors and sample sizes,\n      the program calculates a significance criteria and power estimate that takes into account the among\n      trial variation.",
    "version": "0.1.3",
    "maintainer": "David A. Schoenfeld <dschoenfeld@mgh.harvard.edu>",
    "author": "David A. Schoenfeld",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HCT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HCT Calculates Significance Criteria and Power for a Single Arm\nTrial Given a database of previous treatment/placebo estimates, their standard errors and sample sizes,\n      the program calculates a significance criteria and power estimate that takes into account the among\n      trial variation.  "
  },
  {
    "id": 3917,
    "package_name": "HCUPtools",
    "title": "Access and Work with HCUP Resources and Datasets",
    "description": "A comprehensive R package for accessing and working with publicly \n    available and free resources from the Agency for Healthcare Research and Quality \n    (AHRQ) Healthcare Cost and Utilization Project (HCUP). The package provides \n    streamlined access to HCUP's Clinical Classifications Software Refined (CCSR) \n    mapping files and Summary Trend Tables, enabling researchers and analysts to \n    efficiently map ICD-10-CM diagnosis codes and ICD-10-PCS procedure codes to \n    CCSR categories and access HCUP statistical reports. Key features include: \n    direct download from HCUP website, multiple output formats (long/wide/default), \n    cross-classification support, version management, citation generation, and \n    intelligent caching. The package does not redistribute HCUP data files but \n    facilitates direct download from the official HCUP website, ensuring users \n    always have access to the latest versions and maintain compliance with HCUP \n    data use policies. This package only accesses free public tools and reports; \n    it does NOT access HCUP databases (NIS, KID, SID, NEDS, etc.) that require \n    purchase. For more information, see <https://hcup-us.ahrq.gov/>.",
    "version": "1.0.0",
    "maintainer": "Vikrant Dev Rathore <rathore.vikrant@gmail.com>",
    "author": "Vikrant Dev Rathore [aut, cre]",
    "url": "https://github.com/vikrant31/HCUPtools,\nhttps://vikrant31.github.io/HCUPtools/",
    "bug_reports": "https://github.com/vikrant31/HCUPtools/issues",
    "repository": "https://cran.r-project.org/package=HCUPtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HCUPtools Access and Work with HCUP Resources and Datasets A comprehensive R package for accessing and working with publicly \n    available and free resources from the Agency for Healthcare Research and Quality \n    (AHRQ) Healthcare Cost and Utilization Project (HCUP). The package provides \n    streamlined access to HCUP's Clinical Classifications Software Refined (CCSR) \n    mapping files and Summary Trend Tables, enabling researchers and analysts to \n    efficiently map ICD-10-CM diagnosis codes and ICD-10-PCS procedure codes to \n    CCSR categories and access HCUP statistical reports. Key features include: \n    direct download from HCUP website, multiple output formats (long/wide/default), \n    cross-classification support, version management, citation generation, and \n    intelligent caching. The package does not redistribute HCUP data files but \n    facilitates direct download from the official HCUP website, ensuring users \n    always have access to the latest versions and maintain compliance with HCUP \n    data use policies. This package only accesses free public tools and reports; \n    it does NOT access HCUP databases (NIS, KID, SID, NEDS, etc.) that require \n    purchase. For more information, see <https://hcup-us.ahrq.gov/>.  "
  },
  {
    "id": 3959,
    "package_name": "HLAtools",
    "title": "Toolkit for HLA Immunogenomics",
    "description": "A toolkit for the analysis and management of data for genes in the so-called \"Human Leukocyte Antigen\" (HLA) region. Functions extract reference data from the Anthony Nolan HLA Informatics Group/ImmunoGeneTics HLA 'GitHub' repository (ANHIG/IMGTHLA) <https://github.com/ANHIG/IMGTHLA>, validate Genotype List (GL) Strings, convert between UNIFORMAT and GL String Code (GLSC) formats, translate HLA alleles and GLSCs across ImmunoPolymorphism Database (IPD) IMGT/HLA Database release versions, identify differences between pairs of alleles at a locus, generate customized, multi-position sequence alignments, trim and convert allele-names across nomenclature epochs, and extend existing data-analysis methods.",
    "version": "1.6.3",
    "maintainer": "Steven Mack <Steven.Mack@ucsf.edu>",
    "author": "Livia Tran [aut],\n  Ryan Nickens [aut],\n  Leamon Crooms IV [aut],\n  Derek Pappas [aut],\n  Vinh Luu [ctb],\n  Josh Bredeweg [ctb],\n  Steven Mack [aut, cre] (ORCID: <https://orcid.org/0000-0001-9820-9547>)",
    "url": "<https://github.com/sjmack/HLAtools>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HLAtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HLAtools Toolkit for HLA Immunogenomics A toolkit for the analysis and management of data for genes in the so-called \"Human Leukocyte Antigen\" (HLA) region. Functions extract reference data from the Anthony Nolan HLA Informatics Group/ImmunoGeneTics HLA 'GitHub' repository (ANHIG/IMGTHLA) <https://github.com/ANHIG/IMGTHLA>, validate Genotype List (GL) Strings, convert between UNIFORMAT and GL String Code (GLSC) formats, translate HLA alleles and GLSCs across ImmunoPolymorphism Database (IPD) IMGT/HLA Database release versions, identify differences between pairs of alleles at a locus, generate customized, multi-position sequence alignments, trim and convert allele-names across nomenclature epochs, and extend existing data-analysis methods.  "
  },
  {
    "id": 3964,
    "package_name": "HMDHFDplus",
    "title": "Read Human Mortality Database and Human Fertility Database Data\nfrom the Web",
    "description": "Utilities for reading data from the Human Mortality Database (<https://www.mortality.org>), Human Fertility Database (<https://www.humanfertility.org>), and similar databases from the web or locally into an R session as data.frame objects. These are the two most widely used sources of demographic data to study basic demographic change, trends, and develop new demographic methods. Other supported databases at this time include the Human Fertility Collection (<https://www.fertilitydata.org>), The Japanese Mortality Database (<https://www.ipss.go.jp/p-toukei/JMD/index-en.html>), and the Canadian Human Mortality Database (<http://www.bdlc.umontreal.ca/chmd/>). Arguments and data are standardized.",
    "version": "2.0.8",
    "maintainer": "Tim Riffe <tim.riffe@gmail.com>",
    "author": "Tim Riffe [aut, cre],\n  Carl Boe [aut],\n  Jason Hilton [aut],\n  Josh Goldstein [ctb],\n  Stephen Holzman [ctb],\n  Sam Hyun Yoo [ctb]",
    "url": "https://github.com/timriffe/HMDHFDplus",
    "bug_reports": "https://github.com/timriffe/HMDHFDplus/issues",
    "repository": "https://cran.r-project.org/package=HMDHFDplus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HMDHFDplus Read Human Mortality Database and Human Fertility Database Data\nfrom the Web Utilities for reading data from the Human Mortality Database (<https://www.mortality.org>), Human Fertility Database (<https://www.humanfertility.org>), and similar databases from the web or locally into an R session as data.frame objects. These are the two most widely used sources of demographic data to study basic demographic change, trends, and develop new demographic methods. Other supported databases at this time include the Human Fertility Collection (<https://www.fertilitydata.org>), The Japanese Mortality Database (<https://www.ipss.go.jp/p-toukei/JMD/index-en.html>), and the Canadian Human Mortality Database (<http://www.bdlc.umontreal.ca/chmd/>). Arguments and data are standardized.  "
  },
  {
    "id": 4053,
    "package_name": "HurreconR",
    "title": "Models Hurricane Wind Speed, Wind Direction, and Wind Damage",
    "description": "The HURRECON model estimates wind speed, wind direction, enhanced \n    Fujita scale wind damage, and duration of EF0 to EF5 winds as a function \n    of hurricane location and maximum sustained wind speed. Results may be \n    generated for a single site or an entire region. Hurricane track and \n    intensity data may be imported directly from the US National Hurricane \n    Center's HURDAT2 database. For details on the original version of the \n    model written in Borland Pascal, see: Boose, Chamberlin, and Foster (2001) \n    <doi:10.1890/0012-9615(2001)071[0027:LARIOH]2.0.CO;2> and Boose, Serrano, \n    and Foster (2004) <doi:10.1890/02-4057>.",
    "version": "1.2",
    "maintainer": "Emery Boose <boose@fas.harvard.edu>",
    "author": "Emery Boose [aut, cre],\n  President and Fellows of Harvard College [cph]",
    "url": "https://github.com/hurrecon-model/HurreconR",
    "bug_reports": "https://github.com/hurrecon-model/hurreconR/issues",
    "repository": "https://cran.r-project.org/package=HurreconR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HurreconR Models Hurricane Wind Speed, Wind Direction, and Wind Damage The HURRECON model estimates wind speed, wind direction, enhanced \n    Fujita scale wind damage, and duration of EF0 to EF5 winds as a function \n    of hurricane location and maximum sustained wind speed. Results may be \n    generated for a single site or an entire region. Hurricane track and \n    intensity data may be imported directly from the US National Hurricane \n    Center's HURDAT2 database. For details on the original version of the \n    model written in Borland Pascal, see: Boose, Chamberlin, and Foster (2001) \n    <doi:10.1890/0012-9615(2001)071[0027:LARIOH]2.0.CO;2> and Boose, Serrano, \n    and Foster (2004) <doi:10.1890/02-4057>.  "
  },
  {
    "id": 4115,
    "package_name": "IDMIR",
    "title": "Identification of Dysregulated MiRNAs Based on MiRNA-MiRNA\nInteraction Network",
    "description": "A systematic biology tool was developed to identify dysregulated miRNAs via a miRNA-miRNA interaction network. 'IDMIR' first constructed a weighted miRNA interaction network through integrating miRNA-target interaction information, molecular function data from Gene Ontology (GO) database and gene transcriptomic data in specific-disease context, and then, it used a network propagation algorithm on the network to identify significantly dysregulated miRNAs.",
    "version": "0.1.1",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han [aut, cre, cph],\n  Xilong Zhao [aut],\n  Jiashuo Wu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IDMIR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IDMIR Identification of Dysregulated MiRNAs Based on MiRNA-MiRNA\nInteraction Network A systematic biology tool was developed to identify dysregulated miRNAs via a miRNA-miRNA interaction network. 'IDMIR' first constructed a weighted miRNA interaction network through integrating miRNA-target interaction information, molecular function data from Gene Ontology (GO) database and gene transcriptomic data in specific-disease context, and then, it used a network propagation algorithm on the network to identify significantly dysregulated miRNAs.  "
  },
  {
    "id": 4204,
    "package_name": "ISRaD",
    "title": "Tools and Data for the International Soil Radiocarbon Database",
    "description": "This is the central location for data and tools for the development,\n    maintenance, analysis, and deployment of the International Soil Radiocarbon Database\n    (ISRaD). ISRaD was developed as a collaboration between the U.S. Geological Survey\n    Powell Center and the Max Planck Institute for Biogeochemistry. This R package provides\n    tools for accessing and manipulating ISRaD data, compiling local data using the ISRaD\n    data structure, and simple query and reporting functions for ISRaD. For more detailed\n    information visit the ISRaD website at: <https://soilradiocarbon.org/>.",
    "version": "2.5.5",
    "maintainer": "Jeffrey Beem-Miller <jbeem@bgc-jena.mpg.de>",
    "author": "Alison Hoyt [aut],\n  Jeffrey Beem-Miller [aut, cre],\n  Shane Stoner [aut],\n  J. Grey Monroe [aut],\n  Caitlin Hicks-Pries [aut],\n  Paul A. Levine [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ISRaD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ISRaD Tools and Data for the International Soil Radiocarbon Database This is the central location for data and tools for the development,\n    maintenance, analysis, and deployment of the International Soil Radiocarbon Database\n    (ISRaD). ISRaD was developed as a collaboration between the U.S. Geological Survey\n    Powell Center and the Max Planck Institute for Biogeochemistry. This R package provides\n    tools for accessing and manipulating ISRaD data, compiling local data using the ISRaD\n    data structure, and simple query and reporting functions for ISRaD. For more detailed\n    information visit the ISRaD website at: <https://soilradiocarbon.org/>.  "
  },
  {
    "id": 4230,
    "package_name": "IncidencePrevalence",
    "title": "Estimate Incidence and Prevalence using the OMOP Common Data\nModel",
    "description": "Calculate incidence and prevalence using data mapped to the Observational Medical Outcomes Partnership (OMOP) common data model. Incidence and prevalence can be estimated for the total population in a database or for a stratification cohort.",
    "version": "1.2.1",
    "maintainer": "Edward Burn <edward.burn@ndorms.ox.ac.uk>",
    "author": "Edward Burn [aut, cre] (ORCID: <https://orcid.org/0000-0002-9286-1128>),\n  Berta Raventos [aut] (ORCID: <https://orcid.org/0000-0002-4668-2970>),\n  Mart\u00ed Catal\u00e0 [aut] (ORCID: <https://orcid.org/0000-0003-3308-9905>),\n  Mike Du [ctb] (ORCID: <https://orcid.org/0000-0002-9517-8834>),\n  Yuchen Guo [ctb] (ORCID: <https://orcid.org/0000-0002-0847-4855>),\n  Adam Black [ctb] (ORCID: <https://orcid.org/0000-0001-5576-8701>),\n  Ger Inberg [ctb] (ORCID: <https://orcid.org/0000-0001-8993-8748>),\n  Kim Lopez [ctb] (ORCID: <https://orcid.org/0000-0002-8462-8668>)",
    "url": "https://darwin-eu.github.io/IncidencePrevalence/",
    "bug_reports": "https://github.com/darwin-eu/IncidencePrevalence/issues",
    "repository": "https://cran.r-project.org/package=IncidencePrevalence",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IncidencePrevalence Estimate Incidence and Prevalence using the OMOP Common Data\nModel Calculate incidence and prevalence using data mapped to the Observational Medical Outcomes Partnership (OMOP) common data model. Incidence and prevalence can be estimated for the total population in a database or for a stratification cohort.  "
  },
  {
    "id": 4283,
    "package_name": "IsoMemo",
    "title": "Retrieve Data using the 'IsoMemo' API",
    "description": "API wrapper that contains functions to retrieve data from the 'IsoMemo' partnership databases. Web services for API: <https://isomemodb.com/api/v1/iso-data>. ",
    "version": "23.10.1",
    "maintainer": "Jan Abel <jan.abel@inwt-statistics.de>",
    "author": "Jan Abel [cre],\n  Jianyin Roachell [aut],\n  Andreas Neudecker [aut],\n  Antonia Runge [aut],\n  Ricardo Fernandes [aut]",
    "url": "",
    "bug_reports": "https://github.com/Pandora-IsoMemo/isomemo-data/issues",
    "repository": "https://cran.r-project.org/package=IsoMemo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IsoMemo Retrieve Data using the 'IsoMemo' API API wrapper that contains functions to retrieve data from the 'IsoMemo' partnership databases. Web services for API: <https://isomemodb.com/api/v1/iso-data>.   "
  },
  {
    "id": 4403,
    "package_name": "KnowBR",
    "title": "Discriminating Well Surveyed Spatial Units from Exhaustive\nBiodiversity Databases",
    "description": "It uses species accumulation curves and diverse estimators to assess, at the same time, the levels of survey coverage in multiple geographic cells of a size defined by the user or polygons. It also enables the geographical depiction of observed species richness, survey effort and completeness values including a background with administrative areas.",
    "version": "2.2",
    "maintainer": "Castor Guisande Gonzalez <castor@uvigo.es>",
    "author": "Castor Guisande Gonzalez and Jorge M. Lobo",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=KnowBR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KnowBR Discriminating Well Surveyed Spatial Units from Exhaustive\nBiodiversity Databases It uses species accumulation curves and diverse estimators to assess, at the same time, the levels of survey coverage in multiple geographic cells of a size defined by the user or polygons. It also enables the geographical depiction of observed species richness, survey effort and completeness values including a background with administrative areas.  "
  },
  {
    "id": 4420,
    "package_name": "LAGOSNE",
    "title": "Interface to the Lake Multi-Scaled Geospatial and Temporal\nDatabase",
    "description": "Client for programmatic access to the Lake\n    Multi-scaled Geospatial and Temporal database <https://lagoslakes.org>, with functions\n    for accessing lake water quality and ecological context data for the US.",
    "version": "2.0.4",
    "maintainer": "Jemma Stachelek <jemma.stachelek@gmail.com>",
    "author": "Jemma Stachelek [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5924-2464>),\n  Samantha Oliver [aut] (ORCID: <https://orcid.org/0000-0001-5668-1165>),\n  Farzan Masrour [aut]",
    "url": "https://github.com/cont-limno/LAGOSNE,\nhttps://cont-limno.github.io/LAGOSNE/",
    "bug_reports": "https://github.com/cont-limno/LAGOSNE/issues",
    "repository": "https://cran.r-project.org/package=LAGOSNE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LAGOSNE Interface to the Lake Multi-Scaled Geospatial and Temporal\nDatabase Client for programmatic access to the Lake\n    Multi-scaled Geospatial and Temporal database <https://lagoslakes.org>, with functions\n    for accessing lake water quality and ecological context data for the US.  "
  },
  {
    "id": 4428,
    "package_name": "LBDiscover",
    "title": "Literature-Based Discovery Tools for Biomedical Research",
    "description": "A suite of tools for literature-based discovery in biomedical research. \n    Provides functions for retrieving scientific articles from 'PubMed' and \n    other NCBI databases, extracting biomedical entities (diseases, drugs, genes, etc.), \n    building co-occurrence networks, and applying various discovery models \n    including 'ABC', 'AnC', 'LSI', and 'BITOLA'. The package also includes \n    visualization tools for exploring discovered connections.",
    "version": "0.1.0",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre] (ORCID: <https://orcid.org/0000-0002-9979-8272>)",
    "url": "https://github.com/chaoliu-cl/LBDiscover,\nhttps://liu-chao.site/LBDiscover/",
    "bug_reports": "https://github.com/chaoliu-cl/LBDiscover/issues",
    "repository": "https://cran.r-project.org/package=LBDiscover",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LBDiscover Literature-Based Discovery Tools for Biomedical Research A suite of tools for literature-based discovery in biomedical research. \n    Provides functions for retrieving scientific articles from 'PubMed' and \n    other NCBI databases, extracting biomedical entities (diseases, drugs, genes, etc.), \n    building co-occurrence networks, and applying various discovery models \n    including 'ABC', 'AnC', 'LSI', and 'BITOLA'. The package also includes \n    visualization tools for exploring discovered connections.  "
  },
  {
    "id": 4466,
    "package_name": "LLMAgentR",
    "title": "Language Model Agents in R for AI Workflows and Research",
    "description": "Provides modular, graph-based agents powered by large language models (LLMs) for intelligent task execution in R. \n      Supports structured workflows for tasks such as forecasting, data visualization, feature engineering, data wrangling, data cleaning, 'SQL', code generation, weather reporting, and research-driven question answering. \n      Each agent performs iterative reasoning: recommending steps, generating R code, executing, debugging, and explaining results. \n      Includes built-in support for packages such as 'tidymodels', 'modeltime', 'plotly', 'ggplot2', and 'prophet'. Designed for analysts, developers, and teams building intelligent, reproducible AI workflows in R. \n      Compatible with LLM providers such as 'OpenAI', 'Anthropic', 'Groq', and 'Ollama'. Inspired by the Python package 'langagent'.",
    "version": "0.3.0",
    "maintainer": "Kwadwo Daddy Nyame Owusu Boakye <kwadwo.owusuboakye@outlook.com>",
    "author": "Kwadwo Daddy Nyame Owusu Boakye [aut, cre]",
    "url": "https://github.com/knowusuboaky/LLMAgentR,\nhttps://knowusuboaky.github.io/LLMAgentR/",
    "bug_reports": "https://github.com/knowusuboaky/LLMAgentR/issues",
    "repository": "https://cran.r-project.org/package=LLMAgentR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LLMAgentR Language Model Agents in R for AI Workflows and Research Provides modular, graph-based agents powered by large language models (LLMs) for intelligent task execution in R. \n      Supports structured workflows for tasks such as forecasting, data visualization, feature engineering, data wrangling, data cleaning, 'SQL', code generation, weather reporting, and research-driven question answering. \n      Each agent performs iterative reasoning: recommending steps, generating R code, executing, debugging, and explaining results. \n      Includes built-in support for packages such as 'tidymodels', 'modeltime', 'plotly', 'ggplot2', and 'prophet'. Designed for analysts, developers, and teams building intelligent, reproducible AI workflows in R. \n      Compatible with LLM providers such as 'OpenAI', 'Anthropic', 'Groq', and 'Ollama'. Inspired by the Python package 'langagent'.  "
  },
  {
    "id": 4477,
    "package_name": "LMPdata",
    "title": "Easy Import of the EU Labour Market Policy Data",
    "description": "European Commission's Labour Market Policy (LMP) database\n  (<https://webgate.ec.europa.eu/empl/redisstat/databrowser/explore/all/lmp?lang=en&display=card&sort=category>)\n  provides information on labour market interventions, which are government\n  actions to help and support the unemployed and other disadvantaged groups in\n  the transition from unemployment or inactivity to work. It covers the EU\n  countries and Norway. This package provides functions for downloading and\n  importing the LMP data and metadata (codelists).",
    "version": "0.2.0",
    "maintainer": "Aleksander Rutkowski <alek.rutkowski@gmail.com>",
    "author": "Aleksander Rutkowski [aut, cre]",
    "url": "https://github.com/alekrutkowski/LMPdata",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LMPdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LMPdata Easy Import of the EU Labour Market Policy Data European Commission's Labour Market Policy (LMP) database\n  (<https://webgate.ec.europa.eu/empl/redisstat/databrowser/explore/all/lmp?lang=en&display=card&sort=category>)\n  provides information on labour market interventions, which are government\n  actions to help and support the unemployed and other disadvantaged groups in\n  the transition from unemployment or inactivity to work. It covers the EU\n  countries and Norway. This package provides functions for downloading and\n  importing the LMP data and metadata (codelists).  "
  },
  {
    "id": 4541,
    "package_name": "Lahman",
    "title": "Sean 'Lahman' Baseball Database",
    "description": "Provides the tables from the 'Sean Lahman Baseball Database' as\n    a set of R data.frames. It uses the data on pitching, hitting and fielding\n    performance and other tables from 1871 through 2024, as recorded in the 2025\n    version of the database. Documentation examples show how many baseball\n    questions can be investigated.",
    "version": "13.0-0",
    "maintainer": "Chris Dalzell <cdalzell@gmail.com>",
    "author": "Michael Friendly [aut],\n  Chris Dalzell [cre, aut],\n  Martin Monkman [aut],\n  Dennis Murphy [aut],\n  Vanessa Foot [ctb],\n  Justeena Zaki-Azat [ctb],\n  Daniel J Eck [ctb],\n  Sean Lahman [cph]",
    "url": "https://cdalzell.github.io/Lahman/,\nhttps://CRAN.R-project.org/package=Lahman",
    "bug_reports": "https://github.com/cdalzell/Lahman/issues",
    "repository": "https://cran.r-project.org/package=Lahman",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Lahman Sean 'Lahman' Baseball Database Provides the tables from the 'Sean Lahman Baseball Database' as\n    a set of R data.frames. It uses the data on pitching, hitting and fielding\n    performance and other tables from 1871 through 2024, as recorded in the 2025\n    version of the database. Documentation examples show how many baseball\n    questions can be investigated.  "
  },
  {
    "id": 4699,
    "package_name": "MCOE",
    "title": "Creates New Folders and Loads Standard Practices for Monterey\nCounty Office of Education",
    "description": "Basic Setup for Projects in R for Monterey County Office of Education. It contains functions often used in the analysis of education data in the county office including seeing if an item is not in a list, rounding in the manner the general public expects, including logos for districts, switching between district names and their county-district-school codes, accessing the local 'SQL' table and making thematically consistent graphs.",
    "version": "0.6.0",
    "maintainer": "David Dobrowski <ddobrowski@montereycoe.org>",
    "author": "David Dobrowski [aut, cre, cph]",
    "url": "https://github.com/dobrowski/MCOE",
    "bug_reports": "https://github.com/dobrowski/MCOE/issues",
    "repository": "https://cran.r-project.org/package=MCOE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MCOE Creates New Folders and Loads Standard Practices for Monterey\nCounty Office of Education Basic Setup for Projects in R for Monterey County Office of Education. It contains functions often used in the analysis of education data in the county office including seeing if an item is not in a list, rounding in the manner the general public expects, including logos for districts, switching between district names and their county-district-school codes, accessing the local 'SQL' table and making thematically consistent graphs.  "
  },
  {
    "id": 4716,
    "package_name": "MDMAPR",
    "title": "Molecular Detection Mapping and Analysis Platform",
    "description": "Runs a Shiny web application that merges raw 'qPCR' fluorescence data with related \n    metadata to visualize species presence/absence detection patterns and assess data quality. \n    The application calculates threshold values from raw fluorescence data using a method based \n    on the second derivative method, Luu-The et al (2005) <doi:10.2144/05382RR05>,  and utilizes \n    the \u2018chipPCR\u2019 package by R\u00f6diger, Burdukiewicz, & Schierack (2015) <doi:10.1093/bioinformatics/btv205> \n    to calculate Cq values. The application has the ability to connect to a custom developed MySQL \n    database to populate the applications interface. The application allows users to interact with \n    visualizations such as a dynamic map, amplification curves and standard curves, that allow for \n    zooming and/or filtering. It also enables the generation of customized exportable reports based\n    on filtered mapping data. ",
    "version": "0.2.3",
    "maintainer": "Alka Benawra <alkabenawra@rogers.com>",
    "author": "Alka Benawra <alkabenawra@rogers.com>",
    "url": "https://github.com/HannerLab/MDMAPR",
    "bug_reports": "https://github.com/HannerLab/MDMAPR/issues",
    "repository": "https://cran.r-project.org/package=MDMAPR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MDMAPR Molecular Detection Mapping and Analysis Platform Runs a Shiny web application that merges raw 'qPCR' fluorescence data with related \n    metadata to visualize species presence/absence detection patterns and assess data quality. \n    The application calculates threshold values from raw fluorescence data using a method based \n    on the second derivative method, Luu-The et al (2005) <doi:10.2144/05382RR05>,  and utilizes \n    the \u2018chipPCR\u2019 package by R\u00f6diger, Burdukiewicz, & Schierack (2015) <doi:10.1093/bioinformatics/btv205> \n    to calculate Cq values. The application has the ability to connect to a custom developed MySQL \n    database to populate the applications interface. The application allows users to interact with \n    visualizations such as a dynamic map, amplification curves and standard curves, that allow for \n    zooming and/or filtering. It also enables the generation of customized exportable reports based\n    on filtered mapping data.   "
  },
  {
    "id": 4761,
    "package_name": "MHTrajectoryR",
    "title": "Bayesian Model Selection in Logistic Regression for the\nDetection of Adverse Drug Reactions",
    "description": "Spontaneous adverse event reports have a high potential for detecting adverse drug reactions. However, due to their dimension, the analysis of such databases requires statistical methods. We propose to use a logistic regression whose sparsity is viewed as a model selection challenge. Since the model space is huge, a Metropolis-Hastings algorithm carries out the model selection by maximizing the BIC criterion.",
    "version": "1.0.1",
    "maintainer": "Mohammed Sedki <Mohammed.sedki@u-psud.fr>",
    "author": "Matthieu Marbac and Mohammed Sedki",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MHTrajectoryR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MHTrajectoryR Bayesian Model Selection in Logistic Regression for the\nDetection of Adverse Drug Reactions Spontaneous adverse event reports have a high potential for detecting adverse drug reactions. However, due to their dimension, the analysis of such databases requires statistical methods. We propose to use a logistic regression whose sparsity is viewed as a model selection challenge. Since the model space is huge, a Metropolis-Hastings algorithm carries out the model selection by maximizing the BIC criterion.  "
  },
  {
    "id": 4894,
    "package_name": "MSSQL",
    "title": "Tools to Work with Microsoft SQL Server Databases via 'RODBC'",
    "description": "Tools that extend the functionality of the 'RODBC' package to work\n  with Microsoft SQL Server databases. Makes it easier to browse the database\n  and examine individual tables and views.",
    "version": "1.0.1",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Arni Magnusson [aut, cre]",
    "url": "https://github.com/gfcm/MSSQL",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MSSQL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSSQL Tools to Work with Microsoft SQL Server Databases via 'RODBC' Tools that extend the functionality of the 'RODBC' package to work\n  with Microsoft SQL Server databases. Makes it easier to browse the database\n  and examine individual tables and views.  "
  },
  {
    "id": 4954,
    "package_name": "MantaID",
    "title": "A Machine-Learning Based Tool to Automate the Identification of\nBiological Database IDs",
    "description": "The number of biological databases is growing rapidly, but different databases use different IDs to refer to the same biological entity. The inconsistency in IDs impedes the integration of various types of biological data. To resolve the problem, we developed 'MantaID', a data-driven, machine-learning based approach that automates identifying IDs on a large scale. The 'MantaID' model's prediction accuracy was proven to be 99%, and it correctly and effectively predicted 100,000 ID entries within two minutes. 'MantaID' supports the discovery and exploitation of ID patterns from large quantities of databases. (e.g., up to 542 biological databases). An easy-to-use freely available open-source software R package, a user-friendly web application, and API were also developed for 'MantaID' to improve applicability. To our knowledge, 'MantaID' is the first tool that enables an automatic, quick, accurate, and comprehensive identification of large quantities of IDs, and can therefore be used as a starting point to facilitate the complex assimilation and aggregation of biological data across diverse databases.",
    "version": "1.0.4",
    "maintainer": "Zhengpeng Zeng <molaison@foxmail.com>",
    "author": "Zhengpeng Zeng [aut, cre, ctb] (ORCID:\n    <https://orcid.org/0000-0001-7919-209X>),\n  Longfei Mao [aut, cph] (ORCID: <https://orcid.org/0000-0003-0759-0501>),\n  Feng Yu [aut] (ORCID: <https://orcid.org/0000-0002-5221-281X>),\n  Jiamin Hu [ctb] (ORCID: <https://orcid.org/0000-0003-3030-2117>),\n  Xiting Wang [ctb] (ORCID: <https://orcid.org/0009-0009-5235-0006>)",
    "url": "https://molaison.github.io/MantaID/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MantaID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MantaID A Machine-Learning Based Tool to Automate the Identification of\nBiological Database IDs The number of biological databases is growing rapidly, but different databases use different IDs to refer to the same biological entity. The inconsistency in IDs impedes the integration of various types of biological data. To resolve the problem, we developed 'MantaID', a data-driven, machine-learning based approach that automates identifying IDs on a large scale. The 'MantaID' model's prediction accuracy was proven to be 99%, and it correctly and effectively predicted 100,000 ID entries within two minutes. 'MantaID' supports the discovery and exploitation of ID patterns from large quantities of databases. (e.g., up to 542 biological databases). An easy-to-use freely available open-source software R package, a user-friendly web application, and API were also developed for 'MantaID' to improve applicability. To our knowledge, 'MantaID' is the first tool that enables an automatic, quick, accurate, and comprehensive identification of large quantities of IDs, and can therefore be used as a starting point to facilitate the complex assimilation and aggregation of biological data across diverse databases.  "
  },
  {
    "id": 4957,
    "package_name": "Map2NCBI",
    "title": "Mapping Markers to the Nearest Genomic Feature",
    "description": "Allows the user to generate a list of features (gene, pseudo, RNA,\n    CDS, and/or UTR) directly from NCBI database for any species with a current \n    build available. Option to save downloaded and formatted files is available, \n    and the user can prioritize the feature list based on type and assembly builds \n    present in the current build used. The user can then use the list of features \n    generated or provide a list to map a set of markers (designed for SNP markers \n    with a single base pair position available) to the closest feature based on \n    the map build. This function does require map positions of the markers to be \n    provided and the positions should be based on the build being queried through \n    NCBI. ",
    "version": "1.5",
    "maintainer": "Lauren L. Hulsman Hanna <Lauren.Hanna@ndsu.edu>",
    "author": "Lauren L. Hulsman Hanna [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8392-887X>),\n  David G. Riley [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Map2NCBI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Map2NCBI Mapping Markers to the Nearest Genomic Feature Allows the user to generate a list of features (gene, pseudo, RNA,\n    CDS, and/or UTR) directly from NCBI database for any species with a current \n    build available. Option to save downloaded and formatted files is available, \n    and the user can prioritize the feature list based on type and assembly builds \n    present in the current build used. The user can then use the list of features \n    generated or provide a list to map a set of markers (designed for SNP markers \n    with a single base pair position available) to the closest feature based on \n    the map build. This function does require map positions of the markers to be \n    provided and the positions should be based on the build being queried through \n    NCBI.   "
  },
  {
    "id": 5006,
    "package_name": "MedxR",
    "title": "Access Drug Regulatory Data via FDA and Health Canada APIs",
    "description": "Provides functions to access drug regulatory data from public RESTful APIs \n    including the 'FDA Open API' and the 'Health Canada Drug Product Database API', \n    retrieving real-time or historical information on drug approvals, adverse events, \n    recalls, and product details. Additionally, the package includes a curated collection \n    of open datasets focused on drugs, pharmaceuticals, treatments, and clinical studies. \n    These datasets cover diverse topics such as treatment dosages, pharmacological studies, \n    placebo effects, drug reactions, misuses of pain relievers, and vaccine effectiveness. \n    The package supports reproducible research and teaching in pharmacology, medicine, \n    and healthcare by integrating reliable international APIs and structured datasets \n    from public, academic, and government sources. \n    For more information on the APIs, see: \n    'FDA API' <https://open.fda.gov/apis/> and \n    'Health Canada API' <https://health-products.canada.ca/api/documentation/dpd-documentation-en.html>.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/medxr,\nhttps://lightbluetitan.github.io/medxr/",
    "bug_reports": "https://github.com/lightbluetitan/medxr/issues",
    "repository": "https://cran.r-project.org/package=MedxR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MedxR Access Drug Regulatory Data via FDA and Health Canada APIs Provides functions to access drug regulatory data from public RESTful APIs \n    including the 'FDA Open API' and the 'Health Canada Drug Product Database API', \n    retrieving real-time or historical information on drug approvals, adverse events, \n    recalls, and product details. Additionally, the package includes a curated collection \n    of open datasets focused on drugs, pharmaceuticals, treatments, and clinical studies. \n    These datasets cover diverse topics such as treatment dosages, pharmacological studies, \n    placebo effects, drug reactions, misuses of pain relievers, and vaccine effectiveness. \n    The package supports reproducible research and teaching in pharmacology, medicine, \n    and healthcare by integrating reliable international APIs and structured datasets \n    from public, academic, and government sources. \n    For more information on the APIs, see: \n    'FDA API' <https://open.fda.gov/apis/> and \n    'Health Canada API' <https://health-products.canada.ca/api/documentation/dpd-documentation-en.html>.  "
  },
  {
    "id": 5046,
    "package_name": "MiRSEA",
    "title": "'MicroRNA' Set Enrichment Analysis",
    "description": "The tools for 'MicroRNA Set Enrichment Analysis' can identify risk pathways(or prior gene sets) regulated by microRNA set in the context of microRNA expression data. (1) This package constructs a correlation profile of microRNA and pathways by the hypergeometric statistic test. The gene sets of pathways derived from the three public databases (Kyoto Encyclopedia of Genes and Genomes ('KEGG'); 'Reactome'; 'Biocarta') and the target gene sets of microRNA are provided by four databases('TarBaseV6.0'; 'mir2Disease'; 'miRecords'; 'miRTarBase';). (2) This package can quantify the change of correlation between microRNA for each pathway(or prior gene set) based on a microRNA expression data with cases and controls. (3) This package uses the weighted Kolmogorov-Smirnov statistic to calculate an enrichment score (ES) of a microRNA set that co-regulate to a pathway , which reflects the degree to which a given pathway is associated with the specific phenotype. (4) This package can provide the visualization of the results.",
    "version": "1.1.1",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Junwei Han, Siyao Liu ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MiRSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MiRSEA 'MicroRNA' Set Enrichment Analysis The tools for 'MicroRNA Set Enrichment Analysis' can identify risk pathways(or prior gene sets) regulated by microRNA set in the context of microRNA expression data. (1) This package constructs a correlation profile of microRNA and pathways by the hypergeometric statistic test. The gene sets of pathways derived from the three public databases (Kyoto Encyclopedia of Genes and Genomes ('KEGG'); 'Reactome'; 'Biocarta') and the target gene sets of microRNA are provided by four databases('TarBaseV6.0'; 'mir2Disease'; 'miRecords'; 'miRTarBase';). (2) This package can quantify the change of correlation between microRNA for each pathway(or prior gene set) based on a microRNA expression data with cases and controls. (3) This package uses the weighted Kolmogorov-Smirnov statistic to calculate an enrichment score (ES) of a microRNA set that co-regulate to a pathway , which reflects the degree to which a given pathway is associated with the specific phenotype. (4) This package can provide the visualization of the results.  "
  },
  {
    "id": 5125,
    "package_name": "MortalityLaws",
    "title": "Parametric Mortality Models, Life Tables and HMD",
    "description": "Fit the most popular human mortality 'laws', and construct \n  full and abridge life tables given various input indices. A mortality\n  law is a parametric function that describes the dying-out process of \n  individuals in a population during a significant portion of their \n  life spans. For a comprehensive review of the most important mortality \n  laws see Tabeau (2001) <doi:10.1007/0-306-47562-6_1>. \n  Practical functions for downloading data from various human mortality \n  databases are provided as well.  ",
    "version": "2.1.3",
    "maintainer": "Marius D. Pascariu <mpascariu@outlook.com>",
    "author": "Marius D. Pascariu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2568-6489>),\n  Vladimir Canudas-Romo [ctb]",
    "url": "https://github.com/mpascariu/MortalityLaws",
    "bug_reports": "https://github.com/mpascariu/MortalityLaws/issues",
    "repository": "https://cran.r-project.org/package=MortalityLaws",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MortalityLaws Parametric Mortality Models, Life Tables and HMD Fit the most popular human mortality 'laws', and construct \n  full and abridge life tables given various input indices. A mortality\n  law is a parametric function that describes the dying-out process of \n  individuals in a population during a significant portion of their \n  life spans. For a comprehensive review of the most important mortality \n  laws see Tabeau (2001) <doi:10.1007/0-306-47562-6_1>. \n  Practical functions for downloading data from various human mortality \n  databases are provided as well.    "
  },
  {
    "id": 5290,
    "package_name": "NasdaqDataLink",
    "title": "API Wrapper for Nasdaq Data Link",
    "description": "Functions for interacting directly with the Nasdaq Data Link API to offer data in a number of formats usable in R, downloading a zip with all data from a Nasdaq Data Link database, and the ability to search. This R package uses the Nasdaq Data Link API. For more information go to <https://docs.data.nasdaq.com/>. For more help on the package itself go to <https://data.nasdaq.com/tools/r>.",
    "version": "1.0.0",
    "maintainer": "Jamie Couture <jamie.couture@nasdaq.com>",
    "author": "Jamie Couture [cre],\n  Eric Vautour [aut],\n  Nasdaq Data Link. [cph]",
    "url": "https://github.com/nasdaq/data-link-r",
    "bug_reports": "https://github.com/nasdaq/data-link-r/issues",
    "repository": "https://cran.r-project.org/package=NasdaqDataLink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NasdaqDataLink API Wrapper for Nasdaq Data Link Functions for interacting directly with the Nasdaq Data Link API to offer data in a number of formats usable in R, downloading a zip with all data from a Nasdaq Data Link database, and the ability to search. This R package uses the Nasdaq Data Link API. For more information go to <https://docs.data.nasdaq.com/>. For more help on the package itself go to <https://data.nasdaq.com/tools/r>.  "
  },
  {
    "id": 5331,
    "package_name": "NeuroDataSets",
    "title": "A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets",
    "description": "Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.",
    "version": "0.3.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/neurodatasets,\nhttps://lightbluetitan.github.io/neurodatasets/",
    "bug_reports": "https://github.com/lightbluetitan/neurodatasets/issues",
    "repository": "https://cran.r-project.org/package=NeuroDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NeuroDataSets A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.  "
  },
  {
    "id": 5362,
    "package_name": "NutrienTrackeR",
    "title": "Food Composition Information and Dietary Assessment",
    "description": "Provides a tool set for food information and dietary assessment. It \n    uses food composition data from several reference databases, including: 'USDA' (United States), \n    'CIQUAL' (France), 'BEDCA' (Spain), 'CNF' (Canada) and 'STFCJ' (Japan). 'NutrienTrackeR' calculates \n    the intake levels for both macronutrient and micronutrients, and compares them with the recommended \n    dietary allowances (RDA). It includes a number of visualization tools, such as time series \n    plots of nutrient intake, and pie-charts showing the main foods contributing to the intake \n    level of a given nutrient. A shiny app exposing the main functionalities of the package is also \n    provided.",
    "version": "1.4.0",
    "maintainer": "Rafael Ayala <rafaelayalahernandez@gmail.com>",
    "author": "Andrea Rodriguez-Martinez [aut],\n  Rafael Ayala [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9332-4623>),\n  Mark Balchunas [aut],\n  Pablo Hernandez [aut] (ORCID: <https://orcid.org/0009-0000-9279-6744>),\n  Lara Sell\u00e9s Vidal [aut] (ORCID:\n    <https://orcid.org/0000-0003-2537-6824>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NutrienTrackeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NutrienTrackeR Food Composition Information and Dietary Assessment Provides a tool set for food information and dietary assessment. It \n    uses food composition data from several reference databases, including: 'USDA' (United States), \n    'CIQUAL' (France), 'BEDCA' (Spain), 'CNF' (Canada) and 'STFCJ' (Japan). 'NutrienTrackeR' calculates \n    the intake levels for both macronutrient and micronutrients, and compares them with the recommended \n    dietary allowances (RDA). It includes a number of visualization tools, such as time series \n    plots of nutrient intake, and pie-charts showing the main foods contributing to the intake \n    level of a given nutrient. A shiny app exposing the main functionalities of the package is also \n    provided.  "
  },
  {
    "id": 5416,
    "package_name": "OTrecod",
    "title": "Data Fusion using Optimal Transportation Theory",
    "description": "In the context of data fusion, the package provides a set of functions dedicated to the solving of 'recoding problems' using optimal transportation theory (Gares, Guernec, Savy (2019) <doi:10.1515/ijb-2018-0106> and Gares, Omer (2020) <doi:10.1080/01621459.2020.1775615>). From two databases with no overlapping part except a subset of shared variables, the functions of the package assist users until obtaining a unique synthetic database, where the missing information is fully completed.",
    "version": "0.1.2",
    "maintainer": "Gregory Guernec <otrecod.pkg@gmail.com>",
    "author": "Gregory Guernec [aut, cre],\n  Valerie Gares [aut],\n  Pierre Navaro [ctb],\n  Jeremy Omer [ctb],\n  Philippe Saint-Pierre [ctb],\n  Nicolas Savy [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OTrecod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OTrecod Data Fusion using Optimal Transportation Theory In the context of data fusion, the package provides a set of functions dedicated to the solving of 'recoding problems' using optimal transportation theory (Gares, Guernec, Savy (2019) <doi:10.1515/ijb-2018-0106> and Gares, Omer (2020) <doi:10.1080/01621459.2020.1775615>). From two databases with no overlapping part except a subset of shared variables, the functions of the package assist users until obtaining a unique synthetic database, where the missing information is fully completed.  "
  },
  {
    "id": 5470,
    "package_name": "OpenRange",
    "title": "Code to Access Open Access Species Range Maps",
    "description": "Allows access to a proof-of-concept database containing Open Access species range models and relevant metadata. Access to the database is via both 'PostgreSQL' connection and API <https://github.com/EnquistLab/Biendata-Frontend>, allowing diverse use-cases.",
    "version": "0.0.1",
    "maintainer": "Brian Maitner <bmaitner@usf.edu>",
    "author": "Brian Maitner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2118-9880>),\n  Cory Merow [aut],\n  Brad Boyle [aut],\n  Xiao Feng [aut],\n  Rethvick Sriram Yugendra Babu [aut],\n  Brian Enquist [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OpenRange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OpenRange Code to Access Open Access Species Range Maps Allows access to a proof-of-concept database containing Open Access species range models and relevant metadata. Access to the database is via both 'PostgreSQL' connection and API <https://github.com/EnquistLab/Biendata-Frontend>, allowing diverse use-cases.  "
  },
  {
    "id": 5501,
    "package_name": "OryzaProbe",
    "title": "Rice Microarray Probe ID Conversion, from Probe ID to RAP-DB ID",
    "description": "Microarray probe ID is not convenient for further enrichment analysis and target gene selection.\n    The package is created for the rice microarray probe ID conversion.\n    This package can convert microarray probe ID from GPL6864 <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL6864>, GPL8852 <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL8852>, and GPL2025 <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL2025> platforms to RAP-DB ID. RAP-DB \"The Rice Annotation Project Database\" <https://rapdb.dna.affrc.go.jp> is a well-known database for rice Oryza sativa, and the gene ID in this database is widely used in many areas related to rice research.\n    For multiple probes representing a single gene, This package can merge them by taking the mean, max, or min value of these probes.\n    Or we can keep multiple probes by appending sequence numbers to duplicate the RAP-DB ID.",
    "version": "0.1.0",
    "maintainer": "Youming Liu <lym@g.ecc.u-tokyo.ac.jp>",
    "author": "Youming Liu [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OryzaProbe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OryzaProbe Rice Microarray Probe ID Conversion, from Probe ID to RAP-DB ID Microarray probe ID is not convenient for further enrichment analysis and target gene selection.\n    The package is created for the rice microarray probe ID conversion.\n    This package can convert microarray probe ID from GPL6864 <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL6864>, GPL8852 <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL8852>, and GPL2025 <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL2025> platforms to RAP-DB ID. RAP-DB \"The Rice Annotation Project Database\" <https://rapdb.dna.affrc.go.jp> is a well-known database for rice Oryza sativa, and the gene ID in this database is widely used in many areas related to rice research.\n    For multiple probes representing a single gene, This package can merge them by taking the mean, max, or min value of these probes.\n    Or we can keep multiple probes by appending sequence numbers to duplicate the RAP-DB ID.  "
  },
  {
    "id": 5578,
    "package_name": "PEIMAN2",
    "title": "Post-Translational Modification Enrichment, Integration, and\nMatching Analysis",
    "description": "Functions and mined database from 'UniProt' focusing on post-translational modifications to do single enrichment analysis (SEA) and protein set enrichment analysis (PSEA). Payman Nickchi, Uladzislau Vadadokhau, Mehdi Mirzaie, Marc Baumann, Amir Ata Saei, Mohieddin Jafari (2025) <doi:10.1002/pmic.202400238>.",
    "version": "1.0.1",
    "maintainer": "Payman Nickchi <payman.nickchi@gmail.com>",
    "author": "Mohieddin Jafari [aut],\n  Payman Nickchi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PEIMAN2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PEIMAN2 Post-Translational Modification Enrichment, Integration, and\nMatching Analysis Functions and mined database from 'UniProt' focusing on post-translational modifications to do single enrichment analysis (SEA) and protein set enrichment analysis (PSEA). Payman Nickchi, Uladzislau Vadadokhau, Mehdi Mirzaie, Marc Baumann, Amir Ata Saei, Mohieddin Jafari (2025) <doi:10.1002/pmic.202400238>.  "
  },
  {
    "id": 5696,
    "package_name": "PSF",
    "title": "Forecasting of Univariate Time Series Using the Pattern\nSequence-Based Forecasting (PSF) Algorithm",
    "description": "Pattern Sequence Based Forecasting (PSF) takes univariate\n    time series data as input and assist to forecast its future values.\n    This algorithm forecasts the behavior of time series\n    based on similarity of pattern sequences. Initially, clustering is done with the\n    labeling of samples from database. The labels associated with samples are then\n    used for forecasting the future behaviour of time series data. The further\n    technical details and references regarding PSF are discussed in Vignette.",
    "version": "0.5",
    "maintainer": "Neeraj Bokde <neerajdhanraj@gmail.com>",
    "author": "Neeraj Bokde, Gualberto Asencio-Cortes and Francisco Martinez-Alvarez",
    "url": "https://www.neerajbokde.in/viggnette/2021-10-13-PSF/",
    "bug_reports": "https://github.com/neerajdhanraj/PSF/issues",
    "repository": "https://cran.r-project.org/package=PSF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PSF Forecasting of Univariate Time Series Using the Pattern\nSequence-Based Forecasting (PSF) Algorithm Pattern Sequence Based Forecasting (PSF) takes univariate\n    time series data as input and assist to forecast its future values.\n    This algorithm forecasts the behavior of time series\n    based on similarity of pattern sequences. Initially, clustering is done with the\n    labeling of samples from database. The labels associated with samples are then\n    used for forecasting the future behaviour of time series data. The further\n    technical details and references regarding PSF are discussed in Vignette.  "
  },
  {
    "id": 5779,
    "package_name": "PeakError",
    "title": "Compute the Label Error of Peak Calls",
    "description": "Chromatin immunoprecipitation DNA sequencing results in genomic\n    tracks that show enriched regions or peaks where proteins are bound.\n    This package implements fast C code that computes the true and false\n    positives with respect to a database of annotated region labels.",
    "version": "2023.9.4",
    "maintainer": "Toby Dylan Hocking <toby.hocking@r-project.org>",
    "author": "Toby Dylan Hocking",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PeakError",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PeakError Compute the Label Error of Peak Calls Chromatin immunoprecipitation DNA sequencing results in genomic\n    tracks that show enriched regions or peaks where proteins are bound.\n    This package implements fast C code that computes the true and false\n    positives with respect to a database of annotated region labels.  "
  },
  {
    "id": 5806,
    "package_name": "PesticideLoadIndicator",
    "title": "Computes Danish Pesticide Load Indicator",
    "description": "Computes the Danish Pesticide Load Indicator as described in Kudsk et al. (2018) <doi:10.1016/j.landusepol.2017.11.010> and Moehring et al. (2019) <doi:10.1016/j.scitotenv.2018.07.287> for pesticide use data. Additionally offers the possibility to directly link pesticide use data to pesticide properties given access to the Pesticide properties database (Lewis et al., 2016) <doi:10.1080/10807039.2015.1133242>. ",
    "version": "1.3.1",
    "maintainer": "Niklas Moehring <niklas.moehring@mtec.ethz.ch>",
    "author": "Niklas Moehring [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0292-4402>),\n  Leonie Vidensky [aut],\n  Robert Finger [aut],\n  Per Kudsk [aut],\n  Lise Nistrup J\u00f8rgensen [aut],\n  Jens Erik \u00d8rum [aut],\n  Uwe Schmitt [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PesticideLoadIndicator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PesticideLoadIndicator Computes Danish Pesticide Load Indicator Computes the Danish Pesticide Load Indicator as described in Kudsk et al. (2018) <doi:10.1016/j.landusepol.2017.11.010> and Moehring et al. (2019) <doi:10.1016/j.scitotenv.2018.07.287> for pesticide use data. Additionally offers the possibility to directly link pesticide use data to pesticide properties given access to the Pesticide properties database (Lewis et al., 2016) <doi:10.1080/10807039.2015.1133242>.   "
  },
  {
    "id": 5808,
    "package_name": "PetfindeR",
    "title": "'Petfinder' API Wrapper",
    "description": "\n  Wrapper of the 'Petfinder API' <https://www.petfinder.com/developers/v2/docs/> that implements \n  methods for interacting with and extracting data from the 'Petfinder' database. The 'Petfinder \n  REST API' allows access to the 'Petfinder' database, one of the largest online databases of \n  adoptable animals and animal welfare organizations across North America.",
    "version": "2.1.0",
    "maintainer": "Aaron Schlegel <aaron@aaronschlegel.me>",
    "author": "Aaron Schlegel",
    "url": "https://github.com/aschleg/PetfindeR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PetfindeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PetfindeR 'Petfinder' API Wrapper \n  Wrapper of the 'Petfinder API' <https://www.petfinder.com/developers/v2/docs/> that implements \n  methods for interacting with and extracting data from the 'Petfinder' database. The 'Petfinder \n  REST API' allows access to the 'Petfinder' database, one of the largest online databases of \n  adoptable animals and animal welfare organizations across North America.  "
  },
  {
    "id": 5819,
    "package_name": "PhenotypeR",
    "title": "Assess Study Cohorts Using a Common Data Model",
    "description": "Phenotype study cohorts in data mapped to the \n    Observational Medical Outcomes Partnership Common Data Model. Diagnostics \n    are run at the database, code list, cohort, and population level to assess \n    whether study cohorts are ready for research.",
    "version": "0.2.0",
    "maintainer": "Edward Burn <edward.burn@ndorms.ox.ac.uk>",
    "author": "Edward Burn [aut, cre] (ORCID: <https://orcid.org/0000-0002-9286-1128>),\n  Marti Catala [aut] (ORCID: <https://orcid.org/0000-0003-3308-9905>),\n  Xihang Chen [aut] (ORCID: <https://orcid.org/0009-0001-8112-8959>),\n  Marta Alcalde-Herraiz [aut] (ORCID:\n    <https://orcid.org/0009-0002-4405-1814>),\n  Nuria Mercade-Besora [aut] (ORCID:\n    <https://orcid.org/0009-0006-7948-3747>),\n  Albert Prats-Uribe [aut] (ORCID:\n    <https://orcid.org/0000-0003-1202-9153>)",
    "url": "https://ohdsi.github.io/PhenotypeR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PhenotypeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PhenotypeR Assess Study Cohorts Using a Common Data Model Phenotype study cohorts in data mapped to the \n    Observational Medical Outcomes Partnership Common Data Model. Diagnostics \n    are run at the database, code list, cohort, and population level to assess \n    whether study cohorts are ready for research.  "
  },
  {
    "id": 5843,
    "package_name": "Plasmidprofiler",
    "title": "Visualization of Plasmid Profile Results",
    "description": "Contains functions developed to combine the results of querying a plasmid database using\n    short-read sequence typing with the results of a blast analysis against the query results.",
    "version": "0.1.6",
    "maintainer": "Adrian Zetner <adrian.zetner@phac-aspc.gc.ca>",
    "author": "Adrian Zetner [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Plasmidprofiler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Plasmidprofiler Visualization of Plasmid Profile Results Contains functions developed to combine the results of querying a plasmid database using\n    short-read sequence typing with the results of a blast analysis against the query results.  "
  },
  {
    "id": 5962,
    "package_name": "PubChemR",
    "title": "Interface to the 'PubChem' Database for Chemical Data Retrieval",
    "description": "Provides an interface to the 'PubChem' database via the PUG REST <https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest> and \n            PUG View <https://pubchem.ncbi.nlm.nih.gov/docs/pug-view> services. This package allows users to automatically \n            access chemical and biological data from 'PubChem', including compounds, substances, assays, and various other data types. \n            Functions are available to retrieve data in different formats, perform searches, and access detailed annotations.",
    "version": "2.1.8",
    "maintainer": "Selcuk Korkmaz <selcukorkmaz@gmail.com>",
    "author": "Selcuk Korkmaz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4632-6850>),\n  Bilge Eren Yamasan [aut] (ORCID:\n    <https://orcid.org/0000-0002-6525-2503>),\n  Dincer Goksuluk [aut] (ORCID: <https://orcid.org/0000-0002-2752-7668>)",
    "url": "https://selcukorkmaz.github.io/pubchemr-tutorial/,\nhttps://github.com/selcukorkmaz/PubChemR",
    "bug_reports": "https://github.com/selcukorkmaz/PubChemR/issues",
    "repository": "https://cran.r-project.org/package=PubChemR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PubChemR Interface to the 'PubChem' Database for Chemical Data Retrieval Provides an interface to the 'PubChem' database via the PUG REST <https://pubchem.ncbi.nlm.nih.gov/docs/pug-rest> and \n            PUG View <https://pubchem.ncbi.nlm.nih.gov/docs/pug-view> services. This package allows users to automatically \n            access chemical and biological data from 'PubChem', including compounds, substances, assays, and various other data types. \n            Functions are available to retrieve data in different formats, perform searches, and access detailed annotations.  "
  },
  {
    "id": 5982,
    "package_name": "QBMS",
    "title": "Query the Breeding Management System(s)",
    "description": "This R package assists breeders in linking data systems with their analytic pipelines, \n    a crucial step in digitizing breeding processes. It supports querying and retrieving \n    phenotypic and genotypic data from systems like 'EBS' <https://ebs.excellenceinbreeding.org/>, \n    'BMS' <https://bmspro.io>, 'BreedBase' <https://breedbase.org>,\n    'GIGWA' <https://github.com/SouthGreenPlatform/Gigwa2> (using 'BrAPI' <https://brapi.org> calls),\n    , and 'Germinate' <https://germinateplatform.github.io/get-germinate/>. \n    Extra helper functions support environmental data sources, including \n    'TerraClimate' <https://www.climatologylab.org/terraclimate.html> and 'FAO' \n    'HWSDv2' <https://gaez.fao.org/pages/hwsd> soil database. ",
    "version": "2.0.0",
    "maintainer": "Khaled Al-Shamaa <k.el-shamaa@cgiar.org>",
    "author": "Khaled Al-Shamaa [aut, cre],\n  Mariano Omar Crimi [ctb],\n  Zakaria Kehel [ctb],\n  Johan Aparicio [ctb],\n  ICARDA [cph]",
    "url": "https://icarda.github.io/QBMS/",
    "bug_reports": "https://github.com/icarda/QBMS/issues",
    "repository": "https://cran.r-project.org/package=QBMS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QBMS Query the Breeding Management System(s) This R package assists breeders in linking data systems with their analytic pipelines, \n    a crucial step in digitizing breeding processes. It supports querying and retrieving \n    phenotypic and genotypic data from systems like 'EBS' <https://ebs.excellenceinbreeding.org/>, \n    'BMS' <https://bmspro.io>, 'BreedBase' <https://breedbase.org>,\n    'GIGWA' <https://github.com/SouthGreenPlatform/Gigwa2> (using 'BrAPI' <https://brapi.org> calls),\n    , and 'Germinate' <https://germinateplatform.github.io/get-germinate/>. \n    Extra helper functions support environmental data sources, including \n    'TerraClimate' <https://www.climatologylab.org/terraclimate.html> and 'FAO' \n    'HWSDv2' <https://gaez.fao.org/pages/hwsd> soil database.   "
  },
  {
    "id": 6034,
    "package_name": "Quandl",
    "title": "API Wrapper for Quandl.com",
    "description": "Functions for interacting directly with the Quandl API to offer data in a number of formats usable in R, downloading a zip with all data from a Quandl database, and the ability to search. This R package uses the Quandl API. For more information go to <https://docs.quandl.com>. For more help on the package itself go to <https://www.quandl.com/tools/r>.",
    "version": "2.11.0",
    "maintainer": "Dave Dotson <dave@quandl.com>",
    "author": "Dave Dotson [cre],\n  Raymond McTaggart [aut],\n  Gergely Daroczi [aut],\n  Clement Leung [aut],\n  Quandl Inc. [cph]",
    "url": "https://github.com/quandl/quandl-r",
    "bug_reports": "https://github.com/quandl/quandl-r/issues",
    "repository": "https://cran.r-project.org/package=Quandl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Quandl API Wrapper for Quandl.com Functions for interacting directly with the Quandl API to offer data in a number of formats usable in R, downloading a zip with all data from a Quandl database, and the ability to search. This R package uses the Quandl API. For more information go to <https://docs.quandl.com>. For more help on the package itself go to <https://www.quandl.com/tools/r>.  "
  },
  {
    "id": 6075,
    "package_name": "R4CouchDB",
    "title": "A R Convenience Layer for CouchDB 2.0",
    "description": "Provides a collection of functions for basic\n    database and document management operations such as add, get, list access\n    or delete. Every cdbFunction() gets and returns a list() containing the\n    connection setup. Such a list can be generated by cdbIni().",
    "version": "0.7.5",
    "maintainer": "Thomas Bock <thsteinbock@web.de>",
    "author": "Thomas Bock",
    "url": "https://github.com/wactbprot/R4CouchDB",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=R4CouchDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "R4CouchDB A R Convenience Layer for CouchDB 2.0 Provides a collection of functions for basic\n    database and document management operations such as add, get, list access\n    or delete. Every cdbFunction() gets and returns a list() containing the\n    connection setup. Such a list can be generated by cdbIni().  "
  },
  {
    "id": 6106,
    "package_name": "RAthena",
    "title": "Connect to 'AWS Athena' using 'Boto3' ('DBI' Interface)",
    "description": "Designed to be compatible with the R package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this 'Python' 'Boto3' Software Development Kit ('SDK')\n    <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html> is used as a driver.",
    "version": "2.6.3",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "Dyfan Jones [aut, cre]",
    "url": "https://dyfanjones.github.io/RAthena/,\nhttps://github.com/DyfanJones/RAthena",
    "bug_reports": "https://github.com/DyfanJones/RAthena/issues",
    "repository": "https://cran.r-project.org/package=RAthena",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RAthena Connect to 'AWS Athena' using 'Boto3' ('DBI' Interface) Designed to be compatible with the R package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this 'Python' 'Boto3' Software Development Kit ('SDK')\n    <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html> is used as a driver.  "
  },
  {
    "id": 6112,
    "package_name": "RBaseX",
    "title": "'BaseX' Client",
    "description": "'BaseX' <https://basex.org> is a XML database engine and a compliant 'XQuery 3.1' processor with full support of 'W3C Update Facility'. This package is a full client-implementation of the client/server protocol for 'BaseX' and provides functionalities to create, manipulate and query on XML-data. ",
    "version": "1.1.2",
    "maintainer": "Ben Engbers <Ben.Engbers@Be-Logical.nl>",
    "author": "Ben Engbers [aut, cre]",
    "url": "https://github.com/BenEngbers/RBaseX",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RBaseX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RBaseX 'BaseX' Client 'BaseX' <https://basex.org> is a XML database engine and a compliant 'XQuery 3.1' processor with full support of 'W3C Update Facility'. This package is a full client-implementation of the client/server protocol for 'BaseX' and provides functionalities to create, manipulate and query on XML-data.   "
  },
  {
    "id": 6141,
    "package_name": "RClickhouse",
    "title": "'Yandex Clickhouse' Interface for R with Basic 'dplyr' Support",
    "description": "'Yandex Clickhouse' (<https://clickhouse.com/>) is a high-performance relational column-store database to enable\n    big data exploration and 'analytics' scaling to petabytes of data. Methods are\n    provided that enable working with 'Yandex Clickhouse' databases via\n    'DBI' methods and using 'dplyr'/'dbplyr' idioms.",
    "version": "0.6.10",
    "maintainer": "Christian Hotz-Behofsits <christian.hotz-behofsits@wu.ac.at>",
    "author": "Christian Hotz-Behofsits [aut, cre],\n  Daniel Winkler [aut],\n  Luca Rauchenberger [aut],\n  Peter Knaus [aut],\n  Clemens Danninger [aut],\n  Daria Yudaeva [aut],\n  Simon Stiebellehner [aut],\n  Dan Egnor [aut],\n  Vlad Losev [aut],\n  Keith Ray [aut],\n  Zhanyong Wan [aut],\n  Markus Heule [aut],\n  Oliver Flasch [aut],\n  Google [cph],\n  Yann Collet [cph, aut] (Yann Collet is the author and copyright holder\n    of 'lz4')",
    "url": "https://github.com/IMSMWU/RClickhouse",
    "bug_reports": "https://github.com/IMSMWU/RClickhouse/issues",
    "repository": "https://cran.r-project.org/package=RClickhouse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RClickhouse 'Yandex Clickhouse' Interface for R with Basic 'dplyr' Support 'Yandex Clickhouse' (<https://clickhouse.com/>) is a high-performance relational column-store database to enable\n    big data exploration and 'analytics' scaling to petabytes of data. Methods are\n    provided that enable working with 'Yandex Clickhouse' databases via\n    'DBI' methods and using 'dplyr'/'dbplyr' idioms.  "
  },
  {
    "id": 6165,
    "package_name": "REDCapCAST",
    "title": "REDCap Metadata Casting and Castellated Data Handling",
    "description": "Casting metadata for REDCap database creation and handling of \n    castellated data using repeated instruments and longitudinal projects in \n    'REDCap'. Keeps a focused data export approach, by allowing to only export \n    required data from the database. Also for casting new REDCap databases based \n    on datasets from other sources.\n    Originally forked from the R part of 'REDCapRITS' by Paul Egeler. \n    See <https://github.com/pegeler/REDCapRITS>.\n    'REDCap' (Research Electronic Data Capture) is a secure, web-based software\n    platform designed to support data capture for research studies, providing\n    1) an intuitive interface for validated data capture; 2) audit trails for\n    tracking data manipulation and export procedures; 3) automated export\n    procedures for seamless data downloads to common statistical packages; and\n    4) procedures for data integration and interoperability with external \n    sources (Harris et al (2009) <doi:10.1016/j.jbi.2008.08.010>; \n    Harris et al (2019) <doi:10.1016/j.jbi.2019.103208>).",
    "version": "25.3.2",
    "maintainer": "Andreas Gammelgaard Damsbo <agdamsbo@clin.au.dk>",
    "author": "Andreas Gammelgaard Damsbo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7559-1154>),\n  Paul Egeler [aut] (ORCID: <https://orcid.org/0000-0001-6948-9498>)",
    "url": "https://github.com/agdamsbo/REDCapCAST,\nhttps://agdamsbo.github.io/REDCapCAST/",
    "bug_reports": "https://github.com/agdamsbo/REDCapCAST/issues",
    "repository": "https://cran.r-project.org/package=REDCapCAST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDCapCAST REDCap Metadata Casting and Castellated Data Handling Casting metadata for REDCap database creation and handling of \n    castellated data using repeated instruments and longitudinal projects in \n    'REDCap'. Keeps a focused data export approach, by allowing to only export \n    required data from the database. Also for casting new REDCap databases based \n    on datasets from other sources.\n    Originally forked from the R part of 'REDCapRITS' by Paul Egeler. \n    See <https://github.com/pegeler/REDCapRITS>.\n    'REDCap' (Research Electronic Data Capture) is a secure, web-based software\n    platform designed to support data capture for research studies, providing\n    1) an intuitive interface for validated data capture; 2) audit trails for\n    tracking data manipulation and export procedures; 3) automated export\n    procedures for seamless data downloads to common statistical packages; and\n    4) procedures for data integration and interoperability with external \n    sources (Harris et al (2009) <doi:10.1016/j.jbi.2008.08.010>; \n    Harris et al (2019) <doi:10.1016/j.jbi.2019.103208>).  "
  },
  {
    "id": 6166,
    "package_name": "REDCapDM",
    "title": "'REDCap' Data Management",
    "description": "REDCap Data Management - 'REDCap' (Research Electronic Data CAPture; <https://projectredcap.org>) is a web application developed at Vanderbilt University, designed for creating and managing online surveys and databases and the REDCap API is an interface that allows external applications to connect to REDCap remotely, and is used to programmatically retrieve or modify project data or settings within REDCap, such as importing or exporting data. REDCapDM is an R package that allows users to manage data exported directly from REDCap or using an API connection. This package includes several functions designed for pre-processing data, generating reports of queries such as outliers or missing values, and following up on previously identified queries. ",
    "version": "1.0.0",
    "maintainer": "Jo\u00e3o Carmezim <jcarmezim@igtp.cat>",
    "author": "Jo\u00e3o Carmezim [aut, cre],\n  Pau Satorra [aut],\n  Judith Pe\u00f1afiel [aut],\n  Esther Garc\u00eda [aut],\n  Nat\u00e0lia Pallar\u00e8s [aut],\n  Cristian Teb\u00e9 [aut]",
    "url": "https://bruigtp.github.io/REDCapDM/,\nhttps://doi.org/10.1186/s12874-024-02178-6",
    "bug_reports": "https://github.com/bruigtp/REDCapDM/issues",
    "repository": "https://cran.r-project.org/package=REDCapDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDCapDM 'REDCap' Data Management REDCap Data Management - 'REDCap' (Research Electronic Data CAPture; <https://projectredcap.org>) is a web application developed at Vanderbilt University, designed for creating and managing online surveys and databases and the REDCap API is an interface that allows external applications to connect to REDCap remotely, and is used to programmatically retrieve or modify project data or settings within REDCap, such as importing or exporting data. REDCapDM is an R package that allows users to manage data exported directly from REDCap or using an API connection. This package includes several functions designed for pre-processing data, generating reports of queries such as outliers or missing values, and following up on previously identified queries.   "
  },
  {
    "id": 6168,
    "package_name": "REDCapR",
    "title": "Interaction Between R and REDCap",
    "description": "Encapsulates functions to streamline calls from R to the REDCap\n    API.  REDCap (Research Electronic Data CAPture) is a web application for\n    building and managing online surveys and databases developed at Vanderbilt\n    University.  The Application Programming Interface (API) offers an avenue\n    to access and modify data programmatically, improving the capacity for\n    literate and reproducible programming.",
    "version": "1.6.0",
    "maintainer": "Will Beasley <wibeasley@hotmail.com>",
    "author": "Will Beasley [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5613-5006>),\n  David Bard [ctb] (ORCID: <https://orcid.org/0000-0002-3922-8489>),\n  Thomas Wilson [ctb],\n  John J Aponte [ctb],\n  Rollie Parrish [ctb] (ORCID: <https://orcid.org/0000-0001-8858-6381>),\n  Benjamin Nutter [ctb],\n  Andrew Peters [ctb] (ORCID: <https://orcid.org/0000-0003-2487-1268>),\n  Hao Zhu [ctb] (ORCID: <https://orcid.org/0000-0002-3386-6076>),\n  Janosch Linkersd\u00f6rfer [ctb] (ORCID:\n    <https://orcid.org/0000-0002-1577-1233>),\n  Jonathan Mang [ctb] (ORCID: <https://orcid.org/0000-0003-0518-4710>),\n  Felix Torres [ctb],\n  Philip Chase [ctb] (ORCID: <https://orcid.org/0000-0002-5318-9420>),\n  Victor Castro [ctb] (ORCID: <https://orcid.org/0000-0001-7390-6354>),\n  Greg Botwin [ctb],\n  Stephan Kadauke [ctb] (ORCID: <https://orcid.org/0000-0003-2996-8034>),\n  Ezra Porter [ctb] (ORCID: <https://orcid.org/0000-0002-4690-8343>),\n  Matthew Schuelke [ctb] (ORCID: <https://orcid.org/0000-0001-5755-1725>)",
    "url": "https://ouhscbbmc.github.io/REDCapR/,\nhttps://github.com/OuhscBbmc/REDCapR,\nhttps://www.ouhsc.edu/bbmc/, https://projectredcap.org",
    "bug_reports": "https://github.com/OuhscBbmc/REDCapR/issues",
    "repository": "https://cran.r-project.org/package=REDCapR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDCapR Interaction Between R and REDCap Encapsulates functions to streamline calls from R to the REDCap\n    API.  REDCap (Research Electronic Data CAPture) is a web application for\n    building and managing online surveys and databases developed at Vanderbilt\n    University.  The Application Programming Interface (API) offers an avenue\n    to access and modify data programmatically, improving the capacity for\n    literate and reproducible programming.  "
  },
  {
    "id": 6169,
    "package_name": "REDCapTidieR",
    "title": "Extract 'REDCap' Databases into Tidy 'Tibble's",
    "description": "Convert 'REDCap' exports into tidy tables for easy handling of 'REDCap' repeat instruments and event arms.",
    "version": "1.2.4",
    "maintainer": "Richard Hanna <hannar1@chop.edu>",
    "author": "Richard Hanna [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-6496-8154>),\n  Stephan Kadauke [aut] (ORCID: <https://orcid.org/0000-0003-2996-8034>),\n  Ezra Porter [aut] (ORCID: <https://orcid.org/0000-0002-4690-8343>)",
    "url": "https://chop-cgtinformatics.github.io/REDCapTidieR/,\nhttps://github.com/CHOP-CGTInformatics/REDCapTidieR",
    "bug_reports": "https://github.com/CHOP-CGTInformatics/REDCapTidieR/issues",
    "repository": "https://cran.r-project.org/package=REDCapTidieR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDCapTidieR Extract 'REDCap' Databases into Tidy 'Tibble's Convert 'REDCap' exports into tidy tables for easy handling of 'REDCap' repeat instruments and event arms.  "
  },
  {
    "id": 6186,
    "package_name": "RESS",
    "title": "Integrates R and Essentia",
    "description": "Contains three functions that query AuriQ Systems' Essentia Database and return the results in R. 'essQuery' takes a single Essentia command and captures the output in R, where you can save the output to a dataframe or stream it directly into additional analysis. 'read.essentia' takes an Essentia script and captures the output csv data into R, where you can save the output to a dataframe or stream it directly into additional analysis. 'capture.essentia' takes a file containing any number of Essentia commands and captures the output of the specified statements into R dataframes. Essentia can be downloaded for free at http://www.auriq.com/documentation/source/install/index.html.",
    "version": "1.3",
    "maintainer": "Ben Waxer <bwaxer@auriq.com>",
    "author": "Ben Waxer",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RESS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RESS Integrates R and Essentia Contains three functions that query AuriQ Systems' Essentia Database and return the results in R. 'essQuery' takes a single Essentia command and captures the output in R, where you can save the output to a dataframe or stream it directly into additional analysis. 'read.essentia' takes an Essentia script and captures the output csv data into R, where you can save the output to a dataframe or stream it directly into additional analysis. 'capture.essentia' takes a file containing any number of Essentia commands and captures the output of the specified statements into R dataframes. Essentia can be downloaded for free at http://www.auriq.com/documentation/source/install/index.html.  "
  },
  {
    "id": 6204,
    "package_name": "RGAP",
    "title": "Production Function Output Gap Estimation",
    "description": "The output gap indicates the percentage difference between the actual output of an economy and its potential. Since potential output is a latent process, the estimation of the output gap poses a challenge and numerous filtering techniques have been proposed. 'RGAP' facilitates the estimation of a Cobb-Douglas production function type output gap, as suggested by the European Commission (Havik et al. 2014) <https://ideas.repec.org/p/euf/ecopap/0535.html>. To that end, the non-accelerating wage rate of unemployment (NAWRU) and the trend of total factor productivity (TFP) can be estimated in two bivariate unobserved component models by means of Kalman filtering and smoothing. 'RGAP' features a flexible modeling framework for the appropriate state-space models and offers frequentist as well as Bayesian estimation techniques. Additional functionalities include direct access to the 'AMECO' <https://economy-finance.ec.europa.eu/economic-research-and-databases/economic-databases/ameco-database_en> database and automated model selection procedures. See the paper by Streicher (2022) <http://hdl.handle.net/20.500.11850/552089> for details. ",
    "version": "0.1.1",
    "maintainer": "Sina Streicher <streicher@kof.ethz.ch>",
    "author": "Sina Streicher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7848-1842>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RGAP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RGAP Production Function Output Gap Estimation The output gap indicates the percentage difference between the actual output of an economy and its potential. Since potential output is a latent process, the estimation of the output gap poses a challenge and numerous filtering techniques have been proposed. 'RGAP' facilitates the estimation of a Cobb-Douglas production function type output gap, as suggested by the European Commission (Havik et al. 2014) <https://ideas.repec.org/p/euf/ecopap/0535.html>. To that end, the non-accelerating wage rate of unemployment (NAWRU) and the trend of total factor productivity (TFP) can be estimated in two bivariate unobserved component models by means of Kalman filtering and smoothing. 'RGAP' features a flexible modeling framework for the appropriate state-space models and offers frequentist as well as Bayesian estimation techniques. Additional functionalities include direct access to the 'AMECO' <https://economy-finance.ec.europa.eu/economic-research-and-databases/economic-databases/ameco-database_en> database and automated model selection procedures. See the paper by Streicher (2022) <http://hdl.handle.net/20.500.11850/552089> for details.   "
  },
  {
    "id": 6222,
    "package_name": "RGreenplum",
    "title": "Interface to 'Greenplum' Database",
    "description": "\n    Fully 'DBI'-compliant interface to 'Greenplum' <https://greenplum.org/>,\n    an open-source parallel database. This is an extension of the 'RPostgres'\n    package <https://github.com/r-dbi/RPostgres>.",
    "version": "0.1.2",
    "maintainer": "Michael Williams <mw8765999@gmail.com>",
    "author": "Michael Williams [aut, cre]",
    "url": "https://github.com/mwillumz/RGreenplum",
    "bug_reports": "https://github.com/mwillumz/RGreenplum/issues",
    "repository": "https://cran.r-project.org/package=RGreenplum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RGreenplum Interface to 'Greenplum' Database \n    Fully 'DBI'-compliant interface to 'Greenplum' <https://greenplum.org/>,\n    an open-source parallel database. This is an extension of the 'RPostgres'\n    package <https://github.com/r-dbi/RPostgres>.  "
  },
  {
    "id": 6224,
    "package_name": "RH2",
    "title": "DBI/RJDBC Interface to H2 Database",
    "description": "DBI/RJDBC interface to h2 database. h2 version 2.3.232 is included.",
    "version": "0.2.5",
    "maintainer": "\"David M. Kaplan\" <dmkaplan2000@gmail.com>",
    "author": "G. Grothendieck [aut],\n  \"David M. Kaplan\" [cre] (ORCID:\n    <https://orcid.org/0000-0001-6087-359X>)",
    "url": "https://github.com/dmkaplan2000/RH2",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RH2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RH2 DBI/RJDBC Interface to H2 Database DBI/RJDBC interface to h2 database. h2 version 2.3.232 is included.  "
  },
  {
    "id": 6238,
    "package_name": "RISmed",
    "title": "Download Content from NCBI Databases",
    "description": "A set of tools to extract bibliographic content from the National\n    Center for Biotechnology Information (NCBI) databases, including PubMed. The\n    name RISmed is a portmanteau of RIS (for Research Information Systems, a common\n    tag format for bibliographic data) and PubMed.",
    "version": "2.3.0",
    "maintainer": "Stephanie Kovalchik <s.a.kovalchik@gmail.com>",
    "author": "Stephanie Kovalchik [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RISmed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RISmed Download Content from NCBI Databases A set of tools to extract bibliographic content from the National\n    Center for Biotechnology Information (NCBI) databases, including PubMed. The\n    name RISmed is a portmanteau of RIS (for Research Information Systems, a common\n    tag format for bibliographic data) and PubMed.  "
  },
  {
    "id": 6247,
    "package_name": "RJDBC",
    "title": "Provides Access to Databases Through the JDBC Interface",
    "description": "The RJDBC package is an implementation of R's DBI interface using JDBC as a back-end. This allows R to connect to any DBMS that has a JDBC driver.",
    "version": "0.2-10",
    "maintainer": "Simon Urbanek <Simon.Urbanek@r-project.org>",
    "author": "Simon Urbanek <Simon.Urbanek@r-project.org>",
    "url": "http://www.rforge.net/RJDBC/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RJDBC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RJDBC Provides Access to Databases Through the JDBC Interface The RJDBC package is an implementation of R's DBI interface using JDBC as a back-end. This allows R to connect to any DBMS that has a JDBC driver.  "
  },
  {
    "id": 6296,
    "package_name": "RMariaDB",
    "title": "Database Interface and MariaDB Driver",
    "description": "Implements a DBI-compliant interface to MariaDB\n    (<https://mariadb.org/>) and MySQL (<https://www.mysql.com/>)\n    databases.",
    "version": "1.3.4",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Kirill M\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1416-3412>),\n  Jeroen Ooms [aut] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  David James [aut],\n  Saikat DebRoy [aut],\n  Hadley Wickham [aut],\n  Jeffrey Horner [aut],\n  R Consortium [fnd],\n  RStudio [cph]",
    "url": "https://rmariadb.r-dbi.org, https://github.com/r-dbi/RMariaDB,\nhttps://downloads.mariadb.org/connector-c/",
    "bug_reports": "https://github.com/r-dbi/RMariaDB/issues",
    "repository": "https://cran.r-project.org/package=RMariaDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMariaDB Database Interface and MariaDB Driver Implements a DBI-compliant interface to MariaDB\n    (<https://mariadb.org/>) and MySQL (<https://www.mysql.com/>)\n    databases.  "
  },
  {
    "id": 6303,
    "package_name": "RMySQL",
    "title": "Database Interface and 'MySQL' Driver for R",
    "description": "Legacy 'DBI' interface to 'MySQL' / 'MariaDB' based on old code\n    ported from S-PLUS. A modern 'MySQL' client written in 'C++' is available\n    from the 'RMariaDB' package.",
    "version": "0.11.1",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  David James [aut],\n  Saikat DebRoy [aut],\n  Hadley Wickham [aut],\n  Jeffrey Horner [aut],\n  RStudio [cph]",
    "url": "https://r-dbi.r-universe.dev/RMySQL\nhttps://downloads.mariadb.org/connector-c/",
    "bug_reports": "https://github.com/r-dbi/rmysql/issues",
    "repository": "https://cran.r-project.org/package=RMySQL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMySQL Database Interface and 'MySQL' Driver for R Legacy 'DBI' interface to 'MySQL' / 'MariaDB' based on old code\n    ported from S-PLUS. A modern 'MySQL' client written in 'C++' is available\n    from the 'RMariaDB' package.  "
  },
  {
    "id": 6329,
    "package_name": "RODBCDBI",
    "title": "Provides Access to Databases Through the ODBC Interface",
    "description": "An implementation of R's DBI interface using ODBC package as a\n    back-end. This allows R to connect to any DBMS that has a ODBC driver.",
    "version": "0.1.1",
    "maintainer": "Nagi Teramo <teramonagi@gmail.com>",
    "author": "Nagi Teramo [aut, cre],\n  Shinichi Takayanagi [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RODBCDBI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RODBCDBI Provides Access to Databases Through the ODBC Interface An implementation of R's DBI interface using ODBC package as a\n    back-end. This allows R to connect to any DBMS that has a ODBC driver.  "
  },
  {
    "id": 6346,
    "package_name": "ROracle",
    "title": "OCI Based Oracle Database Interface for R",
    "description": "Oracle Database interface (DBI) driver for R.\n   This is a DBI-compliant Oracle driver based on the OCI.",
    "version": "1.5-1",
    "maintainer": "Rajendra S. Pingte <rajendra.pingte@oracle.com>",
    "author": "Denis Mukhin [aut],\n  David A. James [aut],\n  Jake Luciani [aut],\n  Rajendra S. Pingte [aut, cre]",
    "url": "https://www.oracle.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ROracle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ROracle OCI Based Oracle Database Interface for R Oracle Database interface (DBI) driver for R.\n   This is a DBI-compliant Oracle driver based on the OCI.  "
  },
  {
    "id": 6359,
    "package_name": "RPostgreSQL",
    "title": "R Interface to the 'PostgreSQL' Database System",
    "description": "Database interface and 'PostgreSQL' driver for 'R'.\n This package provides a Database Interface 'DBI' compliant \n driver for 'R' to access 'PostgreSQL' database systems.  \n In order to build and install this package from source, 'PostgreSQL' \n itself must be present your system to provide 'PostgreSQL' functionality \n via its libraries and header files. These files are provided as\n 'postgresql-devel' package under some Linux distributions.\n On 'macOS' and 'Microsoft Windows' system the attached 'libpq' library source will be used.",
    "version": "0.7-8",
    "maintainer": "Tomoaki Nishiyama <tomoaki@sci.u-toyama.ac.jp>",
    "author": "Joe Conway [aut],\n  Dirk Eddelbuettel [aut],\n  Tomoaki Nishiyama [aut, cre],\n  Sameer Kumar Prayaga [aut] (during 2008),\n  Neil Tiffin [aut]",
    "url": "https://github.com/tomoakin/RPostgreSQL,\nhttps://cran.r-project.org/package=DBI,\nhttps://www.postgresql.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RPostgreSQL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPostgreSQL R Interface to the 'PostgreSQL' Database System Database interface and 'PostgreSQL' driver for 'R'.\n This package provides a Database Interface 'DBI' compliant \n driver for 'R' to access 'PostgreSQL' database systems.  \n In order to build and install this package from source, 'PostgreSQL' \n itself must be present your system to provide 'PostgreSQL' functionality \n via its libraries and header files. These files are provided as\n 'postgresql-devel' package under some Linux distributions.\n On 'macOS' and 'Microsoft Windows' system the attached 'libpq' library source will be used.  "
  },
  {
    "id": 6360,
    "package_name": "RPostgres",
    "title": "C++ Interface to PostgreSQL",
    "description": "Fully DBI-compliant C++-backed interface to PostgreSQL\n    <https://www.postgresql.org/>, an open-source relational database.",
    "version": "1.4.8",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Hadley Wickham [aut],\n  Jeroen Ooms [aut],\n  Kirill M\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1416-3412>),\n  RStudio [cph],\n  R Consortium [fnd],\n  Tomoaki Nishiyama [ctb] (Code for encoding vectors into strings derived\n    from RPostgreSQL)",
    "url": "https://rpostgres.r-dbi.org, https://github.com/r-dbi/RPostgres",
    "bug_reports": "https://github.com/r-dbi/RPostgres/issues",
    "repository": "https://cran.r-project.org/package=RPostgres",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPostgres C++ Interface to PostgreSQL Fully DBI-compliant C++-backed interface to PostgreSQL\n    <https://www.postgresql.org/>, an open-source relational database.  "
  },
  {
    "id": 6362,
    "package_name": "RPresto",
    "title": "DBI Connector to Presto",
    "description": "Implements a 'DBI' compliant interface to Presto. Presto is\n    an open source distributed SQL query engine for running interactive\n    analytic queries against data sources of all sizes ranging from\n    gigabytes to petabytes: <https://prestodb.io/>.",
    "version": "1.4.8",
    "maintainer": "Jarod G.R. Meng <jarodm@fb.com>",
    "author": "Onur Ismail Filiz [aut],\n  Sergey Goder [aut],\n  Jarod G.R. Meng [aut, cre],\n  Thomas J. Leeper [ctb],\n  John Myles White [ctb]",
    "url": "https://github.com/prestodb/RPresto",
    "bug_reports": "https://github.com/prestodb/RPresto/issues",
    "repository": "https://cran.r-project.org/package=RPresto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RPresto DBI Connector to Presto Implements a 'DBI' compliant interface to Presto. Presto is\n    an open source distributed SQL query engine for running interactive\n    analytic queries against data sources of all sizes ranging from\n    gigabytes to petabytes: <https://prestodb.io/>.  "
  },
  {
    "id": 6381,
    "package_name": "RRedshiftSQL",
    "title": "R Interface to the 'Redshift' Database",
    "description": "Superclasses 'PostgreSQL' connection to help enable full 'dplyr' functionality on 'Redshift'.",
    "version": "0.1.2",
    "maintainer": "Michael Treadwell <michael.treadwell@interworks.com>",
    "author": "Michael Treadwell",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RRedshiftSQL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RRedshiftSQL R Interface to the 'Redshift' Database Superclasses 'PostgreSQL' connection to help enable full 'dplyr' functionality on 'Redshift'.  "
  },
  {
    "id": 6401,
    "package_name": "RSQL",
    "title": "Database Agnostic Package to Generate and Process 'SQL' Queries\nin R",
    "description": "Allows the user to generate and execute select, insert, update and delete 'SQL' queries the underlying database without having to explicitly write 'SQL' code. ",
    "version": "0.2.2",
    "maintainer": "Alejandro Baranek <abaranek@dc.uba.ar>",
    "author": "Alejandro Baranek [cre, aut],\n  Leonardo Belen [aut]",
    "url": "https://github.com/rOpenStats/RSQL",
    "bug_reports": "https://github.com/rOpenStats/RSQL/issues",
    "repository": "https://cran.r-project.org/package=RSQL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSQL Database Agnostic Package to Generate and Process 'SQL' Queries\nin R Allows the user to generate and execute select, insert, update and delete 'SQL' queries the underlying database without having to explicitly write 'SQL' code.   "
  },
  {
    "id": 6402,
    "package_name": "RSQLite",
    "title": "SQLite Interface for R",
    "description": "Embeds the SQLite database engine in R and provides an\n    interface compliant with the DBI package. The source for the SQLite\n    engine (version 3.51.1) and for various extensions is included.\n    System libraries will never be consulted because this package relies\n    on static linking for the plugins it includes; this also ensures a\n    consistent experience across all installations.",
    "version": "2.4.5",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Kirill M\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1416-3412>),\n  Hadley Wickham [aut],\n  David A. James [aut],\n  Seth Falcon [aut],\n  D. Richard Hipp [ctb] (for the included SQLite sources),\n  Dan Kennedy [ctb] (for the included SQLite sources),\n  Joe Mistachkin [ctb] (for the included SQLite sources),\n  SQLite Authors [ctb] (for the included SQLite sources),\n  Liam Healy [ctb] (for the included SQLite sources),\n  R Consortium [fnd],\n  RStudio [cph]",
    "url": "https://rsqlite.r-dbi.org, https://github.com/r-dbi/RSQLite",
    "bug_reports": "https://github.com/r-dbi/RSQLite/issues",
    "repository": "https://cran.r-project.org/package=RSQLite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSQLite SQLite Interface for R Embeds the SQLite database engine in R and provides an\n    interface compliant with the DBI package. The source for the SQLite\n    engine (version 3.51.1) and for various extensions is included.\n    System libraries will never be consulted because this package relies\n    on static linking for the plugins it includes; this also ensures a\n    consistent experience across all installations.  "
  },
  {
    "id": 6417,
    "package_name": "RSqlParser",
    "title": "Parse 'SQL' Statements",
    "description": "Parser for 'SQL' statements. Currently, it supports parsing of only 'SELECT' statements.",
    "version": "1.5",
    "maintainer": "Subhasree Bose <subhasree10.7@gmail.com>",
    "author": "Subhasree Bose",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSqlParser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSqlParser Parse 'SQL' Statements Parser for 'SQL' statements. Currently, it supports parsing of only 'SELECT' statements.  "
  },
  {
    "id": 6423,
    "package_name": "RTD",
    "title": "Simple TD API Client",
    "description": "\n  Upload R data.frame to Arm Treasure Data, see <https://www.treasuredata.com/>. \n  You can execute database or table handling for resources on Arm Treasure Data.",
    "version": "0.4.2",
    "maintainer": "Aki Ariga <ariga@treasure-data.com>",
    "author": "Aki Ariga [aut, cre, cph]",
    "url": "https://github.com/treasure-data/RTD",
    "bug_reports": "https://github.com/treasure-data/RTD/issues",
    "repository": "https://cran.r-project.org/package=RTD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTD Simple TD API Client \n  Upload R data.frame to Arm Treasure Data, see <https://www.treasuredata.com/>. \n  You can execute database or table handling for resources on Arm Treasure Data.  "
  },
  {
    "id": 6448,
    "package_name": "RWildbook",
    "title": "Interface for the 'Wildbook' Wildlife Data Management Framework",
    "description": "Provides an interface with the 'Wildbook' mark-recapture ecological database framework. It \n    helps users to pull data from the 'Wildbook' framework and format data for further analysis\n    with mark-recapture applications like 'Program MARK' (which can be accessed via the 'RMark' package in 'R').\n    Further information on the 'Wildbook' framework is available at: <http://www.wildbook.org/doku.php>. ",
    "version": "0.9.3",
    "maintainer": "Simon Bonner <sbonner6@uwo.ca>",
    "author": "Simon Bonner [aut, cre],\n  Xinxin Huang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RWildbook",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RWildbook Interface for the 'Wildbook' Wildlife Data Management Framework Provides an interface with the 'Wildbook' mark-recapture ecological database framework. It \n    helps users to pull data from the 'Wildbook' framework and format data for further analysis\n    with mark-recapture applications like 'Program MARK' (which can be accessed via the 'RMark' package in 'R').\n    Further information on the 'Wildbook' framework is available at: <http://www.wildbook.org/doku.php>.   "
  },
  {
    "id": 6461,
    "package_name": "RadData",
    "title": "Nuclear Decay Data for Dosimetric Calculations - ICRP 107",
    "description": "Nuclear Decay Data for Dosimetric Calculations from the \n    International Commission on Radiological Protection from ICRP \n    Publication 107. Ann. ICRP 38 (3). Eckerman, Keith and Endo, Akira 2008 \n    <doi:10.1016/j.icrp.2008.10.004> \n    <https://www.icrp.org/publication.asp?id=ICRP%20Publication%20107>. \n    This is a database of the physical data needed in calculations of \n    radionuclide-specific protection and operational quantities. The \n    data is prescribed by the ICRP, the international authority on \n    radiation dose standards, for estimating dose from the intake of or \n    exposure to radionuclides in the workplace and the environment. \n    The database contains information on the half-lives, decay chains, \n    and yields and energies of radiations emitted in nuclear transformations \n    of 1252 radionuclides of 97 elements. ",
    "version": "1.0.2",
    "maintainer": "Mark Hogue <mark.hogue.chp@gmail.com>",
    "author": "Mark Hogue [aut, cre],\n  KF Eckerman [dtc, cph],\n  A Endo [dtc, cph]",
    "url": "https://github.com/markhogue/RadData",
    "bug_reports": "https://github.com/markhogue/RadData/issues",
    "repository": "https://cran.r-project.org/package=RadData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RadData Nuclear Decay Data for Dosimetric Calculations - ICRP 107 Nuclear Decay Data for Dosimetric Calculations from the \n    International Commission on Radiological Protection from ICRP \n    Publication 107. Ann. ICRP 38 (3). Eckerman, Keith and Endo, Akira 2008 \n    <doi:10.1016/j.icrp.2008.10.004> \n    <https://www.icrp.org/publication.asp?id=ICRP%20Publication%20107>. \n    This is a database of the physical data needed in calculations of \n    radionuclide-specific protection and operational quantities. The \n    data is prescribed by the ICRP, the international authority on \n    radiation dose standards, for estimating dose from the intake of or \n    exposure to radionuclides in the workplace and the environment. \n    The database contains information on the half-lives, decay chains, \n    and yields and energies of radiations emitted in nuclear transformations \n    of 1252 radionuclides of 97 elements.   "
  },
  {
    "id": 6548,
    "package_name": "Rcompadre",
    "title": "Utilities for using the 'COM(P)ADRE' Matrix Model Database",
    "description": "Utility functions for interacting with the 'COMPADRE' and\n    'COMADRE' databases of matrix population models. Described in Jones et\n    al. (2021) <doi:10.1101/2021.04.26.441330>.",
    "version": "1.4.0",
    "maintainer": "Owen Jones <jones@biology.sdu.dk>",
    "author": "Patrick Barks [aut] (ORCID: <https://orcid.org/0000-0002-5947-8151>),\n  Danny Buss [aut],\n  Roberto Salguero-Gomez [aut] (ORCID:\n    <https://orcid.org/0000-0002-6085-4433>),\n  Iain Stott [aut] (ORCID: <https://orcid.org/0000-0003-2724-7436>),\n  William K. Petry [aut] (ORCID: <https://orcid.org/0000-0002-5230-5987>),\n  Tamora James [aut] (ORCID: <https://orcid.org/0000-0003-1363-4742>),\n  Owen Jones [aut, cre] (ORCID: <https://orcid.org/0000-0001-5720-4686>),\n  Julia Jones [aut] (ORCID: <https://orcid.org/0000-0001-9223-1778>),\n  Gesa R\u00f6mer [aut] (ORCID: <https://orcid.org/0000-0002-4859-5870>),\n  Sam Levin [aut] (ORCID: <https://orcid.org/0000-0002-3289-9925>)",
    "url": "https://github.com/jonesor/Rcompadre",
    "bug_reports": "https://github.com/jonesor/Rcompadre/issues",
    "repository": "https://cran.r-project.org/package=Rcompadre",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rcompadre Utilities for using the 'COM(P)ADRE' Matrix Model Database Utility functions for interacting with the 'COMPADRE' and\n    'COMADRE' databases of matrix population models. Described in Jones et\n    al. (2021) <doi:10.1101/2021.04.26.441330>.  "
  },
  {
    "id": 6629,
    "package_name": "ReDaMoR",
    "title": "Relational Data Modeler",
    "description": "The aim of this package is to manipulate relational\n   data models in R.\n   It provides functions to create, modify and export data models\n   in json format.\n   It also allows importing models created\n   with 'MySQL Workbench' (<https://www.mysql.com/products/workbench/>).\n   These functions are accessible through a graphical user\n   interface made with 'shiny'.\n   Constraints such as types, keys, uniqueness and mandatory fields are\n   automatically checked and corrected when editing a model.\n   Finally, real data can be confronted to a model to check their compatibility.",
    "version": "0.8.2",
    "maintainer": "Patrice Godard <patrice.godard@gmail.com>",
    "author": "Patrice Godard [aut, cre, cph],\n  Kai Lin [ctb]",
    "url": "https://patzaw.github.io/ReDaMoR/,\nhttps://github.com/patzaw/ReDaMoR/",
    "bug_reports": "https://github.com/patzaw/ReDaMoR/issues",
    "repository": "https://cran.r-project.org/package=ReDaMoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReDaMoR Relational Data Modeler The aim of this package is to manipulate relational\n   data models in R.\n   It provides functions to create, modify and export data models\n   in json format.\n   It also allows importing models created\n   with 'MySQL Workbench' (<https://www.mysql.com/products/workbench/>).\n   These functions are accessible through a graphical user\n   interface made with 'shiny'.\n   Constraints such as types, keys, uniqueness and mandatory fields are\n   automatically checked and corrected when editing a model.\n   Finally, real data can be confronted to a model to check their compatibility.  "
  },
  {
    "id": 6679,
    "package_name": "ReporterScore",
    "title": "Generalized Reporter Score-Based Enrichment Analysis for Omics\nData",
    "description": "Inspired by the classic 'RSA', we developed the improved 'Generalized Reporter \n    Score-based Analysis (GRSA)' method, implemented in the R package 'ReporterScore', along \n    with comprehensive visualization methods and pathway databases. 'GRSA' is a threshold-free \n    method that works well with all types of biomedical features, such as genes, chemical compounds, \n    and microbial species. Importantly, the 'GRSA' supports multi-group and longitudinal experimental \n    designs, because of the included multi-group-compatible statistical methods. ",
    "version": "0.1.9",
    "maintainer": "Chen Peng <pengchen2001@zju.edu.cn>",
    "author": "Chen Peng [aut, cre] (ORCID: <https://orcid.org/0000-0002-9449-7606>)",
    "url": "https://github.com/Asa12138/ReporterScore",
    "bug_reports": "https://github.com/Asa12138/ReporterScore/issues",
    "repository": "https://cran.r-project.org/package=ReporterScore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ReporterScore Generalized Reporter Score-Based Enrichment Analysis for Omics\nData Inspired by the classic 'RSA', we developed the improved 'Generalized Reporter \n    Score-based Analysis (GRSA)' method, implemented in the R package 'ReporterScore', along \n    with comprehensive visualization methods and pathway databases. 'GRSA' is a threshold-free \n    method that works well with all types of biomedical features, such as genes, chemical compounds, \n    and microbial species. Importantly, the 'GRSA' supports multi-group and longitudinal experimental \n    designs, because of the included multi-group-compatible statistical methods.   "
  },
  {
    "id": 6690,
    "package_name": "ResultModelManager",
    "title": "Result Model Manager",
    "description": "Database data model management utilities for R packages in the Observational Health Data Sciences and\n    Informatics programme. 'ResultModelManager' provides utility functions to allow package\n    maintainers to migrate existing SQL database models, export and import results in consistent patterns.",
    "version": "0.6.2",
    "maintainer": "Jamie Gilbert <gilbert@ohdsi.org>",
    "author": "Jamie Gilbert [aut, cre]",
    "url": "https://github.com/OHDSI/ResultModelManager,\nhttps://ohdsi.github.io/ResultModelManager/",
    "bug_reports": "https://github.com/OHDSI/ResultModelManager/issues",
    "repository": "https://cran.r-project.org/package=ResultModelManager",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ResultModelManager Result Model Manager Database data model management utilities for R packages in the Observational Health Data Sciences and\n    Informatics programme. 'ResultModelManager' provides utility functions to allow package\n    maintainers to migrate existing SQL database models, export and import results in consistent patterns.  "
  },
  {
    "id": 6718,
    "package_name": "Rilostat",
    "title": "R Interface to ILOSTAT Open Data",
    "description": "Provides tools to access, search, and manipulate ILO's ilostat database,\n  including bulk download of statistical data, dictionary lookups, and table of contents.",
    "version": "2.3.4",
    "maintainer": "David Bescond <bescond@ilo.org>",
    "author": "David Bescond [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-0304>),\n  Mabelin Villarreal-Fuentes [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5644-5760>),\n  ILO Department of Statistics [cph, fnd]",
    "url": "https://ilostat.github.io/Rilostat/",
    "bug_reports": "https://github.com/ilostat/Rilostat/issues",
    "repository": "https://cran.r-project.org/package=Rilostat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rilostat R Interface to ILOSTAT Open Data Provides tools to access, search, and manipulate ILO's ilostat database,\n  including bulk download of statistical data, dictionary lookups, and table of contents.  "
  },
  {
    "id": 6799,
    "package_name": "Rpadrino",
    "title": "Interact with the 'PADRINO' IPM Database",
    "description": "'PADRINO' houses textual representations of\n    Integral Projection Models which can be converted from their \n    table format into full kernels to reproduce or extend an \n    already published analysis. 'Rpadrino' is an R interface to this database. For\n    more information on Integral Projection Models, see Easterling et al. (2000) \n  <doi:10.1890/0012-9658(2000)081[0694:SSSAAN]2.0.CO;2>, Merow et al. (2013) \n  <doi:10.1111/2041-210X.12146>, Rees et al. (2014) <doi:10.1111/1365-2656.12178>,\n  and Metcalf et al. (2015) <doi:10.1111/2041-210X.12405>. See Levin et al. (2021)\n  for more information on 'ipmr', the engine that powers model reconstruction\n  <doi:10.1111/2041-210X.13683>.",
    "version": "0.0.5",
    "maintainer": "Sam Levin <levisc8@gmail.com>",
    "author": "Sam Levin [aut, cre] (ORCID: <https://orcid.org/0000-0002-3289-9925>),\n  Aldo Compagnoni [aut],\n  Dylan Childs [aut],\n  Sanne Evers [aut],\n  Tomos Potter [aut],\n  Roberto Salguero-Gomez [aut],\n  Tiffany Knight [aut]",
    "url": "https://github.com/padrinoDB/Rpadrino,\nhttps://padrinoDB.github.io/Rpadrino/",
    "bug_reports": "https://github.com/padrinoDB/Rpadrino/issues",
    "repository": "https://cran.r-project.org/package=Rpadrino",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rpadrino Interact with the 'PADRINO' IPM Database 'PADRINO' houses textual representations of\n    Integral Projection Models which can be converted from their \n    table format into full kernels to reproduce or extend an \n    already published analysis. 'Rpadrino' is an R interface to this database. For\n    more information on Integral Projection Models, see Easterling et al. (2000) \n  <doi:10.1890/0012-9658(2000)081[0694:SSSAAN]2.0.CO;2>, Merow et al. (2013) \n  <doi:10.1111/2041-210X.12146>, Rees et al. (2014) <doi:10.1111/1365-2656.12178>,\n  and Metcalf et al. (2015) <doi:10.1111/2041-210X.12405>. See Levin et al. (2021)\n  for more information on 'ipmr', the engine that powers model reconstruction\n  <doi:10.1111/2041-210X.13683>.  "
  },
  {
    "id": 6800,
    "package_name": "Rparadox",
    "title": "Read Paradox Database Files into R",
    "description": "Provides a simple and efficient way to read data from Paradox \n    database files (.db) directly into R as modern 'tibble' data frames. \n    It uses the underlying 'pxlib' C library, to handle the low-level file format \n    details and provides a clean, user-friendly R interface.",
    "version": "0.1.5",
    "maintainer": "Daniil Popov <popov.daniil@gmail.com>",
    "author": "Daniil Popov [aut, cre]",
    "url": "https://github.com/celebithil/Rparadox,\nhttps://github.com/steinm/pxlib",
    "bug_reports": "https://github.com/celebithil/Rparadox/issues",
    "repository": "https://cran.r-project.org/package=Rparadox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rparadox Read Paradox Database Files into R Provides a simple and efficient way to read data from Paradox \n    database files (.db) directly into R as modern 'tibble' data frames. \n    It uses the underlying 'pxlib' C library, to handle the low-level file format \n    details and provides a clean, user-friendly R interface.  "
  },
  {
    "id": 6805,
    "package_name": "Rpoet",
    "title": "'PoetryDB' API Wrapper",
    "description": "\n  Wrapper for the 'PoetryDB' API <http://poetrydb.org> that allows for interaction and data extraction from the \n  database in an R interface. The 'PoetryDB' API is a database of poetry and poets implemented with 'MongoDB' to \n  enable developers and poets to easily access one of the most comprehensive poetry databases currently available.",
    "version": "1.1.0",
    "maintainer": "Aaron Schlegel <aaron@aaronschlegel.me>",
    "author": "Aaron Schlegel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rpoet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rpoet 'PoetryDB' API Wrapper \n  Wrapper for the 'PoetryDB' API <http://poetrydb.org> that allows for interaction and data extraction from the \n  database in an R interface. The 'PoetryDB' API is a database of poetry and poets implemented with 'MongoDB' to \n  enable developers and poets to easily access one of the most comprehensive poetry databases currently available.  "
  },
  {
    "id": 6903,
    "package_name": "SCDB",
    "title": "Easily Access and Maintain Time-Based Versioned Data\n(Slowly-Changing-Dimension)",
    "description": "A collection of functions that enable easy access and updating of a database of data over time.\n             More specifically, the package facilitates type-2 history for data-warehouses and provides a number\n             of Quality of life improvements for working on SQL databases with R.\n             For reference see Ralph Kimball and Margy Ross (2013, ISBN 9781118530801).",
    "version": "0.5.1",
    "maintainer": "Rasmus Skytte Randl\u00f8v <rske@ssi.dk>",
    "author": "Rasmus Skytte Randl\u00f8v [aut, cre, rev] (ORCID:\n    <https://orcid.org/0000-0002-5860-3838>),\n  Marcus Munch Gr\u00fcnewald [aut] (ORCID:\n    <https://orcid.org/0009-0006-8090-406X>),\n  Lasse Engbo Christiansen [rev] (ORCID:\n    <https://orcid.org/0000-0001-5019-1931>),\n  Sofia Myrup Otero [rev],\n  Kim Daniel Jacobsen [ctb],\n  Statens Serum Institut [cph, fnd]",
    "url": "https://github.com/ssi-dk/SCDB, https://ssi-dk.github.io/SCDB/",
    "bug_reports": "https://github.com/ssi-dk/SCDB/issues",
    "repository": "https://cran.r-project.org/package=SCDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SCDB Easily Access and Maintain Time-Based Versioned Data\n(Slowly-Changing-Dimension) A collection of functions that enable easy access and updating of a database of data over time.\n             More specifically, the package facilitates type-2 history for data-warehouses and provides a number\n             of Quality of life improvements for working on SQL databases with R.\n             For reference see Ralph Kimball and Margy Ross (2013, ISBN 9781118530801).  "
  },
  {
    "id": 6954,
    "package_name": "SEQTaRget",
    "title": "Sequential Trial Emulation",
    "description": "Implementation of sequential trial emulation for the analysis of observational databases.\n    The 'SEQTaRget' software accommodates time-varying treatments and confounders, as well as binary\n    and failure time outcomes. 'SEQTaRget' allows to compare both static and dynamic strategies,\n    can be used to estimate observational analogs of intention-to-treat\n    and per-protocol effects, and can adjust for potential selection bias\n    induced by losses-to-follow-up. (Paper to come).",
    "version": "1.3.2",
    "maintainer": "Ryan O'Dea <ryan.odea@psi.ch>",
    "author": "Ryan O'Dea [aut, cre] (ORCID: <https://orcid.org/0009-0000-0103-9546>),\n  Alejandro Szmulewicz [aut] (ORCID:\n    <https://orcid.org/0000-0002-2664-802X>),\n  Tom Palmer [aut] (ORCID: <https://orcid.org/0000-0003-4655-4511>, ROR:\n    <https://ror.org/0524sp257>),\n  Miguel Hernan [aut] (ORCID: <https://orcid.org/0000-0003-1619-8456>),\n  The President and Fellows of Harvard College [cph] (ROR:\n    <https://ror.org/03vek6s52>)",
    "url": "https://causalinference.github.io/SEQTaRget/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SEQTaRget",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SEQTaRget Sequential Trial Emulation Implementation of sequential trial emulation for the analysis of observational databases.\n    The 'SEQTaRget' software accommodates time-varying treatments and confounders, as well as binary\n    and failure time outcomes. 'SEQTaRget' allows to compare both static and dynamic strategies,\n    can be used to estimate observational analogs of intention-to-treat\n    and per-protocol effects, and can adjust for potential selection bias\n    induced by losses-to-follow-up. (Paper to come).  "
  },
  {
    "id": 7074,
    "package_name": "SP2000",
    "title": "Catalogue of Life Toolkit",
    "description": "A programmatic interface to <http://sp2000.org.cn>, re-written based on an accompanying 'Species 2000' API. Access tables describing catalogue of the Chinese known species of animals, plants, fungi, micro-organisms, and more. This package also supports access to catalogue of life global <http://catalogueoflife.org>, China animal scientific database <http://zoology.especies.cn> and catalogue of life Taiwan <https://taibnet.sinica.edu.tw/home_eng.php>. The development of 'SP2000' package were supported by Biodiversity Survey and Assessment Project of the Ministry of Ecology and Environment, China <2019HJ2096001006>,Yunnan University's \"Double First Class\" Project <C176240405> and Yunnan University's Research Innovation Fund for Graduate Students <2019227>.",
    "version": "0.2.0",
    "maintainer": "Liuyong Ding <ly_ding@126.com>",
    "author": "Liuyong Ding [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5490-182X>),\n  Minrui Huang [ctb],\n  Ke Yang [ctb],\n  Jun Wang [ctb] (ORCID: <https://orcid.org/0000-0003-2481-1409>),\n  Juan Tao [ctb],\n  Chengzhi Ding [ctb] (ORCID: <https://orcid.org/0000-0001-5215-7374>),\n  Daming He [ctb]",
    "url": "https://otoliths.github.io/SP2000/",
    "bug_reports": "https://github.com/Otoliths/SP2000/issues",
    "repository": "https://cran.r-project.org/package=SP2000",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SP2000 Catalogue of Life Toolkit A programmatic interface to <http://sp2000.org.cn>, re-written based on an accompanying 'Species 2000' API. Access tables describing catalogue of the Chinese known species of animals, plants, fungi, micro-organisms, and more. This package also supports access to catalogue of life global <http://catalogueoflife.org>, China animal scientific database <http://zoology.especies.cn> and catalogue of life Taiwan <https://taibnet.sinica.edu.tw/home_eng.php>. The development of 'SP2000' package were supported by Biodiversity Survey and Assessment Project of the Ministry of Ecology and Environment, China <2019HJ2096001006>,Yunnan University's \"Double First Class\" Project <C176240405> and Yunnan University's Research Innovation Fund for Graduate Students <2019227>.  "
  },
  {
    "id": 7105,
    "package_name": "SQL",
    "title": "Executes 'SQL' Statements",
    "description": "Runs 'SQL' statements on in-memory data frames within a temporary in-memory 'duckdb' data base.",
    "version": "0.1.1",
    "maintainer": "Jean-Luc Lipatz <jllipatz@protonmail.com>",
    "author": "Jean-Luc Lipatz [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SQL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SQL Executes 'SQL' Statements Runs 'SQL' statements on in-memory data frames within a temporary in-memory 'duckdb' data base.  "
  },
  {
    "id": 7106,
    "package_name": "SQLFormatteR",
    "title": "Format SQL Queries",
    "description": "A convenient interface for formatting 'SQL' queries directly\n    within 'R'. It acts as a wrapper around the 'sql_format' Rust crate.\n    The package allows you to format 'SQL' code with customizable options,\n    including indentation, case formatting, and more, ensuring your 'SQL'\n    queries are clean, readable, and consistent.",
    "version": "0.0.2",
    "maintainer": "Morgan Durand <morgan@dataupsurge.com>",
    "author": "Morgan Durand [aut, cre, cph],\n  Authors of the dependent Rust crates [aut] (see AUTHORS file)",
    "url": "https://dataupsurge.github.io/SQLFormatteR/,\nhttps://github.com/dataupsurge/SQLFormatteR",
    "bug_reports": "https://github.com/dataupsurge/SQLFormatteR/issues",
    "repository": "https://cran.r-project.org/package=SQLFormatteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SQLFormatteR Format SQL Queries A convenient interface for formatting 'SQL' queries directly\n    within 'R'. It acts as a wrapper around the 'sql_format' Rust crate.\n    The package allows you to format 'SQL' code with customizable options,\n    including indentation, case formatting, and more, ensuring your 'SQL'\n    queries are clean, readable, and consistent.  "
  },
  {
    "id": 7107,
    "package_name": "SQLove",
    "title": "Execute 'SQL' Scripts in 'R' Containing Multiple Queries",
    "description": "The nature of working with structured query language ('SQL') scripts \n             efficiently often requires the creation of temporary tables and there \n             are few clean and simple 'R' 'SQL' execution approaches that allow \n             you to complete this kind of work with the 'R' environment. This \n             package seeks to give 'SQL' implementations in 'R' a little love \n             by deploying functions that allow you to deploy complex 'SQL' \n             scripts within a typical 'R' workflow.",
    "version": "1.0.2",
    "maintainer": "Kerns Sam <samwkerns@gmail.com>",
    "author": "Kerns Sam [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SQLove",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SQLove Execute 'SQL' Scripts in 'R' Containing Multiple Queries The nature of working with structured query language ('SQL') scripts \n             efficiently often requires the creation of temporary tables and there \n             are few clean and simple 'R' 'SQL' execution approaches that allow \n             you to complete this kind of work with the 'R' environment. This \n             package seeks to give 'SQL' implementations in 'R' a little love \n             by deploying functions that allow you to deploy complex 'SQL' \n             scripts within a typical 'R' workflow.  "
  },
  {
    "id": 7110,
    "package_name": "SQRL",
    "title": "Enhances Interaction with 'ODBC' Databases",
    "description": "Provides simple and powerful interfaces that facilitate interaction\n    with 'ODBC' data sources. Each data source gets its own unique and dedicated\n    interface, wrapped around 'RODBC'. Communication settings are remembered\n    between queries, and are managed silently in the background. The interfaces\n    support multi-statement 'SQL' scripts, which can be parameterised via\n    metaprogramming structures and embedded 'R' expressions.",
    "version": "1.0.2",
    "maintainer": "Mike Lee <random.deviate@gmail.com>",
    "author": "Mike Lee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SQRL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SQRL Enhances Interaction with 'ODBC' Databases Provides simple and powerful interfaces that facilitate interaction\n    with 'ODBC' data sources. Each data source gets its own unique and dedicated\n    interface, wrapped around 'RODBC'. Communication settings are remembered\n    between queries, and are managed silently in the background. The interfaces\n    support multi-statement 'SQL' scripts, which can be parameterised via\n    metaprogramming structures and embedded 'R' expressions.  "
  },
  {
    "id": 7123,
    "package_name": "SSHAARP",
    "title": "Searching Shared HLA Amino Acid Residue Prevalence",
    "description": "Processes amino acid alignments produced by the 'IPD-IMGT/HLA (Immuno Polymorphism-ImMunoGeneTics/Human Leukocyte Antigen) Database' to identify user-defined amino acid residue motifs shared across HLA alleles, HLA alleles, or HLA haplotypes, and calculates frequencies based on HLA allele frequency data. 'SSHAARP' (Searching Shared HLA Amino Acid Residue Prevalence) uses 'Generic Mapping Tools (GMT)' software and the 'GMT' R package to generate global frequency heat maps that illustrate the distribution of each user-defined map around the globe. 'SSHAARP' analyzes the allele frequency data described by Solberg et al. (2008) <doi:10.1016/j.humimm.2008.05.001>, a global set of 497 population samples from 185 published datasets, representing 66,800 individuals total. Users may also specify their own datasets, but file conventions must follow the prebundled Solberg dataset, or the mock haplotype dataset.",
    "version": "2.0.8",
    "maintainer": "Livia Tran <livia.tran@ucsf.edu>",
    "author": "Livia Tran [aut, cre],\n  Steven Mack [aut],\n  Josh Bredeweg [ctb],\n  Dale Steinhardt [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SSHAARP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSHAARP Searching Shared HLA Amino Acid Residue Prevalence Processes amino acid alignments produced by the 'IPD-IMGT/HLA (Immuno Polymorphism-ImMunoGeneTics/Human Leukocyte Antigen) Database' to identify user-defined amino acid residue motifs shared across HLA alleles, HLA alleles, or HLA haplotypes, and calculates frequencies based on HLA allele frequency data. 'SSHAARP' (Searching Shared HLA Amino Acid Residue Prevalence) uses 'Generic Mapping Tools (GMT)' software and the 'GMT' R package to generate global frequency heat maps that illustrate the distribution of each user-defined map around the globe. 'SSHAARP' analyzes the allele frequency data described by Solberg et al. (2008) <doi:10.1016/j.humimm.2008.05.001>, a global set of 497 population samples from 185 published datasets, representing 66,800 individuals total. Users may also specify their own datasets, but file conventions must follow the prebundled Solberg dataset, or the mock haplotype dataset.  "
  },
  {
    "id": 7215,
    "package_name": "SelfControlledCaseSeries",
    "title": "Self-Controlled Case Series",
    "description": "Execute the self-controlled case series (SCCS) design using observational \n  data in the OMOP Common Data Model. Extracts all necessary data from the database and \n\ttransforms it to the format required for SCCS. Age and season can be modeled\n\tusing splines assuming constant hazard within calendar months. Event-dependent \n\tcensoring of the observation period can be corrected for. Many exposures can be\n\tincluded at once (MSCCS), with regularization on all coefficients except for the\n\texposure of interest. Includes diagnostics for all major assumptions of the SCCS.",
    "version": "6.1.1",
    "maintainer": "Martijn Schuemie <schuemie@ohdsi.org>",
    "author": "Martijn Schuemie [aut, cre],\n  Patrick Ryan [aut],\n  Trevor Shaddox [aut],\n  Marc Suchard [aut]",
    "url": "https://ohdsi.github.io/SelfControlledCaseSeries/,\nhttps://github.com/OHDSI/SelfControlledCaseSeries",
    "bug_reports": "https://github.com/OHDSI/SelfControlledCaseSeries/issues",
    "repository": "https://cran.r-project.org/package=SelfControlledCaseSeries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SelfControlledCaseSeries Self-Controlled Case Series Execute the self-controlled case series (SCCS) design using observational \n  data in the OMOP Common Data Model. Extracts all necessary data from the database and \n\ttransforms it to the format required for SCCS. Age and season can be modeled\n\tusing splines assuming constant hazard within calendar months. Event-dependent \n\tcensoring of the observation period can be corrected for. Many exposures can be\n\tincluded at once (MSCCS), with regularization on all coefficients except for the\n\texposure of interest. Includes diagnostics for all major assumptions of the SCCS.  "
  },
  {
    "id": 7241,
    "package_name": "SequentialDesign",
    "title": "Observational Database Study Planning using Exact Sequential\nAnalysis for Poisson and Binomial Data",
    "description": "Functions to be used in conjunction with the 'Sequential' package that allows for planning of observational database studies that will be analyzed with exact sequential analysis. This package supports Poisson- and binomial-based data. The primary function, seq_wrapper(...), accepts parameters for simulation of a simple exposure pattern and for the 'Sequential' package setup and analysis functions. The exposure matrix is used to simulate the true and false positive and negative populations (Green (1983) <doi:10.1093/oxfordjournals.aje.a113521>, Brenner (1993) <doi:10.1093/oxfordjournals.aje.a116805>). Functions are then run from the 'Sequential' package on these populations, which allows for the exploration of outcome misclassification in data.",
    "version": "1.0",
    "maintainer": "Judith Maro <judy_maro@harvardpilgrim.org>",
    "author": "Judith Maro [aut, cre],\n  Laura Hou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SequentialDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SequentialDesign Observational Database Study Planning using Exact Sequential\nAnalysis for Poisson and Binomial Data Functions to be used in conjunction with the 'Sequential' package that allows for planning of observational database studies that will be analyzed with exact sequential analysis. This package supports Poisson- and binomial-based data. The primary function, seq_wrapper(...), accepts parameters for simulation of a simple exposure pattern and for the 'Sequential' package setup and analysis functions. The exposure matrix is used to simulate the true and false positive and negative populations (Green (1983) <doi:10.1093/oxfordjournals.aje.a113521>, Brenner (1993) <doi:10.1093/oxfordjournals.aje.a116805>). Functions are then run from the 'Sequential' package on these populations, which allows for the exploration of outcome misclassification in data.  "
  },
  {
    "id": 7349,
    "package_name": "SoilTaxonomy",
    "title": "A System of Soil Classification for Making and Interpreting Soil\nSurveys",
    "description": "Taxonomic dictionaries, formative element lists, and functions related to the maintenance, development and application of U.S. Soil Taxonomy. \n   Data and functionality are based on official U.S. Department of Agriculture sources including the latest edition of the Keys to Soil Taxonomy. Descriptions and metadata are obtained from the National Soil Information System or Soil Survey Geographic databases. Other sources are referenced in the data documentation. \n   Provides tools for understanding and interacting with concepts in the U.S. Soil Taxonomic System. Most of the current utilities are for working with taxonomic concepts at the \"higher\" taxonomic levels: Order, Suborder, Great Group, and Subgroup.",
    "version": "0.2.8",
    "maintainer": "Andrew Brown <andrew.g.brown@usda.gov>",
    "author": "Andrew Brown [aut, cre],\n  Dylan Beaudette [aut]",
    "url": "https://github.com/ncss-tech/SoilTaxonomy,\nhttps://ncss-tech.github.io/SoilTaxonomy/",
    "bug_reports": "https://github.com/ncss-tech/SoilTaxonomy/issues",
    "repository": "https://cran.r-project.org/package=SoilTaxonomy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SoilTaxonomy A System of Soil Classification for Making and Interpreting Soil\nSurveys Taxonomic dictionaries, formative element lists, and functions related to the maintenance, development and application of U.S. Soil Taxonomy. \n   Data and functionality are based on official U.S. Department of Agriculture sources including the latest edition of the Keys to Soil Taxonomy. Descriptions and metadata are obtained from the National Soil Information System or Soil Survey Geographic databases. Other sources are referenced in the data documentation. \n   Provides tools for understanding and interacting with concepts in the U.S. Soil Taxonomic System. Most of the current utilities are for working with taxonomic concepts at the \"higher\" taxonomic levels: Order, Suborder, Great Group, and Subgroup.  "
  },
  {
    "id": 7367,
    "package_name": "SpaCCI",
    "title": "Spatially Aware Cell-Cell Interaction Analysis",
    "description": "Provides tools for analyzing spatial cell-cell interactions based on ligand-receptor pairs, including functions for local, regional, and global analysis using spatial transcriptomics data. Integrates with databases like 'CellChat' <http://www.cellchat.org/>, 'CellPhoneDB' <https://www.cellphonedb.org/>, 'Cellinker' <https://www.rna-society.org/cellinker/index.html>, 'ICELLNET' <https://github.com/soumelis-lab/ICELLNET>, and 'ConnectomeDB' <https://humanconnectome.org/software/connectomedb/> to identify ligand-receptor pairs, visualize interactions through heatmaps, chord diagrams, and infer interactions on different spatial scales. ",
    "version": "1.0.4",
    "maintainer": "Li-Ting Ku <lku@mdanderson.org>",
    "author": "Li-Ting Ku [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpaCCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpaCCI Spatially Aware Cell-Cell Interaction Analysis Provides tools for analyzing spatial cell-cell interactions based on ligand-receptor pairs, including functions for local, regional, and global analysis using spatial transcriptomics data. Integrates with databases like 'CellChat' <http://www.cellchat.org/>, 'CellPhoneDB' <https://www.cellphonedb.org/>, 'Cellinker' <https://www.rna-society.org/cellinker/index.html>, 'ICELLNET' <https://github.com/soumelis-lab/ICELLNET>, and 'ConnectomeDB' <https://humanconnectome.org/software/connectomedb/> to identify ligand-receptor pairs, visualize interactions through heatmaps, chord diagrams, and infer interactions on different spatial scales.   "
  },
  {
    "id": 7432,
    "package_name": "SqlRender",
    "title": "Rendering Parameterized SQL and Translation to Dialects",
    "description": "A rendering tool for parameterized SQL that also translates into\n  different SQL dialects.  These dialects include 'Microsoft SQL Server', 'Oracle', \n  'PostgreSql', 'Amazon RedShift', 'Apache Impala', 'IBM Netezza', 'Google BigQuery', 'Microsoft PDW', 'Snowflake', \n  'Azure Synapse Analytics Dedicated', 'Apache Spark', 'SQLite', and 'InterSystems IRIS'.",
    "version": "1.19.4",
    "maintainer": "Martijn Schuemie <schuemie@ohdsi.org>",
    "author": "Martijn Schuemie [aut, cre],\n  Marc Suchard [aut]",
    "url": "https://ohdsi.github.io/SqlRender/,\nhttps://github.com/OHDSI/SqlRender",
    "bug_reports": "https://github.com/OHDSI/SqlRender/issues",
    "repository": "https://cran.r-project.org/package=SqlRender",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SqlRender Rendering Parameterized SQL and Translation to Dialects A rendering tool for parameterized SQL that also translates into\n  different SQL dialects.  These dialects include 'Microsoft SQL Server', 'Oracle', \n  'PostgreSql', 'Amazon RedShift', 'Apache Impala', 'IBM Netezza', 'Google BigQuery', 'Microsoft PDW', 'Snowflake', \n  'Azure Synapse Analytics Dedicated', 'Apache Spark', 'SQLite', and 'InterSystems IRIS'.  "
  },
  {
    "id": 7487,
    "package_name": "StreamCatTools",
    "title": "'StreamCatTools'",
    "description": "Tools for using the 'StreamCat' and 'LakeCat' API and \n             interacting with the 'StreamCat' and 'LakeCat' database. \n             Convenience functions in the package wrap the API for 'StreamCat' \n             on <https://api.epa.gov/StreamCat/streams/metrics>.",
    "version": "0.9.1",
    "maintainer": "Marc Weber <weber.marc@epa.gov>",
    "author": "Marc Weber [aut, cre],\n  Ryan Hill [ctb],\n  Travis Hudson [ctb],\n  Allen Brookes [ctb],\n  David Rebhuhn [ctb],\n  Michael Dumelle [ctb],\n  Zachary Smith [ctb]",
    "url": "https://usepa.github.io/StreamCatTools/,\nhttps://github.com/USEPA/StreamCatTools",
    "bug_reports": "https://github.com/USEPA/StreamCatTools/issues",
    "repository": "https://cran.r-project.org/package=StreamCatTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StreamCatTools 'StreamCatTools' Tools for using the 'StreamCat' and 'LakeCat' API and \n             interacting with the 'StreamCat' and 'LakeCat' database. \n             Convenience functions in the package wrap the API for 'StreamCat' \n             on <https://api.epa.gov/StreamCat/streams/metrics>.  "
  },
  {
    "id": 7501,
    "package_name": "SubtypeDrug",
    "title": "Prioritization of Candidate Cancer Subtype Specific Drugs",
    "description": "A systematic biology tool was developed to prioritize cancer subtype-specific drugs by integrating genetic perturbation, drug action, biological pathway, and cancer subtype. \n    The capabilities of this tool include inferring patient-specific subpathway activity profiles in the context of gene expression profiles with subtype labels, calculating differentially \n    expressed subpathways based on cultured human cells treated with drugs in the 'cMap' (connectivity map) database, prioritizing cancer subtype specific drugs according to drug-disease \n    reverse association score based on subpathway, and visualization of results (Castelo (2013) <doi:10.1186/1471-2105-14-7>; Han et al (2019) <doi:10.1093/bioinformatics/btz894>; Lamb and Justin (2006) <doi:10.1126/science.1132939>). Please cite using <doi:10.1093/bioinformatics/btab011>.",
    "version": "0.1.9",
    "maintainer": "Junwei Han <hanjunwei1981@163.com>",
    "author": "Xudong Han,\n  Junwei Han,\n  Chonghui Liu",
    "url": "",
    "bug_reports": "https://github.com/hanjunwei-lab/SubtypeDrug/issues",
    "repository": "https://cran.r-project.org/package=SubtypeDrug",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SubtypeDrug Prioritization of Candidate Cancer Subtype Specific Drugs A systematic biology tool was developed to prioritize cancer subtype-specific drugs by integrating genetic perturbation, drug action, biological pathway, and cancer subtype. \n    The capabilities of this tool include inferring patient-specific subpathway activity profiles in the context of gene expression profiles with subtype labels, calculating differentially \n    expressed subpathways based on cultured human cells treated with drugs in the 'cMap' (connectivity map) database, prioritizing cancer subtype specific drugs according to drug-disease \n    reverse association score based on subpathway, and visualization of results (Castelo (2013) <doi:10.1186/1471-2105-14-7>; Han et al (2019) <doi:10.1093/bioinformatics/btz894>; Lamb and Justin (2006) <doi:10.1126/science.1132939>). Please cite using <doi:10.1093/bioinformatics/btab011>.  "
  },
  {
    "id": 7620,
    "package_name": "TKCat",
    "title": "Tailored Knowledge Catalog",
    "description": "Facilitate the management of data from knowledge\n   resources that are frequently used alone or together\n   in research environments.\n   In 'TKCat', knowledge resources are manipulated as modeled database (MDB)\n   objects. These objects provide access to the data tables along with a general\n   description of the resource and a detail data model documenting the\n   tables, their fields and their relationships.\n   These MDBs are then gathered in catalogs that can be easily\n   explored an shared.\n   Finally, 'TKCat' provides tools to easily subset, filter and combine MDBs and\n   create new catalogs suited for specific needs.",
    "version": "1.1.14",
    "maintainer": "Patrice Godard <patrice.godard@gmail.com>",
    "author": "Patrice Godard [aut, cre, cph]",
    "url": "https://patzaw.github.io/TKCat/, https://github.com/patzaw/TKCat/",
    "bug_reports": "https://github.com/patzaw/TKCat/issues",
    "repository": "https://cran.r-project.org/package=TKCat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TKCat Tailored Knowledge Catalog Facilitate the management of data from knowledge\n   resources that are frequently used alone or together\n   in research environments.\n   In 'TKCat', knowledge resources are manipulated as modeled database (MDB)\n   objects. These objects provide access to the data tables along with a general\n   description of the resource and a detail data model documenting the\n   tables, their fields and their relationships.\n   These MDBs are then gathered in catalogs that can be easily\n   explored an shared.\n   Finally, 'TKCat' provides tools to easily subset, filter and combine MDBs and\n   create new catalogs suited for specific needs.  "
  },
  {
    "id": 7624,
    "package_name": "TMDb",
    "title": "Access to TMDb API",
    "description": "Provides an R-interface to the TMDb API (see TMDb API on <https://developers.themoviedb.org/3/getting-started/introduction>). The Movie Database (TMDb) is a popular user editable database for movies and TV shows (see <https://www.themoviedb.org>).",
    "version": "1.1",
    "maintainer": "Andrea Capozio <andreacapozio@gmail.com>",
    "author": "Andrea Capozio <andreacapozio@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TMDb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TMDb Access to TMDb API Provides an R-interface to the TMDb API (see TMDb API on <https://developers.themoviedb.org/3/getting-started/introduction>). The Movie Database (TMDb) is a popular user editable database for movies and TV shows (see <https://www.themoviedb.org>).  "
  },
  {
    "id": 7639,
    "package_name": "TPEA",
    "title": "A Novel Topology-Based Pathway Enrichment Analysis Approach",
    "description": "We described a novel Topology-based pathway enrichment analysis, which integrated the global position of the nodes and the topological property of the pathways in  Kyoto Encyclopedia of Genes and Genomes Database.\n             We also provide some functions to obtain the latest information about pathways to finish pathway enrichment analysis using this method. ",
    "version": "3.1.0",
    "maintainer": "Wei Jiang <jiangwei@hrbmu.edu.cn>",
    "author": "Wei Jiang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TPEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TPEA A Novel Topology-Based Pathway Enrichment Analysis Approach We described a novel Topology-based pathway enrichment analysis, which integrated the global position of the nodes and the topological property of the pathways in  Kyoto Encyclopedia of Genes and Genomes Database.\n             We also provide some functions to obtain the latest information about pathways to finish pathway enrichment analysis using this method.   "
  },
  {
    "id": 7645,
    "package_name": "TR8",
    "title": "A Tool for Downloading Functional Traits Data for Plant Species",
    "description": "Plant ecologists often need to collect \"traits\" data\n    about plant species which are often scattered among various\n    databases: TR8 contains a set of tools which take care of\n    automatically retrieving some of those functional traits data\n    for plant species from publicly available databases (The Ecological Flora\n    of the British Isles, LEDA traitbase, Ellenberg\n    values for Italian Flora, Mycorrhizal intensity databases, BROT,\n    PLANTS, Jepson Flora Project).\n    The TR8 name, inspired by \"car plates\" jokes, was chosen since\n    it both reminds of the main object of the package and is\n    extremely short to type.",
    "version": "0.9.23",
    "maintainer": "Gionata Bocci <boccigionata@gmail.com>",
    "author": "Gionata Bocci [aut, cre]",
    "url": "https://github.com/GioBo/TR8",
    "bug_reports": "https://github.com/GioBo/TR8/issues",
    "repository": "https://cran.r-project.org/package=TR8",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TR8 A Tool for Downloading Functional Traits Data for Plant Species Plant ecologists often need to collect \"traits\" data\n    about plant species which are often scattered among various\n    databases: TR8 contains a set of tools which take care of\n    automatically retrieving some of those functional traits data\n    for plant species from publicly available databases (The Ecological Flora\n    of the British Isles, LEDA traitbase, Ellenberg\n    values for Italian Flora, Mycorrhizal intensity databases, BROT,\n    PLANTS, Jepson Flora Project).\n    The TR8 name, inspired by \"car plates\" jokes, was chosen since\n    it both reminds of the main object of the package and is\n    extremely short to type.  "
  },
  {
    "id": 7647,
    "package_name": "TRAMPR",
    "title": "'TRFLP' Analysis and Matching Package for R",
    "description": "Matching terminal restriction fragment length\n        polymorphism ('TRFLP') profiles between unknown samples and a\n        database of known samples.  'TRAMPR' facilitates analysis of\n        many unknown profiles at once, and provides tools for working\n        directly with electrophoresis output through to generating\n        summaries suitable for community analyses with R's rich set of\n        statistical functions.  'TRAMPR' also resolves the issues of\n        multiple 'TRFLP' profiles within a species, and shared 'TRFLP'\n        profiles across species.",
    "version": "1.0-10",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\n  Ian Dickie [aut]",
    "url": "https://github.com/richfitz/TRAMPR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TRAMPR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TRAMPR 'TRFLP' Analysis and Matching Package for R Matching terminal restriction fragment length\n        polymorphism ('TRFLP') profiles between unknown samples and a\n        database of known samples.  'TRAMPR' facilitates analysis of\n        many unknown profiles at once, and provides tools for working\n        directly with electrophoresis output through to generating\n        summaries suitable for community analyses with R's rich set of\n        statistical functions.  'TRAMPR' also resolves the issues of\n        multiple 'TRFLP' profiles within a species, and shared 'TRFLP'\n        profiles across species.  "
  },
  {
    "id": 7862,
    "package_name": "UCSCXenaShiny",
    "title": "Interactive Analysis of UCSC Xena Data",
    "description": "Provides functions and a Shiny application for downloading,\n    analyzing and visualizing datasets from UCSC Xena\n    (<http://xena.ucsc.edu/>), which is a collection of UCSC-hosted public\n    databases such as TCGA, ICGC, TARGET, GTEx, CCLE, and others.",
    "version": "2.2.0",
    "maintainer": "Shixiang Wang <w_shixiang@163.com>",
    "author": "Shixiang Wang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9855-7357>),\n  Shensuo Li [aut],\n  Yi Xiong [aut] (ORCID: <https://orcid.org/0000-0002-4370-9824>),\n  Longfei Zhao [aut] (ORCID: <https://orcid.org/0000-0002-6277-0137>),\n  Kai Gu [aut] (ORCID: <https://orcid.org/0000-0002-0177-0774>),\n  Yin Li [aut],\n  Fei Zhao [aut]",
    "url": "https://github.com/openbiox/UCSCXenaShiny,\nhttps://openbiox.github.io/UCSCXenaShiny/",
    "bug_reports": "https://github.com/openbiox/UCSCXenaShiny/issues",
    "repository": "https://cran.r-project.org/package=UCSCXenaShiny",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UCSCXenaShiny Interactive Analysis of UCSC Xena Data Provides functions and a Shiny application for downloading,\n    analyzing and visualizing datasets from UCSC Xena\n    (<http://xena.ucsc.edu/>), which is a collection of UCSC-hosted public\n    databases such as TCGA, ICGC, TARGET, GTEx, CCLE, and others.  "
  },
  {
    "id": 7882,
    "package_name": "UVdose",
    "title": "Estimate Ambient UV Dose from Location and Date Info",
    "description": "Estimate ambient vitamin D-effective or erythemal dose using ultraviolet radiation (UV) data from the 'TEMIS' database, based on date and geographical location.",
    "version": "0.1.1",
    "maintainer": "Rasha Shraim <rshraim@tcd.ie>",
    "author": "Rasha Shraim [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3351-1179>),\n  Lina Zgaga [aut] (ORCID: <https://orcid.org/0000-0003-4089-9703>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UVdose",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UVdose Estimate Ambient UV Dose from Location and Date Info Estimate ambient vitamin D-effective or erythemal dose using ultraviolet radiation (UV) data from the 'TEMIS' database, based on date and geographical location.  "
  },
  {
    "id": 7981,
    "package_name": "VirtualPop",
    "title": "Simulation of Populations by Sampling Waiting-Time Distributions",
    "description": "Constructs a virtual population from fertility and mortality rates for any country,\n\tcalendar year and birth cohort in the Human Mortality Database <https://www.mortality.org> and the Human Fertility Database <https://www.humanfertility.org>.  Fertility histories are simulated for every individual and their offspring, producing a multi-generation virtual population. ",
    "version": "2.1.0",
    "maintainer": "Frans Willekens <willekens@nidi.nl>",
    "author": "Frans Willekens [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6125-0212>),\n  Tim Riffe [ctb] (ORCID: <https://orcid.org/0000-0002-2673-4622>)",
    "url": "",
    "bug_reports": "https://github.com/willekens/VirtualPop/issues",
    "repository": "https://cran.r-project.org/package=VirtualPop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VirtualPop Simulation of Populations by Sampling Waiting-Time Distributions Constructs a virtual population from fertility and mortality rates for any country,\n\tcalendar year and birth cohort in the Human Mortality Database <https://www.mortality.org> and the Human Fertility Database <https://www.humanfertility.org>.  Fertility histories are simulated for every individual and their offspring, producing a multi-generation virtual population.   "
  },
  {
    "id": 7987,
    "package_name": "Visualize.CRAN.Downloads",
    "title": "Visualize Downloads from 'CRAN' Packages",
    "description": "Visualize the trends and historical downloads from packages in the 'CRAN' repository. Data is obtained by using the 'API' to query the database from the 'RStudio' 'CRAN' mirror.",
    "version": "1.0.3",
    "maintainer": "Marcelo Ponce <m.ponce@utoronto.ca>",
    "author": "Marcelo Ponce [aut, cre]",
    "url": "https://github.com/mponce0/Visualize.CRAN.Downloads",
    "bug_reports": "https://github.com/mponce0/Visualize.CRAN.Downloads/issues",
    "repository": "https://cran.r-project.org/package=Visualize.CRAN.Downloads",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Visualize.CRAN.Downloads Visualize Downloads from 'CRAN' Packages Visualize the trends and historical downloads from packages in the 'CRAN' repository. Data is obtained by using the 'API' to query the database from the 'RStudio' 'CRAN' mirror.  "
  },
  {
    "id": 8065,
    "package_name": "WeightedCluster",
    "title": "Clustering of Weighted Data",
    "description": "Clusters state sequences and weighted data. It provides an optimized weighted PAM algorithm as well as functions for aggregating replicated cases, computing cluster quality measures for a range of clustering solutions, sequence analysis typology validation using parametric bootstraps and plotting (fuzzy) clusters of state sequences. It further provides a fuzzy and crisp CLARA algorithm to cluster large database with sequence analysis, and a methodological framework for Robustness Assessment of Regressions using Cluster Analysis Typologies (RARCAT).",
    "version": "2.0",
    "maintainer": "Matthias Studer <matthias.studer@unige.ch>",
    "author": "Matthias Studer [aut, cre],\n  Leonard Roth [ctb]",
    "url": "http://mephisto.unige.ch/weightedcluster/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WeightedCluster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WeightedCluster Clustering of Weighted Data Clusters state sequences and weighted data. It provides an optimized weighted PAM algorithm as well as functions for aggregating replicated cases, computing cluster quality measures for a range of clustering solutions, sequence analysis typology validation using parametric bootstraps and plotting (fuzzy) clusters of state sequences. It further provides a fuzzy and crisp CLARA algorithm to cluster large database with sequence analysis, and a methodological framework for Robustness Assessment of Regressions using Cluster Analysis Typologies (RARCAT).  "
  },
  {
    "id": 8173,
    "package_name": "abjData",
    "title": "Databases Used Routinely by the Brazilian Jurimetrics\nAssociation",
    "description": "The Brazilian Jurimetrics Association (ABJ in\n    Portuguese, see <https://abj.org.br/> for more information) is\n    a non-profit organization which aims to investigate and promote the\n    use of statistics and probability in the study of Law and its\n    institutions. This package has a set of datasets commonly used in\n    our book.",
    "version": "1.1.2",
    "maintainer": "Julio Trecenti <julio.trecenti@gmail.com>",
    "author": "Julio Trecenti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1680-6389>),\n  Renata Hirota [ctb],\n  Katerine Witkoski [aut] (ORCID:\n    <https://orcid.org/0000-0002-3691-6569>),\n  Associa\u00e7\u00e3o Brasileira de Jurimetria [cph, fnd]",
    "url": "https://abjur.github.io/abjData/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=abjData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "abjData Databases Used Routinely by the Brazilian Jurimetrics\nAssociation The Brazilian Jurimetrics Association (ABJ in\n    Portuguese, see <https://abj.org.br/> for more information) is\n    a non-profit organization which aims to investigate and promote the\n    use of statistics and probability in the study of Law and its\n    institutions. This package has a set of datasets commonly used in\n    our book.  "
  },
  {
    "id": 8202,
    "package_name": "acdcquery",
    "title": "Query the Attentional Control Data Collection",
    "description": "Interact with the Attentional Control Data Collection (ACDC).\n  Connect to the database via connect_to_db(), set filter arguments via add_argument()\n  and query the database via query_db().",
    "version": "1.1.1",
    "maintainer": "Sven Lesche <sven.lesche@psychologie.uni-heidelberg.de>",
    "author": "Sven Lesche [aut, cre, cph],\n  Julia M. Haaf [ctb, ths],\n  Madlen Hoffstadt [ctb]",
    "url": "https://github.com/SLesche/acdc-query",
    "bug_reports": "https://github.com/SLesche/acdc-query/issues",
    "repository": "https://cran.r-project.org/package=acdcquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acdcquery Query the Attentional Control Data Collection Interact with the Attentional Control Data Collection (ACDC).\n  Connect to the database via connect_to_db(), set filter arguments via add_argument()\n  and query the database via query_db().  "
  },
  {
    "id": 8219,
    "package_name": "acss",
    "title": "Algorithmic Complexity for Short Strings",
    "description": "Main functionality is to provide the algorithmic complexity for\n    short strings, an approximation of the Kolmogorov Complexity of a short\n    string using the coding theorem method (see ?acss). The database containing\n    the complexity is provided in the data only package acss.data, this package\n    provides functions accessing the data such as prob_random returning the\n    posterior probability that a given string was produced by a random process.\n    In addition, two traditional (but problematic) measures of complexity are\n    also provided: entropy and change complexity.",
    "version": "0.3-2",
    "maintainer": "Henrik Singmann <singmann+acss@gmail.com>",
    "author": "Nicolas Gauvrit [aut],\n  Henrik Singmann [aut, cre],\n  Fernando Soler Toscano [ctb],\n  Hector Zenil [ctb]",
    "url": "https://complexity-calculator.com/methodology.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=acss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acss Algorithmic Complexity for Short Strings Main functionality is to provide the algorithmic complexity for\n    short strings, an approximation of the Kolmogorov Complexity of a short\n    string using the coding theorem method (see ?acss). The database containing\n    the complexity is provided in the data only package acss.data, this package\n    provides functions accessing the data such as prob_random returning the\n    posterior probability that a given string was produced by a random process.\n    In addition, two traditional (but problematic) measures of complexity are\n    also provided: entropy and change complexity.  "
  },
  {
    "id": 8244,
    "package_name": "adamethods",
    "title": "Archetypoid Algorithms and Anomaly Detection",
    "description": "Collection of several algorithms to obtain archetypoids with small and large databases, and with both classical \n\tmultivariate data and functional data (univariate and multivariate). Some of these algorithms also allow to detect \n\tanomalies (outliers). Please see Vinue and Epifanio (2020) <doi:10.1007/s11634-020-00412-9>. ",
    "version": "1.2.1",
    "maintainer": "Guillermo Vinue <Guillermo.Vinue@uv.es>",
    "author": "Guillermo Vinue, Irene Epifanio",
    "url": "https://www.r-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=adamethods",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adamethods Archetypoid Algorithms and Anomaly Detection Collection of several algorithms to obtain archetypoids with small and large databases, and with both classical \n\tmultivariate data and functional data (univariate and multivariate). Some of these algorithms also allow to detect \n\tanomalies (outliers). Please see Vinue and Epifanio (2020) <doi:10.1007/s11634-020-00412-9>.   "
  },
  {
    "id": 8256,
    "package_name": "adaptivetau",
    "title": "Tau-Leaping Stochastic Simulation",
    "description": "Implements adaptive tau leaping to approximate the\n        trajectory of a continuous-time stochastic process as\n        described by Cao et al. (2007) The Journal of Chemical Physics\n        <doi:10.1063/1.2745299> (aka. the Gillespie stochastic\n        simulation algorithm).  This package is based upon work\n        supported by NSF DBI-0906041 and NIH K99-GM104158 to Philip\n        Johnson and NIH R01-AI049334 to Rustom Antia.",
    "version": "2.3-2",
    "maintainer": "Philip Johnson <plfj@umd.edu>",
    "author": "Philip Johnson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6087-7064>)",
    "url": "https://github.com/plfjohnson/adaptivetau",
    "bug_reports": "https://github.com/plfjohnson/adaptivetau/issues",
    "repository": "https://cran.r-project.org/package=adaptivetau",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adaptivetau Tau-Leaping Stochastic Simulation Implements adaptive tau leaping to approximate the\n        trajectory of a continuous-time stochastic process as\n        described by Cao et al. (2007) The Journal of Chemical Physics\n        <doi:10.1063/1.2745299> (aka. the Gillespie stochastic\n        simulation algorithm).  This package is based upon work\n        supported by NSF DBI-0906041 and NIH K99-GM104158 to Philip\n        Johnson and NIH R01-AI049334 to Rustom Antia.  "
  },
  {
    "id": 8261,
    "package_name": "adbcdrivermanager",
    "title": "'Arrow' Database Connectivity ('ADBC') Driver Manager",
    "description": "Provides a developer-facing interface to 'Arrow' Database\n  Connectivity ('ADBC') for the purposes of driver development, driver\n  testing, and building high-level database interfaces for users. 'ADBC'\n  <https://arrow.apache.org/adbc/> is an API standard for database access\n  libraries that uses 'Arrow' for result sets and query parameters.",
    "version": "0.21.0",
    "maintainer": "Dewey Dunnington <dewey@dunnington.ca>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Apache Arrow [aut, cph],\n  Apache Software Foundation [cph]",
    "url": "https://arrow.apache.org/adbc/current/r/adbcdrivermanager/,\nhttps://github.com/apache/arrow-adbc",
    "bug_reports": "https://github.com/apache/arrow-adbc/issues",
    "repository": "https://cran.r-project.org/package=adbcdrivermanager",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adbcdrivermanager 'Arrow' Database Connectivity ('ADBC') Driver Manager Provides a developer-facing interface to 'Arrow' Database\n  Connectivity ('ADBC') for the purposes of driver development, driver\n  testing, and building high-level database interfaces for users. 'ADBC'\n  <https://arrow.apache.org/adbc/> is an API standard for database access\n  libraries that uses 'Arrow' for result sets and query parameters.  "
  },
  {
    "id": 8262,
    "package_name": "adbcpostgresql",
    "title": "'Arrow' Database Connectivity ('ADBC') 'PostgreSQL' Driver",
    "description": "Provides a developer-facing interface to the 'Arrow' Database\n  Connectivity ('ADBC') 'PostgreSQL' driver for the purposes of building high-level\n  database interfaces for users. 'ADBC' <https://arrow.apache.org/adbc/> is\n  an API standard for database access libraries that uses 'Arrow' for result\n  sets and query parameters.",
    "version": "0.21.0",
    "maintainer": "Dewey Dunnington <dewey@dunnington.ca>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Apache Arrow [aut, cph],\n  Apache Software Foundation [cph]",
    "url": "https://arrow.apache.org/adbc/current/r/adbcpostgresql/,\nhttps://github.com/apache/arrow-adbc",
    "bug_reports": "https://github.com/apache/arrow-adbc/issues",
    "repository": "https://cran.r-project.org/package=adbcpostgresql",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adbcpostgresql 'Arrow' Database Connectivity ('ADBC') 'PostgreSQL' Driver Provides a developer-facing interface to the 'Arrow' Database\n  Connectivity ('ADBC') 'PostgreSQL' driver for the purposes of building high-level\n  database interfaces for users. 'ADBC' <https://arrow.apache.org/adbc/> is\n  an API standard for database access libraries that uses 'Arrow' for result\n  sets and query parameters.  "
  },
  {
    "id": 8263,
    "package_name": "adbcsqlite",
    "title": "'Arrow' Database Connectivity ('ADBC') 'SQLite' Driver",
    "description": "Provides a developer-facing interface to the 'Arrow' Database\n  Connectivity ('ADBC') 'SQLite' driver for the purposes of building high-level\n  database interfaces for users. 'ADBC' <https://arrow.apache.org/adbc/> is\n  an API standard for database access libraries that uses 'Arrow' for result\n  sets and query parameters.",
    "version": "0.21.0",
    "maintainer": "Dewey Dunnington <dewey@dunnington.ca>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Apache Arrow [aut, cph],\n  Apache Software Foundation [cph]",
    "url": "https://arrow.apache.org/adbc/current/r/adbcsqlite/,\nhttps://github.com/apache/arrow-adbc",
    "bug_reports": "https://github.com/apache/arrow-adbc/issues",
    "repository": "https://cran.r-project.org/package=adbcsqlite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adbcsqlite 'Arrow' Database Connectivity ('ADBC') 'SQLite' Driver Provides a developer-facing interface to the 'Arrow' Database\n  Connectivity ('ADBC') 'SQLite' driver for the purposes of building high-level\n  database interfaces for users. 'ADBC' <https://arrow.apache.org/adbc/> is\n  an API standard for database access libraries that uses 'Arrow' for result\n  sets and query parameters.  "
  },
  {
    "id": 8264,
    "package_name": "adbi",
    "title": "'DBI' Compliant Database Access Using 'ADBC'",
    "description": "In order to make Arrow Database Connectivity ('ADBC' <https://arrow.apache.org/adbc/>) accessible from R, an interface compliant with the 'DBI' package is provided, using driver back-ends that are implemented in the 'adbcdrivermanager' framework. This enables interacting with database systems using the Arrow data format, thereby offering an efficient alternative to 'ODBC' for analytical applications.",
    "version": "0.1.2",
    "maintainer": "Nicolas Bennett <nicolas@cynkra.com>",
    "author": "Nicolas Bennett [aut, cre],\n  Voltron Data [fnd]",
    "url": "https://adbi.r-dbi.org, https://github.com/r-dbi/adbi,\nhttps://arrow.apache.org/adbc/",
    "bug_reports": "https://github.com/r-dbi/adbi/issues",
    "repository": "https://cran.r-project.org/package=adbi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adbi 'DBI' Compliant Database Access Using 'ADBC' In order to make Arrow Database Connectivity ('ADBC' <https://arrow.apache.org/adbc/>) accessible from R, an interface compliant with the 'DBI' package is provided, using driver back-ends that are implemented in the 'adbcdrivermanager' framework. This enables interacting with database systems using the Arrow data format, thereby offering an efficient alternative to 'ODBC' for analytical applications.  "
  },
  {
    "id": 8348,
    "package_name": "africamonitor",
    "title": "Africa Macroeconomic Monitor Database API",
    "description": "An R API providing access to a relational database with macroeconomic data for Africa. \n             The database contains >700 macroeconomic time series from mostly international sources, \n             grouped into 50 macroeconomic and development-related topics. Series are carefully selected\n             on the basis of data coverage for Africa, frequency, and relevance to the macro-development context. \n             The project is part of the 'Kiel Institute Africa Initiative' \n             <https://www.ifw-kiel.de/institute/initiatives/kiel-institute-africa-initiative/>, \n             which, amongst other things, aims to develop a parsimonious database with highly relevant indicators \n             to monitor macroeconomic developments in Africa, accessible through a fast API and a web-based platform\n             at <https://africamonitor.ifw-kiel.de/>. \n             The database is maintained at the Kiel Institute for the World Economy <https://www.ifw-kiel.de/>. ",
    "version": "0.2.4",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre]",
    "url": "https://africamonitor.ifw-kiel.de/",
    "bug_reports": "https://github.com/kielinstitute/africamonitor/issues",
    "repository": "https://cran.r-project.org/package=africamonitor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "africamonitor Africa Macroeconomic Monitor Database API An R API providing access to a relational database with macroeconomic data for Africa. \n             The database contains >700 macroeconomic time series from mostly international sources, \n             grouped into 50 macroeconomic and development-related topics. Series are carefully selected\n             on the basis of data coverage for Africa, frequency, and relevance to the macro-development context. \n             The project is part of the 'Kiel Institute Africa Initiative' \n             <https://www.ifw-kiel.de/institute/initiatives/kiel-institute-africa-initiative/>, \n             which, amongst other things, aims to develop a parsimonious database with highly relevant indicators \n             to monitor macroeconomic developments in Africa, accessible through a fast API and a web-based platform\n             at <https://africamonitor.ifw-kiel.de/>. \n             The database is maintained at the Kiel Institute for the World Economy <https://www.ifw-kiel.de/>.   "
  },
  {
    "id": 8396,
    "package_name": "airGRdatasets",
    "title": "Hydro-Meteorological Catchments Datasets for the 'airGR'\nPackages",
    "description": "Sample of hydro-meteorological datasets extracted from the 'CAMELS-FR' French database <doi:10.57745/WH7FJR>. \n  It provides metadata and catchment-scale aggregated hydro-meteorological time series on a pool of French catchments for use by the 'airGR' packages. ",
    "version": "0.2.3",
    "maintainer": "Olivier Delaigue <airGR@inrae.fr>",
    "author": "Olivier Delaigue [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7668-8468>),\n  Pierre Brigode [aut] (ORCID: <https://orcid.org/0000-0001-8257-0741>),\n  Guillaume Thirel [aut] (ORCID: <https://orcid.org/0000-0002-1444-1830>),\n  Beno\u00eet G\u00e9not [ctb],\n  Guilherme Mendoza Guimar\u00e3es [ctb] (ORCID:\n    <https://orcid.org/0000-0002-4580-6089>)",
    "url": "https://gitlab.irstea.fr/HYCAR-Hydro/airgrgalaxy/airgrdatasets",
    "bug_reports": "https://gitlab.irstea.fr/HYCAR-Hydro/airgrgalaxy/airgrdatasets/-/issues",
    "repository": "https://cran.r-project.org/package=airGRdatasets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "airGRdatasets Hydro-Meteorological Catchments Datasets for the 'airGR'\nPackages Sample of hydro-meteorological datasets extracted from the 'CAMELS-FR' French database <doi:10.57745/WH7FJR>. \n  It provides metadata and catchment-scale aggregated hydro-meteorological time series on a pool of French catchments for use by the 'airGR' packages.   "
  },
  {
    "id": 8402,
    "package_name": "airportr",
    "title": "Convenience Tools for Working with Airport Data",
    "description": "Retrieves open source airport data and provides tools to look up information, translate names into codes and vice-verse, as well as some basic calculation functions for measuring distances. Data is licensed under the Open Database License. ",
    "version": "0.1.3",
    "maintainer": "Dmitry Shkolnik <shkolnikd@gmail.com>",
    "author": "Dmitry Shkolnik [cre, aut]",
    "url": "https://github.com/dshkol/airportr",
    "bug_reports": "https://github.com/dshkol/airportr/issues",
    "repository": "https://cran.r-project.org/package=airportr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "airportr Convenience Tools for Working with Airport Data Retrieves open source airport data and provides tools to look up information, translate names into codes and vice-verse, as well as some basic calculation functions for measuring distances. Data is licensed under the Open Database License.   "
  },
  {
    "id": 8423,
    "package_name": "alfred",
    "title": "Downloading Time Series from ALFRED Database for Various\nVintages",
    "description": "Provides direct access to the ALFRED (<https://alfred.stlouisfed.org>) and FRED (<https://fred.stlouisfed.org>) databases.\n    Its functions return tidy data frames for different releases of the specified time series. \n    Note that this product uses the FRED\u00a9 API but is not endorsed or certified by the Federal Reserve Bank of St. Louis.",
    "version": "0.2.1",
    "maintainer": "Onno Kleen <r@onnokleen.de>",
    "author": "Onno Kleen [aut, cre] (ORCID: <https://orcid.org/0000-0003-4731-4640>)",
    "url": "https://github.com/onnokleen/alfred/",
    "bug_reports": "https://github.com/onnokleen/alfred/issues",
    "repository": "https://cran.r-project.org/package=alfred",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "alfred Downloading Time Series from ALFRED Database for Various\nVintages Provides direct access to the ALFRED (<https://alfred.stlouisfed.org>) and FRED (<https://fred.stlouisfed.org>) databases.\n    Its functions return tidy data frames for different releases of the specified time series. \n    Note that this product uses the FRED\u00a9 API but is not endorsed or certified by the Federal Reserve Bank of St. Louis.  "
  },
  {
    "id": 8424,
    "package_name": "algaeClassify",
    "title": "Tools to Query the 'Algaebase' Online Database, Standardize\nPhytoplankton Taxonomic Data, and Perform Functional Group\nClassifications",
    "description": "Functions that facilitate the use of accepted taxonomic nomenclature, collection of\n\tfunctional trait data, and assignment of functional group classifications to phytoplankton\n\tspecies. Possible classifications include Morpho-functional group (MFG; Salmaso et al. 2015 \n\t<doi:10.1111/fwb.12520>) and CSR (Reynolds 1988; Functional morphology and the \n\tadaptive strategies of phytoplankton. In C.D. Sandgren (ed). Growth and reproductive \n\tstrategies of freshwater phytoplankton, 388-433. Cambridge University Press, New York). \n\tVersions 2.0.0 and later includes new functions for querying the \n\t'algaebase' online taxonomic database (www.algaebase.org), however these functions require\n\ta valid API key that must be acquired from the 'algaebase' administrators. \n\tNote that none of the 'algaeClassify' authors are affiliated with 'algaebase' in any way. Taxonomic \n\tnames can also be checked against a variety of taxonomic databases using \n\tthe 'Global Names Resolver' service via its API (<https://resolver.globalnames.org/api>). In addition,\n\tcurrently accepted and outdated synonyms, and higher taxonomy, can be extracted for lists of \n\tspecies from the 'ITIS' database using wrapper functions for the ritis package.\n\tThe 'algaeClassify' package is a product of the GEISHA (Global Evaluation of the Impacts of \n\tStorms on freshwater Habitat and Structure of phytoplankton Assemblages), funded by CESAB \n    (Centre for Synthesis and Analysis of Biodiversity) and the U.S. Geological Survey John Wesley Powell Center for\n\tSynthesis and Analysis, with data and other support provided by members of GLEON \n\t(Global Lake Ecology Observation Network). \n\tDISCLAIMER: This software has been approved for release by the \n\tU.S. Geological Survey (USGS). Although the software has been subjected to rigorous review, \n\tthe USGS reserves the right to update the software as needed pursuant to further analysis and \n\treview. No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the \n\tfunctionality of the software and related material nor shall the fact of release constitute \n\tany such warranty. Furthermore, the software is released on condition that neither the USGS \n\tnor the U.S. Government shall be held liable for any damages resulting from its authorized \n\tor unauthorized use.",
    "version": "2.0.5",
    "maintainer": "Vijay Patil <vij.patil@gmail.com>",
    "author": "Vijay Patil [aut, cre],\n  Torsten Seltmann [aut],\n  Nico Salmaso [aut],\n  Orlane Anneville [aut],\n  Marc Lajeunesse [aut],\n  Dietmar Straile [aut]",
    "url": "https://doi.org/10.5066/F7S46Q3F",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=algaeClassify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "algaeClassify Tools to Query the 'Algaebase' Online Database, Standardize\nPhytoplankton Taxonomic Data, and Perform Functional Group\nClassifications Functions that facilitate the use of accepted taxonomic nomenclature, collection of\n\tfunctional trait data, and assignment of functional group classifications to phytoplankton\n\tspecies. Possible classifications include Morpho-functional group (MFG; Salmaso et al. 2015 \n\t<doi:10.1111/fwb.12520>) and CSR (Reynolds 1988; Functional morphology and the \n\tadaptive strategies of phytoplankton. In C.D. Sandgren (ed). Growth and reproductive \n\tstrategies of freshwater phytoplankton, 388-433. Cambridge University Press, New York). \n\tVersions 2.0.0 and later includes new functions for querying the \n\t'algaebase' online taxonomic database (www.algaebase.org), however these functions require\n\ta valid API key that must be acquired from the 'algaebase' administrators. \n\tNote that none of the 'algaeClassify' authors are affiliated with 'algaebase' in any way. Taxonomic \n\tnames can also be checked against a variety of taxonomic databases using \n\tthe 'Global Names Resolver' service via its API (<https://resolver.globalnames.org/api>). In addition,\n\tcurrently accepted and outdated synonyms, and higher taxonomy, can be extracted for lists of \n\tspecies from the 'ITIS' database using wrapper functions for the ritis package.\n\tThe 'algaeClassify' package is a product of the GEISHA (Global Evaluation of the Impacts of \n\tStorms on freshwater Habitat and Structure of phytoplankton Assemblages), funded by CESAB \n    (Centre for Synthesis and Analysis of Biodiversity) and the U.S. Geological Survey John Wesley Powell Center for\n\tSynthesis and Analysis, with data and other support provided by members of GLEON \n\t(Global Lake Ecology Observation Network). \n\tDISCLAIMER: This software has been approved for release by the \n\tU.S. Geological Survey (USGS). Although the software has been subjected to rigorous review, \n\tthe USGS reserves the right to update the software as needed pursuant to further analysis and \n\treview. No warranty, expressed or implied, is made by the USGS or the U.S. Government as to the \n\tfunctionality of the software and related material nor shall the fact of release constitute \n\tany such warranty. Furthermore, the software is released on condition that neither the USGS \n\tnor the U.S. Government shall be held liable for any damages resulting from its authorized \n\tor unauthorized use.  "
  },
  {
    "id": 8427,
    "package_name": "aliases2entrez",
    "title": "Converts Human gene symbols to entrez IDs",
    "description": "Queries multiple resources authors HGNC (2019) <https://www.genenames.org>, authors limma (2015) <doi:10.1093/nar/gkv007> \n    to find the correspondence between evolving nomenclature of human gene symbols, aliases, previous symbols or synonyms with \n    stable, curated gene entrezID from NCBI database. This allows fast, accurate and up-to-date correspondence\n    between human gene expression datasets from various date and platform (e.g: gene symbol: BRCA1 - ID: 672).",
    "version": "0.1.2",
    "maintainer": "Raphael Bonnet <raphael.bonnet@univ-cotedazur.fr>",
    "author": "Raphael Bonnet [aut, cre] (Universit\u00e9 C\u00f4te d\u2019Azur),\n  Lee Mariault [ctb] (Universit\u00e9 C\u00f4te d\u2019Azur),\n  Jean-Fran\u00e7ois Peyron [aut] (Inserm)",
    "url": "",
    "bug_reports": "https://github.com/peyronlab/aliases2entrez/issues",
    "repository": "https://cran.r-project.org/package=aliases2entrez",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aliases2entrez Converts Human gene symbols to entrez IDs Queries multiple resources authors HGNC (2019) <https://www.genenames.org>, authors limma (2015) <doi:10.1093/nar/gkv007> \n    to find the correspondence between evolving nomenclature of human gene symbols, aliases, previous symbols or synonyms with \n    stable, curated gene entrezID from NCBI database. This allows fast, accurate and up-to-date correspondence\n    between human gene expression datasets from various date and platform (e.g: gene symbol: BRCA1 - ID: 672).  "
  },
  {
    "id": 8432,
    "package_name": "alleHap",
    "title": "Allele Imputation and Haplotype Reconstruction from Pedigree\nDatabases",
    "description": "Tools to simulate alphanumeric alleles, impute genetic missing data and reconstruct non-recombinant haplotypes from pedigree databases in a deterministic way. Allelic simulations can be implemented taking into account many factors (such as number of families, markers, alleles per marker,\n    probability and proportion of missing genotypes, recombination rate, etc).\n    Genotype imputation can be used with simulated datasets or real databases (previously loaded in .ped format). Haplotype reconstruction can be carried\n    out even with missing data, since the program firstly imputes each family genotype (without a reference panel), to later reconstruct the corresponding\n    haplotypes for each family member. All this considering that each individual (due to meiosis) should unequivocally have two alleles per marker (one inherited\n    from each parent) and thus imputation and reconstruction results can be deterministically calculated.",
    "version": "0.9.9",
    "maintainer": "Nathan Medina-Rodriguez <nathan.medina@ulpgc.es>",
    "author": "Nathan Medina-Rodriguez and Angelo Santana",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=alleHap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "alleHap Allele Imputation and Haplotype Reconstruction from Pedigree\nDatabases Tools to simulate alphanumeric alleles, impute genetic missing data and reconstruct non-recombinant haplotypes from pedigree databases in a deterministic way. Allelic simulations can be implemented taking into account many factors (such as number of families, markers, alleles per marker,\n    probability and proportion of missing genotypes, recombination rate, etc).\n    Genotype imputation can be used with simulated datasets or real databases (previously loaded in .ped format). Haplotype reconstruction can be carried\n    out even with missing data, since the program firstly imputes each family genotype (without a reference panel), to later reconstruct the corresponding\n    haplotypes for each family member. All this considering that each individual (due to meiosis) should unequivocally have two alleles per marker (one inherited\n    from each parent) and thus imputation and reconstruction results can be deterministically calculated.  "
  },
  {
    "id": 8433,
    "package_name": "allelematch",
    "title": "Identifying Unique Multilocus Genotypes where Genotyping Error\nand Missing Data may be Present",
    "description": "Tools for the identification of unique of multilocus genotypes when both genotyping error and missing data may be present; targeted for use with large datasets and databases containing multiple samples of each individual (a common situation in conservation genetics, particularly in non-invasive wildlife sampling applications). Functions explicitly incorporate missing data and can tolerate allele mismatches created by genotyping error. If you use this package, please cite the original publication in Molecular Ecology Resources (Galpern et al., 2012), the details for which can be generated using citation('allelematch'). For a complete vignette, please access via the Data S1 Supplementary documentation and tutorials (PDF) located at <doi:10.1111/j.1755-0998.2012.03137.x>.",
    "version": "2.5.5",
    "maintainer": "Todd Cross <todd.cross@gmail.com>",
    "author": "Paul Galpern [aut],\n  Micheline Manseau [aut],\n  Pete Hettinga [aut],\n  Karen Smith [aut],\n  Paul Wilson [aut],\n  Todd Cross [cre]",
    "url": "<doi:10.1111%2Fj.1755-0998.2012.03137.x>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=allelematch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "allelematch Identifying Unique Multilocus Genotypes where Genotyping Error\nand Missing Data may be Present Tools for the identification of unique of multilocus genotypes when both genotyping error and missing data may be present; targeted for use with large datasets and databases containing multiple samples of each individual (a common situation in conservation genetics, particularly in non-invasive wildlife sampling applications). Functions explicitly incorporate missing data and can tolerate allele mismatches created by genotyping error. If you use this package, please cite the original publication in Molecular Ecology Resources (Galpern et al., 2012), the details for which can be generated using citation('allelematch'). For a complete vignette, please access via the Data S1 Supplementary documentation and tutorials (PDF) located at <doi:10.1111/j.1755-0998.2012.03137.x>.  "
  },
  {
    "id": 8436,
    "package_name": "allofus",
    "title": "Interface for 'All of Us' Researcher Workbench",
    "description": "Streamline use of the 'All of Us' Researcher Workbench (<https://www.researchallofus.org/data-tools/workbench/>)with tools to extract and manipulate data from the 'All of Us' database. Increase interoperability with the Observational Health Data Science and Informatics ('OHDSI') tool stack by decreasing reliance of 'All of Us' tools and allowing for cohort creation via 'Atlas'. Improve reproducible and transparent research using 'All of Us'.",
    "version": "1.2.0",
    "maintainer": "Rob Cavanaugh <r.cavanaugh@northeastern.edu>",
    "author": "Louisa Smith [aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-9029-4644>),\n  Rob Cavanaugh [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2114-6565>)",
    "url": "https://roux-ohdsi.github.io/allofus/,\nhttps://github.com/roux-ohdsi/allofus",
    "bug_reports": "https://github.com/roux-ohdsi/allofus/issues",
    "repository": "https://cran.r-project.org/package=allofus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "allofus Interface for 'All of Us' Researcher Workbench Streamline use of the 'All of Us' Researcher Workbench (<https://www.researchallofus.org/data-tools/workbench/>)with tools to extract and manipulate data from the 'All of Us' database. Increase interoperability with the Observational Health Data Science and Informatics ('OHDSI') tool stack by decreasing reliance of 'All of Us' tools and allowing for cohort creation via 'Atlas'. Improve reproducible and transparent research using 'All of Us'.  "
  },
  {
    "id": 8460,
    "package_name": "altfuelr",
    "title": "Provides an Interface to the NREL Alternate Fuels Locator",
    "description": "Provides a number of functions to access the \n    National Energy Research Laboratory Alternate Fuel Locator API \n    <https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1/>. \n    The Alternate Fuel Locator shows the location of alternate fuel stations in the United States \n    and Canada. This package also includes the data from the US Department of Energy Alternate Fuel\n    database as a data set.",
    "version": "0.1.0",
    "maintainer": "Christopher Burch <christopher.m.burch@gmail.com>",
    "author": "Christopher Burch [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6934-3325>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=altfuelr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "altfuelr Provides an Interface to the NREL Alternate Fuels Locator Provides a number of functions to access the \n    National Energy Research Laboratory Alternate Fuel Locator API \n    <https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1/>. \n    The Alternate Fuel Locator shows the location of alternate fuel stations in the United States \n    and Canada. This package also includes the data from the US Department of Energy Alternate Fuel\n    database as a data set.  "
  },
  {
    "id": 8490,
    "package_name": "anabel",
    "title": "Analysis of Binding Events + l",
    "description": "A free software for a fast and easy analysis of 1:1 molecular interaction studies. \n   This package is suitable for a high-throughput data analysis. \n   Both the online app and the package are completely open source. \n   You provide a table of sensogram, tell 'anabel' which method to use,\n   and it takes care of all fitting details.\n   The first two releases of 'anabel' were created and implemented as in \n   (<doi:10.1177/1177932218821383>, <doi:10.1093/database/baz101>).",
    "version": "3.0.2",
    "maintainer": "Stefan Kraemer <stefan.kraemer.91@gmail.com>",
    "author": "Hoor Al-Hasani [aut] (ORCID: <https://orcid.org/0000-0002-0431-845X>),\n  Oliver Selinger [aut] (ORCID: <https://orcid.org/0000-0001-9723-2809>),\n  Stefan Kraemer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0071-9344>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=anabel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "anabel Analysis of Binding Events + l A free software for a fast and easy analysis of 1:1 molecular interaction studies. \n   This package is suitable for a high-throughput data analysis. \n   Both the online app and the package are completely open source. \n   You provide a table of sensogram, tell 'anabel' which method to use,\n   and it takes care of all fitting details.\n   The first two releases of 'anabel' were created and implemented as in \n   (<doi:10.1177/1177932218821383>, <doi:10.1093/database/baz101>).  "
  },
  {
    "id": 8537,
    "package_name": "aoristic",
    "title": "Generates Aoristic Probability Distributions",
    "description": "It can sometimes be difficult to ascertain when some events (such as property crime)\n    occur because the victim is not present when the crime happens. As a result, police databases often\n    record a 'start' (or 'from') date and time, and an 'end' (or 'to') date and time. The time span between\n    these date/times can be minutes, hours, or sometimes days, hence the term 'Aoristic'. \n    Aoristic is one of the past tenses in Greek and represents an uncertain occurrence in time. \n    For events with a location describes with either a latitude/longitude, or X,Y coordinate pair, \n    and a start and end date/time, this package generates an aoristic data frame with aoristic weighted\n    probability values for each hour of the week, for each observation. The coordinates are not \n    necessary for the program to calculate aoristic weights; however, they are part of this package \n    because a spatial component has been integral to aoristic analysis from the start. Dummy \n    coordinates can be introduced if the user only has temporal data. Outputs include an aoristic \n    data frame, as well as summary graphs and displays.   \n        For more information see:\n    Ratcliffe, JH (2002) Aoristic signatures and the temporal analysis of high volume crime patterns, \n    Journal of Quantitative Criminology. 18 (1): 23-43.\n    Note: This package replaces an original 'aoristic' package (version 0.6) by George Kikuchi that \n    has been discontinued with his permission. ",
    "version": "1.1.1",
    "maintainer": "Jerry Ratcliffe <jhr@temple.edu>",
    "author": "Jerry Ratcliffe",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aoristic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aoristic Generates Aoristic Probability Distributions It can sometimes be difficult to ascertain when some events (such as property crime)\n    occur because the victim is not present when the crime happens. As a result, police databases often\n    record a 'start' (or 'from') date and time, and an 'end' (or 'to') date and time. The time span between\n    these date/times can be minutes, hours, or sometimes days, hence the term 'Aoristic'. \n    Aoristic is one of the past tenses in Greek and represents an uncertain occurrence in time. \n    For events with a location describes with either a latitude/longitude, or X,Y coordinate pair, \n    and a start and end date/time, this package generates an aoristic data frame with aoristic weighted\n    probability values for each hour of the week, for each observation. The coordinates are not \n    necessary for the program to calculate aoristic weights; however, they are part of this package \n    because a spatial component has been integral to aoristic analysis from the start. Dummy \n    coordinates can be introduced if the user only has temporal data. Outputs include an aoristic \n    data frame, as well as summary graphs and displays.   \n        For more information see:\n    Ratcliffe, JH (2002) Aoristic signatures and the temporal analysis of high volume crime patterns, \n    Journal of Quantitative Criminology. 18 (1): 23-43.\n    Note: This package replaces an original 'aoristic' package (version 0.6) by George Kikuchi that \n    has been discontinued with his permission.   "
  },
  {
    "id": 8552,
    "package_name": "aphid",
    "title": "Analysis with Profile Hidden Markov Models",
    "description": "Designed for the development and application of\n    hidden Markov models and profile HMMs for biological sequence analysis. \n    Contains functions for multiple and pairwise sequence alignment, \n    model construction and parameter optimization, file import/export,\n    implementation of the forward, backward and Viterbi algorithms for \n    conditional sequence probabilities, tree-based sequence weighting, \n    and sequence simulation. \n    Features a wide variety of potential applications including \n    database searching, gene-finding and annotation, phylogenetic \n    analysis and sequence classification.\n    Based on the models and algorithms described in Durbin et \n    al (1998, ISBN: 9780521629713).",
    "version": "1.3.5",
    "maintainer": "Shaun Wilkinson <shaunpwilkinson@gmail.com>",
    "author": "Shaun Wilkinson [aut, cre]",
    "url": "https://github.com/shaunpwilkinson/aphid",
    "bug_reports": "https://github.com/shaunpwilkinson/aphid/issues",
    "repository": "https://cran.r-project.org/package=aphid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aphid Analysis with Profile Hidden Markov Models Designed for the development and application of\n    hidden Markov models and profile HMMs for biological sequence analysis. \n    Contains functions for multiple and pairwise sequence alignment, \n    model construction and parameter optimization, file import/export,\n    implementation of the forward, backward and Viterbi algorithms for \n    conditional sequence probabilities, tree-based sequence weighting, \n    and sequence simulation. \n    Features a wide variety of potential applications including \n    database searching, gene-finding and annotation, phylogenetic \n    analysis and sequence classification.\n    Based on the models and algorithms described in Durbin et \n    al (1998, ISBN: 9780521629713).  "
  },
  {
    "id": 8587,
    "package_name": "arakno",
    "title": "ARAchnid KNowledge Online",
    "description": "Allows the user to connect with the World Spider Catalogue (WSC; <https://wsc.nmbe.ch/>) and the World Spider Trait (WST; <https://spidertraits.sci.muni.cz/>) databases. Also performs several basic functions such as checking names validity, retrieving coordinate data from the Global Biodiversity Information Facility (GBIF; <https://www.gbif.org/>), and mapping.",
    "version": "1.3.1",
    "maintainer": "Pedro Cardoso <pmcardoso@ciencias.ulisboa.pt>",
    "author": "Pedro Cardoso [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=arakno",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arakno ARAchnid KNowledge Online Allows the user to connect with the World Spider Catalogue (WSC; <https://wsc.nmbe.ch/>) and the World Spider Trait (WST; <https://spidertraits.sci.muni.cz/>) databases. Also performs several basic functions such as checking names validity, retrieving coordinate data from the Global Biodiversity Information Facility (GBIF; <https://www.gbif.org/>), and mapping.  "
  },
  {
    "id": 8614,
    "package_name": "arealDB",
    "title": "Harmonise and Integrate Heterogeneous Areal Data",
    "description": "Many relevant applications in the environmental and socioeconomic \n    sciences use areal data, such as biodiversity checklists, agricultural statistics, \n    or socioeconomic surveys. For applications that surpass the spatial, temporal or \n    thematic scope of any single data source, data must be integrated from several \n    heterogeneous sources. Inconsistent concepts, definitions, or messy data tables \n    make this a tedious and error-prone process. 'arealDB' tackles those problems and \n    helps the user to integrate a harmonised databases of areal data. Read the paper\n    at Ehrmann, Seppelt & Meyer (2020) <doi:10.1016/j.envsoft.2020.104799>.",
    "version": "0.9.4",
    "maintainer": "Steffen Ehrmann <steffen.ehrmann@posteo.de>",
    "author": "Steffen Ehrmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2958-0796>),\n  Arne R\u00fcmmler [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-8637-9071>),\n  Felipe Melges [ctb] (ORCID: <https://orcid.org/0000-0003-0833-8973>),\n  Carsten Meyer [aut] (ORCID: <https://orcid.org/0000-0003-3927-5856>)",
    "url": "https://github.com/luckinet/arealDB",
    "bug_reports": "https://github.com/luckinet/arealDB/issues",
    "repository": "https://cran.r-project.org/package=arealDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "arealDB Harmonise and Integrate Heterogeneous Areal Data Many relevant applications in the environmental and socioeconomic \n    sciences use areal data, such as biodiversity checklists, agricultural statistics, \n    or socioeconomic surveys. For applications that surpass the spatial, temporal or \n    thematic scope of any single data source, data must be integrated from several \n    heterogeneous sources. Inconsistent concepts, definitions, or messy data tables \n    make this a tedious and error-prone process. 'arealDB' tackles those problems and \n    helps the user to integrate a harmonised databases of areal data. Read the paper\n    at Ehrmann, Seppelt & Meyer (2020) <doi:10.1016/j.envsoft.2020.104799>.  "
  },
  {
    "id": 8736,
    "package_name": "authoritative",
    "title": "Parse and Deduplicate Author Names",
    "description": "Utilities to parse authors fields from DESCRIPTION files and\n    general purpose functions to deduplicate names in database, beyond the\n    specific case of R package authors.",
    "version": "0.2.0",
    "maintainer": "Hugo Gruson <hugo.gruson+R@normalesup.org>",
    "author": "Hugo Gruson [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4094-1476>),\n  Chris Hartgerink [rev] (ORCID: <https://orcid.org/0000-0003-1050-6809>),\n  data.org [fnd] (until version 0.2.0 included)",
    "url": "https://github.com/Bisaloo/authoritative,\nhttps://hugogruson.fr/authoritative/",
    "bug_reports": "https://github.com/Bisaloo/authoritative/issues",
    "repository": "https://cran.r-project.org/package=authoritative",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "authoritative Parse and Deduplicate Author Names Utilities to parse authors fields from DESCRIPTION files and\n    general purpose functions to deduplicate names in database, beyond the\n    specific case of R package authors.  "
  },
  {
    "id": 8749,
    "package_name": "autodb",
    "title": "Automatic Database Normalisation for Data Frames",
    "description": "Automatic normalisation of a data frame to third normal form, with\n  the intention of easing the process of data cleaning. (Usage to design your\n  actual database for you is not advised.)\n  Originally inspired by the 'AutoNormalize' library for 'Python' by 'Alteryx'\n  (<https://github.com/alteryx/autonormalize>), with various changes and\n  improvements. Automatic discovery of functional or approximate dependencies,\n  normalisation based on those, and plotting of the resulting \"database\" via\n  'Graphviz', with options to exclude some attributes at discovery time, or\n  remove discovered dependencies at normalisation time.",
    "version": "3.2.4",
    "maintainer": "Mark Webster <markwebster204@yahoo.co.uk>",
    "author": "Mark Webster [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3351-0686>)",
    "url": "https://charnelmouse.github.io/autodb/,\nhttps://github.com/CharnelMouse/autodb",
    "bug_reports": "https://github.com/CharnelMouse/autodb/issues",
    "repository": "https://cran.r-project.org/package=autodb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "autodb Automatic Database Normalisation for Data Frames Automatic normalisation of a data frame to third normal form, with\n  the intention of easing the process of data cleaning. (Usage to design your\n  actual database for you is not advised.)\n  Originally inspired by the 'AutoNormalize' library for 'Python' by 'Alteryx'\n  (<https://github.com/alteryx/autonormalize>), with various changes and\n  improvements. Automatic discovery of functional or approximate dependencies,\n  normalisation based on those, and plotting of the resulting \"database\" via\n  'Graphviz', with options to exclude some attributes at discovery time, or\n  remove discovered dependencies at normalisation time.  "
  },
  {
    "id": 8775,
    "package_name": "avidaR",
    "title": "A Computational Biologist\u2019s Toolkit To Get Data From 'avidaDB'",
    "description": "Easy-to-use tools for performing complex queries on 'avidaDB', a\n  semantic database that stores genomic and transcriptomic data of\n  self-replicating computer programs (known as digital organisms) that mutate\n  and evolve within a user-defined computational environment.",
    "version": "1.2.1",
    "maintainer": "Ra\u00fal Ortega <raul.ortega@ebd.csic.es>",
    "author": "Miguel A. Fortuna [aut] (ORCID:\n    <https://orcid.org/0000-0002-8374-1941>),\n  Ra\u00fal Ortega [cre] (ORCID: <https://orcid.org/0000-0002-1306-5378>)",
    "url": "https://gitlab.com/fortunalab/avidaR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=avidaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "avidaR A Computational Biologist\u2019s Toolkit To Get Data From 'avidaDB' Easy-to-use tools for performing complex queries on 'avidaDB', a\n  semantic database that stores genomic and transcriptomic data of\n  self-replicating computer programs (known as digital organisms) that mutate\n  and evolve within a user-defined computational environment.  "
  },
  {
    "id": 8779,
    "package_name": "avotrex",
    "title": "A Global Dataset of Anthropogenic Extinct Birds and their\nTraits: Phylogeny Builder",
    "description": "Grafts the extinct bird species from the 'Avotrex' database (Sayol et al., in review) on to the 'BirdTree' phylogenies <https://birdtree.org>, using a set of different commands.",
    "version": "1.3.0",
    "maintainer": "Joseph Wayman <j.wayman@bham.ac.uk>",
    "author": "Joseph Wayman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3122-8070>),\n  Thomas J. Matthews [aut] (ORCID:\n    <https://orcid.org/0000-0002-7624-244X>),\n  Paul Dufour [ctb],\n  Ferran Sayol [ctb],\n  Pedro Cardoso [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=avotrex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "avotrex A Global Dataset of Anthropogenic Extinct Birds and their\nTraits: Phylogeny Builder Grafts the extinct bird species from the 'Avotrex' database (Sayol et al., in review) on to the 'BirdTree' phylogenies <https://birdtree.org>, using a set of different commands.  "
  },
  {
    "id": 8781,
    "package_name": "awdb",
    "title": "Query the USDA NWCC Air and Water Database REST API",
    "description": "Query the four endpoints of the 'Air and Water Database (AWDB) REST\n    API' maintained by the National Water and Climate Center (NWCC) at the \n    United States Department of Agriculture (USDA). Endpoints include data, \n    forecast, reference-data, and metadata. The package is extremely light \n    weight, with 'Rust' via 'extendr' doing most of the heavy lifting to \n    deserialize and flatten deeply nested 'JSON' responses. The AWDB can be \n    found at <https://wcc.sc.egov.usda.gov/awdbRestApi/swagger-ui/index.html>.",
    "version": "0.1.3",
    "maintainer": "Kenneth Blake Vernon <kenneth.b.vernon@gmail.com>",
    "author": "Kenneth Blake Vernon [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0098-5092>)",
    "url": "https://github.com/kbvernon/awdb, https://kbvernon.github.io/awdb/",
    "bug_reports": "https://github.com/kbvernon/awdb/issues",
    "repository": "https://cran.r-project.org/package=awdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "awdb Query the USDA NWCC Air and Water Database REST API Query the four endpoints of the 'Air and Water Database (AWDB) REST\n    API' maintained by the National Water and Climate Center (NWCC) at the \n    United States Department of Agriculture (USDA). Endpoints include data, \n    forecast, reference-data, and metadata. The package is extremely light \n    weight, with 'Rust' via 'extendr' doing most of the heavy lifting to \n    deserialize and flatten deeply nested 'JSON' responses. The AWDB can be \n    found at <https://wcc.sc.egov.usda.gov/awdbRestApi/swagger-ui/index.html>.  "
  },
  {
    "id": 8809,
    "package_name": "babelgene",
    "title": "Gene Orthologs for Model Organisms in a Tidy Data Format",
    "description": "Genomic analysis of model organisms frequently requires the\n    use of databases based on human data or making comparisons to\n    patient-derived resources. This requires harmonization of gene names\n    into the same gene space. The 'babelgene' R package converts between\n    human and non-human gene orthologs/homologs. The package integrates\n    orthology assertion predictions sourced from multiple databases as\n    compiled by the HGNC Comparison of Orthology Predictions (HCOP)\n    (Wright et al. 2005 <doi:10.1007/s00335-005-0103-2>, Eyre et al. 2007\n    <doi:10.1093/bib/bbl030>, Seal et al. 2011 <doi:10.1093/nar/gkq892>).",
    "version": "22.9",
    "maintainer": "Igor Dolgalev <igor.dolgalev@nyumc.org>",
    "author": "Igor Dolgalev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4451-126X>)",
    "url": "https://igordot.github.io/babelgene/",
    "bug_reports": "https://github.com/igordot/babelgene/issues",
    "repository": "https://cran.r-project.org/package=babelgene",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "babelgene Gene Orthologs for Model Organisms in a Tidy Data Format Genomic analysis of model organisms frequently requires the\n    use of databases based on human data or making comparisons to\n    patient-derived resources. This requires harmonization of gene names\n    into the same gene space. The 'babelgene' R package converts between\n    human and non-human gene orthologs/homologs. The package integrates\n    orthology assertion predictions sourced from multiple databases as\n    compiled by the HGNC Comparison of Orthology Predictions (HCOP)\n    (Wright et al. 2005 <doi:10.1007/s00335-005-0103-2>, Eyre et al. 2007\n    <doi:10.1093/bib/bbl030>, Seal et al. 2011 <doi:10.1093/nar/gkq892>).  "
  },
  {
    "id": 8967,
    "package_name": "bbk",
    "title": "Client for Central Bank APIs",
    "description": "A client for retrieving data and metadata from major central\n    bank APIs. It supports access to the 'Bundesbank SDMX Web Service API'\n    (<https://www.bundesbank.de/en/statistics/time-series-databases/help-for-sdmx-web-service/web-service-interface-data>),\n    the 'Swiss National Bank Data Portal' (<https://data.snb.ch/en>), the\n    'European Central Bank Data Portal API'\n    (<https://data.ecb.europa.eu/help/api/overview>), the 'Bank of England\n    Interactive Statistical Database'\n    (<https://www.bankofengland.co.uk/boeapps/database>), the 'Banco de\n    Espa\u00f1a API'\n    (<https://www.bde.es/webbe/en/estadisticas/recursos/api-estadisticas-bde.html>),\n    the 'Banque de France Web Service'\n    (<https://webstat.banque-france.fr/en/pages/guide-migration-api/>),\n    and 'Bank of Canada Valet API'\n    (<https://www.bankofcanada.ca/valet/docs>).",
    "version": "0.8.0",
    "maintainer": "Maximilian M\u00fccke <muecke.maximilian@gmail.com>",
    "author": "Maximilian M\u00fccke [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-9432-9795>)",
    "url": "https://m-muecke.github.io/bbk/, https://github.com/m-muecke/bbk",
    "bug_reports": "https://github.com/m-muecke/bbk/issues",
    "repository": "https://cran.r-project.org/package=bbk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bbk Client for Central Bank APIs A client for retrieving data and metadata from major central\n    bank APIs. It supports access to the 'Bundesbank SDMX Web Service API'\n    (<https://www.bundesbank.de/en/statistics/time-series-databases/help-for-sdmx-web-service/web-service-interface-data>),\n    the 'Swiss National Bank Data Portal' (<https://data.snb.ch/en>), the\n    'European Central Bank Data Portal API'\n    (<https://data.ecb.europa.eu/help/api/overview>), the 'Bank of England\n    Interactive Statistical Database'\n    (<https://www.bankofengland.co.uk/boeapps/database>), the 'Banco de\n    Espa\u00f1a API'\n    (<https://www.bde.es/webbe/en/estadisticas/recursos/api-estadisticas-bde.html>),\n    the 'Banque de France Web Service'\n    (<https://webstat.banque-france.fr/en/pages/guide-migration-api/>),\n    and 'Bank of Canada Valet API'\n    (<https://www.bankofcanada.ca/valet/docs>).  "
  },
  {
    "id": 8988,
    "package_name": "bcputility",
    "title": "Wrapper for SQL Server bcp Utility",
    "description": "Provides functions to utilize a command line utility that does bulk inserts and exports from SQL Server databases. ",
    "version": "0.4.6",
    "maintainer": "Thomas Roh <thomas.roh@delveds.com>",
    "author": "Thomas Roh [aut, cre]",
    "url": "https://bcputility.delveds.com,\nhttps://github.com/tomroh/bcputility",
    "bug_reports": "https://github.com/tomroh/bcputility/issues",
    "repository": "https://cran.r-project.org/package=bcputility",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bcputility Wrapper for SQL Server bcp Utility Provides functions to utilize a command line utility that does bulk inserts and exports from SQL Server databases.   "
  },
  {
    "id": 8994,
    "package_name": "bdc",
    "title": "Biodiversity Data Cleaning",
    "description": "It brings together several aspects of biodiversity\n    data-cleaning in one place. 'bdc' is organized in thematic modules\n    related to different biodiversity dimensions, including 1) Merge\n    datasets: standardization and integration of different datasets; 2)\n    Pre-filter: flagging and removal of invalid or non-interpretable\n    information, followed by data amendments; 3) Taxonomy: cleaning,\n    parsing, and harmonization of scientific names from several taxonomic\n    groups against taxonomic databases locally stored through the\n    application of exact and partial matching algorithms; 4) Space:\n    flagging of erroneous, suspect, and low-precision geographic\n    coordinates; and 5) Time: flagging and, whenever possible, correction\n    of inconsistent collection date. In addition, it contains\n    features to visualize, document, and report data quality \u2013 which is\n    essential for making data quality assessment transparent and\n    reproducible. The reference for the methodology is Bruno et al. (2022)\n    <doi:10.1111/2041-210X.13868>.",
    "version": "1.1.5",
    "maintainer": "Bruno Ribeiro <ribeiro.brr@gmail.com>",
    "author": "Bruno Ribeiro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7755-6715>),\n  Santiago Velazco [aut] (ORCID: <https://orcid.org/0000-0002-7527-0967>),\n  Karlo Guidoni-Martins [aut] (ORCID:\n    <https://orcid.org/0000-0002-8458-8467>),\n  Geiziane Tessarolo [aut] (ORCID:\n    <https://orcid.org/0000-0003-1361-0062>),\n  Lucas Jardim [aut] (ORCID: <https://orcid.org/0000-0003-2602-5575>),\n  Steven Bachman [ctb] (ORCID: <https://orcid.org/0000-0003-1085-6075>),\n  Rafael Loyola [ctb] (ORCID: <https://orcid.org/0000-0001-5323-2735>)",
    "url": "https://brunobrr.github.io/bdc/ (website)\nhttps://github.com/brunobrr/bdc",
    "bug_reports": "https://github.com/brunobrr/bdc/issues",
    "repository": "https://cran.r-project.org/package=bdc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bdc Biodiversity Data Cleaning It brings together several aspects of biodiversity\n    data-cleaning in one place. 'bdc' is organized in thematic modules\n    related to different biodiversity dimensions, including 1) Merge\n    datasets: standardization and integration of different datasets; 2)\n    Pre-filter: flagging and removal of invalid or non-interpretable\n    information, followed by data amendments; 3) Taxonomy: cleaning,\n    parsing, and harmonization of scientific names from several taxonomic\n    groups against taxonomic databases locally stored through the\n    application of exact and partial matching algorithms; 4) Space:\n    flagging of erroneous, suspect, and low-precision geographic\n    coordinates; and 5) Time: flagging and, whenever possible, correction\n    of inconsistent collection date. In addition, it contains\n    features to visualize, document, and report data quality \u2013 which is\n    essential for making data quality assessment transparent and\n    reproducible. The reference for the methodology is Bruno et al. (2022)\n    <doi:10.1111/2041-210X.13868>.  "
  },
  {
    "id": 9089,
    "package_name": "biblio",
    "title": "Interacting with BibTeX Databases",
    "description": "Reading and writing BibTeX files using data frames in R sessions.",
    "version": "0.0.12",
    "maintainer": "Miguel Alvarez <kamapu78@gmail.com>",
    "author": "Miguel Alvarez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1500-1834>)",
    "url": "https://kamapu.github.io/biblio/",
    "bug_reports": "https://github.com/kamapu/biblio/issues",
    "repository": "https://cran.r-project.org/package=biblio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biblio Interacting with BibTeX Databases Reading and writing BibTeX files using data frames in R sessions.  "
  },
  {
    "id": 9090,
    "package_name": "bibliometrixData",
    "title": "Bibliometrix Example Datasets",
    "description": "It contains some example datasets used in 'bibliometrix'. The data are bibliographic datasets exported from the 'SCOPUS' (<https://scopus.com>) and 'Clarivate Analytics Web of Science' (<https://www.webofscience.com/>) databases. They can be used to test the different features of the package 'bibliometrix' (<https://bibliometrix.org>). ",
    "version": "0.3.0",
    "maintainer": "Massimo Aria <massimo.aria@gmail.com>",
    "author": "Massimo Aria [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8517-9411>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bibliometrixData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bibliometrixData Bibliometrix Example Datasets It contains some example datasets used in 'bibliometrix'. The data are bibliographic datasets exported from the 'SCOPUS' (<https://scopus.com>) and 'Clarivate Analytics Web of Science' (<https://www.webofscience.com/>) databases. They can be used to test the different features of the package 'bibliometrix' (<https://bibliometrix.org>).   "
  },
  {
    "id": 9092,
    "package_name": "bibliorefer",
    "title": "Generator of Main Scientific References",
    "description": "Generates a list, with a size defined by the user, containing the main scientific references and the frequency distribution of authors and journals in the list obtained.\n    The database is a dataframe with academic production metadata made available by bibliographic collections such as Scopus, Web of Science, etc.\n    The temporal evolution of scientific production on a given topic is presented and ordered lists of articles are constructed by number of citations and of authors and journals by level of productivity.\n    Massimo Aria, Corrado Cuccurullo. (2017) <doi:10.1016/j.joi.2017.08.007>.\n    Caibo Zhou, Wenyan Song. (2021) <doi:10.1016/j.jclepro.2021.126943>.",
    "version": "0.1.2",
    "maintainer": "M\u00e1rcio Eust\u00e1quio <marcioeustaquio@id.uff.br>",
    "author": "M\u00e1rcio Eust\u00e1quio [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bibliorefer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bibliorefer Generator of Main Scientific References Generates a list, with a size defined by the user, containing the main scientific references and the frequency distribution of authors and journals in the list obtained.\n    The database is a dataframe with academic production metadata made available by bibliographic collections such as Scopus, Web of Science, etc.\n    The temporal evolution of scientific production on a given topic is presented and ordered lists of articles are constructed by number of citations and of authors and journals by level of productivity.\n    Massimo Aria, Corrado Cuccurullo. (2017) <doi:10.1016/j.joi.2017.08.007>.\n    Caibo Zhou, Wenyan Song. (2021) <doi:10.1016/j.jclepro.2021.126943>.  "
  },
  {
    "id": 9127,
    "package_name": "bigrquery",
    "title": "An Interface to Google's 'BigQuery' 'API'",
    "description": "Easily talk to Google's 'BigQuery' database from R.",
    "version": "1.6.1",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4757-117X>),\n  Jennifer Bryan [aut] (ORCID: <https://orcid.org/0000-0002-6983-2759>),\n  Posit Software, PBC [cph, fnd]",
    "url": "https://bigrquery.r-dbi.org, https://github.com/r-dbi/bigrquery",
    "bug_reports": "https://github.com/r-dbi/bigrquery/issues",
    "repository": "https://cran.r-project.org/package=bigrquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigrquery An Interface to Google's 'BigQuery' 'API' Easily talk to Google's 'BigQuery' database from R.  "
  },
  {
    "id": 9176,
    "package_name": "bio3d",
    "title": "Biological Structure Analysis",
    "description": "Utilities to process, organize and explore protein structure,\n    sequence and dynamics data. Features include the ability to read and write\n    structure, sequence and dynamic trajectory data, perform sequence and structure\n    database searches, data summaries, atom selection, alignment, superposition,\n    rigid core identification, clustering, torsion analysis, distance matrix\n    analysis, structure and sequence conservation analysis, normal mode analysis,\n    principal component analysis of heterogeneous structure data, and correlation\n    network analysis from normal mode and molecular dynamics data. In addition,\n    various utility functions are provided to enable the statistical and graphical\n    power of the R environment to work with biological sequence and structural data.\n    Please refer to the URLs below for more information.",
    "version": "2.4-5",
    "maintainer": "Barry Grant <bjgrant@ucsd.edu>",
    "author": "Barry Grant [aut, cre],\n  Xin-Qiu Yao [aut],\n  Lars Skjaerven [aut],\n  Julien Ide [aut]",
    "url": "http://thegrantlab.org/bio3d/,\nhttps://bitbucket.org/Grantlab/bio3d/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bio3d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bio3d Biological Structure Analysis Utilities to process, organize and explore protein structure,\n    sequence and dynamics data. Features include the ability to read and write\n    structure, sequence and dynamic trajectory data, perform sequence and structure\n    database searches, data summaries, atom selection, alignment, superposition,\n    rigid core identification, clustering, torsion analysis, distance matrix\n    analysis, structure and sequence conservation analysis, normal mode analysis,\n    principal component analysis of heterogeneous structure data, and correlation\n    network analysis from normal mode and molecular dynamics data. In addition,\n    various utility functions are provided to enable the statistical and graphical\n    power of the R environment to work with biological sequence and structural data.\n    Please refer to the URLs below for more information.  "
  },
  {
    "id": 9190,
    "package_name": "biolink",
    "title": "Create Hyperlinks to Biological Databases and Resources",
    "description": "Generate urls and hyperlinks to commonly used biological databases\n    and resources based on standard identifiers. This is primarily useful when\n    writing dynamic reports that reference things like gene symbols in text or\n    tables, allowing you to, for example, convert gene identifiers to hyperlinks\n    pointing to their entry in the 'NCBI' Gene database. Currently supports\n    'NCBI' Gene, 'PubMed', Gene Ontology, 'KEGG', CRAN and Bioconductor.",
    "version": "0.1.8",
    "maintainer": "Aaron Wolen <aaron@wolen.com>",
    "author": "Aaron Wolen [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biolink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biolink Create Hyperlinks to Biological Databases and Resources Generate urls and hyperlinks to commonly used biological databases\n    and resources based on standard identifiers. This is primarily useful when\n    writing dynamic reports that reference things like gene symbols in text or\n    tables, allowing you to, for example, convert gene identifiers to hyperlinks\n    pointing to their entry in the 'NCBI' Gene database. Currently supports\n    'NCBI' Gene, 'PubMed', Gene Ontology, 'KEGG', CRAN and Bioconductor.  "
  },
  {
    "id": 9218,
    "package_name": "birdscanR",
    "title": "Migration Traffic Rate Calculation Package for 'Birdscan MR1'\nRadars",
    "description": "Extract data from 'Birdscan MR1' 'SQL' vertical-looking radar databases, filter, and process them to Migration Traffic Rates (#objects per hour and km) or density (#objects per km3) of, for example birds, and insects. Object classifications in the 'Birdscan MR1' databases are based on the dataset of Haest et al. (2021) <doi:10.5281/zenodo.5734960>). Migration Traffic Rates and densities can be calculated separately for different height bins (with a height resolution of choice) as well as over time periods of choice (e.g., 1/2 hour, 1 hour, 1 day, day/night, the full time period of observation, and anything in between). Two plotting functions are also included to explore the data in the 'SQL' databases and the resulting Migration Traffic Rate results. For details on the Migration Traffic Rate calculation procedures, see Schmid et al. (2019) <doi:10.1111/ecog.04025>.",
    "version": "0.3.0",
    "maintainer": "Birgen Haest <birgen.haest@vogelwarte.ch>",
    "author": "Birgen Haest [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8739-6460>),\n  Fabian Hertner [aut],\n  Baptiste Schmid [ctb],\n  Damiano Preatoni [ctb],\n  Johannes De Groeve [ctb],\n  Felix Liechti [ctb]",
    "url": "https://github.com/BirdScanCommunity/birdscanR",
    "bug_reports": "https://github.com/BirdScanCommunity/birdscanR/issues",
    "repository": "https://cran.r-project.org/package=birdscanR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "birdscanR Migration Traffic Rate Calculation Package for 'Birdscan MR1'\nRadars Extract data from 'Birdscan MR1' 'SQL' vertical-looking radar databases, filter, and process them to Migration Traffic Rates (#objects per hour and km) or density (#objects per km3) of, for example birds, and insects. Object classifications in the 'Birdscan MR1' databases are based on the dataset of Haest et al. (2021) <doi:10.5281/zenodo.5734960>). Migration Traffic Rates and densities can be calculated separately for different height bins (with a height resolution of choice) as well as over time periods of choice (e.g., 1/2 hour, 1 hour, 1 day, day/night, the full time period of observation, and anything in between). Two plotting functions are also included to explore the data in the 'SQL' databases and the resulting Migration Traffic Rate results. For details on the Migration Traffic Rate calculation procedures, see Schmid et al. (2019) <doi:10.1111/ecog.04025>.  "
  },
  {
    "id": 9251,
    "package_name": "blindreview",
    "title": "Enables Blind Review of Database",
    "description": "Randomly reassigns the group identifications to one of the variables of the \n   database, say Treatment, and randomly reassigns the observation numbers of the dataset. \n   Reorders the observations according to these new numbers. Centers each group of Treatment\n   at the grand mean in order to further mask the treatment. An unmasking function is \n   provided so that the user can identify the potential outliers in terms of their original \n   values when blinding is no longer needed. It is suggested that a forward search procedure \n   be performed on the masked data. Details of some forward search functions may be found in\n   <https://CRAN.R-project.org/package=forsearch>.",
    "version": "2.0.0",
    "maintainer": "William Fairweather <wrf343@flowervalleyconsulting.com>",
    "author": "William Fairweather [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=blindreview",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blindreview Enables Blind Review of Database Randomly reassigns the group identifications to one of the variables of the \n   database, say Treatment, and randomly reassigns the observation numbers of the dataset. \n   Reorders the observations according to these new numbers. Centers each group of Treatment\n   at the grand mean in order to further mask the treatment. An unmasking function is \n   provided so that the user can identify the potential outliers in terms of their original \n   values when blinding is no longer needed. It is suggested that a forward search procedure \n   be performed on the masked data. Details of some forward search functions may be found in\n   <https://CRAN.R-project.org/package=forsearch>.  "
  },
  {
    "id": 9315,
    "package_name": "boilerplate",
    "title": "Managing and Compiling Manuscript Templates",
    "description": "Managing and generating standardised text for methods and results sections of scientific reports. It handles template variable substitution and supports hierarchical organisation of text through dot-separated paths. The package supports both RDS and JSON database formats, enabling version control and cross-language compatibility.",
    "version": "1.3.0",
    "maintainer": "Joseph Bulbulia <joseph.bulbulia@gmail.com>",
    "author": "Joseph Bulbulia [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5861-2056>)",
    "url": "https://go-bayes.github.io/boilerplate/,\nhttps://github.com/go-bayes/boilerplate",
    "bug_reports": "https://github.com/go-bayes/boilerplate/issues",
    "repository": "https://cran.r-project.org/package=boilerplate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "boilerplate Managing and Compiling Manuscript Templates Managing and generating standardised text for methods and results sections of scientific reports. It handles template variable substitution and supports hierarchical organisation of text through dot-separated paths. The package supports both RDS and JSON database formats, enabling version control and cross-language compatibility.  "
  },
  {
    "id": 9319,
    "package_name": "bolt4jr",
    "title": "Interface for the 'Neo4j Bolt' Protocol",
    "description": "Querying, extracting, and processing large-scale network data from Neo4j databases using the 'Neo4j Bolt' <https://neo4j.com/docs/bolt/current/bolt/> protocol. This interface supports efficient data retrieval, batch processing for large datasets, and seamless conversion of query results into R data frames, making it ideal for bioinformatics, computational biology, and other graph-based applications.",
    "version": "1.4.0",
    "maintainer": "Wanjun Gu <wanjun.gu@ucsf.edu>",
    "author": "Wanjun Gu [aut, cre] (ORCID: <https://orcid.org/0000-0002-7342-7000>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bolt4jr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bolt4jr Interface for the 'Neo4j Bolt' Protocol Querying, extracting, and processing large-scale network data from Neo4j databases using the 'Neo4j Bolt' <https://neo4j.com/docs/bolt/current/bolt/> protocol. This interface supports efficient data retrieval, batch processing for large datasets, and seamless conversion of query results into R data frames, making it ideal for bioinformatics, computational biology, and other graph-based applications.  "
  },
  {
    "id": 9321,
    "package_name": "bonn",
    "title": "Access INKAR Database",
    "description": "Retrieve and import data from the INKAR database (Indikatoren und Karten zur Raum- und Stadtentwicklung Datenbank, <https://www.inkar.de>) of the Federal Office for Building and Regional Planning (BBSR) in Bonn using their JSON API. ",
    "version": "1.0.3",
    "maintainer": "Moritz Marbach <m.marbach@ucl.ac.uk>",
    "author": "Moritz Marbach [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7101-2821>)",
    "url": "https://github.com/sumtxt/bonn/",
    "bug_reports": "https://github.com/sumtxt/bonn/issues",
    "repository": "https://cran.r-project.org/package=bonn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bonn Access INKAR Database Retrieve and import data from the INKAR database (Indikatoren und Karten zur Raum- und Stadtentwicklung Datenbank, <https://www.inkar.de>) of the Federal Office for Building and Regional Planning (BBSR) in Bonn using their JSON API.   "
  },
  {
    "id": 9491,
    "package_name": "bundesbank",
    "title": "Download Data from Bundesbank",
    "description": "Download data from the time-series\n  databases of the Bundesbank, the German central\n  bank. See the overview at the Bundesbank website\n  (<https://www.bundesbank.de/en/statistics/time-series-databases>)\n  for available series. The package provides only a\n  single function, getSeries(), which supports both\n  traditional and real-time datasets; it will also\n  download meta data if available. Downloaded data\n  can automatically be arranged in various formats,\n  such as data frames or 'zoo' series. The data\n  may optionally be cached, so as to avoid repeated\n  downloads of the same series.",
    "version": "0.1-12",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "http://enricoschumann.net/R/packages/bundesbank/index.htm,\nhttps://github.com/enricoschumann/bundesbank",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bundesbank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bundesbank Download Data from Bundesbank Download data from the time-series\n  databases of the Bundesbank, the German central\n  bank. See the overview at the Bundesbank website\n  (<https://www.bundesbank.de/en/statistics/time-series-databases>)\n  for available series. The package provides only a\n  single function, getSeries(), which supports both\n  traditional and real-time datasets; it will also\n  download meta data if available. Downloaded data\n  can automatically be arranged in various formats,\n  such as data frames or 'zoo' series. The data\n  may optionally be cached, so as to avoid repeated\n  downloads of the same series.  "
  },
  {
    "id": 9514,
    "package_name": "bytescircle",
    "title": "Statistics About Bytes Contained in a File as a Circle Plot",
    "description": "Shows statistics about bytes contained in a file \n  as a circle graph of deviations from mean in sigma increments. \n  The function can be useful for statistically analyze the content of files \n  in a glimpse: text files are shown as a green centered crown, compressed \n  and encrypted files should be shown as equally distributed variations with \n  a very low CV (sigma/mean), and other types of files can be classified between \n  these two categories depending on their text vs binary content, which can be \n  useful to quickly determine how information is stored inside them (databases, \n  multimedia files, etc). ",
    "version": "1.1.2",
    "maintainer": "Roberto S. Galende <roberto.s.galende@gmail.com>",
    "author": "Roberto S. Galende [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bytescircle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bytescircle Statistics About Bytes Contained in a File as a Circle Plot Shows statistics about bytes contained in a file \n  as a circle graph of deviations from mean in sigma increments. \n  The function can be useful for statistically analyze the content of files \n  in a glimpse: text files are shown as a green centered crown, compressed \n  and encrypted files should be shown as equally distributed variations with \n  a very low CV (sigma/mean), and other types of files can be classified between \n  these two categories depending on their text vs binary content, which can be \n  useful to quickly determine how information is stored inside them (databases, \n  multimedia files, etc).   "
  },
  {
    "id": 9534,
    "package_name": "caRecall",
    "title": "Government of Canada Vehicle Recalls Database API Wrapper",
    "description": "Provides API access to the Government of Canada Vehicle Recalls \n    Database <https://tc.api.canada.ca/en/detail?api=VRDB> used by the Defect \n    Investigations and Recalls Division for vehicles, tires, and child car \n    seats. The API wrapper provides access to recall summary information \n    searched using make, model, and year range, as well as detailed recall \n    information searched using recall number.",
    "version": "0.1.0",
    "maintainer": "Nathan Smith <nwraysmith@gmail.com>",
    "author": "Nathan Smith [aut, cre],\n  Mitch Harris [aut],\n  Ryan Koenig [aut]",
    "url": "https://github.com/WraySmith/caRecall",
    "bug_reports": "https://github.com/WraySmith/caRecall/issues",
    "repository": "https://cran.r-project.org/package=caRecall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "caRecall Government of Canada Vehicle Recalls Database API Wrapper Provides API access to the Government of Canada Vehicle Recalls \n    Database <https://tc.api.canada.ca/en/detail?api=VRDB> used by the Defect \n    Investigations and Recalls Division for vehicles, tires, and child car \n    seats. The API wrapper provides access to recall summary information \n    searched using make, model, and year range, as well as detailed recall \n    information searched using recall number.  "
  },
  {
    "id": 9568,
    "package_name": "campfin",
    "title": "Wrangle Campaign Finance Data",
    "description": "Explore and normalize American campaign finance\n    data. Created by the Investigative Reporting Workshop to facilitate\n    work on The Accountability Project, an effort to collect public data\n    into a central, standard database that is more easily searched:\n    <https://publicaccountability.org/>.",
    "version": "1.0.11",
    "maintainer": "Kiernan Nicholls <kiernann@protonmail.com>",
    "author": "Kiernan Nicholls [aut, cre, cph],\n  Investigative Reporting Workshop [cph],\n  Yanqi Xu [aut],\n  Schuyler Erle [cph]",
    "url": "https://github.com/irworkshop/campfin,\nhttps://irworkshop.github.io/campfin/",
    "bug_reports": "https://github.com/irworkshop/campfin/issues",
    "repository": "https://cran.r-project.org/package=campfin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "campfin Wrangle Campaign Finance Data Explore and normalize American campaign finance\n    data. Created by the Investigative Reporting Workshop to facilitate\n    work on The Accountability Project, an effort to collect public data\n    into a central, standard database that is more easily searched:\n    <https://publicaccountability.org/>.  "
  },
  {
    "id": 9581,
    "package_name": "canprot",
    "title": "Chemical Analysis of Proteins",
    "description": "Chemical analysis of proteins based on their amino acid\n  compositions. Amino acid compositions can be read from FASTA files and used to\n  calculate chemical metrics including carbon oxidation state and stoichiometric\n  hydration state, as described in Dick et al. (2020) <doi:10.5194/bg-17-6145-2020>. \n  Other properties that can be calculated include protein length, grand average of\n  hydropathy (GRAVY), isoelectric point (pI), molecular weight (MW), standard\n  molal volume (V0), and metabolic costs (Akashi and Gojobori, 2002\n  <doi:10.1073/pnas.062526999>; Wagner, 2005 <doi:10.1093/molbev/msi126>;\n  Zhang et al., 2018 <doi:10.1038/s41467-018-06461-1>). A database of amino acid\n  compositions of human proteins derived from UniProt is provided.",
    "version": "2.0.0",
    "maintainer": "Jeffrey Dick <j3ffdick@gmail.com>",
    "author": "Jeffrey Dick [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0687-5890>)",
    "url": "https://github.com/jedick/canprot",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=canprot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "canprot Chemical Analysis of Proteins Chemical analysis of proteins based on their amino acid\n  compositions. Amino acid compositions can be read from FASTA files and used to\n  calculate chemical metrics including carbon oxidation state and stoichiometric\n  hydration state, as described in Dick et al. (2020) <doi:10.5194/bg-17-6145-2020>. \n  Other properties that can be calculated include protein length, grand average of\n  hydropathy (GRAVY), isoelectric point (pI), molecular weight (MW), standard\n  molal volume (V0), and metabolic costs (Akashi and Gojobori, 2002\n  <doi:10.1073/pnas.062526999>; Wagner, 2005 <doi:10.1093/molbev/msi126>;\n  Zhang et al., 2018 <doi:10.1038/s41467-018-06461-1>). A database of amino acid\n  compositions of human proteins derived from UniProt is provided.  "
  },
  {
    "id": 9582,
    "package_name": "cansim",
    "title": "Accessing Statistics Canada Data Table and Vectors",
    "description": "Searches for, accesses, and retrieves Statistics Canada data \n    tables, as well as individual vectors, as tidy data frames.\n    This package enriches the tables with metadata, deals\n    with encoding issues, allows for bilingual English or French language data retrieval, and bundles\n    convenience functions to make it easier to work with retrieved table data. For more efficient data\n    access the package allows for caching data in a local database and database level filtering, data\n    manipulation and summarizing.",
    "version": "0.4.4",
    "maintainer": "Jens von Bergmann <jens@mountainmath.ca>",
    "author": "Jens von Bergmann [aut, cre],\n  Dmitry Shkolnik [aut]",
    "url": "https://github.com/mountainMath/cansim,\nhttps://mountainmath.github.io/cansim/,\nhttps://www.statcan.gc.ca/",
    "bug_reports": "https://github.com/mountainMath/cansim/issues",
    "repository": "https://cran.r-project.org/package=cansim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cansim Accessing Statistics Canada Data Table and Vectors Searches for, accesses, and retrieves Statistics Canada data \n    tables, as well as individual vectors, as tidy data frames.\n    This package enriches the tables with metadata, deals\n    with encoding issues, allows for bilingual English or French language data retrieval, and bundles\n    convenience functions to make it easier to work with retrieved table data. For more efficient data\n    access the package allows for caching data in a local database and database level filtering, data\n    manipulation and summarizing.  "
  },
  {
    "id": 9586,
    "package_name": "capesData",
    "title": "Data on Scholarships in CAPES International Mobility Programs",
    "description": "Information on activities to promote scholarships in Brazil and abroad for international mobility programs, recorded in Capes' computerized payment systems. The CAPES database refers to international mobility programs for the period from 2010 to 2019 <https://dadosabertos.capes.gov.br/dataset/>.",
    "version": "0.0.1",
    "maintainer": "Leonardo Biazoli <leonardobiazoli19@gmail.com>",
    "author": "Leonardo Biazoli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4069-111X>),\n  Mine \u00c7etinkaya-Rundel [aut] (ORCID:\n    <https://orcid.org/0000-0001-6452-2420>),\n  Eric Fernandes de Mello Araujo [aut] (ORCID:\n    <https://orcid.org/0000-0003-4263-9075>),\n  Izabela R. Cardoso de Oliveira [aut] (ORCID:\n    <https://orcid.org/0000-0003-3254-2827>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=capesData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "capesData Data on Scholarships in CAPES International Mobility Programs Information on activities to promote scholarships in Brazil and abroad for international mobility programs, recorded in Capes' computerized payment systems. The CAPES database refers to international mobility programs for the period from 2010 to 2019 <https://dadosabertos.capes.gov.br/dataset/>.  "
  },
  {
    "id": 9619,
    "package_name": "caroline",
    "title": "A Collection of Database, Data Structure, Visualization, and\nUtility Functions for R",
    "description": "The caroline R library contains dozens of functions useful\n for: database migration (dbWriteTable2), database style joins &\n aggregation (nerge, groupBy, & bestBy), data structure\n conversion (nv, tab2df), legend table making (sstable & leghead),\n automatic legend positioning for scatter and box plots (),  \n plot annotation (labsegs & mvlabs), data visualization \n (pies, sparge, confound.grid & raPlot), character string manipulation (m & pad),\n file I/O (write.delim), batch scripting, data exploration, and more.\n The package's greatest contributions lie in the database style merge, \n aggregation and interface functions as well as in it's extensive\n use and propagation of row, column and vector names in most functions.",
    "version": "0.9.9",
    "maintainer": "David Schruth <code@anthropoidea.org>",
    "author": "David Schruth [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=caroline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "caroline A Collection of Database, Data Structure, Visualization, and\nUtility Functions for R The caroline R library contains dozens of functions useful\n for: database migration (dbWriteTable2), database style joins &\n aggregation (nerge, groupBy, & bestBy), data structure\n conversion (nv, tab2df), legend table making (sstable & leghead),\n automatic legend positioning for scatter and box plots (),  \n plot annotation (labsegs & mvlabs), data visualization \n (pies, sparge, confound.grid & raPlot), character string manipulation (m & pad),\n file I/O (write.delim), batch scripting, data exploration, and more.\n The package's greatest contributions lie in the database style merge, \n aggregation and interface functions as well as in it's extensive\n use and propagation of row, column and vector names in most functions.  "
  },
  {
    "id": 9645,
    "package_name": "catalog",
    "title": "Access the 'Spark Catalog' API via 'sparklyr'",
    "description": "Gain access to the 'Spark Catalog' API making use of the 'sparklyr' API. 'Catalog' <https://spark.apache.org/docs/2.4.3/api/java/org/apache/spark/sql/catalog/Catalog.html> is the interface for managing a metastore (aka metadata catalog) of relational entities (e.g. database(s), tables, functions, table columns and temporary views).",
    "version": "0.1.1",
    "maintainer": "Nathan Eastwood <nathan.eastwood@icloud.com>",
    "author": "Nathan Eastwood [aut, cre]",
    "url": "https://nathaneastwood.github.io/catalog/,\nhttps://github.com/nathaneastwood/catalog",
    "bug_reports": "https://github.com/nathaneastwood/catalog/issues",
    "repository": "https://cran.r-project.org/package=catalog",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catalog Access the 'Spark Catalog' API via 'sparklyr' Gain access to the 'Spark Catalog' API making use of the 'sparklyr' API. 'Catalog' <https://spark.apache.org/docs/2.4.3/api/java/org/apache/spark/sql/catalog/Catalog.html> is the interface for managing a metastore (aka metadata catalog) of relational entities (e.g. database(s), tables, functions, table columns and temporary views).  "
  },
  {
    "id": 9723,
    "package_name": "cdata",
    "title": "Fluid Data Transformations",
    "description": "Supplies higher-order coordinatized data specification and fluid transform operators that include pivot and anti-pivot as special cases. \n    The methodology is describe in 'Zumel', 2018, \"Fluid data reshaping with 'cdata'\", <https://winvector.github.io/FluidData/FluidDataReshapingWithCdata.html> , <DOI:10.5281/zenodo.1173299> .\n    This package introduces the idea of explicit control table specification of data transforms.\n    Works on in-memory data or on remote data using 'rquery' and 'SQL' database interfaces.",
    "version": "1.2.1",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Nina Zumel [aut],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/cdata/,\nhttps://winvector.github.io/cdata/",
    "bug_reports": "https://github.com/WinVector/cdata/issues",
    "repository": "https://cran.r-project.org/package=cdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cdata Fluid Data Transformations Supplies higher-order coordinatized data specification and fluid transform operators that include pivot and anti-pivot as special cases. \n    The methodology is describe in 'Zumel', 2018, \"Fluid data reshaping with 'cdata'\", <https://winvector.github.io/FluidData/FluidDataReshapingWithCdata.html> , <DOI:10.5281/zenodo.1173299> .\n    This package introduces the idea of explicit control table specification of data transforms.\n    Works on in-memory data or on remote data using 'rquery' and 'SQL' database interfaces.  "
  },
  {
    "id": 9726,
    "package_name": "cder",
    "title": "Interface to the California Data Exchange Center (CDEC)",
    "description": "Connect to the California Data Exchange Center (CDEC) \n    Web Service <http://cdec.water.ca.gov/>. 'CDEC' provides a centralized \n    database to store, process, and exchange real-time hydrologic information \n    gathered by various cooperators throughout California. The 'CDEC' Web Service \n    <http://cdec.water.ca.gov/dynamicapp/wsSensorData> provides a data download \n    service for accessing historical records. ",
    "version": "0.3-1",
    "maintainer": "Michael Koohafkan <michael.koohafkan@gmail.com>",
    "author": "Michael Koohafkan [aut, cre]",
    "url": "https://github.com/mkoohafkan/cder",
    "bug_reports": "https://github.com/mkoohafkan/cder/issues",
    "repository": "https://cran.r-project.org/package=cder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cder Interface to the California Data Exchange Center (CDEC) Connect to the California Data Exchange Center (CDEC) \n    Web Service <http://cdec.water.ca.gov/>. 'CDEC' provides a centralized \n    database to store, process, and exchange real-time hydrologic information \n    gathered by various cooperators throughout California. The 'CDEC' Web Service \n    <http://cdec.water.ca.gov/dynamicapp/wsSensorData> provides a data download \n    service for accessing historical records.   "
  },
  {
    "id": 9839,
    "package_name": "chem.databases",
    "title": "Collection of 3 Chemical Databases from Public Sources",
    "description": "Contains the Multi-Species Acute Toxicity Database (CAS & SMILES\n    columns only) [United States (US) Department of Health and Human Services\n    (DHHS) National Institutes of Health (NIH) National Cancer Institute (NCI),\n    \"Multi-Species Acute Toxicity Database\",\n    <https://cactus.nci.nih.gov/download/acute-toxicity-db/>] combined with the\n    Toxic Substances Control Act (TSCA) Inventory [United States Environmental\n    Protection Agency (US EPA), \"Toxic Substances Control Act (TSCA) Chemical\n    Substance Inventory\",\n    <https://www.epa.gov/tsca-inventory/how-access-tsca-inventory} and\n    <https://cdxapps.epa.gov/oms-substance-registry-services/substance-list-details/169>]\n    and the Agency for Toxic Substances and Disease Registry (ATSDR) Database\n    [United States (US) Department of Health and Human Services (DHHS) Centers\n    for Disease Control and Prevention (CDC)/Agency for Toxic Substances and\n    Disease Registry (ATSDR), \"Agency for Toxic Substances and Disease Registry\n    (ATSDR) Database\",\n    <https://cdxapps.epa.gov/oms-substance-registry-services/substance-list-details/105>]\n    in 2 data sets. One data set has a focus on the latter 2 databases and one\n    data set focuses on the former database. Also contains the collection of\n    chemical data from Wikipedia compiled in the US EPA CompTox Chemicals\n    Dashboard [United States Environmental Protection Agency (US EPA) /\n    Wikimedia Foundation, Inc. \"CompTox Chemicals Dashboard v2.2.1\",\n    <https://comptox.epa.gov/dashboard/chemical-lists/WIKIPEDIA>].",
    "version": "1.0.0",
    "maintainer": "Irucka Embry <iembry@ecoccs.com>",
    "author": "Irucka Embry [aut, cre]",
    "url": "https://gitlab.com/iembry/chem.databases",
    "bug_reports": "https://gitlab.com/iembry/chem.databases/-/issues",
    "repository": "https://cran.r-project.org/package=chem.databases",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chem.databases Collection of 3 Chemical Databases from Public Sources Contains the Multi-Species Acute Toxicity Database (CAS & SMILES\n    columns only) [United States (US) Department of Health and Human Services\n    (DHHS) National Institutes of Health (NIH) National Cancer Institute (NCI),\n    \"Multi-Species Acute Toxicity Database\",\n    <https://cactus.nci.nih.gov/download/acute-toxicity-db/>] combined with the\n    Toxic Substances Control Act (TSCA) Inventory [United States Environmental\n    Protection Agency (US EPA), \"Toxic Substances Control Act (TSCA) Chemical\n    Substance Inventory\",\n    <https://www.epa.gov/tsca-inventory/how-access-tsca-inventory} and\n    <https://cdxapps.epa.gov/oms-substance-registry-services/substance-list-details/169>]\n    and the Agency for Toxic Substances and Disease Registry (ATSDR) Database\n    [United States (US) Department of Health and Human Services (DHHS) Centers\n    for Disease Control and Prevention (CDC)/Agency for Toxic Substances and\n    Disease Registry (ATSDR), \"Agency for Toxic Substances and Disease Registry\n    (ATSDR) Database\",\n    <https://cdxapps.epa.gov/oms-substance-registry-services/substance-list-details/105>]\n    in 2 data sets. One data set has a focus on the latter 2 databases and one\n    data set focuses on the former database. Also contains the collection of\n    chemical data from Wikipedia compiled in the US EPA CompTox Chemicals\n    Dashboard [United States Environmental Protection Agency (US EPA) /\n    Wikimedia Foundation, Inc. \"CompTox Chemicals Dashboard v2.2.1\",\n    <https://comptox.epa.gov/dashboard/chemical-lists/WIKIPEDIA>].  "
  },
  {
    "id": 9840,
    "package_name": "chem16S",
    "title": "Chemical Metrics for Microbial Communities",
    "description": "Combines taxonomic classifications of high-throughput 16S rRNA\n  gene sequences with reference proteomes of archaeal and bacterial taxa to\n  generate amino acid compositions of community reference proteomes. Calculates\n  chemical metrics including carbon oxidation state ('Zc'), stoichiometric\n  oxidation and hydration state ('nO2' and 'nH2O'), H/C, N/C, O/C, and S/C\n  ratios, grand average of hydropathicity ('GRAVY'), isoelectric point ('pI'),\n  protein length, and average molecular weight of amino acid residues. Uses\n  precomputed reference proteomes for archaea and bacteria derived from the\n  Genome Taxonomy Database ('GTDB'). Also includes reference proteomes derived\n  from the NCBI Reference Sequence ('RefSeq') database and manual mapping from\n  the 'RDP Classifier' training set to 'RefSeq' taxonomy as described by Dick and\n  Tan (2023) <doi:10.1007/s00248-022-01988-9>. Processes taxonomic\n  classifications in 'RDP Classifier' format or OTU tables in 'phyloseq-class'\n  objects from the Bioconductor package 'phyloseq'.",
    "version": "1.2.0",
    "maintainer": "Jeffrey Dick <j3ffdick@gmail.com>",
    "author": "Jeffrey Dick [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0687-5890>)",
    "url": "https://github.com/jedick/chem16S",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=chem16S",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chem16S Chemical Metrics for Microbial Communities Combines taxonomic classifications of high-throughput 16S rRNA\n  gene sequences with reference proteomes of archaeal and bacterial taxa to\n  generate amino acid compositions of community reference proteomes. Calculates\n  chemical metrics including carbon oxidation state ('Zc'), stoichiometric\n  oxidation and hydration state ('nO2' and 'nH2O'), H/C, N/C, O/C, and S/C\n  ratios, grand average of hydropathicity ('GRAVY'), isoelectric point ('pI'),\n  protein length, and average molecular weight of amino acid residues. Uses\n  precomputed reference proteomes for archaea and bacteria derived from the\n  Genome Taxonomy Database ('GTDB'). Also includes reference proteomes derived\n  from the NCBI Reference Sequence ('RefSeq') database and manual mapping from\n  the 'RDP Classifier' training set to 'RefSeq' taxonomy as described by Dick and\n  Tan (2023) <doi:10.1007/s00248-022-01988-9>. Processes taxonomic\n  classifications in 'RDP Classifier' format or OTU tables in 'phyloseq-class'\n  objects from the Bioconductor package 'phyloseq'.  "
  },
  {
    "id": 9855,
    "package_name": "childesr",
    "title": "Accessing the 'CHILDES' Database",
    "description": "Tools for connecting to 'CHILDES', an open repository for\n    transcripts of parent-child interaction. For more information on the \n    underlying data, see <https://langcog.github.io/childes-db-website/>.",
    "version": "0.2.3",
    "maintainer": "Mika Braginsky <mika.br@gmail.com>",
    "author": "Mika Braginsky [aut, cre],\n  Alessandro Sanchez [aut, ctb],\n  Daniel Yurovsky [aut],\n  Kyle MacDonald [ctb],\n  Stephan Meylan [ctb],\n  Jessica Mankewitz [ctb]",
    "url": "https://github.com/langcog/childesr",
    "bug_reports": "https://github.com/langcog/childesr/issues",
    "repository": "https://cran.r-project.org/package=childesr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "childesr Accessing the 'CHILDES' Database Tools for connecting to 'CHILDES', an open repository for\n    transcripts of parent-child interaction. For more information on the \n    underlying data, see <https://langcog.github.io/childes-db-website/>.  "
  },
  {
    "id": 9856,
    "package_name": "childeswordfreq",
    "title": "Word and Phrase Frequency Tools for CHILDES",
    "description": "\n    Tools for extracting word and phrase frequencies from the Child Language Data Exchange System (CHILDES) \n    database via the 'childesr' API. Supports type-level word counts, \n    token-mode searches with simple wildcard patterns and part-of-speech \n    filters, optional stemming, and Zipf-scaled frequencies. Provides \n    normalization per number of tokens or utterances, speaker-role \n    breakdowns, dataset summaries, and export to Excel workbooks for \n    reproducible child language research. \n    The CHILDES database is maintained at <https://talkbank.org/childes/>.",
    "version": "0.2.0",
    "maintainer": "Nahar Albudoor <n.albudoor@gmail.com>",
    "author": "Nahar Albudoor [aut, cre]",
    "url": "https://github.com/n-albudoor/childeswordfreq",
    "bug_reports": "https://github.com/n-albudoor/childeswordfreq/issues",
    "repository": "https://cran.r-project.org/package=childeswordfreq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "childeswordfreq Word and Phrase Frequency Tools for CHILDES \n    Tools for extracting word and phrase frequencies from the Child Language Data Exchange System (CHILDES) \n    database via the 'childesr' API. Supports type-level word counts, \n    token-mode searches with simple wildcard patterns and part-of-speech \n    filters, optional stemming, and Zipf-scaled frequencies. Provides \n    normalization per number of tokens or utterances, speaker-role \n    breakdowns, dataset summaries, and export to Excel workbooks for \n    reproducible child language research. \n    The CHILDES database is maintained at <https://talkbank.org/childes/>.  "
  },
  {
    "id": 9888,
    "package_name": "chronosphere",
    "title": "Evolving Earth System Variables",
    "description": "The implemented functions allow the query, download, and import of remotely-stored and version-controlled data items. The inherent meta-database maps data files and import code to programming classes and allows access to these items via files deposited in public repositories. The purpose of the project is to increase reproducibility and establish version tracking of results from (paleo)environmental/ecological research.",
    "version": "0.6.1",
    "maintainer": "Adam T. Kocsis <adam.t.kocsis@gmail.com>",
    "author": "Adam T. Kocsis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9028-665X>),\n  Nussaibah B. Raja [aut] (ORCID:\n    <https://orcid.org/0000-0002-0000-3944>),\n  Deutsche Forschungsgemeinschaft [fnd],\n  FAU GeoZentrum Nordbayern [fnd]",
    "url": "",
    "bug_reports": "https://github.com/chronosphere-info/r_client/issues",
    "repository": "https://cran.r-project.org/package=chronosphere",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chronosphere Evolving Earth System Variables The implemented functions allow the query, download, and import of remotely-stored and version-controlled data items. The inherent meta-database maps data files and import code to programming classes and allows access to these items via files deposited in public repositories. The purpose of the project is to increase reproducibility and establish version tracking of results from (paleo)environmental/ecological research.  "
  },
  {
    "id": 9924,
    "package_name": "citationchaser",
    "title": "Perform Forward and Backwards Chasing in Evidence Syntheses",
    "description": "In searching for research articles, we often want to \n  obtain lists of references from across studies, and also obtain lists \n  of articles that cite a particular study. In systematic reviews, this \n  supplementary search technique is known as 'citation chasing': forward \n  citation chasing looks for all records citing one or more articles of \n  known relevance; backward citation chasing looks for all records \n  referenced in one or more articles. Traditionally, this process would \n  be done manually, and the resulting records would need to be checked \n  one-by-one against included studies in a review to identify potentially \n  relevant records that should be included in a review. This package \n  contains functions to automate this process by making use of the \n  Lens.org API. An input article list can be used to return a list of \n  all referenced records, and/or all citing records in the Lens.org \n  database (consisting of PubMed, PubMed Central, CrossRef, Microsoft \n  Academic Graph and CORE; <https://www.lens.org>). ",
    "version": "0.0.4",
    "maintainer": "Neal Haddaway <nealhaddaway@gmail.com>",
    "author": "Neal Haddaway [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3902-2234>),\n  Matthew Grainger [ctb],\n  Charles Gray [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=citationchaser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "citationchaser Perform Forward and Backwards Chasing in Evidence Syntheses In searching for research articles, we often want to \n  obtain lists of references from across studies, and also obtain lists \n  of articles that cite a particular study. In systematic reviews, this \n  supplementary search technique is known as 'citation chasing': forward \n  citation chasing looks for all records citing one or more articles of \n  known relevance; backward citation chasing looks for all records \n  referenced in one or more articles. Traditionally, this process would \n  be done manually, and the resulting records would need to be checked \n  one-by-one against included studies in a review to identify potentially \n  relevant records that should be included in a review. This package \n  contains functions to automate this process by making use of the \n  Lens.org API. An input article list can be used to return a list of \n  all referenced records, and/or all citing records in the Lens.org \n  database (consisting of PubMed, PubMed Central, CrossRef, Microsoft \n  Academic Graph and CORE; <https://www.lens.org>).   "
  },
  {
    "id": 9990,
    "package_name": "clinicalomicsdbR",
    "title": "Interface with the 'ClinicalOmicsDB' API, Allowing for Easy Data\nDownloading and Importing",
    "description": "Provides an interface to the 'ClinicalOmicsDB' API, allowing for easy data downloading and importing. 'ClinicalOmicsDB' is a database of clinical and 'omics' data from cancer patients. The database is accessible at <http://trials.linkedomics.org>.",
    "version": "1.0.5",
    "maintainer": "John Elizarraras <john.elizarraras@bcm.edu>",
    "author": "John Elizarraras [aut, cre, ctb]",
    "url": "https://github.com/bzhanglab/clinicalomicsdbR",
    "bug_reports": "https://github.com/bzhanglab/clinicalomicsdbR/issues",
    "repository": "https://cran.r-project.org/package=clinicalomicsdbR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinicalomicsdbR Interface with the 'ClinicalOmicsDB' API, Allowing for Easy Data\nDownloading and Importing Provides an interface to the 'ClinicalOmicsDB' API, allowing for easy data downloading and importing. 'ClinicalOmicsDB' is a database of clinical and 'omics' data from cancer patients. The database is accessible at <http://trials.linkedomics.org>.  "
  },
  {
    "id": 9998,
    "package_name": "clintrialx",
    "title": "Connect and Work with Clinical Trials Data Sources",
    "description": "Are you spending too much time fetching and managing clinical trial data? Struggling with complex queries and bulk data extraction? What if you could simplify this process with just a few lines of code? Introducing 'clintrialx' - Fetch clinical trial data from sources like 'ClinicalTrials.gov' <https://clinicaltrials.gov/> and the 'Clinical Trials Transformation Initiative - Access to Aggregate Content of ClinicalTrials.gov' database <https://aact.ctti-clinicaltrials.org/>, supporting pagination and bulk downloads. Also, you can generate HTML reports based on the data obtained from the sources!",
    "version": "0.1.1",
    "maintainer": "Indraneel Chakraborty <hello.indraneel@gmail.com>",
    "author": "Indraneel Chakraborty [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6958-8269>)",
    "url": "http://www.indraneelchakraborty.com/clintrialx/,\nhttps://github.com/ineelhere/clintrialx",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clintrialx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clintrialx Connect and Work with Clinical Trials Data Sources Are you spending too much time fetching and managing clinical trial data? Struggling with complex queries and bulk data extraction? What if you could simplify this process with just a few lines of code? Introducing 'clintrialx' - Fetch clinical trial data from sources like 'ClinicalTrials.gov' <https://clinicaltrials.gov/> and the 'Clinical Trials Transformation Initiative - Access to Aggregate Content of ClinicalTrials.gov' database <https://aact.ctti-clinicaltrials.org/>, supporting pagination and bulk downloads. Also, you can generate HTML reports based on the data obtained from the sources!  "
  },
  {
    "id": 10049,
    "package_name": "clustermole",
    "title": "Unbiased Single-Cell Transcriptomic Data Cell Type\nIdentification",
    "description": "Assignment of cell type labels to single-cell RNA sequencing (scRNA-seq) clusters is often a time-consuming process that involves manual inspection of the cluster marker genes complemented with a detailed literature search. This is especially challenging when unexpected or poorly described populations are present. The clustermole R package provides methods to query thousands of human and mouse cell identity markers sourced from a variety of databases.",
    "version": "1.1.1",
    "maintainer": "Igor Dolgalev <igor.dolgalev@nyumc.org>",
    "author": "Igor Dolgalev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4451-126X>)",
    "url": "https://igordot.github.io/clustermole/",
    "bug_reports": "https://github.com/igordot/clustermole/issues",
    "repository": "https://cran.r-project.org/package=clustermole",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clustermole Unbiased Single-Cell Transcriptomic Data Cell Type\nIdentification Assignment of cell type labels to single-cell RNA sequencing (scRNA-seq) clusters is often a time-consuming process that involves manual inspection of the cluster marker genes complemented with a detailed literature search. This is especially challenging when unexpected or poorly described populations are present. The clustermole R package provides methods to query thousands of human and mouse cell identity markers sourced from a variety of databases.  "
  },
  {
    "id": 10086,
    "package_name": "cms",
    "title": "Calculate Medicare Reimbursement",
    "description": "Uses the 'CMS' application programming interface <https://dnav.cms.gov/api/healthdata>\n    to provide users databases containing yearly\n    Medicare reimbursement rates in the United States. Data can be acquired\n    for the entire United States or only for specific localities. Currently,\n    support is only provided for the Medicare Physician Fee Schedule,\n    but support will be expanded for other 'CMS' databases in future versions.",
    "version": "0.1.0",
    "maintainer": "Vigneshwar Subramanian <vs347@cornell.edu>",
    "author": "Vigneshwar Subramanian [aut, cre],\n  Raoul Wadhwa [aut],\n  Milind Desai [ctb]",
    "url": "https://github.com/subramv/cms",
    "bug_reports": "https://github.com/subramv/cms/issues",
    "repository": "https://cran.r-project.org/package=cms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cms Calculate Medicare Reimbursement Uses the 'CMS' application programming interface <https://dnav.cms.gov/api/healthdata>\n    to provide users databases containing yearly\n    Medicare reimbursement rates in the United States. Data can be acquired\n    for the entire United States or only for specific localities. Currently,\n    support is only provided for the Medicare Physician Fee Schedule,\n    but support will be expanded for other 'CMS' databases in future versions.  "
  },
  {
    "id": 10150,
    "package_name": "cofid",
    "title": "Copepod Fish Interaction Database",
    "description": "A curated list of copepod-fish ecological interaction records.\n    It contains the taxonomy of the copepod and the fish and the publication\n    from which the information was obtained. This database contains only marine\n    and brackish water fish species. It excludes fish species that inhabit only\n    freshwater. ",
    "version": "1.0.0",
    "maintainer": "Angel Robles <a.l.robles.fernandez@gmail.com>",
    "author": "Angel Robles [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4674-4270>)",
    "url": "<https://alrobles.github.io/cofid/>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cofid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cofid Copepod Fish Interaction Database A curated list of copepod-fish ecological interaction records.\n    It contains the taxonomy of the copepod and the fish and the publication\n    from which the information was obtained. This database contains only marine\n    and brackish water fish species. It excludes fish species that inhabit only\n    freshwater.   "
  },
  {
    "id": 10247,
    "package_name": "compdb",
    "title": "Generate Compilation Database for Use with 'Clang' Tools",
    "description": "Many modern C/C++ development tools in the 'clang' toolchain,\n  such as 'clang-tidy' or 'clangd', rely on the presence of a compilation database\n  in JSON format <https://clang.llvm.org/docs/JSONCompilationDatabase.html>.\n  This package temporarily injects additional build flags into the R build\n  process to generate such a compilation database.",
    "version": "0.0.1",
    "maintainer": "Felix Held <felix.held@gmail.com>",
    "author": "Felix Held [aut, cre] (ORCID: <https://orcid.org/0000-0002-7679-7752>)",
    "url": "https://github.com/cyianor/r-compdb",
    "bug_reports": "https://github.com/cyianor/r-compdb/issues",
    "repository": "https://cran.r-project.org/package=compdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "compdb Generate Compilation Database for Use with 'Clang' Tools Many modern C/C++ development tools in the 'clang' toolchain,\n  such as 'clang-tidy' or 'clangd', rely on the presence of a compilation database\n  in JSON format <https://clang.llvm.org/docs/JSONCompilationDatabase.html>.\n  This package temporarily injects additional build flags into the R build\n  process to generate such a compilation database.  "
  },
  {
    "id": 10283,
    "package_name": "condensr",
    "title": "Academic Group Website Generator",
    "description": "Helps automate 'Quarto' website creation for small academic groups.\n    Builds a database-like structure of people, projects and publications, linking\n    them together with a string-based ID system. Then, provides functions to automate\n    production of clean markdown for these structures, and in-built CSS formatting\n    using CSS flexbox.",
    "version": "1.0.0",
    "maintainer": "Michael Lydeamore <michael.lydeamore@monash.edu>",
    "author": "Michael Lydeamore [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6515-827X>)",
    "url": "http://www.michaellydeamore.com/condensr/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=condensr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "condensr Academic Group Website Generator Helps automate 'Quarto' website creation for small academic groups.\n    Builds a database-like structure of people, projects and publications, linking\n    them together with a string-based ID system. Then, provides functions to automate\n    production of clean markdown for these structures, and in-built CSS formatting\n    using CSS flexbox.  "
  },
  {
    "id": 10324,
    "package_name": "connector",
    "title": "Streamlining Data Access in Clinical Research",
    "description": "Provides a consistent interface for connecting R to various \n    data sources including file systems and databases. \n    Designed for clinical research, 'connector' streamlines access to 'ADAM', \n    'SDTM' for example. It helps to deal with multiple data formats through a standardized API and centralized \n    configuration.",
    "version": "1.0.0",
    "maintainer": "Cervan Girard <cgid@novonordisk.com>",
    "author": "Cervan Girard [aut, cre],\n  Aksel Thomsen [aut],\n  Vladimir Obucina [aut],\n  Novo Nordisk A/S [cph]",
    "url": "https://novonordisk-opensource.github.io/connector/,\nhttps://github.com/NovoNordisk-OpenSource/connector/",
    "bug_reports": "https://github.com/NovoNordisk-OpenSource/connector/issues",
    "repository": "https://cran.r-project.org/package=connector",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "connector Streamlining Data Access in Clinical Research Provides a consistent interface for connecting R to various \n    data sources including file systems and databases. \n    Designed for clinical research, 'connector' streamlines access to 'ADAM', \n    'SDTM' for example. It helps to deal with multiple data formats through a standardized API and centralized \n    configuration.  "
  },
  {
    "id": 10369,
    "package_name": "convertid",
    "title": "Convert Gene IDs Between Each Other and Fetch Annotations from\nBiomart",
    "description": "Gene Symbols or Ensembl Gene IDs are converted using the Bimap interface in 'AnnotationDbi' in convertId2() but\n    that function is only provided as fallback mechanism for the most common use cases in data analysis. The main function\n    in the package is convert.bm() which queries BioMart using the full capacity of the API provided through the\n    'biomaRt' package. Presets and defaults are provided for convenience but all \"marts\", \"filters\" and \"attributes\"\n    can be set by the user. Function convert.alias() converts Gene Symbols to Aliases and vice versa and function likely_symbol()\n    attempts to determine the most likely current Gene Symbol.",
    "version": "0.1.10",
    "maintainer": "Vidal Fey <vidal.fey@gmail.com>",
    "author": "Vidal Fey [aut, cre],\n  Henrik Edgren [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=convertid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "convertid Convert Gene IDs Between Each Other and Fetch Annotations from\nBiomart Gene Symbols or Ensembl Gene IDs are converted using the Bimap interface in 'AnnotationDbi' in convertId2() but\n    that function is only provided as fallback mechanism for the most common use cases in data analysis. The main function\n    in the package is convert.bm() which queries BioMart using the full capacity of the API provided through the\n    'biomaRt' package. Presets and defaults are provided for convenience but all \"marts\", \"filters\" and \"attributes\"\n    can be set by the user. Function convert.alias() converts Gene Symbols to Aliases and vice versa and function likely_symbol()\n    attempts to determine the most likely current Gene Symbol.  "
  },
  {
    "id": 10478,
    "package_name": "covid19dbcand",
    "title": "Selected 'Drugbank' Drugs for COVID-19 Treatment Related Data in\nR Format",
    "description": "Provides different datasets parsed from 'Drugbank' \n    <https://www.drugbank.ca/covid-19> database using 'dbparser' package. \n    It is a smaller version from 'dbdataset' package. It contains only information\n    about COVID-19 possible treatment.",
    "version": "0.1.1",
    "maintainer": "Mohammed Ali <moh_fcis@yahoo.com>",
    "author": "Mohammed Ali [aut, cre]",
    "url": "https://github.com/MohammedFCIS/covid19dbcand",
    "bug_reports": "https://github.com/MohammedFCIS/covid19dbcand/issues",
    "repository": "https://cran.r-project.org/package=covid19dbcand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covid19dbcand Selected 'Drugbank' Drugs for COVID-19 Treatment Related Data in\nR Format Provides different datasets parsed from 'Drugbank' \n    <https://www.drugbank.ca/covid-19> database using 'dbparser' package. \n    It is a smaller version from 'dbdataset' package. It contains only information\n    about COVID-19 possible treatment.  "
  },
  {
    "id": 10546,
    "package_name": "cranly",
    "title": "Package Directives and Collaboration Networks in CRAN",
    "description": "Core visualizations and summaries for the CRAN package database. The package provides comprehensive methods for cleaning up and organizing the information in the CRAN package database, for building package directives networks (depends, imports, suggests, enhances, linking to) and collaboration networks, producing package dependence trees, and for computing useful summaries and producing interactive visualizations from the resulting networks and summaries. The resulting networks can be coerced to 'igraph' <https://CRAN.R-project.org/package=igraph> objects for further analyses and modelling.",
    "version": "0.6.0",
    "maintainer": "Ioannis Kosmidis <ioannis.kosmidis@warwick.ac.uk>",
    "author": "Ioannis Kosmidis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1556-0302>)",
    "url": "https://github.com/ikosmidis/cranly",
    "bug_reports": "https://github.com/ikosmidis/cranly/issues",
    "repository": "https://cran.r-project.org/package=cranly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cranly Package Directives and Collaboration Networks in CRAN Core visualizations and summaries for the CRAN package database. The package provides comprehensive methods for cleaning up and organizing the information in the CRAN package database, for building package directives networks (depends, imports, suggests, enhances, linking to) and collaboration networks, producing package dependence trees, and for computing useful summaries and producing interactive visualizations from the resulting networks and summaries. The resulting networks can be coerced to 'igraph' <https://CRAN.R-project.org/package=igraph> objects for further analyses and modelling.  "
  },
  {
    "id": 10564,
    "package_name": "crimedata",
    "title": "Access Crime Data from the Open Crime Database",
    "description": "Gives convenient access to publicly available police-recorded open\n    crime data from large cities in the United States that are included in the\n    Crime Open Database <https://osf.io/zyaqn/>.",
    "version": "0.3.5",
    "maintainer": "Matthew Ashby <matthew.ashby@ucl.ac.uk>",
    "author": "Matthew Ashby [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4201-9239>)",
    "url": "http://pkgs.lesscrime.info/crimedata/,\nhttps://github.com/mpjashby/crimedata",
    "bug_reports": "https://github.com/mpjashby/crimedata/issues",
    "repository": "https://cran.r-project.org/package=crimedata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "crimedata Access Crime Data from the Open Crime Database Gives convenient access to publicly available police-recorded open\n    crime data from large cities in the United States that are included in the\n    Crime Open Database <https://osf.io/zyaqn/>.  "
  },
  {
    "id": 10632,
    "package_name": "csdb",
    "title": "An Abstracted System for Easily Working with Databases with\nLarge Datasets",
    "description": "Provides object-oriented database management tools for working with large datasets across multiple database systems. Features include robust connection management for SQL Server and PostgreSQL databases, advanced table operations with bulk data loading and upsert functionality, comprehensive data validation through customizable field type and content validators, efficient index management, and cross-database compatibility. Designed for high-performance data operations in surveillance systems and large-scale data processing workflows.",
    "version": "2025.7.30",
    "maintainer": "Richard Aubrey White <hello@rwhite.no>",
    "author": "Richard Aubrey White [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6747-1726>),\n  August S\u00f8rli Mathisen [aut],\n  CSIDS [cph]",
    "url": "https://www.csids.no/csdb/, https://github.com/csids/csdb",
    "bug_reports": "https://github.com/csids/csdb/issues",
    "repository": "https://cran.r-project.org/package=csdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csdb An Abstracted System for Easily Working with Databases with\nLarge Datasets Provides object-oriented database management tools for working with large datasets across multiple database systems. Features include robust connection management for SQL Server and PostgreSQL databases, advanced table operations with bulk data loading and upsert functionality, comprehensive data validation through customizable field type and content validators, efficient index management, and cross-database compatibility. Designed for high-performance data operations in surveillance systems and large-scale data processing workflows.  "
  },
  {
    "id": 10637,
    "package_name": "csodata",
    "title": "Download Data from the CSO 'PxStat' API",
    "description": "Imports 'PxStat' data in JSON-stat format and (optionally) reshapes it into wide\n\tformat. The Central Statistics Office (CSO) is the national statistical institute of Ireland\n\tand 'PxStat' is the CSOs online database of Official Statistics. This database contains current\n\tand historical data series compiled from CSO statistical releases and is accessed at\n\t<https://data.cso.ie>.\n\tThe CSO 'PxStat' Application Programming Interface (API), which is accessed in this package, provides\n\taccess to 'PxStat' data in JSON-stat format at <https://data.cso.ie>.\n\tThis dissemination tool allows developers machine to machine access to CSO 'PxStat' data.",
    "version": "1.5.1",
    "maintainer": "Conor Crowley <conor.crowley@cso.ie>",
    "author": "Eoin Horgan [aut] (ORCID: <https://orcid.org/0000-0002-3446-6154>),\n  Conor Crowley [aut, cre],\n  Vytas Vaiciulis [aut],\n  Mervyn O'Luing [aut],\n  James O'Rourke [aut]",
    "url": "https://github.com/CSOIreland/csodata",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=csodata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csodata Download Data from the CSO 'PxStat' API Imports 'PxStat' data in JSON-stat format and (optionally) reshapes it into wide\n\tformat. The Central Statistics Office (CSO) is the national statistical institute of Ireland\n\tand 'PxStat' is the CSOs online database of Official Statistics. This database contains current\n\tand historical data series compiled from CSO statistical releases and is accessed at\n\t<https://data.cso.ie>.\n\tThe CSO 'PxStat' Application Programming Interface (API), which is accessed in this package, provides\n\taccess to 'PxStat' data in JSON-stat format at <https://data.cso.ie>.\n\tThis dissemination tool allows developers machine to machine access to CSO 'PxStat' data.  "
  },
  {
    "id": 10650,
    "package_name": "csvread",
    "title": "Fast Specialized CSV File Loader",
    "description": "Functions for loading large (10M+ lines) CSV\n    and other delimited files, similar to read.csv, but typically faster and\n    using less memory than the standard R loader. While not entirely general,\n    it covers many common use cases when the types of columns in the CSV file\n    are known in advance. In addition, the package provides a class 'int64',\n    which represents 64-bit integers exactly when reading from a file. The\n    latter is useful when working with 64-bit integer identifiers exported from\n    databases. The CSV file loader supports common column types including\n    'integer', 'double', 'string', and 'int64', leaving further type\n    transformations  to the user.",
    "version": "1.2.3",
    "maintainer": "Sergei Izrailev <sizrailev@jabiruventures.com>",
    "author": "Sergei Izrailev [aut, cre]",
    "url": "https://github.com/jabiru/csvread",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=csvread",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csvread Fast Specialized CSV File Loader Functions for loading large (10M+ lines) CSV\n    and other delimited files, similar to read.csv, but typically faster and\n    using less memory than the standard R loader. While not entirely general,\n    it covers many common use cases when the types of columns in the CSV file\n    are known in advance. In addition, the package provides a class 'int64',\n    which represents 64-bit integers exactly when reading from a file. The\n    latter is useful when working with 64-bit integer identifiers exported from\n    databases. The CSV file loader supports common column types including\n    'integer', 'double', 'string', and 'int64', leaving further type\n    transformations  to the user.  "
  },
  {
    "id": 10667,
    "package_name": "ctrdata",
    "title": "Retrieve and Analyze Clinical Trials Data from Public Registers",
    "description": "A system for querying, retrieving and analyzing\n        protocol- and results-related information on clinical trials from\n        four public registers, the 'European Union Clinical Trials Register'\n        ('EUCTR', <https://www.clinicaltrialsregister.eu/>), \n        'ClinicalTrials.gov' (<https://clinicaltrials.gov/> and also \n        translating queries the retired classic interface), the \n        'ISRCTN' (<http://www.isrctn.com/>) and the\n        'European Union Clinical Trials Information System'\n        ('CTIS', <https://euclinicaltrials.eu/>). \n        Trial information is downloaded, converted and stored in a database \n        ('PostgreSQL', 'SQLite', 'DuckDB' or 'MongoDB'; via package 'nodbi'). \n        Protocols, statistical analysis plans, informed consent sheets and other\n        documents in registers associated with trials can also be downloaded. \n        Other functions implement trial concepts canonically across registers,\n        identify deduplicated records, easily find and extract variables \n        (fields) of interest even from complex nested data as used by the \n        registers, merge variables and update queries. \n        The package can be used for monitoring, meta- and trend-analysis of\n        the design and conduct as well as of the results of clinical trials \n        across registers.\n        See overview in Herold, R. (2025) <doi:10.1017/rsm.2025.10061>.",
    "version": "1.25.1",
    "maintainer": "Ralf Herold <ralf.herold@mailbox.org>",
    "author": "Ralf Herold [aut, cre] (ORCID: <https://orcid.org/0000-0002-8148-6748>),\n  Marek Kubica [cph] (node-xml2js library),\n  Ivan Bozhanov [cph] (jstree library)",
    "url": "https://cran.r-project.org/package=ctrdata,\nhttps://rfhb.github.io/ctrdata/",
    "bug_reports": "https://github.com/rfhb/ctrdata/issues",
    "repository": "https://cran.r-project.org/package=ctrdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctrdata Retrieve and Analyze Clinical Trials Data from Public Registers A system for querying, retrieving and analyzing\n        protocol- and results-related information on clinical trials from\n        four public registers, the 'European Union Clinical Trials Register'\n        ('EUCTR', <https://www.clinicaltrialsregister.eu/>), \n        'ClinicalTrials.gov' (<https://clinicaltrials.gov/> and also \n        translating queries the retired classic interface), the \n        'ISRCTN' (<http://www.isrctn.com/>) and the\n        'European Union Clinical Trials Information System'\n        ('CTIS', <https://euclinicaltrials.eu/>). \n        Trial information is downloaded, converted and stored in a database \n        ('PostgreSQL', 'SQLite', 'DuckDB' or 'MongoDB'; via package 'nodbi'). \n        Protocols, statistical analysis plans, informed consent sheets and other\n        documents in registers associated with trials can also be downloaded. \n        Other functions implement trial concepts canonically across registers,\n        identify deduplicated records, easily find and extract variables \n        (fields) of interest even from complex nested data as used by the \n        registers, merge variables and update queries. \n        The package can be used for monitoring, meta- and trend-analysis of\n        the design and conduct as well as of the results of clinical trials \n        across registers.\n        See overview in Herold, R. (2025) <doi:10.1017/rsm.2025.10061>.  "
  },
  {
    "id": 10668,
    "package_name": "ctrialsgov",
    "title": "Query Data from U.S. National Library of Medicine's Clinical\nTrials Database",
    "description": "Tools to create and query database from the U.S. National Library\n  of Medicine's Clinical Trials database <https://clinicaltrials.gov/>. Functions\n  provide access a variety of techniques for searching the data using range\n  queries, categorical filtering, and by searching for full-text keywords. ",
    "version": "0.2.7",
    "maintainer": "Taylor Arnold <tarnold2@richmond.edu>",
    "author": "Taylor Arnold [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0576-0669>),\n  Auston Wei [aut],\n  Michael J. Kane [aut] (ORCID: <https://orcid.org/0000-0003-1899-6662>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ctrialsgov",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctrialsgov Query Data from U.S. National Library of Medicine's Clinical\nTrials Database Tools to create and query database from the U.S. National Library\n  of Medicine's Clinical Trials database <https://clinicaltrials.gov/>. Functions\n  provide access a variety of techniques for searching the data using range\n  queries, categorical filtering, and by searching for full-text keywords.   "
  },
  {
    "id": 10774,
    "package_name": "dabr",
    "title": "Database Management with R",
    "description": "Provides functions to manage databases: select, update, insert,\n    and delete records, list tables, backup tables as CSV files, and import\n    CSV files as tables.",
    "version": "0.0.4",
    "maintainer": "Roberto Villegas-Diaz <r.villegas-diaz@outlook.com>",
    "author": "Roberto Villegas-Diaz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5036-8661>),\n  SPECIAL Research Group @ University of Reading [cph]",
    "url": "https://github.com/special-uor/dabr/,\nhttps://special-uor.github.io/dabr/,\nhttps://research.reading.ac.uk/palaeoclimate/",
    "bug_reports": "https://github.com/special-uor/dabr/issues/",
    "repository": "https://cran.r-project.org/package=dabr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dabr Database Management with R Provides functions to manage databases: select, update, insert,\n    and delete records, list tables, backup tables as CSV files, and import\n    CSV files as tables.  "
  },
  {
    "id": 10836,
    "package_name": "datamedios",
    "title": "Scraping Chilean Media",
    "description": "A system for extracting news from Chilean media,\n  specifically through Web Scapping from Chilean media. The package allows for news searches using search phrases and date filters,\n  and returns the results in a structured format, ready for analysis.\n  Additionally, it includes functions to clean the extracted data, visualize it, and\n  store it in databases. All of this can be done automatically, facilitating the collection and analysis of relevant\n  information from Chilean media.",
    "version": "1.2.2",
    "maintainer": "Exequiel Trujillo <exequiel.trujillo@ug.uchile.cl>",
    "author": "Exequiel Trujillo [aut, cre, cph, fnd],\n  Ismael Aguayo [aut, fnd],\n  Klaus Lehmann [aut, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=datamedios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datamedios Scraping Chilean Media A system for extracting news from Chilean media,\n  specifically through Web Scapping from Chilean media. The package allows for news searches using search phrases and date filters,\n  and returns the results in a structured format, ready for analysis.\n  Additionally, it includes functions to clean the extracted data, visualize it, and\n  store it in databases. All of this can be done automatically, facilitating the collection and analysis of relevant\n  information from Chilean media.  "
  },
  {
    "id": 10856,
    "package_name": "dataverifyr",
    "title": "A Lightweight, Flexible, and Fast Data Validation Package that\nCan Handle All Sizes of Data",
    "description": "Allows you to define rules which can be used to verify a given\n    dataset.\n    The package acts as a thin wrapper around more powerful data packages such\n    as 'dplyr', 'data.table', 'arrow', and 'DBI' ('SQL'), which do the heavy lifting.",
    "version": "0.1.8",
    "maintainer": "David Zimmermann-Kollenda <david_j_zimmermann@hotmail.com>",
    "author": "David Zimmermann-Kollenda [aut, cre],\n  Beniamino Green [ctb]",
    "url": "https://github.com/DavZim/dataverifyr,\nhttps://davzim.github.io/dataverifyr/",
    "bug_reports": "https://github.com/DavZim/dataverifyr/issues",
    "repository": "https://cran.r-project.org/package=dataverifyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataverifyr A Lightweight, Flexible, and Fast Data Validation Package that\nCan Handle All Sizes of Data Allows you to define rules which can be used to verify a given\n    dataset.\n    The package acts as a thin wrapper around more powerful data packages such\n    as 'dplyr', 'data.table', 'arrow', and 'DBI' ('SQL'), which do the heavy lifting.  "
  },
  {
    "id": 10870,
    "package_name": "datrProfile",
    "title": "Column Profile for Tables and Datasets",
    "description": "Profiles datasets (collecting statistics and informative summaries \n    about that data) on data frames and 'ODBC' tables: maximum, minimum, mean, standard deviation, nulls,\n    distinct values, data patterns, data/format frequencies.",
    "version": "0.1.0",
    "maintainer": "Arnaldo Vitaliano <vitaliano@gmail.com>",
    "author": "Arnaldo Vitaliano [aut, cre]",
    "url": "https://github.com/avitaliano/datrProfile",
    "bug_reports": "https://github.com/avitaliano/datrProfile/issues",
    "repository": "https://cran.r-project.org/package=datrProfile",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datrProfile Column Profile for Tables and Datasets Profiles datasets (collecting statistics and informative summaries \n    about that data) on data frames and 'ODBC' tables: maximum, minimum, mean, standard deviation, nulls,\n    distinct values, data patterns, data/format frequencies.  "
  },
  {
    "id": 10877,
    "package_name": "dbGaPCheckup",
    "title": "dbGaP Checkup",
    "description": "Contains functions that check for formatting of the Subject Phenotype data set and data dictionary as specified by the National Center for Biotechnology Information (NCBI) Database of Genotypes and Phenotypes (dbGaP) <https://www.ncbi.nlm.nih.gov/gap/docs/submissionguide/>. ",
    "version": "1.1.0",
    "maintainer": "Lacey W. Heinsberg <law145@pitt.edu>",
    "author": "Lacey W. Heinsberg [aut, cre],\n  Daniel E. Weeks [aut],\n  University of Pittsburgh [cph]",
    "url": "https://lwheinsberg.github.io/dbGaPCheckup/,\nhttps://github.com/lwheinsberg/dbGaPCheckup",
    "bug_reports": "https://github.com/lwheinsberg/dbGaPCheckup/issues",
    "repository": "https://cran.r-project.org/package=dbGaPCheckup",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbGaPCheckup dbGaP Checkup Contains functions that check for formatting of the Subject Phenotype data set and data dictionary as specified by the National Center for Biotechnology Information (NCBI) Database of Genotypes and Phenotypes (dbGaP) <https://www.ncbi.nlm.nih.gov/gap/docs/submissionguide/>.   "
  },
  {
    "id": 10879,
    "package_name": "dbWebForms",
    "title": "Produce R Functions to Create HTML Forms Based on SQL Meta Data",
    "description": "Offers meta programming style tools to generate configurable R functions that produce HTML forms based on table input and SQL meta data. \n    Also generates functions for collecting the parameters of those HTML forms after they are submitted. Useful for \n    quickly generating HTML forms based on existing SQL tables. To use the resultant functions, the output files containing those functions must \n    be read into the R environment (perhaps using base::source()).",
    "version": "0.1.0",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dbWebForms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbWebForms Produce R Functions to Create HTML Forms Based on SQL Meta Data Offers meta programming style tools to generate configurable R functions that produce HTML forms based on table input and SQL meta data. \n    Also generates functions for collecting the parameters of those HTML forms after they are submitted. Useful for \n    quickly generating HTML forms based on existing SQL tables. To use the resultant functions, the output files containing those functions must \n    be read into the R environment (perhaps using base::source()).  "
  },
  {
    "id": 10884,
    "package_name": "dbglm",
    "title": "Generalised Linear Models by Subsampling and One-Step Polishing",
    "description": "Fast fitting of generalised linear models on moderately large datasets, by taking an initial sample, fitting in memory, then evaluating the score function for the full data in the database. Thomas Lumley <doi:10.1080/10618600.2019.1610312>.",
    "version": "1.0.0",
    "maintainer": "Shangqing Cao <caoalbert@g.ucla.edu>",
    "author": "Thomas Lumley [aut, cph],\n  Shangqing Cao [ctb, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dbglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbglm Generalised Linear Models by Subsampling and One-Step Polishing Fast fitting of generalised linear models on moderately large datasets, by taking an initial sample, fitting in memory, then evaluating the score function for the full data in the database. Thomas Lumley <doi:10.1080/10618600.2019.1610312>.  "
  },
  {
    "id": 10885,
    "package_name": "dbhydroR",
    "title": "'DBHYDRO' Hydrologic and Water Quality Data",
    "description": "Client for programmatic access to the South Florida Water\n  Management District's 'DBHYDRO' database at \n  <https://www.sfwmd.gov/science-data/dbhydro>, with functions\n  for accessing hydrologic and water quality data. ",
    "version": "0.2-8",
    "maintainer": "Joseph Stachelek <stachel2@msu.edu>",
    "author": "Joseph Stachelek [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5924-2464>)",
    "url": "https://github.com/ropensci/dbhydroR,\nhttps://docs.ropensci.org/dbhydroR/",
    "bug_reports": "https://github.com/ropensci/dbhydroR/issues",
    "repository": "https://cran.r-project.org/package=dbhydroR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbhydroR 'DBHYDRO' Hydrologic and Water Quality Data Client for programmatic access to the South Florida Water\n  Management District's 'DBHYDRO' database at \n  <https://www.sfwmd.gov/science-data/dbhydro>, with functions\n  for accessing hydrologic and water quality data.   "
  },
  {
    "id": 10886,
    "package_name": "dbi.table",
    "title": "Database Queries Using 'data.table' Syntax",
    "description": "\n  Query database tables over a 'DBI' connection using 'data.table' syntax.\n  Attach database schemas to the search path. Automatically merge using foreign\n  key constraints.",
    "version": "1.0.5",
    "maintainer": "Kjell P. Konis <kjellk@gmail.com>",
    "author": "Kjell P. Konis [aut, cre],\n  Luis Rocha [ctb] (Chinook Database - see example_files/LICENSE)",
    "url": "https://github.com/kjellpk/dbi.table",
    "bug_reports": "https://github.com/kjellpk/dbi.table/issues",
    "repository": "https://cran.r-project.org/package=dbi.table",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbi.table Database Queries Using 'data.table' Syntax \n  Query database tables over a 'DBI' connection using 'data.table' syntax.\n  Attach database schemas to the search path. Automatically merge using foreign\n  key constraints.  "
  },
  {
    "id": 10892,
    "package_name": "dbplot",
    "title": "Simplifies Plotting Data Inside Databases",
    "description": "Leverages 'dplyr' to process the calculations of a plot inside a database. \n    This package provides helper functions that abstract the work at three levels:\n    outputs a 'ggplot', outputs the calculations, outputs the formula\n    needed to calculate bins.",
    "version": "0.3.3",
    "maintainer": "Edgar Ruiz <edgararuiz@gmail.com>",
    "author": "Edgar Ruiz [aut, cre]",
    "url": "https://github.com/edgararuiz/dbplot",
    "bug_reports": "https://github.com/edgararuiz/dbplot/issues",
    "repository": "https://cran.r-project.org/package=dbplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbplot Simplifies Plotting Data Inside Databases Leverages 'dplyr' to process the calculations of a plot inside a database. \n    This package provides helper functions that abstract the work at three levels:\n    outputs a 'ggplot', outputs the calculations, outputs the formula\n    needed to calculate bins.  "
  },
  {
    "id": 10897,
    "package_name": "dbx",
    "title": "A Fast, Easy-to-Use Database Interface",
    "description": "Provides select, insert, update, upsert, and delete database operations. Supports 'PostgreSQL', 'MySQL', 'SQLite', and more, and plays nicely with the 'DBI' package.",
    "version": "0.4.0",
    "maintainer": "Andrew Kane <andrew@chartkick.com>",
    "author": "Andrew Kane [aut, cre]",
    "url": "https://github.com/ankane/dbx",
    "bug_reports": "https://github.com/ankane/dbx/issues",
    "repository": "https://cran.r-project.org/package=dbx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dbx A Fast, Easy-to-Use Database Interface Provides select, insert, update, upsert, and delete database operations. Supports 'PostgreSQL', 'MySQL', 'SQLite', and more, and plays nicely with the 'DBI' package.  "
  },
  {
    "id": 10924,
    "package_name": "ddp",
    "title": "Desirable Dietary Pattern",
    "description": "The desirable Dietary Pattern (DDP)/ PPH score\n  measures the variety of food consumption. The (weighted) score is calculated \n  based on the type of food. This package is intended to calculate\n  the DDP/ PPH score that is faster than traditional method via \n  a manual calculation by BKP (2017) <http://bkp.pertanian.go.id/storage/app/uploads/public/5bf/ca9/06b/5bfca906bc654274163456.pdf> and is simpler than the nutrition survey \n  <http://www.nutrisurvey.de>. The database to create weights and baseline values\n  is the Indonesia national survey in 2017.",
    "version": "0.0.3",
    "maintainer": "Weksi Budiaji <budiaji@untirta.ac.id>",
    "author": "Weksi Budiaji [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ddp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ddp Desirable Dietary Pattern The desirable Dietary Pattern (DDP)/ PPH score\n  measures the variety of food consumption. The (weighted) score is calculated \n  based on the type of food. This package is intended to calculate\n  the DDP/ PPH score that is faster than traditional method via \n  a manual calculation by BKP (2017) <http://bkp.pertanian.go.id/storage/app/uploads/public/5bf/ca9/06b/5bfca906bc654274163456.pdf> and is simpler than the nutrition survey \n  <http://www.nutrisurvey.de>. The database to create weights and baseline values\n  is the Indonesia national survey in 2017.  "
  },
  {
    "id": 10965,
    "package_name": "deepdep",
    "title": "Visualise and Explore the Deep Dependencies of R Packages",
    "description": "Provides tools for exploration of R package dependencies. \n    The main deepdep() function allows to acquire deep dependencies of any package and plot them in an elegant way.\n    It also adds some popularity measures for the packages e.g. in the form of download count through the 'cranlogs' package. \n    Uses the CRAN metadata database <http://crandb.r-pkg.org> and Bioconductor metadata <https://bioconductor.org>.\n    Other data acquire functions are: get_dependencies(), get_downloads() and get_description(). \n    The deepdep_shiny() function runs shiny application that helps to produce a nice 'deepdep' plot. ",
    "version": "0.4.4",
    "maintainer": "Dominik Rafacz <dominikrafacz@gmail.com>",
    "author": "Dominik Rafacz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0925-1909>),\n  Hubert Baniecki [aut],\n  Szymon Maksymiuk [aut],\n  Laura Bakala [aut],\n  Dirk Eddelbuettel [ctb]",
    "url": "https://dominikrafacz.github.io/deepdep/,\nhttps://github.com/DominikRafacz/deepdep",
    "bug_reports": "https://github.com/DominikRafacz/deepdep/issues",
    "repository": "https://cran.r-project.org/package=deepdep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deepdep Visualise and Explore the Deep Dependencies of R Packages Provides tools for exploration of R package dependencies. \n    The main deepdep() function allows to acquire deep dependencies of any package and plot them in an elegant way.\n    It also adds some popularity measures for the packages e.g. in the form of download count through the 'cranlogs' package. \n    Uses the CRAN metadata database <http://crandb.r-pkg.org> and Bioconductor metadata <https://bioconductor.org>.\n    Other data acquire functions are: get_dependencies(), get_downloads() and get_description(). \n    The deepdep_shiny() function runs shiny application that helps to produce a nice 'deepdep' plot.   "
  },
  {
    "id": 11120,
    "package_name": "diathor",
    "title": "Calculate Ecological Information and Diatom Based Indices",
    "description": "\n    Calculate multiple biotic indices using diatoms from environmental samples. Diatom species are recognized by their species' name using a heuristic search, and their ecological data is retrieved from multiple sources.\n    It includes number/shape of chloroplasts diversity indices, size classes, ecological guilds, and multiple biotic indices.\n    It outputs both a dataframe with all the results and plots of all the obtained data in a defined output folder.\n    - Sample data was taken from Nicolosi Gelis, Cochero & G\u00f3mez (2020, <doi:10.1016/j.ecolind.2019.105951>).\n    - The package uses the 'Diat.Barcode' database to calculate morphological and ecological information by Rimet & Couchez (2012, <doi:10.1051/kmae/2012018>),and the combined classification of guilds and size classes established by B-B\u00e9res et al. (2017, <doi:10.1016/j.ecolind.2017.07.007>).\n    - Current diatom-based biotic indices include the DES index by Descy (1979)\n    - EPID index by Dell'Uomo (1996, ISBN: 3950009002)\n    - IDAP index by Prygiel & Coste (1993, <doi:10.1007/BF00028033>)\n    - ID-CH index by H\u00fcrlimann & Niederhauser (2007)\n    - IDP index by G\u00f3mez & Licursi (2001, <doi:10.1023/A:1011415209445>)\n    - ILM index by Leclercq & Maquet (1987)\n    - IPS index by Coste (1982)\n    - LOBO index by Lobo, Callegaro, & Bender (2002, ISBN:9788585869908)\n    - SLA by Sl\u00e1de\u010dek (1986, <doi:10.1002/aheh.19860140519>)\n    - TDI index by Kelly, & Whitton (1995, <doi:10.1007/BF00003802>)\n    - SPEAR(herbicide) index by Wood, Mitrovic, Lim, Warne, Dunlop, & Kefford (2019, <doi:10.1016/j.ecolind.2018.12.035>)\n    - PBIDW index by Castro-Roa & Pinilla-Agudelo (2014)\n    - DISP index by Stenger-Kov\u00e1cs et al. (2018, <doi:10.1016/j.ecolind.2018.07.026>)\n    - EDI index by Chamorro et al. (2024, <doi:10.1021/acsestwater.4c00126>)\n    - DDI index by \u00c1lvarez-Blanco et al. (2013, <doi: 10.1007/s10661-012-2607-z>)\n    - PDISE index by Kahlert et al. (2023, <doi:10.1007/s10661-023-11378-4>).",
    "version": "0.1.5",
    "maintainer": "Joaquin Cochero <jcochero@ilpla.edu.ar>",
    "author": "Maria Mercedes Nicolosi Gelis [aut] (ORCID:\n    <https://orcid.org/0000-0001-6324-7930>),\n  Maria Belen Sathicq [aut] (ORCID:\n    <https://orcid.org/0000-0002-3534-8950>),\n  Joaquin Cochero [cre] (ORCID: <https://orcid.org/0000-0003-3957-6819>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=diathor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diathor Calculate Ecological Information and Diatom Based Indices \n    Calculate multiple biotic indices using diatoms from environmental samples. Diatom species are recognized by their species' name using a heuristic search, and their ecological data is retrieved from multiple sources.\n    It includes number/shape of chloroplasts diversity indices, size classes, ecological guilds, and multiple biotic indices.\n    It outputs both a dataframe with all the results and plots of all the obtained data in a defined output folder.\n    - Sample data was taken from Nicolosi Gelis, Cochero & G\u00f3mez (2020, <doi:10.1016/j.ecolind.2019.105951>).\n    - The package uses the 'Diat.Barcode' database to calculate morphological and ecological information by Rimet & Couchez (2012, <doi:10.1051/kmae/2012018>),and the combined classification of guilds and size classes established by B-B\u00e9res et al. (2017, <doi:10.1016/j.ecolind.2017.07.007>).\n    - Current diatom-based biotic indices include the DES index by Descy (1979)\n    - EPID index by Dell'Uomo (1996, ISBN: 3950009002)\n    - IDAP index by Prygiel & Coste (1993, <doi:10.1007/BF00028033>)\n    - ID-CH index by H\u00fcrlimann & Niederhauser (2007)\n    - IDP index by G\u00f3mez & Licursi (2001, <doi:10.1023/A:1011415209445>)\n    - ILM index by Leclercq & Maquet (1987)\n    - IPS index by Coste (1982)\n    - LOBO index by Lobo, Callegaro, & Bender (2002, ISBN:9788585869908)\n    - SLA by Sl\u00e1de\u010dek (1986, <doi:10.1002/aheh.19860140519>)\n    - TDI index by Kelly, & Whitton (1995, <doi:10.1007/BF00003802>)\n    - SPEAR(herbicide) index by Wood, Mitrovic, Lim, Warne, Dunlop, & Kefford (2019, <doi:10.1016/j.ecolind.2018.12.035>)\n    - PBIDW index by Castro-Roa & Pinilla-Agudelo (2014)\n    - DISP index by Stenger-Kov\u00e1cs et al. (2018, <doi:10.1016/j.ecolind.2018.07.026>)\n    - EDI index by Chamorro et al. (2024, <doi:10.1021/acsestwater.4c00126>)\n    - DDI index by \u00c1lvarez-Blanco et al. (2013, <doi: 10.1007/s10661-012-2607-z>)\n    - PDISE index by Kahlert et al. (2023, <doi:10.1007/s10661-023-11378-4>).  "
  },
  {
    "id": 11195,
    "package_name": "discoverableresearch",
    "title": "Checks Title, Abstract and Keywords to Optimise Discoverability",
    "description": "A suite of tools are provided here to support authors in making their research more discoverable. \n    check_keywords() - this function checks the keywords to assess whether they are already represented in the \n    title and abstract. check_fields() - this function compares terminology used across the title, abstract \n    and keywords to assess where terminological diversity (i.e. the use of synonyms) could increase the likelihood \n    of the record being identified in a search. The function looks for terms in the title and abstract that also \n    exist in other fields and highlights these as needing attention. suggest_keywords() - this function takes a \n    full text document and produces a list of unigrams, bigrams and trigrams (1-, 2- or 2-word phrases) \n    present in the full text after removing stop words (words with a low utility in natural language processing) \n    that do not occur in the title or abstract that may be suitable candidates for keywords. suggest_title() - \n    this function takes a full text document and produces a list of the most frequently used unigrams, bigrams \n    and trigrams after removing stop words that do not occur in the abstract or keywords that may be suitable \n    candidates for title words. check_title() - this function carries out a number of sub tasks:  1) it compares \n    the length (number of words) of the title with the mean length of titles in major bibliographic databases to \n    assess whether the title is likely to be too short; 2) it assesses the proportion of stop words in the title \n    to highlight titles with low utility in search engines that strip out stop words; 3) it compares the title \n    with a given sample of record titles from an .ris import and calculates a similarity score based on phrase \n    overlap. This highlights the level of uniqueness of the title. This version of the package also contains \n    functions currently in a non-CRAN package called 'litsearchr' <https://github.com/elizagrames/litsearchr>.",
    "version": "0.0.1",
    "maintainer": "Neal Haddaway <nealhaddaway@gmail.com>",
    "author": "Neal Haddaway [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3902-2234>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=discoverableresearch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "discoverableresearch Checks Title, Abstract and Keywords to Optimise Discoverability A suite of tools are provided here to support authors in making their research more discoverable. \n    check_keywords() - this function checks the keywords to assess whether they are already represented in the \n    title and abstract. check_fields() - this function compares terminology used across the title, abstract \n    and keywords to assess where terminological diversity (i.e. the use of synonyms) could increase the likelihood \n    of the record being identified in a search. The function looks for terms in the title and abstract that also \n    exist in other fields and highlights these as needing attention. suggest_keywords() - this function takes a \n    full text document and produces a list of unigrams, bigrams and trigrams (1-, 2- or 2-word phrases) \n    present in the full text after removing stop words (words with a low utility in natural language processing) \n    that do not occur in the title or abstract that may be suitable candidates for keywords. suggest_title() - \n    this function takes a full text document and produces a list of the most frequently used unigrams, bigrams \n    and trigrams after removing stop words that do not occur in the abstract or keywords that may be suitable \n    candidates for title words. check_title() - this function carries out a number of sub tasks:  1) it compares \n    the length (number of words) of the title with the mean length of titles in major bibliographic databases to \n    assess whether the title is likely to be too short; 2) it assesses the proportion of stop words in the title \n    to highlight titles with low utility in search engines that strip out stop words; 3) it compares the title \n    with a given sample of record titles from an .ris import and calculates a similarity score based on phrase \n    overlap. This highlights the level of uniqueness of the title. This version of the package also contains \n    functions currently in a non-CRAN package called 'litsearchr' <https://github.com/elizagrames/litsearchr>.  "
  },
  {
    "id": 11243,
    "package_name": "divDyn",
    "title": "Diversity Dynamics using Fossil Sampling Data",
    "description": "Functions to describe sampling and diversity dynamics of fossil occurrence datasets (e.g. from the Paleobiology Database). The package includes methods to calculate range- and occurrence-based metrics of taxonomic richness, extinction and origination rates, along with traditional sampling measures. A powerful subsampling tool is also included that implements frequently used sampling standardization methods in a multiple bin-framework. The plotting of time series and the occurrence data can be simplified by the functions incorporated in the package, as well as other calculations, such as environmental affinities and extinction selectivity testing. Details can be found in: Kocsis, A.T.; Reddin, C.J.; Alroy, J. and Kiessling, W. (2019) <doi:10.1101/423780>.",
    "version": "0.8.3",
    "maintainer": "Adam T. Kocsis <adam.t.kocsis@gmail.com>",
    "author": "Adam T. Kocsis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9028-665X>),\n  John Alroy [aut] (ORCID: <https://orcid.org/0000-0002-9882-2111>),\n  Carl J. Reddin [aut] (ORCID: <https://orcid.org/0000-0001-5930-1164>),\n  Wolfgang Kiessling [aut] (ORCID:\n    <https://orcid.org/0000-0002-1088-2014>),\n  Deutsche Forschungsgemeinschaft [fnd],\n  FAU GeoZentrum Nordbayern [fnd]",
    "url": "",
    "bug_reports": "https://github.com/divDyn/r-package/issues",
    "repository": "https://cran.r-project.org/package=divDyn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "divDyn Diversity Dynamics using Fossil Sampling Data Functions to describe sampling and diversity dynamics of fossil occurrence datasets (e.g. from the Paleobiology Database). The package includes methods to calculate range- and occurrence-based metrics of taxonomic richness, extinction and origination rates, along with traditional sampling measures. A powerful subsampling tool is also included that implements frequently used sampling standardization methods in a multiple bin-framework. The plotting of time series and the occurrence data can be simplified by the functions incorporated in the package, as well as other calculations, such as environmental affinities and extinction selectivity testing. Details can be found in: Kocsis, A.T.; Reddin, C.J.; Alroy, J. and Kiessling, W. (2019) <doi:10.1101/423780>.  "
  },
  {
    "id": 11271,
    "package_name": "dm",
    "title": "Relational Data Models",
    "description": "Provides tools for working with multiple related\n    tables, stored as data frames or in a relational database.  Multiple\n    tables (data and metadata) are stored in a compound object, which can\n    then be manipulated with a pipe-friendly syntax.",
    "version": "1.0.12",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Tobias Schieferdecker [aut],\n  Kirill M\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1416-3412>),\n  Antoine Fabri [ctb],\n  Darko Bergant [aut],\n  Katharina Brunner [ctb],\n  James Wondrasek [ctb],\n  Indrajeet Patil [ctb] (ORCID: <https://orcid.org/0000-0003-1995-6531>),\n  Ma\u00eblle Salmon [ctb] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\n  energie360\u00b0 AG [fnd],\n  cynkra GmbH [fnd, cph] (ROR: <https://ror.org/0335t7e62>)",
    "url": "https://dm.cynkra.com/, https://github.com/cynkra/dm",
    "bug_reports": "https://github.com/cynkra/dm/issues",
    "repository": "https://cran.r-project.org/package=dm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dm Relational Data Models Provides tools for working with multiple related\n    tables, stored as data frames or in a relational database.  Multiple\n    tables (data and metadata) are stored in a compound object, which can\n    then be manipulated with a pipe-friendly syntax.  "
  },
  {
    "id": 11295,
    "package_name": "doRedis",
    "title": "'Foreach' Parallel Adapter Using the 'Redis' Database",
    "description": "Create and manage fault-tolerant task queues for the 'foreach' package using the 'Redis' key/value database.",
    "version": "3.0.3",
    "maintainer": "B. W. Lewis <blewis@illposed.net>",
    "author": "B. W. Lewis [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/bwlewis/doRedis/issues",
    "repository": "https://cran.r-project.org/package=doRedis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doRedis 'Foreach' Parallel Adapter Using the 'Redis' Database Create and manage fault-tolerant task queues for the 'foreach' package using the 'Redis' key/value database.  "
  },
  {
    "id": 11453,
    "package_name": "duckdbfs",
    "title": "High Performance Remote File System, Database and 'Geospatial'\nAccess Using 'duckdb'",
    "description": "Provides friendly wrappers for creating 'duckdb'-backed connections\n  to tabular datasets ('csv', parquet, etc) on local or remote file systems.\n  This mimics the behaviour of \"open_dataset\" in the 'arrow' package, \n  but in addition to 'S3' file system also generalizes to any list of 'http' URLs.",
    "version": "0.1.2",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>),\n  Michael D. Sumner [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2471-7511>)",
    "url": "https://github.com/cboettig/duckdbfs,\nhttps://cboettig.github.io/duckdbfs/",
    "bug_reports": "https://github.com/cboettig/duckdbfs/issues",
    "repository": "https://cran.r-project.org/package=duckdbfs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "duckdbfs High Performance Remote File System, Database and 'Geospatial'\nAccess Using 'duckdb' Provides friendly wrappers for creating 'duckdb'-backed connections\n  to tabular datasets ('csv', parquet, etc) on local or remote file systems.\n  This mimics the behaviour of \"open_dataset\" in the 'arrow' package, \n  but in addition to 'S3' file system also generalizes to any list of 'http' URLs.  "
  },
  {
    "id": 11455,
    "package_name": "duckspatial",
    "title": "R Interface to 'DuckDB' Database with Spatial Extension",
    "description": "Provides an interface between R and the 'DuckDB' (see <https://duckdb.org>) database with spatial extensions. It supports reading, writing, and performing some geometric operations.",
    "version": "0.2.0",
    "maintainer": "Adri\u00e1n Cidre Gonz\u00e1lez <adrian.cidre@gmail.com>",
    "author": "Adri\u00e1n Cidre Gonz\u00e1lez [aut, cre]",
    "url": "https://cidree.github.io/duckspatial/,\nhttps://github.com/Cidree/duckspatial",
    "bug_reports": "https://github.com/Cidree/duckspatial/issues",
    "repository": "https://cran.r-project.org/package=duckspatial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "duckspatial R Interface to 'DuckDB' Database with Spatial Extension Provides an interface between R and the 'DuckDB' (see <https://duckdb.org>) database with spatial extensions. It supports reading, writing, and performing some geometric operations.  "
  },
  {
    "id": 11519,
    "package_name": "eFRED",
    "title": "Fetch Data from the Federal Reserve Economic Database",
    "description": "\n      Interact with the FRED API, <https://fred.stlouisfed.org/docs/api/fred/>, \n      to fetch observations across economic series;\n      find information about different economic sources, releases, series, etc.;\n      conduct searches by series name, attributes, or tags; and determine the \n      latest updates. Includes functions for creating panels of related variables \n      with minimal effort and datasets containing data sources, releases, and \n      popular FRED tags.",
    "version": "0.1.0",
    "maintainer": "Chris Mann <cmann3@unl.edu>",
    "author": "Chris Mann <cmann3@unl.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eFRED",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eFRED Fetch Data from the Federal Reserve Economic Database \n      Interact with the FRED API, <https://fred.stlouisfed.org/docs/api/fred/>, \n      to fetch observations across economic series;\n      find information about different economic sources, releases, series, etc.;\n      conduct searches by series name, attributes, or tags; and determine the \n      latest updates. Includes functions for creating panels of related variables \n      with minimal effort and datasets containing data sources, releases, and \n      popular FRED tags.  "
  },
  {
    "id": 11549,
    "package_name": "easyScieloPack",
    "title": "Easy Interface to Search 'SciELO' Database",
    "description": "Provides a simple interface to search and retrieve scientific articles\n    from the 'SciELO' (Scientific Electronic Library Online) database <https://scielo.org>.\n    It allows querying, filtering, and visualizing results in an interactive table.",
    "version": "0.1.1",
    "maintainer": "Pablo Ixcamparij <jose.sorto@ucr.ac.cr>",
    "author": "Pablo Ixcamparij [aut, cre],\n  Keneth Masis Leandro [aut]",
    "url": "https://github.com/Programa-ISA/easyScieloPack",
    "bug_reports": "https://github.com/Programa-ISA/easyScieloPack/issues",
    "repository": "https://cran.r-project.org/package=easyScieloPack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easyScieloPack Easy Interface to Search 'SciELO' Database Provides a simple interface to search and retrieve scientific articles\n    from the 'SciELO' (Scientific Electronic Library Online) database <https://scielo.org>.\n    It allows querying, filtering, and visualizing results in an interactive table.  "
  },
  {
    "id": 11556,
    "package_name": "easybio",
    "title": "Comprehensive Single-Cell Annotation and Transcriptomic Analysis\nToolkit",
    "description": "Provides a comprehensive toolkit for single-cell annotation with the 'CellMarker2.0' database (see Xia Li, Peng Wang, Yunpeng Zhang (2023) <doi: 10.1093/nar/gkac947>). Streamlines biological label assignment in single-cell RNA-seq data and facilitates transcriptomic analysis, including preparation of TCGA<https://portal.gdc.cancer.gov/> and GEO<https://www.ncbi.nlm.nih.gov/geo/> datasets, differential expression analysis and visualization of enrichment analysis results. Additional utility functions support various bioinformatics workflows. See Wei Cui (2024) <doi: 10.1101/2024.09.14.609619> for more details.",
    "version": "1.2.3",
    "maintainer": "Wei Cui <m2c.w@outlook.com>",
    "author": "Wei Cui [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0004-8315-5899>)",
    "url": "https://github.com/person-c/easybio",
    "bug_reports": "https://github.com/person-c/easybio/issues",
    "repository": "https://cran.r-project.org/package=easybio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easybio Comprehensive Single-Cell Annotation and Transcriptomic Analysis\nToolkit Provides a comprehensive toolkit for single-cell annotation with the 'CellMarker2.0' database (see Xia Li, Peng Wang, Yunpeng Zhang (2023) <doi: 10.1093/nar/gkac947>). Streamlines biological label assignment in single-cell RNA-seq data and facilitates transcriptomic analysis, including preparation of TCGA<https://portal.gdc.cancer.gov/> and GEO<https://www.ncbi.nlm.nih.gov/geo/> datasets, differential expression analysis and visualization of enrichment analysis results. Additional utility functions support various bioinformatics workflows. See Wei Cui (2024) <doi: 10.1101/2024.09.14.609619> for more details.  "
  },
  {
    "id": 11559,
    "package_name": "easydb",
    "title": "Easily Connect to Common Types of Databases",
    "description": "A unified interface for connecting to databases ('SQLite', 'MySQL', 'PostgreSQL').\n    Just provide the database name and the package will ask you questions\n    to help you configure the connection and setup your credentials.  Once\n    database configuration and connection has been set up once, you won't\n    have to do it ever again.",
    "version": "1.1.0",
    "maintainer": "Sam El-Kamand <sam.elkamand@gmail.com>",
    "author": "Sam El-Kamand [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2270-8088>)",
    "url": "https://github.com/selkamand/easydb,\nhttps://selkamand.github.io/easydb/",
    "bug_reports": "https://github.com/selkamand/easydb/issues",
    "repository": "https://cran.r-project.org/package=easydb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easydb Easily Connect to Common Types of Databases A unified interface for connecting to databases ('SQLite', 'MySQL', 'PostgreSQL').\n    Just provide the database name and the package will ask you questions\n    to help you configure the connection and setup your credentials.  Once\n    database configuration and connection has been set up once, you won't\n    have to do it ever again.  "
  },
  {
    "id": 11561,
    "package_name": "easynem",
    "title": "Nematode Community Analysis",
    "description": "Provides a built-in Nemaplex database for nematodes, \n    which can be used to search for various nematodes. \n    Also supports various nematode community and functional \n    analyses such as nematode diversity, maturity index, metabolic footprint, \n    and functional guild. The methods are based on\n     <https://shiny.wur.nl/ninja/>, Bongers, T. (1990)\n     <doi:10.1007/BF00324627>, Ferris, H. (2010)\n     <doi:10.1016/j.ejsobi.2010.01.003>, Wan, B. et al. (2022)\n     <doi:10.1016/j.soilbio.2022.108695>, and Van Den Hoogen, J. et al.\n     (2019) <doi:10.1038/s41586-019-1418-6>.",
    "version": "1.0.3",
    "maintainer": "Wang Kunguang <whkygl@163.com>",
    "author": "Wang Kunguang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7384-5002>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=easynem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easynem Nematode Community Analysis Provides a built-in Nemaplex database for nematodes, \n    which can be used to search for various nematodes. \n    Also supports various nematode community and functional \n    analyses such as nematode diversity, maturity index, metabolic footprint, \n    and functional guild. The methods are based on\n     <https://shiny.wur.nl/ninja/>, Bongers, T. (1990)\n     <doi:10.1007/BF00324627>, Ferris, H. (2010)\n     <doi:10.1016/j.ejsobi.2010.01.003>, Wan, B. et al. (2022)\n     <doi:10.1016/j.soilbio.2022.108695>, and Van Den Hoogen, J. et al.\n     (2019) <doi:10.1038/s41586-019-1418-6>.  "
  },
  {
    "id": 11570,
    "package_name": "eatDB",
    "title": "Spreadsheet Interface for Relational Databases",
    "description": "Use 'SQLite3' as a database system via a complete SQL free R interface, treating the data as if it was a single spreadsheet.",
    "version": "0.5.0",
    "maintainer": "Benjamin Becker <b.becker@iqb.hu-berlin.de>",
    "author": "Benjamin Becker [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eatDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eatDB Spreadsheet Interface for Relational Databases Use 'SQLite3' as a database system via a complete SQL free R interface, treating the data as if it was a single spreadsheet.  "
  },
  {
    "id": 11571,
    "package_name": "eatGADS",
    "title": "Data Management of Large Hierarchical Data",
    "description": "Import 'SPSS' data, handle and change 'SPSS' meta data, store and access large hierarchical data in 'SQLite' data bases.",
    "version": "1.2.0",
    "maintainer": "Benjamin Becker <b.becker@iqb.hu-berlin.de>",
    "author": "Benjamin Becker [aut, cre],\n  Karoline Sachse [ctb],\n  Johanna Busse [ctb]",
    "url": "https://github.com/beckerbenj/eatGADS,\nhttps://beckerbenj.github.io/eatGADS/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eatGADS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eatGADS Data Management of Large Hierarchical Data Import 'SPSS' data, handle and change 'SPSS' meta data, store and access large hierarchical data in 'SQLite' data bases.  "
  },
  {
    "id": 11581,
    "package_name": "ebirdst",
    "title": "Access and Analyze eBird Status and Trends Data Products",
    "description": "Tools for accessing and analyzing eBird Status and\n    Trends Data Products\n    (<https://science.ebird.org/en/status-and-trends>). eBird\n    (<https://ebird.org/home>) is a global database of bird observations\n    collected by member of the public. eBird Status and Trends uses these\n    data to model global bird distributions, abundances, and population trends \n    at a high spatial and temporal resolution.",
    "version": "3.2023.1",
    "maintainer": "Matthew Strimas-Mackey <mes335@cornell.edu>",
    "author": "Matthew Strimas-Mackey [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8929-7776>),\n  Shawn Ligocki [aut],\n  Tom Auer [aut] (ORCID: <https://orcid.org/0000-0001-8619-7147>),\n  Daniel Fink [aut] (ORCID: <https://orcid.org/0000-0002-8368-1248>),\n  Cornell Lab of Ornithology [cph]",
    "url": "https://ebird.github.io/ebirdst/, https://github.com/ebird/ebirdst",
    "bug_reports": "https://github.com/ebird/ebirdst/issues",
    "repository": "https://cran.r-project.org/package=ebirdst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ebirdst Access and Analyze eBird Status and Trends Data Products Tools for accessing and analyzing eBird Status and\n    Trends Data Products\n    (<https://science.ebird.org/en/status-and-trends>). eBird\n    (<https://ebird.org/home>) is a global database of bird observations\n    collected by member of the public. eBird Status and Trends uses these\n    data to model global bird distributions, abundances, and population trends \n    at a high spatial and temporal resolution.  "
  },
  {
    "id": 11661,
    "package_name": "edgar",
    "title": "Tool for the U.S. SEC EDGAR Retrieval and Parsing of Corporate\nFilings",
    "description": "In the USA, companies file different forms with the U.S. \n    Securities and Exchange Commission (SEC) through EDGAR (Electronic \n    Data Gathering, Analysis, and Retrieval system). The EDGAR \n    database automated system collects all the different necessary \n    filings and makes it publicly available. This package facilitates\n    retrieving, storing, searching, and parsing of all the available \n    filings on the EDGAR server. It downloads filings from SEC \n    server in bulk with a single query. Additionally, it provides \n    various useful functions: extracts 8-K triggering events, extract\n    \"Business (Item 1)\" and \"Management's Discussion and Analysis(Item 7)\"\n    sections of annual statements, searches filings for desired \n    keywords, provides sentiment measures, parses filing header \n    information, and provides HTML view of SEC filings.  ",
    "version": "2.0.8",
    "maintainer": "Gunratan Lonare <lonare.gunratan@gmail.com>",
    "author": "Gunratan Lonare [aut, cre],\n  Bharat Patil [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=edgar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "edgar Tool for the U.S. SEC EDGAR Retrieval and Parsing of Corporate\nFilings In the USA, companies file different forms with the U.S. \n    Securities and Exchange Commission (SEC) through EDGAR (Electronic \n    Data Gathering, Analysis, and Retrieval system). The EDGAR \n    database automated system collects all the different necessary \n    filings and makes it publicly available. This package facilitates\n    retrieving, storing, searching, and parsing of all the available \n    filings on the EDGAR server. It downloads filings from SEC \n    server in bulk with a single query. Additionally, it provides \n    various useful functions: extracts 8-K triggering events, extract\n    \"Business (Item 1)\" and \"Management's Discussion and Analysis(Item 7)\"\n    sections of annual statements, searches filings for desired \n    keywords, provides sentiment measures, parses filing header \n    information, and provides HTML view of SEC filings.    "
  },
  {
    "id": 11671,
    "package_name": "editbl",
    "title": "'DT' Extension for CRUD (Create, Read, Update, Delete)\nApplications in 'shiny'",
    "description": "The core of this package is a function eDT() which enhances DT::datatable() such that it can be used to interactively modify data in 'shiny'. By the use of generic 'dplyr' methods it supports many types of data storage, with relational databases ('dbplyr') being the main use case.",
    "version": "1.3.0",
    "maintainer": "Jasper Schelfhout <jasper.schelfhout@openanalytics.eu>",
    "author": "Jasper Schelfhout [aut, cre],\n  Maxim Nazarov [rev],\n  Daan Seynaeve [rev],\n  Lennart Tuijnder [rev],\n  Saar Junius [aut]",
    "url": "https://github.com/openanalytics/editbl",
    "bug_reports": "https://github.com/openanalytics/editbl/issues",
    "repository": "https://cran.r-project.org/package=editbl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "editbl 'DT' Extension for CRUD (Create, Read, Update, Delete)\nApplications in 'shiny' The core of this package is a function eDT() which enhances DT::datatable() such that it can be used to interactively modify data in 'shiny'. By the use of generic 'dplyr' methods it supports many types of data storage, with relational databases ('dbplyr') being the main use case.  "
  },
  {
    "id": 11683,
    "package_name": "eegkitdata",
    "title": "Electroencephalography Toolkit Datasets",
    "description": "Contains the example EEG data used in the package eegkit. Also contains code for easily creating larger EEG datasets from the EEG Database on the UCI Machine Learning Repository.",
    "version": "1.1",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "author": "Nathaniel E. Helwig <helwig@umn.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eegkitdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eegkitdata Electroencephalography Toolkit Datasets Contains the example EEG data used in the package eegkit. Also contains code for easily creating larger EEG datasets from the EEG Database on the UCI Machine Learning Repository.  "
  },
  {
    "id": 11729,
    "package_name": "elastic",
    "title": "General Purpose Interface to 'Elasticsearch'",
    "description": "Connect to 'Elasticsearch', a 'NoSQL' database built on the 'Java'\n    Virtual Machine. Interacts with the 'Elasticsearch' 'HTTP' API\n    (<https://www.elastic.co/elasticsearch/>), including functions for\n    setting connection details to 'Elasticsearch' instances, loading bulk data,\n    searching for documents with both 'HTTP' query variables and 'JSON' based body\n    requests. In addition, 'elastic' provides functions for interacting with API's\n    for 'indices', documents, nodes, clusters, an interface to the cat API, and\n    more.",
    "version": "1.2.0",
    "maintainer": "Scott Chamberlain <myrmecocystus@gmail.com>",
    "author": "Scott Chamberlain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1444-9135>)",
    "url": "https://docs.ropensci.org/elastic/ (website),\nhttps://github.com/ropensci/elastic",
    "bug_reports": "https://github.com/ropensci/elastic/issues",
    "repository": "https://cran.r-project.org/package=elastic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "elastic General Purpose Interface to 'Elasticsearch' Connect to 'Elasticsearch', a 'NoSQL' database built on the 'Java'\n    Virtual Machine. Interacts with the 'Elasticsearch' 'HTTP' API\n    (<https://www.elastic.co/elasticsearch/>), including functions for\n    setting connection details to 'Elasticsearch' instances, loading bulk data,\n    searching for documents with both 'HTTP' query variables and 'JSON' based body\n    requests. In addition, 'elastic' provides functions for interacting with API's\n    for 'indices', documents, nodes, clusters, an interface to the cat API, and\n    more.  "
  },
  {
    "id": 11781,
    "package_name": "emuR",
    "title": "Main Package of the EMU Speech Database Management System",
    "description": "Provide the EMU Speech Database Management System (EMU-SDMS) with\n    database management, data extraction, data preparation and data\n    visualization facilities. See <https://ips-lmu.github.io/The-EMU-SDMS-Manual/>\n    for more details.",
    "version": "2.5.2",
    "maintainer": "Markus Jochim <markusjochim@phonetik.uni-muenchen.de>",
    "author": "Markus Jochim [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5638-4870>),\n  Raphael Winkelmann [aut],\n  Klaus Jaensch [aut, ctb],\n  Steve Cassidy [aut, ctb],\n  Jonathan Harrington [aut, ctb]",
    "url": "https://github.com/IPS-LMU/emuR,\nhttps://ips-lmu.github.io/The-EMU-SDMS-Manual/",
    "bug_reports": "https://github.com/IPS-LMU/emuR/issues",
    "repository": "https://cran.r-project.org/package=emuR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "emuR Main Package of the EMU Speech Database Management System Provide the EMU Speech Database Management System (EMU-SDMS) with\n    database management, data extraction, data preparation and data\n    visualization facilities. See <https://ips-lmu.github.io/The-EMU-SDMS-Manual/>\n    for more details.  "
  },
  {
    "id": 11803,
    "package_name": "enrichR",
    "title": "Provides an R Interface to 'Enrichr'",
    "description": "Provides an R interface to all 'Enrichr' databases. 'Enrichr' is a web-based tool for analysing gene sets and returns any enrichment of common annotated biological features. Quoting from their website 'Enrichment analysis is a computational method for inferring knowledge about an input gene set by comparing it to annotated gene sets representing prior biological knowledge.' See <https://maayanlab.cloud/Enrichr/> for further details.",
    "version": "3.4",
    "maintainer": "Wajid Jawaid <wajid.jawaid@gmail.com>",
    "author": "Wajid Jawaid [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=enrichR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "enrichR Provides an R Interface to 'Enrichr' Provides an R interface to all 'Enrichr' databases. 'Enrichr' is a web-based tool for analysing gene sets and returns any enrichment of common annotated biological features. Quoting from their website 'Enrichment analysis is a computational method for inferring knowledge about an input gene set by comparing it to annotated gene sets representing prior biological knowledge.' See <https://maayanlab.cloud/Enrichr/> for further details.  "
  },
  {
    "id": 11807,
    "package_name": "ensembleTax",
    "title": "Ensemble Taxonomic Assignments of Amplicon Sequencing Data",
    "description": "Creates ensemble taxonomic assignments of amplicon sequencing data \n  in R using outputs of multiple taxonomic assignment algorithms and/or\n\treference databases. Includes flexible algorithms for mapping taxonomic \n\tnomenclatures onto one another and for computing ensemble taxonomic \n\tassignments. ",
    "version": "1.1.1",
    "maintainer": "Dylan Catlett <dcat4444@gmail.com>",
    "author": "Dylan Catlett [aut, cre], Kevin Son [ctb], Connie Liang [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ensembleTax",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ensembleTax Ensemble Taxonomic Assignments of Amplicon Sequencing Data Creates ensemble taxonomic assignments of amplicon sequencing data \n  in R using outputs of multiple taxonomic assignment algorithms and/or\n\treference databases. Includes flexible algorithms for mapping taxonomic \n\tnomenclatures onto one another and for computing ensemble taxonomic \n\tassignments.   "
  },
  {
    "id": 11952,
    "package_name": "etl",
    "title": "Extract-Transform-Load Framework for Medium Data",
    "description": "A predictable and pipeable framework for performing ETL \n    (extract-transform-load) operations on publicly-accessible medium-sized data \n    set. This package sets up the method structure and implements generic \n    functions. Packages that depend on this package download specific data sets \n    from the Internet, clean them up, and import them into a local or remote \n    relational database management system.",
    "version": "0.4.2",
    "maintainer": "Benjamin S. Baumer <ben.baumer@gmail.com>",
    "author": "Benjamin S. Baumer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3279-0516>),\n  Carson Sievert [ctb],\n  Natalia Iannucci [ctb]",
    "url": "https://github.com/beanumber/etl",
    "bug_reports": "https://github.com/beanumber/etl/issues",
    "repository": "https://cran.r-project.org/package=etl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "etl Extract-Transform-Load Framework for Medium Data A predictable and pipeable framework for performing ETL \n    (extract-transform-load) operations on publicly-accessible medium-sized data \n    set. This package sets up the method structure and implements generic \n    functions. Packages that depend on this package download specific data sets \n    from the Internet, clean them up, and import them into a local or remote \n    relational database management system.  "
  },
  {
    "id": 11970,
    "package_name": "eurostat",
    "title": "Tools for Eurostat Open Data",
    "description": "Tools to download data from the Eurostat database\n    <https://ec.europa.eu/eurostat> together with search and manipulation\n    utilities.",
    "version": "4.0.0",
    "maintainer": "Leo Lahti <leo.lahti@iki.fi>",
    "author": "Leo Lahti [aut, cre] (ORCID: <https://orcid.org/0000-0001-5537-637X>),\n  Janne Huovari [aut],\n  Markus Kainu [aut],\n  Przemyslaw Biecek [aut],\n  Daniel Antal [ctb],\n  Diego Hernangomez [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>),\n  Joona Lehtomaki [ctb],\n  Francois Briatte [ctb],\n  Reto Stauffer [ctb],\n  Paul Rougieux [ctb],\n  Anna Vasylytsya [ctb],\n  Oliver Reiter [ctb],\n  Pyry Kantanen [ctb] (ORCID: <https://orcid.org/0000-0003-2853-2765>),\n  Enrico Spinielli [ctb] (ORCID: <https://orcid.org/0000-0001-8584-9131>)",
    "url": "https://ropengov.github.io/eurostat/,\nhttps://github.com/rOpenGov/eurostat",
    "bug_reports": "https://github.com/rOpenGov/eurostat/issues",
    "repository": "https://cran.r-project.org/package=eurostat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eurostat Tools for Eurostat Open Data Tools to download data from the Eurostat database\n    <https://ec.europa.eu/eurostat> together with search and manipulation\n    utilities.  "
  },
  {
    "id": 12048,
    "package_name": "expDB",
    "title": "Database for Experiment Dataset",
    "description": "A 'SQLite' database is designed to store all information \n  of experiment-based data including metadata, experiment design, \n  managements, phenotypic values and climate records. The dataset can be\n  imported from an 'Excel' file.",
    "version": "0.3.0",
    "maintainer": "Bangyou Zheng <bangyou.zheng@csiro.au>",
    "author": "Bangyou Zheng [aut, cre]",
    "url": "https://expdb.bangyou.me/, https://github.com/byzheng/expdb",
    "bug_reports": "https://github.com/byzheng/expdb/issues",
    "repository": "https://cran.r-project.org/package=expDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "expDB Database for Experiment Dataset A 'SQLite' database is designed to store all information \n  of experiment-based data including metadata, experiment design, \n  managements, phenotypic values and climate records. The dataset can be\n  imported from an 'Excel' file.  "
  },
  {
    "id": 12078,
    "package_name": "extractFAERS",
    "title": "Extract Data from FAERS Database",
    "description": "Provides functions to extract and process data from the FDA Adverse Event Reporting System (FAERS). It facilitates the conversion of raw FAERS data published after 2014Q3 into structured formats for analysis. See Yang et al. (2022) <doi:10.3389/fphar.2021.772768> for related information. ",
    "version": "0.1.4",
    "maintainer": "Renjun Yang <rjyang@rcees.ac.cn>",
    "author": "Renjun Yang [ctb],\n  Renjun Yang [aut, cre] (ORCID: <https://orcid.org/0000-0002-9353-5041>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=extractFAERS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extractFAERS Extract Data from FAERS Database Provides functions to extract and process data from the FDA Adverse Event Reporting System (FAERS). It facilitates the conversion of raw FAERS data published after 2014Q3 into structured formats for analysis. See Yang et al. (2022) <doi:10.3389/fphar.2021.772768> for related information.   "
  },
  {
    "id": 12080,
    "package_name": "extrafontdb",
    "title": "Holding the Database for the 'extrafont' Package",
    "description": "It is meant to be used with the 'extrafont' package. The 'extrafont' package contains the code to install and use fonts, while the 'extrafontdb' package contains the font database.",
    "version": "1.1",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Winston Chang [aut],\n  Frederic Bertrand [cre] (ORCID:\n    <https://orcid.org/0000-0002-0837-8281>)",
    "url": "https://github.com/fbertran/extrafontdb",
    "bug_reports": "https://github.com/fbertran/extrafontdb/issues",
    "repository": "https://cran.r-project.org/package=extrafontdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "extrafontdb Holding the Database for the 'extrafont' Package It is meant to be used with the 'extrafont' package. The 'extrafont' package contains the code to install and use fonts, while the 'extrafontdb' package contains the font database.  "
  },
  {
    "id": 12142,
    "package_name": "fabricQueryR",
    "title": "Query Data in 'Microsoft Fabric'",
    "description": "\n    Query data hosted in 'Microsoft Fabric'. Provides helpers to open 'DBI'\n    connections to 'SQL' endpoints of 'Lakehouse' and 'Data Warehouse' items; submit\n    'Data Analysis Expressions' ('DAX') queries to semantic model datasets in 'Microsoft\n    Fabric' and 'Power BI'; read 'Delta Lake' tables stored in 'OneLake'\n    ('Azure Data Lake Storage Gen2'); and execute 'Spark' code via the 'Livy API'. ",
    "version": "0.2.0",
    "maintainer": "Luka Koning <l.koning@kennispunttwente.nl>",
    "author": "Luka Koning [aut, cre, cph],\n  Kennispunt Twente [fnd]",
    "url": "https://github.com/kennispunttwente/fabricQueryR,\nhttps://kennispunttwente.github.io/fabricQueryR/",
    "bug_reports": "https://github.com/kennispunttwente/fabricQueryR/issues",
    "repository": "https://cran.r-project.org/package=fabricQueryR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fabricQueryR Query Data in 'Microsoft Fabric' \n    Query data hosted in 'Microsoft Fabric'. Provides helpers to open 'DBI'\n    connections to 'SQL' endpoints of 'Lakehouse' and 'Data Warehouse' items; submit\n    'Data Analysis Expressions' ('DAX') queries to semantic model datasets in 'Microsoft\n    Fabric' and 'Power BI'; read 'Delta Lake' tables stored in 'OneLake'\n    ('Azure Data Lake Storage Gen2'); and execute 'Spark' code via the 'Livy API'.   "
  },
  {
    "id": 12203,
    "package_name": "fasstr",
    "title": "Analyze, Summarize, and Visualize Daily Streamflow Data",
    "description": "The Flow Analysis Summary Statistics Tool for R, 'fasstr', provides various functions to tidy and screen daily stream discharge data, calculate and visualize various summary statistics and metrics, and compute annual trending and volume frequency analyses. \n     It features useful function arguments for filtering of and handling dates, customizing data and metrics, and the ability to pull daily data directly from the Water Survey of Canada hydrometric database (<https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/>).",
    "version": "0.5.3",
    "maintainer": "Jon Goetz <jon.goetz@gov.bc.ca>",
    "author": "Jon Goetz [aut, cre] (ORCID: <https://orcid.org/0000-0002-4993-1119>),\n  Carl James Schwarz [aut],\n  Sam Albers [ctb] (ORCID: <https://orcid.org/0000-0002-9270-7884>),\n  Robin Pike [ctb],\n  Province of British Columbia [cph]",
    "url": "https://bcgov.github.io/fasstr/, https://github.com/bcgov/fasstr",
    "bug_reports": "https://github.com/bcgov/fasstr/issues",
    "repository": "https://cran.r-project.org/package=fasstr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fasstr Analyze, Summarize, and Visualize Daily Streamflow Data The Flow Analysis Summary Statistics Tool for R, 'fasstr', provides various functions to tidy and screen daily stream discharge data, calculate and visualize various summary statistics and metrics, and compute annual trending and volume frequency analyses. \n     It features useful function arguments for filtering of and handling dates, customizing data and metrics, and the ability to pull daily data directly from the Water Survey of Canada hydrometric database (<https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/>).  "
  },
  {
    "id": 12270,
    "package_name": "faunabr",
    "title": "Explore Cat\u00e1logo Tax\u00f4nomico da Fauna do Brasil Database",
    "description": "A collection of functions designed to retrieve, filter and spatialize data from the Cat\u00e1logo Tax\u00f4nomico da Fauna do Brasil. For more information about the dataset, please visit <http://fauna.jbrj.gov.br/fauna/listaBrasil/>.",
    "version": "1.0.0",
    "maintainer": "Weverton Trindade <wevertonf1993@gmail.com>",
    "author": "Weverton Trindade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2045-4555>)",
    "url": "https://wevertonbio.github.io/faunabr/",
    "bug_reports": "https://github.com/wevertonbio/faunabr/issues",
    "repository": "https://cran.r-project.org/package=faunabr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "faunabr Explore Cat\u00e1logo Tax\u00f4nomico da Fauna do Brasil Database A collection of functions designed to retrieve, filter and spatialize data from the Cat\u00e1logo Tax\u00f4nomico da Fauna do Brasil. For more information about the dataset, please visit <http://fauna.jbrj.gov.br/fauna/listaBrasil/>.  "
  },
  {
    "id": 12281,
    "package_name": "fbnet",
    "title": "Forensic Bayesian Networks",
    "description": "Open-source package for computing likelihood ratios in kinship testing and human identification cases. It has the core function of the software GENis, developed by Fundaci\u00f3n Sadosky. It relies on a Bayesian Networks framework and is particularly well suited to efficiently perform large-size queries against databases of missing individuals.",
    "version": "1.0.4",
    "maintainer": "Franco Marsico <franco.lmarsico@gmail.com>",
    "author": "Franco Marsico [aut, cre],\n  Ariel Chernomoretz [aut]",
    "url": "https://marsicofl.github.io/fbnet/,\nhttps://github.com/MarsicoFL/fbnet",
    "bug_reports": "https://github.com/MarsicoFL/fbnet/issues",
    "repository": "https://cran.r-project.org/package=fbnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fbnet Forensic Bayesian Networks Open-source package for computing likelihood ratios in kinship testing and human identification cases. It has the core function of the software GENis, developed by Fundaci\u00f3n Sadosky. It relies on a Bayesian Networks framework and is particularly well suited to efficiently perform large-size queries against databases of missing individuals.  "
  },
  {
    "id": 12320,
    "package_name": "fdq",
    "title": "Forest Data Quality",
    "description": "\n    Forest data quality is a package containing nine methods of analysis for forest databases,\n    from databases containing inventory data and growth models, the focus of the analyzes is related to the\n    quality of the data present in the database with a focus on consistency , punctuality and completeness of data.",
    "version": "0.12",
    "maintainer": "Ca\u00edque de Oliveira de Souza <forestgrowthsoftware@gmail.com>",
    "author": "Ca\u00edque de Oliveira de Souza [aut, cre],\n  Clayton Vieira Fraga Filho [ctb, dtc],\n  Gilson Fernandes da Silva [ctb, dtc],\n  Miqu\u00e9ias Fernandes [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fdq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdq Forest Data Quality \n    Forest data quality is a package containing nine methods of analysis for forest databases,\n    from databases containing inventory data and growth models, the focus of the analyzes is related to the\n    quality of the data present in the database with a focus on consistency , punctuality and completeness of data.  "
  },
  {
    "id": 12388,
    "package_name": "filehash",
    "title": "Simple Key-Value Database",
    "description": "Implements a simple key-value style database where character string keys\n  are associated with data values that are stored on the disk. A simple interface is provided for inserting,\n  retrieving, and deleting data from the database. Utilities are provided that allow 'filehash' databases to be\n  treated much like environments and lists are already used in R. These utilities are provided to encourage\n  interactive and exploratory analysis on large datasets. Three different file formats for representing the\n  database are currently available and new formats can easily be incorporated by third parties for use in the\n  'filehash' framework.",
    "version": "2.4-6",
    "maintainer": "Roger D. Peng <roger.peng@austin.utexas.edu>",
    "author": "Roger D. Peng <roger.peng@austin.utexas.edu>",
    "url": "https://github.com/rdpeng/filehash",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=filehash",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "filehash Simple Key-Value Database Implements a simple key-value style database where character string keys\n  are associated with data values that are stored on the disk. A simple interface is provided for inserting,\n  retrieving, and deleting data from the database. Utilities are provided that allow 'filehash' databases to be\n  treated much like environments and lists are already used in R. These utilities are provided to encourage\n  interactive and exploratory analysis on large datasets. Three different file formats for representing the\n  database are currently available and new formats can easily be incorporated by third parties for use in the\n  'filehash' framework.  "
  },
  {
    "id": 12389,
    "package_name": "filehashSQLite",
    "title": "Simple Key-Value Database using SQLite",
    "description": "Simple key-value database using SQLite as the backend.",
    "version": "0.2-7",
    "maintainer": "Roger D. Peng <roger.peng@austin.utexas.edu>",
    "author": "Roger D. Peng <roger.peng@austin.utexas.edu>",
    "url": "https://github.com/rdpeng/filehashsqlite",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=filehashSQLite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "filehashSQLite Simple Key-Value Database using SQLite Simple key-value database using SQLite as the backend.  "
  },
  {
    "id": 12439,
    "package_name": "fishstat",
    "title": "Global Fishery and Aquaculture Statistics",
    "description": "The Food and Agriculture Organization of the United Nations (FAO)\n  FishStat database is the leading source of global fishery and aquaculture\n  statistics and provides unique information for sector analysis and monitoring.\n  This package provides the global production data from all fisheries and\n  aquaculture in R format, ready for analysis.",
    "version": "2025.1.0.1",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Arni Magnusson [aut, cre],\n  Rishi Sharma [aut],\n  FAO [cph]",
    "url": "https://www.fao.org/fishery/en/fishstat,\nhttps://github.com/sofia-taf/fishstat",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fishstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fishstat Global Fishery and Aquaculture Statistics The Food and Agriculture Organization of the United Nations (FAO)\n  FishStat database is the leading source of global fishery and aquaculture\n  statistics and provides unique information for sector analysis and monitoring.\n  This package provides the global production data from all fisheries and\n  aquaculture in R format, ready for analysis.  "
  },
  {
    "id": 12471,
    "package_name": "flagr",
    "title": "Implementation of Flag Aggregation",
    "description": "Three methods are implemented in R to facilitate the aggregations of flags in official statistics.  \n            From the underlying flags the highest in the hierarchy, the most frequent, or with the highest total weight\n            is propagated to the flag(s) for EU or other aggregates. Below there are some reference documents for the topic: \n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_v2_1.docx>,\n            <https://sdmx.org/wp-content/uploads/CL_CONF_STATUS_1_2_2018.docx>,\n            <http://ec.europa.eu/eurostat/data/database/information>,\n            <http://www.oecd.org/sdd/33869551.pdf>,\n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_implementation_20-10-2014.pdf>.",
    "version": "0.3.2",
    "maintainer": "M\u00e1ty\u00e1s M\u00e9sz\u00e1ros <matyas.meszaros@ec.europa.eu>",
    "author": "M\u00e1ty\u00e1s M\u00e9sz\u00e1ros [aut, cre],\n  Matteo Salvati [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=flagr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flagr Implementation of Flag Aggregation Three methods are implemented in R to facilitate the aggregations of flags in official statistics.  \n            From the underlying flags the highest in the hierarchy, the most frequent, or with the highest total weight\n            is propagated to the flag(s) for EU or other aggregates. Below there are some reference documents for the topic: \n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_v2_1.docx>,\n            <https://sdmx.org/wp-content/uploads/CL_CONF_STATUS_1_2_2018.docx>,\n            <http://ec.europa.eu/eurostat/data/database/information>,\n            <http://www.oecd.org/sdd/33869551.pdf>,\n            <https://sdmx.org/wp-content/uploads/CL_OBS_STATUS_implementation_20-10-2014.pdf>.  "
  },
  {
    "id": 12527,
    "package_name": "florabr",
    "title": "Explore Flora e Funga do Brasil Database",
    "description": "A collection of functions designed to retrieve, filter and spatialize data from the Flora e Funga do Brasil dataset. For more information about the dataset, please visit <https://floradobrasil.jbrj.gov.br/consulta/>.",
    "version": "1.3.1",
    "maintainer": "Weverton Trindade <wevertonf1993@gmail.com>",
    "author": "Weverton Trindade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2045-4555>)",
    "url": "https://wevertonbio.github.io/florabr/",
    "bug_reports": "https://github.com/wevertonbio/florabr/issues",
    "repository": "https://cran.r-project.org/package=florabr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "florabr Explore Flora e Funga do Brasil Database A collection of functions designed to retrieve, filter and spatialize data from the Flora e Funga do Brasil dataset. For more information about the dataset, please visit <https://floradobrasil.jbrj.gov.br/consulta/>.  "
  },
  {
    "id": 12634,
    "package_name": "fossilbrush",
    "title": "Automated Cleaning of Fossil Occurrence Data",
    "description": "Functions to automate the detection and resolution of taxonomic and stratigraphic errors in fossil occurrence datasets. Functions were developed using data from the Paleobiology Database.",
    "version": "1.0.6",
    "maintainer": "Joe Flannery-Sutherland <josephflannerysutherland@gmail.com>",
    "author": "Joe Flannery-Sutherland [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8232-6773>),\n  Nussa\u00efbah Raja-Schoob [aut, ctb],\n  \u00c1dam Kocsis [aut, ctb],\n  Wolfgang Kiessling [aut]",
    "url": "https://cran.r-project.org/package=fossilbrush",
    "bug_reports": "https://cran.r-project.org/package=fossilbrush",
    "repository": "https://cran.r-project.org/package=fossilbrush",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fossilbrush Automated Cleaning of Fossil Occurrence Data Functions to automate the detection and resolution of taxonomic and stratigraphic errors in fossil occurrence datasets. Functions were developed using data from the Paleobiology Database.  "
  },
  {
    "id": 12655,
    "package_name": "fqacalc",
    "title": "Calculate Floristic Quality Assessment Metrics",
    "description": "A collection of functions for calculating Floristic Quality\n    Assessment (FQA) metrics using regional FQA databases that have been\n    approved or approved with reservations as ecological planning models\n    by the U.S. Army Corps of Engineers (USACE). For information on FQA\n    see Spyreas (2019) <doi:10.1002/ecs2.2825>. These databases are stored\n    in a sister R package, 'fqadata'. Both packages were developed for the\n    USACE by the U.S. Army Engineer Research and Development Center\u2019s\n    Environmental Laboratory.",
    "version": "1.1.1",
    "maintainer": "Todd Swannack <tswannack@gmail.com>",
    "author": "Iris Foxfoot [aut],\n  Todd Swannack [cre],\n  U.S. Army Engineer Research and Development Center [cph, fnd]",
    "url": "https://github.com/EcoModTeam/fqacalc",
    "bug_reports": "https://github.com/EcoModTeam/fqacalc/issues",
    "repository": "https://cran.r-project.org/package=fqacalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fqacalc Calculate Floristic Quality Assessment Metrics A collection of functions for calculating Floristic Quality\n    Assessment (FQA) metrics using regional FQA databases that have been\n    approved or approved with reservations as ecological planning models\n    by the U.S. Army Corps of Engineers (USACE). For information on FQA\n    see Spyreas (2019) <doi:10.1002/ecs2.2825>. These databases are stored\n    in a sister R package, 'fqadata'. Both packages were developed for the\n    USACE by the U.S. Army Engineer Research and Development Center\u2019s\n    Environmental Laboratory.  "
  },
  {
    "id": 12656,
    "package_name": "fqadata",
    "title": "Contains Regional Floristic Quality Assessment Databases",
    "description": "Contains regional Floristic Quality Assessment databases that\n    have been approved or approved with reservations by the U.S. Army\n    Corps of Engineers (USACE). Paired with the 'fqacalc' R package, these\n    data sets allow for Floristic Quality Assessment metrics to be\n    calculated. For information on FQA see Spyreas (2019)\n    <doi:10.1002/ecs2.2825>. Both packages were developed for the USACE by\n    the U.S.  Army Engineer Research and Development Center's\n    Environmental Laboratory.",
    "version": "1.1.1",
    "maintainer": "Todd Swannack <tswannack@gmail.com>",
    "author": "Iris Foxfoot [aut],\n  Todd Swannack [cre],\n  U.S. Army Engineer Research and Development Center [cph, fnd]",
    "url": "https://github.com/EcoModTeam/fqadata",
    "bug_reports": "https://github.com/EcoModTeam/fqadata/issues",
    "repository": "https://cran.r-project.org/package=fqadata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fqadata Contains Regional Floristic Quality Assessment Databases Contains regional Floristic Quality Assessment databases that\n    have been approved or approved with reservations by the U.S. Army\n    Corps of Engineers (USACE). Paired with the 'fqacalc' R package, these\n    data sets allow for Floristic Quality Assessment metrics to be\n    calculated. For information on FQA see Spyreas (2019)\n    <doi:10.1002/ecs2.2825>. Both packages were developed for the USACE by\n    the U.S.  Army Engineer Research and Development Center's\n    Environmental Laboratory.  "
  },
  {
    "id": 12657,
    "package_name": "fqar",
    "title": "Floristic Quality Assessment Tools for R",
    "description": "Tools for downloading and analyzing floristic quality assessment data. \n  See Freyman et al. (2015) <doi:10.1111/2041-210X.12491> for more information \n  about floristic quality assessment and the associated database.  ",
    "version": "0.5.6",
    "maintainer": "Andrew Gard <agard@lakeforest.edu>",
    "author": "Andrew Gard [aut, cre] (ORCID: <https://orcid.org/0000-0003-4434-0755>),\n  Alexia Myers [aut],\n  Irene Luwabelwa [aut]",
    "url": "https://github.com/equitable-equations/fqar/",
    "bug_reports": "https://github.com/equitable-equations/fqar/issues",
    "repository": "https://cran.r-project.org/package=fqar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fqar Floristic Quality Assessment Tools for R Tools for downloading and analyzing floristic quality assessment data. \n  See Freyman et al. (2015) <doi:10.1111/2041-210X.12491> for more information \n  about floristic quality assessment and the associated database.    "
  },
  {
    "id": 12757,
    "package_name": "func2vis",
    "title": "Clean and Visualize Over Expression Results from\n'ConsensusPathDB'",
    "description": "Provides functions to have visualization and clean-up of enriched gene ontologies (GO) terms, \n    protein complexes and pathways (obtained from multiple databases) using 'ConsensusPathDB' \n    from gene set over-expression analysis. Performs clustering of pathway based on similarity \n    of over-expressed gene sets and visualizations similar to Ingenuity Pathway Analysis (IPA) \n    when up and down regulated genes are known. The methods are described in a paper currently\n    submitted by Orecchioni et al, 2020 in Nanoscale.",
    "version": "1.0-3",
    "maintainer": "Raghvendra Mall <raghvendra5688@gmail.com>",
    "author": "Raghvendra Mall [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=func2vis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "func2vis Clean and Visualize Over Expression Results from\n'ConsensusPathDB' Provides functions to have visualization and clean-up of enriched gene ontologies (GO) terms, \n    protein complexes and pathways (obtained from multiple databases) using 'ConsensusPathDB' \n    from gene set over-expression analysis. Performs clustering of pathway based on similarity \n    of over-expressed gene sets and visualizations similar to Ingenuity Pathway Analysis (IPA) \n    when up and down regulated genes are known. The methods are described in a paper currently\n    submitted by Orecchioni et al, 2020 in Nanoscale.  "
  },
  {
    "id": 12806,
    "package_name": "fwtraits",
    "title": "Extract Species Ecological Parameters from\nWww.freshwaterecology.info",
    "description": "Support the extraction and seamless integration of species ecological traits or preferences from the www.freshwaterecology.info into several ecological model workflows. During data extraction, different taxonomic levels are acceptable, including species, genus, and family, based on the availability of data in the database. The data is cached after the first search and can be accessed during and after online interactions. Only scientific names are acceptable in the search; local or English names are not allowed. A user API key is required to start using the package.",
    "version": "1.0.0",
    "maintainer": "Anthony Basooma <anthony.basooma@boku.ac.at>",
    "author": "Anthony Basooma [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8994-9989>),\n  Florian Borgwardt [ctb, fnd] (ORCID:\n    <https://orcid.org/0000-0002-8974-7834>),\n  Thomas Hein [ctb, fnd, ths] (ORCID:\n    <https://orcid.org/0000-0002-7767-4607>),\n  Sami Domisch [ctb],\n  Merret Buurman [ctb],\n  Vanessa Bremerich [ctb],\n  Sonia Steffany Recinos Brizuela [ctb],\n  Martin Tschikof [ctb],\n  Astrid Schmidt-Kloiber [ctb, fnd, dtc] (ORCID:\n    <https://orcid.org/0000-0001-8839-5913>)",
    "url": "https://github.com/AnthonyBasooma/fwtraits,\nhttps://anthonybasooma.github.io/fwtraits/,\nhttps://github.com/AnthonyBasooma/fwtraits/releases/tag/V1.0.0",
    "bug_reports": "https://github.com/AnthonyBasooma/fwtraits/issues",
    "repository": "https://cran.r-project.org/package=fwtraits",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fwtraits Extract Species Ecological Parameters from\nWww.freshwaterecology.info Support the extraction and seamless integration of species ecological traits or preferences from the www.freshwaterecology.info into several ecological model workflows. During data extraction, different taxonomic levels are acceptable, including species, genus, and family, based on the availability of data in the database. The data is cached after the first search and can be accessed during and after online interactions. Only scientific names are acceptable in the search; local or English names are not allowed. A user API key is required to start using the package.  "
  },
  {
    "id": 12810,
    "package_name": "fy",
    "title": "Utilities for Financial Years",
    "description": "In Australia, a financial year (or fiscal year) is the period from 1 July to 30 June\n     of the following calendar year. As such, many databases need to represent and \n     validate financial years efficiently. While the use of integer years with a convention that  \n     they represent the year ending is common, it may lead to ambiguity with calendar years.\n     On the other hand, string representations may be too inefficient and do not easily admit\n     arithmetic operations. This package tries to make validation of financial years quicker while\n     retaining clarity.",
    "version": "0.4.2",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "author": "Hugh Parsonage [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fy Utilities for Financial Years In Australia, a financial year (or fiscal year) is the period from 1 July to 30 June\n     of the following calendar year. As such, many databases need to represent and \n     validate financial years efficiently. While the use of integer years with a convention that  \n     they represent the year ending is common, it may lead to ambiguity with calendar years.\n     On the other hand, string representations may be too inefficient and do not easily admit\n     arithmetic operations. This package tries to make validation of financial years quicker while\n     retaining clarity.  "
  },
  {
    "id": 12846,
    "package_name": "gains",
    "title": "Lift (Gains) Tables and Charts",
    "description": "Constructs gains tables and lift charts for prediction algorithms. Gains tables and lift charts are commonly used in direct marketing applications.  The method is described in Drozdenko and Drake (2002), \"Optimal Database Marketing\", Chapter 11.",
    "version": "1.2",
    "maintainer": "Craig A. Rolling <craig.rolling@slu.edu>",
    "author": "Craig A. Rolling <craig.rolling@slu.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gains",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gains Lift (Gains) Tables and Charts Constructs gains tables and lift charts for prediction algorithms. Gains tables and lift charts are commonly used in direct marketing applications.  The method is described in Drozdenko and Drake (2002), \"Optimal Database Marketing\", Chapter 11.  "
  },
  {
    "id": 12945,
    "package_name": "gdalcubes",
    "title": "Earth Observation Data Cubes from Satellite Image Collections",
    "description": "Processing collections of Earth observation images as on-demand multispectral, multitemporal raster data cubes. Users\n    define cubes by spatiotemporal extent, resolution, and spatial reference system and let 'gdalcubes' automatically apply cropping, reprojection, and \n    resampling using the 'Geospatial Data Abstraction Library' ('GDAL'). Implemented functions on data cubes include reduction over space and time, \n    applying arithmetic expressions on pixel band values, moving window aggregates over time, filtering by space, time, bands, and predicates on pixel values, \n    exporting data cubes as 'netCDF' or 'GeoTIFF' files, plotting, and extraction from spatial and or spatiotemporal features.  \n    All computational parts are implemented in C++, linking to the 'GDAL', 'netCDF', 'CURL', and 'SQLite' libraries. \n    See Appel and Pebesma (2019) <doi:10.3390/data4030092> for further details.",
    "version": "0.7.2",
    "maintainer": "Marius Appel <marius.appel@hs-bochum.de>",
    "author": "Marius Appel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5281-3896>),\n  Edzer Pebesma [ctb] (ORCID: <https://orcid.org/0000-0001-8049-7069>),\n  Roger Bivand [ctb],\n  Jeroen Ooms [ctb] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  Lewis Van Winkle [cph],\n  Ole Christian Eidheim [cph],\n  Howard Hinnant [cph],\n  Adrian Colomitchi [cph],\n  Florian Dang [cph],\n  Paul Thompson [cph],\n  Tomasz Kami\u0144ski [cph],\n  Dropbox, Inc. [cph]",
    "url": "https://github.com/appelmar/gdalcubes",
    "bug_reports": "https://github.com/appelmar/gdalcubes/issues/",
    "repository": "https://cran.r-project.org/package=gdalcubes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdalcubes Earth Observation Data Cubes from Satellite Image Collections Processing collections of Earth observation images as on-demand multispectral, multitemporal raster data cubes. Users\n    define cubes by spatiotemporal extent, resolution, and spatial reference system and let 'gdalcubes' automatically apply cropping, reprojection, and \n    resampling using the 'Geospatial Data Abstraction Library' ('GDAL'). Implemented functions on data cubes include reduction over space and time, \n    applying arithmetic expressions on pixel band values, moving window aggregates over time, filtering by space, time, bands, and predicates on pixel values, \n    exporting data cubes as 'netCDF' or 'GeoTIFF' files, plotting, and extraction from spatial and or spatiotemporal features.  \n    All computational parts are implemented in C++, linking to the 'GDAL', 'netCDF', 'CURL', and 'SQLite' libraries. \n    See Appel and Pebesma (2019) <doi:10.3390/data4030092> for further details.  "
  },
  {
    "id": 13003,
    "package_name": "geneHummus",
    "title": "A Pipeline to Define Gene Families in Legumes and Beyond",
    "description": "A pipeline with high specificity and sensitivity in extracting \n  proteins from the RefSeq database (National Center for Biotechnology \n  Information). Manual identification of gene families is highly \n  time-consuming and laborious, requiring an iterative process of manual and \n  computational analysis to identify members of a given family. The pipelines \n  implements an automatic approach for the identification of gene families \n  based on the conserved domains that specifically define that family. See \n  Die et al. (2018) <doi:10.1101/436659> for more information and examples.",
    "version": "1.0.11",
    "maintainer": "Jose V. Die <jose.die@uco.es>",
    "author": "Jose V. Die [aut, cre] (ORCID: <https://orcid.org/0000-0002-7506-8590>),\n  Moamen M. Elmassry [ctb],\n  Kimberly H. LeBlanc [ctb],\n  Olaitan I. Awe [ctb],\n  Allissa Dillman [ctb],\n  Ben Busby [aut]",
    "url": "https://github.com/NCBI-Hackathons/GeneHummus",
    "bug_reports": "https://github.com/NCBI-Hackathons/GeneHummus/issues",
    "repository": "https://cran.r-project.org/package=geneHummus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geneHummus A Pipeline to Define Gene Families in Legumes and Beyond A pipeline with high specificity and sensitivity in extracting \n  proteins from the RefSeq database (National Center for Biotechnology \n  Information). Manual identification of gene families is highly \n  time-consuming and laborious, requiring an iterative process of manual and \n  computational analysis to identify members of a given family. The pipelines \n  implements an automatic approach for the identification of gene families \n  based on the conserved domains that specifically define that family. See \n  Die et al. (2018) <doi:10.1101/436659> for more information and examples.  "
  },
  {
    "id": 13014,
    "package_name": "geneset",
    "title": "Get Gene Sets for Gene Enrichment Analysis",
    "description": "Gene sets are fundamental for gene enrichment analysis. The package 'geneset' enables querying \n    gene sets from public databases including 'GO' (Gene Ontology Consortium. (2004) <doi:10.1093/nar/gkh036>), \n    'KEGG' (Minoru et al. (2000) <doi:10.1093/nar/28.1.27>), \n    'WikiPathway' (Marvin et al. (2020) <doi:10.1093/nar/gkaa1024>), \n    'MsigDb' (Arthur et al. (2015) <doi:10.1016/j.cels.2015.12.004>), \n    'Reactome' (David et al. (2011) <doi:10.1093/nar/gkq1018>),\n    'MeSH' (Ish et al. (2014) <doi:10.4103/0019-5413.139827>), \n    'DisGeNET' (Janet et al. (2017) <doi:10.1093/nar/gkw943>),\n    'Disease Ontology' (Lynn et al. (2011) <doi:10.1093/nar/gkr972>), \n    'Network of Cancer Genes' (Dimitra et al. (2019) <doi:10.1186/s13059-018-1612-0>) and \n    'COVID-19' (Maxim et al. (2020) <doi:10.21203/rs.3.rs-28582/v1>).\n    Gene sets are stored in the list object which provides data frame of 'geneset' and 'geneset_name'. \n    The 'geneset' has two columns of term ID and gene ID. The 'geneset_name' has two columns of \n    terms ID and term description.",
    "version": "0.2.7",
    "maintainer": "Yunze Liu <jieandze1314@gmail.com>",
    "author": "Yunze Liu [aut, cre] (ORCID: <https://orcid.org/0000-0002-7414-8556>)",
    "url": "https://github.com/GangLiLab/geneset",
    "bug_reports": "https://github.com/GangLiLab/geneset/issues",
    "repository": "https://cran.r-project.org/package=geneset",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geneset Get Gene Sets for Gene Enrichment Analysis Gene sets are fundamental for gene enrichment analysis. The package 'geneset' enables querying \n    gene sets from public databases including 'GO' (Gene Ontology Consortium. (2004) <doi:10.1093/nar/gkh036>), \n    'KEGG' (Minoru et al. (2000) <doi:10.1093/nar/28.1.27>), \n    'WikiPathway' (Marvin et al. (2020) <doi:10.1093/nar/gkaa1024>), \n    'MsigDb' (Arthur et al. (2015) <doi:10.1016/j.cels.2015.12.004>), \n    'Reactome' (David et al. (2011) <doi:10.1093/nar/gkq1018>),\n    'MeSH' (Ish et al. (2014) <doi:10.4103/0019-5413.139827>), \n    'DisGeNET' (Janet et al. (2017) <doi:10.1093/nar/gkw943>),\n    'Disease Ontology' (Lynn et al. (2011) <doi:10.1093/nar/gkr972>), \n    'Network of Cancer Genes' (Dimitra et al. (2019) <doi:10.1186/s13059-018-1612-0>) and \n    'COVID-19' (Maxim et al. (2020) <doi:10.21203/rs.3.rs-28582/v1>).\n    Gene sets are stored in the list object which provides data frame of 'geneset' and 'geneset_name'. \n    The 'geneset' has two columns of term ID and gene ID. The 'geneset_name' has two columns of \n    terms ID and term description.  "
  },
  {
    "id": 13067,
    "package_name": "geogenr",
    "title": "Generator from American Community Survey Geodatabases",
    "description": "The American Community Survey (ACS)\n    <https://www.census.gov/programs-surveys/acs> offers geodatabases with\n    geographic information and associated data of interest to researchers\n    in the area. The goal of this package is to generate objects that\n    allow us to access and consult the information available in various\n    formats, such as in 'GeoPackage' format or in multidimensional 'ROLAP'\n    (Relational On-Line Analytical Processing) star format.",
    "version": "2.0.1",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/geogenr/,\nhttps://github.com/josesamos/geogenr",
    "bug_reports": "https://github.com/josesamos/geogenr/issues",
    "repository": "https://cran.r-project.org/package=geogenr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geogenr Generator from American Community Survey Geodatabases The American Community Survey (ACS)\n    <https://www.census.gov/programs-surveys/acs> offers geodatabases with\n    geographic information and associated data of interest to researchers\n    in the area. The goal of this package is to generate objects that\n    allow us to access and consult the information available in various\n    formats, such as in 'GeoPackage' format or in multidimensional 'ROLAP'\n    (Relational On-Line Analytical Processing) star format.  "
  },
  {
    "id": 13111,
    "package_name": "gerda",
    "title": "German Election Database (GERDA)",
    "description": "Provides tools to download comprehensive datasets of local, \n    state, and federal election results in Germany from 1990 to 2025. The package \n    facilitates access to data on turnout, vote shares for major parties, and \n    demographic information across different levels of government (municipal, state, \n    and federal). It offers access to geographically harmonized datasets \n    that account for changes in municipal boundaries over time and incorporate \n    mail-in voting districts. Users can easily retrieve, clean, and standardize \n    German electoral data, making it ready for analysis. Data is sourced from \n    <https://github.com/awiedem/german_election_data>.",
    "version": "0.4.0",
    "maintainer": "Hanno Hilbig <hhilbig@ucdavis.edu>",
    "author": "Hanno Hilbig [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5849-9172>)",
    "url": "https://github.com/hhilbig/gerda,\nhttps://github.com/awiedem/german_election_data",
    "bug_reports": "https://github.com/hhilbig/gerda/issues",
    "repository": "https://cran.r-project.org/package=gerda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gerda German Election Database (GERDA) Provides tools to download comprehensive datasets of local, \n    state, and federal election results in Germany from 1990 to 2025. The package \n    facilitates access to data on turnout, vote shares for major parties, and \n    demographic information across different levels of government (municipal, state, \n    and federal). It offers access to geographically harmonized datasets \n    that account for changes in municipal boundaries over time and incorporate \n    mail-in voting districts. Users can easily retrieve, clean, and standardize \n    German electoral data, making it ready for analysis. Data is sourced from \n    <https://github.com/awiedem/german_election_data>.  "
  },
  {
    "id": 13207,
    "package_name": "ggfigdone",
    "title": "Manage & Modify 'ggplot' Figures using 'ggfigdone'",
    "description": "When you prepare a presentation or a report, you often need to manage a large number of 'ggplot' figures. You need to change the figure size, modify the title, label, themes, etc. It is inconvenient to go back to the original code to make these changes. This package provides a simple way to manage 'ggplot' figures. You can easily add the figure to the database and update them later using CLI (command line interface) or GUI (graphical user interface).",
    "version": "0.1.2",
    "maintainer": "Wenjie SUN <sunwjie@gmail.com>",
    "author": "Wenjie SUN [aut, cre] (ORCID: <https://orcid.org/0000-0002-3100-2346>)",
    "url": "",
    "bug_reports": "https://github.com/wenjie1991/ggfigdone/issues",
    "repository": "https://cran.r-project.org/package=ggfigdone",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggfigdone Manage & Modify 'ggplot' Figures using 'ggfigdone' When you prepare a presentation or a report, you often need to manage a large number of 'ggplot' figures. You need to change the figure size, modify the title, label, themes, etc. It is inconvenient to go back to the original code to make these changes. This package provides a simple way to manage 'ggplot' figures. You can easily add the figure to the database and update them later using CLI (command line interface) or GUI (graphical user interface).  "
  },
  {
    "id": 13382,
    "package_name": "giscoR",
    "title": "Download Map Data from GISCO API - Eurostat",
    "description": "Tools to download data from the GISCO (Geographic Information\n    System of the Commission) Eurostat database\n    <https://ec.europa.eu/eurostat/web/gisco>. Global and European map\n    data available.  This package is in no way officially related to or\n    endorsed by Eurostat.",
    "version": "1.0.0",
    "maintainer": "Diego Hernang\u00f3mez <diego.hernangomezherrero@gmail.com>",
    "author": "Diego Hernang\u00f3mez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>),\n  Eurostat [cph] (ROR: <https://ror.org/033d3q980>),\n  EuroGeographics [cph]",
    "url": "https://ropengov.github.io/giscoR/,\nhttps://github.com/rOpenGov/giscoR",
    "bug_reports": "https://github.com/rOpenGov/giscoR/issues",
    "repository": "https://cran.r-project.org/package=giscoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "giscoR Download Map Data from GISCO API - Eurostat Tools to download data from the GISCO (Geographic Information\n    System of the Commission) Eurostat database\n    <https://ec.europa.eu/eurostat/web/gisco>. Global and European map\n    data available.  This package is in no way officially related to or\n    endorsed by Eurostat.  "
  },
  {
    "id": 13476,
    "package_name": "gmDatabase",
    "title": "Accessing a Geometallurgical Database with R",
    "description": "A template for a geometallurgical database and a fast and easy\n    interface for accessing it.",
    "version": "0.5.1",
    "maintainer": "K. Gerald van den Boogaart <support@boogaart.de>",
    "author": "K. Gerald van den Boogaart [aut, cre],\n  Stephan Matos Camacho [aut]",
    "url": "https://cran.r-project.org/package=gmDatabase,\nhttp://www.stat.boogaart.de",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gmDatabase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gmDatabase Accessing a Geometallurgical Database with R A template for a geometallurgical database and a fast and easy\n    interface for accessing it.  "
  },
  {
    "id": 13564,
    "package_name": "gpkg",
    "title": "Utilities for the Open Geospatial Consortium 'GeoPackage' Format",
    "description": "Build Open Geospatial Consortium 'GeoPackage' files (<https://www.geopackage.org/>). 'GDAL' utilities for reading and writing spatial data are provided by the 'terra' package. Additional 'GeoPackage' and 'SQLite' features for attributes and tabular data are implemented with the 'RSQLite' package.",
    "version": "0.0.12",
    "maintainer": "Andrew Brown <brown.andrewg@gmail.com>",
    "author": "Andrew Brown [aut, cre]",
    "url": "https://humus.rocks/gpkg/, https://github.com/brownag/gpkg",
    "bug_reports": "https://github.com/brownag/gpkg/issues",
    "repository": "https://cran.r-project.org/package=gpkg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gpkg Utilities for the Open Geospatial Consortium 'GeoPackage' Format Build Open Geospatial Consortium 'GeoPackage' files (<https://www.geopackage.org/>). 'GDAL' utilities for reading and writing spatial data are provided by the 'terra' package. Additional 'GeoPackage' and 'SQLite' features for attributes and tabular data are implemented with the 'RSQLite' package.  "
  },
  {
    "id": 13719,
    "package_name": "gsubfn",
    "title": "Utilities for Strings and Function Arguments",
    "description": "The gsubfn function is like gsub but can take a replacement \n   function or certain other objects instead of the replacement string.\n   Matches and back references are input to the replacement function and \n   replaced by the function output.   gsubfn can be used to split strings \n   based on content rather than delimiters and for quasi-perl-style string \n   interpolation. The package also has facilities for translating formulas \n   to functions and allowing such formulas in function calls instead of \n   functions.  This can be used with R functions such as apply, sapply,\n   lapply, optim, integrate, xyplot, Filter and any other function that \n   expects another function as an input argument or functions like cat\n   or sql calls that may involve strings where substitution is desirable.\n   There is also a facility for returning multiple objects from functions\n   and a version of transform that allows the RHS to refer to LHS used in\n   the same transform.",
    "version": "0.7",
    "maintainer": "G. Grothendieck <ggrothendieck@gmail.com>",
    "author": "G. Grothendieck",
    "url": "https://github.com/ggrothendieck/gsubfn",
    "bug_reports": "https://github.com/ggrothendieck/gsubfn/issues",
    "repository": "https://cran.r-project.org/package=gsubfn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gsubfn Utilities for Strings and Function Arguments The gsubfn function is like gsub but can take a replacement \n   function or certain other objects instead of the replacement string.\n   Matches and back references are input to the replacement function and \n   replaced by the function output.   gsubfn can be used to split strings \n   based on content rather than delimiters and for quasi-perl-style string \n   interpolation. The package also has facilities for translating formulas \n   to functions and allowing such formulas in function calls instead of \n   functions.  This can be used with R functions such as apply, sapply,\n   lapply, optim, integrate, xyplot, Filter and any other function that \n   expects another function as an input argument or functions like cat\n   or sql calls that may involve strings where substitution is desirable.\n   There is also a facility for returning multiple objects from functions\n   and a version of transform that allows the RHS to refer to LHS used in\n   the same transform.  "
  },
  {
    "id": 13745,
    "package_name": "guildai",
    "title": "Track Machine Learning Experiments",
    "description": "'Guild AI' is an open-source tool for managing machine learning\n    experiments. It's for scientists, engineers, and researchers who want to\n    run scripts, compare results, measure progress, and automate machine\n    learning workflow. 'Guild AI' is a light weight, external tool that runs\n    locally. It works with any framework, doesn't require any changes to\n    your code, or access to any web services. Users can easily record\n    experiment metadata, track model changes, manage experiment artifacts,\n    tune hyperparameters, and share results. 'Guild AI' combines features\n    from 'Git', 'SQLite', and 'Make' to provide a lab notebook for machine\n    learning.",
    "version": "0.0.1",
    "maintainer": "Tomasz Kalinowski <tomasz@posit.co>",
    "author": "Tomasz Kalinowski [aut, cph, cre],\n  Posit, PBC [cph, fnd]",
    "url": "https://guildai.github.io/guildai-r/,\nhttps://github.com/guildai/guildai-r, https://my.guild.ai/",
    "bug_reports": "https://github.com/guildai/guildai-r/issues",
    "repository": "https://cran.r-project.org/package=guildai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "guildai Track Machine Learning Experiments 'Guild AI' is an open-source tool for managing machine learning\n    experiments. It's for scientists, engineers, and researchers who want to\n    run scripts, compare results, measure progress, and automate machine\n    learning workflow. 'Guild AI' is a light weight, external tool that runs\n    locally. It works with any framework, doesn't require any changes to\n    your code, or access to any web services. Users can easily record\n    experiment metadata, track model changes, manage experiment artifacts,\n    tune hyperparameters, and share results. 'Guild AI' combines features\n    from 'Git', 'SQLite', and 'Make' to provide a lab notebook for machine\n    learning.  "
  },
  {
    "id": 13808,
    "package_name": "happign",
    "title": "R Interface to 'IGN' Web Services",
    "description": "Automatic open data acquisition from resources of IGN\n    ('Institut National de Information Geographique et forestiere')\n    (<https://www.ign.fr/>). Available datasets include various types of\n    raster and vector data, such as digital elevation models, state\n    borders, spatial databases, cadastral parcels, and more. 'happign' also \n    provide access to API Carto (<https://apicarto.ign.fr/api/doc/>).",
    "version": "0.3.6",
    "maintainer": "Paul Carteron <carteronpaul@gmail.com>",
    "author": "Paul Carteron [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6942-6662>)",
    "url": "https://github.com/paul-carteron,\nhttps://paul-carteron.github.io/happign/",
    "bug_reports": "https://github.com/paul-carteron/happign/issues",
    "repository": "https://cran.r-project.org/package=happign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "happign R Interface to 'IGN' Web Services Automatic open data acquisition from resources of IGN\n    ('Institut National de Information Geographique et forestiere')\n    (<https://www.ign.fr/>). Available datasets include various types of\n    raster and vector data, such as digital elevation models, state\n    borders, spatial databases, cadastral parcels, and more. 'happign' also \n    provide access to API Carto (<https://apicarto.ign.fr/api/doc/>).  "
  },
  {
    "id": 13818,
    "package_name": "hashids",
    "title": "Generate Short Unique YouTube-Like IDs (Hashes) from Integers",
    "description": "An R port of the hashids library.  hashids generates YouTube-like hashes from integers or vector of integers.  Hashes generated from integers are relatively short, unique and non-seqential.  hashids can be used to generate unique ids for URLs and hide database row numbers from the user.  By default hashids will avoid generating common English cursewords by preventing certain letters being next to each other.  hashids are not one-way: it is easy to encode an integer to a hashid and decode a hashid back into an integer.",
    "version": "0.9.0",
    "maintainer": "Alex Shum <Alex@ALShum.com>",
    "author": "Alex Shum [aut, cre],\n  Ivan Akimov [aut] (original author of hashids -- implemented in\n    javascript),\n  David Aurelio [ctb] (implemented hashids in python 2 and 3)",
    "url": "https://github.com/ALShum/hashids-r/, http://hashids.org",
    "bug_reports": "https://github.com/ALShum/hashids-r/issues",
    "repository": "https://cran.r-project.org/package=hashids",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hashids Generate Short Unique YouTube-Like IDs (Hashes) from Integers An R port of the hashids library.  hashids generates YouTube-like hashes from integers or vector of integers.  Hashes generated from integers are relatively short, unique and non-seqential.  hashids can be used to generate unique ids for URLs and hide database row numbers from the user.  By default hashids will avoid generating common English cursewords by preventing certain letters being next to each other.  hashids are not one-way: it is easy to encode an integer to a hashid and decode a hashid back into an integer.  "
  },
  {
    "id": 13874,
    "package_name": "healthdb",
    "title": "Working with Healthcare Databases",
    "description": "A system for identifying diseases or events from healthcare databases and\n    preparing data for epidemiological studies. It includes capabilities not\n    supported by 'SQL', such as matching strings by 'stringr' style regular\n    expressions, and can compute comorbidity scores (Quan et al. (2005)\n    <doi:10.1097/01.mlr.0000182534.19832.83>) directly on a database server. The\n    implementation is based on 'dbplyr' with full 'tidyverse' compatibility.",
    "version": "0.4.1",
    "maintainer": "Kevin Hu <kevin.hu@bccdc.ca>",
    "author": "Kevin Hu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0254-5277>)",
    "url": "https://github.com/KevinHzq/healthdb,\nhttps://kevinhzq.github.io/healthdb/",
    "bug_reports": "https://github.com/KevinHzq/healthdb/issues",
    "repository": "https://cran.r-project.org/package=healthdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "healthdb Working with Healthcare Databases A system for identifying diseases or events from healthcare databases and\n    preparing data for epidemiological studies. It includes capabilities not\n    supported by 'SQL', such as matching strings by 'stringr' style regular\n    expressions, and can compute comorbidity scores (Quan et al. (2005)\n    <doi:10.1097/01.mlr.0000182534.19832.83>) directly on a database server. The\n    implementation is based on 'dbplyr' with full 'tidyverse' compatibility.  "
  },
  {
    "id": 13945,
    "package_name": "hgnc",
    "title": "Import Human Gene Nomenclature",
    "description": "A set of routines to quickly download and import the\n    HUGO Gene Nomenclature Committee (HGNC) data set on mapping of\n    gene symbols to gene entries in other genomic databases or resources.",
    "version": "0.3.0",
    "maintainer": "Ramiro Magno <rmagno@pattern.institute>",
    "author": "Ramiro Magno [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5226-3441>),\n  Isabel Duarte [aut] (ORCID: <https://orcid.org/0000-0003-0060-2936>),\n  Jacob Munro [aut] (ORCID: <https://orcid.org/0000-0002-2751-0989>),\n  Ana-Teresa Maia [ctb] (ORCID: <https://orcid.org/0000-0002-0454-9207>),\n  Pattern Institute [cph, fnd] (ROR: <https://ror.org/04jrgd746>)",
    "url": "https://github.com/patterninstitute/hgnc,\nhttps://www.pattern.institute/hgnc/",
    "bug_reports": "https://github.com/patterninstitute/hgnc/issues",
    "repository": "https://cran.r-project.org/package=hgnc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hgnc Import Human Gene Nomenclature A set of routines to quickly download and import the\n    HUGO Gene Nomenclature Committee (HGNC) data set on mapping of\n    gene symbols to gene entries in other genomic databases or resources.  "
  },
  {
    "id": 13954,
    "package_name": "hicp",
    "title": "Harmonised Index of Consumer Prices",
    "description": "The Harmonised Index of Consumer Prices (HICP) is the key economic figure to measure inflation in the euro area.\n              The methodology underlying the HICP is documented in the HICP Methodological Manual (<https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/w/ks-gq-24-003>).\n              Based on the manual, this package provides functions to access and work with HICP data from Eurostat's public database (<https://ec.europa.eu/eurostat/data/database>).",
    "version": "1.0.0",
    "maintainer": "Sebastian Weinand <sebastian.weinand@ec.europa.eu>",
    "author": "Sebastian Weinand [aut, cre]",
    "url": "https://github.com/eurostat/hicp",
    "bug_reports": "https://github.com/eurostat/hicp/issues",
    "repository": "https://cran.r-project.org/package=hicp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hicp Harmonised Index of Consumer Prices The Harmonised Index of Consumer Prices (HICP) is the key economic figure to measure inflation in the euro area.\n              The methodology underlying the HICP is documented in the HICP Methodological Manual (<https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/w/ks-gq-24-003>).\n              Based on the manual, this package provides functions to access and work with HICP data from Eurostat's public database (<https://ec.europa.eu/eurostat/data/database>).  "
  },
  {
    "id": 14006,
    "package_name": "hlaR",
    "title": "Tools for HLA Data",
    "description": "A streamlined tool for eplet analysis of donor and recipient HLA (human leukocyte antigen) mismatch. Messy, low-resolution HLA typing data is cleaned, and imputed to high-resolution using the NMDP (National Marrow Donor Program) haplotype reference database <https://haplostats.org/haplostats>. High resolution data is analyzed for overall or single antigen eplet mismatch using a reference table (currently supporting 'HLAMatchMaker' <http://www.epitopes.net> versions 2 and 3). Data can enter or exit the workflow at different points depending on the user's aims and initial data quality.",
    "version": "1.0.0",
    "maintainer": "Joan Zhang <joan.zhang@emory.edu>",
    "author": "Joan Zhang [aut, cre],\n  Aileen Johnson [aut],\n  Christian P Larsen [cph, aut]",
    "url": "https://pubmed.ncbi.nlm.nih.gov/35101308/,\nhttps://emory-larsenlab.shinyapps.io/hlar_shiny/",
    "bug_reports": "https://github.com/LarsenLab/hlaR/issues",
    "repository": "https://cran.r-project.org/package=hlaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hlaR Tools for HLA Data A streamlined tool for eplet analysis of donor and recipient HLA (human leukocyte antigen) mismatch. Messy, low-resolution HLA typing data is cleaned, and imputed to high-resolution using the NMDP (National Marrow Donor Program) haplotype reference database <https://haplostats.org/haplostats>. High resolution data is analyzed for overall or single antigen eplet mismatch using a reference table (currently supporting 'HLAMatchMaker' <http://www.epitopes.net> versions 2 and 3). Data can enter or exit the workflow at different points depending on the user's aims and initial data quality.  "
  },
  {
    "id": 14023,
    "package_name": "hockeystick",
    "title": "Download and Visualize Essential Climate Change Data",
    "description": "Provides easy access to essential climate change datasets to non-climate experts. Users can download the latest raw data from authoritative sources and view it via pre-defined 'ggplot2' charts. Datasets include atmospheric CO2, methane, emissions, instrumental and proxy temperature records, sea levels, Arctic/Antarctic sea-ice, Hurricanes, and Paleoclimate data. Sources include: NOAA Mauna Loa Laboratory <https://gml.noaa.gov/ccgg/trends/data.html>, Global Carbon Project <https://www.globalcarbonproject.org/carbonbudget/>, NASA GISTEMP <https://data.giss.nasa.gov/gistemp/>, National Snow and Sea Ice Data Center <https://nsidc.org/home>, CSIRO <https://research.csiro.au/slrwavescoast/sea-level/measurements-and-data/sea-level-data/>, NOAA Laboratory for Satellite Altimetry <https://www.star.nesdis.noaa.gov/socd/lsa/SeaLevelRise/> and HURDAT Atlantic Hurricane Database <https://www.aoml.noaa.gov/hrd/hurdat/Data_Storm.html>, Vostok Paleo carbon dioxide and temperature data: <doi:10.3334/CDIAC/ATG.009>.",
    "version": "0.8.6",
    "maintainer": "Hernando Cortina <hch@alum.mit.edu>",
    "author": "Hernando Cortina [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6790-4870>)",
    "url": "https://cortinah.github.io/hockeystick/,\nhttps://github.com/cortinah/hockeystick",
    "bug_reports": "https://github.com/cortinah/hockeystick/issues",
    "repository": "https://cran.r-project.org/package=hockeystick",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hockeystick Download and Visualize Essential Climate Change Data Provides easy access to essential climate change datasets to non-climate experts. Users can download the latest raw data from authoritative sources and view it via pre-defined 'ggplot2' charts. Datasets include atmospheric CO2, methane, emissions, instrumental and proxy temperature records, sea levels, Arctic/Antarctic sea-ice, Hurricanes, and Paleoclimate data. Sources include: NOAA Mauna Loa Laboratory <https://gml.noaa.gov/ccgg/trends/data.html>, Global Carbon Project <https://www.globalcarbonproject.org/carbonbudget/>, NASA GISTEMP <https://data.giss.nasa.gov/gistemp/>, National Snow and Sea Ice Data Center <https://nsidc.org/home>, CSIRO <https://research.csiro.au/slrwavescoast/sea-level/measurements-and-data/sea-level-data/>, NOAA Laboratory for Satellite Altimetry <https://www.star.nesdis.noaa.gov/socd/lsa/SeaLevelRise/> and HURDAT Atlantic Hurricane Database <https://www.aoml.noaa.gov/hrd/hurdat/Data_Storm.html>, Vostok Paleo carbon dioxide and temperature data: <doi:10.3334/CDIAC/ATG.009>.  "
  },
  {
    "id": 14024,
    "package_name": "holi",
    "title": "Higher Order Likelihood Inference Web Applications",
    "description": "Higher order likelihood inference is a promising approach for\n    analyzing small sample size data. The 'holi' package provides web applications\n    for higher order likelihood inference. It currently supports linear, logistic,\n    and Poisson generalized linear models through the rstar_glm() function, based\n    on Pierce and Bellio (2017) <doi:10.1111/insr.12232> and 'likelihoodAsy'.\n    The package offers two main features: LA_rstar(), which launches an interactive\n    'shiny' application allowing users to fit models with rstar_glm() through their\n    web browser, and sim_rstar_glm_pgsql(), which streamlines the process of\n    launching a web-based 'shiny' simulation application that saves results to a\n    user-created 'PostgreSQL' database.",
    "version": "0.1.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/holi",
    "bug_reports": "https://github.com/mightymetrika/holi/issues",
    "repository": "https://cran.r-project.org/package=holi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "holi Higher Order Likelihood Inference Web Applications Higher order likelihood inference is a promising approach for\n    analyzing small sample size data. The 'holi' package provides web applications\n    for higher order likelihood inference. It currently supports linear, logistic,\n    and Poisson generalized linear models through the rstar_glm() function, based\n    on Pierce and Bellio (2017) <doi:10.1111/insr.12232> and 'likelihoodAsy'.\n    The package offers two main features: LA_rstar(), which launches an interactive\n    'shiny' application allowing users to fit models with rstar_glm() through their\n    web browser, and sim_rstar_glm_pgsql(), which streamlines the process of\n    launching a web-based 'shiny' simulation application that saves results to a\n    user-created 'PostgreSQL' database.  "
  },
  {
    "id": 14033,
    "package_name": "homologene",
    "title": "Quick Access to Homologene and Gene Annotation Updates",
    "description": "A wrapper for the homologene database by the National Center for\n    Biotechnology Information ('NCBI'). It allows searching for gene homologs across \n    species. Data in this package can be found at <ftp://ftp.ncbi.nih.gov/pub/HomoloGene/build68/>.\n    The package also includes an updated version of the homologene database where \n    gene identifiers and symbols are replaced with their latest (at the time of\n    submission) version and functions to fetch latest annotation data to keep updated.",
    "version": "1.4.68.19.3.27",
    "maintainer": "Ogan Mancarci <ogan.mancarci@gmail.com>",
    "author": "Ogan Mancarci [aut, cre],\n  Leon French [ctb]",
    "url": "https://github.com/oganm/homologene",
    "bug_reports": "https://github.com/oganm/homologene/issues",
    "repository": "https://cran.r-project.org/package=homologene",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "homologene Quick Access to Homologene and Gene Annotation Updates A wrapper for the homologene database by the National Center for\n    Biotechnology Information ('NCBI'). It allows searching for gene homologs across \n    species. Data in this package can be found at <ftp://ftp.ncbi.nih.gov/pub/HomoloGene/build68/>.\n    The package also includes an updated version of the homologene database where \n    gene identifiers and symbols are replaced with their latest (at the time of\n    submission) version and functions to fetch latest annotation data to keep updated.  "
  },
  {
    "id": 14085,
    "package_name": "htsr",
    "title": "Hydro-Meteorology Time-Series",
    "description": "Functions for the management and treatment of hydrology and \n  meteorology time-series stored in a 'Sqlite' data base.",
    "version": "2.1.6",
    "maintainer": "Pierre Chevallier <pierre.chevallier@mailo.com>",
    "author": "Pierre Chevallier [aut, cre]",
    "url": "https://github.com/p-chevallier/htsr",
    "bug_reports": "https://github.com/p-chevallier/htsr/issues",
    "repository": "https://cran.r-project.org/package=htsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "htsr Hydro-Meteorology Time-Series Functions for the management and treatment of hydrology and \n  meteorology time-series stored in a 'Sqlite' data base.  "
  },
  {
    "id": 14096,
    "package_name": "hubeau",
    "title": "Get Data from the French National Database on Water 'Hub'Eau'",
    "description": "Collection of functions to help retrieving data from\n    'Hub'Eau' the free and public French National APIs on water\n    <https://hubeau.eaufrance.fr/>.",
    "version": "0.5.2",
    "maintainer": "David Dorchies <david.dorchies@inrae.fr>",
    "author": "David Dorchies [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6595-7984>),\n  Pascal Irz [ctb] (ORCID: <https://orcid.org/0000-0002-2066-8935>),\n  S\u00e9bastien Grall [ctb],\n  Philippe Amiotte Suchet [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3514-1447>)",
    "url": "https://inrae.github.io/hubeau/,\nhttps://github.com/inrae/hubeau#readme",
    "bug_reports": "https://github.com/inrae/hubeau/issues",
    "repository": "https://cran.r-project.org/package=hubeau",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hubeau Get Data from the French National Database on Water 'Hub'Eau' Collection of functions to help retrieving data from\n    'Hub'Eau' the free and public French National APIs on water\n    <https://hubeau.eaufrance.fr/>.  "
  },
  {
    "id": 14116,
    "package_name": "hwsdr",
    "title": "Interface to the 'HWSD' Web Services",
    "description": "Programmatic interface to the Harmonized World Soil Database \n    'HWSD' web services (<https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1247>).\n    Allows for easy downloads of 'HWSD' soil data directly to your R workspace \n    or your computer. Routines for both single pixel data downloads and\n    gridded data are provided.",
    "version": "1.2",
    "maintainer": "Koen Hufkens <koen.hufkens@gmail.com>",
    "author": "Koen Hufkens [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5070-8109>),\n  BlueGreen Labs [cph, fnd]",
    "url": "https://github.com/bluegreen-labs/hwsdr,\nhttps://bluegreen-labs.github.io/hwsdr/",
    "bug_reports": "https://github.com/bluegreen-labs/hwsdr/issues",
    "repository": "https://cran.r-project.org/package=hwsdr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hwsdr Interface to the 'HWSD' Web Services Programmatic interface to the Harmonized World Soil Database \n    'HWSD' web services (<https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1247>).\n    Allows for easy downloads of 'HWSD' soil data directly to your R workspace \n    or your computer. Routines for both single pixel data downloads and\n    gridded data are provided.  "
  },
  {
    "id": 14123,
    "package_name": "hyd1d",
    "title": "1d Water Level Interpolation along the Rivers Elbe and Rhine",
    "description": "An S4 class and several functions which utilize internally stored\n    datasets and gauging data enable 1d water level interpolation. The S4 class\n    (WaterLevelDataFrame) structures the computation and visualisation\n    of 1d water level information along the German federal waterways Elbe and\n    Rhine. 'hyd1d' delivers 1d water level data - extracted from the 'FLYS'\n    database - and validated gauging data - extracted from the hydrological\n    database 'WISKI7' - package-internally. For computations near real time\n    gauging data are queried externally from the 'PEGELONLINE REST API' \n    <https://pegelonline.wsv.de/webservice/dokuRestapi>.",
    "version": "0.5.4",
    "maintainer": "Arnd Weber <arnd.weber@bafg.de>",
    "author": "Arnd Weber [aut, cre] (ORCID: <https://orcid.org/0000-0002-5973-2770>),\n  Marcus Hatz [aut],\n  Wolfgang St\u00fcrmer [ctb],\n  Wilfried Wiechmann [ctb],\n  Benjamin Eberhardt [ctb]",
    "url": "https://hyd1d.bafg.de, https://github.com/bafg-bund/hyd1d",
    "bug_reports": "https://github.com/bafg-bund/hyd1d/issues/",
    "repository": "https://cran.r-project.org/package=hyd1d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hyd1d 1d Water Level Interpolation along the Rivers Elbe and Rhine An S4 class and several functions which utilize internally stored\n    datasets and gauging data enable 1d water level interpolation. The S4 class\n    (WaterLevelDataFrame) structures the computation and visualisation\n    of 1d water level information along the German federal waterways Elbe and\n    Rhine. 'hyd1d' delivers 1d water level data - extracted from the 'FLYS'\n    database - and validated gauging data - extracted from the hydrological\n    database 'WISKI7' - package-internally. For computations near real time\n    gauging data are queried externally from the 'PEGELONLINE REST API' \n    <https://pegelonline.wsv.de/webservice/dokuRestapi>.  "
  },
  {
    "id": 14179,
    "package_name": "iGSEA",
    "title": "Integrative Gene Set Enrichment Analysis Approaches",
    "description": "To integrate multiple GSEA studies, we propose a hybrid strategy,\n    iGSEA-AT, for choosing random effects (RE) versus fixed effect (FE) models,\n    with an attempt to achieve the potential maximum statistical efficiency as \n    well as stability in performance in various practical situations. In addition\n    to iGSEA-AT, this package also provides options to perform integrative GSEA\n    with testing based on a FE model (iGSEA-FE) and testing based on a RE model\n    (iGSEA-RE). The approaches account for different set sizes when testing a\n    database of gene sets. The function is easy to use, and the three approaches\n    can be applied to both binary and continuous phenotypes. ",
    "version": "1.2",
    "maintainer": "Wentao Lu <wlu1026@yahoo.com>",
    "author": "Wentao Lu, Xinlei Wang, Xiaowei Zhan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iGSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iGSEA Integrative Gene Set Enrichment Analysis Approaches To integrate multiple GSEA studies, we propose a hybrid strategy,\n    iGSEA-AT, for choosing random effects (RE) versus fixed effect (FE) models,\n    with an attempt to achieve the potential maximum statistical efficiency as \n    well as stability in performance in various practical situations. In addition\n    to iGSEA-AT, this package also provides options to perform integrative GSEA\n    with testing based on a FE model (iGSEA-FE) and testing based on a RE model\n    (iGSEA-RE). The approaches account for different set sizes when testing a\n    database of gene sets. The function is easy to use, and the three approaches\n    can be applied to both binary and continuous phenotypes.   "
  },
  {
    "id": 14198,
    "package_name": "iRfcb",
    "title": "Tools for Managing Imaging FlowCytobot (IFCB) Data",
    "description": "A comprehensive suite of tools for managing, processing, and\n    analyzing data from the IFCB. I R FlowCytobot ('iRfcb') supports\n    quality control, geospatial analysis, and preparation of IFCB data for\n    publication in databases like <https://www.gbif.org>,\n    <https://www.obis.org>, <https://emodnet.ec.europa.eu/en>,\n    <https://shark.smhi.se/>, and <https://www.ecotaxa.org>. The package\n    integrates with the MATLAB 'ifcb-analysis' tool, which is described in\n    Sosik and Olson (2007) <doi:10.4319/lom.2007.5.204>, and provides\n    features for working with raw, manually classified, and machine\n    learning\u2013classified image datasets. Key functionalities include image\n    extraction, particle size distribution analysis, taxonomic data\n    handling, and biomass concentration calculations, essential for\n    plankton research.",
    "version": "0.6.0",
    "maintainer": "Anders Torstensson <anders.torstensson@smhi.se>",
    "author": "Anders Torstensson [aut, cre] (Swedish Meteorological and Hydrological\n    Institute, ORCID: <https://orcid.org/0000-0002-8283-656X>),\n  Kendra Hayashi [ctb] (ORCID: <https://orcid.org/0000-0003-1600-9504>),\n  Jamie Enslein [ctb],\n  Raphael Kudela [ctb] (ORCID: <https://orcid.org/0000-0002-8640-1205>),\n  Alle Lie [ctb] (ORCID: <https://orcid.org/0009-0001-8709-4841>),\n  Jayme Smith [ctb] (ORCID: <https://orcid.org/0000-0002-9669-4427>),\n  DTO-BioFlow [fnd] (Horizon Europe, HORIZON-MISS-2022-OCEAN-01-07),\n  SBDI [fnd] (Swedish Research Council, 2019-00242)",
    "url": "https://europeanifcbgroup.github.io/iRfcb/,\nhttps://github.com/EuropeanIFCBGroup/iRfcb",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iRfcb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iRfcb Tools for Managing Imaging FlowCytobot (IFCB) Data A comprehensive suite of tools for managing, processing, and\n    analyzing data from the IFCB. I R FlowCytobot ('iRfcb') supports\n    quality control, geospatial analysis, and preparation of IFCB data for\n    publication in databases like <https://www.gbif.org>,\n    <https://www.obis.org>, <https://emodnet.ec.europa.eu/en>,\n    <https://shark.smhi.se/>, and <https://www.ecotaxa.org>. The package\n    integrates with the MATLAB 'ifcb-analysis' tool, which is described in\n    Sosik and Olson (2007) <doi:10.4319/lom.2007.5.204>, and provides\n    features for working with raw, manually classified, and machine\n    learning\u2013classified image datasets. Key functionalities include image\n    extraction, particle size distribution analysis, taxonomic data\n    handling, and biomass concentration calculations, essential for\n    plankton research.  "
  },
  {
    "id": 14225,
    "package_name": "ibmdbR",
    "title": "IBM in-Database Analytics for R",
    "description": "\n        Functionality required to efficiently use R with IBM(R) Db2(R)\n        Warehouse offerings (formerly IBM dashDB(R)) and IBM Db2 for z/OS(R) in\n        conjunction with IBM Db2 Analytics Accelerator for z/OS.\n        Many basic and complex R operations are pushed down into the database, \n        which removes the main memory boundary of R and allows to make full \n        use of parallel processing in the underlying database.\n        For executing R-functions in a multi-node environment in parallel the idaTApply() function\n        requires the 'SparkR' package (<https://spark.apache.org/docs/latest/sparkr.html>).\n        The optional 'ggplot2' package is needed for the plot.idaLm() function only.",
    "version": "1.51.0",
    "maintainer": "Shaikh Quader <db2-analytics@de.ibm.com>",
    "author": "Shaikh Quader [aut, cre],\n  Toni Bollinger [aut],\n  Ming-pang Wei [aut],\n  Alexander Eckert [aut],\n  Michael Wurst [aut],\n  Craig Blaha [ctb] (documentation),\n  IBM Corporation [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ibmdbR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ibmdbR IBM in-Database Analytics for R \n        Functionality required to efficiently use R with IBM(R) Db2(R)\n        Warehouse offerings (formerly IBM dashDB(R)) and IBM Db2 for z/OS(R) in\n        conjunction with IBM Db2 Analytics Accelerator for z/OS.\n        Many basic and complex R operations are pushed down into the database, \n        which removes the main memory boundary of R and allows to make full \n        use of parallel processing in the underlying database.\n        For executing R-functions in a multi-node environment in parallel the idaTApply() function\n        requires the 'SparkR' package (<https://spark.apache.org/docs/latest/sparkr.html>).\n        The optional 'ggplot2' package is needed for the plot.idaLm() function only.  "
  },
  {
    "id": 14251,
    "package_name": "icesConnect",
    "title": "Provides User Tokens for Access to ICES Web Services",
    "description": "Provides user tokens for ICES web services that require\n  authentication and authorization.  Web services covered by this\n  package are ICES VMS database, the ICES DATSU web services, and the\n  ICES SharePoint site <https://www.ices.dk/data/tools/Pages/WebServices.aspx>.",
    "version": "1.1.4",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "author": "Colin Millar [aut, cre]",
    "url": "https://www.ices.dk/data/tools/Pages/WebServices.aspx,\nhttps://github.com/ices-tools-prod/icesConnect",
    "bug_reports": "https://github.com/ices-tools-prod/icesConnect/issues",
    "repository": "https://cran.r-project.org/package=icesConnect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icesConnect Provides User Tokens for Access to ICES Web Services Provides user tokens for ICES web services that require\n  authentication and authorization.  Web services covered by this\n  package are ICES VMS database, the ICES DATSU web services, and the\n  ICES SharePoint site <https://www.ices.dk/data/tools/Pages/WebServices.aspx>.  "
  },
  {
    "id": 14252,
    "package_name": "icesDatras",
    "title": "DATRAS Trawl Survey Database Web Services",
    "description": "R interface to access the web services of the ICES (International\n             Council for the Exploration of the Sea) DATRAS trawl survey\n             database <https://datras.ices.dk/WebServices/Webservices.aspx>.",
    "version": "1.4.1",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "author": "Colin Millar [aut, cre],\n  Cecilia Kvaavik [aut],\n  Adriana Villamor [aut],\n  Scott Large [aut],\n  Arni Magnusson [aut],\n  Vaishav Soni [ctb]",
    "url": "https://datras.ices.dk/WebServices/Webservices.aspx,\nhttps://github.com/ices-tools-prod/icesDatras",
    "bug_reports": "https://github.com/ices-tools-prod/icesDatras/issues",
    "repository": "https://cran.r-project.org/package=icesDatras",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icesDatras DATRAS Trawl Survey Database Web Services R interface to access the web services of the ICES (International\n             Council for the Exploration of the Sea) DATRAS trawl survey\n             database <https://datras.ices.dk/WebServices/Webservices.aspx>.  "
  },
  {
    "id": 14255,
    "package_name": "icesSAG",
    "title": "Stock Assessment Graphs Database Web Services",
    "description": "R interface to access the web services of the ICES Stock Assessment\n             Graphs database <https://sg.ices.dk>.",
    "version": "1.6.2",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "author": "Colin Millar [aut, cre],\n  Scott Large [aut],\n  Arni Magnusson [aut],\n  Carlos Pinto [aut],\n  Laura Andreea Petre [aut]",
    "url": "https://sg.ices.dk, https://github.com/ices-tools-prod/icesSAG",
    "bug_reports": "https://github.com/ices-tools-prod/icesSAG/issues",
    "repository": "https://cran.r-project.org/package=icesSAG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icesSAG Stock Assessment Graphs Database Web Services R interface to access the web services of the ICES Stock Assessment\n             Graphs database <https://sg.ices.dk>.  "
  },
  {
    "id": 14256,
    "package_name": "icesSD",
    "title": "Stock Database Web Services",
    "description": "R interface to access the web services of the ICES Stock Database <https://sd.ices.dk>.",
    "version": "2.1.0",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "author": "Colin Millar [aut, cre],\n  Scott Large [aut],\n  Arni Magnusson [aut]",
    "url": "https://sd.ices.dk, https://github.com/ices-tools-prod/icesSD",
    "bug_reports": "https://github.com/ices-tools-prod/icesSD/issues",
    "repository": "https://cran.r-project.org/package=icesSD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icesSD Stock Database Web Services R interface to access the web services of the ICES Stock Database <https://sd.ices.dk>.  "
  },
  {
    "id": 14258,
    "package_name": "icesVocab",
    "title": "ICES Vocabularies Database Web Services",
    "description": "R interface to access the Vocabularies REST API of the ICES\n  (International Council for the Exploration of the Sea) Vocabularies database\n  <https://vocab.ices.dk/services/>.",
    "version": "1.3.2",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "author": "Colin Millar [aut, cre],\n  Arni Magnusson [aut],\n  Cedric Briand [ctb]",
    "url": "https://vocab.ices.dk/services/api/swagger/index.html,\nhttps://github.com/ices-tools-prod/icesVocab",
    "bug_reports": "https://github.com/ices-tools-prod/icesVocab/issues",
    "repository": "https://cran.r-project.org/package=icesVocab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "icesVocab ICES Vocabularies Database Web Services R interface to access the Vocabularies REST API of the ICES\n  (International Council for the Exploration of the Sea) Vocabularies database\n  <https://vocab.ices.dk/services/>.  "
  },
  {
    "id": 14268,
    "package_name": "idbr",
    "title": "R Interface to the US Census Bureau International Data Base API",
    "description": "Use R to make requests to the US Census Bureau's International Data Base API.\n             Results are returned as R data frames.  For more information about the IDB API, visit\n             <https://www.census.gov/data/developers/data-sets/international-database.html>.",
    "version": "1.2",
    "maintainer": "Kyle Walker <kyle.walker@tcu.edu>",
    "author": "Kyle Walker [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=idbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "idbr R Interface to the US Census Bureau International Data Base API Use R to make requests to the US Census Bureau's International Data Base API.\n             Results are returned as R data frames.  For more information about the IDB API, visit\n             <https://www.census.gov/data/developers/data-sets/international-database.html>.  "
  },
  {
    "id": 14291,
    "package_name": "ieugwasr",
    "title": "Interface to the 'OpenGWAS' Database API",
    "description": "Interface to the 'OpenGWAS' database API <https://api.opengwas.io/api/>. Includes a wrapper\n    to make generic calls to the API, plus convenience functions for\n    specific queries.",
    "version": "1.1.0",
    "maintainer": "Gibran Hemani <g.hemani@bristol.ac.uk>",
    "author": "Gibran Hemani [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0920-1055>),\n  Ben Elsworth [aut] (ORCID: <https://orcid.org/0000-0001-7328-4233>),\n  Tom Palmer [aut] (ORCID: <https://orcid.org/0000-0003-4655-4511>),\n  Rita Rasteiro [aut] (ORCID: <https://orcid.org/0000-0002-4217-3060>)",
    "url": "https://github.com/MRCIEU/ieugwasr,\nhttps://mrcieu.github.io/ieugwasr/",
    "bug_reports": "https://github.com/MRCIEU/ieugwasr/issues",
    "repository": "https://cran.r-project.org/package=ieugwasr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ieugwasr Interface to the 'OpenGWAS' Database API Interface to the 'OpenGWAS' database API <https://api.opengwas.io/api/>. Includes a wrapper\n    to make generic calls to the API, plus convenience functions for\n    specific queries.  "
  },
  {
    "id": 14307,
    "package_name": "igoR",
    "title": "Intergovernmental Organizations Database",
    "description": "Tools to extract information from the Intergovernmental\n    Organizations ('IGO') Database , version 3, provided by the Correlates\n    of War Project <https://correlatesofwar.org/>. See also Pevehouse, J.\n    C. et al. (2020).  Version 3 includes information from 1815 to 2014.",
    "version": "0.2.1",
    "maintainer": "Diego Hernang\u00f3mez <diego.hernangomezherrero@gmail.com>",
    "author": "Diego Hernang\u00f3mez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>)",
    "url": "https://dieghernan.github.io/igoR/,\nhttps://github.com/dieghernan/igoR",
    "bug_reports": "https://github.com/dieghernan/igoR/issues",
    "repository": "https://cran.r-project.org/package=igoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "igoR Intergovernmental Organizations Database Tools to extract information from the Intergovernmental\n    Organizations ('IGO') Database , version 3, provided by the Correlates\n    of War Project <https://correlatesofwar.org/>. See also Pevehouse, J.\n    C. et al. (2020).  Version 3 includes information from 1815 to 2014.  "
  },
  {
    "id": 14317,
    "package_name": "ihpdr",
    "title": "Download Data from the International House Price Database",
    "description": "Web scraping the <https://www.dallasfed.org> for\n    up-to-date data on international house prices and exuberance\n    indicators. Download data in tidy format.",
    "version": "1.2.1",
    "maintainer": "Kostas Vasilopoulos <k.vasilopoulo@gmail.com>",
    "author": "Kostas Vasilopoulos [aut, cre]",
    "url": "https://github.com/kvasilopoulos/ihpdr",
    "bug_reports": "https://github.com/kvasilopoulos/ihpdr/issues",
    "repository": "https://cran.r-project.org/package=ihpdr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ihpdr Download Data from the International House Price Database Web scraping the <https://www.dallasfed.org> for\n    up-to-date data on international house prices and exuberance\n    indicators. Download data in tidy format.  "
  },
  {
    "id": 14345,
    "package_name": "imdbapi",
    "title": "Get Movie, Television Data from the 'imdb' Database",
    "description": "Provides API access to the <http://imdbapi.net> which maintains metadata\n             about movies, games and television shows through a public API.",
    "version": "0.1.0",
    "maintainer": "Yuan Li <851277048@qq.com>",
    "author": "Daxuan Deng  Yuan Li",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=imdbapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imdbapi Get Movie, Television Data from the 'imdb' Database Provides API access to the <http://imdbapi.net> which maintains metadata\n             about movies, games and television shows through a public API.  "
  },
  {
    "id": 14348,
    "package_name": "imfweo",
    "title": "Seamless Access to IMF World Economic Outlook (WEO) Data",
    "description": "Provides tools to download, process, and analyze data from the\n    International Monetary Fund's World Economic Outlook (WEO) database \n    <https://www.imf.org/en/Publications/SPROLLs/world-economic-outlook-databases>. \n    Functions support downloading complete WEO releases, accessing specific \n    economic indicators for selected countries, and listing available data.",
    "version": "0.1.0",
    "maintainer": "Teal Emery <lte@tealinsights.com>",
    "author": "Teal Emery [aut, cre],\n  Teal Insights [cph],\n  Christoph Scheuch [aut] (ORCID:\n    <https://orcid.org/0009-0004-0423-6819>)",
    "url": "https://teal-insights.github.io/r-imfweo/,\nhttps://github.com/teal-insights/r-imfweo/",
    "bug_reports": "https://github.com/teal-insights/r-imfweo/issues",
    "repository": "https://cran.r-project.org/package=imfweo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imfweo Seamless Access to IMF World Economic Outlook (WEO) Data Provides tools to download, process, and analyze data from the\n    International Monetary Fund's World Economic Outlook (WEO) database \n    <https://www.imf.org/en/Publications/SPROLLs/world-economic-outlook-databases>. \n    Functions support downloading complete WEO releases, accessing specific \n    economic indicators for selected countries, and listing available data.  "
  },
  {
    "id": 14363,
    "package_name": "implyr",
    "title": "R Interface for Apache Impala",
    "description": "'SQL' back-end to 'dplyr' for Apache Impala, the massively\n    parallel processing query engine for Apache 'Hadoop'. Impala enables\n    low-latency 'SQL' queries on data stored in the 'Hadoop' Distributed\n    File System '(HDFS)', Apache 'HBase', Apache 'Kudu', Amazon Simple \n    Storage Service '(S3)', Microsoft Azure Data Lake Store '(ADLS)', \n    and Dell 'EMC' 'Isilon'. See <https://impala.apache.org> for more\n    information about Impala.",
    "version": "0.5.0",
    "maintainer": "Ian Cook <ianmcook@gmail.com>",
    "author": "Ian Cook [aut, cre],\n  Cloudera [cph]",
    "url": "https://github.com/ianmcook/implyr",
    "bug_reports": "https://github.com/ianmcook/implyr/issues",
    "repository": "https://cran.r-project.org/package=implyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "implyr R Interface for Apache Impala 'SQL' back-end to 'dplyr' for Apache Impala, the massively\n    parallel processing query engine for Apache 'Hadoop'. Impala enables\n    low-latency 'SQL' queries on data stored in the 'Hadoop' Distributed\n    File System '(HDFS)', Apache 'HBase', Apache 'Kudu', Amazon Simple \n    Storage Service '(S3)', Microsoft Azure Data Lake Store '(ADLS)', \n    and Dell 'EMC' 'Isilon'. See <https://impala.apache.org> for more\n    information about Impala.  "
  },
  {
    "id": 14429,
    "package_name": "influxdbr",
    "title": "R Interface to InfluxDB",
    "description": "An R interface to the InfluxDB time series database <https://www.influxdata.com>. This package allows you to fetch and write time series data from/to an InfluxDB server. Additionally, handy wrappers for the Influx Query Language (IQL) to manage and explore a remote database are provided. ",
    "version": "0.14.2",
    "maintainer": "Dominik Leutnant <leutnant@fh-muenster.de>",
    "author": "Dominik Leutnant [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3293-2315>)",
    "url": "https://github.com/dleutnant/influxdbr",
    "bug_reports": "http://github.com/dleutnant/influxdbr/issues",
    "repository": "https://cran.r-project.org/package=influxdbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "influxdbr R Interface to InfluxDB An R interface to the InfluxDB time series database <https://www.influxdata.com>. This package allows you to fetch and write time series data from/to an InfluxDB server. Additionally, handy wrappers for the Influx Query Language (IQL) to manage and explore a remote database are provided.   "
  },
  {
    "id": 14455,
    "package_name": "insee",
    "title": "Tools to Easily Download Data from INSEE BDM Database",
    "description": "Using embedded sdmx queries, get the data of more than 150 000 insee series from 'bdm' macroeconomic database. ",
    "version": "1.1.7",
    "maintainer": "Hadrien Leclerc <leclerc.hadrien@gmail.com>",
    "author": "Hadrien Leclerc [aut, cre],\n  INSEE [cph]",
    "url": "https://pyr-opendatafr.github.io/R-Insee-Data/",
    "bug_reports": "https://github.com/pyr-opendatafr/R-Insee-Data/issues",
    "repository": "https://cran.r-project.org/package=insee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "insee Tools to Easily Download Data from INSEE BDM Database Using embedded sdmx queries, get the data of more than 150 000 insee series from 'bdm' macroeconomic database.   "
  },
  {
    "id": 14520,
    "package_name": "invacost",
    "title": "Analyse Biological Invasion Costs with the 'InvaCost' Database",
    "description": "Provides an up-to-date version of the 'InvaCost' database \n    (<doi:10.6084/m9.figshare.12668570>) in R, and\n    several functions to analyse the costs of invasive alien species \n    (<doi:10.1111/2041-210X.13929>). ",
    "version": "1.1-7",
    "maintainer": "Boris Leroy <leroy.boris@gmail.com>",
    "author": "Boris Leroy [cre, aut],\n  Andrew Kramer [aut],\n  Anne-Charlotte Vaissi\u00e8re [ctb],\n  Christophe Diagne [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=invacost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "invacost Analyse Biological Invasion Costs with the 'InvaCost' Database Provides an up-to-date version of the 'InvaCost' database \n    (<doi:10.6084/m9.figshare.12668570>) in R, and\n    several functions to analyse the costs of invasive alien species \n    (<doi:10.1111/2041-210X.13929>).   "
  },
  {
    "id": 14540,
    "package_name": "ip2location",
    "title": "Lookup for IP Address Information",
    "description": "Enables the user to find the country, region, district, city, coordinates, zip code, time zone, ISP, domain name, connection type, area code, weather, Mobile Country Code, Mobile Network Code, mobile brand name, elevation, usage type, address type, IAB category and Autonomous system information that any IP address or hostname originates from. Supported IPv4 and IPv6.\n        Please visit <https://www.ip2location.com> to learn more. You may also want to visit <https://lite.ip2location.com> for free database download.\n        This package requires 'IP2Location Python' module. At the terminal, please run 'pip install IP2Location' to install the module.",
    "version": "8.1.3",
    "maintainer": "Kai Wen Ooi <support@ip2location.com>",
    "author": "Kai Wen Ooi [aut, cre],\n  IP2Location [cph]",
    "url": "https://github.com/ip2location/ip2location-r",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ip2location",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ip2location Lookup for IP Address Information Enables the user to find the country, region, district, city, coordinates, zip code, time zone, ISP, domain name, connection type, area code, weather, Mobile Country Code, Mobile Network Code, mobile brand name, elevation, usage type, address type, IAB category and Autonomous system information that any IP address or hostname originates from. Supported IPv4 and IPv6.\n        Please visit <https://www.ip2location.com> to learn more. You may also want to visit <https://lite.ip2location.com> for free database download.\n        This package requires 'IP2Location Python' module. At the terminal, please run 'pip install IP2Location' to install the module.  "
  },
  {
    "id": 14543,
    "package_name": "ip2proxy",
    "title": "Lookup for IP Address Proxy Information",
    "description": "Enable user to find the IP addresses which are used as VPN anonymizer, open proxies, web proxies and Tor exits.\n    The package lookup the proxy IP address from IP2Proxy BIN Data file. You may visit <https://lite.ip2location.com> for free database download. ",
    "version": "1.2.0",
    "maintainer": "Kai Wen Ooi <support@ip2location.com>",
    "author": "Kai Wen Ooi [aut, cre],\n  IP2Location [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ip2proxy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ip2proxy Lookup for IP Address Proxy Information Enable user to find the IP addresses which are used as VPN anonymizer, open proxies, web proxies and Tor exits.\n    The package lookup the proxy IP address from IP2Proxy BIN Data file. You may visit <https://lite.ip2location.com> for free database download.   "
  },
  {
    "id": 14553,
    "package_name": "ipeadatar",
    "title": "API Wrapper for 'Ipeadata'",
    "description": "Allows direct access to the macroeconomic, \n             financial and regional database maintained by \n             Brazilian Institute for Applied Economic Research ('Ipea').\n             This R package uses the 'Ipeadata' API. For more information, \n             see <http://www.ipeadata.gov.br/>.",
    "version": "0.1.6",
    "maintainer": "Luiz Eduardo S. Gomes <gomes.leduardo@gmail.com>",
    "author": "Luiz Eduardo S. Gomes [aut, cre],\n  Jessyka A. P. Goltara [ctb]",
    "url": "https://github.com/gomesleduardo/ipeadatar",
    "bug_reports": "https://github.com/gomesleduardo/ipeadatar/issues",
    "repository": "https://cran.r-project.org/package=ipeadatar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ipeadatar API Wrapper for 'Ipeadata' Allows direct access to the macroeconomic, \n             financial and regional database maintained by \n             Brazilian Institute for Applied Economic Research ('Ipea').\n             This R package uses the 'Ipeadata' API. For more information, \n             see <http://www.ipeadata.gov.br/>.  "
  },
  {
    "id": 14630,
    "package_name": "istat",
    "title": "Download and Manipulate Data from Istat",
    "description": "Download data from ISTAT (Italian Institute of Statistics) database, both old and new provider (respectively, <http://dati.istat.it/> and <https://esploradati.istat.it/databrowser/>). Additional functions for manipulating data are provided. Moreover, a 'shiny' application called 'shinyIstat' can be used to search, download and filter datasets in an easier way.",
    "version": "1.1.1",
    "maintainer": "Elena Gradi <elenaagradi@gmail.com>",
    "author": "Elena Gradi [aut, cre],\n  Alissa Lelli [aut],\n  Daniela Ichim [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=istat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "istat Download and Manipulate Data from Istat Download data from ISTAT (Italian Institute of Statistics) database, both old and new provider (respectively, <http://dati.istat.it/> and <https://esploradati.istat.it/databrowser/>). Additional functions for manipulating data are provided. Moreover, a 'shiny' application called 'shinyIstat' can be used to search, download and filter datasets in an easier way.  "
  },
  {
    "id": 14760,
    "package_name": "jqbr",
    "title": "'jQuery QueryBuilder' Input for 'Shiny'",
    "description": "A highly configurable 'jQuery' plugin offering a simple\n    interface to create complex queries/filters in 'Shiny'. The outputted\n    rules can easily be parsed into a set of 'R' and/or 'SQL' queries and\n    used to filter data. Custom parsing of the rules is also supported.\n    For more information about 'jQuery QueryBuilder' see\n    <https://querybuilder.js.org/>.",
    "version": "1.0.4",
    "maintainer": "Harry Fisher <harryfisher21@gmail.com>",
    "author": "Harry Fisher [aut, cre]",
    "url": "https://github.com/hfshr/jqbr, https://hfshr.github.io/jqbr/",
    "bug_reports": "https://github.com/hfshr/jqbr/issues",
    "repository": "https://cran.r-project.org/package=jqbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jqbr 'jQuery QueryBuilder' Input for 'Shiny' A highly configurable 'jQuery' plugin offering a simple\n    interface to create complex queries/filters in 'Shiny'. The outputted\n    rules can easily be parsed into a set of 'R' and/or 'SQL' queries and\n    used to filter data. Custom parsing of the rules is also supported.\n    For more information about 'jQuery QueryBuilder' see\n    <https://querybuilder.js.org/>.  "
  },
  {
    "id": 14802,
    "package_name": "kangar00",
    "title": "Kernel Approaches for Nonlinear Genetic Association Regression",
    "description": "Methods to extract information on pathways, genes and various single-nucleotid polymorphisms (SNPs) from online databases. It provides functions for data preparation and evaluation of genetic influence on a binary outcome using the logistic kernel machine test (LKMT). Three different kernel functions are offered to analyze genotype information in this variance component test: A linear kernel, a size-adjusted kernel and a network-based kernel).",
    "version": "1.4.2",
    "maintainer": "Juliane Manitz <r@manitz.org>",
    "author": "Juliane Manitz [aut, cre],\n  Benjamin Hofner [aut],\n  Stefanie Friedrichs [aut],\n  Patricia Burger [aut],\n  Ngoc Thuy Ha [aut],\n  Saskia Freytag [ctb],\n  Heike Bickeboeller [ctb]",
    "url": "https://kangar00.manitz.org/",
    "bug_reports": "https://github.com/jmanitz/kangar00/issues",
    "repository": "https://cran.r-project.org/package=kangar00",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kangar00 Kernel Approaches for Nonlinear Genetic Association Regression Methods to extract information on pathways, genes and various single-nucleotid polymorphisms (SNPs) from online databases. It provides functions for data preparation and evaluation of genetic influence on a binary outcome using the logistic kernel machine test (LKMT). Three different kernel functions are offered to analyze genotype information in this variance component test: A linear kernel, a size-adjusted kernel and a network-based kernel).  "
  },
  {
    "id": 14878,
    "package_name": "kidsides",
    "title": "Download, Cache, and Connect to KidSIDES",
    "description": "Caches and then connects to a 'sqlite' database containing half a million pediatric drug safety signals. \n    The database is part of a family of resources catalogued at <https://nsides.io>. The database\n    contains 17 tables where the description table provides a map between the fields the field's details. \n    The database was created by Nicholas Giangreco during his PhD thesis which you can read in Giangreco (2022) <doi:10.7916/d8-5d9b-6738>. \n    The observations are from the Food and Drug Administration's Adverse Event Reporting System. Generalized additive models estimated\n    drug effects across child development stages for the occurrence of an adverse event when exposed to a drug compared to other drugs.\n    Read more at the methods detailed in Giangreco (2022) <doi:10.1016/j.medj.2022.06.001>. ",
    "version": "0.5.0",
    "maintainer": "Nicholas Giangreco <nick.giangreco@gmail.com>",
    "author": "Nicholas Giangreco [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-8138-4947>)",
    "url": "https://github.com/ngiangre/kidsides,\nhttps://ngiangre.github.io/kidsides/, https://nsides.io",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kidsides",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kidsides Download, Cache, and Connect to KidSIDES Caches and then connects to a 'sqlite' database containing half a million pediatric drug safety signals. \n    The database is part of a family of resources catalogued at <https://nsides.io>. The database\n    contains 17 tables where the description table provides a map between the fields the field's details. \n    The database was created by Nicholas Giangreco during his PhD thesis which you can read in Giangreco (2022) <doi:10.7916/d8-5d9b-6738>. \n    The observations are from the Food and Drug Administration's Adverse Event Reporting System. Generalized additive models estimated\n    drug effects across child development stages for the occurrence of an adverse event when exposed to a drug compared to other drugs.\n    Read more at the methods detailed in Giangreco (2022) <doi:10.1016/j.medj.2022.06.001>.   "
  },
  {
    "id": 14895,
    "package_name": "kiwisR",
    "title": "A Wrapper for Querying KISTERS 'WISKI' Databases via the 'KiWIS'\nAPI",
    "description": "A wrapper for querying 'WISKI' databases via the 'KiWIS' 'REST' API. 'WISKI' is an 'SQL' relational database \n  used for the collection and storage of water data developed by KISTERS and 'KiWIS' is a 'REST' service that provides\n  access to 'WISKI' databases via HTTP requests (<https://www.kisters.eu/water-weather-and-environment/>). \n  Contains a list of default databases (called 'hubs') and also allows users to provide their own 'KiWIS' URL. \n  Supports the entire query process- from metadata to specific time series values. All data is returned as tidy tibbles.",
    "version": "0.2.4",
    "maintainer": "Ryan Whaley <rdgwhaley@gmail.com>",
    "author": "Ryan Whaley [aut, cre],\n  Sam Albers [ctb]",
    "url": "https://github.com/rywhale/kiwisR",
    "bug_reports": "https://github.com/rywhale/kiwisR/issues",
    "repository": "https://cran.r-project.org/package=kiwisR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kiwisR A Wrapper for Querying KISTERS 'WISKI' Databases via the 'KiWIS'\nAPI A wrapper for querying 'WISKI' databases via the 'KiWIS' 'REST' API. 'WISKI' is an 'SQL' relational database \n  used for the collection and storage of water data developed by KISTERS and 'KiWIS' is a 'REST' service that provides\n  access to 'WISKI' databases via HTTP requests (<https://www.kisters.eu/water-weather-and-environment/>). \n  Contains a list of default databases (called 'hubs') and also allows users to provide their own 'KiWIS' URL. \n  Supports the entire query process- from metadata to specific time series values. All data is returned as tidy tibbles.  "
  },
  {
    "id": 14916,
    "package_name": "knfi",
    "title": "Analysis of Korean National Forest Inventory Database",
    "description": "Understanding the current status of forest resources is essential for monitoring changes \n    in forest ecosystems and generating related statistics. In South Korea, the National Forest\n    Inventory (NFI) surveys over 4,500 sample plots nationwide every five years and records 70 items,\n    including forest stand, forest resource, and forest vegetation surveys. Many researchers use NFI \n    as the primary data for research, such as biomass estimation or analyzing the importance value of\n    each species over time and space, depending on the research purpose. However, the large volume\n    of accumulated forest survey data from across the country can make it challenging to manage and\n    utilize such a vast dataset. To address this issue, we developed an R package that efficiently\n    handles large-scale NFI data across time and space. The package offers a comprehensive \n    workflow for NFI data analysis. It starts with data processing, where read_nfi() function \n    reconstructs NFI data according to the researcher's needs while performing basic integrity checks\n    for data quality.Following this, the package provides analytical tools that operate on the verified\n    data. These include functions like summary_nfi() for summary statistics, diversity_nfi() for\n    biodiversity analysis, iv_nfi() for calculating species importance value, and biomass_nfi() and\n    cwd_biomass_nfi() for biomass estimation. Finally, for visualization, the tsvis_nfi() function\n    generates graphs and maps, allowing users to visualize forest ecosystem changes across various\n    spatial and temporal scales. This integrated approach and its specialized functions can enhance\n    the efficiency of processing and analyzing NFI data, providing researchers with insights into\n    forest ecosystems. The NFI Excel files (.xlsx) are not included in the R package and must be \n    downloaded  separately. Users can access these NFI Excel files by visiting \n    the Korea Forest Service Forestry Statistics Platform <https://kfss.forest.go.kr/stat/ptl/article/articleList.do?curMenu=11694&bbsId=microdataboard>\n    to download the annual NFI Excel files, which are bundled in .zip archives. Please note that this \n    website is only available in Korean, and direct download links can be found in the notes section \n    of the read_nfi() function.",
    "version": "1.0.1.9",
    "maintainer": "Sinyoung Park <youngsin0306@kookmin.ac.kr>",
    "author": "Sinyoung Park [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3658-0935>),\n  Wonhee Cho [aut, ctb] (ORCID: <https://orcid.org/0000-0002-9598-6188>),\n  Inyoo Kim [aut, ctb] (ORCID: <https://orcid.org/0000-0002-7709-8224>),\n  Wontaek Lim [aut, ctb] (ORCID: <https://orcid.org/0000-0002-5872-1121>),\n  Dongwook W. Ko [aut, ths] (ORCID:\n    <https://orcid.org/0000-0002-6944-0261>)",
    "url": "https://github.com/SYOUNG9836/knfi,\nhttps://syoung9836.github.io/knfi/",
    "bug_reports": "https://github.com/SYOUNG9836/knfi/issues",
    "repository": "https://cran.r-project.org/package=knfi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knfi Analysis of Korean National Forest Inventory Database Understanding the current status of forest resources is essential for monitoring changes \n    in forest ecosystems and generating related statistics. In South Korea, the National Forest\n    Inventory (NFI) surveys over 4,500 sample plots nationwide every five years and records 70 items,\n    including forest stand, forest resource, and forest vegetation surveys. Many researchers use NFI \n    as the primary data for research, such as biomass estimation or analyzing the importance value of\n    each species over time and space, depending on the research purpose. However, the large volume\n    of accumulated forest survey data from across the country can make it challenging to manage and\n    utilize such a vast dataset. To address this issue, we developed an R package that efficiently\n    handles large-scale NFI data across time and space. The package offers a comprehensive \n    workflow for NFI data analysis. It starts with data processing, where read_nfi() function \n    reconstructs NFI data according to the researcher's needs while performing basic integrity checks\n    for data quality.Following this, the package provides analytical tools that operate on the verified\n    data. These include functions like summary_nfi() for summary statistics, diversity_nfi() for\n    biodiversity analysis, iv_nfi() for calculating species importance value, and biomass_nfi() and\n    cwd_biomass_nfi() for biomass estimation. Finally, for visualization, the tsvis_nfi() function\n    generates graphs and maps, allowing users to visualize forest ecosystem changes across various\n    spatial and temporal scales. This integrated approach and its specialized functions can enhance\n    the efficiency of processing and analyzing NFI data, providing researchers with insights into\n    forest ecosystems. The NFI Excel files (.xlsx) are not included in the R package and must be \n    downloaded  separately. Users can access these NFI Excel files by visiting \n    the Korea Forest Service Forestry Statistics Platform <https://kfss.forest.go.kr/stat/ptl/article/articleList.do?curMenu=11694&bbsId=microdataboard>\n    to download the annual NFI Excel files, which are bundled in .zip archives. Please note that this \n    website is only available in Korean, and direct download links can be found in the notes section \n    of the read_nfi() function.  "
  },
  {
    "id": 14918,
    "package_name": "knitcitations",
    "title": "Citations for 'Knitr' Markdown Files",
    "description": "Provides the ability to create dynamic citations\n    in which the bibliographic information is pulled from the web rather\n    than having to be entered into a local database such as 'bibtex' ahead of\n    time. The package is primarily aimed at authoring in the R 'markdown'\n    format, and can provide outputs for web-based authoring such as linked\n    text for inline citations.  Cite using a 'DOI', URL, or\n    'bibtex' file key.  See the package URL for details.",
    "version": "1.0.12",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>)",
    "url": "https://github.com/cboettig/knitcitations",
    "bug_reports": "https://github.com/cboettig/knitcitations/issues",
    "repository": "https://cran.r-project.org/package=knitcitations",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "knitcitations Citations for 'Knitr' Markdown Files Provides the ability to create dynamic citations\n    in which the bibliographic information is pulled from the web rather\n    than having to be entered into a local database such as 'bibtex' ahead of\n    time. The package is primarily aimed at authoring in the R 'markdown'\n    format, and can provide outputs for web-based authoring such as linked\n    text for inline citations.  Cite using a 'DOI', URL, or\n    'bibtex' file key.  See the package URL for details.  "
  },
  {
    "id": 14967,
    "package_name": "kuzuR",
    "title": "Interface to 'kuzu' Graph Database",
    "description": "Provides a high-performance 'R' interface to the 'kuzu' graph database.\n    It uses the 'reticulate' package to wrap the official 'Python' client\n    ('kuzu', 'pandas', and 'networkx'), allowing users to interact with 'kuzu'\n    seamlessly from within 'R'. Key features include managing database\n    connections, executing 'Cypher' queries, and efficiently loading data from\n    'R' data frames. It also provides seamless integration with the 'R' ecosystem\n    by converting query results directly into popular 'R' data structures,\n    including 'tibble', 'igraph', 'tidygraph', and 'g6R' objects, making\n    'kuzu's powerful graph computation capabilities readily available for data\n    analysis and visualization workflows in 'R'.\n    The 'kuzu' documentation can be found at <https://kuzudb.github.io/docs/>.",
    "version": "0.2.3",
    "maintainer": "Manuel Wick-Eckl <manuel.wick@gmail.com>",
    "author": "Manuel Wick-Eckl [aut, cre]",
    "url": "https://github.com/WickM/kuzuR, https://wickm.github.io/kuzuR/",
    "bug_reports": "https://github.com/WickM/kuzuR/issues",
    "repository": "https://cran.r-project.org/package=kuzuR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kuzuR Interface to 'kuzu' Graph Database Provides a high-performance 'R' interface to the 'kuzu' graph database.\n    It uses the 'reticulate' package to wrap the official 'Python' client\n    ('kuzu', 'pandas', and 'networkx'), allowing users to interact with 'kuzu'\n    seamlessly from within 'R'. Key features include managing database\n    connections, executing 'Cypher' queries, and efficiently loading data from\n    'R' data frames. It also provides seamless integration with the 'R' ecosystem\n    by converting query results directly into popular 'R' data structures,\n    including 'tibble', 'igraph', 'tidygraph', and 'g6R' objects, making\n    'kuzu's powerful graph computation capabilities readily available for data\n    analysis and visualization workflows in 'R'.\n    The 'kuzu' documentation can be found at <https://kuzudb.github.io/docs/>.  "
  },
  {
    "id": 15070,
    "package_name": "lazysql",
    "title": "Lazy SQL Programming",
    "description": "\n    Helper functions to build SQL statements\n    for dbGetQuery or dbSendQuery under program control.\n    They are intended to increase speed of coding and\n    to reduce coding errors. Arguments are carefully checked,\n    in particular SQL identifiers such as names of tables or columns.\n    More patterns will be added as required.",
    "version": "0.1.3",
    "maintainer": "Uwe Block <u.block.mz@gmail.com>",
    "author": "Uwe Block [aut, cre]",
    "url": "https://github.com/UweBlock/lazysql",
    "bug_reports": "https://github.com/UweBlock/lazysql/issues",
    "repository": "https://cran.r-project.org/package=lazysql",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lazysql Lazy SQL Programming \n    Helper functions to build SQL statements\n    for dbGetQuery or dbSendQuery under program control.\n    They are intended to increase speed of coding and\n    to reduce coding errors. Arguments are carefully checked,\n    in particular SQL identifiers such as names of tables or columns.\n    More patterns will be added as required.  "
  },
  {
    "id": 15128,
    "package_name": "legislatoR",
    "title": "Interface to the Comparative Legislators Database",
    "description": "Facilitates access to the Comparative Legislators Database (CLD). The CLD includes political, sociodemographic, career, online presence, public attention, and visual information for over 67,000 contemporary and historical politicians from 16 countries.",
    "version": "1.1.0",
    "maintainer": "Sascha Goebel <sascha.goebel@soz.uni-frankfurt.de>",
    "author": "Sascha Goebel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9032-5874>),\n  Simon Munzert [aut]",
    "url": "https://github.com/saschagobel/legislatoR",
    "bug_reports": "https://github.com/saschagobel/legislatoR/issues",
    "repository": "https://cran.r-project.org/package=legislatoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "legislatoR Interface to the Comparative Legislators Database Facilitates access to the Comparative Legislators Database (CLD). The CLD includes political, sociodemographic, career, online presence, public attention, and visual information for over 67,000 contemporary and historical politicians from 16 countries.  "
  },
  {
    "id": 15143,
    "package_name": "lero.lero",
    "title": "Generate 'Lero Lero' Quotes",
    "description": "Generates quotes from 'Lero Lero', a database for meaningless sentences filled with corporate buzzwords, intended to be used as corporate lorem ipsum (see <http://www.lerolero.com/> for more information). Unfortunately, quotes are currently portuguese-only.",
    "version": "0.2",
    "maintainer": "Lucas Processi <lucasprocessi@gmail.com>",
    "author": "Lucas Processi and Luiz Aizemberg",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lero.lero",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lero.lero Generate 'Lero Lero' Quotes Generates quotes from 'Lero Lero', a database for meaningless sentences filled with corporate buzzwords, intended to be used as corporate lorem ipsum (see <http://www.lerolero.com/> for more information). Unfortunately, quotes are currently portuguese-only.  "
  },
  {
    "id": 15146,
    "package_name": "lest",
    "title": "Vectorised Nested if-else Statements Similar to CASE WHEN in\n'SQL'",
    "description": "Functions for vectorised conditional recoding of\n    variables. case_when() enables you to vectorise multiple if and else\n    statements (like 'CASE WHEN' in 'SQL'). if_else() is a stricter and\n    more predictable version of ifelse() in 'base' that preserves\n    attributes. These functions are forked from 'dplyr' with all package\n    dependencies removed and behave identically to the originals.",
    "version": "1.1.0",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>),\n  Hadley Wickham [aut] (ORCID: <https://orcid.org/0000-0003-4757-117X>),\n  Romain Fran\u00e7ois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\n  Lionel Henry [aut],\n  Kirill M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-1416-3412>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lest Vectorised Nested if-else Statements Similar to CASE WHEN in\n'SQL' Functions for vectorised conditional recoding of\n    variables. case_when() enables you to vectorise multiple if and else\n    statements (like 'CASE WHEN' in 'SQL'). if_else() is a stricter and\n    more predictable version of ifelse() in 'base' that preserves\n    attributes. These functions are forked from 'dplyr' with all package\n    dependencies removed and behave identically to the originals.  "
  },
  {
    "id": 15148,
    "package_name": "letsHerp",
    "title": "An Interface to the Reptile Database",
    "description": "Provides tools to retrieve and summarize taxonomic information and synonymy data for reptile species using data scraped from The Reptile Database website (<https://reptile-database.reptarium.cz/>). Outputs include clean and structured data frames useful for ecological, evolutionary, and conservation research.",
    "version": "0.1.0",
    "maintainer": "Jo\u00e3o Paulo dos Santos Vieira-Alencar <joaopaulo.valencar@gmail.com>",
    "author": "Jo\u00e3o Paulo dos Santos Vieira-Alencar [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6894-6773>),\n  Christoph Liedtke [aut] (ORCID:\n    <https://orcid.org/0000-0002-6221-8043>)",
    "url": "https://github.com/joao-svalencar/letsHerp",
    "bug_reports": "https://github.com/joao-svalencar/letsHerp/issues",
    "repository": "https://cran.r-project.org/package=letsHerp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "letsHerp An Interface to the Reptile Database Provides tools to retrieve and summarize taxonomic information and synonymy data for reptile species using data scraped from The Reptile Database website (<https://reptile-database.reptarium.cz/>). Outputs include clean and structured data frames useful for ecological, evolutionary, and conservation research.  "
  },
  {
    "id": 15149,
    "package_name": "letsRept",
    "title": "An Interface to the Reptile Database",
    "description": "Provides tools to retrieve and summarize taxonomic information and synonymy data for reptile species using data scraped from The Reptile Database website (<https://reptile-database.reptarium.cz/>). Outputs include clean and structured data frames useful for ecological, evolutionary, and conservation research.",
    "version": "1.1.0",
    "maintainer": "Jo\u00e3o Paulo dos Santos Vieira-Alencar <joaopaulo.valencar@gmail.com>",
    "author": "Jo\u00e3o Paulo dos Santos Vieira-Alencar [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6894-6773>),\n  Christoph Liedtke [aut] (ORCID:\n    <https://orcid.org/0000-0002-6221-8043>)",
    "url": "https://joao-svalencar.github.io/letsRept/",
    "bug_reports": "https://github.com/joao-svalencar/letsRept/issues",
    "repository": "https://cran.r-project.org/package=letsRept",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "letsRept An Interface to the Reptile Database Provides tools to retrieve and summarize taxonomic information and synonymy data for reptile species using data scraped from The Reptile Database website (<https://reptile-database.reptarium.cz/>). Outputs include clean and structured data frames useful for ecological, evolutionary, and conservation research.  "
  },
  {
    "id": 15167,
    "package_name": "lgr",
    "title": "A Fully Featured Logging Framework",
    "description": "A flexible, feature-rich yet light-weight logging\n    framework based on 'R6' classes. It supports hierarchical loggers,\n    custom log levels, arbitrary data fields in log events, logging to\n    plaintext, 'JSON', (rotating) files, memory buffers. For extra\n    appenders that support logging to databases, email and push\n    notifications see the the package lgr.app.",
    "version": "0.5.0",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>)",
    "url": "https://s-fleck.github.io/lgr/",
    "bug_reports": "https://github.com/s-fleck/lgr/issues/",
    "repository": "https://cran.r-project.org/package=lgr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lgr A Fully Featured Logging Framework A flexible, feature-rich yet light-weight logging\n    framework based on 'R6' classes. It supports hierarchical loggers,\n    custom log levels, arbitrary data fields in log events, logging to\n    plaintext, 'JSON', (rotating) files, memory buffers. For extra\n    appenders that support logging to databases, email and push\n    notifications see the the package lgr.app.  "
  },
  {
    "id": 15168,
    "package_name": "lgrExtra",
    "title": "Extra Appenders for 'lgr'",
    "description": "Additional appenders for the logging package 'lgr' that\n    support logging to 'Elasticsearch', 'Dynatrace', 'AWSCloudWatchLog',\n    databases, 'syslog', email- and push notifications, and more.",
    "version": "0.2.2",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>),\n  Jimmy Briggs [ctb, rev] (ORCID:\n    <https://orcid.org/0000-0002-7489-8787>)",
    "url": "https://s-fleck.github.io/lgrExtra/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lgrExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lgrExtra Extra Appenders for 'lgr' Additional appenders for the logging package 'lgr' that\n    support logging to 'Elasticsearch', 'Dynatrace', 'AWSCloudWatchLog',\n    databases, 'syslog', email- and push notifications, and more.  "
  },
  {
    "id": 15188,
    "package_name": "lifeR",
    "title": "Identify Sites for Your Bird List",
    "description": "A suite of tools to use the 'eBird' database \n    (<https://ebird.org/home/>) and APIs to compare users' species lists to \n    recent observations and create a report of the top sites to visit to see \n    new species.",
    "version": "1.0.3",
    "maintainer": "Jeffrey Oliver <jcoliver@arizona.edu>",
    "author": "Jeffrey Oliver [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2160-1086>)",
    "url": "https://jcoliver.github.io/lifeR/,\nhttps://github.com/jcoliver/lifeR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lifeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lifeR Identify Sites for Your Bird List A suite of tools to use the 'eBird' database \n    (<https://ebird.org/home/>) and APIs to compare users' species lists to \n    recent observations and create a report of the top sites to visit to see \n    new species.  "
  },
  {
    "id": 15226,
    "package_name": "lingglosses",
    "title": "Interlinear Glossed Linguistic Examples and Abbreviation Lists\nGeneration",
    "description": "Helps to render interlinear glossed linguistic examples in html \n    'rmarkdown' documents and then semi-automatically compiles the list of\n    glosses at the end of the document. It also provides a database of linguistic\n    glosses.",
    "version": "0.0.11",
    "maintainer": "George Moroz <agricolamz@gmail.com>",
    "author": "George Moroz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1990-6083>)",
    "url": "https://CRAN.R-project.org/package=phonfieldwork,\nhttps://agricolamz.github.io/lingglosses/",
    "bug_reports": "https://github.com/agricolamz/lingglosses/issues",
    "repository": "https://cran.r-project.org/package=lingglosses",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lingglosses Interlinear Glossed Linguistic Examples and Abbreviation Lists\nGeneration Helps to render interlinear glossed linguistic examples in html \n    'rmarkdown' documents and then semi-automatically compiles the list of\n    glosses at the end of the document. It also provides a database of linguistic\n    glosses.  "
  },
  {
    "id": 15514,
    "package_name": "maddison",
    "title": "The Maddison Project Database",
    "description": "Contains the Maddison Project 2018 database, which provides \n    estimates of GDP per capita for all countries in the world between AD 1 and\n    2016. See <https://www.rug.nl/ggdc/historicaldevelopment/maddison/> for more information.",
    "version": "0.2",
    "maintainer": "Eric Persson <expersso5@gmail.com>",
    "author": "Eric Persson [aut, cre],\n  Francois Briatte [ctb]",
    "url": "https://www.rug.nl/ggdc/historicaldevelopment/maddison/\nhttps://github.com/expersso/maddison",
    "bug_reports": "https://github.com/expersso/maddison/issues",
    "repository": "https://cran.r-project.org/package=maddison",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maddison The Maddison Project Database Contains the Maddison Project 2018 database, which provides \n    estimates of GDP per capita for all countries in the world between AD 1 and\n    2016. See <https://www.rug.nl/ggdc/historicaldevelopment/maddison/> for more information.  "
  },
  {
    "id": 15575,
    "package_name": "mapSpain",
    "title": "Administrative Boundaries of Spain",
    "description": "Administrative Boundaries of Spain at several levels\n    (Autonomous Communities, Provinces, Municipalities) based on the\n    'GISCO' 'Eurostat' database <https://ec.europa.eu/eurostat/web/gisco>\n    and 'CartoBase SIANE' from 'Instituto Geografico Nacional'\n    <https://www.ign.es/>.  It also provides a 'leaflet' plugin and the\n    ability of downloading and processing static tiles.",
    "version": "0.10.0",
    "maintainer": "Diego Hernang\u00f3mez <diego.hernangomezherrero@gmail.com>",
    "author": "Diego Hernang\u00f3mez [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>)",
    "url": "https://ropenspain.github.io/mapSpain/,\nhttps://github.com/rOpenSpain/mapSpain",
    "bug_reports": "https://github.com/rOpenSpain/mapSpain/issues",
    "repository": "https://cran.r-project.org/package=mapSpain",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapSpain Administrative Boundaries of Spain Administrative Boundaries of Spain at several levels\n    (Autonomous Communities, Provinces, Municipalities) based on the\n    'GISCO' 'Eurostat' database <https://ec.europa.eu/eurostat/web/gisco>\n    and 'CartoBase SIANE' from 'Instituto Geografico Nacional'\n    <https://www.ign.es/>.  It also provides a 'leaflet' plugin and the\n    ability of downloading and processing static tiles.  "
  },
  {
    "id": 15583,
    "package_name": "mapdata",
    "title": "Extra Map Databases",
    "description": "Supplement to maps package, providing some larger and/or\n\thigher-resolution databases. NOTE: this is a legacy package. The world map is out-dated.",
    "version": "2.3.1",
    "maintainer": "Alex Deckmyn <alex.deckmyn@meteo.be>",
    "author": "Original S code by Richard A. Becker and Allan R. Wilks.\n\tR version by Ray Brownrigg <Ray.Brownrigg@ecs.vuw.ac.nz>.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mapdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mapdata Extra Map Databases Supplement to maps package, providing some larger and/or\n\thigher-resolution databases. NOTE: this is a legacy package. The world map is out-dated.  "
  },
  {
    "id": 15785,
    "package_name": "mddmaps",
    "title": "Download World Mammal Maps",
    "description": "Lightweight maps of mammals of the world. These maps are a \n    comprehensive collection of maps aligned with the Mammal Diversity Database\n    taxonomy of the American Society of Mammalogists. They are generated at low\n    resolution for easy access, consultation and manipulation in shapefile\n    format. The package connects to a binary backup hosted in the Digital Ocean\n    cloud service and allows individual or batch download of any mammal species\n    in the mdd taxonomy by providing the scientific species name. ",
    "version": "1.3.0",
    "maintainer": "Angel Robles <a.l.robles.fernandez@gmail.com>",
    "author": "Angel Robles [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4674-4270>)",
    "url": "<https://github.com/alrobles/mddmaps>,\n<https://alrobles.github.io/mddmaps/>\n<https://zenodo.org/records/10974868>,\nhttps://alrobles.github.io/mddmaps/",
    "bug_reports": "https://github.com/alrobles/mdd/issues",
    "repository": "https://cran.r-project.org/package=mddmaps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mddmaps Download World Mammal Maps Lightweight maps of mammals of the world. These maps are a \n    comprehensive collection of maps aligned with the Mammal Diversity Database\n    taxonomy of the American Society of Mammalogists. They are generated at low\n    resolution for easy access, consultation and manipulation in shapefile\n    format. The package connects to a binary backup hosted in the Digital Ocean\n    cloud service and allows individual or batch download of any mammal species\n    in the mdd taxonomy by providing the scientific species name.   "
  },
  {
    "id": 15798,
    "package_name": "mdsr",
    "title": "Complement to 'Modern Data Science with R'",
    "description": "A complement to all editions of *Modern Data\n    Science with R* \n     (ISBN: 978-0367191498, publisher URL: \n    <https://www.routledge.com/Modern-Data-Science-with-R/Baumer-Kaplan-Horton/p/book/9780367191498>).\n    This package contains data and code to complete exercises and \n    reproduce examples from the text. It also facilitates connections \n    to the SQL database server used in the book. All editions of the book are \n    supported by this package.",
    "version": "0.2.8",
    "maintainer": "Benjamin S. Baumer <ben.baumer@gmail.com>",
    "author": "Benjamin S. Baumer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3279-0516>),\n  Nicholas Horton [aut] (ORCID: <https://orcid.org/0000-0003-3332-4311>),\n  Daniel Kaplan [aut]",
    "url": "https://github.com/mdsr-book/mdsr",
    "bug_reports": "https://github.com/mdsr-book/mdsr/issues",
    "repository": "https://cran.r-project.org/package=mdsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mdsr Complement to 'Modern Data Science with R' A complement to all editions of *Modern Data\n    Science with R* \n     (ISBN: 978-0367191498, publisher URL: \n    <https://www.routledge.com/Modern-Data-Science-with-R/Baumer-Kaplan-Horton/p/book/9780367191498>).\n    This package contains data and code to complete exercises and \n    reproduce examples from the text. It also facilitates connections \n    to the SQL database server used in the book. All editions of the book are \n    supported by this package.  "
  },
  {
    "id": 15902,
    "package_name": "metagear",
    "title": "Comprehensive Research Synthesis Tools for Systematic Reviews\nand Meta-Analysis",
    "description": "Functionalities for facilitating systematic reviews, data\n    extractions, and meta-analyses. It includes a GUI (graphical user interface)\n    to help screen the abstracts and titles of bibliographic data; tools to assign\n    screening effort across multiple collaborators/reviewers and to assess inter-\n    reviewer reliability; tools to help automate the download and retrieval of\n    journal PDF articles from online databases; figure and image extractions \n    from PDFs; web scraping of citations; automated and manual data extraction \n    from scatter-plot and bar-plot images; PRISMA (Preferred Reporting Items for\n    Systematic Reviews and Meta-Analyses) flow diagrams; simple imputation tools\n    to fill gaps in incomplete or missing study parameters; generation of random\n    effects sizes for Hedges' d, log response ratio, odds ratio, and correlation\n    coefficients for Monte Carlo experiments; covariance equations for modelling\n    dependencies among multiple effect sizes (e.g., effect sizes with a common\n    control); and finally summaries that replicate analyses and outputs from \n    widely used but no longer updated meta-analysis software (i.e., metawin).\n\tFunding for this package was supported by National Science Foundation (NSF) \n\tgrants DBI-1262545 and DEB-1451031. CITE: Lajeunesse, M.J. (2016) \n\tFacilitating systematic reviews, data extraction and meta-analysis with the \n\tmetagear package for R. Methods in Ecology and Evolution 7, 323-330 \n\t<doi:10.1111/2041-210X.12472>.",
    "version": "0.7",
    "maintainer": "Marc J. Lajeunesse <lajeunesse@usf.edu>",
    "author": "Marc J. Lajeunesse [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9678-2080>)",
    "url": "http://lajeunesse.myweb.usf.edu/ https://github.com/mjlajeunesse/\nhttps://www.youtube.com/c/LajeunesseLab/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metagear",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metagear Comprehensive Research Synthesis Tools for Systematic Reviews\nand Meta-Analysis Functionalities for facilitating systematic reviews, data\n    extractions, and meta-analyses. It includes a GUI (graphical user interface)\n    to help screen the abstracts and titles of bibliographic data; tools to assign\n    screening effort across multiple collaborators/reviewers and to assess inter-\n    reviewer reliability; tools to help automate the download and retrieval of\n    journal PDF articles from online databases; figure and image extractions \n    from PDFs; web scraping of citations; automated and manual data extraction \n    from scatter-plot and bar-plot images; PRISMA (Preferred Reporting Items for\n    Systematic Reviews and Meta-Analyses) flow diagrams; simple imputation tools\n    to fill gaps in incomplete or missing study parameters; generation of random\n    effects sizes for Hedges' d, log response ratio, odds ratio, and correlation\n    coefficients for Monte Carlo experiments; covariance equations for modelling\n    dependencies among multiple effect sizes (e.g., effect sizes with a common\n    control); and finally summaries that replicate analyses and outputs from \n    widely used but no longer updated meta-analysis software (i.e., metawin).\n\tFunding for this package was supported by National Science Foundation (NSF) \n\tgrants DBI-1262545 and DEB-1451031. CITE: Lajeunesse, M.J. (2016) \n\tFacilitating systematic reviews, data extraction and meta-analysis with the \n\tmetagear package for R. Methods in Ecology and Evolution 7, 323-330 \n\t<doi:10.1111/2041-210X.12472>.  "
  },
  {
    "id": 15961,
    "package_name": "mfdb",
    "title": "MareFrame DB Querying Library",
    "description": "Creates and manages a PostgreSQL database suitable for storing fisheries data\n             and aggregating ready for use within a Gadget <https://gadget-framework.github.io/gadget2/> model.\n             See <https://mareframe.github.io/mfdb/> for more information.",
    "version": "7.3-1",
    "maintainer": "Jamie Lentin <lentinj@shuttlethread.com>",
    "author": "Jamie Lentin [aut, cre, cph],\n  Bjarki Thor Elvarsson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mfdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mfdb MareFrame DB Querying Library Creates and manages a PostgreSQL database suitable for storing fisheries data\n             and aggregating ready for use within a Gadget <https://gadget-framework.github.io/gadget2/> model.\n             See <https://mareframe.github.io/mfdb/> for more information.  "
  },
  {
    "id": 16025,
    "package_name": "micronutr",
    "title": "Determining Vitamin and Mineral Status of Populations",
    "description": "Vitamin and mineral deficiencies continue to be a significant \n    public health problem. This is particularly critical in developing countries\n    where deficiencies to vitamin A, iron, iodine, and other micronutrients\n    lead to adverse health consequences. Cross-sectional surveys are helpful in\n    answering questions related to the magnitude and distribution of\n    deficiencies of selected vitamins and minerals. This package provides tools\n    for calculating and determining select vitamin and mineral deficiencies\n    based on World Health Organization (WHO) guidelines found at\n    <https://www.who.int/teams/nutrition-and-food-safety/databases/vitamin-and-mineral-nutrition-information-system>.",
    "version": "0.1.1",
    "maintainer": "Ernest Guevarra <ernest@guevarra.io>",
    "author": "Ernest Guevarra [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4887-4415>),\n  Nicholus Tint Zaw [aut, cph]",
    "url": "https://nutriverse.io/micronutr/,\nhttps://github.com/nutriverse/micronutr",
    "bug_reports": "https://github.com/nutriverse/micronutr/issues",
    "repository": "https://cran.r-project.org/package=micronutr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "micronutr Determining Vitamin and Mineral Status of Populations Vitamin and mineral deficiencies continue to be a significant \n    public health problem. This is particularly critical in developing countries\n    where deficiencies to vitamin A, iron, iodine, and other micronutrients\n    lead to adverse health consequences. Cross-sectional surveys are helpful in\n    answering questions related to the magnitude and distribution of\n    deficiencies of selected vitamins and minerals. This package provides tools\n    for calculating and determining select vitamin and mineral deficiencies\n    based on World Health Organization (WHO) guidelines found at\n    <https://www.who.int/teams/nutrition-and-food-safety/databases/vitamin-and-mineral-nutrition-information-system>.  "
  },
  {
    "id": 16039,
    "package_name": "midfieldr",
    "title": "Tools and Methods for Working with MIDFIELD Data in 'R'",
    "description": "Provides tools and demonstrates methods for working with individual \n  undergraduate student-level records (registrar's data) in 'R'. Tools include \n  filters for program codes, data sufficiency, and timely completion. Methods \n  include gathering blocs of records, computing quantitative metrics such as \n  graduation rate, and creating charts to visualize comparisons. 'midfieldr' \n  interacts with practice data provided in 'midfielddata', an R data package \n  available at <https://midfieldr.github.io/midfielddata/>. \n  'midfieldr' also interacts with the full MIDFIELD database for users who have \n  access. This work is supported by the US National Science Foundation through \n  grant numbers 1545667 and 2142087.",
    "version": "1.0.2",
    "maintainer": "Richard Layton <graphdoctor@gmail.com>",
    "author": "Richard Layton [cre, aut, cph],\n  Russell Long [aut, cph, dtm],\n  Matthew Ohland [aut, cph],\n  Marisa Orr [aut, cph],\n  Susan Lord [aut, cph]",
    "url": "https://midfieldr.github.io/midfieldr/",
    "bug_reports": "https://github.com/MIDFIELDR/midfieldr/issues",
    "repository": "https://cran.r-project.org/package=midfieldr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "midfieldr Tools and Methods for Working with MIDFIELD Data in 'R' Provides tools and demonstrates methods for working with individual \n  undergraduate student-level records (registrar's data) in 'R'. Tools include \n  filters for program codes, data sufficiency, and timely completion. Methods \n  include gathering blocs of records, computing quantitative metrics such as \n  graduation rate, and creating charts to visualize comparisons. 'midfieldr' \n  interacts with practice data provided in 'midfielddata', an R data package \n  available at <https://midfieldr.github.io/midfielddata/>. \n  'midfieldr' also interacts with the full MIDFIELD database for users who have \n  access. This work is supported by the US National Science Foundation through \n  grant numbers 1545667 and 2142087.  "
  },
  {
    "id": 16083,
    "package_name": "minimalistGODB",
    "title": "Build a Minimalist Gene Ontology (GO) Database (GODB)",
    "description": "Normally building a GODB is fairly complicated, involving downloading \n multiple database files and using these to build e.g. a 'mySQL' database. \n Accessing this database is also complicated, involving an intimate knowledge\n of the database in order to construct reliable queries.\n Here we have a more modest goal, generating GOGOA3, which is a stripped down\n version of the GODB that \n was originally restricted to human genes as designated by the HUGO Gene Nomenclature\n Committee (HGNC) (see <https://geneontology.org/>). I have now added about two dozen\n additional species, namely all species represented on the Gene Ontology\n download page <https://current.geneontology.org/products/pages/downloads.html>.\n This covers most of the model organisms that are commonly used in bio-medical\n and basic research (assuming that anyone still has a grant to do such research).\n This can be built in a matter of seconds from 2 easily\n downloaded files (see <https://current.geneontology.org/products/pages/downloads.html>\n and <https://geneontology.org/docs/download-ontology/>), and it can be queried by e.g.\n w<-which(GOGOA3[,\"HGNC\"] %in% hgncList) where GOGOA3\n is a matrix representing the minimalist GODB and hgncList is a list of\n gene identifiers. This database will be used in my upcoming package 'GoMiner'\n which is based on my previous publication (see Zeeberg, B.R., Feng, W., Wang, G. et al. (2003)<doi:10.1186/gb-2003-4-4-r28>).\n Relevant .RData files are available from GitHub (<https://github.com/barryzee/GO/tree/main/databases>).",
    "version": "1.1.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minimalistGODB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minimalistGODB Build a Minimalist Gene Ontology (GO) Database (GODB) Normally building a GODB is fairly complicated, involving downloading \n multiple database files and using these to build e.g. a 'mySQL' database. \n Accessing this database is also complicated, involving an intimate knowledge\n of the database in order to construct reliable queries.\n Here we have a more modest goal, generating GOGOA3, which is a stripped down\n version of the GODB that \n was originally restricted to human genes as designated by the HUGO Gene Nomenclature\n Committee (HGNC) (see <https://geneontology.org/>). I have now added about two dozen\n additional species, namely all species represented on the Gene Ontology\n download page <https://current.geneontology.org/products/pages/downloads.html>.\n This covers most of the model organisms that are commonly used in bio-medical\n and basic research (assuming that anyone still has a grant to do such research).\n This can be built in a matter of seconds from 2 easily\n downloaded files (see <https://current.geneontology.org/products/pages/downloads.html>\n and <https://geneontology.org/docs/download-ontology/>), and it can be queried by e.g.\n w<-which(GOGOA3[,\"HGNC\"] %in% hgncList) where GOGOA3\n is a matrix representing the minimalist GODB and hgncList is a list of\n gene identifiers. This database will be used in my upcoming package 'GoMiner'\n which is based on my previous publication (see Zeeberg, B.R., Feng, W., Wang, G. et al. (2003)<doi:10.1186/gb-2003-4-4-r28>).\n Relevant .RData files are available from GitHub (<https://github.com/barryzee/GO/tree/main/databases>).  "
  },
  {
    "id": 16090,
    "package_name": "minired",
    "title": "R Interface to 'Redatam' Library",
    "description": "This package is deprecated. Please use 'redatamx' instead.\n  Provides an API to work with 'Redatam' (see <https://redatam.org>) \n  databases in both formats: 'RXDB' (new format) and 'DICX' (old format) and running \n  'Redatam' programs written in 'SPC' language. It's a wrapper around 'Redatam' \n  core and provides functions to open/close a database (redatam_open()/redatam_close()), \n  list entities and variables from the database (redatam_entities(), redatam_variables()) \n  and execute a 'SPC' program and gets the results as data frames \n  (redatam_query(), redatam_run()).",
    "version": "1.0.1",
    "maintainer": "Jaime Salvador <jaime.salvador@ideasybits.com>",
    "author": "Jaime Salvador [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3564-8929>),\n  CELADE [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=minired",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minired R Interface to 'Redatam' Library This package is deprecated. Please use 'redatamx' instead.\n  Provides an API to work with 'Redatam' (see <https://redatam.org>) \n  databases in both formats: 'RXDB' (new format) and 'DICX' (old format) and running \n  'Redatam' programs written in 'SPC' language. It's a wrapper around 'Redatam' \n  core and provides functions to open/close a database (redatam_open()/redatam_close()), \n  list entities and variables from the database (redatam_entities(), redatam_variables()) \n  and execute a 'SPC' program and gets the results as data frames \n  (redatam_query(), redatam_run()).  "
  },
  {
    "id": 16115,
    "package_name": "mispitools",
    "title": "Missing Person Identification Tools",
    "description": "An open source software package written in R statistical language. It consist in a set of decision making tools to conduct missing person searches.  Particularly, it allows computing optimal LR threshold for declaring potential matches in DNA-based database search. More recently 'mispitools' incorporates preliminary investigation data based LRs. Statistical weight of different traces of evidence such as biological sex, age and hair color are presented. For citing mispitools please use the following references: Marsico and Caridi, 2023 <doi:10.1016/j.fsigen.2023.102891> and  Marsico, Vigeland et al. 2021 <doi:10.1016/j.fsigen.2021.102519>.",
    "version": "1.2.0",
    "maintainer": "Franco Marsico <franco.lmarsico@gmail.com>",
    "author": "Franco Marsico [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0740-5516>)",
    "url": "https://github.com/MarsicoFL/mispitools",
    "bug_reports": "https://github.com/MarsicoFL/mispitools/issues",
    "repository": "https://cran.r-project.org/package=mispitools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mispitools Missing Person Identification Tools An open source software package written in R statistical language. It consist in a set of decision making tools to conduct missing person searches.  Particularly, it allows computing optimal LR threshold for declaring potential matches in DNA-based database search. More recently 'mispitools' incorporates preliminary investigation data based LRs. Statistical weight of different traces of evidence such as biological sex, age and hair color are presented. For citing mispitools please use the following references: Marsico and Caridi, 2023 <doi:10.1016/j.fsigen.2023.102891> and  Marsico, Vigeland et al. 2021 <doi:10.1016/j.fsigen.2021.102519>.  "
  },
  {
    "id": 16219,
    "package_name": "mlr3",
    "title": "Machine Learning in R - Next Generation",
    "description": "Efficient, object-oriented programming on the\n    building blocks of machine learning. Provides 'R6' objects for tasks,\n    learners, resamplings, and measures. The package is geared towards\n    scalability and larger datasets by supporting parallelization and\n    out-of-memory data-backends like databases. While 'mlr3' focuses on\n    the core computational operations, add-on packages provide additional\n    functionality.",
    "version": "1.3.0",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>),\n  Bernd Bischl [aut] (ORCID: <https://orcid.org/0000-0001-6002-6980>),\n  Jakob Richter [aut] (ORCID: <https://orcid.org/0000-0003-4481-5554>),\n  Patrick Schratz [aut] (ORCID: <https://orcid.org/0000-0003-0748-6624>),\n  Giuseppe Casalicchio [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5324-5966>),\n  Stefan Coors [ctb] (ORCID: <https://orcid.org/0000-0002-7465-2146>),\n  Quay Au [ctb] (ORCID: <https://orcid.org/0000-0002-5252-8902>),\n  Martin Binder [aut],\n  Florian Pfisterer [aut] (ORCID:\n    <https://orcid.org/0000-0001-8867-762X>),\n  Raphael Sonabend [aut] (ORCID: <https://orcid.org/0000-0001-9225-4654>),\n  Lennart Schneider [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4152-5308>),\n  Marc Becker [cre, aut] (ORCID: <https://orcid.org/0000-0002-8115-0400>),\n  Sebastian Fischer [aut] (ORCID:\n    <https://orcid.org/0000-0002-9609-3197>),\n  Lona Koers [ctb],\n  John Zobolas [ctb] (ORCID: <https://orcid.org/0000-0002-3609-8674>),\n  Maximilian M\u00fccke [ctb] (ORCID: <https://orcid.org/0009-0000-9432-9795>)",
    "url": "https://mlr3.mlr-org.com, https://github.com/mlr-org/mlr3",
    "bug_reports": "https://github.com/mlr-org/mlr3/issues",
    "repository": "https://cran.r-project.org/package=mlr3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3 Machine Learning in R - Next Generation Efficient, object-oriented programming on the\n    building blocks of machine learning. Provides 'R6' objects for tasks,\n    learners, resamplings, and measures. The package is geared towards\n    scalability and larger datasets by supporting parallelization and\n    out-of-memory data-backends like databases. While 'mlr3' focuses on\n    the core computational operations, add-on packages provide additional\n    functionality.  "
  },
  {
    "id": 16224,
    "package_name": "mlr3db",
    "title": "Data Base Backend for 'mlr3'",
    "description": "Extends the 'mlr3' package with a backend to transparently\n    work with databases such as 'SQLite', 'DuckDB', 'MySQL', 'MariaDB', or\n    'PostgreSQL'. The package provides three additional backends:\n    'DataBackendDplyr' relies on the abstraction of package 'dbplyr' to\n    interact with most DBMS. 'DataBackendDuckDB' operates on 'DuckDB' data\n    bases and also on Apache Parquet files. 'DataBackendPolars' operates on\n    'Polars' data frames.",
    "version": "0.7.0",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Michel Lang [aut] (ORCID: <https://orcid.org/0000-0001-9754-0393>),\n  Lona Koers [aut],\n  Marc Becker [cre, aut] (ORCID: <https://orcid.org/0000-0002-8115-0400>)",
    "url": "https://mlr3db.mlr-org.com, https://github.com/mlr-org/mlr3db",
    "bug_reports": "https://github.com/mlr-org/mlr3db/issues",
    "repository": "https://cran.r-project.org/package=mlr3db",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlr3db Data Base Backend for 'mlr3' Extends the 'mlr3' package with a backend to transparently\n    work with databases such as 'SQLite', 'DuckDB', 'MySQL', 'MariaDB', or\n    'PostgreSQL'. The package provides three additional backends:\n    'DataBackendDplyr' relies on the abstraction of package 'dbplyr' to\n    interact with most DBMS. 'DataBackendDuckDB' operates on 'DuckDB' data\n    bases and also on Apache Parquet files. 'DataBackendPolars' operates on\n    'Polars' data frames.  "
  },
  {
    "id": 16254,
    "package_name": "mlstrOpalr",
    "title": "Support Compatibility Between 'Maelstrom' R Packages and 'Opal'\nEnvironment",
    "description": "Functions to support compatibility between 'Maelstrom' R packages \n     and 'Opal' environment. 'Opal' is the 'OBiBa' core database application for \n     biobanks. It is used to build data repositories that integrates data \n     collected from multiple sources. 'Opal Maelstrom' is a specific \n     implementation of this software. This 'Opal' client is specifically \n     designed to interact with 'Opal Maelstrom' distributions to perform \n     operations on the R server side. The user must have adequate credentials.\n     Please see <https://opaldoc.obiba.org/> for complete documentation.",
    "version": "1.0.3",
    "maintainer": "Guillaume Fabre <guijoseph.fabre@gmail.com>",
    "author": "Guillaume Fabre [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0124-9970>),\n  Maelstrom-research group [fnd, cph],\n  OBiBa group [ctb]",
    "url": "https://github.com/maelstrom-research/mlstrOpalr",
    "bug_reports": "https://github.com/maelstrom-research/mlstrOpalr/issues",
    "repository": "https://cran.r-project.org/package=mlstrOpalr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlstrOpalr Support Compatibility Between 'Maelstrom' R Packages and 'Opal'\nEnvironment Functions to support compatibility between 'Maelstrom' R packages \n     and 'Opal' environment. 'Opal' is the 'OBiBa' core database application for \n     biobanks. It is used to build data repositories that integrates data \n     collected from multiple sources. 'Opal Maelstrom' is a specific \n     implementation of this software. This 'Opal' client is specifically \n     designed to interact with 'Opal Maelstrom' distributions to perform \n     operations on the R server side. The user must have adequate credentials.\n     Please see <https://opaldoc.obiba.org/> for complete documentation.  "
  },
  {
    "id": 16278,
    "package_name": "mmints",
    "title": "Workflows for Building Web Applications",
    "description": "Sharing statistical methods or simulation frameworks through 'shiny'\n    applications often requires workflows for handling data. To help save and\n    display simulation results, the postgresUI() and postgresServer() functions\n    in 'mmints' help with persistent data storage using a 'PostgreSQL' database.\n    The 'mmints' package also offers data upload functionality through the\n    csvUploadUI() and csvUploadServer() functions which allow users to upload\n    data, view variables and their types, and edit variable types before fitting\n    statistical models within the 'shiny' application. These tools aim to\n    enhance efficiency and user interaction in 'shiny' based statistical and\n    simulation applications.",
    "version": "0.2.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/mmints",
    "bug_reports": "https://github.com/mightymetrika/mmints/issues",
    "repository": "https://cran.r-project.org/package=mmints",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmints Workflows for Building Web Applications Sharing statistical methods or simulation frameworks through 'shiny'\n    applications often requires workflows for handling data. To help save and\n    display simulation results, the postgresUI() and postgresServer() functions\n    in 'mmints' help with persistent data storage using a 'PostgreSQL' database.\n    The 'mmints' package also offers data upload functionality through the\n    csvUploadUI() and csvUploadServer() functions which allow users to upload\n    data, view variables and their types, and edit variable types before fitting\n    statistical models within the 'shiny' application. These tools aim to\n    enhance efficiency and user interaction in 'shiny' based statistical and\n    simulation applications.  "
  },
  {
    "id": 16321,
    "package_name": "modelc",
    "title": "A Linear Model to 'SQL' Compiler",
    "description": "This is a cross-platform linear model to 'SQL' compiler. It generates 'SQL' from linear and generalized linear models. Its interface consists of a single function, modelc(), which takes the output of lm() or glm() functions (or any object which has the same signature) and outputs a 'SQL' character vector representing the predictions on the scale of the response variable as described in Dunn & Smith (2018) <doi:10.1007/978-1-4419-0118-7> and originating in Nelder & Wedderburn (1972) <doi:10.2307/2344614>. The resultant 'SQL' can be included in a 'SELECT' statement and returns output similar to that of the glm.predict() or lm.predict() predictions, assuming numeric types are represented in the database using sufficient precision. Currently log and identity link functions are supported.",
    "version": "1.0.0.0",
    "maintainer": "Hugo Saavedra <analytics+hugo@sparkfish.com>",
    "author": "Sparkfish Analytics [cph],\n  Hugo Saavedra [aut, cre]",
    "url": "https://github.com/sparkfish/modelc",
    "bug_reports": "https://github.com/sparkfish/modelc/issues",
    "repository": "https://cran.r-project.org/package=modelc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modelc A Linear Model to 'SQL' Compiler This is a cross-platform linear model to 'SQL' compiler. It generates 'SQL' from linear and generalized linear models. Its interface consists of a single function, modelc(), which takes the output of lm() or glm() functions (or any object which has the same signature) and outputs a 'SQL' character vector representing the predictions on the scale of the response variable as described in Dunn & Smith (2018) <doi:10.1007/978-1-4419-0118-7> and originating in Nelder & Wedderburn (1972) <doi:10.2307/2344614>. The resultant 'SQL' can be included in a 'SELECT' statement and returns output similar to that of the glm.predict() or lm.predict() predictions, assuming numeric types are represented in the database using sufficient precision. Currently log and identity link functions are supported.  "
  },
  {
    "id": 16359,
    "package_name": "mongolite",
    "title": "Fast and Simple 'MongoDB' Client for R",
    "description": "High-performance MongoDB client based on 'mongo-c-driver' and 'jsonlite'.\n    Includes support for aggregation, indexing, map-reduce, streaming, encryption,\n    enterprise authentication, and GridFS. The online user manual provides an overview \n    of the available methods in the package: <https://jeroen.github.io/mongolite/>.",
    "version": "4.0.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  MongoDB, Inc [cph] (Bundled mongo-c-driver, see AUTHORS file)",
    "url": "https://jeroen.r-universe.dev/mongolite",
    "bug_reports": "https://github.com/jeroen/mongolite/issues",
    "repository": "https://cran.r-project.org/package=mongolite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mongolite Fast and Simple 'MongoDB' Client for R High-performance MongoDB client based on 'mongo-c-driver' and 'jsonlite'.\n    Includes support for aggregation, indexing, map-reduce, streaming, encryption,\n    enterprise authentication, and GridFS. The online user manual provides an overview \n    of the available methods in the package: <https://jeroen.github.io/mongolite/>.  "
  },
  {
    "id": 16360,
    "package_name": "mongopipe",
    "title": "Write MongoDB Queries with R",
    "description": "Translate R code into MongoDB aggregation pipelines.",
    "version": "0.1.2",
    "maintainer": "Oliver Haag <oliver_haag@e.mail.de>",
    "author": "Oliver Haag [aut, cre]",
    "url": "https://rpkgs.gitlab.io/mongopipe,\nhttps://gitlab.com/rpkgs/mongopipe",
    "bug_reports": "https://gitlab.com/rpkgs/mongopipe/-/issues",
    "repository": "https://cran.r-project.org/package=mongopipe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mongopipe Write MongoDB Queries with R Translate R code into MongoDB aggregation pipelines.  "
  },
  {
    "id": 16362,
    "package_name": "monitoR",
    "title": "Acoustic Template Detection in R",
    "description": "Acoustic template detection and monitoring database interface. Create, modify, save, and use templates for detection of animal vocalizations. View, verify, and extract results. Upload a MySQL schema to a existing instance, manage survey metadata, write and read templates and detections locally or to the database. ",
    "version": "1.2",
    "maintainer": "Sasha D. Hafner <sasha.hafner@bce.au.dk>",
    "author": "Sasha D. Hafner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0955-0327>),\n  Jon Katz [aut],\n  Jerome Sueur [aut] (seewave package author (code from Fourier transform\n    used in monitoR)),\n  Thierry Aubin [aut] (seewave package author (code from Fourier\n    transform used in monitoR)),\n  Caroline Simonis [aut] (seewave package author (code from Fourier\n    transform used in monitoR)),\n  Uwe Ligges [aut] (tuneR package author (code from readMP3() used in\n    monitoR)),\n  Therese Donovan [ctb] (creative direction and database design support)",
    "url": "https://github.com/jonkatz2/monitor",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=monitoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monitoR Acoustic Template Detection in R Acoustic template detection and monitoring database interface. Create, modify, save, and use templates for detection of animal vocalizations. View, verify, and extract results. Upload a MySQL schema to a existing instance, manage survey metadata, write and read templates and detections locally or to the database.   "
  },
  {
    "id": 16377,
    "package_name": "moodleR",
    "title": "Helper Functions to Work with 'Moodle' Data",
    "description": "A collection of functions to connect to a 'Moodle' database, cache relevant tables locally and generate learning analytics. \n    'Moodle' is an open source Learning Management System (LMS) developed by MoodleHQ. For more information about Moodle, visit <https://moodle.org>.",
    "version": "1.0.1",
    "maintainer": "Aleksander Dietrichson <dietrichson@gmail.com>",
    "author": "Aleksander Dietrichson [aut, cre],\n  Chi Square Laboratories [cph],\n  Darko Miletic [ctb],\n  Pablo Pagnone [ctb],\n  Alex Ondrus [ctb]",
    "url": "https://github.com/chi2labs/moodleR,\nhttps://chi2labs.github.io/moodleR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=moodleR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "moodleR Helper Functions to Work with 'Moodle' Data A collection of functions to connect to a 'Moodle' database, cache relevant tables locally and generate learning analytics. \n    'Moodle' is an open source Learning Management System (LMS) developed by MoodleHQ. For more information about Moodle, visit <https://moodle.org>.  "
  },
  {
    "id": 16406,
    "package_name": "motherduck",
    "title": "Utilities for Managing a 'Motherduck' Database",
    "description": "Provides helper functions, metadata utilities, and workflows for \n    administering and managing databases on the 'Motherduck' cloud platform. \n    Some features require a 'Motherduck' account (<https://motherduck.com/>).",
    "version": "0.2.0",
    "maintainer": "Alejandro Hagan <alejandro.hagan@outlook.com>",
    "author": "Alejandro Hagan [aut, cre]",
    "url": "https://usrbinr.github.io/motherduck/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=motherduck",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "motherduck Utilities for Managing a 'Motherduck' Database Provides helper functions, metadata utilities, and workflows for \n    administering and managing databases on the 'Motherduck' cloud platform. \n    Some features require a 'Motherduck' account (<https://motherduck.com/>).  "
  },
  {
    "id": 16432,
    "package_name": "mpathsenser",
    "title": "Process and Analyse Data from m-Path Sense",
    "description": "Overcomes one of the major challenges in mobile (passive)\n    sensing, namely being able to pre-process the raw data that comes from\n    a mobile sensing app, specifically 'm-Path Sense' <https://m-path.io>.\n    The main task of 'mpathsenser' is therefore to read 'm-Path Sense'\n    JSON files into a database and provide several convenience functions\n    to aid in data processing.",
    "version": "1.2.4",
    "maintainer": "Koen Niemeijer <koen.niemeijer@kuleuven.be>",
    "author": "Koen Niemeijer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0816-534X>),\n  Kristof Meers [ctb] (ORCID: <https://orcid.org/0000-0002-9610-7712>),\n  KU Leuven [cph, fnd]",
    "url": "https://github.com/koenniem/mpathsenser,\nhttps://ppw-okpiv.pages.gitlab.kuleuven.be/researchers/u0134047/mpathsenser",
    "bug_reports": "https://github.com/koenniem/mpathsenser/issues",
    "repository": "https://cran.r-project.org/package=mpathsenser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mpathsenser Process and Analyse Data from m-Path Sense Overcomes one of the major challenges in mobile (passive)\n    sensing, namely being able to pre-process the raw data that comes from\n    a mobile sensing app, specifically 'm-Path Sense' <https://m-path.io>.\n    The main task of 'mpathsenser' is therefore to read 'm-Path Sense'\n    JSON files into a database and provide several convenience functions\n    to aid in data processing.  "
  },
  {
    "id": 16487,
    "package_name": "msig",
    "title": "An R Package for Exploring Molecular Signatures Database",
    "description": "The Molecular Signatures Database ('MSigDB') is one of the most widely \n    used and comprehensive databases of gene sets for performing gene set \n    enrichment analysis <doi:10.1016/j.cels.2015.12.004>. The 'msig' package provides \n    you with powerful, easy-to-use and flexible query functions for the 'MsigDB' \n    database.\n        There are 2 query modes in the 'msig' package: online query and local query. \n    Both queries contain 2 steps: gene set name and gene.\n        The online search is divided into 2 modes: registered search and \n    non-registered browse. For registered search, email that you registered should \n    be provided.\n        Local queries can be made from local database, which can be updated by msig_update() function.",
    "version": "1.0",
    "maintainer": "Jing Zhang <zj391120@163.com>",
    "author": "Jing Zhang [aut, cre],\n  Zhi Jin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=msig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "msig An R Package for Exploring Molecular Signatures Database The Molecular Signatures Database ('MSigDB') is one of the most widely \n    used and comprehensive databases of gene sets for performing gene set \n    enrichment analysis <doi:10.1016/j.cels.2015.12.004>. The 'msig' package provides \n    you with powerful, easy-to-use and flexible query functions for the 'MsigDB' \n    database.\n        There are 2 query modes in the 'msig' package: online query and local query. \n    Both queries contain 2 steps: gene set name and gene.\n        The online search is divided into 2 modes: registered search and \n    non-registered browse. For registered search, email that you registered should \n    be provided.\n        Local queries can be made from local database, which can be updated by msig_update() function.  "
  },
  {
    "id": 16488,
    "package_name": "msigdbr",
    "title": "MSigDB Gene Sets for Multiple Organisms in a Tidy Data Format",
    "description": "Provides the 'Molecular Signatures Database' (MSigDB) gene\n    sets typically used with the 'Gene Set Enrichment Analysis' (GSEA)\n    software (Subramanian et al. 2005 <doi:10.1073/pnas.0506580102>,\n    Liberzon et al. 2015 <doi:10.1016/j.cels.2015.12.004>, Castanza et al.\n    2023 <doi:10.1038/s41592-023-02014-7>) as an R data frame. The package\n    includes the human genes as listed in MSigDB as well as the\n    corresponding symbols and IDs for frequently studied model organisms\n    such as mouse, rat, pig, fly, and yeast.",
    "version": "25.1.1",
    "maintainer": "Igor Dolgalev <igor.dolgalev@nyumc.org>",
    "author": "Igor Dolgalev [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4451-126X>)",
    "url": "https://igordot.github.io/msigdbr/",
    "bug_reports": "https://github.com/igordot/msigdbr/issues",
    "repository": "https://cran.r-project.org/package=msigdbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "msigdbr MSigDB Gene Sets for Multiple Organisms in a Tidy Data Format Provides the 'Molecular Signatures Database' (MSigDB) gene\n    sets typically used with the 'Gene Set Enrichment Analysis' (GSEA)\n    software (Subramanian et al. 2005 <doi:10.1073/pnas.0506580102>,\n    Liberzon et al. 2015 <doi:10.1016/j.cels.2015.12.004>, Castanza et al.\n    2023 <doi:10.1038/s41592-023-02014-7>) as an R data frame. The package\n    includes the human genes as listed in MSigDB as well as the\n    corresponding symbols and IDs for frequently studied model organisms\n    such as mouse, rat, pig, fly, and yeast.  "
  },
  {
    "id": 16495,
    "package_name": "mssearchr",
    "title": "Library Search Against Electron Ionization Mass Spectral\nDatabases",
    "description": "Perform library searches against electron ionization mass spectral\n    databases using either the API provided by 'MS Search' software\n    (<https://chemdata.nist.gov/dokuwiki/doku.php?id=chemdata:nistlibs>) or\n    custom implementations of the Identity and Similarity algorithms.",
    "version": "0.2.0",
    "maintainer": "Andrey Samokhin <andrey.s.samokhin@gmail.com>",
    "author": "Andrey Samokhin [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0223-6087>)",
    "url": "https://mass-spec.ru/projects/gcmsdata/mssearchr/eng/",
    "bug_reports": "https://github.com/AndreySamokhin/mssearchr/issues",
    "repository": "https://cran.r-project.org/package=mssearchr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mssearchr Library Search Against Electron Ionization Mass Spectral\nDatabases Perform library searches against electron ionization mass spectral\n    databases using either the API provided by 'MS Search' software\n    (<https://chemdata.nist.gov/dokuwiki/doku.php?id=chemdata:nistlibs>) or\n    custom implementations of the Identity and Similarity algorithms.  "
  },
  {
    "id": 16524,
    "package_name": "mulea",
    "title": "Enrichment Analysis Using Multiple Ontologies and False\nDiscovery Rate",
    "description": "Background - Traditional gene set enrichment analyses are \n    typically limited to a few ontologies and do not account for the \n    interdependence of gene sets or terms, resulting in overcorrected p-values. \n    To address these challenges, we introduce mulea, an R package offering \n    comprehensive overrepresentation and functional enrichment analysis. \n    Results - mulea employs a progressive empirical false discovery rate \n    (eFDR) method, specifically designed for interconnected biological data, \n    to accurately identify significant terms within diverse ontologies. mulea \n    expands beyond traditional tools by incorporating a wide range of \n    ontologies, encompassing Gene Ontology, pathways, regulatory elements, \n    genomic locations, and protein domains. This flexibility enables \n    researchers to tailor enrichment analysis to their specific questions, \n    such as identifying enriched transcriptional regulators in gene expression \n    data or overrepresented protein domains in protein sets. To facilitate \n    seamless analysis, mulea provides gene sets (in standardised GMT format) \n    for 27 model organisms, covering 22 ontology types from 16 databases and \n    various identifiers resulting in almost 900 files. Additionally, the \n    muleaData ExperimentData Bioconductor package simplifies access to these \n    pre-defined ontologies. Finally, mulea's architecture allows for easy \n    integration of user-defined ontologies, or GMT files from external \n    sources (e.g., MSigDB or Enrichr), expanding its applicability across \n    diverse research areas. Conclusions - mulea is distributed as a CRAN R \n    package. It offers researchers a powerful and flexible toolkit for \n    functional enrichment analysis, addressing limitations of traditional \n    tools with its progressive eFDR and by supporting a variety of ontologies. \n    Overall, mulea fosters the exploration of diverse biological questions \n    across various model organisms.",
    "version": "1.1.1",
    "maintainer": "Tamas Stirling <stirling.tamas@gmail.com>",
    "author": "Cezary Turek [aut] (ORCID: <https://orcid.org/0000-0002-1445-5378>),\n  Marton Olbei [aut] (ORCID: <https://orcid.org/0000-0002-4903-6237>),\n  Tamas Stirling [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8964-6443>),\n  Gergely Fekete [aut] (ORCID: <https://orcid.org/0000-0001-9939-4860>),\n  Ervin Tasnadi [aut] (ORCID: <https://orcid.org/0000-0002-4713-5397>),\n  Leila Gul [aut],\n  Balazs Bohar [aut] (ORCID: <https://orcid.org/0000-0002-3033-5448>),\n  Balazs Papp [aut] (ORCID: <https://orcid.org/0000-0003-3093-8852>),\n  Wiktor Jurkowski [aut] (ORCID: <https://orcid.org/0000-0002-7820-1991>),\n  Eszter Ari [aut, cph] (ORCID: <https://orcid.org/0000-0001-7774-1067>)",
    "url": "https://github.com/ELTEbioinformatics/mulea",
    "bug_reports": "https://github.com/ELTEbioinformatics/mulea/issues",
    "repository": "https://cran.r-project.org/package=mulea",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mulea Enrichment Analysis Using Multiple Ontologies and False\nDiscovery Rate Background - Traditional gene set enrichment analyses are \n    typically limited to a few ontologies and do not account for the \n    interdependence of gene sets or terms, resulting in overcorrected p-values. \n    To address these challenges, we introduce mulea, an R package offering \n    comprehensive overrepresentation and functional enrichment analysis. \n    Results - mulea employs a progressive empirical false discovery rate \n    (eFDR) method, specifically designed for interconnected biological data, \n    to accurately identify significant terms within diverse ontologies. mulea \n    expands beyond traditional tools by incorporating a wide range of \n    ontologies, encompassing Gene Ontology, pathways, regulatory elements, \n    genomic locations, and protein domains. This flexibility enables \n    researchers to tailor enrichment analysis to their specific questions, \n    such as identifying enriched transcriptional regulators in gene expression \n    data or overrepresented protein domains in protein sets. To facilitate \n    seamless analysis, mulea provides gene sets (in standardised GMT format) \n    for 27 model organisms, covering 22 ontology types from 16 databases and \n    various identifiers resulting in almost 900 files. Additionally, the \n    muleaData ExperimentData Bioconductor package simplifies access to these \n    pre-defined ontologies. Finally, mulea's architecture allows for easy \n    integration of user-defined ontologies, or GMT files from external \n    sources (e.g., MSigDB or Enrichr), expanding its applicability across \n    diverse research areas. Conclusions - mulea is distributed as a CRAN R \n    package. It offers researchers a powerful and flexible toolkit for \n    functional enrichment analysis, addressing limitations of traditional \n    tools with its progressive eFDR and by supporting a variety of ontologies. \n    Overall, mulea fosters the exploration of diverse biological questions \n    across various model organisms.  "
  },
  {
    "id": 16556,
    "package_name": "multicastR",
    "title": "A Companion to the Multi-CAST Collection",
    "description": "Provides a basic interface for accessing annotation data from\n   the Multi-CAST collection, a database of spoken natural language texts\n   edited by Geoffrey Haig and Stefan Schnell. The collection draws from a\n   diverse set of languages and has been annotated across multiple levels.\n   Annotation data is downloaded on request from the servers of the\n   University of Bamberg. See the Multi-CAST website\n   <https://multicast.aspra.uni-bamberg.de/> for more information and a list\n   of related publications.",
    "version": "2.0.0",
    "maintainer": "Nils Norman Schiborr <nils-norman.schiborr@uni-bamberg.de>",
    "author": "Nils Norman Schiborr [aut, cre]",
    "url": "https://multicast.aspra.uni-bamberg.de/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multicastR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multicastR A Companion to the Multi-CAST Collection Provides a basic interface for accessing annotation data from\n   the Multi-CAST collection, a database of spoken natural language texts\n   edited by Geoffrey Haig and Stefan Schnell. The collection draws from a\n   diverse set of languages and has been annotated across multiple levels.\n   Annotation data is downloaded on request from the servers of the\n   University of Bamberg. See the Multi-CAST website\n   <https://multicast.aspra.uni-bamberg.de/> for more information and a list\n   of related publications.  "
  },
  {
    "id": 16796,
    "package_name": "neo2R",
    "title": "Neo4j to R",
    "description": "The aim of neo2R is to provide simple and low level connectors\n   for querying neo4j graph databases (<https://neo4j.com/>).\n   The objects returned by the query functions are either lists or data.frames\n   with very few post-processing.\n   It allows fast processing of queries returning many records.\n   And it let the user handle post-processing according to the data model\n   and his needs.",
    "version": "2.4.2",
    "maintainer": "Patrice Godard <patrice.godard@gmail.com>",
    "author": "Patrice Godard [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6257-9730>),\n  Eusebiu Marcu [ctb]",
    "url": "https://github.com/patzaw/neo2r",
    "bug_reports": "https://github.com/patzaw/neo2r/issues",
    "repository": "https://cran.r-project.org/package=neo2R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neo2R Neo4j to R The aim of neo2R is to provide simple and low level connectors\n   for querying neo4j graph databases (<https://neo4j.com/>).\n   The objects returned by the query functions are either lists or data.frames\n   with very few post-processing.\n   It allows fast processing of queries returning many records.\n   And it let the user handle post-processing according to the data model\n   and his needs.  "
  },
  {
    "id": 16797,
    "package_name": "neo4jshell",
    "title": "Querying and Managing 'Neo4J' Databases in 'R'",
    "description": "Sends queries to a specified 'Neo4J' graph database, capturing results in a dataframe where appropriate.  \n    Other useful functions for the importing and management of data on the 'Neo4J' server and basic local server admin.  ",
    "version": "0.1.2",
    "maintainer": "Keith McNulty <keith.mcnulty@gmail.com>",
    "author": "Keith McNulty [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-2332-1654>),\n  Liz Romero [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=neo4jshell",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neo4jshell Querying and Managing 'Neo4J' Databases in 'R' Sends queries to a specified 'Neo4J' graph database, capturing results in a dataframe where appropriate.  \n    Other useful functions for the importing and management of data on the 'Neo4J' server and basic local server admin.    "
  },
  {
    "id": 16805,
    "package_name": "neonstore",
    "title": "NEON Data Store",
    "description": "The National Ecological Observatory Network (NEON) provides access\n  to its numerous data products through its REST API, \n  <https://data.neonscience.org/data-api/>. This package provides a\n  high-level user interface for downloading and storing NEON data products.\n  Unlike 'neonUtilities', this package will avoid repeated downloading,\n  provides persistent storage, and improves performance.  'neonstore' can also\n  construct a local 'duckdb' database of stacked tables, making it possible\n  to work with tables that are far to big to fit into memory.",
    "version": "0.5.1",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>),\n  Quinn Thomas [aut] (ORCID: <https://orcid.org/0000-0003-1282-7825>),\n  Christine Laney [aut] (ORCID: <https://orcid.org/0000-0002-4944-2083>),\n  Claire Lunch [aut] (ORCID: <https://orcid.org/0000-0001-8753-6593>),\n  Noam Ross [ctb] (ORCID: <https://orcid.org/0000-0002-2136-0000>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=neonstore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "neonstore NEON Data Store The National Ecological Observatory Network (NEON) provides access\n  to its numerous data products through its REST API, \n  <https://data.neonscience.org/data-api/>. This package provides a\n  high-level user interface for downloading and storing NEON data products.\n  Unlike 'neonUtilities', this package will avoid repeated downloading,\n  provides persistent storage, and improves performance.  'neonstore' can also\n  construct a local 'duckdb' database of stacked tables, making it possible\n  to work with tables that are far to big to fit into memory.  "
  },
  {
    "id": 16849,
    "package_name": "networkR",
    "title": "Network Analysis and Visualization",
    "description": "Collection of functions for fast manipulation, handling, and analysis of large-scale\n    networks based on family and social data. Functions are utility functions used to manipulate data\n    in three \"formats\": sparse adjacency matrices, pedigree trio family data, and pedigree family data.\n    When possible, the functions should be able to handle millions of data points quickly for use in combination\n    with data from large public national registers and databases.\n    Kenneth Lange (2003, ISBN:978-8181281135).",
    "version": "0.1.5",
    "maintainer": "Claus Thorn Ekstr\u00f8m <ekstrom@sund.ku.dk>",
    "author": "Claus Thorn Ekstr\u00f8m [aut, cre],\n  Bendix Carstensen [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=networkR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "networkR Network Analysis and Visualization Collection of functions for fast manipulation, handling, and analysis of large-scale\n    networks based on family and social data. Functions are utility functions used to manipulate data\n    in three \"formats\": sparse adjacency matrices, pedigree trio family data, and pedigree family data.\n    When possible, the functions should be able to handle millions of data points quickly for use in combination\n    with data from large public national registers and databases.\n    Kenneth Lange (2003, ISBN:978-8181281135).  "
  },
  {
    "id": 16954,
    "package_name": "nlpembeds",
    "title": "Natural Language Processing Embeddings",
    "description": "Provides efficient methods to compute co-occurrence matrices, pointwise mutual information (PMI) and singular value decomposition (SVD). In the biomedical and clinical settings, one challenge is the huge size of databases, e.g. when analyzing data of millions of patients over tens of years. To address this, this package provides functions to efficiently compute monthly co-occurrence matrices, which is the computational bottleneck of the analysis, by using the 'RcppAlgos' package and sparse matrices. Furthermore, the functions can be called on 'SQL' databases, enabling the computation of co-occurrence matrices of tens of gigabytes of data, representing millions of patients over tens of years. Partly based on Hong C. (2021) <doi:10.1038/s41746-021-00519-z>.",
    "version": "1.0.0",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "author": "Thomas Charlon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7497-0470>),\n  Doudou Zhou [ctb] (ORCID: <https://orcid.org/0000-0002-0830-2287>),\n  CELEHS [aut] (<https://celehs.hms.harvard.edu>)",
    "url": "https://gitlab.com/thomaschln/nlpembeds",
    "bug_reports": "https://gitlab.com/thomaschln/nlpembeds/-/issues",
    "repository": "https://cran.r-project.org/package=nlpembeds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nlpembeds Natural Language Processing Embeddings Provides efficient methods to compute co-occurrence matrices, pointwise mutual information (PMI) and singular value decomposition (SVD). In the biomedical and clinical settings, one challenge is the huge size of databases, e.g. when analyzing data of millions of patients over tens of years. To address this, this package provides functions to efficiently compute monthly co-occurrence matrices, which is the computational bottleneck of the analysis, by using the 'RcppAlgos' package and sparse matrices. Furthermore, the functions can be called on 'SQL' databases, enabling the computation of co-occurrence matrices of tens of gigabytes of data, representing millions of patients over tens of years. Partly based on Hong C. (2021) <doi:10.1038/s41746-021-00519-z>.  "
  },
  {
    "id": 16999,
    "package_name": "noctua",
    "title": "Connect to 'AWS Athena' using R 'AWS SDK' 'paws' ('DBI'\nInterface)",
    "description": "Designed to be compatible with the 'R' package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this the 'R' 'AWS' Software Development Kit ('SDK') 'paws' \n    <https://github.com/paws-r/paws> is used as a driver.",
    "version": "2.6.3",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "Dyfan Jones [aut, cre]",
    "url": "https://dyfanjones.github.io/noctua/,\nhttps://github.com/DyfanJones/noctua",
    "bug_reports": "https://github.com/DyfanJones/noctua/issues",
    "repository": "https://cran.r-project.org/package=noctua",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "noctua Connect to 'AWS Athena' using R 'AWS SDK' 'paws' ('DBI'\nInterface) Designed to be compatible with the 'R' package 'DBI' (Database Interface)\n    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.\n    To do this the 'R' 'AWS' Software Development Kit ('SDK') 'paws' \n    <https://github.com/paws-r/paws> is used as a driver.  "
  },
  {
    "id": 17011,
    "package_name": "nolock",
    "title": "Append 'WITH (NOLOCK)' to 'SQL' Queries, Get Packages in Active\nScript",
    "description": "Provides a suite of tools that can assist in enhancing the \n    processing efficiency of 'SQL' and 'R' scripts. \n    - The 'libr_unused()' retrieves a vector of package names that are called \n    within an 'R' script but are never actually used in the script.\n    - The 'libr_used()' retrieves a vector of package names actively utilized \n    within an 'R' script; packages loaded using 'library()' but not actually \n    used in the script will not be included.\n    - The 'libr_called()' retrieves a vector of all package names which are \n    called within an 'R' script.\n    - 'nolock()' appends 'WITH (nolock)' to all tables in 'SQL' queries. This \n    facilitates reading from databases in scenarios where non-blocking reads are \n    preferable, such as in high-transaction environments.",
    "version": "1.1.0",
    "maintainer": "Arkadiusz W. Pajda <arkadiusz.pajda.97@onet.pl>",
    "author": "Arkadiusz W. Pajda [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nolock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nolock Append 'WITH (NOLOCK)' to 'SQL' Queries, Get Packages in Active\nScript Provides a suite of tools that can assist in enhancing the \n    processing efficiency of 'SQL' and 'R' scripts. \n    - The 'libr_unused()' retrieves a vector of package names that are called \n    within an 'R' script but are never actually used in the script.\n    - The 'libr_used()' retrieves a vector of package names actively utilized \n    within an 'R' script; packages loaded using 'library()' but not actually \n    used in the script will not be included.\n    - The 'libr_called()' retrieves a vector of all package names which are \n    called within an 'R' script.\n    - 'nolock()' appends 'WITH (nolock)' to all tables in 'SQL' queries. This \n    facilitates reading from databases in scenarios where non-blocking reads are \n    preferable, such as in high-transaction environments.  "
  },
  {
    "id": 17014,
    "package_name": "nomesbr",
    "title": "Limpa e Simplifica Nomes de Pessoas (Name Cleaner and\nSimplifier)",
    "description": "Limpa e simplifica nomes de pessoas para auxiliar no pareamento \n    de banco de dados na aus\u00eancia de chaves \u00fanicas n\u00e3o amb\u00edguas. Detecta e \n    corrige erros tipogr\u00e1ficos mais comuns, simplifica opcionalmente termos\n    sujeitos eventualmente a omiss\u00e3o em cadastros, e simplifica foneticamente \n    suas palavras, aplicando varia\u00e7\u00e3o pr\u00f3pria do algoritmo metaphoneBR.\n    (Cleans and simplifies person names to assist in database matching when \n    unambiguous unique keys are unavailable. Detects and corrects common \n    typos, optionally simplifies terms prone to omission in records, and \n    applies phonetic simplification using a custom variation of the metaphoneBR \n    algorithm.)\n    Mation (2025) <doi:10.6082/uchicago.15104>.",
    "version": "0.0.9",
    "maintainer": "Rodrigo Borges <rodrigoesborges@gmail.com>",
    "author": "Rodrigo Borges [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2076-1424>),\n  Lucas Mation [aut] (ORCID: <https://orcid.org/0000-0002-7461-932X>),\n  Ipea - Institue for Applied Economic Research [cph, fnd]",
    "url": "https://github.com/ipeadata-lab/nomesbr,\nhttps://ipeadata-lab.github.io/nomesbr/",
    "bug_reports": "https://github.com/ipeadata-lab/nomesbr/issues",
    "repository": "https://cran.r-project.org/package=nomesbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nomesbr Limpa e Simplifica Nomes de Pessoas (Name Cleaner and\nSimplifier) Limpa e simplifica nomes de pessoas para auxiliar no pareamento \n    de banco de dados na aus\u00eancia de chaves \u00fanicas n\u00e3o amb\u00edguas. Detecta e \n    corrige erros tipogr\u00e1ficos mais comuns, simplifica opcionalmente termos\n    sujeitos eventualmente a omiss\u00e3o em cadastros, e simplifica foneticamente \n    suas palavras, aplicando varia\u00e7\u00e3o pr\u00f3pria do algoritmo metaphoneBR.\n    (Cleans and simplifies person names to assist in database matching when \n    unambiguous unique keys are unavailable. Detects and corrects common \n    typos, optionally simplifies terms prone to omission in records, and \n    applies phonetic simplification using a custom variation of the metaphoneBR \n    algorithm.)\n    Mation (2025) <doi:10.6082/uchicago.15104>.  "
  },
  {
    "id": 17016,
    "package_name": "nomisdata",
    "title": "Access 'Nomis' UK Labour Market Data and Statistics",
    "description": "Interface to the 'Nomis' database (<https://www.nomisweb.co.uk>), a comprehensive resource of United Kingdom labour market statistics provided by the Office for National Statistics (ONS). Facilitates programmatic access to census data, labour force surveys, benefit statistics, and socioeconomic indicators through a modern HTTP client with intelligent caching, automatic query pagination, and tidy data principles. Includes spatial data integration, interactive helpers, and visualization utilities. Independent implementation unaffiliated with ONS or Durham University.",
    "version": "0.1.1",
    "maintainer": "Cheryl Isabella Lim <cheryl.academic@gmail.com>",
    "author": "Cheryl Isabella Lim [aut, cre] (ORCID:\n    <https://orcid.org/0009-0004-5766-1392>)",
    "url": "https://github.com/cherylisabella/nomisdata",
    "bug_reports": "https://github.com/cherylisabella/nomisdata/issues",
    "repository": "https://cran.r-project.org/package=nomisdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nomisdata Access 'Nomis' UK Labour Market Data and Statistics Interface to the 'Nomis' database (<https://www.nomisweb.co.uk>), a comprehensive resource of United Kingdom labour market statistics provided by the Office for National Statistics (ONS). Facilitates programmatic access to census data, labour force surveys, benefit statistics, and socioeconomic indicators through a modern HTTP client with intelligent caching, automatic query pagination, and tidy data principles. Includes spatial data integration, interactive helpers, and visualization utilities. Independent implementation unaffiliated with ONS or Durham University.  "
  },
  {
    "id": 17040,
    "package_name": "norSTR",
    "title": "Allele Frequencies for 50 Forensic STR Markers",
    "description": "Allele frequency databases for 50 forensic short tandem\n    repeat (STR) markers, covering Norway and several broader regional\n    populations: Europe, Africa, South America, West Asia, Middle Asia,\n    and East Asia. Developed and maintained for use at the Department of\n    Forensic Sciences, Oslo, Norway.",
    "version": "0.2.1",
    "maintainer": "Magnus Dehli Vigeland <m.d.vigeland@medisin.uio.no>",
    "author": "Magnus Dehli Vigeland [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9134-4962>)",
    "url": "https://github.com/magnusdv/norSTR",
    "bug_reports": "https://github.com/magnusdv/norSTR/issues",
    "repository": "https://cran.r-project.org/package=norSTR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "norSTR Allele Frequencies for 50 Forensic STR Markers Allele frequency databases for 50 forensic short tandem\n    repeat (STR) markers, covering Norway and several broader regional\n    populations: Europe, Africa, South America, West Asia, Middle Asia,\n    and East Asia. Developed and maintained for use at the Department of\n    Forensic Sciences, Oslo, Norway.  "
  },
  {
    "id": 17060,
    "package_name": "notionR",
    "title": "R Wrapper for 'Notion' API",
    "description": "Provides functions to query databases and notes in 'Notion', using the official REST API. To learn more about the functionality of the 'Notion' API, see <https://developers.notion.com/>.",
    "version": "0.0.9",
    "maintainer": "Eduardo Flores <eduardo@enelmargen.org>",
    "author": "Eduardo Flores [aut, cre]",
    "url": "<https://www.enelmargen.org/notionR/>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=notionR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "notionR R Wrapper for 'Notion' API Provides functions to query databases and notes in 'Notion', using the official REST API. To learn more about the functionality of the 'Notion' API, see <https://developers.notion.com/>.  "
  },
  {
    "id": 17061,
    "package_name": "notionapi",
    "title": "Client for the 'Notion API'",
    "description": "Enable programmatic interaction with 'Notion' pages,\n    databases, blocks, comments, and users through the 'Notion API'\n    <https://developers.notion.com/>.  Provides both synchronous and\n    asynchronous client interfaces for building workflows and automations\n    that integrate with 'Notion' workspaces. Supports all 'Notion API'\n    endpoints including content creation, data retrieval, and workspace\n    management.",
    "version": "0.1.0",
    "maintainer": "Brenwin Ang <brenwinalj@gmail.com>",
    "author": "Brenwin Ang [aut, cre, cph]",
    "url": "https://brenwin1.github.io/notionapi/,\nhttps://github.com/brenwin1/notionapi",
    "bug_reports": "https://github.com/brenwin1/notionapi/issues",
    "repository": "https://cran.r-project.org/package=notionapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "notionapi Client for the 'Notion API' Enable programmatic interaction with 'Notion' pages,\n    databases, blocks, comments, and users through the 'Notion API'\n    <https://developers.notion.com/>.  Provides both synchronous and\n    asynchronous client interfaces for building workflows and automations\n    that integrate with 'Notion' workspaces. Supports all 'Notion API'\n    endpoints including content creation, data retrieval, and workspace\n    management.  "
  },
  {
    "id": 17148,
    "package_name": "nzffdr",
    "title": "Import, Clean and Update Data from the New Zealand Freshwater\nFish Database",
    "description": "Access the New Zealand Freshwater Fish Database from R and a few functions to clean the data once in R.",
    "version": "2.1.0",
    "maintainer": "Finnbar Lee <lee.finnbar@gmail.com>",
    "author": "Finnbar Lee [aut, cre],\n  Nick Young [aut]",
    "url": "https://flee598.github.io/nzffdr/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nzffdr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nzffdr Import, Clean and Update Data from the New Zealand Freshwater\nFish Database Access the New Zealand Freshwater Fish Database from R and a few functions to clean the data once in R.  "
  },
  {
    "id": 17163,
    "package_name": "obcost",
    "title": "Obesity Cost Database",
    "description": "This database contains necessary data relevant to medical costs on obesity throughout the United States. This database, in form of an R package, could output necessary data frames relevant to obesity costs, where the clients could easily manipulate the output using difference parameters, e.g. relative risks for each illnesses. This package contributes to parts of our published journal named \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\". Please use the following citation for the journal: Woods Thomas, Tatjana Miljkovic (2022) \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\" <doi:10.3390/risks10100197>. The database is composed of the following main tables: 1. Relative_Risks: (constant) Relative risks for a given disease group with a risk factor of obesity; 2. Disease_Cost: (obesity_cost_disease) Supplementary output with all variables related to individual disease groups in a given state and year; 3. Full_Cost: (obesity_cost_full) Complete output with all variables used to make cost calculations, as well as cost calculations in a given state and year; 4. National_Summary: (obesity_cost_national_summary) National summary cost calculations in a given year. Three functions are included to assist users in calling and adjusting the mentioned tables and they are data_load(), data_produce(), and rel_risk_fun(). ",
    "version": "0.1.0",
    "maintainer": "Tianyue Zang <zangt2@miamioh.edu>",
    "author": "Tianyue Zang [aut, cre, cph],\n  Thomas Woods [aut],\n  Tatjana Miljkovic [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=obcost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "obcost Obesity Cost Database This database contains necessary data relevant to medical costs on obesity throughout the United States. This database, in form of an R package, could output necessary data frames relevant to obesity costs, where the clients could easily manipulate the output using difference parameters, e.g. relative risks for each illnesses. This package contributes to parts of our published journal named \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\". Please use the following citation for the journal: Woods Thomas, Tatjana Miljkovic (2022) \"Modeling the Economic Cost of Obesity Risk and Its Relation to the Health Insurance Premium in the United States: A State Level Analysis\" <doi:10.3390/risks10100197>. The database is composed of the following main tables: 1. Relative_Risks: (constant) Relative risks for a given disease group with a risk factor of obesity; 2. Disease_Cost: (obesity_cost_disease) Supplementary output with all variables related to individual disease groups in a given state and year; 3. Full_Cost: (obesity_cost_full) Complete output with all variables used to make cost calculations, as well as cost calculations in a given state and year; 4. National_Summary: (obesity_cost_national_summary) National summary cost calculations in a given year. Three functions are included to assist users in calling and adjusting the mentioned tables and they are data_load(), data_produce(), and rel_risk_fun().   "
  },
  {
    "id": 17191,
    "package_name": "octopus",
    "title": "A Database Management Tool",
    "description": "A database management tool built as a 'shiny' application. Connect to various \n    databases to send queries, upload files, preview tables, and more. ",
    "version": "0.4.2",
    "maintainer": "Marcus Codrescu <m.codrescu@outlook.com>",
    "author": "Marcus Codrescu [aut, cre]",
    "url": "https://github.com/MCodrescu/octopus,\nhttps://mcodrescu.github.io/octopus/",
    "bug_reports": "https://github.com/MCodrescu/octopus/issues",
    "repository": "https://cran.r-project.org/package=octopus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "octopus A Database Management Tool A database management tool built as a 'shiny' application. Connect to various \n    databases to send queries, upload files, preview tables, and more.   "
  },
  {
    "id": 17194,
    "package_name": "odbc",
    "title": "Connect to ODBC Compatible Databases (using the DBI Interface)",
    "description": "A DBI-compatible interface to ODBC databases.",
    "version": "1.6.4",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Jim Hester [aut],\n  Hadley Wickham [aut, cre],\n  Oliver Gjoneski [aut],\n  Simon Couch [aut],\n  lexicalunit [cph] (nanodbc library),\n  Google Inc. [cph] (cctz library),\n  Posit Software, PBC [cph, fnd]",
    "url": "https://odbc.r-dbi.org, https://github.com/r-dbi/odbc,\nhttps://solutions.posit.co/connections/db/",
    "bug_reports": "https://github.com/r-dbi/odbc/issues",
    "repository": "https://cran.r-project.org/package=odbc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "odbc Connect to ODBC Compatible Databases (using the DBI Interface) A DBI-compatible interface to ODBC databases.  "
  },
  {
    "id": 17195,
    "package_name": "odbc.resourcer",
    "title": "Open Database Connectivity Resource Resolver",
    "description": "A database resource that is accessible through the Open Database Connectivity ('ODBC') API. This package uses the Resource \n  model, with URL \"resolver\" and \"client\", to dynamically discover and make accessible tables stored in a 'MS SQL Server' database.\n  For more details see Marcon (2021) <doi:10.1371/journal.pcbi.1008880>. ",
    "version": "1.0.0",
    "maintainer": "Yannick Marcon <yannick.marcon@obiba.org>",
    "author": "Yannick Marcon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0138-2023>),\n  OBiBa group [cph]",
    "url": "",
    "bug_reports": "https://github.com/obiba/odbc.resourcer/issues",
    "repository": "https://cran.r-project.org/package=odbc.resourcer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "odbc.resourcer Open Database Connectivity Resource Resolver A database resource that is accessible through the Open Database Connectivity ('ODBC') API. This package uses the Resource \n  model, with URL \"resolver\" and \"client\", to dynamically discover and make accessible tables stored in a 'MS SQL Server' database.\n  For more details see Marcon (2021) <doi:10.1371/journal.pcbi.1008880>.   "
  },
  {
    "id": 17220,
    "package_name": "oglcnac",
    "title": "Processing and Analysis of 'O-GlcNAcAtlas' Data",
    "description": "Provides tools for processing and analyzing data from the 'O-GlcNAcAtlas' database <https://oglcnac.org/>, \n    as described in Ma (2021) <doi:10.1093/glycob/cwab003>. It integrates 'UniProt' <https://www.uniprot.org/> API calls \n    to retrieve additional information. It is specifically designed for research workflows \n    involving 'O-GlcNAcAtlas' data, providing a flexible and user-friendly interface for customizing \n    and downloading processed results. Interactive elements allow users to easily adjust parameters \n    and handle various biological datasets.",
    "version": "0.1.5",
    "maintainer": "Yaoxiang Li <liyaoxiang@outlook.com>",
    "author": "Yaoxiang Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-9200-1016>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=oglcnac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oglcnac Processing and Analysis of 'O-GlcNAcAtlas' Data Provides tools for processing and analyzing data from the 'O-GlcNAcAtlas' database <https://oglcnac.org/>, \n    as described in Ma (2021) <doi:10.1093/glycob/cwab003>. It integrates 'UniProt' <https://www.uniprot.org/> API calls \n    to retrieve additional information. It is specifically designed for research workflows \n    involving 'O-GlcNAcAtlas' data, providing a flexible and user-friendly interface for customizing \n    and downloading processed results. Interactive elements allow users to easily adjust parameters \n    and handle various biological datasets.  "
  },
  {
    "id": 17246,
    "package_name": "onbabynames",
    "title": "Names Given to Babies in Ontario Between 1917 and 2018",
    "description": "A database containing the names \n  of the babies born in Ontario between 1917 and 2018. \n  Counts of fewer than 5 names were suppressed for privacy.",
    "version": "0.0.1",
    "maintainer": "Marc-Andre Desautels <marc-andre.desautels@cstjean.qc.ca>",
    "author": "Marc-Andre Desautels [aut, cre]",
    "url": "<https://github.com/desautm/onbabynames>",
    "bug_reports": "https://github.com/desautm/onbabynames/issues",
    "repository": "https://cran.r-project.org/package=onbabynames",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "onbabynames Names Given to Babies in Ontario Between 1917 and 2018 A database containing the names \n  of the babies born in Ontario between 1917 and 2018. \n  Counts of fewer than 5 names were suppressed for privacy.  "
  },
  {
    "id": 17286,
    "package_name": "opalr",
    "title": "'Opal' Data Repository Client and 'DataSHIELD' Utils",
    "description": "Data integration Web application for biobanks by 'OBiBa'. 'Opal' is\n    the core database application for biobanks. Participant data, once\n    collected from any data source, must be integrated and stored in a central\n    data repository under a uniform model. 'Opal' is such a central repository.\n    It can import, process, validate, query, analyze, report, and export data.\n    'Opal' is typically used in a research center to analyze the data acquired at\n    assessment centres. Its ultimate purpose is to achieve seamless\n    data-sharing among biobanks. This 'Opal' client allows to interact with 'Opal'\n    web services and to perform operations on the R server side. 'DataSHIELD'\n    administration tools are also provided.",
    "version": "3.5.2",
    "maintainer": "Yannick Marcon <yannick.marcon@obiba.org>",
    "author": "Yannick Marcon [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0138-2023>),\n  Amadou Gaye [ctb] (ORCID: <https://orcid.org/0000-0002-1180-2792>),\n  OBiBa group [cph]",
    "url": "https://github.com/obiba/opalr/, https://www.obiba.org/opalr/,\nhttps://www.obiba.org/pages/products/opal/,\nhttps://academic.oup.com/ije/article/46/5/1372/4102813,\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008880,\nhttps://datashield.org/",
    "bug_reports": "https://github.com/obiba/opalr/issues",
    "repository": "https://cran.r-project.org/package=opalr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "opalr 'Opal' Data Repository Client and 'DataSHIELD' Utils Data integration Web application for biobanks by 'OBiBa'. 'Opal' is\n    the core database application for biobanks. Participant data, once\n    collected from any data source, must be integrated and stored in a central\n    data repository under a uniform model. 'Opal' is such a central repository.\n    It can import, process, validate, query, analyze, report, and export data.\n    'Opal' is typically used in a research center to analyze the data acquired at\n    assessment centres. Its ultimate purpose is to achieve seamless\n    data-sharing among biobanks. This 'Opal' client allows to interact with 'Opal'\n    web services and to perform operations on the R server side. 'DataSHIELD'\n    administration tools are also provided.  "
  },
  {
    "id": 17289,
    "package_name": "openEBGM",
    "title": "EBGM Disproportionality Scores for Adverse Event Data Mining",
    "description": "An implementation of DuMouchel's (1999) <doi:10.1080/00031305.1999.10474456>\n  Bayesian data mining method for the market basket problem.\n  Calculates Empirical Bayes Geometric Mean (EBGM) and posterior quantile scores\n  using the Gamma-Poisson Shrinker (GPS) model to find unusually large cell\n  counts in large, sparse contingency tables. Can be used to find unusually high\n  reporting rates of adverse events associated with products. In general, can be\n  used to mine any database where the co-occurrence of two variables or items is\n  of interest. Also calculates relative and proportional reporting ratios.\n  Builds on the work of the 'PhViD' package, from which much of the code is\n  derived. Some of the added features include stratification to adjust for\n  confounding variables and data squashing to improve computational efficiency.\n  Includes an implementation of the EM algorithm for hyperparameter estimation\n  loosely derived from the 'mederrRank' package.",
    "version": "0.9.1",
    "maintainer": "John Ihrie <John.Ihrie@fda.hhs.gov>",
    "author": "John Ihrie [cre, aut],\n  Travis Canida [aut],\n  Isma\u00efl Ahmed [ctb] (author of 'PhViD' package (derived code)),\n  Antoine Poncet [ctb] (author of 'PhViD'),\n  Sergio Venturini [ctb] (author of 'mederrRank' package (derived code)),\n  Jessica Myers [ctb] (author of 'mederrRank')",
    "url": "https://journal.r-project.org/archive/2017/RJ-2017-063/index.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=openEBGM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openEBGM EBGM Disproportionality Scores for Adverse Event Data Mining An implementation of DuMouchel's (1999) <doi:10.1080/00031305.1999.10474456>\n  Bayesian data mining method for the market basket problem.\n  Calculates Empirical Bayes Geometric Mean (EBGM) and posterior quantile scores\n  using the Gamma-Poisson Shrinker (GPS) model to find unusually large cell\n  counts in large, sparse contingency tables. Can be used to find unusually high\n  reporting rates of adverse events associated with products. In general, can be\n  used to mine any database where the co-occurrence of two variables or items is\n  of interest. Also calculates relative and proportional reporting ratios.\n  Builds on the work of the 'PhViD' package, from which much of the code is\n  derived. Some of the added features include stratification to adjust for\n  confounding variables and data squashing to improve computational efficiency.\n  Includes an implementation of the EM algorithm for hyperparameter estimation\n  loosely derived from the 'mederrRank' package.  "
  },
  {
    "id": 17299,
    "package_name": "openappr",
    "title": "Retrieve App Data from 'OpenAppBuilder'",
    "description": "Provides an interface to connect R with the <https://github.com/IDEMSInternational/open-app-builder> 'OpenAppBuilder' platform, enabling users to retrieve and work with user and notification data for analysis and processing. It is designed for developers and analysts to seamlessly integrate data from 'OpenAppBuilder' into R workflows via a 'Postgres' database connection, allowing direct querying and import of app data into R.",
    "version": "0.2.0",
    "maintainer": "Lily Clements <lily@idems.international>",
    "author": "Lily Clements [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8864-0552>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=openappr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openappr Retrieve App Data from 'OpenAppBuilder' Provides an interface to connect R with the <https://github.com/IDEMSInternational/open-app-builder> 'OpenAppBuilder' platform, enabling users to retrieve and work with user and notification data for analysis and processing. It is designed for developers and analysts to seamlessly integrate data from 'OpenAppBuilder' into R workflows via a 'Postgres' database connection, allowing direct querying and import of app data into R.  "
  },
  {
    "id": 17307,
    "package_name": "openesm",
    "title": "Access the Open Experience Sampling Method Database",
    "description": "Provides programmatic access to the Open Experience Sampling Method ('openESM') database (<https://openesmdata.org>), \n    a collection of harmonized experience sampling datasets. The package enables researchers to discover, download, \n    and work with the datasets while ensuring proper citation and license compliance.  ",
    "version": "0.1.2",
    "maintainer": "Bj\u00f6rn S. Siepe <bjoernsiepe@gmail.com>",
    "author": "Bj\u00f6rn S. Siepe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9558-4648>),\n  Matthias Kloft [aut] (ORCID: <https://orcid.org/0000-0003-1845-6957>)",
    "url": "https://github.com/openesm-project/openesm-r",
    "bug_reports": "https://github.com/openesm-project/openesm-r/issues",
    "repository": "https://cran.r-project.org/package=openesm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openesm Access the Open Experience Sampling Method Database Provides programmatic access to the Open Experience Sampling Method ('openESM') database (<https://openesmdata.org>), \n    a collection of harmonized experience sampling datasets. The package enables researchers to discover, download, \n    and work with the datasets while ensuring proper citation and license compliance.    "
  },
  {
    "id": 17410,
    "package_name": "orsifronts",
    "title": "Southern Ocean Frontal Distributions (Orsi)",
    "description": "A data set package with the \"Orsi\" and \"Park/Durand\" fronts as\n    'SpatialLinesDataFrame' objects. The Orsi et al. (1995) fronts are published at\n    the Southern Ocean Atlas Database Page, and the Park et al. (2019) fronts are published at the 'SEANOE' \n    Altimetry-derived Antarctic Circumpolar Current fronts page, please see package CITATION for details.",
    "version": "0.2.0",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre]",
    "url": "https://australianantarcticdivision.github.io/orsifronts/",
    "bug_reports": "https://github.com/AustralianAntarcticDivision/orsifronts/issues",
    "repository": "https://cran.r-project.org/package=orsifronts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "orsifronts Southern Ocean Frontal Distributions (Orsi) A data set package with the \"Orsi\" and \"Park/Durand\" fronts as\n    'SpatialLinesDataFrame' objects. The Orsi et al. (1995) fronts are published at\n    the Southern Ocean Atlas Database Page, and the Park et al. (2019) fronts are published at the 'SEANOE' \n    Altimetry-derived Antarctic Circumpolar Current fronts page, please see package CITATION for details.  "
  },
  {
    "id": 17445,
    "package_name": "outliers.ts.oga",
    "title": "Efficient Outlier Detection for Large Time Series Databases",
    "description": "Programs for detecting and cleaning outliers in single time series and in time series from homogeneous and heterogeneous databases using an Orthogonal Greedy Algorithm (OGA) for saturated linear regression models. The programs implement the procedures presented in the paper entitled \"Efficient Outlier Detection for Large Time Series Databases\" by Pedro Galeano, Daniel Pe\u00f1a and Ruey S. Tsay (2025), working paper, Universidad Carlos III de Madrid. Version 1.1.1 contains some improvements in parallelization with respect to version 1.0.1.",
    "version": "1.1.1",
    "maintainer": "Pedro Galeano <pedro.galeano@uc3m.es>",
    "author": "Pedro Galeano [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2577-2747>),\n  Daniel Pe\u00f1a [aut] (ORCID: <https://orcid.org/0000-0002-9137-1557>),\n  Ruey S. Tsay [aut] (ORCID: <https://orcid.org/0000-0002-4949-4035>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=outliers.ts.oga",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "outliers.ts.oga Efficient Outlier Detection for Large Time Series Databases Programs for detecting and cleaning outliers in single time series and in time series from homogeneous and heterogeneous databases using an Orthogonal Greedy Algorithm (OGA) for saturated linear regression models. The programs implement the procedures presented in the paper entitled \"Efficient Outlier Detection for Large Time Series Databases\" by Pedro Galeano, Daniel Pe\u00f1a and Ruey S. Tsay (2025), working paper, Universidad Carlos III de Madrid. Version 1.1.1 contains some improvements in parallelization with respect to version 1.0.1.  "
  },
  {
    "id": 17454,
    "package_name": "overtureR",
    "title": "Load 'Overture' Datasets as 'dbplyr' and 'sf'-Ready Data Frames",
    "description": "An integrated R interface to the 'Overture' API \n  (<https://docs.overturemaps.org/>). Allows R users to return 'Overture' data as \n  'dbplyr' data frames or materialized 'sf' spatial data frames.",
    "version": "0.2.5",
    "maintainer": "Arthur Gailes <agailes1@gmail.com>",
    "author": "Arthur Gailes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0006-8176-8653>)",
    "url": "https://github.com/arthurgailes/overtureR,\nhttps://arthurgailes.github.io/overtureR/",
    "bug_reports": "https://github.com/arthurgailes/overtureR/issues",
    "repository": "https://cran.r-project.org/package=overtureR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "overtureR Load 'Overture' Datasets as 'dbplyr' and 'sf'-Ready Data Frames An integrated R interface to the 'Overture' API \n  (<https://docs.overturemaps.org/>). Allows R users to return 'Overture' data as \n  'dbplyr' data frames or materialized 'sf' spatial data frames.  "
  },
  {
    "id": 17477,
    "package_name": "pKSEA",
    "title": "Prediction-Based Kinase-Substrate Enrichment Analysis",
    "description": "A tool for inferring kinase activity changes from \n    phosphoproteomics data. 'pKSEA' uses kinase-substrate \n    prediction scores to weight observed changes in \n    phosphopeptide abundance to calculate a phosphopeptide-level \n    contribution score, then sums up these contribution scores by \n    kinase to obtain a phosphoproteome-level kinase activity \n    change score (KAC score). 'pKSEA' then assesses the \n    significance of changes in predicted substrate abundances for \n    each kinase using permutation testing. This results in a \n    permutation score (pKSEA significance score) reflecting the \n    likelihood of a similarly high or low KAC from random chance, \n    which can then be interpreted in an analogous manner to an \n    empirically calculated p-value. 'pKSEA' contains default \n    databases of kinase-substrate predictions from 'NetworKIN' \n    (NetworKINPred_db) <http://networkin.info> \n    Horn, et. al (2014) <doi:10.1038/nmeth.2968>\n    and of known kinase-substrate links from 'PhosphoSitePlus' \n    (KSEAdb) <https://www.phosphosite.org/>\n    Hornbeck PV, et. al (2015) <doi:10.1093/nar/gku1267>.",
    "version": "0.0.1",
    "maintainer": "Peter Liao <pll21@case.edu>",
    "author": "Peter Liao [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pKSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pKSEA Prediction-Based Kinase-Substrate Enrichment Analysis A tool for inferring kinase activity changes from \n    phosphoproteomics data. 'pKSEA' uses kinase-substrate \n    prediction scores to weight observed changes in \n    phosphopeptide abundance to calculate a phosphopeptide-level \n    contribution score, then sums up these contribution scores by \n    kinase to obtain a phosphoproteome-level kinase activity \n    change score (KAC score). 'pKSEA' then assesses the \n    significance of changes in predicted substrate abundances for \n    each kinase using permutation testing. This results in a \n    permutation score (pKSEA significance score) reflecting the \n    likelihood of a similarly high or low KAC from random chance, \n    which can then be interpreted in an analogous manner to an \n    empirically calculated p-value. 'pKSEA' contains default \n    databases of kinase-substrate predictions from 'NetworKIN' \n    (NetworKINPred_db) <http://networkin.info> \n    Horn, et. al (2014) <doi:10.1038/nmeth.2968>\n    and of known kinase-substrate links from 'PhosphoSitePlus' \n    (KSEAdb) <https://www.phosphosite.org/>\n    Hornbeck PV, et. al (2015) <doi:10.1093/nar/gku1267>.  "
  },
  {
    "id": 17526,
    "package_name": "paleoDiv",
    "title": "Extracting and Visualizing Paleobiodiversity",
    "description": "Contains various tools for conveniently downloading and editing taxon-specific datasets from the Paleobiology Database <https://paleobiodb.org>, extracting information on abundance, temporal distribution of subtaxa and taxonomic diversity through deep time, and visualizing these data in relation to phylogeny and stratigraphy.",
    "version": "0.4.6",
    "maintainer": "Darius Nau <dariusnau@gmx.at>",
    "author": "Darius Nau [aut, cre] (ORCID: <https://orcid.org/0009-0000-4343-6830>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=paleoDiv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paleoDiv Extracting and Visualizing Paleobiodiversity Contains various tools for conveniently downloading and editing taxon-specific datasets from the Paleobiology Database <https://paleobiodb.org>, extracting information on abundance, temporal distribution of subtaxa and taxonomic diversity through deep time, and visualizing these data in relation to phylogeny and stratigraphy.  "
  },
  {
    "id": 17597,
    "package_name": "parquetize",
    "title": "Convert Files to Parquet Format",
    "description": "Collection of functions to get files in parquet format.\n    Parquet is a columnar storage file format <https://parquet.apache.org/>. \n    The files to convert can be of several formats \n    (\"csv\", \"RData\", \"rds\", \"RSQLite\", \n    \"json\", \"ndjson\", \"SAS\", \"SPSS\"...).",
    "version": "0.5.8",
    "maintainer": "Damien Dotta <damien.dotta@live.fr>",
    "author": "Damien Dotta [aut, cre],\n  Nicolas Chuche [aut]",
    "url": "https://ddotta.github.io/parquetize/,\nhttps://github.com/ddotta/parquetize",
    "bug_reports": "https://github.com/ddotta/parquetize/issues",
    "repository": "https://cran.r-project.org/package=parquetize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parquetize Convert Files to Parquet Format Collection of functions to get files in parquet format.\n    Parquet is a columnar storage file format <https://parquet.apache.org/>. \n    The files to convert can be of several formats \n    (\"csv\", \"RData\", \"rds\", \"RSQLite\", \n    \"json\", \"ndjson\", \"SAS\", \"SPSS\"...).  "
  },
  {
    "id": 17599,
    "package_name": "parseRPDR",
    "title": "Parse and Manipulate Research Patient Data Registry ('RPDR')\nText Queries",
    "description": "Functions to load Research Patient Data Registry ('RPDR') text queries from Partners Healthcare institutions into R.\n             The package also provides helper functions to manipulate data and execute common procedures\n             such as finding the closest radiological exams considering a given timepoint, or creating a DICOM header database\n             from the downloaded images. All functionalities are parallelized for fast and efficient analyses.",
    "version": "1.1.2",
    "maintainer": "Marton Kolossvary <mkolossvary@mgh.harvard.edu>",
    "author": "Marton Kolossvary [aut, cre]",
    "url": "https://github.com/martonkolossvary/parseRPDR",
    "bug_reports": "https://github.com/martonkolossvary/parseRPDR/issues",
    "repository": "https://cran.r-project.org/package=parseRPDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "parseRPDR Parse and Manipulate Research Patient Data Registry ('RPDR')\nText Queries Functions to load Research Patient Data Registry ('RPDR') text queries from Partners Healthcare institutions into R.\n             The package also provides helper functions to manipulate data and execute common procedures\n             such as finding the closest radiological exams considering a given timepoint, or creating a DICOM header database\n             from the downloaded images. All functionalities are parallelized for fast and efficient analyses.  "
  },
  {
    "id": 17631,
    "package_name": "patentr",
    "title": "Access USPTO Bulk Data in Tidy Rectangular Format",
    "description": "Converts TXT and XML data curated by the United States Patent and\n    Trademark Office (USPTO). Allows conversion of bulk data after downloading\n    directly from the USPTO bulk data website, eliminating need for users to\n    wrangle multiple data formats to get large patent databases in tidy,\n    rectangular format. Data details can be found on the USPTO website\n    <https://bulkdata.uspto.gov/>. Currently, all 3 formats: 1. TXT data\n    (1976-2001); 2. XML format 1 data (2002-2004); and 3. XML format 2 data\n    (2005-current) can be converted to rectangular, CSV format.\n    Relevant literature that uses data from USPTO includes Wada (2020)\n    <doi:10.1007/s11192-020-03674-4> and Plaza & Albert (2008)\n    <doi:10.1007/s11192-007-1763-3>.",
    "version": "0.1.4",
    "maintainer": "Raoul Wadhwa <raoulwadhwa@gmail.com>",
    "author": "Raoul Wadhwa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0503-9580>),\n  James Yu [aut],\n  Hayley Beltz [aut],\n  Milind Desai [aut],\n  Jacob Scott [aut],\n  Peter Erdi [aut]",
    "url": "https://JYProjs.github.io/patentr/",
    "bug_reports": "https://github.com/JYProjs/patentr/issues",
    "repository": "https://cran.r-project.org/package=patentr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "patentr Access USPTO Bulk Data in Tidy Rectangular Format Converts TXT and XML data curated by the United States Patent and\n    Trademark Office (USPTO). Allows conversion of bulk data after downloading\n    directly from the USPTO bulk data website, eliminating need for users to\n    wrangle multiple data formats to get large patent databases in tidy,\n    rectangular format. Data details can be found on the USPTO website\n    <https://bulkdata.uspto.gov/>. Currently, all 3 formats: 1. TXT data\n    (1976-2001); 2. XML format 1 data (2002-2004); and 3. XML format 2 data\n    (2005-current) can be converted to rectangular, CSV format.\n    Relevant literature that uses data from USPTO includes Wada (2020)\n    <doi:10.1007/s11192-020-03674-4> and Plaza & Albert (2008)\n    <doi:10.1007/s11192-007-1763-3>.  "
  },
  {
    "id": 17649,
    "package_name": "paws",
    "title": "Amazon Web Services Software Development Kit",
    "description": "Interface to Amazon Web Services <https://aws.amazon.com>,\n    including storage, database, and compute services, such as 'Simple\n    Storage Service' ('S3'), 'DynamoDB' 'NoSQL' database, and 'Lambda'\n    functions-as-a-service.",
    "version": "0.9.0",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "David Kretch [aut],\n  Adam Banker [aut],\n  Dyfan Jones [cre],\n  Amazon.com, Inc. [cph]",
    "url": "https://github.com/paws-r/paws, https://paws-r.r-universe.dev/paws",
    "bug_reports": "https://github.com/paws-r/paws/issues",
    "repository": "https://cran.r-project.org/package=paws",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paws Amazon Web Services Software Development Kit Interface to Amazon Web Services <https://aws.amazon.com>,\n    including storage, database, and compute services, such as 'Simple\n    Storage Service' ('S3'), 'DynamoDB' 'NoSQL' database, and 'Lambda'\n    functions-as-a-service.  "
  },
  {
    "id": 17656,
    "package_name": "paws.database",
    "title": "'Amazon Web Services' Database Services",
    "description": "Interface to 'Amazon Web Services' database services,\n    including 'Relational Database Service' ('RDS'), 'DynamoDB' 'NoSQL'\n    database, and more <https://aws.amazon.com/>.",
    "version": "0.9.0",
    "maintainer": "Dyfan Jones <dyfan.r.jones@gmail.com>",
    "author": "David Kretch [aut],\n  Adam Banker [aut],\n  Dyfan Jones [cre],\n  Amazon.com, Inc. [cph]",
    "url": "https://github.com/paws-r/paws,\nhttps://paws-r.r-universe.dev/paws.database",
    "bug_reports": "https://github.com/paws-r/paws/issues",
    "repository": "https://cran.r-project.org/package=paws.database",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paws.database 'Amazon Web Services' Database Services Interface to 'Amazon Web Services' database services,\n    including 'Relational Database Service' ('RDS'), 'DynamoDB' 'NoSQL'\n    database, and more <https://aws.amazon.com/>.  "
  },
  {
    "id": 17821,
    "package_name": "pestr",
    "title": "Interface to Download Data on Pests and Hosts from 'EPPO'",
    "description": "Set of tools to automatize extraction of data on pests from 'EPPO\n    Data Services' and 'EPPO Global Database' and to put them into tables with\n    human readable format. Those function use 'EPPO database API', thus you \n    first need to register on <https://data.eppo.int> (free of charge).\n    Additional helpers allow to download, check and connect to\n    'SQLite EPPO database'.",
    "version": "0.8.2",
    "maintainer": "Michal Jan Czyz <m.czyz.j@gmail.com>",
    "author": "Michal Jan Czyz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7156-4530>)",
    "url": "https://github.com/mczyzj/pestr",
    "bug_reports": "https://github.com/mczyzj/pestr/issues",
    "repository": "https://cran.r-project.org/package=pestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pestr Interface to Download Data on Pests and Hosts from 'EPPO' Set of tools to automatize extraction of data on pests from 'EPPO\n    Data Services' and 'EPPO Global Database' and to put them into tables with\n    human readable format. Those function use 'EPPO database API', thus you \n    first need to register on <https://data.eppo.int> (free of charge).\n    Additional helpers allow to download, check and connect to\n    'SQLite EPPO database'.  "
  },
  {
    "id": 17831,
    "package_name": "pgTools",
    "title": "Functions for Generating PostgreSQL Statements/Scripts",
    "description": "Create PostgreSQL statements/scripts from R, optionally executing the SQL statements.\n    Common SQL operations are included, although not every configurable option is available at this time. \n    SQL output is intended to be compliant with PostgreSQL syntax specifications. PostgreSQL documentation is available here\n    <https://www.postgresql.org/docs/current/index.html>.",
    "version": "1.0.2",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell",
    "url": "https://github.com/tconwell/pgTools",
    "bug_reports": "https://github.com/tconwell/pgTools/issues",
    "repository": "https://cran.r-project.org/package=pgTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pgTools Functions for Generating PostgreSQL Statements/Scripts Create PostgreSQL statements/scripts from R, optionally executing the SQL statements.\n    Common SQL operations are included, although not every configurable option is available at this time. \n    SQL output is intended to be compliant with PostgreSQL syntax specifications. PostgreSQL documentation is available here\n    <https://www.postgresql.org/docs/current/index.html>.  "
  },
  {
    "id": 17925,
    "package_name": "phylotypr",
    "title": "Classifying DNA Sequences to Taxonomic Groupings",
    "description": "Classification based analysis of DNA sequences to taxonomic groupings. This package primarily implements Naive Bayesian Classifier from the Ribosomal Database Project. This approach has traditionally been used to classify 16S rRNA gene sequences to bacterial taxonomic outlines; however, it can be used for any type of gene sequence. The method was originally described by Wang, Garrity, Tiedje, and Cole in Applied and Environmental Microbiology 73(16):5261-7 <doi:10.1128/AEM.00062-07>. The package also provides functions to read in 'FASTA'-formatted sequence data.",
    "version": "0.1.1",
    "maintainer": "Pat Schloss <pschloss@umich.edu>",
    "author": "Pat Schloss [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6935-4275>)",
    "url": "https://github.com/mothur/phylotypr, https://mothur.org/phylotypr/",
    "bug_reports": "https://github.com/mothur/phylotypr/issues",
    "repository": "https://cran.r-project.org/package=phylotypr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylotypr Classifying DNA Sequences to Taxonomic Groupings Classification based analysis of DNA sequences to taxonomic groupings. This package primarily implements Naive Bayesian Classifier from the Ribosomal Database Project. This approach has traditionally been used to classify 16S rRNA gene sequences to bacterial taxonomic outlines; however, it can be used for any type of gene sequence. The method was originally described by Wang, Garrity, Tiedje, and Cole in Applied and Environmental Microbiology 73(16):5261-7 <doi:10.1128/AEM.00062-07>. The package also provides functions to read in 'FASTA'-formatted sequence data.  "
  },
  {
    "id": 17946,
    "package_name": "pikchr",
    "title": "R Wrapper for 'pikchr' (PIC) Diagram Language",
    "description": "An 'R' interface to 'pikchr' (<https://pikchr.org>, pronounced \u201cpicture\u201d), a 'PIC'-like markup language for creating diagrams within technical documentation. Originally developed by Brian Kernighan, 'PIC' has been adapted into 'pikchr' by D. Richard Hipp, the creator of 'SQLite'. 'pikchr' is designed to be embedded in fenced code blocks of Markdown or other documentation markup languages, making it ideal for generating diagrams in text-based formats. This package allows R users to seamlessly integrate the descriptive syntax of 'pikchr' for diagram creation directly within the 'R' environment.",
    "version": "1.0.3",
    "maintainer": "Andre Leite <leite@castlab.org>",
    "author": "Andre Leite [aut, cre],\n  Hugo Vaconcelos [aut],\n  Richard Hipp [ctb],\n  Brian Kernighan [ctb]",
    "url": "<https://github.com/StrategicProjects/pikchr>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pikchr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pikchr R Wrapper for 'pikchr' (PIC) Diagram Language An 'R' interface to 'pikchr' (<https://pikchr.org>, pronounced \u201cpicture\u201d), a 'PIC'-like markup language for creating diagrams within technical documentation. Originally developed by Brian Kernighan, 'PIC' has been adapted into 'pikchr' by D. Richard Hipp, the creator of 'SQLite'. 'pikchr' is designed to be embedded in fenced code blocks of Markdown or other documentation markup languages, making it ideal for generating diagrams in text-based formats. This package allows R users to seamlessly integrate the descriptive syntax of 'pikchr' for diagram creation directly within the 'R' environment.  "
  },
  {
    "id": 17967,
    "package_name": "pisaRT",
    "title": "Small Example Response and Response Time Data from PISA 2018",
    "description": "Scored responses and responses times from the Canadian subsample of the PISA 2018 assessment, accessible as the \"Cognitive items total time/visits data file\" by OECD (2020) <https://www.oecd.org/pisa/data/2018database/>. ",
    "version": "2.0.2",
    "maintainer": "Benjamin Becker <b.becker@iqb.hu-berlin.de>",
    "author": "Benjamin Becker [aut, cre],\n  Esther Ulitzsch [ctb],\n  Christoph Koenig [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pisaRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pisaRT Small Example Response and Response Time Data from PISA 2018 Scored responses and responses times from the Canadian subsample of the PISA 2018 assessment, accessible as the \"Cognitive items total time/visits data file\" by OECD (2020) <https://www.oecd.org/pisa/data/2018database/>.   "
  },
  {
    "id": 17990,
    "package_name": "pkgndep",
    "title": "Analyze Dependency Heaviness of R Packages",
    "description": "A new metric named 'dependency heaviness' is proposed that measures the number \n    of additional dependency packages that a parent package brings to its child package \n    and are unique to the dependency packages imported by all other parents.  \n    The dependency heaviness analysis is visualized by a customized heatmap. \n    The package is described in <doi:10.1093/bioinformatics/btac449>. \n    We have also performed the dependency heaviness analysis on the CRAN/Bioconductor \n    package ecosystem and the results are implemented as a web-based database \n    which provides comprehensive tools for querying dependencies of individual R packages. \n    The systematic analysis on the CRAN/Bioconductor ecosystem is described in \n    <doi:10.1016/j.jss.2023.111610>. From 'pkgndep' version 2.0.0, the heaviness \n    database includes snapshots of the CRAN/Bioconductor ecosystems for many old R versions.",
    "version": "1.99.3",
    "maintainer": "Zuguang Gu <z.gu@dkfz.de>",
    "author": "Zuguang Gu [aut, cre] (ORCID: <https://orcid.org/0000-0002-7395-8709>)",
    "url": "https://github.com/jokergoo/pkgndep",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pkgndep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pkgndep Analyze Dependency Heaviness of R Packages A new metric named 'dependency heaviness' is proposed that measures the number \n    of additional dependency packages that a parent package brings to its child package \n    and are unique to the dependency packages imported by all other parents.  \n    The dependency heaviness analysis is visualized by a customized heatmap. \n    The package is described in <doi:10.1093/bioinformatics/btac449>. \n    We have also performed the dependency heaviness analysis on the CRAN/Bioconductor \n    package ecosystem and the results are implemented as a web-based database \n    which provides comprehensive tools for querying dependencies of individual R packages. \n    The systematic analysis on the CRAN/Bioconductor ecosystem is described in \n    <doi:10.1016/j.jss.2023.111610>. From 'pkgndep' version 2.0.0, the heaviness \n    database includes snapshots of the CRAN/Bioconductor ecosystems for many old R versions.  "
  },
  {
    "id": 18095,
    "package_name": "pmmlTransformations",
    "title": "Transforms Input Data from a PMML Perspective",
    "description": "Allows for data to be transformed before using it to construct models. Builds structures to allow functions in the PMML package to\n    output transformation details in addition to the model in the resulting PMML file. The Predictive Model Markup Language (PMML) is an XML-based language which provides a way for applications to define machine learning, statistical and data mining models and to share models between PMML compliant applications. More information about the PMML industry standard and the Data Mining Group can be found at <http://www.dmg.org>. The generated PMML can be imported into any PMML consuming application, such as Zementis Predictive Analytics products, which integrate with web services, relational database systems and deploy natively on Hadoop in conjunction with Hive, Spark or Storm, as well as allow predictive analytics to be executed for IBM z Systems mainframe applications and real-time, streaming analytics platforms.",
    "version": "1.3.3",
    "maintainer": "Dmitriy Bolotov <rpmmlsupport@softwareag.com>",
    "author": "Tridivesh Jena, Wen Ching Lin, Dmitriy Bolotov",
    "url": "https://www.softwareag.com/zementis",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pmmlTransformations",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmmlTransformations Transforms Input Data from a PMML Perspective Allows for data to be transformed before using it to construct models. Builds structures to allow functions in the PMML package to\n    output transformation details in addition to the model in the resulting PMML file. The Predictive Model Markup Language (PMML) is an XML-based language which provides a way for applications to define machine learning, statistical and data mining models and to share models between PMML compliant applications. More information about the PMML industry standard and the Data Mining Group can be found at <http://www.dmg.org>. The generated PMML can be imported into any PMML consuming application, such as Zementis Predictive Analytics products, which integrate with web services, relational database systems and deploy natively on Hadoop in conjunction with Hive, Spark or Storm, as well as allow predictive analytics to be executed for IBM z Systems mainframe applications and real-time, streaming analytics platforms.  "
  },
  {
    "id": 18260,
    "package_name": "ppendemic",
    "title": "A Glimpse at the Diversity of Peru's Endemic Plants",
    "description": "Introducing a novel and updated database showcasing Peru's endemic plants. This meticulously compiled and revised botanical collection encompasses a remarkable assemblage of over 7,898 distinct species. The data for this resource was sourced from the work of Govaerts, R., Nic Lughadha, E., Black, N. et al., titled 'The World Checklist of Vascular Plants: A continuously updated resource for exploring global plant diversity', published in Sci Data 8, 215 (2021) <doi:10.1038/s41597-021-00997-6>.",
    "version": "0.1.9",
    "maintainer": "Paul E. Santos Andrade <paulefrens@gmail.com>",
    "author": "Paul E. Santos Andrade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6635-0375>),\n  Lucely L. Vilca Bustamante [aut] (ORCID:\n    <https://orcid.org/0000-0002-5559-1296>)",
    "url": "https://github.com/PaulESantos/ppendemic/",
    "bug_reports": "https://github.com/PaulESantos/ppendemic/issues/",
    "repository": "https://cran.r-project.org/package=ppendemic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ppendemic A Glimpse at the Diversity of Peru's Endemic Plants Introducing a novel and updated database showcasing Peru's endemic plants. This meticulously compiled and revised botanical collection encompasses a remarkable assemblage of over 7,898 distinct species. The data for this resource was sourced from the work of Govaerts, R., Nic Lughadha, E., Black, N. et al., titled 'The World Checklist of Vascular Plants: A continuously updated resource for exploring global plant diversity', published in Sci Data 8, 215 (2021) <doi:10.1038/s41597-021-00997-6>.  "
  },
  {
    "id": 18310,
    "package_name": "predictoR",
    "title": "Predictive Data Analysis System",
    "description": "Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning, Support Vector Machines and Bayesian Methods.",
    "version": "4.1.5",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "author": "Oldemar Rodriguez [aut, cre],\n  Diego Jim\u00e9nez [ctb, prg],\n  Andr\u00e9s Navarro [ctb, prg]",
    "url": "https://promidat.website/",
    "bug_reports": "https://github.com/PROMiDAT/predictoR/issues",
    "repository": "https://cran.r-project.org/package=predictoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "predictoR Predictive Data Analysis System Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as K-Nearest Neighbors, Decision Trees, ADA Boosting, Extreme Gradient Boosting, Random Forest, Neural Networks, Deep Learning, Support Vector Machines and Bayesian Methods.  "
  },
  {
    "id": 18312,
    "package_name": "predictsr",
    "title": "Access the 'PREDICTS' Biodiversity Database",
    "description": "Fetches the 'PREDICTS' database and relevant metadata from the Data\n    Portal at the Natural History Museum, London <https://data.nhm.ac.uk>. Data\n    were collated from over 400 existing spatial comparisons of local-scale\n    biodiversity exposed to different intensities and types of anthropogenic\n    pressures, from sites around the world. These data are described in Hudson\n    et al. (2013) <doi:10.1002/ece3.2579>.",
    "version": "0.2.0",
    "maintainer": "Connor Duffin <connor.p.duffin@gmail.com>",
    "author": "Connor Duffin [aut, cre],\n  The Trustees of The Natural History Museum, London [cph]",
    "url": "https://biodiversity-futures-lab.github.io/predictsr/,\nhttps://github.com/Biodiversity-Futures-Lab/predictsr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=predictsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "predictsr Access the 'PREDICTS' Biodiversity Database Fetches the 'PREDICTS' database and relevant metadata from the Data\n    Portal at the Natural History Museum, London <https://data.nhm.ac.uk>. Data\n    were collated from over 400 existing spatial comparisons of local-scale\n    biodiversity exposed to different intensities and types of anthropogenic\n    pressures, from sites around the world. These data are described in Hudson\n    et al. (2013) <doi:10.1002/ece3.2579>.  "
  },
  {
    "id": 18323,
    "package_name": "prenoms",
    "title": "Names Given to Babies in Quebec Between 1980 and 2020",
    "description": "A database containing the names \n  of the babies born in Quebec between 1980 and 2020.",
    "version": "0.0.1",
    "maintainer": "Marc-Andre Desautels <marc-andre.desautels@cstjean.qc.ca>",
    "author": "Marc-Andre Desautels [aut, cre]",
    "url": "<https://github.com/desautm/prenoms>",
    "bug_reports": "https://github.com/desautm/prenoms/issues",
    "repository": "https://cran.r-project.org/package=prenoms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prenoms Names Given to Babies in Quebec Between 1980 and 2020 A database containing the names \n  of the babies born in Quebec between 1980 and 2020.  "
  },
  {
    "id": 18357,
    "package_name": "primate",
    "title": "Tools and Methods for Primatological Data Science",
    "description": "Data from All the World's Primates relational SQL database and other tabular datasets are made available via drivers and connection functions. Additionally we provide several functions and examples to facilitate the merging and aggregation of these tabular inputs. ",
    "version": "0.2.0",
    "maintainer": "David Schruth <data@anthropoidea.org>",
    "author": "David Schruth [aut][cre], Marc Myers [ctb], Noel Rowe [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=primate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "primate Tools and Methods for Primatological Data Science Data from All the World's Primates relational SQL database and other tabular datasets are made available via drivers and connection functions. Additionally we provide several functions and examples to facilitate the merging and aggregation of these tabular inputs.   "
  },
  {
    "id": 18431,
    "package_name": "promr",
    "title": "Prometheus 'PromQL' Query Client for 'R'",
    "description": "A native 'R' client library for querying the 'Prometheus' \n    time-series database, using the 'PromQL' query language.",
    "version": "0.1.3",
    "maintainer": "Dom Dwyer <dom@itsallbroken.com>",
    "author": "Dom Dwyer [aut, cre]",
    "url": "https://github.com/domodwyer/promr",
    "bug_reports": "https://github.com/domodwyer/promr/issues",
    "repository": "https://cran.r-project.org/package=promr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "promr Prometheus 'PromQL' Query Client for 'R' A native 'R' client library for querying the 'Prometheus' \n    time-series database, using the 'PromQL' query language.  "
  },
  {
    "id": 18443,
    "package_name": "protein8k",
    "title": "Perform Analysis and Create Visualizations of Proteins",
    "description": "Read Protein Data Bank (PDB) files, performs its analysis, and \n    presents the result using different visualization types including 3D. The \n    package also has additional capability for handling Virus Report data from \n    the National Center for Biotechnology Information (NCBI) database.\n    Nature Structural Biology 10, 980 (2003) <doi:10.1038/nsb1203-980>.\n    US National Library of Medicine (2021) <https://www.ncbi.nlm.nih.gov/datasets/docs/reference-docs/data-reports/virus/>.",
    "version": "0.0.1",
    "maintainer": "Simon Liles <simon@quantknot.com>",
    "author": "Simon Liles",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=protein8k",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "protein8k Perform Analysis and Create Visualizations of Proteins Read Protein Data Bank (PDB) files, performs its analysis, and \n    presents the result using different visualization types including 3D. The \n    package also has additional capability for handling Virus Report data from \n    the National Center for Biotechnology Information (NCBI) database.\n    Nature Structural Biology 10, 980 (2003) <doi:10.1038/nsb1203-980>.\n    US National Library of Medicine (2021) <https://www.ncbi.nlm.nih.gov/datasets/docs/reference-docs/data-reports/virus/>.  "
  },
  {
    "id": 18466,
    "package_name": "prozor",
    "title": "Minimal Protein Set Explaining Peptide Spectrum Matches",
    "description": "Determine minimal protein set explaining\n    peptide spectrum matches. Utility functions for creating fasta amino acid databases with decoys and contaminants.\n    Peptide false discovery rate estimation for target decoy search results on psm, precursor, peptide and protein\n    level. Computing dynamic swath window sizes based on MS1 or MS2 signal distributions.",
    "version": "0.3.1",
    "maintainer": "Witold Wolski <wewolski@gmail.com>",
    "author": "Witold Wolski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6468-120X>)",
    "url": "https://github.com/protviz/prozor",
    "bug_reports": "https://github.com/protviz/prozor/issues",
    "repository": "https://cran.r-project.org/package=prozor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prozor Minimal Protein Set Explaining Peptide Spectrum Matches Determine minimal protein set explaining\n    peptide spectrum matches. Utility functions for creating fasta amino acid databases with decoys and contaminants.\n    Peptide false discovery rate estimation for target decoy search results on psm, precursor, peptide and protein\n    level. Computing dynamic swath window sizes based on MS1 or MS2 signal distributions.  "
  },
  {
    "id": 18467,
    "package_name": "prqlr",
    "title": "R Bindings for the 'prqlc' Rust Library",
    "description": "\n    Provides a function to convert 'PRQL' strings to 'SQL' strings.\n    Combined with other R functions that take 'SQL' as an argument,\n    'PRQL' can be used on R.",
    "version": "0.10.1",
    "maintainer": "Tatsuya Shima <ts1s1andn@gmail.com>",
    "author": "Tatsuya Shima [aut, cre],\n  Authors of the dependency Rust crates [aut] (see inst/AUTHORS file for\n    details)",
    "url": "https://prql.github.io/prqlc-r/, https://github.com/PRQL/prqlc-r",
    "bug_reports": "https://github.com/PRQL/prqlc-r/issues",
    "repository": "https://cran.r-project.org/package=prqlr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prqlr R Bindings for the 'prqlc' Rust Library \n    Provides a function to convert 'PRQL' strings to 'SQL' strings.\n    Combined with other R functions that take 'SQL' as an argument,\n    'PRQL' can be used on R.  "
  },
  {
    "id": 18514,
    "package_name": "psychmeta",
    "title": "Psychometric Meta-Analysis Toolkit",
    "description": "Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.",
    "version": "2.7.0",
    "maintainer": "Jeffrey A. Dahlke <jeff.dahlke.phd@gmail.com>",
    "author": "Jeffrey A. Dahlke [aut, cre],\n  Brenton M. Wiernik [aut],\n  Wesley Gardiner [ctb] (Unit tests),\n  Michael T. Brannick [ctb] (Testing),\n  Jack Kostal [ctb] (Code for reshape_mat2dat function),\n  Sean Potter [ctb] (Testing; Code for cumulative and leave1out plots),\n  John Sakaluk [ctb] (Code for funnel and forest plots),\n  Yuejia (Mandy) Teng [ctb] (Testing)",
    "url": "",
    "bug_reports": "https://github.com/psychmeta/psychmeta/issues",
    "repository": "https://cran.r-project.org/package=psychmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychmeta Psychometric Meta-Analysis Toolkit Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.  "
  },
  {
    "id": 18529,
    "package_name": "ptm",
    "title": "Analyses of Protein Post-Translational Modifications",
    "description": "Contains utilities for the analysis of post-translational modifications (PTMs) in proteins, \n    with particular emphasis on the sulfoxidation of methionine residues. Features include the ability to download, \n    filter and analyze data from the sulfoxidation database 'MetOSite'. Utilities to search and characterize S-aromatic motifs in proteins are also provided. \n    In addition, functions to analyze sequence environments around modifiable residues in proteins can be found. \n    For instance, 'ptm' allows to search for amino acids either overrepresented or avoided around the modifiable \n    residues from the proteins of interest. Functions tailored to test statistical hypothesis related to\n    these differential sequence environments are also implemented. \n    Further and detailed information regarding the methods in this package can be found in (Aledo (2020) <https://metositeptm.com>).",
    "version": "1.0.1",
    "maintainer": "Juan Carlos Aledo <caledo@uma.es>",
    "author": "Juan Carlos Aledo [aut, cre]",
    "url": "https://bitbucket.org/jcaledo/ptm,\nhttps://github.com/jcaledo/Rptm, https://metositeptm.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ptm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ptm Analyses of Protein Post-Translational Modifications Contains utilities for the analysis of post-translational modifications (PTMs) in proteins, \n    with particular emphasis on the sulfoxidation of methionine residues. Features include the ability to download, \n    filter and analyze data from the sulfoxidation database 'MetOSite'. Utilities to search and characterize S-aromatic motifs in proteins are also provided. \n    In addition, functions to analyze sequence environments around modifiable residues in proteins can be found. \n    For instance, 'ptm' allows to search for amino acids either overrepresented or avoided around the modifiable \n    residues from the proteins of interest. Functions tailored to test statistical hypothesis related to\n    these differential sequence environments are also implemented. \n    Further and detailed information regarding the methods in this package can be found in (Aledo (2020) <https://metositeptm.com>).  "
  },
  {
    "id": 18541,
    "package_name": "public.ctn0094extra",
    "title": "Helper Files for the CTN-0094 Relational Database",
    "description": "Engineered features and \"helper\" functions ancillary to the\n    'public.ctn0094data' package, extending this package for ease of use (see \n    <https://CRAN.R-project.org/package=public.ctn0094data>). This\n    'public.ctn0094data' package contains harmonized datasets from some of the\n    National Institute of Drug Abuse's Clinical Trials Network (NIDA's CTN)\n    projects. Specifically, the CTN-0094 project is to harmonize and de-identify\n    clinical trials data from the CTN-0027, CTN-0030, and CTN-51 studies for\n    opioid use disorder. This current version is built from 'public.ctn0094data'\n    v. 1.0.6.",
    "version": "1.0.4",
    "maintainer": "Gabriel Odom <gabriel.j.odom@gmail.com>",
    "author": "Gabriel Odom [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1341-4555>),\n  Raymond Balise [aut] (ORCID: <https://orcid.org/0000-0002-9856-5901>)",
    "url": "https://ctn-0094.github.io/public.ctn0094extra/",
    "bug_reports": "https://github.com/CTN-0094/public.ctn0094extra/issues",
    "repository": "https://cran.r-project.org/package=public.ctn0094extra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "public.ctn0094extra Helper Files for the CTN-0094 Relational Database Engineered features and \"helper\" functions ancillary to the\n    'public.ctn0094data' package, extending this package for ease of use (see \n    <https://CRAN.R-project.org/package=public.ctn0094data>). This\n    'public.ctn0094data' package contains harmonized datasets from some of the\n    National Institute of Drug Abuse's Clinical Trials Network (NIDA's CTN)\n    projects. Specifically, the CTN-0094 project is to harmonize and de-identify\n    clinical trials data from the CTN-0027, CTN-0030, and CTN-51 studies for\n    opioid use disorder. This current version is built from 'public.ctn0094data'\n    v. 1.0.6.  "
  },
  {
    "id": 18544,
    "package_name": "pubmedR",
    "title": "Gathering Metadata About Publications, Grants, Clinical Trials\nfrom 'PubMed' Database",
    "description": "A set of tools to extract bibliographic content from 'PubMed' database using 'NCBI' REST API <https://www.ncbi.nlm.nih.gov/home/develop/api/>.",
    "version": "0.0.3",
    "maintainer": "Massimo Aria <massimo.aria@gmail.com>",
    "author": "Massimo Aria [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8517-9411>)",
    "url": "https://github.com/massimoaria/pubmedR",
    "bug_reports": "https://github.com/massimoaria/pubmedR/issues",
    "repository": "https://cran.r-project.org/package=pubmedR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pubmedR Gathering Metadata About Publications, Grants, Clinical Trials\nfrom 'PubMed' Database A set of tools to extract bibliographic content from 'PubMed' database using 'NCBI' REST API <https://www.ncbi.nlm.nih.gov/home/develop/api/>.  "
  },
  {
    "id": 18682,
    "package_name": "qryflow",
    "title": "Execute Multi-Step 'SQL' Workflows",
    "description": "Execute multi-step 'SQL' workflows by\n    leveraging specially formatted comments to define and control\n    execution. This enables users to mix queries, commands, and metadata\n    within a single script. Results are returned as named objects for use\n    in downstream workflows.",
    "version": "0.1.0",
    "maintainer": "Christian Million <christianmillion93@gmail.com>",
    "author": "Christian Million [aut, cre, cph]",
    "url": "https://christian-million.github.io/qryflow/,\nhttps://github.com/christian-million/qryflow",
    "bug_reports": "https://github.com/christian-million/qryflow/issues",
    "repository": "https://cran.r-project.org/package=qryflow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qryflow Execute Multi-Step 'SQL' Workflows Execute multi-step 'SQL' workflows by\n    leveraging specially formatted comments to define and control\n    execution. This enables users to mix queries, commands, and metadata\n    within a single script. Results are returned as named objects for use\n    in downstream workflows.  "
  },
  {
    "id": 18689,
    "package_name": "qst",
    "title": "Store Tables in SQL Database",
    "description": "Provides functions for quickly writing (and \n  reading back) a data.frame to file in 'SQLite' format. The name \n  stands for *Store Tables using 'SQLite'*, or alternatively for *Quick \n  Store Tables* (either way, it could be pronounced as *Quest*). \n  For data.frames containing the supported data \n  types it is intended to work as a drop-in replacement for the \n  'write_*()' and 'read_*()' functions provided by similar packages. ",
    "version": "0.1.2",
    "maintainer": "Magnus Thor Torfason <m@zulutime.net>",
    "author": "Magnus Thor Torfason",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qst Store Tables in SQL Database Provides functions for quickly writing (and \n  reading back) a data.frame to file in 'SQLite' format. The name \n  stands for *Store Tables using 'SQLite'*, or alternatively for *Quick \n  Store Tables* (either way, it could be pronounced as *Quest*). \n  For data.frames containing the supported data \n  types it is intended to work as a drop-in replacement for the \n  'write_*()' and 'read_*()' functions provided by similar packages.   "
  },
  {
    "id": 18696,
    "package_name": "qtl2fst",
    "title": "Database Storage of Genotype Probabilities for QTL Mapping",
    "description": "Uses the 'fst' package to store genotype probabilities on disk for the 'qtl2' package. These genotype probabilities are a central data object for mapping quantitative trait loci (QTL), but they can be quite large. The facilities in this package enable the genotype probabilities to be stored on disk, leading to reduced memory usage with only a modest increase in computation time.",
    "version": "0.30",
    "maintainer": "Karl W Broman <broman@wisc.edu>",
    "author": "Karl W Broman [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4914-6671>),\n  Brian S Yandell [aut] (ORCID: <https://orcid.org/0000-0002-8774-9377>),\n  Petr Simecek [aut] (ORCID: <https://orcid.org/0000-0002-2922-7183>)",
    "url": "https://github.com/rqtl/qtl2fst",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qtl2fst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qtl2fst Database Storage of Genotype Probabilities for QTL Mapping Uses the 'fst' package to store genotype probabilities on disk for the 'qtl2' package. These genotype probabilities are a central data object for mapping quantitative trait loci (QTL), but they can be quite large. The facilities in this package enable the genotype probabilities to be stored on disk, leading to reduced memory usage with only a modest increase in computation time.  "
  },
  {
    "id": 18752,
    "package_name": "queryparser",
    "title": "Translate 'SQL' Queries into 'R' Expressions",
    "description": "Translate 'SQL' 'SELECT' statements into lists of 'R' expressions.",
    "version": "0.3.2",
    "maintainer": "Ian Cook <ianmcook@gmail.com>",
    "author": "Ian Cook [aut, cre],\n  Cloudera [cph]",
    "url": "https://github.com/ianmcook/queryparser",
    "bug_reports": "https://github.com/ianmcook/queryparser/issues",
    "repository": "https://cran.r-project.org/package=queryparser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "queryparser Translate 'SQL' Queries into 'R' Expressions Translate 'SQL' 'SELECT' statements into lists of 'R' expressions.  "
  },
  {
    "id": 18788,
    "package_name": "r2dii.match",
    "title": "Tools to Match Corporate Lending Portfolios with Climate Data",
    "description": "These tools implement in R a fundamental part of the software\n    'PACTA' (Paris Agreement Capital Transition Assessment), which is a\n    free tool that calculates the alignment between financial portfolios\n    and climate scenarios (<https://www.transitionmonitor.com/>). Financial\n    institutions use 'PACTA' to study how their capital allocation\n    decisions align with climate change mitigation goals. This package\n    matches data from corporate lending portfolios to asset level data\n    from market-intelligence databases (e.g. power plant capacities,\n    emission factors, etc.). This is the first step to assess if a\n    financial portfolio aligns with climate goals.",
    "version": "0.4.1",
    "maintainer": "Jacob Kastl <jacob.kastl@gmail.com>",
    "author": "Jacob Kastl [aut, cre, ctr] (ORCID:\n    <https://orcid.org/0009-0000-8281-8129>),\n  Alex Axthelm [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0001-8579-8565>),\n  Jackson Hoffart [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-8600-5042>),\n  Mauro Lepore [aut, ctr] (ORCID:\n    <https://orcid.org/0000-0002-1986-7988>),\n  Klaus Hagedorn [aut],\n  Florence Palandri [aut],\n  Evgeny Petrovsky [aut],\n  RMI [cph, fnd]",
    "url": "https://rmi-pacta.github.io/r2dii.match/,\nhttps://github.com/RMI-PACTA/r2dii.match",
    "bug_reports": "https://github.com/RMI-PACTA/r2dii.match/issues",
    "repository": "https://cran.r-project.org/package=r2dii.match",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2dii.match Tools to Match Corporate Lending Portfolios with Climate Data These tools implement in R a fundamental part of the software\n    'PACTA' (Paris Agreement Capital Transition Assessment), which is a\n    free tool that calculates the alignment between financial portfolios\n    and climate scenarios (<https://www.transitionmonitor.com/>). Financial\n    institutions use 'PACTA' to study how their capital allocation\n    decisions align with climate change mitigation goals. This package\n    matches data from corporate lending portfolios to asset level data\n    from market-intelligence databases (e.g. power plant capacities,\n    emission factors, etc.). This is the first step to assess if a\n    financial portfolio aligns with climate goals.  "
  },
  {
    "id": 18820,
    "package_name": "rATTAINS",
    "title": "Access EPA 'ATTAINS' Data",
    "description": "An R interface to United States Environmental Protection Agency (EPA)  \n    Assessment, Total Maximum Daily Load (TMDL) Tracking and Implementation System \n    ('ATTAINS') data. 'ATTAINS' is the EPA database used to track information \n    provided by states about water quality assessments conducted under federal \n    Clean Water Act requirements. ATTAINS information and API information is available at <https://www.epa.gov/waterdata/attains>.",
    "version": "1.1.0",
    "maintainer": "Michael Schramm <mpschramm@gmail.com>",
    "author": "Michael Schramm [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1876-6592>)",
    "url": "https://github.com/mps9506/rATTAINS,\nhttps://mps9506.github.io/rATTAINS/",
    "bug_reports": "https://github.com/mps9506/rATTAINS/issues",
    "repository": "https://cran.r-project.org/package=rATTAINS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rATTAINS Access EPA 'ATTAINS' Data An R interface to United States Environmental Protection Agency (EPA)  \n    Assessment, Total Maximum Daily Load (TMDL) Tracking and Implementation System \n    ('ATTAINS') data. 'ATTAINS' is the EPA database used to track information \n    provided by states about water quality assessments conducted under federal \n    Clean Water Act requirements. ATTAINS information and API information is available at <https://www.epa.gov/waterdata/attains>.  "
  },
  {
    "id": 18845,
    "package_name": "rFIA",
    "title": "Estimation of Forest Variables using the FIA Database",
    "description": "The goal of 'rFIA' is to increase the accessibility and use of the United States Forest Services (USFS) Forest Inventory and Analysis (FIA) Database by providing a user-friendly, open source toolkit to easily query and analyze FIA Data. Designed to accommodate a wide range of potential user objectives, 'rFIA' simplifies the estimation of forest variables from the FIA Database and allows all R users (experts and newcomers alike) to unlock the flexibility inherent to the Enhanced FIA design. Specifically, 'rFIA' improves accessibility to the spatial-temporal estimation capacity of the FIA Database by producing space-time indexed summaries of forest variables within user-defined population boundaries. Direct integration with other popular R packages (e.g., 'dplyr', 'tidyr', and 'sf') facilitates efficient space-time query and data summary, and supports common data representations and API design. The package implements design-based estimation procedures outlined by Bechtold & Patterson (2005) <doi:10.2737/SRS-GTR-80>, and has been validated against estimates and sampling errors produced by FIA 'EVALIDator'. Current development is focused on the implementation of spatially-enabled model-assisted and model-based estimators to improve population, change, and ratio estimates.",
    "version": "1.1.2",
    "maintainer": "Jeffrey Doser <jwdoser@ncsu.edu>",
    "author": "Jeffrey Doser [aut, cre],\n  Hunter Stanke [aut],\n  Andrew Finley [aut]",
    "url": "https://github.com/doserjef/rFIA,\nhttps://www.doserlab.com/files/rFIA",
    "bug_reports": "https://github.com/doserjef/rFIA/issues",
    "repository": "https://cran.r-project.org/package=rFIA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rFIA Estimation of Forest Variables using the FIA Database The goal of 'rFIA' is to increase the accessibility and use of the United States Forest Services (USFS) Forest Inventory and Analysis (FIA) Database by providing a user-friendly, open source toolkit to easily query and analyze FIA Data. Designed to accommodate a wide range of potential user objectives, 'rFIA' simplifies the estimation of forest variables from the FIA Database and allows all R users (experts and newcomers alike) to unlock the flexibility inherent to the Enhanced FIA design. Specifically, 'rFIA' improves accessibility to the spatial-temporal estimation capacity of the FIA Database by producing space-time indexed summaries of forest variables within user-defined population boundaries. Direct integration with other popular R packages (e.g., 'dplyr', 'tidyr', and 'sf') facilitates efficient space-time query and data summary, and supports common data representations and API design. The package implements design-based estimation procedures outlined by Bechtold & Patterson (2005) <doi:10.2737/SRS-GTR-80>, and has been validated against estimates and sampling errors produced by FIA 'EVALIDator'. Current development is focused on the implementation of spatially-enabled model-assisted and model-based estimators to improve population, change, and ratio estimates.  "
  },
  {
    "id": 18861,
    "package_name": "rKolada",
    "title": "Access Data from the 'Kolada' Database",
    "description": "Methods for downloading and processing data and metadata from 'Kolada', the official Swedish regions and municipalities database <https://www.kolada.se/>.",
    "version": "0.2.3",
    "maintainer": "Love Hansson <love.hansson@gmail.com>",
    "author": "Love Hansson [aut, cre, cph]",
    "url": "https://lchansson.github.io/rKolada/,\nhttps://github.com/lchansson/rKolada",
    "bug_reports": "https://github.com/lchansson/rKolada/issues",
    "repository": "https://cran.r-project.org/package=rKolada",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rKolada Access Data from the 'Kolada' Database Methods for downloading and processing data and metadata from 'Kolada', the official Swedish regions and municipalities database <https://www.kolada.se/>.  "
  },
  {
    "id": 18881,
    "package_name": "rPanglaoDB",
    "title": "Download and Merge Single-Cell RNA-Seq Data from the PanglaoDB\nDatabase",
    "description": "Download and merge labeled single-cell RNA-seq data from the PanglaoDB <https://panglaodb.se/> into a Seurat object.",
    "version": "0.2.1",
    "maintainer": "Daniel Osorio <daniecos@uio.no>",
    "author": "Daniel Osorio [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4424-8422>),\n  Marieke Kuijjer [aut] (ORCID: <https://orcid.org/0000-0001-6280-3130>),\n  James J. Cai [aut] (ORCID: <https://orcid.org/0000-0002-8081-6725>)",
    "url": "https://github.com/dosorio/rPanglaoDB/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rPanglaoDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rPanglaoDB Download and Merge Single-Cell RNA-Seq Data from the PanglaoDB\nDatabase Download and merge labeled single-cell RNA-seq data from the PanglaoDB <https://panglaodb.se/> into a Seurat object.  "
  },
  {
    "id": 18884,
    "package_name": "rPref",
    "title": "Database Preferences and Skyline Computation",
    "description": "Routines to select and visualize the maxima for a given strict\n    partial order. This especially includes the computation of the Pareto\n    frontier, also known as (Top-k) Skyline operator (see B\u00f6rzs\u00f6nyi, et al. \n    (2001) <doi:10.1109/ICDE.2001.914855>), and some generalizations \n    known as database preferences (see Kie\u00dfling (2002) \n    <doi:10.1016/B978-155860869-6/50035-4>).",
    "version": "1.5.0",
    "maintainer": "Patrick Roocks <mail@p-roocks.de>",
    "author": "Patrick Roocks [aut, cre]",
    "url": "https://www.p-roocks.de/rpref/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rPref",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rPref Database Preferences and Skyline Computation Routines to select and visualize the maxima for a given strict\n    partial order. This especially includes the computation of the Pareto\n    frontier, also known as (Top-k) Skyline operator (see B\u00f6rzs\u00f6nyi, et al. \n    (2001) <doi:10.1109/ICDE.2001.914855>), and some generalizations \n    known as database preferences (see Kie\u00dfling (2002) \n    <doi:10.1016/B978-155860869-6/50035-4>).  "
  },
  {
    "id": 18908,
    "package_name": "rTwig",
    "title": "Realistic Quantitative Structure Models",
    "description": "Real Twig is a method to correct branch overestimation in quantitative structure models. Overestimated cylinders are correctly tapered using measured twig diameters of corresponding tree species. Supported quantitative structure modeling software includes 'TreeQSM', 'SimpleForest', 'Treegraph', and 'aRchi'. Also included is a novel database of twig diameters and tools for fractal analysis of point clouds.",
    "version": "1.4.0",
    "maintainer": "Aidan Morales <moral169@msu.edu>",
    "author": "Aidan Morales [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0000-4513-4908>),\n  David W. MacFarlane [aut, cph]",
    "url": "https://aidanmorales.github.io/rTwig/,\nhttps://github.com/aidanmorales/rTwig",
    "bug_reports": "https://github.com/aidanmorales/rTwig/issues",
    "repository": "https://cran.r-project.org/package=rTwig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rTwig Realistic Quantitative Structure Models Real Twig is a method to correct branch overestimation in quantitative structure models. Overestimated cylinders are correctly tapered using measured twig diameters of corresponding tree species. Supported quantitative structure modeling software includes 'TreeQSM', 'SimpleForest', 'Treegraph', and 'aRchi'. Also included is a novel database of twig diameters and tools for fractal analysis of point clouds.  "
  },
  {
    "id": 18912,
    "package_name": "rYoutheria",
    "title": "Access to the YouTheria Mammal Trait Database",
    "description": "A programmatic interface to web-services of YouTheria. YouTheria is\n    an online database of mammalian trait data <http://www.utheria.org/>.",
    "version": "1.0.3",
    "maintainer": "Tom August <tomaug@ceh.ac.uk>",
    "author": "Tom August",
    "url": "https://github.com/BiologicalRecordsCentre/rYoutheria",
    "bug_reports": "https://github.com/biologicalrecordscentre/rYoutheria/issues",
    "repository": "https://cran.r-project.org/package=rYoutheria",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rYoutheria Access to the YouTheria Mammal Trait Database A programmatic interface to web-services of YouTheria. YouTheria is\n    an online database of mammalian trait data <http://www.utheria.org/>.  "
  },
  {
    "id": 18964,
    "package_name": "randomGODB",
    "title": "Random GO Database",
    "description": "The Gene Ontology (GO) Consortium <https://geneontology.org/> organizes genes\n  into hierarchical categories based on biological process (BP), molecular function (MF) and\n  cellular component (CC, i.e., subcellular localization). Tools such as 'GoMiner' (see Zeeberg, B.R.,\n  Feng, W., Wang, G. et al. (2003) <doi:10.1186/gb-2003-4-4-r28>) can leverage GO to perform\n  ontological analysis of microarray and proteomics studies, typically generating a list of\n  significant functional categories. The significance is traditionally determined by randomizing\n  the input gene list to computing the false discovery rate (FDR) of the enrichment p-value for\n  each category. We explore here the novel alternative of randomizing the GO database rather than\n  the gene list.",
    "version": "1.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=randomGODB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "randomGODB Random GO Database The Gene Ontology (GO) Consortium <https://geneontology.org/> organizes genes\n  into hierarchical categories based on biological process (BP), molecular function (MF) and\n  cellular component (CC, i.e., subcellular localization). Tools such as 'GoMiner' (see Zeeberg, B.R.,\n  Feng, W., Wang, G. et al. (2003) <doi:10.1186/gb-2003-4-4-r28>) can leverage GO to perform\n  ontological analysis of microarray and proteomics studies, typically generating a list of\n  significant functional categories. The significance is traditionally determined by randomizing\n  the input gene list to computing the false discovery rate (FDR) of the enrichment p-value for\n  each category. We explore here the novel alternative of randomizing the GO database rather than\n  the gene list.  "
  },
  {
    "id": 19027,
    "package_name": "ratdat",
    "title": "Portal Project Teaching Database",
    "description": "A simplified version of the Portal Project Database designed for\n    teaching. It provides a real world example of life-history, population, and\n    ecological data, with sufficient complexity to teach many aspects of data\n    analysis and management, but with many complexities removed to allow\n    students to focus on the core ideas and skills being taught. The full\n    database (which should be used for research) is available at\n    <https://github.com/weecology/PortalData>.",
    "version": "1.1.0",
    "maintainer": "Ethan P. White <ethan@weecology.org>",
    "author": "S.K. Morgan Ernest [aut] (ORCID:\n    <https://orcid.org/0000-0002-6026-8530>),\n  James H. Brown [aut],\n  Thomas J. Valone [aut],\n  Michael J. Culshaw-Maurer [aut] (ORCID:\n    <https://orcid.org/0000-0003-2205-8679>),\n  Ethan P. White [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6728-7745>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ratdat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ratdat Portal Project Teaching Database A simplified version of the Portal Project Database designed for\n    teaching. It provides a real world example of life-history, population, and\n    ecological data, with sufficient complexity to teach many aspects of data\n    analysis and management, but with many complexities removed to allow\n    students to focus on the core ideas and skills being taught. The full\n    database (which should be used for research) is available at\n    <https://github.com/weecology/PortalData>.  "
  },
  {
    "id": 19037,
    "package_name": "rattle",
    "title": "Graphical User Interface for Data Science in R",
    "description": "The R Analytic Tool To Learn Easily (Rattle) provides a \n  collection of utilities functions for the data scientist. A\n  Gnome (RGtk2) based graphical interface is included with \n  the aim to provide a simple and intuitive introduction to R \n  for data science, allowing a user to quickly load data from a CSV file \n  (or via ODBC), transform and explore the data, \n  build and evaluate models, and export models as PMML (predictive\n  modelling markup language) or as scores. A key aspect of the GUI\n  is that all R commands are logged and commented through the log tab.\n  This can be saved as a standalone R script file and as\n  an aid for the user to \n  learn R or to copy-and-paste directly into R itself.\n  Note that RGtk2 and cairoDevice have been archived on CRAN.\n  See <https://rattle.togaware.com> for installation instructions.",
    "version": "5.5.1",
    "maintainer": "Graham Williams <Graham.Williams@togaware.com>",
    "author": "Graham Williams [aut, cph, cre],\n  Mark Vere Culp [cph],\n  Ed Cox [ctb],\n  Anthony Nolan [ctb],\n  Denis White [cph],\n  Daniele Medri [ctb],\n  Akbar Waljee [ctb] (OOB AUC for Random Forest),\n  Brian Ripley [cph] (print.summary.nnet),\n  Jose Magana [ctb] (ggpairs plots),\n  Surendra Tipparaju [ctb] (initial RevoScaleR/XDF),\n  Durga Prasad Chappidi [ctb] (initial RevoScaleR/XDF),\n  Dinesh Manyam Venkata [ctb] (initial RevoScaleR/XDF),\n  Mrinal Chakraborty [ctb] (initial RevoScaleR/XDF),\n  Fang Zhou [ctb] (initial xgboost),\n  Cameron Chisholm [ctb] (risk plot on risk chart)",
    "url": "https://rattle.togaware.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rattle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rattle Graphical User Interface for Data Science in R The R Analytic Tool To Learn Easily (Rattle) provides a \n  collection of utilities functions for the data scientist. A\n  Gnome (RGtk2) based graphical interface is included with \n  the aim to provide a simple and intuitive introduction to R \n  for data science, allowing a user to quickly load data from a CSV file \n  (or via ODBC), transform and explore the data, \n  build and evaluate models, and export models as PMML (predictive\n  modelling markup language) or as scores. A key aspect of the GUI\n  is that all R commands are logged and commented through the log tab.\n  This can be saved as a standalone R script file and as\n  an aid for the user to \n  learn R or to copy-and-paste directly into R itself.\n  Note that RGtk2 and cairoDevice have been archived on CRAN.\n  See <https://rattle.togaware.com> for installation instructions.  "
  },
  {
    "id": 19048,
    "package_name": "raymolecule",
    "title": "Parse and Render Molecular Structures in 3D",
    "description": "Downloads and parses 'SDF' (Structural Description Format) and 'PDB' (Protein Database) files for 3D rendering.",
    "version": "0.5.3",
    "maintainer": "Tyler Morgan-Wall <tylermw@gmail.com>",
    "author": "Tyler Morgan-Wall [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-3131-3814>)",
    "url": "http://www.raymolecule.com/,\nhttps://github.com/tylermorganwall/raymolecule",
    "bug_reports": "https://github.com/tylermorganwall/raymolecule/issues",
    "repository": "https://cran.r-project.org/package=raymolecule",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raymolecule Parse and Render Molecular Structures in 3D Downloads and parses 'SDF' (Structural Description Format) and 'PDB' (Protein Database) files for 3D rendering.  "
  },
  {
    "id": 19065,
    "package_name": "rbioapi",
    "title": "User-Friendly R Interface to Biologic Web Services' API",
    "description": "Currently fully supports Enrichr, JASPAR, miEAA, PANTHER,\n    Reactome, STRING, and UniProt! The goal of rbioapi is to provide a\n    user-friendly and consistent interface to biological databases and\n    services. In a way that insulates the user from the technicalities of\n    using web services API and creates a unified and easy-to-use interface\n    to biological and medical web services. This is an ongoing project; New\n    databases and services will be added periodically. Feel free to\n    suggest any databases or services you often use.",
    "version": "0.8.3",
    "maintainer": "Moosa Rezwani <moosa.rezwani@gmail.com>",
    "author": "Moosa Rezwani [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6325-4444>)",
    "url": "https://rbioapi.moosa-r.com, https://github.com/moosa-r/rbioapi",
    "bug_reports": "https://github.com/moosa-r/rbioapi/issues",
    "repository": "https://cran.r-project.org/package=rbioapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rbioapi User-Friendly R Interface to Biologic Web Services' API Currently fully supports Enrichr, JASPAR, miEAA, PANTHER,\n    Reactome, STRING, and UniProt! The goal of rbioapi is to provide a\n    user-friendly and consistent interface to biological databases and\n    services. In a way that insulates the user from the technicalities of\n    using web services API and creates a unified and easy-to-use interface\n    to biological and medical web services. This is an ongoing project; New\n    databases and services will be added periodically. Feel free to\n    suggest any databases or services you often use.  "
  },
  {
    "id": 19100,
    "package_name": "rchroma",
    "title": "A Client for 'ChromaDB'",
    "description": "Client for 'ChromaDB', a vector database for storing and querying embeddings. This package provides a convenient interface to interact with the REST API of 'ChromaDB' <https://docs.trychroma.com>.",
    "version": "0.2.0",
    "maintainer": "David Schoch <david@schochastics.net>",
    "author": "David Schoch [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2952-4812>),\n  Christoph Sax [aut],\n  cynkra LLC [cph]",
    "url": "https://github.com/cynkra/rchroma",
    "bug_reports": "https://github.com/cynkra/rchroma/issues",
    "repository": "https://cran.r-project.org/package=rchroma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rchroma A Client for 'ChromaDB' Client for 'ChromaDB', a vector database for storing and querying embeddings. This package provides a convenient interface to interact with the REST API of 'ChromaDB' <https://docs.trychroma.com>.  "
  },
  {
    "id": 19119,
    "package_name": "rcprd",
    "title": "Extraction and Management of Clinical Practice Research Datalink\nData",
    "description": "Simplify the process of extracting and processing Clinical Practice\n  Research Datalink (CPRD) data in order to build datasets ready for statistical\n  analysis. This process is difficult in 'R', as the raw data is very large and \n  cannot be read into the R workspace. 'rcprd' utilises 'RSQLite' to create \n  'SQLite' databases which are stored on the hard disk. These are then  queried \n  to extract the required information for a cohort of interest, and create \n  datasets ready for statistical analysis. The  processes follow closely that \n  from the 'rEHR' package, see Springate et al., (2017) \n  <doi:10.1371/journal.pone.0171784>.",
    "version": "0.0.2",
    "maintainer": "Alexander Pate <alexander.pate@manchester.ac.uk>",
    "author": "Alexander Pate [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0849-3458>)",
    "url": "https://alexpate30.github.io/rcprd/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rcprd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcprd Extraction and Management of Clinical Practice Research Datalink\nData Simplify the process of extracting and processing Clinical Practice\n  Research Datalink (CPRD) data in order to build datasets ready for statistical\n  analysis. This process is difficult in 'R', as the raw data is very large and \n  cannot be read into the R workspace. 'rcprd' utilises 'RSQLite' to create \n  'SQLite' databases which are stored on the hard disk. These are then  queried \n  to extract the required information for a cohort of interest, and create \n  datasets ready for statistical analysis. The  processes follow closely that \n  from the 'rEHR' package, see Springate et al., (2017) \n  <doi:10.1371/journal.pone.0171784>.  "
  },
  {
    "id": 19167,
    "package_name": "read.gb",
    "title": "Open GenBank Files",
    "description": "Opens complete record(s) with .gb extension from the NCBI/GenBank Nucleotide database and returns a list containing shaped record(s). These kind of files contains detailed records of DNA samples (locus, organism, type of sequence, source of the sequence...). An example of record can be found at <https://www.ncbi.nlm.nih.gov/nuccore/HE799070>.",
    "version": "2.2",
    "maintainer": "Robin Mercier <robin.largon.mercier@hotmail.fr>",
    "author": "Robin Mercier [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=read.gb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "read.gb Open GenBank Files Opens complete record(s) with .gb extension from the NCBI/GenBank Nucleotide database and returns a list containing shaped record(s). These kind of files contains detailed records of DNA samples (locus, organism, type of sequence, source of the sequence...). An example of record can be found at <https://www.ncbi.nlm.nih.gov/nuccore/HE799070>.  "
  },
  {
    "id": 19215,
    "package_name": "receptiviti",
    "title": "Text Analysis Through the 'Receptiviti' API",
    "description": "Sends texts to the <https://www.receptiviti.com> API to be scored,\n    and facilitates the creation of custom norms and local results databases.",
    "version": "0.2.0",
    "maintainer": "Kent English <kenglish@receptiviti.com>",
    "author": "Receptiviti Inc. [fnd, cph],\n  Kent English [cre],\n  Micah Iserman [aut, ctr]",
    "url": "https://receptiviti.github.io/receptiviti-r/,\nhttps://github.com/Receptiviti/receptiviti-r",
    "bug_reports": "https://github.com/Receptiviti/receptiviti-r/issues",
    "repository": "https://cran.r-project.org/package=receptiviti",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "receptiviti Text Analysis Through the 'Receptiviti' API Sends texts to the <https://www.receptiviti.com> API to be scored,\n    and facilitates the creation of custom norms and local results databases.  "
  },
  {
    "id": 19243,
    "package_name": "redatamx",
    "title": "R Interface to 'Redatam' Library",
    "description": "Provides an API to work with 'Redatam' (see <https://redatam.org>) \n  databases in both formats: 'RXDB' (new format) and 'DICX' (old format) and running \n  'Redatam' programs written in 'SPC' language. It's a wrapper around 'Redatam' \n  core and provides functions to open/close a database (redatam_open()/redatam_close()), \n  list entities and variables from the database (redatam_entities(), redatam_variables()) \n  and execute a 'SPC' program and gets the results as data frames \n  (redatam_query(), redatam_run()).",
    "version": "1.2.1",
    "maintainer": "Jaime Salvador <jaime.salvador@ideasybits.com>",
    "author": "Jaime Salvador [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3564-8929>),\n  CELADE [cph]",
    "url": "https://ideasybits.github.io/redatamx4r/,\nhttps://github.com/ideasybits/redatamx4r/",
    "bug_reports": "https://github.com/ideasybits/redatamx4r/issues",
    "repository": "https://cran.r-project.org/package=redatamx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redatamx R Interface to 'Redatam' Library Provides an API to work with 'Redatam' (see <https://redatam.org>) \n  databases in both formats: 'RXDB' (new format) and 'DICX' (old format) and running \n  'Redatam' programs written in 'SPC' language. It's a wrapper around 'Redatam' \n  core and provides functions to open/close a database (redatam_open()/redatam_close()), \n  list entities and variables from the database (redatam_entities(), redatam_variables()) \n  and execute a 'SPC' program and gets the results as data frames \n  (redatam_query(), redatam_run()).  "
  },
  {
    "id": 19245,
    "package_name": "redcapAPI",
    "title": "Interface to 'REDCap'",
    "description": "Access data stored in 'REDCap' databases using the Application\n    Programming Interface (API).  'REDCap' (Research Electronic Data CAPture;\n    <https://projectredcap.org>, Harris, et al. (2009) <doi:10.1016/j.jbi.2008.08.010>, \n    Harris, et al. (2019) <doi:10.1016/j.jbi.2019.103208>) is\n    a web application for building and managing online surveys and databases\n    developed at Vanderbilt University.  The API allows users to access data\n    and project meta data (such as the data dictionary) from the web\n    programmatically. The 'redcapAPI' package facilitates the process of\n    accessing data with options to prepare an analysis-ready data set\n    consistent with the definitions in a database's data dictionary.",
    "version": "2.11.5",
    "maintainer": "Shawn Garbett <shawn.garbett@vumc.org>",
    "author": "Benjamin Nutter [ctb, aut],\n  Shawn Garbett [cre, ctb] (ORCID:\n    <https://orcid.org/0000-0003-4079-5621>),\n  Savannah Obregon [ctb],\n  Thomas Obadia [ctb],\n  Marcus Lehr [ctb],\n  Brian High [ctb],\n  Stephen Lane [ctb],\n  Will Beasley [ctb],\n  Will Gray [ctb],\n  Nick Kennedy [ctb],\n  Tan Hsi-Nien [ctb],\n  Jeffrey Horner [aut],\n  Jeremy Stephens [ctb],\n  Cole Beck [ctb],\n  Bradley Johnson [ctb],\n  Philip Chase [ctb],\n  Paddy Tobias [ctb],\n  Michael Chirico [ctb],\n  William Sharp [ctb]",
    "url": "https://github.com/vubiostat/redcapAPI",
    "bug_reports": "https://github.com/vubiostat/redcapAPI/issues",
    "repository": "https://cran.r-project.org/package=redcapAPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redcapAPI Interface to 'REDCap' Access data stored in 'REDCap' databases using the Application\n    Programming Interface (API).  'REDCap' (Research Electronic Data CAPture;\n    <https://projectredcap.org>, Harris, et al. (2009) <doi:10.1016/j.jbi.2008.08.010>, \n    Harris, et al. (2019) <doi:10.1016/j.jbi.2019.103208>) is\n    a web application for building and managing online surveys and databases\n    developed at Vanderbilt University.  The API allows users to access data\n    and project meta data (such as the data dictionary) from the web\n    programmatically. The 'redcapAPI' package facilitates the process of\n    accessing data with options to prepare an analysis-ready data set\n    consistent with the definitions in a database's data dictionary.  "
  },
  {
    "id": 19253,
    "package_name": "redquack",
    "title": "Transfer 'REDCap' Data to Database",
    "description": "Transfer 'REDCap' (Research Electronic Data Capture) data to a database,\n    specifically optimized for 'DuckDB'. \n    Processes data in chunks to handle large datasets without exceeding available memory. \n    Features include data labeling, coded value conversion, and hearing a \"quack\" sound on success.",
    "version": "0.3.0",
    "maintainer": "Dylan Pieper <dylanpieper@gmail.com>",
    "author": "Dylan Pieper [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=redquack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "redquack Transfer 'REDCap' Data to Database Transfer 'REDCap' (Research Electronic Data Capture) data to a database,\n    specifically optimized for 'DuckDB'. \n    Processes data in chunks to handle large datasets without exceeding available memory. \n    Features include data labeling, coded value conversion, and hearing a \"quack\" sound on success.  "
  },
  {
    "id": 19266,
    "package_name": "refseqR",
    "title": "Common Computational Operations Working with RefSeq Entries\n(GenBank)",
    "description": "Fetches NCBI data (RefSeq <https://www.ncbi.nlm.nih.gov/refseq/> database) and provides an environment to  \n    extract information at the level of gene, mRNA or protein accessions. ",
    "version": "1.1.5",
    "maintainer": "Jose V. Die <jose.die@uco.es>",
    "author": "Jose V. Die [aut, cre] (ORCID: <https://orcid.org/0000-0002-7506-8590>),\n  Llu\u00eds Revilla Sancho [ctb] (ORCID:\n    <https://orcid.org/0000-0001-9747-2570>)",
    "url": "https://github.com/jdieramon/refseqR",
    "bug_reports": "https://github.com/jdieramon/refseqR/issues",
    "repository": "https://cran.r-project.org/package=refseqR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "refseqR Common Computational Operations Working with RefSeq Entries\n(GenBank) Fetches NCBI data (RefSeq <https://www.ncbi.nlm.nih.gov/refseq/> database) and provides an environment to  \n    extract information at the level of gene, mRNA or protein accessions.   "
  },
  {
    "id": 19269,
    "package_name": "refugees",
    "title": "UNHCR Refugee Population Statistics Database",
    "description": "The Refugee Population Statistics Database published by\n    The Office of The United Nations High Commissioner for Refugees (UNHCR)\n    contains information about forcibly displaced populations\n    spanning more than 70 years of statistical activities.\n    It covers displaced populations such as refugees, asylum-seekers and\n    internally displaced people, including their demographics.\n    Stateless people are also included, most of who have never been displaced.\n    The database also reflects the different types of solutions\n    for displaced populations such as repatriation or resettlement. \n    More information on the data and methodology can be found on\n    the UNHCR Refugee Data Finder <https://www.unhcr.org/refugee-statistics/>.",
    "version": "2025.06.0",
    "maintainer": "Janis Kreuder <kreuder@unhcr.org>",
    "author": "Hisham Galal [aut],\n  Ahmadou Dicko [aut],\n  Janis Kreuder [cre],\n  UNHCR [cph]",
    "url": "https://populationstatistics.github.io/refugees/,\nhttps://github.com/PopulationStatistics/refugees",
    "bug_reports": "https://github.com/PopulationStatistics/refugees/issues",
    "repository": "https://cran.r-project.org/package=refugees",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "refugees UNHCR Refugee Population Statistics Database The Refugee Population Statistics Database published by\n    The Office of The United Nations High Commissioner for Refugees (UNHCR)\n    contains information about forcibly displaced populations\n    spanning more than 70 years of statistical activities.\n    It covers displaced populations such as refugees, asylum-seekers and\n    internally displaced people, including their demographics.\n    Stateless people are also included, most of who have never been displaced.\n    The database also reflects the different types of solutions\n    for displaced populations such as repatriation or resettlement. \n    More information on the data and methodology can be found on\n    the UNHCR Refugee Data Finder <https://www.unhcr.org/refugee-statistics/>.  "
  },
  {
    "id": 19297,
    "package_name": "regressoR",
    "title": "Regression Data Analysis System",
    "description": "Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as linear regression, penalized regression, k-nearest neighbors, decision trees, ada boosting, extreme gradient boosting, random forest, neural networks, deep learning and support vector machines.",
    "version": "4.0.4",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "author": "Oldemar Rodriguez [aut, cre],\n  Andres Navarro D. [ctb, prg],\n  Diego Jimenez A. [ctb, prg],\n  Ariel Arroyo S. [ctb, prg],\n  Joseline Quiros M. [ctb, prg]",
    "url": "https://promidat.website/",
    "bug_reports": "https://github.com/PROMiDAT/predictoR/issues",
    "repository": "https://cran.r-project.org/package=regressoR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "regressoR Regression Data Analysis System Perform a supervised data analysis on a database through a 'shiny' graphical interface. It includes methods such as linear regression, penalized regression, k-nearest neighbors, decision trees, ada boosting, extreme gradient boosting, random forest, neural networks, deep learning and support vector machines.  "
  },
  {
    "id": 19373,
    "package_name": "reptiledb.data",
    "title": "Reptile Database Data",
    "description": "Provides easy access to 'The Reptile Database', a comprehensive catalogue of all living reptile species and their classification. This package includes taxonomic data for over 10,000 reptile species, approximately 2,800 of which are subspecies, covering all extant reptiles. The dataset features taxonomic names, synonyms, distribution data, type specimens, and literature references, making it ready for research and analysis. Data is sourced from 'The Reptile Database' <http://www.reptile-database.org/>.",
    "version": "0.0.0.1",
    "maintainer": "Paul Efren Santos Andrade <paulefrens@gmail.com>",
    "author": "Paul Efren Santos Andrade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6635-0375>)",
    "url": "https://github.com/PaulESantos/reptiledb.data",
    "bug_reports": "https://github.com/PaulESantos/reptiledb.data/issues",
    "repository": "https://cran.r-project.org/package=reptiledb.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reptiledb.data Reptile Database Data Provides easy access to 'The Reptile Database', a comprehensive catalogue of all living reptile species and their classification. This package includes taxonomic data for over 10,000 reptile species, approximately 2,800 of which are subspecies, covering all extant reptiles. The dataset features taxonomic names, synonyms, distribution data, type specimens, and literature references, making it ready for research and analysis. Data is sourced from 'The Reptile Database' <http://www.reptile-database.org/>.  "
  },
  {
    "id": 19374,
    "package_name": "reptiledbr",
    "title": "Interface to the Reptile Database for Querying and Retrieving\nTaxonomic Data",
    "description": "Provides tools to search, access, and format taxonomic information \n    from the Reptile Database (<http://reptile-database.org>) directly within R. \n    Users can retrieve species-level data, distribution, etymology, synonyms, \n    common names, and other relevant information for reptiles. Designed for \n    taxonomists, ecologists, and biodiversity researchers.",
    "version": "0.0.1",
    "maintainer": "Paul Efren Santos Andrade <paulefrens@gmail.com>",
    "author": "Paul Efren Santos Andrade [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6635-0375>)",
    "url": "https://github.com/PaulESantos/reptiledbr,\nhttps://paulesantos.github.io/reptiledbr/",
    "bug_reports": "https://github.com/PaulESantos/reptiledbr/issues",
    "repository": "https://cran.r-project.org/package=reptiledbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reptiledbr Interface to the Reptile Database for Querying and Retrieving\nTaxonomic Data Provides tools to search, access, and format taxonomic information \n    from the Reptile Database (<http://reptile-database.org>) directly within R. \n    Users can retrieve species-level data, distribution, etymology, synonyms, \n    common names, and other relevant information for reptiles. Designed for \n    taxonomists, ecologists, and biodiversity researchers.  "
  },
  {
    "id": 19397,
    "package_name": "resourcecodedata",
    "title": "Resourcecode Database Configuration Data",
    "description": "Includes Resourcecode hindcast database (see\n    <https://resourcecode.ifremer.fr>) configuration data: nodes locations\n    for both the sea-state parameters and the spectra data; examples of\n    time series of 1D and 2D surface elevation variance spectral density.",
    "version": "1.0.0",
    "maintainer": "Nicolas Raillard <nicolas.raillard@ifremer.fr>",
    "author": "Nicolas Raillard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3385-5104>)",
    "url": "https://github.com/Resourcecode-project/r-resourcecodedata/,\nhttps://resourcecode-project.r-universe.dev/resourcecodedata/,\nhttps://resourcecode-project.github.io/r-resourcecodedata/",
    "bug_reports": "https://github.com/Resourcecode-project/r-resourcecodedata/issues",
    "repository": "https://cran.r-project.org/package=resourcecodedata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "resourcecodedata Resourcecode Database Configuration Data Includes Resourcecode hindcast database (see\n    <https://resourcecode.ifremer.fr>) configuration data: nodes locations\n    for both the sea-state parameters and the spectra data; examples of\n    time series of 1D and 2D surface elevation variance spectral density.  "
  },
  {
    "id": 19403,
    "package_name": "restatapi",
    "title": "Search and Retrieve Data from Eurostat Database",
    "description": "Eurostat is the statistical office of the European Union and provides high quality statistics for Europe.\n             Large set of the data is disseminated through the Eurostat database (<https://ec.europa.eu/eurostat/web/main/data/database>). \n             The tools are using the REST API with the Statistical Data and Metadata eXchange (SDMX) Web Services \n             (<https://wikis.ec.europa.eu/pages/viewpage.action?pageId=44165555>) to search and download data from \n             the Eurostat database using the SDMX standard. ",
    "version": "0.24.2",
    "maintainer": "M\u00e1ty\u00e1s M\u00e9sz\u00e1ros <matyas.meszaros@ec.europa.eu>",
    "author": "M\u00e1ty\u00e1s M\u00e9sz\u00e1ros [aut, cre],\n  Sebastian Weinand [ctb]",
    "url": "https://github.com/eurostat/restatapi",
    "bug_reports": "https://github.com/eurostat/restatapi/issues",
    "repository": "https://cran.r-project.org/package=restatapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "restatapi Search and Retrieve Data from Eurostat Database Eurostat is the statistical office of the European Union and provides high quality statistics for Europe.\n             Large set of the data is disseminated through the Eurostat database (<https://ec.europa.eu/eurostat/web/main/data/database>). \n             The tools are using the REST API with the Statistical Data and Metadata eXchange (SDMX) Web Services \n             (<https://wikis.ec.europa.eu/pages/viewpage.action?pageId=44165555>) to search and download data from \n             the Eurostat database using the SDMX standard.   "
  },
  {
    "id": 19404,
    "package_name": "restatis",
    "title": "R Wrapper to Access a Wide Range of Germany's Federal\nStatistical System Databases Based on the GENESIS Web Service\nRESTful API of the German Federal Statistical Office\n(Statistisches Bundesamt/Destatis)",
    "description": "A RESTful API wrapper for accessing the GENESIS database of\n    the German Federal Statistical Office (Destatis) as well as its Census \n    Database and the database of Germany's regional statistics. Supports data \n    search functions, credential management, result caching, and handling \n    remote background jobs for large datasets.",
    "version": "0.3.0",
    "maintainer": "Yannik Buhl <ybuhl@posteo.de>",
    "author": "Yannik Buhl [aut, cre],\n  Zoran Kovacevic [aut] (ORCID: <https://orcid.org/0009-0002-0156-0862>),\n  Dorian Le Jeune [aut],\n  Long Nguyen [aut] (ORCID: <https://orcid.org/0000-0001-8878-7386>),\n  Johannes Ritter [aut]",
    "url": "https://correlaid.github.io/restatis/,\nhttps://github.com/CorrelAid/restatis",
    "bug_reports": "https://github.com/CorrelAid/restatis/issues",
    "repository": "https://cran.r-project.org/package=restatis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "restatis R Wrapper to Access a Wide Range of Germany's Federal\nStatistical System Databases Based on the GENESIS Web Service\nRESTful API of the German Federal Statistical Office\n(Statistisches Bundesamt/Destatis) A RESTful API wrapper for accessing the GENESIS database of\n    the German Federal Statistical Office (Destatis) as well as its Census \n    Database and the database of Germany's regional statistics. Supports data \n    search functions, credential management, result caching, and handling \n    remote background jobs for large datasets.  "
  },
  {
    "id": 19482,
    "package_name": "rgugik",
    "title": "Search and Retrieve Spatial Data from 'GUGiK'",
    "description": "Automatic open data acquisition from resources of Polish Head Office\n    of Geodesy and Cartography ('G\u0142\u00f3wny Urz\u0105d Geodezji i Kartografii')\n    (<https://www.gov.pl/web/gugik>).\n    Available datasets include various types of numeric, raster and vector data,\n    such as orthophotomaps, digital elevation models (digital terrain models,\n    digital surface model, point clouds), state register of borders, spatial\n    databases, geometries of cadastral parcels, 3D models of buildings, and more.\n    It is also possible to geocode addresses or objects using the geocodePL_get()\n    function.",
    "version": "0.4.2",
    "maintainer": "Krzysztof Dyba <adres7@gmail.com>",
    "author": "Krzysztof Dyba [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8614-3816>),\n  Jakub Nowosad [aut] (ORCID: <https://orcid.org/0000-0002-1057-3721>),\n  Maciej Ber\u0119sewicz [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8281-4301>),\n  Grzegorz Sapijaszko [ctb],\n  GUGiK [ctb] (source of the data)",
    "url": "https://kadyb.github.io/rgugik/, https://github.com/kadyb/rgugik",
    "bug_reports": "https://github.com/kadyb/rgugik/issues",
    "repository": "https://cran.r-project.org/package=rgugik",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rgugik Search and Retrieve Spatial Data from 'GUGiK' Automatic open data acquisition from resources of Polish Head Office\n    of Geodesy and Cartography ('G\u0142\u00f3wny Urz\u0105d Geodezji i Kartografii')\n    (<https://www.gov.pl/web/gugik>).\n    Available datasets include various types of numeric, raster and vector data,\n    such as orthophotomaps, digital elevation models (digital terrain models,\n    digital surface model, point clouds), state register of borders, spatial\n    databases, geometries of cadastral parcels, 3D models of buildings, and more.\n    It is also possible to geocode addresses or objects using the geocodePL_get()\n    function.  "
  },
  {
    "id": 19542,
    "package_name": "ritalic",
    "title": "Interface to the ITALIC Database of Lichen Biodiversity",
    "description": "A programmatic interface to the Web Service methods provided by ITALIC (<https://italic.units.it>).\n    ITALIC is a database of lichen data in Italy and bordering European countries. 'ritalic' includes functions for retrieving information\n    about lichen scientific names, geographic distribution, ecological data, morpho-functional traits and identification keys.\n    More information about the data is available at <https://italic.units.it/?procedure=base&t=59&c=60>.\n    The API documentation is available at <https://italic.units.it/?procedure=api>.",
    "version": "0.11.0",
    "maintainer": "Matteo Conti <matt.ciao@gmail.com>",
    "author": "Matteo Conti [aut, cre] (ORCID:\n    <https://orcid.org/0009-0003-4917-2639>),\n  Luana Francesconi [aut],\n  Alice Musina [aut],\n  Luca Di Nuzzo [aut],\n  Gabriele Gheza [aut],\n  Chiara Pistocchi [aut],\n  Juri Nascimbene [aut],\n  Pier Luigi Nimis [aut],\n  Stefano Martellos [aut]",
    "url": "https://github.com/plant-data/ritalic",
    "bug_reports": "https://github.com/plant-data/ritalic/issues",
    "repository": "https://cran.r-project.org/package=ritalic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ritalic Interface to the ITALIC Database of Lichen Biodiversity A programmatic interface to the Web Service methods provided by ITALIC (<https://italic.units.it>).\n    ITALIC is a database of lichen data in Italy and bordering European countries. 'ritalic' includes functions for retrieving information\n    about lichen scientific names, geographic distribution, ecological data, morpho-functional traits and identification keys.\n    More information about the data is available at <https://italic.units.it/?procedure=base&t=59&c=60>.\n    The API documentation is available at <https://italic.units.it/?procedure=api>.  "
  },
  {
    "id": 19583,
    "package_name": "rlowdb",
    "title": "Lightweight JSON-Based Database",
    "description": "The goal of 'rlowdb' is to provide a lightweight, file-based JSON database. \n    Inspired by 'LowDB' in 'JavaScript', it generates an intuitive interface for\n    storing, retrieving, updating, and querying structured data without requiring a full-fledged database system. \n    Ideal for prototyping, small-scale applications, and lightweight data management needs.",
    "version": "0.2.0",
    "maintainer": "Mohamed El Fodil Ihaddaden <ihaddaden.fodeil@gmail.com>",
    "author": "Mohamed El Fodil Ihaddaden [aut, cre],\n  lowdb developers [ctb, cph] (developers of the lowdb package)",
    "url": "https://github.com/feddelegrand7/rlowdb",
    "bug_reports": "https://github.com/feddelegrand7/rlowdb/issues",
    "repository": "https://cran.r-project.org/package=rlowdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rlowdb Lightweight JSON-Based Database The goal of 'rlowdb' is to provide a lightweight, file-based JSON database. \n    Inspired by 'LowDB' in 'JavaScript', it generates an intuitive interface for\n    storing, retrieving, updating, and querying structured data without requiring a full-fledged database system. \n    Ideal for prototyping, small-scale applications, and lightweight data management needs.  "
  },
  {
    "id": 19708,
    "package_name": "rocker",
    "title": "Database Interface Class",
    "description": "'R6' class interface for handling relational database connections using 'DBI' package as backend.\n  The class allows handling of connections to e.g. PostgreSQL, MariaDB and SQLite.\n  The purpose is having an intuitive object allowing straightforward handling of SQL databases.",
    "version": "0.3.2",
    "maintainer": "Nikolaus Pawlowski <niko@fr33.net>",
    "author": "Nikolaus Pawlowski [aut, cre, cph]",
    "url": "https://github.com/nikolaus77/rocker",
    "bug_reports": "https://github.com/nikolaus77/rocker/issues",
    "repository": "https://cran.r-project.org/package=rocker",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rocker Database Interface Class 'R6' class interface for handling relational database connections using 'DBI' package as backend.\n  The class allows handling of connections to e.g. PostgreSQL, MariaDB and SQLite.\n  The purpose is having an intuitive object allowing straightforward handling of SQL databases.  "
  },
  {
    "id": 19721,
    "package_name": "rolap",
    "title": "Obtaining Star Databases from Flat Tables",
    "description": "Data in multidimensional systems is obtained from operational\n    systems and is transformed to adapt it to the new structure.\n    Frequently, the operations to be performed aim to transform a flat\n    table into a ROLAP (Relational On-Line Analytical Processing) star\n    database. The main objective of the package is to allow the definition\n    of these transformations easily. The implementation of the\n    multidimensional database obtained can be exported to work with\n    multidimensional analysis tools on spreadsheets or relational\n    databases.",
    "version": "2.5.2",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/rolap/,\nhttps://github.com/josesamos/rolap",
    "bug_reports": "https://github.com/josesamos/rolap/issues",
    "repository": "https://cran.r-project.org/package=rolap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rolap Obtaining Star Databases from Flat Tables Data in multidimensional systems is obtained from operational\n    systems and is transformed to adapt it to the new structure.\n    Frequently, the operations to be performed aim to transform a flat\n    table into a ROLAP (Relational On-Line Analytical Processing) star\n    database. The main objective of the package is to allow the definition\n    of these transformations easily. The implementation of the\n    multidimensional database obtained can be exported to work with\n    multidimensional analysis tools on spreadsheets or relational\n    databases.  "
  },
  {
    "id": 19741,
    "package_name": "roperators",
    "title": "Additional Operators to Help you Write Cleaner R Code",
    "description": "Provides string arithmetic, reassignment operators, logical operators\n  that handle missing values, and extra logical operators such as floating point\n  equality and all or nothing. The intent is to allow R users to write code that\n  is easier to read, write, and maintain while providing a friendlier experience\n  to new R users from other language backgrounds (such as 'Python') who are used\n  to concepts such as x += 1 and 'foo' + 'bar'.\n  Includes operators for not in, easy floating point comparisons, === equivalent, and SQL-like \n  like operations (), etc. \n  We also added in some extra helper functions, such as OS checks, pasting\n  in Oxford comma format, and functions to get the first, last, nth, or most common \n  element of a vector or word in a string. ",
    "version": "1.3.14",
    "maintainer": "Ben Wiseman <benjamin.wiseman@kornferry.com>",
    "author": "Ben Wiseman [cre, aut, ccp],\n  Steven Nydick [aut, ccp] (ORCID:\n    <https://orcid.org/0000-0002-2908-1188>),\n  Jeff Jones [aut, led]",
    "url": "https://benwiseman.github.io/roperators/,\nhttps://github.com/BenWiseman/roperators",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=roperators",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roperators Additional Operators to Help you Write Cleaner R Code Provides string arithmetic, reassignment operators, logical operators\n  that handle missing values, and extra logical operators such as floating point\n  equality and all or nothing. The intent is to allow R users to write code that\n  is easier to read, write, and maintain while providing a friendlier experience\n  to new R users from other language backgrounds (such as 'Python') who are used\n  to concepts such as x += 1 and 'foo' + 'bar'.\n  Includes operators for not in, easy floating point comparisons, === equivalent, and SQL-like \n  like operations (), etc. \n  We also added in some extra helper functions, such as OS checks, pasting\n  in Oxford comma format, and functions to get the first, last, nth, or most common \n  element of a vector or word in a string.   "
  },
  {
    "id": 19752,
    "package_name": "rosv",
    "title": "Client to Access and Operate on the 'Open Source Vulnerability'\nAPI",
    "description": "Connect, query, and operate on information available from the \n    'Open Source Vulnerability' database <https://osv.dev/>. Although 'CRAN' \n    has vulnerabilities listed, these are few compared to projects such as \n    'PyPI'. With tighter integration between 'R' and 'Python', having an \n    'R' specific package to access details about vulnerabilities from \n    various sources is a worthwhile enterprise.",
    "version": "0.5.1",
    "maintainer": "Allen OBrien <allen.g.obrien@gmail.com>",
    "author": "Allen OBrien [aut, cre, cph]",
    "url": "https://al-obrien.github.io/rosv/,\nhttps://github.com/al-obrien/rosv",
    "bug_reports": "https://github.com/al-obrien/rosv/issues",
    "repository": "https://cran.r-project.org/package=rosv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rosv Client to Access and Operate on the 'Open Source Vulnerability'\nAPI Connect, query, and operate on information available from the \n    'Open Source Vulnerability' database <https://osv.dev/>. Although 'CRAN' \n    has vulnerabilities listed, these are few compared to projects such as \n    'PyPI'. With tighter integration between 'R' and 'Python', having an \n    'R' specific package to access details about vulnerabilities from \n    various sources is a worthwhile enterprise.  "
  },
  {
    "id": 19761,
    "package_name": "roundhouse",
    "title": "Random Chuck Norris Facts",
    "description": "R functions for generating and/or displaying random Chuck Norris \n  facts. Based on data from the 'Internet Chuck Norris database' ('ICNDb').",
    "version": "0.0.2",
    "maintainer": "Brandon Greenwell <greenwell.brandon@gmail.com>",
    "author": "Brandon Greenwell [aut, cre]",
    "url": "https://github.com/bgreenwell/roundhouse",
    "bug_reports": "https://github.com/bgreenwell/roundhouse/issues",
    "repository": "https://cran.r-project.org/package=roundhouse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roundhouse Random Chuck Norris Facts R functions for generating and/or displaying random Chuck Norris \n  facts. Based on data from the 'Internet Chuck Norris database' ('ICNDb').  "
  },
  {
    "id": 19780,
    "package_name": "rpgconn",
    "title": "User-Friendly PostgreSQL Connection Management",
    "description": "Provides a user-friendly interface for managing PostgreSQL\n    database connection settings.  The package supplies helper functions to\n    create, edit and load connection and option configuration files stored in a\n    user-specific directory using the 'odbc' and 'RPostgres' back ends.  These\n    helpers make it easy to construct a reproducible connection string from a\n    configuration file, either by reading user-defined YAML files or by parsing\n    an environment variable.",
    "version": "0.3.2",
    "maintainer": "Bobby Fatemi <bfatemi07@gmail.com>",
    "author": "Bobby Fatemi [aut, cre]",
    "url": "https://github.com/r-data-science/rpgconn",
    "bug_reports": "https://github.com/r-data-science/rpgconn/issues",
    "repository": "https://cran.r-project.org/package=rpgconn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rpgconn User-Friendly PostgreSQL Connection Management Provides a user-friendly interface for managing PostgreSQL\n    database connection settings.  The package supplies helper functions to\n    create, edit and load connection and option configuration files stored in a\n    user-specific directory using the 'odbc' and 'RPostgres' back ends.  These\n    helpers make it easy to construct a reproducible connection string from a\n    configuration file, either by reading user-defined YAML files or by parsing\n    an environment variable.  "
  },
  {
    "id": 19794,
    "package_name": "rpostgis",
    "title": "R Interface to a 'PostGIS' Database",
    "description": "Provides an interface between R and 'PostGIS'-enabled\n    'PostgreSQL' databases to transparently transfer spatial\n    data. Both vector (points, lines, polygons) and raster data are\n    supported in read and write modes. Also provides convenience\n    functions to execute common procedures in 'PostgreSQL/PostGIS'.",
    "version": "1.6.0",
    "maintainer": "Adrian Cidre Gonzalez <adrian.cidre@gmail.com>",
    "author": "Adrian Cidre Gonzalez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3310-3052>),\n  Mathieu Basille [aut] (ORCID: <https://orcid.org/0000-0001-9366-7127>),\n  David Bucklin [aut]",
    "url": "https://cidree.github.io/rpostgis/,\nhttps://github.com/Cidree/rpostgis",
    "bug_reports": "https://github.com/Cidree/rpostgis/issues",
    "repository": "https://cran.r-project.org/package=rpostgis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rpostgis R Interface to a 'PostGIS' Database Provides an interface between R and 'PostGIS'-enabled\n    'PostgreSQL' databases to transparently transfer spatial\n    data. Both vector (points, lines, polygons) and raster data are\n    supported in read and write modes. Also provides convenience\n    functions to execute common procedures in 'PostgreSQL/PostGIS'.  "
  },
  {
    "id": 19807,
    "package_name": "rquery",
    "title": "Relational Query Generator for Data Manipulation at Scale",
    "description": "A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.",
    "version": "1.4.99",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/rquery/,\nhttps://winvector.github.io/rquery/",
    "bug_reports": "https://github.com/WinVector/rquery/issues",
    "repository": "https://cran.r-project.org/package=rquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rquery Relational Query Generator for Data Manipulation at Scale A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.  "
  },
  {
    "id": 19839,
    "package_name": "rscopus",
    "title": "Scopus Database 'API' Interface",
    "description": "Uses Elsevier 'Scopus' API\n    <https://dev.elsevier.com/sc_apis.html> to download \n    information about authors and their citations.",
    "version": "0.9.0",
    "maintainer": "John Muschelli <muschellij2@gmail.com>",
    "author": "John Muschelli [aut, cre]",
    "url": "https://dev.elsevier.com/sc_apis.html,\nhttps://github.com/muschellij2/rscopus",
    "bug_reports": "https://github.com/muschellij2/rscopus/issues",
    "repository": "https://cran.r-project.org/package=rscopus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rscopus Scopus Database 'API' Interface Uses Elsevier 'Scopus' API\n    <https://dev.elsevier.com/sc_apis.html> to download \n    information about authors and their citations.  "
  },
  {
    "id": 19861,
    "package_name": "rsolr",
    "title": "R to Solr Interface",
    "description": "A comprehensive R API for querying Apache Solr databases.\n             A Solr core is represented as a data frame or list that\n             supports Solr-side filtering, sorting,\n             transformation and aggregation, all through the familiar\n             base R API. Queries are processed\n             lazily, i.e., a query is only sent to the database when\n             the data are required. ",
    "version": "0.0.13",
    "maintainer": "Michael Lawrence <michafla@gene.com>",
    "author": "Michael Lawrence, Gabe Becker, Jan Vogel",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rsolr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsolr R to Solr Interface A comprehensive R API for querying Apache Solr databases.\n             A Solr core is represented as a data frame or list that\n             supports Solr-side filtering, sorting,\n             transformation and aggregation, all through the familiar\n             base R API. Queries are processed\n             lazily, i.e., a query is only sent to the database when\n             the data are required.   "
  },
  {
    "id": 19926,
    "package_name": "rtry",
    "title": "Preprocessing Plant Trait Data",
    "description": "Designed to support the application of plant trait data providing easy applicable functions\n    for the basic steps of data preprocessing, e.g. data import, data exploration, selection of columns\n    and rows, excluding trait data according to different attributes, geocoding, long- to wide-table\n    transformation, and data export. 'rtry' was initially developed as part of the TRY R project to\n    preprocess trait data received via the TRY database.",
    "version": "1.1.0",
    "maintainer": "Olee Hoi Ying Lam <hlam9@wisc.edu>",
    "author": "Olee Hoi Ying Lam [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7731-3246>),\n  Susanne Tautenhahn [aut] (ORCID:\n    <https://orcid.org/0000-0002-2753-3443>),\n  Gabriel Walther [aut] (ORCID: <https://orcid.org/0000-0003-3550-9045>),\n  Gerhard Boenisch [aut],\n  Pramod Baddam [aut] (ORCID: <https://orcid.org/0000-0002-7858-9295>),\n  Jens Kattge [aut] (ORCID: <https://orcid.org/0000-0002-1022-8469>),\n  Fellowship Group Functional Biogeography at MPI-BGC, Jena, Germany\n    [cph]",
    "url": "https://github.com/MPI-BGC-Functional-Biogeography/rtry,\nhttps://www.try-db.org/TryWeb/Home.php",
    "bug_reports": "https://github.com/MPI-BGC-Functional-Biogeography/rtry/issues",
    "repository": "https://cran.r-project.org/package=rtry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rtry Preprocessing Plant Trait Data Designed to support the application of plant trait data providing easy applicable functions\n    for the basic steps of data preprocessing, e.g. data import, data exploration, selection of columns\n    and rows, excluding trait data according to different attributes, geocoding, long- to wide-table\n    transformation, and data export. 'rtry' was initially developed as part of the TRY R project to\n    preprocess trait data received via the TRY database.  "
  },
  {
    "id": 19950,
    "package_name": "rush",
    "title": "Rapid Asynchronous and Distributed Computing",
    "description": "Package to tackle large-scale problems asynchronously across\n    a distributed network. Employing a database centric model, rush\n    enables workers to communicate tasks and their results over a shared\n    'Redis' database. Key features include low task overhead, efficient\n    caching, and robust error handling. The package powers the\n    asynchronous optimization algorithms in the 'bbotk' and 'mlr3tuning'\n    packages.",
    "version": "0.4.1",
    "maintainer": "Marc Becker <marcbecker@posteo.de>",
    "author": "Marc Becker [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-8115-0400>)",
    "url": "https://rush.mlr-org.com, https://github.com/mlr-org/rush",
    "bug_reports": "https://github.com/mlr-org/rush/issues",
    "repository": "https://cran.r-project.org/package=rush",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rush Rapid Asynchronous and Distributed Computing Package to tackle large-scale problems asynchronously across\n    a distributed network. Employing a database centric model, rush\n    enables workers to communicate tasks and their results over a shared\n    'Redis' database. Key features include low task overhead, efficient\n    caching, and robust error handling. The package powers the\n    asynchronous optimization algorithms in the 'bbotk' and 'mlr3tuning'\n    packages.  "
  },
  {
    "id": 20066,
    "package_name": "samadb",
    "title": "South Africa Macroeconomic Database API",
    "description": "An R API providing access to a relational database with macroeconomic time series data for South Africa,\n obtained from the South African Reserve Bank (SARB) and Statistics South Africa (STATSSA), and updated on a weekly basis\n via the EconData <https://www.econdata.co.za/> platform and automated scraping of the SARB and STATSSA websites.\n The database is maintained at the Department of Economics at Stellenbosch University.",
    "version": "0.3.1",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/Stellenbosch-Econometrics/SAMADB-Issues/issues",
    "repository": "https://cran.r-project.org/package=samadb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "samadb South Africa Macroeconomic Database API An R API providing access to a relational database with macroeconomic time series data for South Africa,\n obtained from the South African Reserve Bank (SARB) and Statistics South Africa (STATSSA), and updated on a weekly basis\n via the EconData <https://www.econdata.co.za/> platform and automated scraping of the SARB and STATSSA websites.\n The database is maintained at the Department of Economics at Stellenbosch University.  "
  },
  {
    "id": 20073,
    "package_name": "sampleVADIR",
    "title": "Draw Stratified Samples from the VADIR Database",
    "description": "Affords researchers the ability to draw stratified samples from the U.S. Department of Veteran's Affairs/Department of Defense Identity Repository (VADIR) database according to a variety of population characteristics. The VADIR database contains information for all veterans who were separated from the military after 1980. The central utility of the present package is to integrate data cleaning and formatting for the VADIR database with the stratification methods described by Mahto (2019) <https://CRAN.R-project.org/package=splitstackshape>. Data from VADIR are not provided as part of this package.",
    "version": "1.0.0",
    "maintainer": "Trevor Swanson <trevorswanson222@gmail.com>",
    "author": "Trevor Swanson [aut, cre],\n  Kelsie Forbush [aut],\n  Joanna Wiese [ctb],\n  Melinda Gaddy [ctb],\n  Mary Oehlert [ctb]",
    "url": "https://github.com/tswanson222/sampleVADIR",
    "bug_reports": "https://github.com/tswanson222/sampleVADIR/issues",
    "repository": "https://cran.r-project.org/package=sampleVADIR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sampleVADIR Draw Stratified Samples from the VADIR Database Affords researchers the ability to draw stratified samples from the U.S. Department of Veteran's Affairs/Department of Defense Identity Repository (VADIR) database according to a variety of population characteristics. The VADIR database contains information for all veterans who were separated from the military after 1980. The central utility of the present package is to integrate data cleaning and formatting for the VADIR database with the stratification methods described by Mahto (2019) <https://CRAN.R-project.org/package=splitstackshape>. Data from VADIR are not provided as part of this package.  "
  },
  {
    "id": 20110,
    "package_name": "sapfluxnetr",
    "title": "Working with 'Sapfluxnet' Project Data",
    "description": "Access, modify, aggregate and plot data from the 'Sapfluxnet' project,\n  the first global database of sap flow measurements.",
    "version": "0.1.5",
    "maintainer": "Victor Granda <victorgrandagarcia@gmail.com>",
    "author": "Victor Granda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0469-1991>),\n  Rafael Poyatos [aut] (ORCID: <https://orcid.org/0000-0003-0521-2523>),\n  Victor Flo [aut] (ORCID: <https://orcid.org/0000-0003-1908-4577>),\n  Jacob Nelson [ctb] (ORCID: <https://orcid.org/0000-0002-4663-2420>),\n  Sapfluxnet Core Team [cph]",
    "url": "https://github.com/sapfluxnet/sapfluxnetr",
    "bug_reports": "https://github.com/sapfluxnet/sapfluxnetr/issues",
    "repository": "https://cran.r-project.org/package=sapfluxnetr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sapfluxnetr Working with 'Sapfluxnet' Project Data Access, modify, aggregate and plot data from the 'Sapfluxnet' project,\n  the first global database of sap flow measurements.  "
  },
  {
    "id": 20112,
    "package_name": "saqgetr",
    "title": "Import Air Quality Monitoring Data in a Fast and Easy Way",
    "description": "A collection of tools to access prepared air quality monitoring\n    data files from web servers with ease and speed. Air quality data are \n    sourced from open and publicly accessible repositories and can be found in \n    these locations: \n    <https://www.eea.europa.eu/data-and-maps/data/airbase-the-european-air-quality-database-8> \n    and <https://discomap.eea.europa.eu/map/fme/AirQualityExport.htm>. The web \n    server space has been provided by Ricardo Energy & Environment.",
    "version": "0.2.21",
    "maintainer": "Stuart K. Grange <stuart.grange@york.ac.uk>",
    "author": "Stuart K. Grange [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-4093-3596>)",
    "url": "https://github.com/skgrange/saqgetr",
    "bug_reports": "https://github.com/skgrange/saqgetr/issues",
    "repository": "https://cran.r-project.org/package=saqgetr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "saqgetr Import Air Quality Monitoring Data in a Fast and Easy Way A collection of tools to access prepared air quality monitoring\n    data files from web servers with ease and speed. Air quality data are \n    sourced from open and publicly accessible repositories and can be found in \n    these locations: \n    <https://www.eea.europa.eu/data-and-maps/data/airbase-the-european-air-quality-database-8> \n    and <https://discomap.eea.europa.eu/map/fme/AirQualityExport.htm>. The web \n    server space has been provided by Ricardo Energy & Environment.  "
  },
  {
    "id": 20127,
    "package_name": "satin",
    "title": "Visualisation and Analysis of Ocean Data Derived from Satellites",
    "description": "With 'satin' functions, visualisation, data extraction and further analysis like producing climatologies from several images, and anomalies of satellite derived ocean data can be easily done.  Reading functions can import a user defined geographical extent of data stored in netCDF files.  Currently supported ocean data sources include NASA's Oceancolor web page <https://oceancolor.gsfc.nasa.gov/>, sensors VIIRS-SNPP; MODIS-Terra; MODIS-Aqua; and SeaWiFS.  Available variables from this source includes chlorophyll concentration, sea surface temperature (SST), and several others.  Data sources specific for SST that can be imported too includes Pathfinder AVHRR <https://www.ncei.noaa.gov/products/avhrr-pathfinder-sst> and GHRSST <https://www.ghrsst.org/>.  In addition, ocean productivity data produced by Oregon State University can also be handled previous conversion from HDF4 to HDF5 format.  Many other ocean variables can be processed by importing netCDF data files from two European Union's Copernicus Marine Service databases <https://marine.copernicus.eu/>, namely Global Ocean Physical Reanalysis and Global Ocean Biogeochemistry Hindcast.",
    "version": "1.2.0",
    "maintainer": "H\u00e9ctor Villalobos <hvillalo@ipn.mx>",
    "author": "H\u00e9ctor Villalobos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6424-4050>),\n  Eduardo Gonz\u00e1lez-Rodr\u00edguez [aut]",
    "url": "https://github.com/hvillalo/satin",
    "bug_reports": "https://github.com/hvillalo/satin/issues",
    "repository": "https://cran.r-project.org/package=satin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "satin Visualisation and Analysis of Ocean Data Derived from Satellites With 'satin' functions, visualisation, data extraction and further analysis like producing climatologies from several images, and anomalies of satellite derived ocean data can be easily done.  Reading functions can import a user defined geographical extent of data stored in netCDF files.  Currently supported ocean data sources include NASA's Oceancolor web page <https://oceancolor.gsfc.nasa.gov/>, sensors VIIRS-SNPP; MODIS-Terra; MODIS-Aqua; and SeaWiFS.  Available variables from this source includes chlorophyll concentration, sea surface temperature (SST), and several others.  Data sources specific for SST that can be imported too includes Pathfinder AVHRR <https://www.ncei.noaa.gov/products/avhrr-pathfinder-sst> and GHRSST <https://www.ghrsst.org/>.  In addition, ocean productivity data produced by Oregon State University can also be handled previous conversion from HDF4 to HDF5 format.  Many other ocean variables can be processed by importing netCDF data files from two European Union's Copernicus Marine Service databases <https://marine.copernicus.eu/>, namely Global Ocean Physical Reanalysis and Global Ocean Biogeochemistry Hindcast.  "
  },
  {
    "id": 20150,
    "package_name": "scCATCH",
    "title": "Single Cell Cluster-Based Annotation Toolkit for Cellular\nHeterogeneity",
    "description": "An automatic cluster-based annotation pipeline based on evidence-based score by matching the marker genes with known cell markers in tissue-specific cell taxonomy reference database for single-cell RNA-seq data. See Shao X, et al (2020) <doi:10.1016/j.isci.2020.100882> for more details.",
    "version": "3.2.2",
    "maintainer": "Xin Shao<xin_shao@zju.edu.cn>",
    "author": "Xin Shao",
    "url": "https://github.com/ZJUFanLab/scCATCH",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scCATCH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scCATCH Single Cell Cluster-Based Annotation Toolkit for Cellular\nHeterogeneity An automatic cluster-based annotation pipeline based on evidence-based score by matching the marker genes with known cell markers in tissue-specific cell taxonomy reference database for single-cell RNA-seq data. See Shao X, et al (2020) <doi:10.1016/j.isci.2020.100882> for more details.  "
  },
  {
    "id": 20154,
    "package_name": "scDiffCom",
    "title": "Differential Analysis of Intercellular Communication from\nscRNA-Seq Data",
    "description": "Analysis tools to investigate changes in intercellular\n    communication from scRNA-seq data. Using a Seurat object as input,\n    the package infers which cell-cell interactions are present in the dataset\n    and how these interactions change between two conditions of interest\n    (e.g. young vs old). It relies on an internal database of ligand-receptor\n    interactions (available for human, mouse and rat) that have been gathered\n    from several published studies. Detection and differential analyses\n    rely on permutation tests. The package also contains several tools\n    to perform over-representation analysis and visualize the results. See\n    Lagger, C. et al. (2023) <doi:10.1038/s43587-023-00514-x> for a full \n    description of the methodology.",
    "version": "1.2.0",
    "maintainer": "Cyril Lagger <lagger.cyril@gmail.com>",
    "author": "Cyril Lagger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1701-6896>),\n  Eugen Ursu [aut],\n  Anais Equey [ctb]",
    "url": "https://cyrillagger.github.io/scDiffCom/,\nhttps://github.com/CyrilLagger/scDiffCom",
    "bug_reports": "https://github.com/CyrilLagger/scDiffCom/issues",
    "repository": "https://cran.r-project.org/package=scDiffCom",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scDiffCom Differential Analysis of Intercellular Communication from\nscRNA-Seq Data Analysis tools to investigate changes in intercellular\n    communication from scRNA-seq data. Using a Seurat object as input,\n    the package infers which cell-cell interactions are present in the dataset\n    and how these interactions change between two conditions of interest\n    (e.g. young vs old). It relies on an internal database of ligand-receptor\n    interactions (available for human, mouse and rat) that have been gathered\n    from several published studies. Detection and differential analyses\n    rely on permutation tests. The package also contains several tools\n    to perform over-representation analysis and visualize the results. See\n    Lagger, C. et al. (2023) <doi:10.1038/s43587-023-00514-x> for a full \n    description of the methodology.  "
  },
  {
    "id": 20187,
    "package_name": "scaper",
    "title": "Single Cell Transcriptomics-Level Cytokine Activity Prediction\nand Estimation",
    "description": "Generates cell-level cytokine activity estimates using relevant information from gene sets constructed with the 'CytoSig' and the 'Reactome' databases and scored using the modified 'Variance-adjusted Mahalanobis (VAM)' framework for single-cell RNA-sequencing (scRNA-seq) data. 'CytoSig' database is described in: Jiang at al., (2021) <doi:10.1038/s41592-021-01274-5>. 'Reactome' database is described in: Gillespie et al., (2021) <doi:10.1093/nar/gkab1028>. The 'VAM' method is outlined in: Frost (2020) <doi:10.1093/nar/gkaa582>.",
    "version": "0.2.0",
    "maintainer": "Azka Javaid <azka.javaid.gr@dartmouth.edu>",
    "author": "H. Robert Frost [aut],\n  Azka Javaid [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=scaper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scaper Single Cell Transcriptomics-Level Cytokine Activity Prediction\nand Estimation Generates cell-level cytokine activity estimates using relevant information from gene sets constructed with the 'CytoSig' and the 'Reactome' databases and scored using the modified 'Variance-adjusted Mahalanobis (VAM)' framework for single-cell RNA-sequencing (scRNA-seq) data. 'CytoSig' database is described in: Jiang at al., (2021) <doi:10.1038/s41592-021-01274-5>. 'Reactome' database is described in: Gillespie et al., (2021) <doi:10.1093/nar/gkab1028>. The 'VAM' method is outlined in: Frost (2020) <doi:10.1093/nar/gkaa582>.  "
  },
  {
    "id": 20198,
    "package_name": "sccca",
    "title": "Single-Cell Correlation Based Cell Type Annotation",
    "description": "Performing cell type annotation based on cell markers from a unified database. The approach utilizes correlation-based approach combined with association analysis using Fisher-exact and phyper statistical tests (Upton, Graham JG. (1992) <DOI:10.2307/2982890>).",
    "version": "0.1.1",
    "maintainer": "Mohamed Soudy <Mohmedsoudy2009@gmail.com>",
    "author": "Mohamed Soudy [aut, cre],\n  Sophie LE BARS [aut],\n  Enrico Glaab [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sccca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sccca Single-Cell Correlation Based Cell Type Annotation Performing cell type annotation based on cell markers from a unified database. The approach utilizes correlation-based approach combined with association analysis using Fisher-exact and phyper statistical tests (Upton, Graham JG. (1992) <DOI:10.2307/2982890>).  "
  },
  {
    "id": 20265,
    "package_name": "scrobbler",
    "title": "Download 'Scrobbles' from 'Last.fm'",
    "description": "'Last.fm'<https://www.last.fm> is a music platform focussed on building a \n    detailed profile of a users listening habits. It does this by 'scrobbling' (recording) \n    every track you listen to on other platforms ('spotify', 'youtube', 'soundcloud' etc)\n    and transferring them to your 'Last.fm' database. This allows 'Last.fm' to act as a \n    complete record of your entire listening history. 'scrobbler' provides helper functions\n    to download and analyse your listening history in R.",
    "version": "1.0.3",
    "maintainer": "Conor Neilson <condwanaland@gmail.com>",
    "author": "Conor Neilson",
    "url": "https://github.com/condwanaland/scrobbler",
    "bug_reports": "https://github.com/condwanaland/scrobbler/issues",
    "repository": "https://cran.r-project.org/package=scrobbler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scrobbler Download 'Scrobbles' from 'Last.fm' 'Last.fm'<https://www.last.fm> is a music platform focussed on building a \n    detailed profile of a users listening habits. It does this by 'scrobbling' (recording) \n    every track you listen to on other platforms ('spotify', 'youtube', 'soundcloud' etc)\n    and transferring them to your 'Last.fm' database. This allows 'Last.fm' to act as a \n    complete record of your entire listening history. 'scrobbler' provides helper functions\n    to download and analyse your listening history in R.  "
  },
  {
    "id": 20304,
    "package_name": "searchAnalyzeR",
    "title": "Advanced Analytics and Testing Framework for Systematic Review\nSearch Strategies",
    "description": "Provides comprehensive analytics, reporting, and testing capabilities \n    for systematic review search strategies. The package focuses on validating search \n    performance, generating standardized 'PRISMA'-compliant reports, and ensuring \n    reproducibility in evidence synthesis. Features include precision-recall analysis, \n    cross-database performance comparison, benchmark validation against gold standards, \n    sensitivity analysis, temporal coverage assessment, automated report generation,\n    and statistical comparison of search strategies. Supports multiple export formats \n    including 'CSV', 'Excel', 'RIS', 'BibTeX', and 'EndNote'. Includes tools for duplicate \n    detection, search strategy optimization, cross-validation frameworks, meta-analysis \n    of benchmark results, power analysis for study design, and reproducibility package \n    creation. Optionally connects to 'PubMed' for direct database searching and real-time \n    strategy comparison using the 'E-utilities' 'API'. Enhanced with bootstrap comparison \n    methods, 'McNemar' test for strategy evaluation, and comprehensive visualization tools \n    for performance assessment. Methods based on Manning et al. (2008) \n    for information retrieval metrics, Moher et al. (2009)\n    for 'PRISMA' guidelines, and Sampson et al. (2006) for \n    systematic review search methodology.",
    "version": "0.1.0",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre] (ORCID: <https://orcid.org/0000-0002-9979-8272>)",
    "url": "https://github.com/chaoliu-cl/searchAnalyzeR",
    "bug_reports": "https://github.com/chaoliu-cl/searchAnalyzeR/issues",
    "repository": "https://cran.r-project.org/package=searchAnalyzeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "searchAnalyzeR Advanced Analytics and Testing Framework for Systematic Review\nSearch Strategies Provides comprehensive analytics, reporting, and testing capabilities \n    for systematic review search strategies. The package focuses on validating search \n    performance, generating standardized 'PRISMA'-compliant reports, and ensuring \n    reproducibility in evidence synthesis. Features include precision-recall analysis, \n    cross-database performance comparison, benchmark validation against gold standards, \n    sensitivity analysis, temporal coverage assessment, automated report generation,\n    and statistical comparison of search strategies. Supports multiple export formats \n    including 'CSV', 'Excel', 'RIS', 'BibTeX', and 'EndNote'. Includes tools for duplicate \n    detection, search strategy optimization, cross-validation frameworks, meta-analysis \n    of benchmark results, power analysis for study design, and reproducibility package \n    creation. Optionally connects to 'PubMed' for direct database searching and real-time \n    strategy comparison using the 'E-utilities' 'API'. Enhanced with bootstrap comparison \n    methods, 'McNemar' test for strategy evaluation, and comprehensive visualization tools \n    for performance assessment. Methods based on Manning et al. (2008) \n    for information retrieval metrics, Moher et al. (2009)\n    for 'PRISMA' guidelines, and Sampson et al. (2006) for \n    systematic review search methodology.  "
  },
  {
    "id": 20386,
    "package_name": "semnar",
    "title": "Constructing and Interacting with Databases of Presentations",
    "description": "Provides methods for constructing and maintaining a database of presentations in R. The presentations are either ones that the user gives or gave or presentations at a particular event or event series. The package also provides a plot method for the interactive mapping of the presentations using 'leaflet' by grouping them according to country, city, year and other presentation attributes. The markers on the map come with popups providing presentation details (title, institution, event, links to materials and events, and so on).",
    "version": "0.8.2",
    "maintainer": "Ioannis Kosmidis <ioannis.kosmidis@warwick.ac.uk>",
    "author": "Ioannis Kosmidis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1556-0302>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=semnar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semnar Constructing and Interacting with Databases of Presentations Provides methods for constructing and maintaining a database of presentations in R. The presentations are either ones that the user gives or gave or presentations at a particular event or event series. The package also provides a plot method for the interactive mapping of the presentations using 'leaflet' by grouping them according to country, city, year and other presentation attributes. The markers on the map come with popups providing presentation details (title, institution, event, links to materials and events, and so on).  "
  },
  {
    "id": 20394,
    "package_name": "sendigR",
    "title": "Enable Cross-Study Analysis of 'CDISC' 'SEND' Datasets",
    "description": "A system enables cross study Analysis by extracting and filtering study data for \n    control animals from  'CDISC' 'SEND' Study Repository. \n    These data types are supported: Body Weights, Laboratory test results and Microscopic findings. \n    These database types are supported: 'SQLite' and 'Oracle'.",
    "version": "1.0.0",
    "maintainer": "Wenxian Wang <wenxian.wang@bms.com>",
    "author": "Bo Larsen [aut],\n  Yousuf Ali [aut],\n  Kevin Snyder [aut],\n  William Houser [aut],\n  Brianna Paisley [aut],\n  Cmsabbir Ahmed [aut],\n  Susan Butler [aut],\n  Michael Rosentreter [aut],\n  Michael Denieu [aut],\n  Wenxian Wang [cre, aut],\n  BioCelerate [cph]",
    "url": "https://github.com/phuse-org/sendigR",
    "bug_reports": "https://github.com/phuse-org/sendigR/issues",
    "repository": "https://cran.r-project.org/package=sendigR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sendigR Enable Cross-Study Analysis of 'CDISC' 'SEND' Datasets A system enables cross study Analysis by extracting and filtering study data for \n    control animals from  'CDISC' 'SEND' Study Repository. \n    These data types are supported: Body Weights, Laboratory test results and Microscopic findings. \n    These database types are supported: 'SQLite' and 'Oracle'.  "
  },
  {
    "id": 20440,
    "package_name": "sergeant",
    "title": "Tools to Transform and Query Data with Apache Drill",
    "description": "Apache Drill is a low-latency distributed query engine designed to enable \n    data exploration and analysis on both relational and non-relational data stores, \n    scaling to petabytes of data. Methods are provided that enable working with Apache \n    Drill instances via the REST API, DBI methods\n    and using 'dplyr'/'dbplyr' idioms. Helper functions are included to facilitate\n    using official Drill Docker images/containers.",
    "version": "0.9.1",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis [aut, cre] (ORCID: <https://orcid.org/0000-0001-5670-2640>),\n  Edward Visel [ctb],\n  Andy Hine [ctb],\n  Scott Came [ctb],\n  David Severski [ctb] (ORCID: <https://orcid.org/0000-0001-7867-0459>),\n  James Lamb [ctb]",
    "url": "https://gitlab.com/hrbrmstr/sergeant",
    "bug_reports": "https://gitlab.com/hrbrmstr/sergeant/-/issues",
    "repository": "https://cran.r-project.org/package=sergeant",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sergeant Tools to Transform and Query Data with Apache Drill Apache Drill is a low-latency distributed query engine designed to enable \n    data exploration and analysis on both relational and non-relational data stores, \n    scaling to petabytes of data. Methods are provided that enable working with Apache \n    Drill instances via the REST API, DBI methods\n    and using 'dplyr'/'dbplyr' idioms. Helper functions are included to facilitate\n    using official Drill Docker images/containers.  "
  },
  {
    "id": 20545,
    "package_name": "shiny.reglog",
    "title": "Optional Login and Registration Module System for ShinyApps",
    "description": "RegLog system provides a set of shiny modules to handle register\n   procedure for your users, alongside with login, edit credentials and\n   password reset functionality. \n   It provides support for popular SQL databases\n   and optionally googlesheet-based database for easy setup. For email sending\n   it provides support for 'emayili' and 'gmailr' backends. Architecture makes \n   customizing usability pretty straightforward.\n   The authentication system created \n   with shiny.reglog is designed to be optional: user don't need to be logged-in \n   to access your application, but when logged-in the user data can be used \n   to read from and write to relational databases.",
    "version": "0.5.2",
    "maintainer": "Michal Kosinski <kosinski.mich@gmail.com>",
    "author": "Michal Kosinski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8426-3654>)",
    "url": "https://statismike.github.io/shiny.reglog/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shiny.reglog",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shiny.reglog Optional Login and Registration Module System for ShinyApps RegLog system provides a set of shiny modules to handle register\n   procedure for your users, alongside with login, edit credentials and\n   password reset functionality. \n   It provides support for popular SQL databases\n   and optionally googlesheet-based database for easy setup. For email sending\n   it provides support for 'emayili' and 'gmailr' backends. Architecture makes \n   customizing usability pretty straightforward.\n   The authentication system created \n   with shiny.reglog is designed to be optional: user don't need to be logged-in \n   to access your application, but when logged-in the user data can be used \n   to read from and write to relational databases.  "
  },
  {
    "id": 20555,
    "package_name": "shinyChatR",
    "title": "R Shiny Chat Module",
    "description": "Provides an easy-to-use module for adding a chat to a Shiny app. Allows\n    users to send messages and view messages from other users. \n    Messages can be stored in a database or a .rds file.",
    "version": "1.2.0",
    "maintainer": "Julian Schmocker <julian.schmocker@gmail.com>",
    "author": "Julian Schmocker [aut, cre, cph],\n  Ivo Kwee [aut]",
    "url": "https://github.com/julianschmocker/shinyChatR,\nhttps://julianschmocker.github.io/shinyChatR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shinyChatR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinyChatR R Shiny Chat Module Provides an easy-to-use module for adding a chat to a Shiny app. Allows\n    users to send messages and view messages from other users. \n    Messages can be stored in a database or a .rds file.  "
  },
  {
    "id": 20584,
    "package_name": "shinyNotes",
    "title": "Shiny Module for Taking Free-Form Notes",
    "description": "An enterprise-targeted scalable and customizable 'shiny' module providing an easy way to incorporate free-form note taking or discussion boards into applications.\n    The package includes a 'shiny' module that can be included in any 'shiny' application to create a panel containing searchable, editable text broken down by section headers.\n    Can be used with a local 'SQLite' database, or a compatible remote database of choice.",
    "version": "0.0.3",
    "maintainer": "Daniel Kovtun <quantumfusetrader@gmail.com>",
    "author": "Daniel Kovtun [cre, aut]",
    "url": "https://github.com/danielkovtun/shinyNotes",
    "bug_reports": "https://github.com/danielkovtun/shinyNotes/issues",
    "repository": "https://cran.r-project.org/package=shinyNotes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinyNotes Shiny Module for Taking Free-Form Notes An enterprise-targeted scalable and customizable 'shiny' module providing an easy way to incorporate free-form note taking or discussion boards into applications.\n    The package includes a 'shiny' module that can be included in any 'shiny' application to create a panel containing searchable, editable text broken down by section headers.\n    Can be used with a local 'SQLite' database, or a compatible remote database of choice.  "
  },
  {
    "id": 20596,
    "package_name": "shinyStorePlus",
    "title": "Secure in-Browser and Database Storage for 'shiny' Inputs,\nOutputs, Views and User Likes",
    "description": "Store persistent and synchronized data from 'shiny' inputs within the browser. Refresh 'shiny' applications and preserve user-inputs over multiple sessions. A database-like storage format is implemented using 'Dexie.js' <https://dexie.org>, a minimal wrapper for 'IndexedDB'. Transfer browser link parameters to 'shiny' input or output values. Store app visitor views, likes and followers.",
    "version": "1.6",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://shinystoreplus.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/shinyStorePlus/issues",
    "repository": "https://cran.r-project.org/package=shinyStorePlus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinyStorePlus Secure in-Browser and Database Storage for 'shiny' Inputs,\nOutputs, Views and User Likes Store persistent and synchronized data from 'shiny' inputs within the browser. Refresh 'shiny' applications and preserve user-inputs over multiple sessions. A database-like storage format is implemented using 'Dexie.js' <https://dexie.org>, a minimal wrapper for 'IndexedDB'. Transfer browser link parameters to 'shiny' input or output values. Store app visitor views, likes and followers.  "
  },
  {
    "id": 20613,
    "package_name": "shinydbauth",
    "title": "Simple Authentification for 'shiny' Applications",
    "description": "Provides a simple authentification mechanism for single 'shiny' applications.\n    Authentification and password change functionality are performed calling user provided functions that typically access some database backend.\n    Source code of main applications is protected until authentication is successful.",
    "version": "1.0.0.1",
    "maintainer": "Diego Florio <diegoefe@gmail.com>",
    "author": "Diego Florio [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-1799-0189>)",
    "url": "https://github.com/diegoefe/shinydbauth",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shinydbauth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinydbauth Simple Authentification for 'shiny' Applications Provides a simple authentification mechanism for single 'shiny' applications.\n    Authentification and password change functionality are performed calling user provided functions that typically access some database backend.\n    Source code of main applications is protected until authentication is successful.  "
  },
  {
    "id": 20625,
    "package_name": "shinymanager",
    "title": "Authentication Management for 'Shiny' Applications",
    "description": "Simple and secure authentification mechanism for single 'Shiny' applications.\n    Credentials are stored in an encrypted 'SQLite' database. Source code of main application\n    is protected until authentication is successful.",
    "version": "1.0.410",
    "maintainer": "Benoit Thieurmel <bthieurmel@gmail.com>",
    "author": "Benoit Thieurmel [aut, cre],\n  Victor Perrier [aut]",
    "url": "https://github.com/datastorm-open/shinymanager",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shinymanager",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinymanager Authentication Management for 'Shiny' Applications Simple and secure authentification mechanism for single 'Shiny' applications.\n    Credentials are stored in an encrypted 'SQLite' database. Source code of main application\n    is protected until authentication is successful.  "
  },
  {
    "id": 20758,
    "package_name": "simplegraphdb",
    "title": "A Simple Graph Database",
    "description": "This is a graph database in 'SQLite'.  It is inspired by Denis Papathanasiou's Python simple-graph project on 'GitHub'.",
    "version": "2021.03.10",
    "maintainer": "Michael Silva <mike.a.silva@gmail.com>",
    "author": "Michael Silva [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7344-660X>)",
    "url": "https://github.com/mikeasilva/simplegraphdb",
    "bug_reports": "https://github.com/mikeasilva/simplegraphdb/issues",
    "repository": "https://cran.r-project.org/package=simplegraphdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simplegraphdb A Simple Graph Database This is a graph database in 'SQLite'.  It is inspired by Denis Papathanasiou's Python simple-graph project on 'GitHub'.  "
  },
  {
    "id": 20819,
    "package_name": "sjdbc",
    "title": "JDBC Driver Interface",
    "description": "Provides a database-independent JDBC interface.",
    "version": "1.6.1",
    "maintainer": "Joe Roberts <jorobert@tibco.com>",
    "author": "TIBCO Software Inc.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sjdbc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sjdbc JDBC Driver Interface Provides a database-independent JDBC interface.  "
  },
  {
    "id": 20955,
    "package_name": "snowquery",
    "title": "SQL Interface to 'Snowflake', 'Redshift', 'Postgres', 'SQLite',\nand 'DuckDB'",
    "description": "Run 'SQL' queries across 'Snowflake', 'Amazon Redshift', 'PostgreSQL', 'SQLite', and 'DuckDB' from R with a single function. Optionally stream and cache large query results to a local 'DuckDB' database for efficient work with larger-than-memory datasets.",
    "version": "1.3.0",
    "maintainer": "Dani Mermelstein <dmermelstein@hey.com>",
    "author": "Dani Mermelstein [aut, cre, cph]",
    "url": "https://github.com/mermelstein/snowquery",
    "bug_reports": "https://github.com/mermelstein/snowquery/issues",
    "repository": "https://cran.r-project.org/package=snowquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "snowquery SQL Interface to 'Snowflake', 'Redshift', 'Postgres', 'SQLite',\nand 'DuckDB' Run 'SQL' queries across 'Snowflake', 'Amazon Redshift', 'PostgreSQL', 'SQLite', and 'DuckDB' from R with a single function. Optionally stream and cache large query results to a local 'DuckDB' database for efficient work with larger-than-memory datasets.  "
  },
  {
    "id": 20958,
    "package_name": "snplist",
    "title": "Tools to Create Gene Sets",
    "description": "A set of functions to create SQL tables of gene and SNP information and compose them into a SNP Set, for example to export to a PLINK set.",
    "version": "0.18.3",
    "maintainer": "Alexander Sibley <dcibioinformatics@duke.edu>",
    "author": "Chanhee Yi [aut],\n  Alexander Sibley [aut, cre],\n  Kouros Owzar [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=snplist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "snplist Tools to Create Gene Sets A set of functions to create SQL tables of gene and SNP information and compose them into a SNP Set, for example to export to a PLINK set.  "
  },
  {
    "id": 20975,
    "package_name": "soilDB",
    "title": "Soil Database Interface",
    "description": "A collection of functions for reading soil data from U.S. Department of Agriculture Natural Resources Conservation Service (USDA-NRCS) and National Cooperative Soil Survey (NCSS) databases.",
    "version": "2.8.13",
    "maintainer": "Andrew Brown <andrew.g.brown@usda.gov>",
    "author": "Dylan Beaudette [aut] (ORCID: <https://orcid.org/0009-0008-2780-4785>),\n  Jay Skovlin [aut],\n  Stephen Roecker [aut],\n  Andrew Brown [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4565-533X>)",
    "url": "https://github.com/ncss-tech/soilDB/,\nhttps://ncss-tech.github.io/soilDB/,\nhttps://ncss-tech.github.io/AQP/",
    "bug_reports": "https://github.com/ncss-tech/soilDB/issues",
    "repository": "https://cran.r-project.org/package=soilDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "soilDB Soil Database Interface A collection of functions for reading soil data from U.S. Department of Agriculture Natural Resources Conservation Service (USDA-NRCS) and National Cooperative Soil Survey (NCSS) databases.  "
  },
  {
    "id": 20976,
    "package_name": "soilassessment",
    "title": "Soil Health Assessment Models for Assessing Soil Conditions and\nSuitability",
    "description": "Soil health assessment builds information to improve decision in\n    soil management. It facilitates assessment of soil conditions for crop suitability [such as those given by FAO\n    <https://www.fao.org/land-water/databases-and-software/crop-information/en/>], \n    groundwater recharge, fertility, erosion, salinization [<doi:10.1002/ldr.4211>], \n    carbon sequestration, irrigation potential, and status of soil resources. ",
    "version": "0.3.0",
    "maintainer": "Christian Thine Omuto <thineomuto@yahoo.com>",
    "author": "Christian Thine Omuto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5792-3485>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=soilassessment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "soilassessment Soil Health Assessment Models for Assessing Soil Conditions and\nSuitability Soil health assessment builds information to improve decision in\n    soil management. It facilitates assessment of soil conditions for crop suitability [such as those given by FAO\n    <https://www.fao.org/land-water/databases-and-software/crop-information/en/>], \n    groundwater recharge, fertility, erosion, salinization [<doi:10.1002/ldr.4211>], \n    carbon sequestration, irrigation potential, and status of soil resources.   "
  },
  {
    "id": 20990,
    "package_name": "solrium",
    "title": "General Purpose R Interface to 'Solr'",
    "description": "Provides a set of functions for querying and parsing data\n    from 'Solr' (<https://solr.apache.org/>) 'endpoints' (local and\n    remote), including search, 'faceting', 'highlighting', 'stats', and\n    'more like this'. In addition, some functionality is included for\n    creating, deleting, and updating documents in a 'Solr' 'database'.",
    "version": "1.2.0",
    "maintainer": "Scott Chamberlain <myrmecocystus@gmail.com>",
    "author": "Scott Chamberlain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1444-9135>),\n  rOpenSci [fnd] (https://ropensci.org/)",
    "url": "https://github.com/ropensci/solrium (devel),\nhttps://docs.ropensci.org/solrium/ (user manual)",
    "bug_reports": "https://github.com/ropensci/solrium/issues",
    "repository": "https://cran.r-project.org/package=solrium",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "solrium General Purpose R Interface to 'Solr' Provides a set of functions for querying and parsing data\n    from 'Solr' (<https://solr.apache.org/>) 'endpoints' (local and\n    remote), including search, 'faceting', 'highlighting', 'stats', and\n    'more like this'. In addition, some functionality is included for\n    creating, deleting, and updating documents in a 'Solr' 'database'.  "
  },
  {
    "id": 20995,
    "package_name": "somaticflags",
    "title": "Database of Somatic Flags",
    "description": "Database of genes which frequently sustain somatic mutations, but are unlikely to drive cancer.",
    "version": "0.1.0",
    "maintainer": "Sam El-Kamand <selkamand@ccia.org.au>",
    "author": "Sam El-Kamand [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2270-8088>),\n  Children's Cancer Institute Australia [cph]",
    "url": "https://github.com/CCICB/somaticflags",
    "bug_reports": "https://github.com/CCICB/somaticflags/issues",
    "repository": "https://cran.r-project.org/package=somaticflags",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "somaticflags Database of Somatic Flags Database of genes which frequently sustain somatic mutations, but are unlikely to drive cancer.  "
  },
  {
    "id": 21013,
    "package_name": "soundClass",
    "title": "Sound Classification Using Convolutional Neural Networks",
    "description": "Provides an all-in-one solution for automatic classification of \n    sound events using convolutional neural networks (CNN). The main purpose \n    is to provide a sound classification workflow, from annotating sound events\n    in recordings to training and automating model usage in real-life\n    situations. Using the package requires a pre-compiled collection of \n    recordings with sound events of interest and it can be employed for: \n    1) Annotation: create a database of annotated recordings, \n    2) Training: prepare train data from annotated recordings and fit CNN models, \n    3) Classification: automate the use of the fitted model for classifying \n    new recordings. By using automatic feature selection and a user-friendly GUI\n    for managing data and training/deploying models, this package is intended \n    to be used by a broad audience as it does not require specific expertise in \n    statistics, programming or sound analysis. Please refer to the vignette for\n    further information.\n    Gibb, R., et al. (2019) <doi:10.1111/2041-210X.13101>\n    Mac Aodha, O., et al. (2018) <doi:10.1371/journal.pcbi.1005995>\n    Stowell, D., et al. (2019) <doi:10.1111/2041-210X.13103>\n    LeCun, Y., et al. (2012) <doi:10.1007/978-3-642-35289-8_3>.",
    "version": "0.0.9.2",
    "maintainer": "Bruno Silva <bmsasilva@gmail.com>",
    "author": "Bruno Silva [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/bmsasilva/soundClass/issues",
    "repository": "https://cran.r-project.org/package=soundClass",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "soundClass Sound Classification Using Convolutional Neural Networks Provides an all-in-one solution for automatic classification of \n    sound events using convolutional neural networks (CNN). The main purpose \n    is to provide a sound classification workflow, from annotating sound events\n    in recordings to training and automating model usage in real-life\n    situations. Using the package requires a pre-compiled collection of \n    recordings with sound events of interest and it can be employed for: \n    1) Annotation: create a database of annotated recordings, \n    2) Training: prepare train data from annotated recordings and fit CNN models, \n    3) Classification: automate the use of the fitted model for classifying \n    new recordings. By using automatic feature selection and a user-friendly GUI\n    for managing data and training/deploying models, this package is intended \n    to be used by a broad audience as it does not require specific expertise in \n    statistics, programming or sound analysis. Please refer to the vignette for\n    further information.\n    Gibb, R., et al. (2019) <doi:10.1111/2041-210X.13101>\n    Mac Aodha, O., et al. (2018) <doi:10.1371/journal.pcbi.1005995>\n    Stowell, D., et al. (2019) <doi:10.1111/2041-210X.13103>\n    LeCun, Y., et al. (2012) <doi:10.1007/978-3-642-35289-8_3>.  "
  },
  {
    "id": 21059,
    "package_name": "spanishoddata",
    "title": "Get Spanish Origin-Destination Data",
    "description": "Gain seamless access to origin-destination (OD) data from the\n    Spanish Ministry of Transport, hosted at\n    <https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad>.\n    This package simplifies the management of these large datasets by\n    providing tools to download zone boundaries, handle associated\n    origin-destination data, and process it efficiently with the 'duckdb'\n    database interface.  Local caching minimizes repeated downloads,\n    streamlining workflows for researchers and analysts. Extensive\n    documentation is available at\n    <https://ropenspain.github.io/spanishoddata/index.html>, offering\n    guides on creating static and dynamic mobility flow visualizations and\n    transforming large datasets into analysis-ready formats.",
    "version": "0.2.1",
    "maintainer": "Egor Kotov <kotov.egor@gmail.com>",
    "author": "Egor Kotov [aut, cre] (ORCID: <https://orcid.org/0000-0001-6690-5345>),\n  Robin Lovelace [aut] (ORCID: <https://orcid.org/0000-0001-5679-6536>),\n  Eugeni Vidal-Tortosa [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5199-4103>)",
    "url": "https://rOpenSpain.github.io/spanishoddata/,\nhttps://github.com/rOpenSpain/spanishoddata",
    "bug_reports": "https://github.com/rOpenSpain/spanishoddata/issues",
    "repository": "https://cran.r-project.org/package=spanishoddata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spanishoddata Get Spanish Origin-Destination Data Gain seamless access to origin-destination (OD) data from the\n    Spanish Ministry of Transport, hosted at\n    <https://www.transportes.gob.es/ministerio/proyectos-singulares/estudios-de-movilidad-con-big-data/opendata-movilidad>.\n    This package simplifies the management of these large datasets by\n    providing tools to download zone boundaries, handle associated\n    origin-destination data, and process it efficiently with the 'duckdb'\n    database interface.  Local caching minimizes repeated downloads,\n    streamlining workflows for researchers and analysts. Extensive\n    documentation is available at\n    <https://ropenspain.github.io/spanishoddata/index.html>, offering\n    guides on creating static and dynamic mobility flow visualizations and\n    transforming large datasets into analysis-ready formats.  "
  },
  {
    "id": 21184,
    "package_name": "speedytax",
    "title": "Rapidly Import Classifier Results into 'phyloseq'",
    "description": "Import classification results from the 'RDP Classifier' (Ribosomal Database Project),' 'USEARCH sintax,' 'vsearch sintax' and the 'QIIME2' (Quantitative Insights into Microbial Ecology) classifiers into 'phyloseq' tax_table objects.",
    "version": "1.0.4",
    "maintainer": "John Quensen <quensenj@msu.edu>",
    "author": "John Quensen [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-7669-1595>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=speedytax",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "speedytax Rapidly Import Classifier Results into 'phyloseq' Import classification results from the 'RDP Classifier' (Ribosomal Database Project),' 'USEARCH sintax,' 'vsearch sintax' and the 'QIIME2' (Quantitative Insights into Microbial Ecology) classifiers into 'phyloseq' tax_table objects.  "
  },
  {
    "id": 21206,
    "package_name": "spidR",
    "title": "Spider Knowledge Online",
    "description": "Allows the user to connect with the World Spider Catalogue (WSC; <https://wsc.nmbe.ch/>) and the World Spider Trait (WST; <https://spidertraits.sci.muni.cz/>) databases. Also performs several basic functions such as checking names validity, retrieving coordinate data from the Global Biodiversity Information Facility (GBIF; <https://www.gbif.org/>), and mapping.",
    "version": "1.0.2",
    "maintainer": "Pedro Cardoso <pedro.cardoso@helsinki.fi>",
    "author": "Pedro Cardoso [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spidR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spidR Spider Knowledge Online Allows the user to connect with the World Spider Catalogue (WSC; <https://wsc.nmbe.ch/>) and the World Spider Trait (WST; <https://spidertraits.sci.muni.cz/>) databases. Also performs several basic functions such as checking names validity, retrieving coordinate data from the Global Biodiversity Information Facility (GBIF; <https://www.gbif.org/>), and mapping.  "
  },
  {
    "id": 21278,
    "package_name": "sqlHelpers",
    "title": "Collection of 'SQL' Utilities for 'T-SQL' and 'Postgresql'",
    "description": "Includes functions for interacting with common meta data fields, \n  writing insert statements, calling functions, and more for 'T-SQL' and 'Postgresql'.",
    "version": "0.1.2",
    "maintainer": "Timothy Conwell <timconwell@gmail.com>",
    "author": "Timothy Conwell [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sqlHelpers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqlHelpers Collection of 'SQL' Utilities for 'T-SQL' and 'Postgresql' Includes functions for interacting with common meta data fields, \n  writing insert statements, calling functions, and more for 'T-SQL' and 'Postgresql'.  "
  },
  {
    "id": 21279,
    "package_name": "sqlcaser",
    "title": "'SQL' Case Statement Generator",
    "description": "Includes built-in methods for generating long 'SQL' CASE statements,\n             and other 'SQL' statements that may otherwise be arduous to construct\n             by hand.The generated statement can easily be concatenated to string\n             literals to form queries to 'SQL'-like databases, such as when using \n             the 'RODBC' package. The current methods include casewhen() for \n             building CASE statements, inlist() for building IN statements, and\n             updatetable() for building UPDATE statements.",
    "version": "0.2.1",
    "maintainer": "Leoson Hoay <leoson.public@gmail.com>",
    "author": "Leoson Hoay [aut, cre] (ORCID: <https://orcid.org/0000-0003-2079-5579>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sqlcaser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqlcaser 'SQL' Case Statement Generator Includes built-in methods for generating long 'SQL' CASE statements,\n             and other 'SQL' statements that may otherwise be arduous to construct\n             by hand.The generated statement can easily be concatenated to string\n             literals to form queries to 'SQL'-like databases, such as when using \n             the 'RODBC' package. The current methods include casewhen() for \n             building CASE statements, inlist() for building IN statements, and\n             updatetable() for building UPDATE statements.  "
  },
  {
    "id": 21280,
    "package_name": "sqldf",
    "title": "Manipulate R Data Frames Using SQL",
    "description": "The sqldf() function is typically passed a single argument which \n\tis an SQL select statement where the table names are ordinary R data \n\tframe names.  sqldf() transparently sets up a database, imports the \n\tdata frames into that database, performs the SQL select or other\n\tstatement and returns the result using a heuristic to determine which \n\tclass to assign to each column of the returned data frame.  The sqldf() \n\tor read.csv.sql() functions can also be used to read filtered files \n\tinto R even if the original files are larger than R itself can handle.\n\t'RSQLite', 'RH2', 'RMySQL' and 'RPostgreSQL' backends are supported.",
    "version": "0.4-11",
    "maintainer": "G. Grothendieck <ggrothendieck@gmail.com>",
    "author": "G. Grothendieck <ggrothendieck@gmail.com>",
    "url": "https://github.com/ggrothendieck/sqldf,\nhttps://groups.google.com/group/sqldf",
    "bug_reports": "https://github.com/ggrothendieck/sqldf/issues",
    "repository": "https://cran.r-project.org/package=sqldf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqldf Manipulate R Data Frames Using SQL The sqldf() function is typically passed a single argument which \n\tis an SQL select statement where the table names are ordinary R data \n\tframe names.  sqldf() transparently sets up a database, imports the \n\tdata frames into that database, performs the SQL select or other\n\tstatement and returns the result using a heuristic to determine which \n\tclass to assign to each column of the returned data frame.  The sqldf() \n\tor read.csv.sql() functions can also be used to read filtered files \n\tinto R even if the original files are larger than R itself can handle.\n\t'RSQLite', 'RH2', 'RMySQL' and 'RPostgreSQL' backends are supported.  "
  },
  {
    "id": 21281,
    "package_name": "sqlhelper",
    "title": "Easier 'SQL' Integration",
    "description": "Execute files of 'SQL' and manage database connections. 'SQL' statements and queries may be interpolated with string literals. Execution of individual statements and queries may be controlled with keywords. Multiple connections may be defined with 'YAML' and accessed by name. ",
    "version": "0.2.1",
    "maintainer": "Matthew Roberts <matthew@zsmr.uk>",
    "author": "Matthew Roberts [aut, cre, cph]",
    "url": "https://majerr.github.io/sqlhelper/dev/,\nhttps://github.com/majerr/sqlhelper/",
    "bug_reports": "https://github.com/majerr/sqlhelper/issues",
    "repository": "https://cran.r-project.org/package=sqlhelper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqlhelper Easier 'SQL' Integration Execute files of 'SQL' and manage database connections. 'SQL' statements and queries may be interpolated with string literals. Execution of individual statements and queries may be controlled with keywords. Multiple connections may be defined with 'YAML' and accessed by name.   "
  },
  {
    "id": 21282,
    "package_name": "sqliter",
    "title": "Connection wrapper to SQLite databases",
    "description": "sqliter helps users, mainly data munging practioneers, to organize\n    their sql calls in a clean structure. It simplifies the process of\n    extracting and transforming data into useful formats.",
    "version": "0.1.0",
    "maintainer": "Wilson Freitas <wilson.freitas@gmail.com>",
    "author": "Wilson Freitas <wilson.freitas@gmail.com>",
    "url": "https://github.com/wilsonfreitas/sqliter/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sqliter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqliter Connection wrapper to SQLite databases sqliter helps users, mainly data munging practioneers, to organize\n    their sql calls in a clean structure. It simplifies the process of\n    extracting and transforming data into useful formats.  "
  },
  {
    "id": 21283,
    "package_name": "sqliteutils",
    "title": "Utility Functions for 'SQLite'",
    "description": "A tool for working with 'SQLite' databases. 'SQLite' has some idiosyncrasies and limitations that impose some hurdles to the R developer who is using this database as a repository. For instance, 'SQLite' doesn't have a date type and 'sqliteutils' has some functions to deal with that.",
    "version": "0.1.0",
    "maintainer": "Bruno Crotman <crotman@gmail.com>",
    "author": "Bruno Crotman [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sqliteutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqliteutils Utility Functions for 'SQLite' A tool for working with 'SQLite' databases. 'SQLite' has some idiosyncrasies and limitations that impose some hurdles to the R developer who is using this database as a repository. For instance, 'SQLite' doesn't have a date type and 'sqliteutils' has some functions to deal with that.  "
  },
  {
    "id": 21284,
    "package_name": "sqlparseR",
    "title": "Wrapper for 'Python' Module 'sqlparse': Parse, Split, and Format\n'SQL'",
    "description": "Wrapper for the non-validating 'SQL' parser 'Python' module 'sqlparse' <https://github.com/andialbrecht/sqlparse>. It allows parsing, splitting, and formatting 'SQL' statements.",
    "version": "0.1.0",
    "maintainer": "Michael Simmler <michael.simmler@agroscope.admin.ch>",
    "author": "Michael Simmler [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sqlparseR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqlparseR Wrapper for 'Python' Module 'sqlparse': Parse, Split, and Format\n'SQL' Wrapper for the non-validating 'SQL' parser 'Python' module 'sqlparse' <https://github.com/andialbrecht/sqlparse>. It allows parsing, splitting, and formatting 'SQL' statements.  "
  },
  {
    "id": 21285,
    "package_name": "sqlq",
    "title": "'SQL' Query Builder",
    "description": "Allows to build complex 'SQL' (Structured Query Language) queries dynamically. Classes and/or factory functions are used to produce a syntax tree from which the final character string is generated. Strings and identifiers are automatically quoted using the right quotes, using either ANSI (American National Standards Institute) quoting or the quoting style of an existing database connector. Style can be configured to set uppercase/lowercase for keywords, remove unnecessary spaces, or omit optional keywords.",
    "version": "1.0.1",
    "maintainer": "Pierrick Roger <pierrick.roger@cea.fr>",
    "author": "Pierrick Roger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8177-4873>)",
    "url": "https://gitlab.com/cnrgh/databases/r-sqlq",
    "bug_reports": "https://gitlab.com/cnrgh/databases/r-sqlq/-/issues",
    "repository": "https://cran.r-project.org/package=sqlq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqlq 'SQL' Query Builder Allows to build complex 'SQL' (Structured Query Language) queries dynamically. Classes and/or factory functions are used to produce a syntax tree from which the final character string is generated. Strings and identifiers are automatically quoted using the right quotes, using either ANSI (American National Standards Institute) quoting or the quoting style of an existing database connector. Style can be configured to set uppercase/lowercase for keywords, remove unnecessary spaces, or omit optional keywords.  "
  },
  {
    "id": 21286,
    "package_name": "sqlscore",
    "title": "Utilities for Generating SQL Queries from Model Objects",
    "description": "Provides utilities for generating SQL queries (particularly CREATE\n    TABLE statements) from R model objects. The most important use case is\n    generating SQL to score a generalized linear model or related model\n    represented as an R object, in which case the package handles parsing\n    formula operators and including the model's response function.",
    "version": "0.1.4",
    "maintainer": "William Brannon <wwbrannon@email.wm.edu>",
    "author": "William Brannon [aut, cre]",
    "url": "https://github.com/wwbrannon/sqlscore/",
    "bug_reports": "https://github.com/wwbrannon/sqlscore/issues",
    "repository": "https://cran.r-project.org/package=sqlscore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqlscore Utilities for Generating SQL Queries from Model Objects Provides utilities for generating SQL queries (particularly CREATE\n    TABLE statements) from R model objects. The most important use case is\n    generating SQL to score a generalized linear model or related model\n    represented as an R object, in which case the package handles parsing\n    formula operators and including the model's response function.  "
  },
  {
    "id": 21287,
    "package_name": "sqlstrings",
    "title": "Map 'SQL' Code to R Lists",
    "description": "Provides a helper function, to bulk read 'SQL' code from separate files and load it into an 'R' list, where the list elements contain the individual statements and queries as strings. This works by annotating the 'SQL' code with a name comment, which also will be the name of the list element. ",
    "version": "1.0.0",
    "maintainer": "Dejan Prvulovic <dejan.prv@gmail.com>",
    "author": "Dejan Prvulovic [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sqlstrings",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqlstrings Map 'SQL' Code to R Lists Provides a helper function, to bulk read 'SQL' code from separate files and load it into an 'R' list, where the list elements contain the individual statements and queries as strings. This works by annotating the 'SQL' code with a name comment, which also will be the name of the list element.   "
  },
  {
    "id": 21288,
    "package_name": "sqltargets",
    "title": "'Targets' Extension for 'SQL' Queries",
    "description": "Provides an extension for 'SQL' queries as separate file \n    within 'targets' pipelines. The shorthand creates two targets,\n    the query file and the query result.",
    "version": "0.3.0",
    "maintainer": "David Ranzolin <daranzolin@gmail.com>",
    "author": "David Ranzolin [aut, cre, cph],\n  Roy Storey [ctb] (ORCID: <https://orcid.org/0000-0003-1375-7136>)",
    "url": "https://github.com/daranzolin/sqltargets",
    "bug_reports": "https://github.com/daranzolin/sqltargets/issues",
    "repository": "https://cran.r-project.org/package=sqltargets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sqltargets 'Targets' Extension for 'SQL' Queries Provides an extension for 'SQL' queries as separate file \n    within 'targets' pipelines. The shorthand creates two targets,\n    the query file and the query result.  "
  },
  {
    "id": 21296,
    "package_name": "srcr",
    "title": "Simplify Connections to Database Sources",
    "description": "Connecting to databases requires boilerplate code to specify\n    connection parameters and to set up sessions properly with the DBMS.\n    This package provides a simple tool to fill two purposes: abstracting\n    connection details, including secret credentials, out of your source\n    code and managing configuration for frequently-used database connections\n    in a persistent and flexible way, while minimizing requirements on the\n    runtime environment.",
    "version": "1.1.1",
    "maintainer": "Charles Bailey <baileyc@chop.edu>",
    "author": "Charles Bailey [aut, cre],\n  Hanieh Razzaghi [aut]",
    "url": "https://github.com/baileych/srcr",
    "bug_reports": "https://github.com/baileych/srcr/issues",
    "repository": "https://cran.r-project.org/package=srcr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "srcr Simplify Connections to Database Sources Connecting to databases requires boilerplate code to specify\n    connection parameters and to set up sessions properly with the DBMS.\n    This package provides a simple tool to fill two purposes: abstracting\n    connection details, including secret credentials, out of your source\n    code and managing configuration for frequently-used database connections\n    in a persistent and flexible way, while minimizing requirements on the\n    runtime environment.  "
  },
  {
    "id": 21340,
    "package_name": "ssw",
    "title": "Striped Smith-Waterman Algorithm for Sequence Alignment using\nSIMD",
    "description": "Provides an R interface for 'SSW' (Striped Smith-Waterman)\n    via its 'Python' binding 'ssw-py'. 'SSW' is a fast 'C' and 'C++'\n    implementation of the Smith-Waterman algorithm for pairwise sequence\n    alignment using Single-Instruction-Multiple-Data (SIMD) instructions.\n    'SSW' enhances the standard algorithm by efficiently returning alignment\n    information and suboptimal alignment scores.\n    The core 'SSW' library offers performance improvements for various\n    bioinformatics tasks, including protein database searches,\n    short-read alignments, primary and split-read mapping,\n    structural variant detection, and read-overlap graph generation.\n    These features make 'SSW' particularly useful for genomic applications.\n    Zhao et al. (2013) <doi:10.1371/journal.pone.0082138> developed the\n    original 'C' and 'C++' implementation.",
    "version": "0.2.1",
    "maintainer": "Nan Xiao <me@nanx.me>",
    "author": "Nan Xiao [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0250-5673>)",
    "url": "https://nanx.me/ssw-r/, https://github.com/nanxstats/ssw-r",
    "bug_reports": "https://github.com/nanxstats/ssw-r/issues",
    "repository": "https://cran.r-project.org/package=ssw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ssw Striped Smith-Waterman Algorithm for Sequence Alignment using\nSIMD Provides an R interface for 'SSW' (Striped Smith-Waterman)\n    via its 'Python' binding 'ssw-py'. 'SSW' is a fast 'C' and 'C++'\n    implementation of the Smith-Waterman algorithm for pairwise sequence\n    alignment using Single-Instruction-Multiple-Data (SIMD) instructions.\n    'SSW' enhances the standard algorithm by efficiently returning alignment\n    information and suboptimal alignment scores.\n    The core 'SSW' library offers performance improvements for various\n    bioinformatics tasks, including protein database searches,\n    short-read alignments, primary and split-read mapping,\n    structural variant detection, and read-overlap graph generation.\n    These features make 'SSW' particularly useful for genomic applications.\n    Zhao et al. (2013) <doi:10.1371/journal.pone.0082138> developed the\n    original 'C' and 'C++' implementation.  "
  },
  {
    "id": 21360,
    "package_name": "stacomirtools",
    "title": "Connection Class for Package stacomiR",
    "description": "S4 class wrappers for the 'ODBC' and Pool DBI connection, also provides some \n    utilities to paste small datasets to clipboard, rename columns. It is used by the package 'stacomiR' for\n    connections to the database. Development versions of 'stacomiR' are available in R-forge.",
    "version": "0.6.0.1",
    "maintainer": "Cedric Briand <cedric.briand00@gmail.com>",
    "author": "Cedric Briand [aut, cre],\n  Marion Legrand [aut],\n  Beaulaton Laurent [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stacomirtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stacomirtools Connection Class for Package stacomiR S4 class wrappers for the 'ODBC' and Pool DBI connection, also provides some \n    utilities to paste small datasets to clipboard, rename columns. It is used by the package 'stacomiR' for\n    connections to the database. Development versions of 'stacomiR' are available in R-forge.  "
  },
  {
    "id": 21368,
    "package_name": "standartox",
    "title": "Ecotoxicological Information from the Standartox Database",
    "description": "The <http://standartox.uni-landau.de> database offers cleaned,\n    harmonized and aggregated ecotoxicological test data, which can\n    be used for assessing effects and risks of chemical concentrations\n    found in the environment.",
    "version": "0.0.2",
    "maintainer": "Andreas Scharm\u00fcller <andschar@protonmail.com>",
    "author": "Andreas Scharm\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9290-3965>)",
    "url": "https://github.com/andschar/standartox",
    "bug_reports": "https://github.com/andschar/standartox/issues",
    "repository": "https://cran.r-project.org/package=standartox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "standartox Ecotoxicological Information from the Standartox Database The <http://standartox.uni-landau.de> database offers cleaned,\n    harmonized and aggregated ecotoxicological test data, which can\n    be used for assessing effects and risks of chemical concentrations\n    found in the environment.  "
  },
  {
    "id": 21388,
    "package_name": "starwarsdb",
    "title": "Relational Data from the 'Star Wars' API for Learning and\nTeaching",
    "description": "Provides data about the 'Star Wars' movie franchise in a set\n    of relational tables or as a complete 'DuckDB' database. All data was\n    collected from the open source 'Star Wars' API.",
    "version": "0.1.3",
    "maintainer": "Garrick Aden-Buie <garrick@adenbuie.com>",
    "author": "Garrick Aden-Buie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7111-0077>)",
    "url": "https://github.com/gadenbuie/starwarsdb,\nhttps://pkg.garrickadenbuie.com/starwarsdb/",
    "bug_reports": "https://github.com/gadenbuie/starwarsdb/issues",
    "repository": "https://cran.r-project.org/package=starwarsdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "starwarsdb Relational Data from the 'Star Wars' API for Learning and\nTeaching Provides data about the 'Star Wars' movie franchise in a set\n    of relational tables or as a complete 'DuckDB' database. All data was\n    collected from the open source 'Star Wars' API.  "
  },
  {
    "id": 21418,
    "package_name": "statnipokladna",
    "title": "Use Data from the Czech Public Finance Database",
    "description": "Get programmatic access to data from the Czech public\n    budgeting and accounting database, St\u00e1tn\u00ed pokladna\n    <https://monitor.statnipokladna.gov.cz/>.",
    "version": "0.7.7",
    "maintainer": "Petr Bouchal <pbouchal@gmail.com>",
    "author": "Petr Bouchal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0471-716X>)",
    "url": "https://github.com/petrbouchal/statnipokladna,\nhttps://petrbouchal.xyz/statnipokladna/",
    "bug_reports": "https://github.com/petrbouchal/statnipokladna/issues",
    "repository": "https://cran.r-project.org/package=statnipokladna",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statnipokladna Use Data from the Czech Public Finance Database Get programmatic access to data from the Czech public\n    budgeting and accounting database, St\u00e1tn\u00ed pokladna\n    <https://monitor.statnipokladna.gov.cz/>.  "
  },
  {
    "id": 21421,
    "package_name": "statquotes",
    "title": "Quotes on Statistics, Data Visualization and Science",
    "description": "Generates a random quotation from a database of quotes on topics\n    in statistics, data visualization and science. Other functions allow searching\n    the quotes database by key term tags, or authors or creating a word cloud.\n    The output is designed to be suitable for use at the console, in Rmarkdown\n    and LaTeX.",
    "version": "0.3.3",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "author": "Michael Friendly [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3237-0941>, Twitter: @datavisFriendly),\n  Kevin Wright [aut] (ORCID: <https://orcid.org/0000-0002-0617-8673>),\n  Phil Chalmers [aut],\n  Matthew Sigal [ctb]",
    "url": "https://github.com/friendly/statquotes/",
    "bug_reports": "https://github.com/friendly/statquotes/issues",
    "repository": "https://cran.r-project.org/package=statquotes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statquotes Quotes on Statistics, Data Visualization and Science Generates a random quotation from a database of quotes on topics\n    in statistics, data visualization and science. Other functions allow searching\n    the quotes database by key term tags, or authors or creating a word cloud.\n    The output is designed to be suitable for use at the console, in Rmarkdown\n    and LaTeX.  "
  },
  {
    "id": 21439,
    "package_name": "stellaR",
    "title": "Evolutionary Tracks and Isochrones from Pisa Stellar Evolution\nDatabase",
    "description": "Manages and display stellar tracks and isochrones from \n\t     Pisa low-mass database. Includes tools for isochrones\n\t     construction and tracks interpolation. ",
    "version": "0.3-6",
    "maintainer": "Matteo Dell'Omodarme <mattdell@fastmail.fm>",
    "author": "Matteo Dell'Omodarme [aut, cre],\n  Giada Valle [aut]",
    "url": "http://astro.df.unipi.it/stellar-models/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stellaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stellaR Evolutionary Tracks and Isochrones from Pisa Stellar Evolution\nDatabase Manages and display stellar tracks and isochrones from \n\t     Pisa low-mass database. Includes tools for isochrones\n\t     construction and tracks interpolation.   "
  },
  {
    "id": 21443,
    "package_name": "stenographer",
    "title": "Flexible and Customisable Logging System",
    "description": "A comprehensive logging framework for R applications that provides hierarchical logging levels, database integration, and contextual logging capabilities. The package supports 'SQLite' storage for persistent logs, provides colour-coded console output for better readability, includes parallel processing support, and implements structured error reporting with 'JSON' formatting.",
    "version": "1.0.0",
    "maintainer": "Dereck Mezquita <dereck@mezquita.io>",
    "author": "Dereck Mezquita [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9307-6762>)",
    "url": "https://github.com/dereckmezquita/stenographer",
    "bug_reports": "https://github.com/dereckmezquita/stenographer/issues",
    "repository": "https://cran.r-project.org/package=stenographer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stenographer Flexible and Customisable Logging System A comprehensive logging framework for R applications that provides hierarchical logging levels, database integration, and contextual logging capabilities. The package supports 'SQLite' storage for persistent logs, provides colour-coded console output for better readability, includes parallel processing support, and implements structured error reporting with 'JSON' formatting.  "
  },
  {
    "id": 21490,
    "package_name": "stoichUtilities",
    "title": "User Tools for Accessing the STOICH Project Database",
    "description": "\n    User tools for working with The STOICH (Stoichiometric Traits of Organisms \n    in their Chemical Habitats) Project database <https://snr-stoich.unl.edu/>. \n    This package is designed to aid in data discovery, filtering, pairing water \n    samples with organism samples, and merging data tables to assist users in \n    preparing data for analyses. For additional examples see \"Additional Examples\"\n    and the readme file at <https://github.com/STOICH-project/STOICH-utilities>.",
    "version": "1.0.2",
    "maintainer": "Chad Petersen <cpetersen4@unl.edu>",
    "author": "Chad Petersen [aut, cre],\n  Jessica Corman [ctb],\n  Halvor Halvorson [ctb],\n  Casey Brucker [ctb],\n  Eli Wess [ctb],\n  U.S. National Science Foundation [fnd]",
    "url": "https://github.com/STOICH-project/STOICH-utilities,\nhttps://snr-stoich.unl.edu/,\nhttps://stoichproject.wordpress.com/",
    "bug_reports": "https://github.com/STOICH-project/STOICH-utilities/issues",
    "repository": "https://cran.r-project.org/package=stoichUtilities",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stoichUtilities User Tools for Accessing the STOICH Project Database \n    User tools for working with The STOICH (Stoichiometric Traits of Organisms \n    in their Chemical Habitats) Project database <https://snr-stoich.unl.edu/>. \n    This package is designed to aid in data discovery, filtering, pairing water \n    samples with organism samples, and merging data tables to assist users in \n    preparing data for analyses. For additional examples see \"Additional Examples\"\n    and the readme file at <https://github.com/STOICH-project/STOICH-utilities>.  "
  },
  {
    "id": 21496,
    "package_name": "storr",
    "title": "Simple Key Value Stores",
    "description": "Creates and manages simple key-value stores.  These can\n    use a variety of approaches for storing the data.  This package\n    implements the base methods and support for file system, in-memory\n    and DBI-based database stores.",
    "version": "1.2.6",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\n  William Michael Landau [ctb] (ORCID:\n    <https://orcid.org/0000-0003-1878-3253>)",
    "url": "https://richfitz.github.io/storr/,\nhttps://github.com/richfitz/storr",
    "bug_reports": "https://github.com/richfitz/storr/issues",
    "repository": "https://cran.r-project.org/package=storr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "storr Simple Key Value Stores Creates and manages simple key-value stores.  These can\n    use a variety of approaches for storing the data.  This package\n    implements the base methods and support for file system, in-memory\n    and DBI-based database stores.  "
  },
  {
    "id": 21568,
    "package_name": "subincomeR",
    "title": "Access to Global Sub-National Income Data",
    "description": "Provides access to granular sub-national income data from the\n    MCC-PIK Database Of Sub-national Economic Output (DOSE). The package\n    downloads and processes the data from its open repository on 'Zenodo'\n    (<https://zenodo.org/records/13773040>). Functions are provided to\n    fetch data at multiple geographic levels, match coordinates to\n    administrative regions, and access associated geometries.",
    "version": "0.4.0",
    "maintainer": "Pablo Garc\u00eda Guzm\u00e1n <garciagp@ebrd.com>",
    "author": "Pablo Garc\u00eda Guzm\u00e1n [aut, cre, cph]",
    "url": "https://github.com/pablogguz/subincomeR,\nhttps://pablogguz.github.io/subincomeR/",
    "bug_reports": "https://github.com/pablogguz/subincomeR/issues",
    "repository": "https://cran.r-project.org/package=subincomeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "subincomeR Access to Global Sub-National Income Data Provides access to granular sub-national income data from the\n    MCC-PIK Database Of Sub-national Economic Output (DOSE). The package\n    downloads and processes the data from its open repository on 'Zenodo'\n    (<https://zenodo.org/records/13773040>). Functions are provided to\n    fetch data at multiple geographic levels, match coordinates to\n    administrative regions, and access associated geometries.  "
  },
  {
    "id": 21662,
    "package_name": "surveydown",
    "title": "Markdown-Based Programmable Surveys Using 'Quarto' and 'shiny'",
    "description": "Generate programmable surveys using markdown and R code chunks. Surveys\n    are composed of two files: a survey.qmd 'Quarto' file defining the\n    survey content (pages, questions, etc), and an app.R file defining a\n    'shiny' app with global settings (libraries, database configuration,\n    etc.) and server configuration options (e.g., conditional skipping /\n    display, etc.). Survey data collected from respondents is stored in a\n    'PostgreSQL' database. Features include controls for conditional skip\n    logic (skip to a page based on an answer to a question), conditional\n    display logic (display a question based on an answer to a question), a\n    customizable progress bar, and a wide variety of question types,\n    including multiple choice (single choice and multiple choices),\n    select, text, numeric, multiple choice buttons, text area, and dates.\n    Because the surveys render into a 'shiny' app, designers can also\n    leverage the reactive capabilities of 'shiny' to create dynamic and\n    interactive surveys.",
    "version": "0.14.0",
    "maintainer": "John Paul Helveston <john.helveston@gmail.com>",
    "author": "John Paul Helveston [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2657-9191>),\n  Pingfan Hu [aut, cph] (ORCID: <https://orcid.org/0009-0001-4877-4844>),\n  Bogdan Bunea [aut, cph] (ORCID:\n    <https://orcid.org/0009-0006-2942-0588>),\n  Stefan Munnes [ctb]",
    "url": "https://pkg.surveydown.org",
    "bug_reports": "https://github.com/surveydown-dev/surveydown/issues",
    "repository": "https://cran.r-project.org/package=surveydown",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "surveydown Markdown-Based Programmable Surveys Using 'Quarto' and 'shiny' Generate programmable surveys using markdown and R code chunks. Surveys\n    are composed of two files: a survey.qmd 'Quarto' file defining the\n    survey content (pages, questions, etc), and an app.R file defining a\n    'shiny' app with global settings (libraries, database configuration,\n    etc.) and server configuration options (e.g., conditional skipping /\n    display, etc.). Survey data collected from respondents is stored in a\n    'PostgreSQL' database. Features include controls for conditional skip\n    logic (skip to a page based on an answer to a question), conditional\n    display logic (display a question based on an answer to a question), a\n    customizable progress bar, and a wide variety of question types,\n    including multiple choice (single choice and multiple choices),\n    select, text, numeric, multiple choice buttons, text area, and dates.\n    Because the surveys render into a 'shiny' app, designers can also\n    leverage the reactive capabilities of 'shiny' to create dynamic and\n    interactive surveys.  "
  },
  {
    "id": 21775,
    "package_name": "synthesisr",
    "title": "Import, Assemble, and Deduplicate Bibliographic Datasets",
    "description": "A critical first step in systematic literature reviews\n  and mining of academic texts is to identify relevant texts from a range\n  of sources, particularly databases such as 'Web of Science' or 'Scopus'.\n  These databases often export in different formats or with different metadata\n  tags. 'synthesisr' expands on the tools outlined by Westgate (2019)\n  <doi:10.1002/jrsm.1374> to import bibliographic data from a range of formats\n  (such as 'bibtex', 'ris', or 'ciw') in a standard way, and allows merging\n  and deduplication of the resulting dataset.",
    "version": "0.3.0",
    "maintainer": "Martin Westgate <martinjwestgate@gmail.com>",
    "author": "Martin Westgate [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0854-2034>),\n  Eliza Grames [aut] (ORCID: <https://orcid.org/0000-0003-1743-6815>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=synthesisr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "synthesisr Import, Assemble, and Deduplicate Bibliographic Datasets A critical first step in systematic literature reviews\n  and mining of academic texts is to identify relevant texts from a range\n  of sources, particularly databases such as 'Web of Science' or 'Scopus'.\n  These databases often export in different formats or with different metadata\n  tags. 'synthesisr' expands on the tools outlined by Westgate (2019)\n  <doi:10.1002/jrsm.1374> to import bibliographic data from a range of formats\n  (such as 'bibtex', 'ris', or 'ciw') in a standard way, and allows merging\n  and deduplication of the resulting dataset.  "
  },
  {
    "id": 21851,
    "package_name": "taskqueue",
    "title": "Task Queue for Parallel Computing Based on PostgreSQL",
    "description": "Implements a task queue system for asynchronous parallel computing \n    using 'PostgreSQL' <https://www.postgresql.org/> as a backend. Designed for \n    embarrassingly parallel problems where tasks do not communicate with each other.\n    Dynamically distributes tasks to workers, handles uneven load balancing, and \n    allows new workers to join at any time. Particularly useful for running large \n    numbers of independent tasks on high-performance computing (HPC) clusters with \n    'SLURM' <https://slurm.schedmd.com/> job schedulers.",
    "version": "0.2.0",
    "maintainer": "Bangyou Zheng <bangyou.zheng@csiro.au>",
    "author": "Bangyou Zheng [aut, cre]",
    "url": "https://taskqueue.bangyou.me/,\nhttps://github.com/byzheng/taskqueue",
    "bug_reports": "https://github.com/byzheng/taskqueue/issues",
    "repository": "https://cran.r-project.org/package=taskqueue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "taskqueue Task Queue for Parallel Computing Based on PostgreSQL Implements a task queue system for asynchronous parallel computing \n    using 'PostgreSQL' <https://www.postgresql.org/> as a backend. Designed for \n    embarrassingly parallel problems where tasks do not communicate with each other.\n    Dynamically distributes tasks to workers, handles uneven load balancing, and \n    allows new workers to join at any time. Particularly useful for running large \n    numbers of independent tasks on high-performance computing (HPC) clusters with \n    'SLURM' <https://slurm.schedmd.com/> job schedulers.  "
  },
  {
    "id": 21860,
    "package_name": "taxalight",
    "title": "A Lightweight and Lightning-Fast Taxonomic Naming Interface",
    "description": "Creates a local Lightning Memory-Mapped Database ('LMDB') \n             of many commonly used taxonomic authorities\n             and provides functions that can quickly query this data.\n             Supported taxonomic authorities include \n             the Integrated Taxonomic Information System ('ITIS'),\n             National Center for Biotechnology Information ('NCBI'),\n             Global Biodiversity Information Facility ('GBIF'), \n             Catalogue of Life ('COL'), and Open Tree Taxonomy ('OTT'). \n             Name and identifier resolution using 'LMDB' can\n             be hundreds of times faster than either relational databases or\n             internet-based queries. Precise data provenance information for\n             data derived from naming providers is also included.",
    "version": "0.1.5",
    "maintainer": "Carl Boettiger <cboettig@gmail.com>",
    "author": "Carl Boettiger [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1642-628X>),\n  Kari Norman [aut] (ORCID: <https://orcid.org/0000-0002-2029-2325>)",
    "url": "https://github.com/cboettig/taxalight",
    "bug_reports": "https://github.com/cboettig/taxalight",
    "repository": "https://cran.r-project.org/package=taxalight",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "taxalight A Lightweight and Lightning-Fast Taxonomic Naming Interface Creates a local Lightning Memory-Mapped Database ('LMDB') \n             of many commonly used taxonomic authorities\n             and provides functions that can quickly query this data.\n             Supported taxonomic authorities include \n             the Integrated Taxonomic Information System ('ITIS'),\n             National Center for Biotechnology Information ('NCBI'),\n             Global Biodiversity Information Facility ('GBIF'), \n             Catalogue of Life ('COL'), and Open Tree Taxonomy ('OTT'). \n             Name and identifier resolution using 'LMDB' can\n             be hundreds of times faster than either relational databases or\n             internet-based queries. Precise data provenance information for\n             data derived from naming providers is also included.  "
  },
  {
    "id": 21862,
    "package_name": "taxonomizr",
    "title": "Functions to Work with NCBI Accessions and Taxonomy",
    "description": "Functions for assigning taxonomy to NCBI accession numbers and taxon IDs based on NCBI's accession2taxid and taxdump files. This package allows the user to download NCBI data dumps and create a local database for fast and local taxonomic assignment.",
    "version": "0.11.1",
    "maintainer": "Scott Sherrill-Mix <ssm@msu.edu>",
    "author": "Scott Sherrill-Mix [aut, cre]",
    "url": "https://github.com/sherrillmix/taxonomizr/",
    "bug_reports": "https://github.com/sherrillmix/taxonomizr/issues",
    "repository": "https://cran.r-project.org/package=taxonomizr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "taxonomizr Functions to Work with NCBI Accessions and Taxonomy Functions for assigning taxonomy to NCBI accession numbers and taxon IDs based on NCBI's accession2taxid and taxdump files. This package allows the user to download NCBI data dumps and create a local database for fast and local taxonomic assignment.  "
  },
  {
    "id": 21872,
    "package_name": "tcgaViz",
    "title": "Visualization Tool for the Cancer Genome Atlas Program (TCGA)",
    "description": "Differential analysis of tumor tissue immune cell type\n    abundance based on RNA-seq gene-level expression from The Cancer\n    Genome Atlas (TCGA; <https://pancanatlas.xenahubs.net>) database.",
    "version": "1.0.2",
    "maintainer": "Etienne Camenen <etienne.camenen@gmail.com>",
    "author": "Etienne Camenen [aut, cre],\n  Gilles Marodon [aut],\n  Nicolas Aubert [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tcgaViz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tcgaViz Visualization Tool for the Cancer Genome Atlas Program (TCGA) Differential analysis of tumor tissue immune cell type\n    abundance based on RNA-seq gene-level expression from The Cancer\n    Genome Atlas (TCGA; <https://pancanatlas.xenahubs.net>) database.  "
  },
  {
    "id": 21877,
    "package_name": "tcpl",
    "title": "ToxCast Data Analysis Pipeline",
    "description": "The ToxCast Data Analysis Pipeline ('tcpl') is an R package that manages, curve-fits, plots, and stores ToxCast data to populate its linked MySQL database, 'invitrodb'. The package was developed for the chemical screening data curated by the US EPA's Toxicity Forecaster (ToxCast) program, but 'tcpl' can be used to support diverse chemical screening efforts.",
    "version": "3.3.1",
    "maintainer": "Madison Feshuk <feshuk.madison@epa.gov>",
    "author": "Dayne L Filer [aut],\n  Jason Brown [ctb] (ORCID: <https://orcid.org/0009-0000-2294-641X>),\n  Madison Feshuk [cre] (ORCID: <https://orcid.org/0000-0002-1390-6405>),\n  Carter Thunes [ctb],\n  Sarah E Davidson-Fritz [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2891-9380>),\n  Kelly Carstens [ctb] (ORCID: <https://orcid.org/0000-0002-1746-5379>),\n  Elizabeth Gilson [ctb],\n  Lindsay Knupp [ctb],\n  Lori Kolaczkowski [ctb],\n  Ashley Ko [ctb],\n  Zhihui Zhao [ctb],\n  Kurt Dunham [ctb],\n  Todd Zurlinden [ctb] (ORCID: <https://orcid.org/0000-0003-1372-3913>),\n  Parth Kothiya [ctb],\n  Woodrow R Setzer [ctb],\n  Matthew T Martin [ctb, ths],\n  Richard S Judson [ctb, ths] (ORCID:\n    <https://orcid.org/0000-0002-2348-9633>),\n  Katie Paul Friedman [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2710-1691>)",
    "url": "https://github.com/USEPA/CompTox-ToxCast-tcpl,\nhttps://www.epa.gov/comptox-tools/toxicity-forecasting-toxcast",
    "bug_reports": "https://github.com/USEPA/CompTox-ToxCast-tcpl/issues",
    "repository": "https://cran.r-project.org/package=tcpl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tcpl ToxCast Data Analysis Pipeline The ToxCast Data Analysis Pipeline ('tcpl') is an R package that manages, curve-fits, plots, and stores ToxCast data to populate its linked MySQL database, 'invitrodb'. The package was developed for the chemical screening data curated by the US EPA's Toxicity Forecaster (ToxCast) program, but 'tcpl' can be used to support diverse chemical screening efforts.  "
  },
  {
    "id": 21919,
    "package_name": "templates",
    "title": "A System for Working with Templates",
    "description": "Provides tools to work with template code and text in R. It aims to\n    provide a simple substitution mechanism for R-expressions inside these\n    templates. Templates can be written in other languages like 'SQL', can\n    simply be represented by characters in R, or can themselves be R-expressions\n    or functions.",
    "version": "0.4.0",
    "maintainer": "Sebastian Warnholz <wahani@gmail.com>",
    "author": "Sebastian Warnholz [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=templates",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "templates A System for Working with Templates Provides tools to work with template code and text in R. It aims to\n    provide a simple substitution mechanism for R-expressions inside these\n    templates. Templates can be written in other languages like 'SQL', can\n    simply be represented by characters in R, or can themselves be R-expressions\n    or functions.  "
  },
  {
    "id": 22031,
    "package_name": "thor",
    "title": "Interface to 'LMDB'",
    "description": "Key-value store, implemented as a wrapper around 'LMDB';\n    the \"lightning memory-mapped database\" <https://www.symas.com/mdb>.\n    'LMDB' is a transactional key value store that uses a memory map\n    for efficient access.  This package wraps the entire 'LMDB'\n    interface (except duplicated keys), and provides objects for\n    transactions and cursors.",
    "version": "1.2.0",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\n  Howard Chu [aut, cph],\n  Symas Corporation [cph],\n  Martin Hedenfalk [aut, cph],\n  The OpenLDAP Foundation [cph]",
    "url": "https://github.com/richfitz/thor, https://richfitz.github.io/thor/",
    "bug_reports": "https://github.com/richfitz/thor/issues",
    "repository": "https://cran.r-project.org/package=thor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "thor Interface to 'LMDB' Key-value store, implemented as a wrapper around 'LMDB';\n    the \"lightning memory-mapped database\" <https://www.symas.com/mdb>.\n    'LMDB' is a transactional key value store that uses a memory map\n    for efficient access.  This package wraps the entire 'LMDB'\n    interface (except duplicated keys), and provides objects for\n    transactions and cursors.  "
  },
  {
    "id": 22036,
    "package_name": "threesixtygiving",
    "title": "Download Charitable Grants from the '360Giving' Platform",
    "description": "Access open data from <https://www.threesixtygiving.org>, a \n    database of charitable grant giving in the UK operated by '360Giving'.\n    The package provides functions to search and retrieve data on charitable \n    grant giving, and process that data into tidy formats. It relies on the \n    '360Giving' data standard, described at \n    <https://standard.threesixtygiving.org/>.",
    "version": "0.2.2",
    "maintainer": "Evan Odell <evanodell91@gmail.com>",
    "author": "Evan Odell [aut, cre] (ORCID: <https://orcid.org/0000-0003-1845-808X>)",
    "url": "https://docs.evanodell.com/threesixtygiving,\nhttps://github.com/evanodell/threesixtygiving,",
    "bug_reports": "https://github.com/evanodell/threesixtygiving/issues",
    "repository": "https://cran.r-project.org/package=threesixtygiving",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "threesixtygiving Download Charitable Grants from the '360Giving' Platform Access open data from <https://www.threesixtygiving.org>, a \n    database of charitable grant giving in the UK operated by '360Giving'.\n    The package provides functions to search and retrieve data on charitable \n    grant giving, and process that data into tidy formats. It relies on the \n    '360Giving' data standard, described at \n    <https://standard.threesixtygiving.org/>.  "
  },
  {
    "id": 22048,
    "package_name": "tidier",
    "title": "Enhanced 'mutate'",
    "description": "Provides 'Apache Spark' style window aggregation for R dataframes and remote 'dbplyr' tables via 'mutate' in 'dplyr' flavour.",
    "version": "0.2.0",
    "maintainer": "Srikanth Komala Sheshachala <sri.teach@gmail.com>",
    "author": "Srikanth Komala Sheshachala [aut, cre]",
    "url": "https://github.com/talegari/tidier",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidier",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidier Enhanced 'mutate' Provides 'Apache Spark' style window aggregation for R dataframes and remote 'dbplyr' tables via 'mutate' in 'dplyr' flavour.  "
  },
  {
    "id": 22059,
    "package_name": "tidyREDCap",
    "title": "Helper Functions for Working with 'REDCap' Data",
    "description": "\n    Helper functions for processing 'REDCap' data in R. 'REDCap' is a \n    web-enabled application for building and managing surveys and databases \n    developed at Vanderbilt University.",
    "version": "1.1.2",
    "maintainer": "Raymond Balise <balise@miami.edu>",
    "author": "Raymond Balise [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9856-5901>),\n  Gabriel Odom [aut] (ORCID: <https://orcid.org/0000-0003-1341-4555>),\n  Anna Calderon [aut] (ORCID: <https://orcid.org/0000-0002-0139-3841>),\n  Layla Bouzoubaa [aut] (ORCID: <https://orcid.org/0000-0002-6616-0950>),\n  Wayne DeFreitas [aut] (ORCID: <https://orcid.org/0000-0002-2584-6278>),\n  Lauren Nahodyl [ctb] (ORCID: <https://orcid.org/0000-0001-6241-2615>),\n  Kyle Grealis [aut] (ORCID: <https://orcid.org/0000-0002-9223-8854>)",
    "url": "https://raymondbalise.github.io/tidyREDCap/index.html",
    "bug_reports": "https://github.com/RaymondBalise/tidyREDCap/issues",
    "repository": "https://cran.r-project.org/package=tidyREDCap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyREDCap Helper Functions for Working with 'REDCap' Data \n    Helper functions for processing 'REDCap' data in R. 'REDCap' is a \n    web-enabled application for building and managing surveys and databases \n    developed at Vanderbilt University.  "
  },
  {
    "id": 22113,
    "package_name": "tidyquery",
    "title": "Query 'R' Data Frames with 'SQL'",
    "description": "Use 'SQL' 'SELECT' statements to query 'R' data\n    frames.",
    "version": "0.2.4",
    "maintainer": "Ian Cook <ianmcook@gmail.com>",
    "author": "Ian Cook [aut, cre],\n  Cloudera [cph]",
    "url": "https://github.com/ianmcook/tidyquery",
    "bug_reports": "https://github.com/ianmcook/tidyquery/issues",
    "repository": "https://cran.r-project.org/package=tidyquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyquery Query 'R' Data Frames with 'SQL' Use 'SQL' 'SELECT' statements to query 'R' data\n    frames.  "
  },
  {
    "id": 22118,
    "package_name": "tidyrules",
    "title": "Utilities to Retrieve Rulelists from Model Fits, Filter, Prune,\nReorder and Predict on Unseen Data",
    "description": "Provides a framework to work with decision rules. Rules can be extracted from supported models, augmented with (custom) metrics using validation data, manipulated using standard dataframe operations, reordered and pruned based on a metric, predict on unseen (test) data. Utilities include; Creating a rulelist manually, Exporting a rulelist as a SQL case statement and so on. The package offers two classes; rulelist and ruleset based on dataframe.",
    "version": "0.2.7",
    "maintainer": "Srikanth Komala Sheshachala <sri.teach@gmail.com>",
    "author": "Srikanth Komala Sheshachala [aut, cre],\n  Amith Kumar Ullur Raghavendra [aut]",
    "url": "https://github.com/talegari/tidyrules,\nhttps://talegari.github.io/tidyrules/",
    "bug_reports": "https://github.com/talegari/tidyrules/issues",
    "repository": "https://cran.r-project.org/package=tidyrules",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyrules Utilities to Retrieve Rulelists from Model Fits, Filter, Prune,\nReorder and Predict on Unseen Data Provides a framework to work with decision rules. Rules can be extracted from supported models, augmented with (custom) metrics using validation data, manipulated using standard dataframe operations, reordered and pruned based on a metric, predict on unseen (test) data. Utilities include; Creating a rulelist manually, Exporting a rulelist as a SQL case statement and so on. The package offers two classes; rulelist and ruleset based on dataframe.  "
  },
  {
    "id": 22140,
    "package_name": "tidywikidatar",
    "title": "Explore 'Wikidata' Through Tidy Data Frames",
    "description": "Query 'Wikidata' API <https://www.wikidata.org/wiki/Wikidata:Main_Page> with ease, get tidy data frames in response, and cache data in a local database.",
    "version": "0.5.9",
    "maintainer": "Giorgio Comai <giorgio.comai@cci.tn.it>",
    "author": "Giorgio Comai [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-0515-9542>),\n  EDJNet [fnd]",
    "url": "https://edjnet.github.io/tidywikidatar/,\nhttps://github.com/EDJNet/tidywikidatar",
    "bug_reports": "https://github.com/EDJNet/tidywikidatar/issues",
    "repository": "https://cran.r-project.org/package=tidywikidatar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidywikidatar Explore 'Wikidata' Through Tidy Data Frames Query 'Wikidata' API <https://www.wikidata.org/wiki/Wikidata:Main_Page> with ease, get tidy data frames in response, and cache data in a local database.  "
  },
  {
    "id": 22148,
    "package_name": "tigreBrowserWriter",
    "title": "'tigreBrowser' Database Writer",
    "description": "Write modelling results into a database for\n    'tigreBrowser', a web-based tool for browsing figures and summary\n    data of independent model fits, such as Gaussian process models\n    fitted for each gene or other genomic element. The browser is\n    available at <https://github.com/PROBIC/tigreBrowser>.",
    "version": "0.1.5",
    "maintainer": "Antti Honkela <antti.honkela@helsinki.fi>",
    "author": "Antti Honkela [aut, cre],\n  Miika-Petteri Matikainen [aut]",
    "url": "https://github.com/PROBIC/tigreBrowserWriter",
    "bug_reports": "https://github.com/PROBIC/tigreBrowserWriter/issues",
    "repository": "https://cran.r-project.org/package=tigreBrowserWriter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tigreBrowserWriter 'tigreBrowser' Database Writer Write modelling results into a database for\n    'tigreBrowser', a web-based tool for browsing figures and summary\n    data of independent model fits, such as Gaussian process models\n    fitted for each gene or other genomic element. The browser is\n    available at <https://github.com/PROBIC/tigreBrowser>.  "
  },
  {
    "id": 22153,
    "package_name": "tiledb",
    "title": "Modern Database Engine for Complex Data Based on\nMulti-Dimensional Arrays",
    "description": "The modern database 'TileDB' introduces a powerful on-disk\n  format for storing and accessing any complex data based on multi-dimensional\n  arrays. It supports dense and sparse arrays, dataframes and key-values stores,\n  cloud storage ('S3', 'GCS', 'Azure'), chunked arrays, multiple compression,\n  encryption and checksum filters, uses a fully multi-threaded implementation,\n  supports parallel I/O, data versioning ('time travel'), metadata and groups.\n  It is implemented as an embeddable cross-platform C++ library with APIs from\n  several languages, and integrations. This package provides the R support.",
    "version": "0.33.0",
    "maintainer": "Isaiah Norton <isaiah@tiledb.com>",
    "author": "TileDB, Inc. [aut, cph],\n  Isaiah Norton [cre]",
    "url": "https://github.com/TileDB-Inc/TileDB-R,\nhttps://tiledb-inc.github.io/TileDB-R/",
    "bug_reports": "https://github.com/TileDB-Inc/TileDB-R/issues",
    "repository": "https://cran.r-project.org/package=tiledb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tiledb Modern Database Engine for Complex Data Based on\nMulti-Dimensional Arrays The modern database 'TileDB' introduces a powerful on-disk\n  format for storing and accessing any complex data based on multi-dimensional\n  arrays. It supports dense and sparse arrays, dataframes and key-values stores,\n  cloud storage ('S3', 'GCS', 'Azure'), chunked arrays, multiple compression,\n  encryption and checksum filters, uses a fully multi-threaded implementation,\n  supports parallel I/O, data versioning ('time travel'), metadata and groups.\n  It is implemented as an embeddable cross-platform C++ library with APIs from\n  several languages, and integrations. This package provides the R support.  "
  },
  {
    "id": 22176,
    "package_name": "timeseriesdb",
    "title": "A Time Series Database for Official Statistics with R and\nPostgreSQL",
    "description": "Archive and manage times series data from official statistics. The 'timeseriesdb' package was designed to manage a large catalog of time series from official statistics which are typically published on a monthly, quarterly or yearly basis. Thus timeseriesdb is optimized to handle updates caused by data revision as well as elaborate, multi-lingual meta information. ",
    "version": "1.0.0-1.1.2",
    "maintainer": "Matthias Bannert <bannert@kof.ethz.ch>",
    "author": "Matthias Bannert [aut, cre],\n  Severin Th\u00f6ni [aut],\n  Ioan Gabriel Bucur [ctb]",
    "url": "https://github.com/mbannert/timeseriesdb",
    "bug_reports": "https://github.com/mbannert/timeseriesdb/issues",
    "repository": "https://cran.r-project.org/package=timeseriesdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "timeseriesdb A Time Series Database for Official Statistics with R and\nPostgreSQL Archive and manage times series data from official statistics. The 'timeseriesdb' package was designed to manage a large catalog of time series from official statistics which are typically published on a monthly, quarterly or yearly basis. Thus timeseriesdb is optimized to handle updates caused by data revision as well as elaborate, multi-lingual meta information.   "
  },
  {
    "id": 22186,
    "package_name": "tinyarray",
    "title": "Expression Data Analysis and Visualization",
    "description": "The Gene Expression Omnibus (<https://www.ncbi.nlm.nih.gov/geo/>) and The Cancer Genome Atlas (<https://portal.gdc.cancer.gov/>) are widely used medical public databases. Our platform integrates routine analysis and visualization tools for expression data to provide concise and intuitive data analysis and presentation.",
    "version": "2.4.3",
    "maintainer": "Xiaojie Sun <18763899370@163.com>",
    "author": "Xiaojie Sun [aut, cre]",
    "url": "https://github.com/xjsun1221/tinyarray",
    "bug_reports": "https://github.com/xjsun1221/tinyarray/issues",
    "repository": "https://cran.r-project.org/package=tinyarray",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tinyarray Expression Data Analysis and Visualization The Gene Expression Omnibus (<https://www.ncbi.nlm.nih.gov/geo/>) and The Cancer Genome Atlas (<https://portal.gdc.cancer.gov/>) are widely used medical public databases. Our platform integrates routine analysis and visualization tools for expression data to provide concise and intuitive data analysis and presentation.  "
  },
  {
    "id": 22256,
    "package_name": "tomba",
    "title": "Official R Library for Tomba Email Finder",
    "description": "Email Finder R Client Library.\n        Search emails are based on the website You give one domain name and it returns all the email addresses found on the internet.\n        Email Finder generates or retrieves the most likely email address from a domain name, a first name and a last name.\n        Email verify checks the deliverability of a given email address, verifies if it has been found in our database, and returns their sources.",
    "version": "1.0.1",
    "maintainer": "Abedrahim Ben rebia <b.abedrahim@tomba.io>",
    "author": "Abedrahim Ben rebia [aut, cre],Mohamed Ben rebia [ctb], Tomba.io [cph]",
    "url": "https://tomba.io/,https://github.com/tomba-io/r",
    "bug_reports": "https://github.com/tomba-io/r/issues",
    "repository": "https://cran.r-project.org/package=tomba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tomba Official R Library for Tomba Email Finder Email Finder R Client Library.\n        Search emails are based on the website You give one domain name and it returns all the email addresses found on the internet.\n        Email Finder generates or retrieves the most likely email address from a domain name, a first name and a last name.\n        Email verify checks the deliverability of a given email address, verifies if it has been found in our database, and returns their sources.  "
  },
  {
    "id": 22313,
    "package_name": "tracenma",
    "title": "Database for Developing Transitivity Methodology in Network\nMeta-Analysis",
    "description": "Functions to access the database of 217 data-frames with aggregate \n    study-level characteristics (that may act as effect modifiers) extracted \n    from published systematic reviews with network meta-analysis. The database shall \n    only be used for developing and appraising the methodology to assess \n    the transitivity assumption quantitatively.",
    "version": "0.1.1",
    "maintainer": "Loukia Spineli <Spineli.Loukia@mh-hannover.de>",
    "author": "Loukia Spineli [aut, cre]",
    "url": "https://CRAN.R-project.org/package=tracenma,\nhttps://github.com/LoukiaSpin/tracenma,\nhttps://loukiaspin.github.io/tracenma/",
    "bug_reports": "https://github.com/LoukiaSpin/tracenma/issues",
    "repository": "https://cran.r-project.org/package=tracenma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tracenma Database for Developing Transitivity Methodology in Network\nMeta-Analysis Functions to access the database of 217 data-frames with aggregate \n    study-level characteristics (that may act as effect modifiers) extracted \n    from published systematic reviews with network meta-analysis. The database shall \n    only be used for developing and appraising the methodology to assess \n    the transitivity assumption quantitatively.  "
  },
  {
    "id": 22451,
    "package_name": "tsdb",
    "title": "Terribly-Simple Data Base for Time Series",
    "description": "A terribly-simple data base for numeric\n  time series, written purely in R, so no external\n  database-software is needed. Series are stored in\n  plain-text files (the most-portable and enduring file\n  type) in CSV format. Timestamps are encoded using R's\n  native numeric representation for 'Date'/'POSIXct',\n  which makes them fast to parse, but keeps them\n  accessible with other software. The package provides\n  tools for saving and updating series in this\n  standardised format, for retrieving and joining data,\n  for summarising files and directories, and for\n  coercing series from and to other data types (such as\n  'zoo' series).",
    "version": "1.1-0",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "http://enricoschumann.net/R/packages/tsdb/,\nhttps://github.com/enricoschumann/tsdb,\nhttps://gitlab.com/enricoschumann/tsdb",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tsdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsdb Terribly-Simple Data Base for Time Series A terribly-simple data base for numeric\n  time series, written purely in R, so no external\n  database-software is needed. Series are stored in\n  plain-text files (the most-portable and enduring file\n  type) in CSV format. Timestamps are encoded using R's\n  native numeric representation for 'Date'/'POSIXct',\n  which makes them fast to parse, but keeps them\n  accessible with other software. The package provides\n  tools for saving and updating series in this\n  standardised format, for retrieving and joining data,\n  for summarising files and directories, and for\n  coercing series from and to other data types (such as\n  'zoo' series).  "
  },
  {
    "id": 22457,
    "package_name": "tsensembler",
    "title": "Dynamic Ensembles for Time Series Forecasting",
    "description": "A framework for dynamically combining forecasting models for time series forecasting predictive tasks. It leverages machine learning models from other packages to automatically combine expert advice using metalearning and other state-of-the-art forecasting combination approaches. The predictive methods receive a data matrix as input, representing an embedded time series, and return a predictive ensemble model. The ensemble use generic functions 'predict()' and 'forecast()' to forecast future values of the time series. Moreover, an ensemble can be updated using methods, such as 'update_weights()' or 'update_base_models()'. A complete description of the methods can be found in: Cerqueira, V., Torgo, L., Pinto, F., and Soares, C. \"Arbitrated Ensemble for Time Series Forecasting.\" to appear at: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer International Publishing, 2017; and Cerqueira, V., Torgo, L., and Soares, C.: \"Arbitrated Ensemble for Solar Radiation Forecasting.\" International Work-Conference on Artificial Neural Networks. Springer, 2017 <doi:10.1007/978-3-319-59153-7_62>.",
    "version": "0.1.0",
    "maintainer": "Vitor Cerqueira <cerqueira.vitormanuel@gmail.com>",
    "author": "Vitor Cerqueira [aut, cre],\n  Luis Torgo [ctb],\n  Carlos Soares [ctb]",
    "url": "https://github.com/vcerqueira/tsensembler",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tsensembler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsensembler Dynamic Ensembles for Time Series Forecasting A framework for dynamically combining forecasting models for time series forecasting predictive tasks. It leverages machine learning models from other packages to automatically combine expert advice using metalearning and other state-of-the-art forecasting combination approaches. The predictive methods receive a data matrix as input, representing an embedded time series, and return a predictive ensemble model. The ensemble use generic functions 'predict()' and 'forecast()' to forecast future values of the time series. Moreover, an ensemble can be updated using methods, such as 'update_weights()' or 'update_base_models()'. A complete description of the methods can be found in: Cerqueira, V., Torgo, L., Pinto, F., and Soares, C. \"Arbitrated Ensemble for Time Series Forecasting.\" to appear at: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer International Publishing, 2017; and Cerqueira, V., Torgo, L., and Soares, C.: \"Arbitrated Ensemble for Solar Radiation Forecasting.\" International Work-Conference on Artificial Neural Networks. Springer, 2017 <doi:10.1007/978-3-319-59153-7_62>.  "
  },
  {
    "id": 22573,
    "package_name": "tzupdater",
    "title": "Time Zone Database Updater",
    "description": "Download and compile any version of the\n  IANA Time Zone Database (also known as Olson database)\n  and make it current in your R session.\n  Beware: on Windows 'Cygwin' is required!",
    "version": "0.1.5",
    "maintainer": "Sebastien Thonnard <sebastien.thonnard@icloud.com>",
    "author": "Sebastien Thonnard",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tzupdater",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tzupdater Time Zone Database Updater Download and compile any version of the\n  IANA Time Zone Database (also known as Olson database)\n  and make it current in your R session.\n  Beware: on Windows 'Cygwin' is required!  "
  },
  {
    "id": 22591,
    "package_name": "ugatsdb",
    "title": "Uganda Time Series Database API",
    "description": "An R API providing easy access to a relational database with macroeconomic, \n             financial and development related time series data for Uganda. \n             Overall more than 5000 series at varying frequency (daily, monthly, \n             quarterly, annual in fiscal or calendar years) can be accessed through \n             the API. The data is provided by the Bank of Uganda, \n             the Ugandan Ministry of Finance, Planning and Economic Development,\n             the IMF and the World Bank. The database is being updated once a month. ",
    "version": "0.2.3",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre]",
    "url": "https://mepd.finance.go.ug/apps.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ugatsdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ugatsdb Uganda Time Series Database API An R API providing easy access to a relational database with macroeconomic, \n             financial and development related time series data for Uganda. \n             Overall more than 5000 series at varying frequency (daily, monthly, \n             quarterly, annual in fiscal or calendar years) can be accessed through \n             the API. The data is provided by the Bank of Uganda, \n             the Ugandan Ministry of Finance, Planning and Economic Development,\n             the IMF and the World Bank. The database is being updated once a month.   "
  },
  {
    "id": 22643,
    "package_name": "units",
    "title": "Measurement Units for R Vectors",
    "description": "Support for measurement units in R vectors, matrices\n    and arrays: automatic propagation, conversion, derivation\n    and simplification of units; raising errors in case of unit\n    incompatibility. Compatible with the POSIXct, Date and difftime \n    classes. Uses the UNIDATA udunits library and unit database for \n    unit compatibility checking and conversion.\n    Documentation about 'units' is provided in the paper by Pebesma, Mailund &\n    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a\n    vignette; see 'citation(\"units\")' for details.",
    "version": "1.0-0",
    "maintainer": "Edzer Pebesma <edzer.pebesma@uni-muenster.de>",
    "author": "Edzer Pebesma [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8049-7069>),\n  Thomas Mailund [aut],\n  Tomasz Kalinowski [aut],\n  James Hiebert [ctb],\n  I\u00f1aki Ucar [aut] (ORCID: <https://orcid.org/0000-0001-6403-5550>),\n  Thomas Lin Pedersen [ctb]",
    "url": "https://r-quantities.github.io/units/,\nhttps://github.com/r-quantities/units",
    "bug_reports": "https://github.com/r-quantities/units/issues",
    "repository": "https://cran.r-project.org/package=units",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "units Measurement Units for R Vectors Support for measurement units in R vectors, matrices\n    and arrays: automatic propagation, conversion, derivation\n    and simplification of units; raising errors in case of unit\n    incompatibility. Compatible with the POSIXct, Date and difftime \n    classes. Uses the UNIDATA udunits library and unit database for \n    unit compatibility checking and conversion.\n    Documentation about 'units' is provided in the paper by Pebesma, Mailund &\n    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a\n    vignette; see 'citation(\"units\")' for details.  "
  },
  {
    "id": 22650,
    "package_name": "unjoin",
    "title": "Separate a Data Frame by Normalization",
    "description": "Separate a data frame in two based on key columns. The function\n unjoin() provides an inside-out version of a nested data frame. This is used\n to identify duplication and normalize it (in the database sense) by linking\n two tables with the redundancy removed. This is a basic requirement for\n detecting topology within spatial structures that has motivated the need for\n this package as a building block for workflows within more applied projects.",
    "version": "0.1.0",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre],\n  Simon Wotherspoon [ctb],\n  Hadley Wickham [ctb] (named the concept, provided excellent guidance\n    via tidyr source code)",
    "url": "https://github.com/hypertidy/unjoin",
    "bug_reports": "https://github.com/hypertidy/unjoin/issues",
    "repository": "https://cran.r-project.org/package=unjoin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unjoin Separate a Data Frame by Normalization Separate a data frame in two based on key columns. The function\n unjoin() provides an inside-out version of a nested data frame. This is used\n to identify duplication and normalize it (in the database sense) by linking\n two tables with the redundancy removed. This is a basic requirement for\n detecting topology within spatial structures that has motivated the need for\n this package as a building block for workflows within more applied projects.  "
  },
  {
    "id": 22671,
    "package_name": "uptasticsearch",
    "title": "Get Data Frame Representations of 'Elasticsearch' Results",
    "description": "\n    'Elasticsearch' is an open-source, distributed, document-based datastore\n    (<https://www.elastic.co/products/elasticsearch>).\n    It provides an 'HTTP' 'API' for querying the database and extracting datasets, but that\n    'API' was not designed for common data science workflows like pulling large batches of\n    records and normalizing those documents into a data frame that can be used as a training\n    dataset for statistical models. 'uptasticsearch' provides an interface for 'Elasticsearch'\n    that is explicitly designed to make these data science workflows easy and fun.",
    "version": "1.0.0",
    "maintainer": "James Lamb <jaylamb20@gmail.com>",
    "author": "James Lamb [aut, cre],\n  Nick Paras [aut],\n  Austin Dickey [aut],\n  Michael Frasco [ctb],\n  Weiwen Gu [ctb],\n  Will Dearden [ctb],\n  Uptake Technologies Inc. [cph]",
    "url": "https://github.com/uptake/uptasticsearch",
    "bug_reports": "https://github.com/uptake/uptasticsearch/issues",
    "repository": "https://cran.r-project.org/package=uptasticsearch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uptasticsearch Get Data Frame Representations of 'Elasticsearch' Results \n    'Elasticsearch' is an open-source, distributed, document-based datastore\n    (<https://www.elastic.co/products/elasticsearch>).\n    It provides an 'HTTP' 'API' for querying the database and extracting datasets, but that\n    'API' was not designed for common data science workflows like pulling large batches of\n    records and normalizing those documents into a data frame that can be used as a training\n    dataset for statistical models. 'uptasticsearch' provides an interface for 'Elasticsearch'\n    that is explicitly designed to make these data science workflows easy and fun.  "
  },
  {
    "id": 22684,
    "package_name": "uscoauditlog",
    "title": "United States Copyright Office Product Management Division SR\nAudit Data Dataset Cleaning Algorithms",
    "description": "Intended to be used by the United States Copyright Office Product Management Division Business Analysts. Include algorithms for the United States Copyright Office Product Management Division SR Audit Data dataset. The algorithm takes in the SR Audit Data excel file and reformat the spreadsheet such that the values and variables fit the format of the online database. Support functions in this package include clean_str(), which cleans instances of variable AUDIT_LOG; clean_data_to_excel(), which cleans and output the reorganized SR Audit Data dataset in excel format; clean_data_to_dataframe(), which cleans and stores the reorganized SR Audit Data data set to a data frame; format_from_excel(), which reads in the outputted excel file from the clean_data_to_excel() function and formats and returns the data as a dictionary that uses FIELD types as keys and NON-FIELD types as the values of those keys. format_from_dataframe(), which reads in the outputted data frame from the clean_data_to_dataframe() function and formats and returns the data as a dictionary that uses FIELD types as keys and NON-FIELD types as the values of those keys; support_function(), which takes in the dictionary outputted either from the format_from_dataframe() or format_from_excel() function and returns the data as a formatted data frame according to the original U.S. Copyright Office SR Audit Data online database. The main function of this package is clean_format_all(), which takes in an excel file and returns the formatted data into a new excel and text file according to the format from the U.S. Copyright Office SR Audit Data online database. ",
    "version": "1.0.3",
    "maintainer": "Frederick Liu <sliu85@u.rochester.edu>",
    "author": "Frederick Liu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=uscoauditlog",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uscoauditlog United States Copyright Office Product Management Division SR\nAudit Data Dataset Cleaning Algorithms Intended to be used by the United States Copyright Office Product Management Division Business Analysts. Include algorithms for the United States Copyright Office Product Management Division SR Audit Data dataset. The algorithm takes in the SR Audit Data excel file and reformat the spreadsheet such that the values and variables fit the format of the online database. Support functions in this package include clean_str(), which cleans instances of variable AUDIT_LOG; clean_data_to_excel(), which cleans and output the reorganized SR Audit Data dataset in excel format; clean_data_to_dataframe(), which cleans and stores the reorganized SR Audit Data data set to a data frame; format_from_excel(), which reads in the outputted excel file from the clean_data_to_excel() function and formats and returns the data as a dictionary that uses FIELD types as keys and NON-FIELD types as the values of those keys. format_from_dataframe(), which reads in the outputted data frame from the clean_data_to_dataframe() function and formats and returns the data as a dictionary that uses FIELD types as keys and NON-FIELD types as the values of those keys; support_function(), which takes in the dictionary outputted either from the format_from_dataframe() or format_from_excel() function and returns the data as a formatted data frame according to the original U.S. Copyright Office SR Audit Data online database. The main function of this package is clean_format_all(), which takes in an excel file and returns the formatted data into a new excel and text file according to the format from the U.S. Copyright Office SR Audit Data online database.   "
  },
  {
    "id": 22815,
    "package_name": "vegdata",
    "title": "Access Vegetation Databases and Treat Taxonomy",
    "description": "Handling of vegetation data from different sources ( Turboveg\n  2.0 <https://www.synbiosys.alterra.nl/turboveg/>; the German national\n  repository <https://www.vegetweb.de> and others. Taxonomic\n  harmonization (given appropriate taxonomic lists, e.g. the Euro+Med\n  list <https://eurosl.infinitenature.org>).",
    "version": "1.9.15",
    "maintainer": "Florian Jansen <florian.jansen@uni-rostock.de>",
    "author": "Florian Jansen [aut, cre]",
    "url": "https://git.loe.auf.uni-rostock.de/jansen/vegdata.git",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vegdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vegdata Access Vegetation Databases and Treat Taxonomy Handling of vegetation data from different sources ( Turboveg\n  2.0 <https://www.synbiosys.alterra.nl/turboveg/>; the German national\n  repository <https://www.vegetweb.de> and others. Taxonomic\n  harmonization (given appropriate taxonomic lists, e.g. the Euro+Med\n  list <https://eurosl.infinitenature.org>).  "
  },
  {
    "id": 22817,
    "package_name": "vegtable",
    "title": "Handling Vegetation Data Sets",
    "description": "Import and handling data from vegetation-plot databases, especially\n    data stored in 'Turboveg 2' (<https://www.synbiosys.alterra.nl/turboveg/>).\n    Also import/export routines for exchange of data with 'Juice'\n    (<https://www.sci.muni.cz/botany/juice/>) are implemented.",
    "version": "0.1.10",
    "maintainer": "Miguel Alvarez <kamapu78@gmail.com>",
    "author": "Miguel Alvarez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1500-1834>)",
    "url": "https://github.com/kamapu/vegtable,\nhttp://kamapu.github.io/vegtable/",
    "bug_reports": "https://github.com/kamapu/vegtable/issues",
    "repository": "https://cran.r-project.org/package=vegtable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vegtable Handling Vegetation Data Sets Import and handling data from vegetation-plot databases, especially\n    data stored in 'Turboveg 2' (<https://www.synbiosys.alterra.nl/turboveg/>).\n    Also import/export routines for exchange of data with 'Juice'\n    (<https://www.sci.muni.cz/botany/juice/>) are implemented.  "
  },
  {
    "id": 22818,
    "package_name": "vein",
    "title": "Vehicular Emissions Inventories",
    "description": "Elaboration of vehicular emissions inventories,\n    consisting in four stages, pre-processing activity data, preparing \n    emissions factors, estimating the emissions and post-processing of emissions \n    in maps and databases. More details in Ibarra-Espinosa et al (2018) <doi:10.5194/gmd-11-2209-2018>.\n    Before using VEIN you need to know the vehicular composition of your study area, in other words,\n    the combination of of type of vehicles, size and fuel of the fleet. Then, it is recommended to\n    start with the project to download a template to create a structure of directories and scripts.",
    "version": "1.5.0",
    "maintainer": "Sergio Ibarra-Espinosa <zergioibarra@gmail.com>",
    "author": "Sergio Ibarra-Espinosa [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3162-1905>),\n  Daniel Schuch [ctb] (ORCID: <https://orcid.org/0000-0001-5977-4519>),\n  Joao Bazzo [ctb] (ORCID: <https://orcid.org/0000-0002-7371-1116>),\n  Mario Gavidia-Calder\u00f3n [ctb] (ORCID:\n    <https://orcid.org/0000-0003-4536-5006>),\n  Karl Ropkins [ctb] (ORCID: <https://orcid.org/0000-0002-0294-6997>)",
    "url": "https://github.com/atmoschem/vein",
    "bug_reports": "https://github.com/atmoschem/vein/issues",
    "repository": "https://cran.r-project.org/package=vein",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vein Vehicular Emissions Inventories Elaboration of vehicular emissions inventories,\n    consisting in four stages, pre-processing activity data, preparing \n    emissions factors, estimating the emissions and post-processing of emissions \n    in maps and databases. More details in Ibarra-Espinosa et al (2018) <doi:10.5194/gmd-11-2209-2018>.\n    Before using VEIN you need to know the vehicular composition of your study area, in other words,\n    the combination of of type of vehicles, size and fuel of the fleet. Then, it is recommended to\n    start with the project to download a template to create a structure of directories and scripts.  "
  },
  {
    "id": 22820,
    "package_name": "velociraptr",
    "title": "Fossil Analysis",
    "description": "Functions for downloading, reshaping, culling, cleaning, and analyzing fossil data from the Paleobiology Database <https://paleobiodb.org>.",
    "version": "1.1.0",
    "maintainer": "Andrew A Zaffos <azaffos@email.arizona.edu>",
    "author": "Andrew A. Zaffos",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=velociraptr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "velociraptr Fossil Analysis Functions for downloading, reshaping, culling, cleaning, and analyzing fossil data from the Paleobiology Database <https://paleobiodb.org>.  "
  },
  {
    "id": 22854,
    "package_name": "vigicaen",
    "title": "'VigiBase' Pharmacovigilance Database Toolbox",
    "description": "Perform the analysis of the World Health Organization\n       (WHO) Pharmacovigilance database 'VigiBase' (Extract Case Level version),\n       <https://who-umc.org/>\n       e.g., load data, perform data management,\n       disproportionality analysis, and descriptive statistics. Intended for\n       pharmacovigilance routine use or studies.\n       This package is NOT supported nor reflect the opinion of the WHO, or the\n       Uppsala Monitoring Centre.\n       Disproportionality methods are described by Nor\u00e9n et \n       al (2013) <doi:10.1177/0962280211403604>.",
    "version": "0.16.1",
    "maintainer": "Charles Dolladille <cdolladille@hotmail.com>",
    "author": "Charles Dolladille [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0449-6261>),\n  Basile Chr\u00e9tien [aut] (ORCID: <https://orcid.org/0000-0002-7483-2489>),\n  Universite de Caen Normandie [cph] (Caen, France),\n  Unite de pharmaco-epidemiologie [cph] (Service de pharmacologie, Centre\n    Hospitalier Universitaire de Caen, Caen, France)",
    "url": "https://github.com/pharmacologie-caen/vigicaen,\nhttps://pharmacologie-caen.github.io/vigicaen/",
    "bug_reports": "https://github.com/pharmacologie-caen/vigicaen/issues",
    "repository": "https://cran.r-project.org/package=vigicaen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vigicaen 'VigiBase' Pharmacovigilance Database Toolbox Perform the analysis of the World Health Organization\n       (WHO) Pharmacovigilance database 'VigiBase' (Extract Case Level version),\n       <https://who-umc.org/>\n       e.g., load data, perform data management,\n       disproportionality analysis, and descriptive statistics. Intended for\n       pharmacovigilance routine use or studies.\n       This package is NOT supported nor reflect the opinion of the WHO, or the\n       Uppsala Monitoring Centre.\n       Disproportionality methods are described by Nor\u00e9n et \n       al (2013) <doi:10.1177/0962280211403604>.  "
  },
  {
    "id": 22881,
    "package_name": "vismeteor",
    "title": "Analysis of Visual Meteor Data",
    "description": "Provides a suite of analytical functionalities to process and analyze\n    visual meteor observations from the Visual Meteor Database\n    of the International Meteor Organization <https://www.imo.net/>.",
    "version": "2.0.1",
    "maintainer": "Janko Richter <janko@richtej.de>",
    "author": "Janko Richter [aut, cre]",
    "url": "https://github.com/jankorichter/vismeteor",
    "bug_reports": "https://github.com/jankorichter/vismeteor/issues",
    "repository": "https://cran.r-project.org/package=vismeteor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vismeteor Analysis of Visual Meteor Data Provides a suite of analytical functionalities to process and analyze\n    visual meteor observations from the Visual Meteor Database\n    of the International Meteor Organization <https://www.imo.net/>.  "
  },
  {
    "id": 22978,
    "package_name": "walkscoreAPI",
    "title": "Walk Score and Transit Score API",
    "description": "A collection of functions to perform the Application\n        Programming Interface (API) calls associated with the Walk\n        Score website (www.walkscore.com) within the R environment.\n        These functions can be used to query the Walk Score and Transit\n        Score database for a wide variety of information using R\n        scripts. This package includes the simple Walk Score and\n        Transit Score API calls, which return the scores associated\n        with an input location, as well as calls which return some data\n        used to calculate the scores. These functions are especially\n        useful for mass data collection and gathering Walk Score and\n        Transit Score values for large lists of locations.",
    "version": "1.2",
    "maintainer": "John Whalen <whalenjf@gmail.com>",
    "author": "John Whalen",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=walkscoreAPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "walkscoreAPI Walk Score and Transit Score API A collection of functions to perform the Application\n        Programming Interface (API) calls associated with the Walk\n        Score website (www.walkscore.com) within the R environment.\n        These functions can be used to query the Walk Score and Transit\n        Score database for a wide variety of information using R\n        scripts. This package includes the simple Walk Score and\n        Transit Score API calls, which return the scores associated\n        with an input location, as well as calls which return some data\n        used to calculate the scores. These functions are especially\n        useful for mass data collection and gathering Walk Score and\n        Transit Score values for large lists of locations.  "
  },
  {
    "id": 22983,
    "package_name": "wand",
    "title": "Retrieve 'Magic' Attributes from Files and Directories",
    "description": "'MIME' types are shorthand descriptors for\n      file contents and can be determined from \"magic\"\n      bytes in file headers, file contents or intuited\n      from file extensions. Tools are provided to\n      perform curated \"magic\" tests as well as mapping\n      'MIME' types from a database of over 1,500\n      extension mappings.",
    "version": "0.5.0",
    "maintainer": "Bob Rudis <bob@rud.is>",
    "author": "Bob Rudis [aut, cre] (ORCID: <https://orcid.org/0000-0001-5670-2640>)",
    "url": "http://gitlab.com/hrbrmstr/wand",
    "bug_reports": "https://gitlab.com/hrbrmstr/wand/issues",
    "repository": "https://cran.r-project.org/package=wand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wand Retrieve 'Magic' Attributes from Files and Directories 'MIME' types are shorthand descriptors for\n      file contents and can be determined from \"magic\"\n      bytes in file headers, file contents or intuited\n      from file extensions. Tools are provided to\n      perform curated \"magic\" tests as well as mapping\n      'MIME' types from a database of over 1,500\n      extension mappings.  "
  },
  {
    "id": 22994,
    "package_name": "waspasR",
    "title": "Tool Kit to Implement a W.A.S.P.A.S. Based Multi-Criteria\nDecision Analysis Solution",
    "description": "Provides a set of functions to implement decision-making systems\n    based on the W.A.S.P.A.S. method (Weighted Aggregated Sum Product Assessment),\n    Chakraborty and Zavadskas (2012) <doi:10.5755/j01.eee.122.6.1810>.\n    So this package offers functions that analyze and validate the\n    raw data, which must be entered in a determined format;\n    extract specific vectors and matrices from this raw database;\n    normalize the input data; calculate rankings by intermediate methods;\n    apply the lambda parameter for the main method; and a function that does\n    everything at once. The package has an example database called choppers,\n    with which the user can see how the input data should be organized so that\n    everything works as recommended by the decision methods based on multiple\n    criteria that this package solves. Basically, the data are composed of a set\n    of alternatives, which will be ranked, a set of choice criteria, a matrix\n    of values for each Alternative-Criterion relationship, a vector of weights\n    associated with the criteria, since certain criteria are considered more\n    important than others, as well as a vector that defines each criterion as\n    cost or benefit, this determines the calculation formula, as there are those\n    criteria that we want the highest possible value (e.g. durability)\n    and others that we want the lowest possible value (e.g. price).",
    "version": "0.1.5",
    "maintainer": "Flavio Barbara <flavio.barbara@gmail.com>",
    "author": "Flavio Barbara [cre, aut],\n  Marcos Santos [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=waspasR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "waspasR Tool Kit to Implement a W.A.S.P.A.S. Based Multi-Criteria\nDecision Analysis Solution Provides a set of functions to implement decision-making systems\n    based on the W.A.S.P.A.S. method (Weighted Aggregated Sum Product Assessment),\n    Chakraborty and Zavadskas (2012) <doi:10.5755/j01.eee.122.6.1810>.\n    So this package offers functions that analyze and validate the\n    raw data, which must be entered in a determined format;\n    extract specific vectors and matrices from this raw database;\n    normalize the input data; calculate rankings by intermediate methods;\n    apply the lambda parameter for the main method; and a function that does\n    everything at once. The package has an example database called choppers,\n    with which the user can see how the input data should be organized so that\n    everything works as recommended by the decision methods based on multiple\n    criteria that this package solves. Basically, the data are composed of a set\n    of alternatives, which will be ranked, a set of choice criteria, a matrix\n    of values for each Alternative-Criterion relationship, a vector of weights\n    associated with the criteria, since certain criteria are considered more\n    important than others, as well as a vector that defines each criterion as\n    cost or benefit, this determines the calculation formula, as there are those\n    criteria that we want the highest possible value (e.g. durability)\n    and others that we want the lowest possible value (e.g. price).  "
  },
  {
    "id": 23026,
    "package_name": "wdpar",
    "title": "Interface to the World Database on Protected Areas",
    "description": "Fetch and clean data from the World Database on Protected\n    Areas (WDPA) and the World Database on Other Effective Area-Based\n    Conservation Measures (WDOECM). Data is obtained from Protected Planet\n    <https://www.protectedplanet.net/en>. To augment data cleaning procedures,\n    users can install the 'prepr' R package (available at\n    <https://github.com/prioritizr/prepr>). For more information on this\n    package, see Hanson (2022) <doi:10.21105/joss.04594>.",
    "version": "1.3.8",
    "maintainer": "Jeffrey O Hanson <jeffrey.hanson@uqconnect.edu.au>",
    "author": "Jeffrey O Hanson [aut, cre]",
    "url": "https://prioritizr.github.io/wdpar/,\nhttps://github.com/prioritizr/wdpar",
    "bug_reports": "https://github.com/prioritizr/wdpar/issues",
    "repository": "https://cran.r-project.org/package=wdpar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wdpar Interface to the World Database on Protected Areas Fetch and clean data from the World Database on Protected\n    Areas (WDPA) and the World Database on Other Effective Area-Based\n    Conservation Measures (WDOECM). Data is obtained from Protected Planet\n    <https://www.protectedplanet.net/en>. To augment data cleaning procedures,\n    users can install the 'prepr' R package (available at\n    <https://github.com/prioritizr/prepr>). For more information on this\n    package, see Hanson (2022) <doi:10.21105/joss.04594>.  "
  },
  {
    "id": 23048,
    "package_name": "weed",
    "title": "Wrangler for Emergency Events Database",
    "description": "Makes research involving EMDAT and related datasets easier. These Datasets are manually filled and have several formatting and compatibility issues. Weed aims to resolve these with its functions.",
    "version": "1.1.2",
    "maintainer": "Ram Kripa <ram.m.kripa@berkeley.edu>",
    "author": "Ram Kripa [aut, cre]",
    "url": "https://github.com/rammkripa/weed",
    "bug_reports": "https://github.com/rammkripa/weed/issues",
    "repository": "https://cran.r-project.org/package=weed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "weed Wrangler for Emergency Events Database Makes research involving EMDAT and related datasets easier. These Datasets are manually filled and have several formatting and compatibility issues. Weed aims to resolve these with its functions.  "
  },
  {
    "id": 23077,
    "package_name": "when",
    "title": "Definition of Date and Time Dimension Tables",
    "description": "In Multidimensional Systems the When dimension allows us to\n    express when the analysed facts have occurred. The purpose of this\n    package is to provide support for implementing this dimension in the\n    form of date and time tables for Relational On-Line Analytical Processing \n    star database systems.",
    "version": "1.0.0",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/when/,\nhttps://github.com/josesamos/when",
    "bug_reports": "https://github.com/josesamos/when/issues",
    "repository": "https://cran.r-project.org/package=when",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "when Definition of Date and Time Dimension Tables In Multidimensional Systems the When dimension allows us to\n    express when the analysed facts have occurred. The purpose of this\n    package is to provide support for implementing this dimension in the\n    form of date and time tables for Relational On-Line Analytical Processing \n    star database systems.  "
  },
  {
    "id": 23121,
    "package_name": "wizaRdry",
    "title": "A Magical Framework for Collaborative & Reproducible Data\nAnalysis",
    "description": "A comprehensive data analysis framework for NIH-funded research that streamlines workflows for both data cleaning and preparing and modifying NIH Data Archive ('NDA') data structures and submission templates. Provides unified access to multiple data sources ('REDCap', 'MongoDB', 'Qualtrics', 'SQL', 'ORACLE') through interfaces to their APIs, with specialized functions for data cleaning, filtering, merging, and parsing. Features automatic validation, field harmonization, and memory-aware processing to enhance reproducibility in multi-site collaborative research as described in Mittal et al. (2021) <doi:10.20900/jpbs.20210011>.",
    "version": "0.5.0",
    "maintainer": "Joshua G. Kenney <joshua.kenney@yale.edu>",
    "author": "Joshua G. Kenney [aut, cre],\n  Trevor F. Williams [aut],\n  Minerva K. Pappu [aut],\n  Michael J. Spilka [aut],\n  Danielle N. Pratt [ctb],\n  Victor J. Pokorny [ctb],\n  Santiago Castiello de Obeso [ctb],\n  Praveen Suthaharan [ctb],\n  Christian R. Horgan [ctb]",
    "url": "https://github.com/belieflab/wizaRdry",
    "bug_reports": "https://github.com/belieflab/wizaRdry/issues",
    "repository": "https://cran.r-project.org/package=wizaRdry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wizaRdry A Magical Framework for Collaborative & Reproducible Data\nAnalysis A comprehensive data analysis framework for NIH-funded research that streamlines workflows for both data cleaning and preparing and modifying NIH Data Archive ('NDA') data structures and submission templates. Provides unified access to multiple data sources ('REDCap', 'MongoDB', 'Qualtrics', 'SQL', 'ORACLE') through interfaces to their APIs, with specialized functions for data cleaning, filtering, merging, and parsing. Features automatic validation, field harmonization, and memory-aware processing to enhance reproducibility in multi-site collaborative research as described in Mittal et al. (2021) <doi:10.20900/jpbs.20210011>.  "
  },
  {
    "id": 23143,
    "package_name": "wordnet",
    "title": "WordNet Interface",
    "description": "An interface to WordNet using the Jawbone Java API to WordNet.\n  WordNet (<https://wordnet.princeton.edu/>) is a large lexical database of\n  English.  Nouns, verbs, adjectives and adverbs are grouped into sets of\n  cognitive synonyms (synsets), each expressing a distinct concept.  Synsets\n  are interlinked by means of conceptual-semantic and lexical relations.\n  Please note that WordNet(R) is a registered tradename.  Princeton\n  University makes WordNet available to research and commercial users\n  free of charge provided the terms of their license\n  (<https://wordnet.princeton.edu/license-and-commercial-use>) are followed,\n  and proper reference is made to the project using an appropriate\n  citation (<https://wordnet.princeton.edu/citing-wordnet>).\n  The WordNet database files need to be made available separately,\n  either via package 'wordnetDicts' from <https://datacube.wu.ac.at>,\n  installing system packages where available, or direct download from\n  <https://wordnetcode.princeton.edu/3.0/WNdb-3.0.tar.gz>.",
    "version": "0.1-17",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "author": "Ingo Feinerer [aut],\n  Kurt Hornik [aut, cre] (ORCID: <https://orcid.org/0000-0003-4198-9911>),\n  Mike Wallace [ctb, cph] (Jawbone Java WordNet API library)",
    "url": "https://wordnet.princeton.edu/,\nhttps://sites.google.com/site/mfwallace/jawbone",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wordnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordnet WordNet Interface An interface to WordNet using the Jawbone Java API to WordNet.\n  WordNet (<https://wordnet.princeton.edu/>) is a large lexical database of\n  English.  Nouns, verbs, adjectives and adverbs are grouped into sets of\n  cognitive synonyms (synsets), each expressing a distinct concept.  Synsets\n  are interlinked by means of conceptual-semantic and lexical relations.\n  Please note that WordNet(R) is a registered tradename.  Princeton\n  University makes WordNet available to research and commercial users\n  free of charge provided the terms of their license\n  (<https://wordnet.princeton.edu/license-and-commercial-use>) are followed,\n  and proper reference is made to the project using an appropriate\n  citation (<https://wordnet.princeton.edu/citing-wordnet>).\n  The WordNet database files need to be made available separately,\n  either via package 'wordnetDicts' from <https://datacube.wu.ac.at>,\n  installing system packages where available, or direct download from\n  <https://wordnetcode.princeton.edu/3.0/WNdb-3.0.tar.gz>.  "
  },
  {
    "id": 23154,
    "package_name": "worldmet",
    "title": "Import Surface Meteorological Data from NOAA Integrated Surface\nDatabase (ISD)",
    "description": "Functions to import data from more than 30,000 surface\n    meteorological sites around the world managed by the National Oceanic\n    and Atmospheric Administration (NOAA) Integrated Surface Database\n    (ISD, see\n    <https://www.ncei.noaa.gov/products/land-based-station/integrated-surface-database>).",
    "version": "0.10.2",
    "maintainer": "David Carslaw <david.carslaw@york.ac.uk>",
    "author": "David Carslaw [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0991-950X>),\n  Jack Davison [aut] (ORCID: <https://orcid.org/0000-0003-2653-6615>)",
    "url": "https://openair-project.github.io/worldmet/,\nhttps://github.com/openair-project/worldmet",
    "bug_reports": "https://github.com/openair-project/worldmet/issues",
    "repository": "https://cran.r-project.org/package=worldmet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "worldmet Import Surface Meteorological Data from NOAA Integrated Surface\nDatabase (ISD) Functions to import data from more than 30,000 surface\n    meteorological sites around the world managed by the National Oceanic\n    and Atmospheric Administration (NOAA) Integrated Surface Database\n    (ISD, see\n    <https://www.ncei.noaa.gov/products/land-based-station/integrated-surface-database>).  "
  },
  {
    "id": 23156,
    "package_name": "wosr",
    "title": "Clients to the 'Web of Science' and 'InCites' APIs",
    "description": "R clients to the 'Web of Science' and 'InCites' \n  <https://clarivate.com/products/data-integration/> APIs, which \n  allow you to programmatically download publication and citation data\n  indexed in the 'Web of Science' and 'InCites' databases.",
    "version": "0.3.0",
    "maintainer": "Christopher Baker <chriscrewbaker@gmail.com>",
    "author": "Christopher Baker [aut, cre]",
    "url": "https://vt-arc.github.io/wosr/index.html",
    "bug_reports": "https://github.com/vt-arc/wosr/issues",
    "repository": "https://cran.r-project.org/package=wosr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wosr Clients to the 'Web of Science' and 'InCites' APIs R clients to the 'Web of Science' and 'InCites' \n  <https://clarivate.com/products/data-integration/> APIs, which \n  allow you to programmatically download publication and citation data\n  indexed in the 'Web of Science' and 'InCites' databases.  "
  },
  {
    "id": 23181,
    "package_name": "writer",
    "title": "Write from Multiple Sources to a Database Table",
    "description": "Provides unified syntax to write data from lazy 'dplyr' 'tbl' or\n    'dplyr' 'sql' query or a dataframe to a database table with modes such as\n    create, append, insert, update, upsert, patch, delete, overwrite,\n    overwrite_schema.",
    "version": "0.1.0",
    "maintainer": "Komala Sheshachala Srikanth <sri.teach@gmail.com>",
    "author": "Komala Sheshachala Srikanth [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1865-5668>)",
    "url": "https://github.com/talegari/writer",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=writer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "writer Write from Multiple Sources to a Database Table Provides unified syntax to write data from lazy 'dplyr' 'tbl' or\n    'dplyr' 'sql' query or a dataframe to a database table with modes such as\n    create, append, insert, update, upsert, patch, delete, overwrite,\n    overwrite_schema.  "
  },
  {
    "id": 23240,
    "package_name": "xml2relational",
    "title": "Converting XML Documents into Relational Data Models",
    "description": "Import an XML document with nested object structures and convert\n    it into a relational data model. The result is a set of R dataframes \n    with foreign key relationships. The data model and the data can be exported as\n    SQL code of different SQL flavors.",
    "version": "0.1.1",
    "maintainer": "Joachim Zuckarelli <joachim@zuckarelli.de>",
    "author": "Joachim Zuckarelli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9280-3016>)",
    "url": "https://github.com/jsugarelli/xml2relational/",
    "bug_reports": "https://github.com/jsugarelli/xml2relational/issues",
    "repository": "https://cran.r-project.org/package=xml2relational",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xml2relational Converting XML Documents into Relational Data Models Import an XML document with nested object structures and convert\n    it into a relational data model. The result is a set of R dataframes \n    with foreign key relationships. The data model and the data can be exported as\n    SQL code of different SQL flavors.  "
  }
]