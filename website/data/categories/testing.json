[
  {
    "id": 1318,
    "package_name": "testthat",
    "title": "Unit Testing for R",
    "description": "Software testing is important, but, in part because it is\nfrustrating and boring, many of us avoid it. 'testthat' is a\ntesting framework for R that is easy to learn and use, and\nintegrates with your existing 'workflow'.",
    "version": "3.3.1",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre],\nPosit Software, PBC [cph, fnd],\nR Core team [ctb] (Implementation of utils::recover())",
    "url": "https://testthat.r-lib.org, https://github.com/r-lib/testthat",
    "bug_reports": "https://github.com/r-lib/testthat/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "announce_snapshot_file"
      ],
      [
        "auto_test"
      ],
      [
        "auto_test_package"
      ],
      [
        "capture_condition"
      ],
      [
        "capture_error"
      ],
      [
        "capture_expectation"
      ],
      [
        "capture_message"
      ],
      [
        "capture_messages"
      ],
      [
        "capture_output"
      ],
      [
        "capture_output_lines"
      ],
      [
        "capture_warning"
      ],
      [
        "capture_warnings"
      ],
      [
        "check_reporter"
      ],
      [
        "CheckReporter"
      ],
      [
        "CompactProgressReporter"
      ],
      [
        "compare"
      ],
      [
        "compare_file_binary"
      ],
      [
        "compare_file_text"
      ],
      [
        "context"
      ],
      [
        "context_start_file"
      ],
      [
        "DebugReporter"
      ],
      [
        "default_compact_reporter"
      ],
      [
        "default_parallel_reporter"
      ],
      [
        "default_reporter"
      ],
      [
        "describe"
      ],
      [
        "edition_get"
      ],
      [
        "equals"
      ],
      [
        "equals_reference"
      ],
      [
        "evaluate_promise"
      ],
      [
        "exp_signal"
      ],
      [
        "expect"
      ],
      [
        "expect_all_equal"
      ],
      [
        "expect_all_false"
      ],
      [
        "expect_all_true"
      ],
      [
        "expect_condition"
      ],
      [
        "expect_contains"
      ],
      [
        "expect_cpp_tests_pass"
      ],
      [
        "expect_disjoint"
      ],
      [
        "expect_equal"
      ],
      [
        "expect_equal_to_reference"
      ],
      [
        "expect_equivalent"
      ],
      [
        "expect_error"
      ],
      [
        "expect_failure"
      ],
      [
        "expect_false"
      ],
      [
        "expect_gt"
      ],
      [
        "expect_gte"
      ],
      [
        "expect_identical"
      ],
      [
        "expect_in"
      ],
      [
        "expect_invisible"
      ],
      [
        "expect_is"
      ],
      [
        "expect_known_hash"
      ],
      [
        "expect_known_output"
      ],
      [
        "expect_known_value"
      ],
      [
        "expect_length"
      ],
      [
        "expect_less_than"
      ],
      [
        "expect_lt"
      ],
      [
        "expect_lte"
      ],
      [
        "expect_mapequal"
      ],
      [
        "expect_match"
      ],
      [
        "expect_message"
      ],
      [
        "expect_more_than"
      ],
      [
        "expect_named"
      ],
      [
        "expect_no_condition"
      ],
      [
        "expect_no_error"
      ],
      [
        "expect_no_failure"
      ],
      [
        "expect_no_match"
      ],
      [
        "expect_no_message"
      ],
      [
        "expect_no_success"
      ],
      [
        "expect_no_warning"
      ],
      [
        "expect_null"
      ],
      [
        "expect_output"
      ],
      [
        "expect_output_file"
      ],
      [
        "expect_r6_class"
      ],
      [
        "expect_reference"
      ],
      [
        "expect_s3_class"
      ],
      [
        "expect_s4_class"
      ],
      [
        "expect_s7_class"
      ],
      [
        "expect_setequal"
      ],
      [
        "expect_shape"
      ],
      [
        "expect_silent"
      ],
      [
        "expect_snapshot"
      ],
      [
        "expect_snapshot_error"
      ],
      [
        "expect_snapshot_failure"
      ],
      [
        "expect_snapshot_file"
      ],
      [
        "expect_snapshot_output"
      ],
      [
        "expect_snapshot_value"
      ],
      [
        "expect_snapshot_warning"
      ],
      [
        "expect_success"
      ],
      [
        "expect_that"
      ],
      [
        "expect_true"
      ],
      [
        "expect_type"
      ],
      [
        "expect_vector"
      ],
      [
        "expect_visible"
      ],
      [
        "expect_warning"
      ],
      [
        "expectation"
      ],
      [
        "extract_test"
      ],
      [
        "fail"
      ],
      [
        "FailReporter"
      ],
      [
        "find_test_scripts"
      ],
      [
        "get_reporter"
      ],
      [
        "gives_warning"
      ],
      [
        "has_names"
      ],
      [
        "is_a"
      ],
      [
        "is_checking"
      ],
      [
        "is_equivalent_to"
      ],
      [
        "is_identical_to"
      ],
      [
        "is_informative_error"
      ],
      [
        "is_less_than"
      ],
      [
        "is_more_than"
      ],
      [
        "is_parallel"
      ],
      [
        "is_snapshot"
      ],
      [
        "is_testing"
      ],
      [
        "is.expectation"
      ],
      [
        "it"
      ],
      [
        "JunitReporter"
      ],
      [
        "ListReporter"
      ],
      [
        "local_edition"
      ],
      [
        "local_mock"
      ],
      [
        "local_mocked_bindings"
      ],
      [
        "local_mocked_r6_class"
      ],
      [
        "local_mocked_s3_method"
      ],
      [
        "local_mocked_s4_method"
      ],
      [
        "local_on_cran"
      ],
      [
        "local_reproducible_output"
      ],
      [
        "local_snapshotter"
      ],
      [
        "local_test_context"
      ],
      [
        "local_test_directory"
      ],
      [
        "LocationReporter"
      ],
      [
        "make_expectation"
      ],
      [
        "MinimalReporter"
      ],
      [
        "mock_output_sequence"
      ],
      [
        "MultiReporter"
      ],
      [
        "new_expectation"
      ],
      [
        "not"
      ],
      [
        "ParallelProgressReporter"
      ],
      [
        "pass"
      ],
      [
        "prints_text"
      ],
      [
        "ProgressReporter"
      ],
      [
        "quasi_label"
      ],
      [
        "Reporter"
      ],
      [
        "RStudioReporter"
      ],
      [
        "run_cpp_tests"
      ],
      [
        "set_max_fails"
      ],
      [
        "set_reporter"
      ],
      [
        "set_state_inspector"
      ],
      [
        "setup"
      ],
      [
        "show_failure"
      ],
      [
        "shows_message"
      ],
      [
        "SilentReporter"
      ],
      [
        "simulate_test_env"
      ],
      [
        "skip"
      ],
      [
        "skip_if"
      ],
      [
        "skip_if_not"
      ],
      [
        "skip_if_not_installed"
      ],
      [
        "skip_if_offline"
      ],
      [
        "skip_if_translated"
      ],
      [
        "skip_on_appveyor"
      ],
      [
        "skip_on_bioc"
      ],
      [
        "skip_on_ci"
      ],
      [
        "skip_on_covr"
      ],
      [
        "skip_on_cran"
      ],
      [
        "skip_on_os"
      ],
      [
        "skip_on_travis"
      ],
      [
        "skip_unless_r"
      ],
      [
        "SlowReporter"
      ],
      [
        "snapshot_accept"
      ],
      [
        "snapshot_download_gh"
      ],
      [
        "snapshot_reject"
      ],
      [
        "snapshot_review"
      ],
      [
        "source_dir"
      ],
      [
        "source_file"
      ],
      [
        "source_test_helpers"
      ],
      [
        "source_test_setup"
      ],
      [
        "source_test_teardown"
      ],
      [
        "StopReporter"
      ],
      [
        "succeed"
      ],
      [
        "SummaryReporter"
      ],
      [
        "takes_less_than"
      ],
      [
        "TapReporter"
      ],
      [
        "TeamcityReporter"
      ],
      [
        "teardown"
      ],
      [
        "teardown_env"
      ],
      [
        "test_check"
      ],
      [
        "test_dir"
      ],
      [
        "test_env"
      ],
      [
        "test_example"
      ],
      [
        "test_examples"
      ],
      [
        "test_file"
      ],
      [
        "test_local"
      ],
      [
        "test_package"
      ],
      [
        "test_path"
      ],
      [
        "test_rd"
      ],
      [
        "test_that"
      ],
      [
        "testing_package"
      ],
      [
        "testthat_example"
      ],
      [
        "testthat_examples"
      ],
      [
        "testthat_print"
      ],
      [
        "testthat_tolerance"
      ],
      [
        "throws_error"
      ],
      [
        "try_again"
      ],
      [
        "use_catch"
      ],
      [
        "verify_output"
      ],
      [
        "watch"
      ],
      [
        "with_mock"
      ],
      [
        "with_mocked_bindings"
      ],
      [
        "with_reporter"
      ]
    ],
    "topics": [
      [
        "unit-testing"
      ],
      [
        "cpp"
      ]
    ],
    "score": 21.8914,
    "stars": 918,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "testthat Unit Testing for R Software testing is important, but, in part because it is\nfrustrating and boring, many of us avoid it. 'testthat' is a\ntesting framework for R that is easy to learn and use, and\nintegrates with your existing 'workflow'. %>% announce_snapshot_file auto_test auto_test_package capture_condition capture_error capture_expectation capture_message capture_messages capture_output capture_output_lines capture_warning capture_warnings check_reporter CheckReporter CompactProgressReporter compare compare_file_binary compare_file_text context context_start_file DebugReporter default_compact_reporter default_parallel_reporter default_reporter describe edition_get equals equals_reference evaluate_promise exp_signal expect expect_all_equal expect_all_false expect_all_true expect_condition expect_contains expect_cpp_tests_pass expect_disjoint expect_equal expect_equal_to_reference expect_equivalent expect_error expect_failure expect_false expect_gt expect_gte expect_identical expect_in expect_invisible expect_is expect_known_hash expect_known_output expect_known_value expect_length expect_less_than expect_lt expect_lte expect_mapequal expect_match expect_message expect_more_than expect_named expect_no_condition expect_no_error expect_no_failure expect_no_match expect_no_message expect_no_success expect_no_warning expect_null expect_output expect_output_file expect_r6_class expect_reference expect_s3_class expect_s4_class expect_s7_class expect_setequal expect_shape expect_silent expect_snapshot expect_snapshot_error expect_snapshot_failure expect_snapshot_file expect_snapshot_output expect_snapshot_value expect_snapshot_warning expect_success expect_that expect_true expect_type expect_vector expect_visible expect_warning expectation extract_test fail FailReporter find_test_scripts get_reporter gives_warning has_names is_a is_checking is_equivalent_to is_identical_to is_informative_error is_less_than is_more_than is_parallel is_snapshot is_testing is.expectation it JunitReporter ListReporter local_edition local_mock local_mocked_bindings local_mocked_r6_class local_mocked_s3_method local_mocked_s4_method local_on_cran local_reproducible_output local_snapshotter local_test_context local_test_directory LocationReporter make_expectation MinimalReporter mock_output_sequence MultiReporter new_expectation not ParallelProgressReporter pass prints_text ProgressReporter quasi_label Reporter RStudioReporter run_cpp_tests set_max_fails set_reporter set_state_inspector setup show_failure shows_message SilentReporter simulate_test_env skip skip_if skip_if_not skip_if_not_installed skip_if_offline skip_if_translated skip_on_appveyor skip_on_bioc skip_on_ci skip_on_covr skip_on_cran skip_on_os skip_on_travis skip_unless_r SlowReporter snapshot_accept snapshot_download_gh snapshot_reject snapshot_review source_dir source_file source_test_helpers source_test_setup source_test_teardown StopReporter succeed SummaryReporter takes_less_than TapReporter TeamcityReporter teardown teardown_env test_check test_dir test_env test_example test_examples test_file test_local test_package test_path test_rd test_that testing_package testthat_example testthat_examples testthat_print testthat_tolerance throws_error try_again use_catch verify_output watch with_mock with_mocked_bindings with_reporter unit-testing cpp"
  },
  {
    "id": 1405,
    "package_name": "usethis",
    "title": "Automate Package and Project Setup",
    "description": "Automate package and project setup tasks that are\notherwise performed manually. This includes setting up unit\ntesting, test coverage, continuous integration, Git, 'GitHub',\nlicenses, 'Rcpp', 'RStudio' projects, and more.",
    "version": "3.2.1.9000",
    "maintainer": "Jennifer Bryan <jenny@posit.co>",
    "author": "Hadley Wickham [aut] (ORCID: <https://orcid.org/0000-0003-4757-117X>),\nJennifer Bryan [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-6983-2759>),\nMalcolm Barrett [aut] (ORCID: <https://orcid.org/0000-0003-0299-5825>),\nAndy Teucher [aut] (ORCID: <https://orcid.org/0000-0002-7840-692X>),\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://usethis.r-lib.org, https://github.com/r-lib/usethis",
    "bug_reports": "https://github.com/r-lib/usethis/issues",
    "repository": "",
    "exports": [
      [
        "browse_circleci"
      ],
      [
        "browse_cran"
      ],
      [
        "browse_github"
      ],
      [
        "browse_github_actions"
      ],
      [
        "browse_github_issues"
      ],
      [
        "browse_github_pulls"
      ],
      [
        "browse_package"
      ],
      [
        "browse_project"
      ],
      [
        "create_download_url"
      ],
      [
        "create_from_github"
      ],
      [
        "create_github_token"
      ],
      [
        "create_package"
      ],
      [
        "create_project"
      ],
      [
        "create_quarto_project"
      ],
      [
        "create_tidy_package"
      ],
      [
        "edit_file"
      ],
      [
        "edit_git_config"
      ],
      [
        "edit_git_ignore"
      ],
      [
        "edit_pkgdown_config"
      ],
      [
        "edit_r_buildignore"
      ],
      [
        "edit_r_environ"
      ],
      [
        "edit_r_makevars"
      ],
      [
        "edit_r_profile"
      ],
      [
        "edit_rstudio_prefs"
      ],
      [
        "edit_rstudio_snippets"
      ],
      [
        "edit_template"
      ],
      [
        "gh_token_help"
      ],
      [
        "git_default_branch"
      ],
      [
        "git_default_branch_configure"
      ],
      [
        "git_default_branch_rediscover"
      ],
      [
        "git_default_branch_rename"
      ],
      [
        "git_protocol"
      ],
      [
        "git_remotes"
      ],
      [
        "git_sitrep"
      ],
      [
        "git_vaccinate"
      ],
      [
        "issue_close_community"
      ],
      [
        "issue_reprex_needed"
      ],
      [
        "local_project"
      ],
      [
        "pr_fetch"
      ],
      [
        "pr_finish"
      ],
      [
        "pr_forget"
      ],
      [
        "pr_init"
      ],
      [
        "pr_merge_main"
      ],
      [
        "pr_pause"
      ],
      [
        "pr_pull"
      ],
      [
        "pr_push"
      ],
      [
        "pr_resume"
      ],
      [
        "pr_view"
      ],
      [
        "proj_activate"
      ],
      [
        "proj_get"
      ],
      [
        "proj_path"
      ],
      [
        "proj_set"
      ],
      [
        "proj_sitrep"
      ],
      [
        "rename_files"
      ],
      [
        "tidy_label_colours"
      ],
      [
        "tidy_label_descriptions"
      ],
      [
        "tidy_labels"
      ],
      [
        "tidy_labels_rename"
      ],
      [
        "ui_code"
      ],
      [
        "ui_code_block"
      ],
      [
        "ui_done"
      ],
      [
        "ui_field"
      ],
      [
        "ui_info"
      ],
      [
        "ui_line"
      ],
      [
        "ui_nope"
      ],
      [
        "ui_oops"
      ],
      [
        "ui_path"
      ],
      [
        "ui_silence"
      ],
      [
        "ui_stop"
      ],
      [
        "ui_todo"
      ],
      [
        "ui_unset"
      ],
      [
        "ui_value"
      ],
      [
        "ui_warn"
      ],
      [
        "ui_yeah"
      ],
      [
        "use_addin"
      ],
      [
        "use_agpl_license"
      ],
      [
        "use_agpl3_license"
      ],
      [
        "use_air"
      ],
      [
        "use_apache_license"
      ],
      [
        "use_apl2_license"
      ],
      [
        "use_article"
      ],
      [
        "use_author"
      ],
      [
        "use_badge"
      ],
      [
        "use_binder_badge"
      ],
      [
        "use_bioc_badge"
      ],
      [
        "use_blank_slate"
      ],
      [
        "use_build_ignore"
      ],
      [
        "use_c"
      ],
      [
        "use_cc0_license"
      ],
      [
        "use_ccby_license"
      ],
      [
        "use_circleci"
      ],
      [
        "use_circleci_badge"
      ],
      [
        "use_citation"
      ],
      [
        "use_code_of_conduct"
      ],
      [
        "use_conflicted"
      ],
      [
        "use_course"
      ],
      [
        "use_coverage"
      ],
      [
        "use_covr_ignore"
      ],
      [
        "use_cpp11"
      ],
      [
        "use_cran_badge"
      ],
      [
        "use_cran_comments"
      ],
      [
        "use_data"
      ],
      [
        "use_data_raw"
      ],
      [
        "use_data_table"
      ],
      [
        "use_description"
      ],
      [
        "use_description_defaults"
      ],
      [
        "use_dev_package"
      ],
      [
        "use_dev_version"
      ],
      [
        "use_devtools"
      ],
      [
        "use_directory"
      ],
      [
        "use_git"
      ],
      [
        "use_git_config"
      ],
      [
        "use_git_hook"
      ],
      [
        "use_git_ignore"
      ],
      [
        "use_git_protocol"
      ],
      [
        "use_git_remote"
      ],
      [
        "use_github"
      ],
      [
        "use_github_action"
      ],
      [
        "use_github_actions_badge"
      ],
      [
        "use_github_file"
      ],
      [
        "use_github_labels"
      ],
      [
        "use_github_links"
      ],
      [
        "use_github_pages"
      ],
      [
        "use_github_release"
      ],
      [
        "use_gitlab_ci"
      ],
      [
        "use_gpl_license"
      ],
      [
        "use_gpl3_license"
      ],
      [
        "use_import_from"
      ],
      [
        "use_jenkins"
      ],
      [
        "use_latest_dependencies"
      ],
      [
        "use_lgpl_license"
      ],
      [
        "use_lifecycle"
      ],
      [
        "use_lifecycle_badge"
      ],
      [
        "use_logo"
      ],
      [
        "use_make"
      ],
      [
        "use_mit_license"
      ],
      [
        "use_namespace"
      ],
      [
        "use_news_md"
      ],
      [
        "use_package"
      ],
      [
        "use_package_doc"
      ],
      [
        "use_partial_warnings"
      ],
      [
        "use_pipe"
      ],
      [
        "use_pkgdown"
      ],
      [
        "use_pkgdown_github_pages"
      ],
      [
        "use_posit_cloud_badge"
      ],
      [
        "use_proprietary_license"
      ],
      [
        "use_r"
      ],
      [
        "use_r_universe_badge"
      ],
      [
        "use_rcpp"
      ],
      [
        "use_rcpp_armadillo"
      ],
      [
        "use_rcpp_eigen"
      ],
      [
        "use_readme_md"
      ],
      [
        "use_readme_rmd"
      ],
      [
        "use_release_issue"
      ],
      [
        "use_reprex"
      ],
      [
        "use_revdep"
      ],
      [
        "use_rmarkdown_template"
      ],
      [
        "use_roxygen_md"
      ],
      [
        "use_rstudio"
      ],
      [
        "use_rstudio_preferences"
      ],
      [
        "use_spell_check"
      ],
      [
        "use_standalone"
      ],
      [
        "use_template"
      ],
      [
        "use_test"
      ],
      [
        "use_test_helper"
      ],
      [
        "use_testthat"
      ],
      [
        "use_tibble"
      ],
      [
        "use_tidy_coc"
      ],
      [
        "use_tidy_contributing"
      ],
      [
        "use_tidy_dependencies"
      ],
      [
        "use_tidy_description"
      ],
      [
        "use_tidy_github"
      ],
      [
        "use_tidy_github_actions"
      ],
      [
        "use_tidy_github_labels"
      ],
      [
        "use_tidy_issue_template"
      ],
      [
        "use_tidy_logo"
      ],
      [
        "use_tidy_style"
      ],
      [
        "use_tidy_support"
      ],
      [
        "use_tidy_thanks"
      ],
      [
        "use_tidy_upkeep_issue"
      ],
      [
        "use_tutorial"
      ],
      [
        "use_upkeep_issue"
      ],
      [
        "use_usethis"
      ],
      [
        "use_version"
      ],
      [
        "use_vignette"
      ],
      [
        "use_zip"
      ],
      [
        "with_project"
      ],
      [
        "write_over"
      ],
      [
        "write_union"
      ]
    ],
    "topics": [
      [
        "github"
      ],
      [
        "setup"
      ]
    ],
    "score": 17.7882,
    "stars": 893,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "usethis Automate Package and Project Setup Automate package and project setup tasks that are\notherwise performed manually. This includes setting up unit\ntesting, test coverage, continuous integration, Git, 'GitHub',\nlicenses, 'Rcpp', 'RStudio' projects, and more. browse_circleci browse_cran browse_github browse_github_actions browse_github_issues browse_github_pulls browse_package browse_project create_download_url create_from_github create_github_token create_package create_project create_quarto_project create_tidy_package edit_file edit_git_config edit_git_ignore edit_pkgdown_config edit_r_buildignore edit_r_environ edit_r_makevars edit_r_profile edit_rstudio_prefs edit_rstudio_snippets edit_template gh_token_help git_default_branch git_default_branch_configure git_default_branch_rediscover git_default_branch_rename git_protocol git_remotes git_sitrep git_vaccinate issue_close_community issue_reprex_needed local_project pr_fetch pr_finish pr_forget pr_init pr_merge_main pr_pause pr_pull pr_push pr_resume pr_view proj_activate proj_get proj_path proj_set proj_sitrep rename_files tidy_label_colours tidy_label_descriptions tidy_labels tidy_labels_rename ui_code ui_code_block ui_done ui_field ui_info ui_line ui_nope ui_oops ui_path ui_silence ui_stop ui_todo ui_unset ui_value ui_warn ui_yeah use_addin use_agpl_license use_agpl3_license use_air use_apache_license use_apl2_license use_article use_author use_badge use_binder_badge use_bioc_badge use_blank_slate use_build_ignore use_c use_cc0_license use_ccby_license use_circleci use_circleci_badge use_citation use_code_of_conduct use_conflicted use_course use_coverage use_covr_ignore use_cpp11 use_cran_badge use_cran_comments use_data use_data_raw use_data_table use_description use_description_defaults use_dev_package use_dev_version use_devtools use_directory use_git use_git_config use_git_hook use_git_ignore use_git_protocol use_git_remote use_github use_github_action use_github_actions_badge use_github_file use_github_labels use_github_links use_github_pages use_github_release use_gitlab_ci use_gpl_license use_gpl3_license use_import_from use_jenkins use_latest_dependencies use_lgpl_license use_lifecycle use_lifecycle_badge use_logo use_make use_mit_license use_namespace use_news_md use_package use_package_doc use_partial_warnings use_pipe use_pkgdown use_pkgdown_github_pages use_posit_cloud_badge use_proprietary_license use_r use_r_universe_badge use_rcpp use_rcpp_armadillo use_rcpp_eigen use_readme_md use_readme_rmd use_release_issue use_reprex use_revdep use_rmarkdown_template use_roxygen_md use_rstudio use_rstudio_preferences use_spell_check use_standalone use_template use_test use_test_helper use_testthat use_tibble use_tidy_coc use_tidy_contributing use_tidy_dependencies use_tidy_description use_tidy_github use_tidy_github_actions use_tidy_github_labels use_tidy_issue_template use_tidy_logo use_tidy_style use_tidy_support use_tidy_thanks use_tidy_upkeep_issue use_tutorial use_upkeep_issue use_usethis use_version use_vignette use_zip with_project write_over write_union github setup"
  },
  {
    "id": 280,
    "package_name": "bayestestR",
    "title": "Understand and Describe Bayesian Models and Posterior\nDistributions",
    "description": "Provides utilities to describe posterior distributions and\nBayesian models. It includes point-estimates such as Maximum A\nPosteriori (MAP), measures of dispersion (Highest Density\nInterval - HDI; Kruschke, 2015 <doi:10.1016/C2012-0-00477-2>)\nand indices used for null-hypothesis testing (such as ROPE\npercentage, pd and Bayes factors). References: Makowski et al.\n(2021) <doi:10.21105/joss.01541>.",
    "version": "0.17.0.1",
    "maintainer": "Dominique Makowski <officialeasystats@gmail.com>",
    "author": "Dominique Makowski [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-5375-9967>),\nDaniel L\u00fcdecke [aut] (ORCID: <https://orcid.org/0000-0002-8895-3206>),\nMattan S. Ben-Shachar [aut] (ORCID:\n<https://orcid.org/0000-0002-4287-4801>),\nIndrajeet Patil [aut] (ORCID: <https://orcid.org/0000-0003-1995-6531>),\nMicah K. Wilson [aut] (ORCID: <https://orcid.org/0000-0003-4143-7308>),\nBrenton M. Wiernik [aut] (ORCID:\n<https://orcid.org/0000-0001-9560-6336>),\nPaul-Christian B\u00fcrkner [rev],\nTristan Mahr [rev] (ORCID: <https://orcid.org/0000-0002-8890-5116>),\nHenrik Singmann [ctb] (ORCID: <https://orcid.org/0000-0002-4842-3657>),\nQuentin F. Gronau [ctb] (ORCID:\n<https://orcid.org/0000-0001-5510-6943>),\nSam Crawley [ctb] (ORCID: <https://orcid.org/0000-0002-7847-0411>)",
    "url": "https://easystats.github.io/bayestestR/",
    "bug_reports": "https://github.com/easystats/bayestestR/issues",
    "repository": "",
    "exports": [
      [
        "area_under_curve"
      ],
      [
        "auc"
      ],
      [
        "bayesfactor"
      ],
      [
        "bayesfactor_inclusion"
      ],
      [
        "bayesfactor_models"
      ],
      [
        "bayesfactor_parameters"
      ],
      [
        "bayesfactor_pointnull"
      ],
      [
        "bayesfactor_restricted"
      ],
      [
        "bayesfactor_rope"
      ],
      [
        "bayesian_as_frequentist"
      ],
      [
        "bcai"
      ],
      [
        "bci"
      ],
      [
        "bf_inclusion"
      ],
      [
        "bf_models"
      ],
      [
        "bf_parameters"
      ],
      [
        "bf_pointnull"
      ],
      [
        "bf_restricted"
      ],
      [
        "bf_rope"
      ],
      [
        "bic_to_bf"
      ],
      [
        "check_prior"
      ],
      [
        "ci"
      ],
      [
        "contr.bayes"
      ],
      [
        "contr.equalprior"
      ],
      [
        "contr.equalprior_deviations"
      ],
      [
        "contr.equalprior_pairs"
      ],
      [
        "contr.orthonorm"
      ],
      [
        "convert_bayesian_as_frequentist"
      ],
      [
        "convert_p_to_pd"
      ],
      [
        "convert_pd_to_p"
      ],
      [
        "density_at"
      ],
      [
        "describe_posterior"
      ],
      [
        "describe_prior"
      ],
      [
        "diagnostic_draws"
      ],
      [
        "diagnostic_posterior"
      ],
      [
        "display"
      ],
      [
        "distribution"
      ],
      [
        "distribution_beta"
      ],
      [
        "distribution_binom"
      ],
      [
        "distribution_binomial"
      ],
      [
        "distribution_cauchy"
      ],
      [
        "distribution_chisq"
      ],
      [
        "distribution_chisquared"
      ],
      [
        "distribution_custom"
      ],
      [
        "distribution_gamma"
      ],
      [
        "distribution_gaussian"
      ],
      [
        "distribution_mixture_normal"
      ],
      [
        "distribution_nbinom"
      ],
      [
        "distribution_normal"
      ],
      [
        "distribution_poisson"
      ],
      [
        "distribution_student"
      ],
      [
        "distribution_student_t"
      ],
      [
        "distribution_t"
      ],
      [
        "distribution_tweedie"
      ],
      [
        "distribution_uniform"
      ],
      [
        "effective_sample"
      ],
      [
        "equivalence_test"
      ],
      [
        "estimate_density"
      ],
      [
        "eti"
      ],
      [
        "hdi"
      ],
      [
        "map_estimate"
      ],
      [
        "mcse"
      ],
      [
        "mediation"
      ],
      [
        "model_to_priors"
      ],
      [
        "overlap"
      ],
      [
        "p_direction"
      ],
      [
        "p_map"
      ],
      [
        "p_pointnull"
      ],
      [
        "p_rope"
      ],
      [
        "p_significance"
      ],
      [
        "p_to_bf"
      ],
      [
        "p_to_pd"
      ],
      [
        "pd"
      ],
      [
        "pd_to_p"
      ],
      [
        "point_estimate"
      ],
      [
        "print_html"
      ],
      [
        "print_md"
      ],
      [
        "reshape_draws"
      ],
      [
        "reshape_iterations"
      ],
      [
        "rope"
      ],
      [
        "rope_range"
      ],
      [
        "sensitivity_to_prior"
      ],
      [
        "sexit"
      ],
      [
        "sexit_thresholds"
      ],
      [
        "si"
      ],
      [
        "simulate_correlation"
      ],
      [
        "simulate_difference"
      ],
      [
        "simulate_prior"
      ],
      [
        "simulate_simpson"
      ],
      [
        "simulate_ttest"
      ],
      [
        "spi"
      ],
      [
        "unupdate"
      ],
      [
        "weighted_posteriors"
      ]
    ],
    "topics": [
      [
        "bayes-factors"
      ],
      [
        "bayesfactor"
      ],
      [
        "bayesian"
      ],
      [
        "bayesian-framework"
      ],
      [
        "credible-interval"
      ],
      [
        "easystats"
      ],
      [
        "hacktoberfest"
      ],
      [
        "hdi"
      ],
      [
        "map"
      ],
      [
        "posterior-distributions"
      ],
      [
        "rope"
      ]
    ],
    "score": 17.2354,
    "stars": 586,
    "primary_category": "statistics",
    "source_universe": "easystats",
    "search_text": "bayestestR Understand and Describe Bayesian Models and Posterior\nDistributions Provides utilities to describe posterior distributions and\nBayesian models. It includes point-estimates such as Maximum A\nPosteriori (MAP), measures of dispersion (Highest Density\nInterval - HDI; Kruschke, 2015 <doi:10.1016/C2012-0-00477-2>)\nand indices used for null-hypothesis testing (such as ROPE\npercentage, pd and Bayes factors). References: Makowski et al.\n(2021) <doi:10.21105/joss.01541>. area_under_curve auc bayesfactor bayesfactor_inclusion bayesfactor_models bayesfactor_parameters bayesfactor_pointnull bayesfactor_restricted bayesfactor_rope bayesian_as_frequentist bcai bci bf_inclusion bf_models bf_parameters bf_pointnull bf_restricted bf_rope bic_to_bf check_prior ci contr.bayes contr.equalprior contr.equalprior_deviations contr.equalprior_pairs contr.orthonorm convert_bayesian_as_frequentist convert_p_to_pd convert_pd_to_p density_at describe_posterior describe_prior diagnostic_draws diagnostic_posterior display distribution distribution_beta distribution_binom distribution_binomial distribution_cauchy distribution_chisq distribution_chisquared distribution_custom distribution_gamma distribution_gaussian distribution_mixture_normal distribution_nbinom distribution_normal distribution_poisson distribution_student distribution_student_t distribution_t distribution_tweedie distribution_uniform effective_sample equivalence_test estimate_density eti hdi map_estimate mcse mediation model_to_priors overlap p_direction p_map p_pointnull p_rope p_significance p_to_bf p_to_pd pd pd_to_p point_estimate print_html print_md reshape_draws reshape_iterations rope rope_range sensitivity_to_prior sexit sexit_thresholds si simulate_correlation simulate_difference simulate_prior simulate_simpson simulate_ttest spi unupdate weighted_posteriors bayes-factors bayesfactor bayesian bayesian-framework credible-interval easystats hacktoberfest hdi map posterior-distributions rope"
  },
  {
    "id": 419,
    "package_name": "covr",
    "title": "Test Coverage for Packages",
    "description": "Track and report code coverage for your package and\n(optionally) upload the results to a coverage service like\n'Codecov' <https://about.codecov.io> or 'Coveralls'\n<https://coveralls.io>. Code coverage is a measure of the\namount of code being exercised by a set of tests. It is an\nindirect measure of test quality and completeness. This package\nis compatible with any testing methodology or framework and\ntracks coverage of both R code and compiled C/C++/FORTRAN code.",
    "version": "3.6.5.9000",
    "maintainer": "Jim Hester <james.f.hester@gmail.com>",
    "author": "Jim Hester [aut, cre],\nWillem Ligtenberg [ctb],\nKirill M\u00fcller [ctb],\nHenrik Bengtsson [ctb],\nSteve Peak [ctb],\nKirill Sevastyanenko [ctb],\nJon Clayden [ctb],\nRobert Flight [ctb],\nEric Brown [ctb],\nBrodie Gaslam [ctb],\nWill Beasley [ctb],\nRobert Krzyzanowski [ctb],\nMarkus Wamser [ctb],\nKarl Forner [ctb],\nGergely Dar\u00f3czi [ctb],\nJouni Helske [ctb],\nKun Ren [ctb],\nJeroen Ooms [ctb],\nKen Williams [ctb],\nChris Campbell [ctb],\nDavid Hugh-Jones [ctb],\nQin Wang [ctb],\nDoug Kelkhoff [ctb],\nIvan Sagalaev [ctb, cph] (highlight.js library),\nMark Otto [ctb] (Bootstrap library),\nJacob Thornton [ctb] (Bootstrap library),\nBootstrap contributors [ctb] (Bootstrap library),\nTwitter, Inc [cph] (Bootstrap library)",
    "url": "https://covr.r-lib.org, https://github.com/r-lib/covr",
    "bug_reports": "https://github.com/r-lib/covr/issues",
    "repository": "",
    "exports": [
      [
        "azure"
      ],
      [
        "code_coverage"
      ],
      [
        "codecov"
      ],
      [
        "coverage_to_list"
      ],
      [
        "coveralls"
      ],
      [
        "display_name"
      ],
      [
        "environment_coverage"
      ],
      [
        "file_coverage"
      ],
      [
        "file_report"
      ],
      [
        "function_coverage"
      ],
      [
        "gitlab"
      ],
      [
        "in_covr"
      ],
      [
        "package_coverage"
      ],
      [
        "percent_coverage"
      ],
      [
        "report"
      ],
      [
        "tally_coverage"
      ],
      [
        "to_cobertura"
      ],
      [
        "to_sonarqube"
      ],
      [
        "value"
      ],
      [
        "zero_coverage"
      ]
    ],
    "topics": [
      [
        "codecov"
      ],
      [
        "coverage"
      ],
      [
        "coverage-report"
      ],
      [
        "travis-ci"
      ]
    ],
    "score": 15.8274,
    "stars": 345,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "covr Test Coverage for Packages Track and report code coverage for your package and\n(optionally) upload the results to a coverage service like\n'Codecov' <https://about.codecov.io> or 'Coveralls'\n<https://coveralls.io>. Code coverage is a measure of the\namount of code being exercised by a set of tests. It is an\nindirect measure of test quality and completeness. This package\nis compatible with any testing methodology or framework and\ntracks coverage of both R code and compiled C/C++/FORTRAN code. azure code_coverage codecov coverage_to_list coveralls display_name environment_coverage file_coverage file_report function_coverage gitlab in_covr package_coverage percent_coverage report tally_coverage to_cobertura to_sonarqube value zero_coverage codecov coverage coverage-report travis-ci"
  },
  {
    "id": 1425,
    "package_name": "waldo",
    "title": "Find Differences Between R Objects",
    "description": "Compare complex R objects and reveal the key differences.\nDesigned particularly for use in testing packages where being\nable to quickly isolate key differences makes understanding\ntest failures much easier.",
    "version": "0.6.2.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre],\nPosit Software, PBC [cph, fnd]",
    "url": "https://waldo.r-lib.org, https://github.com/r-lib/waldo",
    "bug_reports": "https://github.com/r-lib/waldo/issues",
    "repository": "",
    "exports": [
      [
        "compare"
      ],
      [
        "compare_proxy"
      ]
    ],
    "topics": [
      [
        "diff"
      ],
      [
        "testing"
      ]
    ],
    "score": 13.8196,
    "stars": 301,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "waldo Find Differences Between R Objects Compare complex R objects and reveal the key differences.\nDesigned particularly for use in testing packages where being\nable to quickly isolate key differences makes understanding\ntest failures much easier. compare compare_proxy diff testing"
  },
  {
    "id": 1218,
    "package_name": "shinytest2",
    "title": "Testing for Shiny Applications",
    "description": "Automated unit testing of Shiny applications through a\nheadless 'Chromium' browser.",
    "version": "0.4.1.9001",
    "maintainer": "Barret Schloerke <barret@posit.co>",
    "author": "Barret Schloerke [cre, aut] (ORCID:\n<https://orcid.org/0000-0001-9986-114X>),\nPosit Software, PBC [cph, fnd],\nWinston Chang [ctb] (Original author to rstudio/shinytest),\nG\u00e1bor Cs\u00e1rdi [ctb] (Original author to rstudio/shinytest),\nHadley Wickham [ctb] (Original author to rstudio/shinytest),\nMango Solutions [cph, ccp] (Original author to rstudio/shinytest)",
    "url": "https://rstudio.github.io/shinytest2/,\nhttps://github.com/rstudio/shinytest2",
    "bug_reports": "https://github.com/rstudio/shinytest2/issues",
    "repository": "",
    "exports": [
      [
        "AppDriver"
      ],
      [
        "compare_screenshot_threshold"
      ],
      [
        "get_input_processors"
      ],
      [
        "load_app_env"
      ],
      [
        "load_app_support"
      ],
      [
        "local_app_support"
      ],
      [
        "migrate_from_shinytest"
      ],
      [
        "platform_variant"
      ],
      [
        "record_test"
      ],
      [
        "register_input_processor"
      ],
      [
        "screenshot_max_difference"
      ],
      [
        "test_app"
      ],
      [
        "use_shinytest2"
      ],
      [
        "use_shinytest2_test"
      ],
      [
        "with_app_support"
      ]
    ],
    "topics": [
      [
        "cpp"
      ]
    ],
    "score": 12.6475,
    "stars": 115,
    "primary_category": "visualization",
    "source_universe": "rstudio",
    "search_text": "shinytest2 Testing for Shiny Applications Automated unit testing of Shiny applications through a\nheadless 'Chromium' browser. AppDriver compare_screenshot_threshold get_input_processors load_app_env load_app_support local_app_support migrate_from_shinytest platform_variant record_test register_input_processor screenshot_max_difference test_app use_shinytest2 use_shinytest2_test with_app_support cpp"
  },
  {
    "id": 824,
    "package_name": "mockery",
    "title": "Mocking Library for R",
    "description": "The two main functionalities of this package are creating\nmock objects (functions) and selectively intercepting calls to\na given function that originate in some other function. It can\nbe used with any testing framework available for R. Mock\nobjects can be injected with either this package's own stub()\nfunction or a similar with_mocked_binding() facility present in\nthe 'testthat' package.",
    "version": "0.4.5.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Noam Finkelstein [aut],\nLukasz Bartnik [aut],\nJim Hester [aut],\nHadley Wickham [aut, cre]",
    "url": "https://github.com/r-lib/mockery",
    "bug_reports": "https://github.com/r-lib/mockery/issues",
    "repository": "",
    "exports": [
      [
        "expect_args"
      ],
      [
        "expect_call"
      ],
      [
        "expect_called"
      ],
      [
        "mock"
      ],
      [
        "mock_args"
      ],
      [
        "mock_calls"
      ],
      [
        "stub"
      ]
    ],
    "topics": [],
    "score": 11.9499,
    "stars": 104,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "mockery Mocking Library for R The two main functionalities of this package are creating\nmock objects (functions) and selectively intercepting calls to\na given function that originate in some other function. It can\nbe used with any testing framework available for R. Mock\nobjects can be injected with either this package's own stub()\nfunction or a similar with_mocked_binding() facility present in\nthe 'testthat' package. expect_args expect_call expect_called mock mock_args mock_calls stub "
  },
  {
    "id": 1415,
    "package_name": "vdiffr",
    "title": "Visual Regression Testing and Graphical Diffing",
    "description": "An extension to the 'testthat' package that makes it easy\nto add graphical unit tests. It provides a Shiny application to\nmanage the test cases.",
    "version": "1.0.8.9000",
    "maintainer": "Lionel Henry <lionel@posit.co>",
    "author": "Lionel Henry [cre, aut],\nThomas Lin Pedersen [aut] (ORCID:\n<https://orcid.org/0000-0002-5147-4711>),\nPosit Software, PBC [cph, fnd],\nT Jake Luciani [aut] (svglite),\nMatthieu Decorde [aut] (svglite),\nVaudor Lise [aut] (svglite),\nTony Plate [ctb] (svglite: Early line dashing code),\nDavid Gohel [ctb] (svglite: Line dashing code and raster code),\nYixuan Qiu [ctb] (svglite: Improved styles; polypath implementation),\nH\u00e5kon Malmedal [ctb] (svglite: Opacity code)",
    "url": "https://vdiffr.r-lib.org/, https://github.com/r-lib/vdiffr",
    "bug_reports": "https://github.com/r-lib/vdiffr/issues",
    "repository": "",
    "exports": [
      [
        "expect_doppelganger"
      ],
      [
        "write_svg"
      ]
    ],
    "topics": [
      [
        "ggplot2"
      ],
      [
        "graphics"
      ],
      [
        "testthat"
      ],
      [
        "libpng"
      ],
      [
        "cpp"
      ]
    ],
    "score": 11.6,
    "stars": 195,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "vdiffr Visual Regression Testing and Graphical Diffing An extension to the 'testthat' package that makes it easy\nto add graphical unit tests. It provides a Shiny application to\nmanage the test cases. expect_doppelganger write_svg ggplot2 graphics testthat libpng cpp"
  },
  {
    "id": 829,
    "package_name": "modeldata",
    "title": "Data Sets Useful for Modeling Examples",
    "description": "Data sets used for demonstrating or testing model-related\npackages are contained in this package.",
    "version": "1.5.1.9000",
    "maintainer": "Max Kuhn <max@posit.co>",
    "author": "Max Kuhn [aut, cre],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>)",
    "url": "https://modeldata.tidymodels.org,\nhttps://github.com/tidymodels/modeldata",
    "bug_reports": "https://github.com/tidymodels/modeldata/issues",
    "repository": "",
    "exports": [
      [
        "sim_classification"
      ],
      [
        "sim_logistic"
      ],
      [
        "sim_multinomial"
      ],
      [
        "sim_noise"
      ],
      [
        "sim_regression"
      ]
    ],
    "topics": [],
    "score": 11.4818,
    "stars": 23,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "modeldata Data Sets Useful for Modeling Examples Data sets used for demonstrating or testing model-related\npackages are contained in this package. sim_classification sim_logistic sim_multinomial sim_noise sim_regression "
  },
  {
    "id": 1116,
    "package_name": "rix",
    "title": "Reproducible Data Science Environments with 'Nix'",
    "description": "Simplifies the creation of reproducible data science\nenvironments using the 'Nix' package manager, as described in\nDolstra (2006) <ISBN 90-393-4130-3>. The included `rix()`\nfunction generates a complete description of the environment as\na `default.nix` file, which can then be built using 'Nix'. This\nresults in project specific software environments with pinned\nversions of R, packages, linked system dependencies, and other\ntools or programming languages such as Python or Julia.\nAdditional helpers make it easy to run R code in 'Nix' software\nenvironments for testing and production.",
    "version": "0.17.3",
    "maintainer": "Bruno Rodrigues <bruno@brodrigues.co>",
    "author": "Bruno Rodrigues [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-3211-3689>),\nPhilipp Baumann [aut] (ORCID: <https://orcid.org/0000-0002-3194-8975>),\nDavid Watkins [rev] (David reviewed the package (v. 0.9.1) for\nrOpenSci, see\n<https://github.com/ropensci/software-review/issues/625>),\nJacob Wujiciak-Jens [rev] (ORCID:\n<https://orcid.org/0000-0002-7281-3989>, Jacob reviewed the package\n(v. 0.9.1) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/625>),\nRichard J. Acton [ctb] (ORCID: <https://orcid.org/0000-0002-2574-9611>),\nJordi Rosell [ctb] (ORCID: <https://orcid.org/0000-0002-4349-1458>),\nElio Campitelli [ctb] (ORCID: <https://orcid.org/0000-0002-7742-9230>),\nL\u00e1szl\u00f3 Kupcsik [ctb] (ORCID: <https://orcid.org/0000-0003-3535-5496>),\nMichael Heming [ctb] (ORCID: <https://orcid.org/0000-0002-9568-2790>)",
    "url": "https://docs.ropensci.org/rix/",
    "bug_reports": "https://github.com/ropensci/rix/issues",
    "repository": "",
    "exports": [
      [
        "available_dates"
      ],
      [
        "available_df"
      ],
      [
        "available_r"
      ],
      [
        "ga_cachix"
      ],
      [
        "make_launcher"
      ],
      [
        "nix_build"
      ],
      [
        "renv2nix"
      ],
      [
        "rix"
      ],
      [
        "rix_init"
      ],
      [
        "setup_cachix"
      ],
      [
        "tar_nix_ga"
      ],
      [
        "with_nix"
      ]
    ],
    "topics": [
      [
        "nix"
      ],
      [
        "peer-reviewed"
      ],
      [
        "reproducibility"
      ],
      [
        "reproducible-research"
      ]
    ],
    "score": 11.4222,
    "stars": 338,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "rix Reproducible Data Science Environments with 'Nix' Simplifies the creation of reproducible data science\nenvironments using the 'Nix' package manager, as described in\nDolstra (2006) <ISBN 90-393-4130-3>. The included `rix()`\nfunction generates a complete description of the environment as\na `default.nix` file, which can then be built using 'Nix'. This\nresults in project specific software environments with pinned\nversions of R, packages, linked system dependencies, and other\ntools or programming languages such as Python or Julia.\nAdditional helpers make it easy to run R code in 'Nix' software\nenvironments for testing and production. available_dates available_df available_r ga_cachix make_launcher nix_build renv2nix rix rix_init setup_cachix tar_nix_ga with_nix nix peer-reviewed reproducibility reproducible-research"
  },
  {
    "id": 1260,
    "package_name": "spelling",
    "title": "Tools for Spell Checking in R",
    "description": "Spell checking common document formats including latex,\nmarkdown, manual pages, and description files. Includes\nutilities to automate checking of documentation and vignettes\nas a unit test during 'R CMD check'. Both British and American\nEnglish are supported out of the box and other languages can be\nadded. In addition, packages may define a 'wordlist' to allow\ncustom terminology without having to abuse punctuation.",
    "version": "2.3.2",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [cre, aut] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\nJim Hester [aut]",
    "url": "https://ropensci.r-universe.dev/spelling\nhttps://docs.ropensci.org/spelling/",
    "bug_reports": "https://github.com/ropensci/spelling/issues",
    "repository": "",
    "exports": [
      [
        "get_wordlist"
      ],
      [
        "spell_check_files"
      ],
      [
        "spell_check_package"
      ],
      [
        "spell_check_setup"
      ],
      [
        "spell_check_test"
      ],
      [
        "spell_check_text"
      ],
      [
        "update_wordlist"
      ]
    ],
    "topics": [
      [
        "spell-check"
      ],
      [
        "spell-checker"
      ],
      [
        "spellcheck"
      ],
      [
        "spellchecker"
      ],
      [
        "spelling"
      ]
    ],
    "score": 11.0486,
    "stars": 108,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "spelling Tools for Spell Checking in R Spell checking common document formats including latex,\nmarkdown, manual pages, and description files. Includes\nutilities to automate checking of documentation and vignettes\nas a unit test during 'R CMD check'. Both British and American\nEnglish are supported out of the box and other languages can be\nadded. In addition, packages may define a 'wordlist' to allow\ncustom terminology without having to abuse punctuation. get_wordlist spell_check_files spell_check_package spell_check_setup spell_check_test spell_check_text update_wordlist spell-check spell-checker spellcheck spellchecker spelling"
  },
  {
    "id": 1413,
    "package_name": "vcr",
    "title": "Record 'HTTP' Calls to Disk",
    "description": "Record test suite 'HTTP' requests and replays them during\nfuture runs. A port of the Ruby gem of the same name\n(<https://github.com/vcr/vcr/>). Works by recording real 'HTTP'\nrequests/responses on disk in 'cassettes', and then replaying\nmatching responses on subsequent requests.",
    "version": "2.1.0",
    "maintainer": "Scott Chamberlain <myrmecocystus@gmail.com>",
    "author": "Scott Chamberlain [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nAaron Wolen [aut] (ORCID: <https://orcid.org/0000-0003-2542-2202>),\nMa\u00eblle Salmon [aut] (ORCID: <https://orcid.org/0000-0002-2815-0399>),\nDaniel Possenriede [aut] (ORCID:\n<https://orcid.org/0000-0002-6738-9845>),\nHadley Wickham [aut],\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/vcr/,\nhttps://books.ropensci.org/http-testing/,\nhttps://docs.ropensci.org/vcr/",
    "bug_reports": "https://github.com/ropensci/vcr/issues",
    "repository": "",
    "exports": [
      [
        "Cassette"
      ],
      [
        "cassette_path"
      ],
      [
        "cassettes"
      ],
      [
        "check_cassette_names"
      ],
      [
        "current_cassette"
      ],
      [
        "current_cassette_recording"
      ],
      [
        "current_cassette_replaying"
      ],
      [
        "eject_cassette"
      ],
      [
        "insert_cassette"
      ],
      [
        "insert_example_cassette"
      ],
      [
        "is_recording"
      ],
      [
        "is_replaying"
      ],
      [
        "local_cassette"
      ],
      [
        "local_vcr_configure"
      ],
      [
        "local_vcr_configure_log"
      ],
      [
        "setup_knitr"
      ],
      [
        "skip_if_vcr_off"
      ],
      [
        "turn_off"
      ],
      [
        "turn_on"
      ],
      [
        "turned_off"
      ],
      [
        "turned_on"
      ],
      [
        "use_cassette"
      ],
      [
        "use_vcr"
      ],
      [
        "vcr_config_defaults"
      ],
      [
        "vcr_configuration"
      ],
      [
        "vcr_configure"
      ],
      [
        "vcr_configure_log"
      ],
      [
        "vcr_configure_reset"
      ],
      [
        "vcr_last_request"
      ],
      [
        "vcr_last_response"
      ],
      [
        "vcr_test_path"
      ]
    ],
    "topics": [
      [
        "http"
      ],
      [
        "https"
      ],
      [
        "api"
      ],
      [
        "web-services"
      ],
      [
        "curl"
      ],
      [
        "mock"
      ],
      [
        "mocking"
      ],
      [
        "http-mocking"
      ],
      [
        "testing"
      ],
      [
        "testing-tools"
      ],
      [
        "tdd"
      ],
      [
        "unit-testing"
      ],
      [
        "vcr"
      ]
    ],
    "score": 10.9253,
    "stars": 93,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "vcr Record 'HTTP' Calls to Disk Record test suite 'HTTP' requests and replays them during\nfuture runs. A port of the Ruby gem of the same name\n(<https://github.com/vcr/vcr/>). Works by recording real 'HTTP'\nrequests/responses on disk in 'cassettes', and then replaying\nmatching responses on subsequent requests. Cassette cassette_path cassettes check_cassette_names current_cassette current_cassette_recording current_cassette_replaying eject_cassette insert_cassette insert_example_cassette is_recording is_replaying local_cassette local_vcr_configure local_vcr_configure_log setup_knitr skip_if_vcr_off turn_off turn_on turned_off turned_on use_cassette use_vcr vcr_config_defaults vcr_configuration vcr_configure vcr_configure_log vcr_configure_reset vcr_last_request vcr_last_response vcr_test_path http https api web-services curl mock mocking http-mocking testing testing-tools tdd unit-testing vcr"
  },
  {
    "id": 487,
    "package_name": "dittodb",
    "title": "A Test Environment for Database Requests",
    "description": "Testing and documenting code that communicates with remote\ndatabases can be painful. Although the interaction with R is\nusually relatively simple (e.g. data(frames) passed to and from\na database), because they rely on a separate service and the\ndata there, testing them can be difficult to set up,\nunsustainable in a continuous integration environment, or\nimpossible without replicating an entire production cluster.\nThis package addresses that by allowing you to make recordings\nfrom your database interactions and then play them back while\ntesting (or in other contexts) all without needing to spin up\nor have access to the database your code would typically\nconnect to.",
    "version": "0.1.9.9000",
    "maintainer": "Jonathan Keane <jkeane@gmail.com>",
    "author": "Jonathan Keane [aut, cre] (ORCID:\n<https://orcid.org/0000-0001-7087-9776>),\nMauricio Vargas [aut] (ORCID: <https://orcid.org/0000-0003-1017-7574>),\nHelen Miller [rev] (reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/366),\nEtienne Racine [rev] (reviewed the package for rOpenSci, see\nhttps://github.com/ropensci/software-review/issues/366)",
    "url": "https://dittodb.jonkeane.com/, https://github.com/ropensci/dittodb",
    "bug_reports": "https://github.com/ropensci/dittodb/issues",
    "repository": "",
    "exports": [
      [
        ".db_mock_paths"
      ],
      [
        ".dittodb_env"
      ],
      [
        "capture_db_requests"
      ],
      [
        "check_db_path"
      ],
      [
        "check_for_pkg"
      ],
      [
        "clean_statement"
      ],
      [
        "db_mock_paths"
      ],
      [
        "db_path_sanitize"
      ],
      [
        "dbBegin"
      ],
      [
        "dbClearResult"
      ],
      [
        "dbColumnInfo"
      ],
      [
        "dbCommit"
      ],
      [
        "dbDisconnect"
      ],
      [
        "dbExistsTable"
      ],
      [
        "dbFetch"
      ],
      [
        "dbGetInfo"
      ],
      [
        "dbGetQuery"
      ],
      [
        "dbGetRowsAffected"
      ],
      [
        "dbHasCompleted"
      ],
      [
        "dbListFields"
      ],
      [
        "dbListTables"
      ],
      [
        "dbMockConnect"
      ],
      [
        "dbQuoteIdentifier"
      ],
      [
        "dbQuoteString"
      ],
      [
        "dbRemoveTable"
      ],
      [
        "dbRollback"
      ],
      [
        "dbSendQuery"
      ],
      [
        "dbSendStatement"
      ],
      [
        "dbWriteTable"
      ],
      [
        "dittodb_debug_level"
      ],
      [
        "expect_sql"
      ],
      [
        "fetch"
      ],
      [
        "get_dbname"
      ],
      [
        "get_redactor"
      ],
      [
        "get_type"
      ],
      [
        "hash"
      ],
      [
        "hash_db_object"
      ],
      [
        "make_path"
      ],
      [
        "nycflights_sqlite"
      ],
      [
        "nycflights13_create_sql"
      ],
      [
        "nycflights13_create_sqlite"
      ],
      [
        "redact_columns"
      ],
      [
        "sanitize_table_id"
      ],
      [
        "serialize_bit64"
      ],
      [
        "set_dittodb_debug_level"
      ],
      [
        "start_db_capturing"
      ],
      [
        "start_mock_db"
      ],
      [
        "stop_db_capturing"
      ],
      [
        "stop_mock_db"
      ],
      [
        "use_dittodb"
      ],
      [
        "with_mock_db"
      ],
      [
        "with_mock_path"
      ]
    ],
    "topics": [],
    "score": 8.5756,
    "stars": 81,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "dittodb A Test Environment for Database Requests Testing and documenting code that communicates with remote\ndatabases can be painful. Although the interaction with R is\nusually relatively simple (e.g. data(frames) passed to and from\na database), because they rely on a separate service and the\ndata there, testing them can be difficult to set up,\nunsustainable in a continuous integration environment, or\nimpossible without replicating an entire production cluster.\nThis package addresses that by allowing you to make recordings\nfrom your database interactions and then play them back while\ntesting (or in other contexts) all without needing to spin up\nor have access to the database your code would typically\nconnect to. .db_mock_paths .dittodb_env capture_db_requests check_db_path check_for_pkg clean_statement db_mock_paths db_path_sanitize dbBegin dbClearResult dbColumnInfo dbCommit dbDisconnect dbExistsTable dbFetch dbGetInfo dbGetQuery dbGetRowsAffected dbHasCompleted dbListFields dbListTables dbMockConnect dbQuoteIdentifier dbQuoteString dbRemoveTable dbRollback dbSendQuery dbSendStatement dbWriteTable dittodb_debug_level expect_sql fetch get_dbname get_redactor get_type hash hash_db_object make_path nycflights_sqlite nycflights13_create_sql nycflights13_create_sqlite redact_columns sanitize_table_id serialize_bit64 set_dittodb_debug_level start_db_capturing start_mock_db stop_db_capturing stop_mock_db use_dittodb with_mock_db with_mock_path "
  },
  {
    "id": 1435,
    "package_name": "webfakes",
    "title": "Fake Web Apps for HTTP Testing",
    "description": "Create a web app that makes it easier to test web clients\nwithout using the internet. It includes a web app framework\nwith path matching, parameters and templates. Can parse various\n'HTTP' request bodies. Can send 'JSON' data or files from the\ndisk. Includes a web app that implements the 'httpbin.org' web\nservice.",
    "version": "1.4.0.9000",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi [aut, cre],\nPosit Software, PBC [cph, fnd] (ROR: <https://ror.org/03wc8by49>),\nCivetweb contributors [ctb] (see inst/credits/ciwetweb.md),\nRedoc contributors [ctb] (see inst/credits/redoc.md),\nL. Peter Deutsch [ctb] (src/md5.h),\nMartin Purschke [ctb] (src/md5.h),\nAladdin Enterprises [cph] (src/md5.h),\nMa\u00eblle Salmon [ctb] (ORCID: <https://orcid.org/0000-0002-2815-0399>)",
    "url": "https://webfakes.r-lib.org/, https://github.com/r-lib/webfakes",
    "bug_reports": "https://github.com/r-lib/webfakes/issues",
    "repository": "",
    "exports": [
      [
        "git_app"
      ],
      [
        "http_time_stamp"
      ],
      [
        "httpbin_app"
      ],
      [
        "local_app_process"
      ],
      [
        "mw_cgi"
      ],
      [
        "mw_cookie_parser"
      ],
      [
        "mw_etag"
      ],
      [
        "mw_json"
      ],
      [
        "mw_log"
      ],
      [
        "mw_multipart"
      ],
      [
        "mw_range_parser"
      ],
      [
        "mw_raw"
      ],
      [
        "mw_static"
      ],
      [
        "mw_text"
      ],
      [
        "mw_urlencoded"
      ],
      [
        "new_app"
      ],
      [
        "new_app_process"
      ],
      [
        "new_regexp"
      ],
      [
        "oauth2_httr_login"
      ],
      [
        "oauth2_login"
      ],
      [
        "oauth2_resource_app"
      ],
      [
        "oauth2_third_party_app"
      ],
      [
        "server_opts"
      ],
      [
        "tmpl_glue"
      ]
    ],
    "topics": [],
    "score": 8.2631,
    "stars": 63,
    "primary_category": "tidyverse",
    "source_universe": "r-lib",
    "search_text": "webfakes Fake Web Apps for HTTP Testing Create a web app that makes it easier to test web clients\nwithout using the internet. It includes a web app framework\nwith path matching, parameters and templates. Can parse various\n'HTTP' request bodies. Can send 'JSON' data or files from the\ndisk. Includes a web app that implements the 'httpbin.org' web\nservice. git_app http_time_stamp httpbin_app local_app_process mw_cgi mw_cookie_parser mw_etag mw_json mw_log mw_multipart mw_range_parser mw_raw mw_static mw_text mw_urlencoded new_app new_app_process new_regexp oauth2_httr_login oauth2_login oauth2_resource_app oauth2_third_party_app server_opts tmpl_glue "
  },
  {
    "id": 1436,
    "package_name": "webmockr",
    "title": "Stubbing and Setting Expectations on 'HTTP' Requests",
    "description": "Stubbing and setting expectations on 'HTTP' requests.\nIncludes tools for stubbing 'HTTP' requests, including expected\nrequest conditions and response conditions. Match on 'HTTP'\nmethod, query parameters, request body, headers and more. Can\nbe used for unit tests or outside of a testing context.",
    "version": "2.2.1.92",
    "maintainer": "Scott Chamberlain <myrmecocystus+r@gmail.com>",
    "author": "Scott Chamberlain [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1444-9135>),\nAaron Wolen [ctb] (ORCID: <https://orcid.org/0000-0003-2542-2202>),\nrOpenSci [fnd] (ROR: <https://ror.org/019jywm96>)",
    "url": "https://github.com/ropensci/webmockr,\nhttps://books.ropensci.org/http-testing/,\nhttps://docs.ropensci.org/webmockr/",
    "bug_reports": "https://github.com/ropensci/webmockr/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "build_crul_response"
      ],
      [
        "build_httr_response"
      ],
      [
        "build_httr2_request"
      ],
      [
        "build_httr2_response"
      ],
      [
        "CrulAdapter"
      ],
      [
        "disable"
      ],
      [
        "enable"
      ],
      [
        "enabled"
      ],
      [
        "excluding"
      ],
      [
        "httr_mock"
      ],
      [
        "httr2_mock"
      ],
      [
        "Httr2Adapter"
      ],
      [
        "HttrAdapter"
      ],
      [
        "including"
      ],
      [
        "last_request"
      ],
      [
        "last_stub"
      ],
      [
        "mock_file"
      ],
      [
        "pluck_body"
      ],
      [
        "remove_request_stub"
      ],
      [
        "request_registry"
      ],
      [
        "request_registry_clear"
      ],
      [
        "request_registry_filter"
      ],
      [
        "RequestSignature"
      ],
      [
        "Response"
      ],
      [
        "stub_body_diff"
      ],
      [
        "stub_registry"
      ],
      [
        "stub_registry_clear"
      ],
      [
        "stub_request"
      ],
      [
        "to_raise"
      ],
      [
        "to_return"
      ],
      [
        "to_return_"
      ],
      [
        "to_timeout"
      ],
      [
        "webmockr_allow_net_connect"
      ],
      [
        "webmockr_configuration"
      ],
      [
        "webmockr_configure"
      ],
      [
        "webmockr_configure_reset"
      ],
      [
        "webmockr_crul_fetch"
      ],
      [
        "webmockr_disable"
      ],
      [
        "webmockr_disable_net_connect"
      ],
      [
        "webmockr_enable"
      ],
      [
        "webmockr_net_connect_allowed"
      ],
      [
        "webmockr_reset"
      ],
      [
        "wi_th"
      ],
      [
        "wi_th_"
      ]
    ],
    "topics": [
      [
        "http"
      ],
      [
        "https"
      ],
      [
        "api"
      ],
      [
        "web-services"
      ],
      [
        "curl"
      ],
      [
        "mock"
      ],
      [
        "mocking"
      ],
      [
        "fakeweb"
      ],
      [
        "http-mocking"
      ],
      [
        "testing"
      ],
      [
        "testing-tools"
      ],
      [
        "tdd"
      ],
      [
        "http-mock"
      ]
    ],
    "score": 8.2078,
    "stars": 50,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "webmockr Stubbing and Setting Expectations on 'HTTP' Requests Stubbing and setting expectations on 'HTTP' requests.\nIncludes tools for stubbing 'HTTP' requests, including expected\nrequest conditions and response conditions. Match on 'HTTP'\nmethod, query parameters, request body, headers and more. Can\nbe used for unit tests or outside of a testing context. %>% build_crul_response build_httr_response build_httr2_request build_httr2_response CrulAdapter disable enable enabled excluding httr_mock httr2_mock Httr2Adapter HttrAdapter including last_request last_stub mock_file pluck_body remove_request_stub request_registry request_registry_clear request_registry_filter RequestSignature Response stub_body_diff stub_registry stub_registry_clear stub_request to_raise to_return to_return_ to_timeout webmockr_allow_net_connect webmockr_configuration webmockr_configure webmockr_configure_reset webmockr_crul_fetch webmockr_disable webmockr_disable_net_connect webmockr_enable webmockr_net_connect_allowed webmockr_reset wi_th wi_th_ http https api web-services curl mock mocking fakeweb http-mocking testing testing-tools tdd http-mock"
  },
  {
    "id": 806,
    "package_name": "melt",
    "title": "Multiple Empirical Likelihood Tests",
    "description": "Performs multiple empirical likelihood tests. It offers an\neasy-to-use interface and flexibility in specifying hypotheses\nand calibration methods, extending the framework to\nsimultaneous inferences.  The core computational routines are\nimplemented using the 'Eigen' 'C++' library and 'RcppEigen'\ninterface, with 'OpenMP' for parallel computation.  Details of\nthe testing procedures are provided in Kim, MacEachern, and\nPeruggia (2023) <doi:10.1080/10485252.2023.2206919>. A\ncompanion paper by Kim, MacEachern, and Peruggia (2024)\n<doi:10.18637/jss.v108.i05> is available for further\ninformation. This work was supported by the U.S. National\nScience Foundation under Grants No. SES-1921523 and\nDMS-2015552.",
    "version": "1.11.4",
    "maintainer": "Eunseop Kim <markean@pm.me>",
    "author": "Eunseop Kim [aut, cph, cre],\nSteven MacEachern [ctb, ths],\nMario Peruggia [ctb, ths],\nPierre Chausse [rev],\nAlex Stringer [rev]",
    "url": "https://docs.ropensci.org/melt/, https://github.com/ropensci/melt",
    "bug_reports": "https://github.com/ropensci/melt/issues",
    "repository": "",
    "exports": [
      [
        "chisq"
      ],
      [
        "coef"
      ],
      [
        "confint"
      ],
      [
        "confreg"
      ],
      [
        "conv"
      ],
      [
        "critVal"
      ],
      [
        "el_control"
      ],
      [
        "el_eval"
      ],
      [
        "el_glm"
      ],
      [
        "el_lm"
      ],
      [
        "el_mean"
      ],
      [
        "el_sd"
      ],
      [
        "eld"
      ],
      [
        "elmt"
      ],
      [
        "elt"
      ],
      [
        "formula"
      ],
      [
        "getDF"
      ],
      [
        "getOptim"
      ],
      [
        "logL"
      ],
      [
        "logLR"
      ],
      [
        "logProb"
      ],
      [
        "nobs"
      ],
      [
        "plot"
      ],
      [
        "print"
      ],
      [
        "pVal"
      ],
      [
        "sigTests"
      ],
      [
        "summary"
      ],
      [
        "weights"
      ]
    ],
    "topics": [
      [
        "cpp"
      ],
      [
        "openmp"
      ]
    ],
    "score": 6.8281,
    "stars": 17,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "melt Multiple Empirical Likelihood Tests Performs multiple empirical likelihood tests. It offers an\neasy-to-use interface and flexibility in specifying hypotheses\nand calibration methods, extending the framework to\nsimultaneous inferences.  The core computational routines are\nimplemented using the 'Eigen' 'C++' library and 'RcppEigen'\ninterface, with 'OpenMP' for parallel computation.  Details of\nthe testing procedures are provided in Kim, MacEachern, and\nPeruggia (2023) <doi:10.1080/10485252.2023.2206919>. A\ncompanion paper by Kim, MacEachern, and Peruggia (2024)\n<doi:10.18637/jss.v108.i05> is available for further\ninformation. This work was supported by the U.S. National\nScience Foundation under Grants No. SES-1921523 and\nDMS-2015552. chisq coef confint confreg conv critVal el_control el_eval el_glm el_lm el_mean el_sd eld elmt elt formula getDF getOptim logL logLR logProb nobs plot print pVal sigTests summary weights cpp openmp"
  },
  {
    "id": 1359,
    "package_name": "tinysnapshot",
    "title": "Snapshots for Unit Tests using the 'tinytest' Framework",
    "description": "Snapshots for unit tests using the 'tinytest' framework\nfor R. Includes expectations to test base R and 'ggplot2' plots\nas well as console output from print().",
    "version": "0.2.0.1",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "author": "Vincent Arel-Bundock [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2042-7063>)",
    "url": "https://github.com/vincentarelbundock/tinysnapshot",
    "bug_reports": "https://github.com/vincentarelbundock/tinysnapshot/issues",
    "repository": "",
    "exports": [
      [
        "expect_equivalent_images"
      ],
      [
        "expect_snapshot_plot"
      ],
      [
        "expect_snapshot_print"
      ]
    ],
    "topics": [],
    "score": 6.4062,
    "stars": 18,
    "primary_category": "statistics",
    "source_universe": "vincentarelbundock",
    "search_text": "tinysnapshot Snapshots for Unit Tests using the 'tinytest' Framework Snapshots for unit tests using the 'tinytest' framework\nfor R. Includes expectations to test base R and 'ggplot2' plots\nas well as console output from print(). expect_equivalent_images expect_snapshot_plot expect_snapshot_print "
  },
  {
    "id": 677,
    "package_name": "hdcuremodels",
    "title": "High-Dimensional Cure Models",
    "description": "Provides functions for fitting various penalized\nparametric and semi-parametric mixture cure models with\ndifferent penalty functions, testing for a significant cure\nfraction, and testing for sufficient follow-up as described in\nFu et al (2022)<doi:10.1002/sim.9513> and Archer et al\n(2024)<doi:10.1186/s13045-024-01553-6>. False discovery rate\ncontrolled variable selection is provided using model-X\nknock-offs.",
    "version": "0.0.6",
    "maintainer": "Kellie J. Archer <archer.43@osu.edu>",
    "author": "Han Fu [aut],\nKellie J. Archer [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-1555-5781>),\nTung Lam Nguyen [rev] (Reviewed the package for ROpenSci),\nPanagiotis Papastamoulis [rev] (Reviewed the package for ROpenSci)",
    "url": "https://github.com/ropensci/hdcuremodels",
    "bug_reports": "https://github.com/ropensci/hdcuremodels/issues",
    "repository": "",
    "exports": [
      [
        "auc_mcm"
      ],
      [
        "concordance_mcm"
      ],
      [
        "cure_estimate"
      ],
      [
        "cureem"
      ],
      [
        "curegmifs"
      ],
      [
        "cv_cureem"
      ],
      [
        "cv_curegmifs"
      ],
      [
        "generate_cure_data"
      ],
      [
        "nonzerocure_test"
      ],
      [
        "sufficient_fu_test"
      ]
    ],
    "topics": [],
    "score": 6.1888,
    "stars": 0,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "hdcuremodels High-Dimensional Cure Models Provides functions for fitting various penalized\nparametric and semi-parametric mixture cure models with\ndifferent penalty functions, testing for a significant cure\nfraction, and testing for sufficient follow-up as described in\nFu et al (2022)<doi:10.1002/sim.9513> and Archer et al\n(2024)<doi:10.1186/s13045-024-01553-6>. False discovery rate\ncontrolled variable selection is provided using model-X\nknock-offs. auc_mcm concordance_mcm cure_estimate cureem curegmifs cv_cureem cv_curegmifs generate_cure_data nonzerocure_test sufficient_fu_test "
  },
  {
    "id": 1261,
    "package_name": "spiro",
    "title": "Manage Data from Cardiopulmonary Exercise Testing",
    "description": "Import, process, summarize and visualize raw data from\nmetabolic carts. See Robergs, Dwyer, and Astorino (2010)\n<doi:10.2165/11319670-000000000-00000> for more details on data\nprocessing.",
    "version": "0.2.3.9000",
    "maintainer": "Simon Nolte <s.nolte@dshs-koeln.de>",
    "author": "Simon Nolte [aut, cre] (ORCID: <https://orcid.org/0000-0003-1643-1860>),\nManuel Ramon [rev] (reviewed the package (v. 0.0.5) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/541>),\nJames Hunter [rev] (reviewed the package (v. 0.0.5) for rOpenSci, see\n<https://github.com/ropensci/software-review/issues/541>)",
    "url": "https://github.com/ropensci/spiro,\nhttps://docs.ropensci.org/spiro/",
    "bug_reports": "https://github.com/ropensci/spiro/issues",
    "repository": "",
    "exports": [
      [
        "add_bodymass"
      ],
      [
        "add_hr"
      ],
      [
        "add_protocol"
      ],
      [
        "bw_filter"
      ],
      [
        "get_anonid"
      ],
      [
        "get_protocol"
      ],
      [
        "pt_const"
      ],
      [
        "pt_pre"
      ],
      [
        "pt_steps"
      ],
      [
        "pt_wu"
      ],
      [
        "set_protocol"
      ],
      [
        "set_protocol_manual"
      ],
      [
        "spiro"
      ],
      [
        "spiro_example"
      ],
      [
        "spiro_import"
      ],
      [
        "spiro_max"
      ],
      [
        "spiro_plot"
      ],
      [
        "spiro_raw"
      ],
      [
        "spiro_smooth"
      ],
      [
        "spiro_summary"
      ]
    ],
    "topics": [],
    "score": 6.1492,
    "stars": 15,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "spiro Manage Data from Cardiopulmonary Exercise Testing Import, process, summarize and visualize raw data from\nmetabolic carts. See Robergs, Dwyer, and Astorino (2010)\n<doi:10.2165/11319670-000000000-00000> for more details on data\nprocessing. add_bodymass add_hr add_protocol bw_filter get_anonid get_protocol pt_const pt_pre pt_steps pt_wu set_protocol set_protocol_manual spiro spiro_example spiro_import spiro_max spiro_plot spiro_raw spiro_smooth spiro_summary "
  },
  {
    "id": 263,
    "package_name": "autotest",
    "title": "Automatic Package Testing",
    "description": "Automatic testing of R packages via a simple YAML schema.",
    "version": "0.0.2.215",
    "maintainer": "Mark Padgham <mark.padgham@email.com>",
    "author": "Mark Padgham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-2172-5265>),\nJouni Helske [ctb] (ORCID: <https://orcid.org/0000-0001-7130-793X>)",
    "url": "https://docs.ropensci.org/autotest/,\nhttps://github.com/ropensci-review-tools/autotest",
    "bug_reports": "https://github.com/ropensci-review-tools/autotest/issues",
    "repository": "",
    "exports": [
      [
        "at_yaml_template"
      ],
      [
        "autotest_obj"
      ],
      [
        "autotest_package"
      ],
      [
        "autotest_types"
      ],
      [
        "autotest_yaml"
      ],
      [
        "examples_to_yaml"
      ],
      [
        "expect_autotest_no_err"
      ],
      [
        "expect_autotest_no_testdata"
      ],
      [
        "expect_autotest_no_warn"
      ],
      [
        "expect_autotest_notes"
      ],
      [
        "expect_autotest_testdata"
      ]
    ],
    "topics": [
      [
        "automated-testing"
      ],
      [
        "fuzzing"
      ],
      [
        "testing"
      ]
    ],
    "score": 6.0765,
    "stars": 53,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "autotest Automatic Package Testing Automatic testing of R packages via a simple YAML schema. at_yaml_template autotest_obj autotest_package autotest_types autotest_yaml examples_to_yaml expect_autotest_no_err expect_autotest_no_testdata expect_autotest_no_warn expect_autotest_notes expect_autotest_testdata automated-testing fuzzing testing"
  },
  {
    "id": 981,
    "package_name": "pkgreviewr",
    "title": "rOpenSci package review project template",
    "description": "Creates files and collects materials necessary to complete\nan rOpenSci package review.  Review files are prepopulated with\nreview package specific metadata. Review package source code is\nalso cloned for local testing and inspection.",
    "version": "0.3.9999",
    "maintainer": "Ma\u00eblle Salmon <maelle@ropensci.org>",
    "author": "Anna Krystalli [aut] (ORCID: <https://orcid.org/0000-0002-2378-4915>,\nCreated the package),\nMa\u00eblle Salmon [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-2815-0399>)",
    "url": "https://docs.ropensci.org/pkgreviewr/,\nhttps://github.com/ropensci-org/pkgreviewr",
    "bug_reports": "https://github.com/ropensci-org/pkgreviewr/issues",
    "repository": "",
    "exports": [
      [
        "pkgreview_create"
      ],
      [
        "pkgreview_getdata"
      ],
      [
        "pkgreview_index_rmd"
      ],
      [
        "pkgreview_init"
      ],
      [
        "pkgreview_readme_md"
      ],
      [
        "pkgreview_request"
      ],
      [
        "render_request"
      ],
      [
        "try_whoami"
      ],
      [
        "use_onboarding_tmpl"
      ]
    ],
    "topics": [
      [
        "review"
      ],
      [
        "ropensci"
      ],
      [
        "ropensci-reviews"
      ],
      [
        "unconf"
      ],
      [
        "unconf18"
      ]
    ],
    "score": 6.0668,
    "stars": 36,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "pkgreviewr rOpenSci package review project template Creates files and collects materials necessary to complete\nan rOpenSci package review.  Review files are prepopulated with\nreview package specific metadata. Review package source code is\nalso cloned for local testing and inspection. pkgreview_create pkgreview_getdata pkgreview_index_rmd pkgreview_init pkgreview_readme_md pkgreview_request render_request try_whoami use_onboarding_tmpl review ropensci ropensci-reviews unconf unconf18"
  },
  {
    "id": 825,
    "package_name": "mockr",
    "title": "Mocking in R",
    "description": "Provides a means to mock a package function, i.e.,\ntemporarily substitute it for testing. Designed as a drop-in\nreplacement for the now deprecated 'testthat::with_mock()' and\n'testthat::local_mock()'.",
    "version": "0.2.0.9002",
    "maintainer": "Kirill M\u00fcller <krlmlr+r@mailbox.org>",
    "author": "Kirill M\u00fcller [aut, cre]",
    "url": "https://krlmlr.github.io/mockr/, https://github.com/krlmlr/mockr",
    "bug_reports": "https://github.com/krlmlr/mockr/issues",
    "repository": "",
    "exports": [
      [
        "get_mock_env"
      ],
      [
        "local_mock"
      ],
      [
        "with_mock"
      ]
    ],
    "topics": [],
    "score": 5.875,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "mockr Mocking in R Provides a means to mock a package function, i.e.,\ntemporarily substitute it for testing. Designed as a drop-in\nreplacement for the now deprecated 'testthat::with_mock()' and\n'testthat::local_mock()'. get_mock_env local_mock with_mock "
  },
  {
    "id": 830,
    "package_name": "modeldatatoo",
    "title": "More Data Sets Useful for Modeling Examples",
    "description": "More data sets used for demonstrating or testing\nmodel-related packages are contained in this package. The data\nsets are downloaded and cached, allowing for more and bigger\ndata sets.",
    "version": "0.3.0.9000",
    "maintainer": "Emil Hvitfeldt <emil.hvitfeldt@posit.co>",
    "author": "Emil Hvitfeldt [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-0679-1945>),\nPosit Software, PBC [cph, fnd]",
    "url": "https://github.com/tidymodels/modeldatatoo,\nhttps://modeldatatoo.tidymodels.org/",
    "bug_reports": "https://github.com/tidymodels/modeldatatoo/issues",
    "repository": "",
    "exports": [
      [
        "attach_small_fine_foods"
      ],
      [
        "data_animals"
      ],
      [
        "data_building_complaints"
      ],
      [
        "data_chimiometrie_2019"
      ],
      [
        "data_detectors"
      ],
      [
        "data_elevators"
      ],
      [
        "data_hotel_rates"
      ],
      [
        "data_pharma_bioreactors"
      ],
      [
        "data_taxi"
      ],
      [
        "internal_board"
      ]
    ],
    "topics": [],
    "score": 4.634,
    "stars": 7,
    "primary_category": "tidyverse",
    "source_universe": "tidymodels",
    "search_text": "modeldatatoo More Data Sets Useful for Modeling Examples More data sets used for demonstrating or testing\nmodel-related packages are contained in this package. The data\nsets are downloaded and cached, allowing for more and bigger\ndata sets. attach_small_fine_foods data_animals data_building_complaints data_chimiometrie_2019 data_detectors data_elevators data_hotel_rates data_pharma_bioreactors data_taxi internal_board "
  },
  {
    "id": 1004,
    "package_name": "porcelain",
    "title": "Turn a Package into an HTTP API",
    "description": "Wrapper around the plumber package to turn a package into\nan HTTP API. This adds some conventions that we find useful,\nsuch as some testing infrastructure and automatic validation of\nresponses against a json schema.",
    "version": "0.1.16",
    "maintainer": "Rich FitzJohn <rich.fitzjohn@gmail.com>",
    "author": "Rich FitzJohn [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "https://github.com/reside-ic/porcelain",
    "bug_reports": "https://github.com/reside-ic/porcelain/issues",
    "repository": "",
    "exports": [
      [
        "porcelain"
      ],
      [
        "porcelain_add_headers"
      ],
      [
        "porcelain_background"
      ],
      [
        "porcelain_endpoint"
      ],
      [
        "porcelain_input_body_binary"
      ],
      [
        "porcelain_input_body_json"
      ],
      [
        "porcelain_input_query"
      ],
      [
        "porcelain_logger"
      ],
      [
        "porcelain_package_endpoint"
      ],
      [
        "porcelain_returning"
      ],
      [
        "porcelain_returning_binary"
      ],
      [
        "porcelain_returning_json"
      ],
      [
        "porcelain_returning_text"
      ],
      [
        "porcelain_roclet"
      ],
      [
        "porcelain_state"
      ],
      [
        "porcelain_stop"
      ]
    ],
    "topics": [],
    "score": 4.5563,
    "stars": 4,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "porcelain Turn a Package into an HTTP API Wrapper around the plumber package to turn a package into\nan HTTP API. This adds some conventions that we find useful,\nsuch as some testing infrastructure and automatic validation of\nresponses against a json schema. porcelain porcelain_add_headers porcelain_background porcelain_endpoint porcelain_input_body_binary porcelain_input_body_json porcelain_input_query porcelain_logger porcelain_package_endpoint porcelain_returning porcelain_returning_binary porcelain_returning_json porcelain_returning_text porcelain_roclet porcelain_state porcelain_stop "
  },
  {
    "id": 485,
    "package_name": "distrTEst",
    "title": "Estimation and Testing Classes Based on Package 'distr'",
    "description": "Evaluation (S4-)classes based on package distr for\nevaluating procedures (estimators/tests) at data/simulation in\na unified way.",
    "version": "2.8.3",
    "maintainer": "Peter Ruckdeschel <peter.ruckdeschel@uni-oldenburg.de>",
    "author": "Florian Camphausen [ctb] (contributed as student in the initial phase\n--2005),\nMatthias Kohl [aut, cph],\nPeter Ruckdeschel [cre, cph],\nThomas Stabla [ctb] (contributed as student in the initial phase\n--2005)",
    "url": "http://distr.r-forge.r-project.org/",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "call.ev"
      ],
      [
        "Data"
      ],
      [
        "distrTEstoptions"
      ],
      [
        "Elist"
      ],
      [
        "estimator"
      ],
      [
        "evaluate"
      ],
      [
        "EvaluationList"
      ],
      [
        "filename"
      ],
      [
        "getdistrTEstOption"
      ],
      [
        "initialize"
      ],
      [
        "name"
      ],
      [
        "plot"
      ],
      [
        "print"
      ],
      [
        "result"
      ],
      [
        "savedata"
      ],
      [
        "show"
      ],
      [
        "summary"
      ]
    ],
    "topics": [],
    "score": 3.3802,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "distrTEst Estimation and Testing Classes Based on Package 'distr' Evaluation (S4-)classes based on package distr for\nevaluating procedures (estimators/tests) at data/simulation in\na unified way. call.ev Data distrTEstoptions Elist estimator evaluate EvaluationList filename getdistrTEstOption initialize name plot print result savedata show summary "
  },
  {
    "id": 1319,
    "package_name": "testthat.buildkite",
    "title": "A testthat reporter for buildkite",
    "description": "A testthat reporter that prints progress output in a\nformat for use in buildkite logs.",
    "version": "0.0.1",
    "maintainer": "Robert Ashton <r.ashton@imperial.ac.uk>",
    "author": "Robert Ashton [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "BuildkiteReporter"
      ]
    ],
    "topics": [],
    "score": 2.5441,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "testthat.buildkite A testthat reporter for buildkite A testthat reporter that prints progress output in a\nformat for use in buildkite logs. BuildkiteReporter "
  },
  {
    "id": 560,
    "package_name": "fakerbase",
    "title": "Fake Database Tables For Unit Testing",
    "description": "Connect to a Postgres database and generate a suite of\nfunctions that can be used to mock database tables for unit\ntesting.",
    "version": "0.0.1",
    "maintainer": "Alex Hill <alex.hill@gmail.com>",
    "author": "Alex Hill [aut, cre],\nImperial College of Science, Technology and Medicine [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "fb_generate"
      ],
      [
        "fb_load"
      ]
    ],
    "topics": [],
    "score": 2,
    "stars": 0,
    "primary_category": "epidemiology",
    "source_universe": "mrc-ide",
    "search_text": "fakerbase Fake Database Tables For Unit Testing Connect to a Postgres database and generate a suite of\nfunctions that can be used to mock database tables for unit\ntesting. fb_generate fb_load "
  },
  {
    "id": 52,
    "package_name": "FAfA",
    "title": "Factor Analysis for All",
    "description": "Provides a comprehensive Shiny-based graphical user interface\n    for conducting a wide range of factor analysis procedures. 'FAfA'\n    (Factor Analysis for All) guides users through data uploading,\n    assumption checking (descriptives, collinearity, multivariate\n    normality, outliers), data wrangling (variable exclusion, data\n    splitting), factor retention analysis (e.g., Parallel Analysis, Hull\n    method, EGA), Exploratory Factor Analysis (EFA) with various rotation\n    and extraction methods, Confirmatory Factor Analysis (CFA) for model\n    testing, Reliability Analysis (e.g., Cronbach's Alpha, McDonald's\n    Omega), Measurement Invariance testing across groups, and item\n    weighting techniques. The application leverages established R packages\n    such as 'lavaan' and 'psych' to perform these analyses, offering an\n    accessible platform for researchers and students. Results are\n    presented in user-friendly tables and plots, with options for\n    downloading outputs.",
    "version": "0.5",
    "maintainer": "Abdullah Faruk KILIC <afarukkilic@trakya.edu.tr>",
    "author": "Abdullah Faruk KILIC [aut, cre],\n  Ahmet Caliskan [aut]",
    "url": "https://github.com/AFarukKILIC/FAfA",
    "bug_reports": "https://github.com/AFarukKILIC/FAfA/issues",
    "repository": "https://cran.r-project.org/package=FAfA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FAfA Factor Analysis for All Provides a comprehensive Shiny-based graphical user interface\n    for conducting a wide range of factor analysis procedures. 'FAfA'\n    (Factor Analysis for All) guides users through data uploading,\n    assumption checking (descriptives, collinearity, multivariate\n    normality, outliers), data wrangling (variable exclusion, data\n    splitting), factor retention analysis (e.g., Parallel Analysis, Hull\n    method, EGA), Exploratory Factor Analysis (EFA) with various rotation\n    and extraction methods, Confirmatory Factor Analysis (CFA) for model\n    testing, Reliability Analysis (e.g., Cronbach's Alpha, McDonald's\n    Omega), Measurement Invariance testing across groups, and item\n    weighting techniques. The application leverages established R packages\n    such as 'lavaan' and 'psych' to perform these analyses, offering an\n    accessible platform for researchers and students. Results are\n    presented in user-friendly tables and plots, with options for\n    downloading outputs.  "
  },
  {
    "id": 107,
    "package_name": "PMwR",
    "title": "Portfolio Management with R",
    "description": "Tools for the practical management of financial\n  portfolios: backtesting investment and trading strategies,\n  computing profit/loss and returns, analysing trades,\n  handling lists of transactions, reporting, and more.  The\n  package provides a small set of reliable, efficient and\n  convenient tools for processing and analysing\n  trade/portfolio data.  The manual provides all the details;\n  it is available from\n  <https://enricoschumann.net/R/packages/PMwR/manual/PMwR.html>.\n  Examples and descriptions of new features are provided at\n  <https://enricoschumann.net/notes/PMwR/>.",
    "version": "1.2-0",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "author": "Enrico Schumann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7601-6576>)",
    "url": "https://enricoschumann.net/PMwR/ ,\nhttps://git.sr.ht/~enricoschumann/PMwR ,\nhttps://gitlab.com/enricoschumann/PMwR ,\nhttps://github.com/enricoschumann/PMwR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PMwR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PMwR Portfolio Management with R Tools for the practical management of financial\n  portfolios: backtesting investment and trading strategies,\n  computing profit/loss and returns, analysing trades,\n  handling lists of transactions, reporting, and more.  The\n  package provides a small set of reliable, efficient and\n  convenient tools for processing and analysing\n  trade/portfolio data.  The manual provides all the details;\n  it is available from\n  <https://enricoschumann.net/R/packages/PMwR/manual/PMwR.html>.\n  Examples and descriptions of new features are provided at\n  <https://enricoschumann.net/notes/PMwR/>.  "
  },
  {
    "id": 220,
    "package_name": "XYomics",
    "title": "Analysis of Sex Differences in Omics Data for Complex Diseases",
    "description": "Tools to analyze sex differences in omics data for complex diseases. It includes functions for differential expression analysis using the 'limma' method <doi:10.1093/nar/gkv007>, interaction testing between sex and disease, pathway enrichment with 'clusterProfiler' <doi:10.1089/omi.2011.0118>, and gene regulatory network (GRN) construction and analysis using 'igraph'. The package enables a reproducible workflow from raw data processing to biological interpretation. ",
    "version": "0.1.2",
    "maintainer": "Enrico Glaab <enrico.glaab@uni.lu>",
    "author": "Enrico Glaab [aut, cre],\n  Sophie Le Bars [aut],\n  Mohamed Soudy [aut],\n  Murodzhon Akhmedov [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=XYomics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "XYomics Analysis of Sex Differences in Omics Data for Complex Diseases Tools to analyze sex differences in omics data for complex diseases. It includes functions for differential expression analysis using the 'limma' method <doi:10.1093/nar/gkv007>, interaction testing between sex and disease, pathway enrichment with 'clusterProfiler' <doi:10.1089/omi.2011.0118>, and gene regulatory network (GRN) construction and analysis using 'igraph'. The package enables a reproducible workflow from raw data processing to biological interpretation.   "
  },
  {
    "id": 281,
    "package_name": "bayestransmission",
    "title": "Bayesian Transmission Models",
    "description": "Provides Bayesian inference methods for infectious disease transmission \n    models in healthcare settings. Implements Markov Chain Monte Carlo (MCMC) algorithms \n    for estimating transmission parameters from patient-level data including admission, \n    discharge, and testing events as described in Thomas 'et al.' (2015) <doi:10.1093/imammb/dqt021>.",
    "version": "0.1.0",
    "maintainer": "Andrew Redd <andrew.redd@hsc.utah.edu>",
    "author": "Andrew Redd [aut, cre] (ORCID: <https://orcid.org/0000-0002-6149-2438>),\n  Alun Thomas [aut],\n  University of Utah [cph],\n  CDC's Center for Forecasting and Outbreak Analytics [fnd] (This project\n    was made possible by cooperative agreement CDC-RFA-FT-23-0069\n    (grant # NU38FT000009-01-00) from the CDC's Center for Forecasting\n    and Outbreak Analytics. Its contents are solely the responsibility\n    of the authors and do not necessarily represent the official views\n    of the Centers for Disease Control and Prevention.)",
    "url": "https://epiforesite.github.io/bayestransmission/,\nhttps://github.com/EpiForeSITE/bayestransmission",
    "bug_reports": "https://github.com/EpiForeSITE/bayestransmission/issues",
    "repository": "https://cran.r-project.org/package=bayestransmission",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bayestransmission Bayesian Transmission Models Provides Bayesian inference methods for infectious disease transmission \n    models in healthcare settings. Implements Markov Chain Monte Carlo (MCMC) algorithms \n    for estimating transmission parameters from patient-level data including admission, \n    discharge, and testing events as described in Thomas 'et al.' (2015) <doi:10.1093/imammb/dqt021>.  "
  },
  {
    "id": 340,
    "package_name": "cardinalR",
    "title": "Collection of Data Structures",
    "description": "A collection of functions to generate a large variety of\n    structures in high dimensions. These data structures are useful for\n    testing, validating, and improving algorithms used in dimensionality\n    reduction, clustering, machine learning, and visualization.",
    "version": "1.0.4",
    "maintainer": "Jayani P. Gamage <jayanilakshika76@gmail.com>",
    "author": "Jayani P. Gamage [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6265-6481>),\n  Dianne Cook [aut] (ORCID: <https://orcid.org/0000-0002-3813-7155>),\n  Paul Harrison [aut] (ORCID: <https://orcid.org/0000-0002-3980-268X>),\n  Michael Lydeamore [aut] (ORCID:\n    <https://orcid.org/0000-0001-6515-827X>),\n  Thiyanga S. Talagala [aut] (ORCID:\n    <https://orcid.org/0000-0002-0656-9789>)",
    "url": "https://jayanilakshika.github.io/cardinalR/",
    "bug_reports": "https://github.com/JayaniLakshika/cardinalR/issues",
    "repository": "https://cran.r-project.org/package=cardinalR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cardinalR Collection of Data Structures A collection of functions to generate a large variety of\n    structures in high dimensions. These data structures are useful for\n    testing, validating, and improving algorithms used in dimensionality\n    reduction, clustering, machine learning, and visualization.  "
  },
  {
    "id": 429,
    "package_name": "csmpv",
    "title": "Biomarker Confirmation, Selection, Modelling, Prediction, and\nValidation",
    "description": "\n   There are diverse purposes such as biomarker confirmation, novel biomarker discovery, constructing predictive models, model-based prediction, and validation. \n   It handles binary, continuous, and time-to-event outcomes at the sample or patient level.\n   - Biomarker confirmation utilizes established functions like glm() from 'stats', coxph() from 'survival', surv_fit(), and ggsurvplot() from 'survminer'.\n   - Biomarker discovery and variable selection are facilitated by three LASSO-related functions LASSO2(), LASSO_plus(), and LASSO2plus(), leveraging the 'glmnet' R package with additional steps.\n   - Eight versatile modeling functions are offered, each designed for predictive models across various outcomes and data types.\n     1) LASSO2(), LASSO_plus(), LASSO2plus(), and LASSO2_reg() perform variable selection using LASSO methods and construct predictive models based on selected variables.\n     2) XGBtraining() employs 'XGBoost' for model building and is the only function not involving variable selection.\n     3) Functions like LASSO2_XGBtraining(), LASSOplus_XGBtraining(), and LASSO2plus_XGBtraining() combine LASSO-related variable selection with 'XGBoost' for model construction.\n   - All models support prediction and validation, requiring a testing dataset comparable to the training dataset.\n   Additionally, the package introduces XGpred() for risk prediction based on survival data, with the XGpred_predict() function available for predicting risk groups in new datasets.\n   The methodology is based on our new algorithms and various references:\n   - Hastie et al. (1992, ISBN 0 534 16765-9), \n   - Therneau et al. (2000, ISBN 0-387-98784-3), \n   - Kassambara et al. (2021) <https://CRAN.R-project.org/package=survminer>,\n   - Friedman et al. (2010) <doi:10.18637/jss.v033.i01>,\n   - Simon et al. (2011) <doi:10.18637/jss.v039.i05>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=rms>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=Hmisc>,\n   - Chen and Guestrin (2016) <doi:10.48550/arXiv.1603.02754>,\n   - Aoki et al. (2023) <doi:10.1200/JCO.23.01115>.",
    "version": "1.0.5",
    "maintainer": "Aixiang Jiang <aijiang@bccrc.ca>",
    "author": "Aixiang Jiang [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6153-7595>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=csmpv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csmpv Biomarker Confirmation, Selection, Modelling, Prediction, and\nValidation \n   There are diverse purposes such as biomarker confirmation, novel biomarker discovery, constructing predictive models, model-based prediction, and validation. \n   It handles binary, continuous, and time-to-event outcomes at the sample or patient level.\n   - Biomarker confirmation utilizes established functions like glm() from 'stats', coxph() from 'survival', surv_fit(), and ggsurvplot() from 'survminer'.\n   - Biomarker discovery and variable selection are facilitated by three LASSO-related functions LASSO2(), LASSO_plus(), and LASSO2plus(), leveraging the 'glmnet' R package with additional steps.\n   - Eight versatile modeling functions are offered, each designed for predictive models across various outcomes and data types.\n     1) LASSO2(), LASSO_plus(), LASSO2plus(), and LASSO2_reg() perform variable selection using LASSO methods and construct predictive models based on selected variables.\n     2) XGBtraining() employs 'XGBoost' for model building and is the only function not involving variable selection.\n     3) Functions like LASSO2_XGBtraining(), LASSOplus_XGBtraining(), and LASSO2plus_XGBtraining() combine LASSO-related variable selection with 'XGBoost' for model construction.\n   - All models support prediction and validation, requiring a testing dataset comparable to the training dataset.\n   Additionally, the package introduces XGpred() for risk prediction based on survival data, with the XGpred_predict() function available for predicting risk groups in new datasets.\n   The methodology is based on our new algorithms and various references:\n   - Hastie et al. (1992, ISBN 0 534 16765-9), \n   - Therneau et al. (2000, ISBN 0-387-98784-3), \n   - Kassambara et al. (2021) <https://CRAN.R-project.org/package=survminer>,\n   - Friedman et al. (2010) <doi:10.18637/jss.v033.i01>,\n   - Simon et al. (2011) <doi:10.18637/jss.v039.i05>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=rms>,\n   - Harrell (2023) <https://CRAN.R-project.org/package=Hmisc>,\n   - Chen and Guestrin (2016) <doi:10.48550/arXiv.1603.02754>,\n   - Aoki et al. (2023) <doi:10.1200/JCO.23.01115>.  "
  },
  {
    "id": 469,
    "package_name": "did",
    "title": "Treatment Effects with Multiple Periods and Groups",
    "description": "The standard Difference-in-Differences (DID) setup involves two periods and two groups -- a treated group and untreated group.  Many applications of DID methods involve more than two periods and have individuals that are treated at different points in time.  This package contains tools for computing average treatment effect parameters in Difference in Differences setups with more than two periods and with variation in treatment timing using the methods developed in Callaway and Sant'Anna (2021) <doi:10.1016/j.jeconom.2020.12.001>.  The main parameters are group-time average treatment effects which are the average treatment effect for a particular group at a a particular time.  These can be aggregated into a fewer number of treatment effect parameters, and the package deals with the cases where there is selective treatment timing, dynamic treatment effects, calendar time effects, or combinations of these.  There are also functions for testing the Difference in Differences assumption, and plotting group-time average treatment effects.",
    "version": "2.3.0",
    "maintainer": "Brantly Callaway <brantly.callaway@uga.edu>",
    "author": "Brantly Callaway [aut, cre],\n  Pedro H. C. Sant'Anna [aut]",
    "url": "https://bcallaway11.github.io/did/,\nhttps://github.com/bcallaway11/did/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=did",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "did Treatment Effects with Multiple Periods and Groups The standard Difference-in-Differences (DID) setup involves two periods and two groups -- a treated group and untreated group.  Many applications of DID methods involve more than two periods and have individuals that are treated at different points in time.  This package contains tools for computing average treatment effect parameters in Difference in Differences setups with more than two periods and with variation in treatment timing using the methods developed in Callaway and Sant'Anna (2021) <doi:10.1016/j.jeconom.2020.12.001>.  The main parameters are group-time average treatment effects which are the average treatment effect for a particular group at a a particular time.  These can be aggregated into a fewer number of treatment effect parameters, and the package deals with the cases where there is selective treatment timing, dynamic treatment effects, calendar time effects, or combinations of these.  There are also functions for testing the Difference in Differences assumption, and plotting group-time average treatment effects.  "
  },
  {
    "id": 538,
    "package_name": "ergm.sign",
    "title": "Exponential-Family Models for Signed Networks",
    "description": "Extends the 'ergm.multi' packages from the Statnet suite to fit (temporal) exponential-family random graph models for signed networks. The framework models positive and negative ties as interdependent, which allows estimation and testing of structural balance theory. The package also includes options for descriptive summaries, visualization, and simulation of signed networks. See Krivitsky, Koehly, and Marcum (2020) <doi:10.1007/s11336-020-09720-7> and Fritz, C., Mehrl, M., Thurner, P. W., & Kauermann, G. (2025) <doi:10.1017/pan.2024.21>.",
    "version": "0.1.1",
    "maintainer": "Marc Schalberger <m.schalberger@fu-berlin.de>",
    "author": "Marc Schalberger [cre],\n  Cornelius Fritz [aut],\n  Pavel Krivitsky [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ergm.sign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ergm.sign Exponential-Family Models for Signed Networks Extends the 'ergm.multi' packages from the Statnet suite to fit (temporal) exponential-family random graph models for signed networks. The framework models positive and negative ties as interdependent, which allows estimation and testing of structural balance theory. The package also includes options for descriptive summaries, visualization, and simulation of signed networks. See Krivitsky, Koehly, and Marcum (2020) <doi:10.1007/s11336-020-09720-7> and Fritz, C., Mehrl, M., Thurner, P. W., & Kauermann, G. (2025) <doi:10.1017/pan.2024.21>.  "
  },
  {
    "id": 698,
    "package_name": "hypothesize",
    "title": "A Consistent API for Hypothesis Testing",
    "description": "Provides a consistent API for hypothesis testing built on\n    principles from 'Structure and Interpretation of Computer Programs':\n    data abstraction, closure (combining tests yields tests), and\n    higher-order functions (transforming tests). Implements z-tests,\n    Wald tests, likelihood ratio tests, Fisher's method for combining\n    p-values, and multiple testing corrections. Designed for use by\n    other packages that want to wrap their hypothesis tests in a\n    consistent interface.",
    "version": "0.10.0",
    "maintainer": "Alexander Towell <lex@metafunctor.com>",
    "author": "Alexander Towell [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6443-9897>)",
    "url": "https://github.com/queelius/hypothesize,\nhttps://queelius.github.io/hypothesize/",
    "bug_reports": "https://github.com/queelius/hypothesize/issues",
    "repository": "https://cran.r-project.org/package=hypothesize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hypothesize A Consistent API for Hypothesis Testing Provides a consistent API for hypothesis testing built on\n    principles from 'Structure and Interpretation of Computer Programs':\n    data abstraction, closure (combining tests yields tests), and\n    higher-order functions (transforming tests). Implements z-tests,\n    Wald tests, likelihood ratio tests, Fisher's method for combining\n    p-values, and multiple testing corrections. Designed for use by\n    other packages that want to wrap their hypothesis tests in a\n    consistent interface.  "
  },
  {
    "id": 699,
    "package_name": "iccCompare",
    "title": "Comparison of Dependent Intraclass Correlation Coefficients",
    "description": "Provides methods for testing the equality of dependent intraclass correlation coefficients (ICCs) estimated using linear mixed-effects models. Several of the implemented approaches are based on the work of Donner and Zou (2002) <doi:10.1111/1467-9884.00324>.",
    "version": "1.1.0",
    "maintainer": "Josep Lluis Carrasco <jlcarrasco@ub.edu>",
    "author": "Josep Lluis Carrasco [aut, cre],\n  Gonzalo Peon Pena [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iccCompare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iccCompare Comparison of Dependent Intraclass Correlation Coefficients Provides methods for testing the equality of dependent intraclass correlation coefficients (ICCs) estimated using linear mixed-effects models. Several of the implemented approaches are based on the work of Donner and Zou (2002) <doi:10.1111/1467-9884.00324>.  "
  },
  {
    "id": 838,
    "package_name": "modernBoot",
    "title": "Modern Resampling Methods: Bootstraps, Wild, Block, Permutation,\nand Selection Guidance",
    "description": "Implements modern resampling and permutation methods for robust \n    statistical inference without restrictive parametric assumptions. Provides \n    bias-corrected and accelerated (BCa) bootstrap (Efron and Tibshirani (1993) \n    <doi:10.1201/9780429246593>), wild bootstrap for heteroscedastic regression \n    (Liu (1988) <doi:10.1214/aos/1176351062>, Davidson and Flachaire (2008) \n    <doi:10.1016/j.jeconom.2008.08.003>), block bootstrap for time series \n    (Politis and Romano (1994) <doi:10.1080/01621459.1994.10476870>), and \n    permutation-based multiple testing correction (Westfall and Young (1993) \n    <ISBN:0-471-55761-7>). Methods handle non-normal data, \n    heteroscedasticity, time series correlation, and multiple comparisons.",
    "version": "0.1.1",
    "maintainer": "Ibrahim Kholil Rakib <ikrakib1010@gmail.com>",
    "author": "Ibrahim Kholil Rakib [aut, cre]",
    "url": "https://github.com/ikrakib/modernBoot",
    "bug_reports": "https://github.com/ikrakib/modernBoot/issues",
    "repository": "https://cran.r-project.org/package=modernBoot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modernBoot Modern Resampling Methods: Bootstraps, Wild, Block, Permutation,\nand Selection Guidance Implements modern resampling and permutation methods for robust \n    statistical inference without restrictive parametric assumptions. Provides \n    bias-corrected and accelerated (BCa) bootstrap (Efron and Tibshirani (1993) \n    <doi:10.1201/9780429246593>), wild bootstrap for heteroscedastic regression \n    (Liu (1988) <doi:10.1214/aos/1176351062>, Davidson and Flachaire (2008) \n    <doi:10.1016/j.jeconom.2008.08.003>), block bootstrap for time series \n    (Politis and Romano (1994) <doi:10.1080/01621459.1994.10476870>), and \n    permutation-based multiple testing correction (Westfall and Young (1993) \n    <ISBN:0-471-55761-7>). Methods handle non-normal data, \n    heteroscedasticity, time series correlation, and multiple comparisons.  "
  },
  {
    "id": 1137,
    "package_name": "rpact",
    "title": "Confirmatory Adaptive Clinical Trial Design and Analysis",
    "description": "Design and analysis of confirmatory adaptive clinical trials with continuous, binary, and survival endpoints according to the methods described in the monograph by Wassmer and Brannath (2025) <doi:10.1007/978-3-031-89669-9>. This includes classical group sequential as well as multi-stage adaptive hypotheses tests that are based on the combination testing principle.",
    "version": "4.3.0",
    "maintainer": "Friedrich Pahlke <friedrich.pahlke@rpact.com>",
    "author": "Gernot Wassmer [aut] (ORCID: <https://orcid.org/0000-0001-9397-1794>),\n  Friedrich Pahlke [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2105-2582>),\n  Till Jensen [ctb],\n  Daniel Sabanes Bove [ctb] (ORCID:\n    <https://orcid.org/0000-0002-0176-9239>),\n  Stephen Schueuerhuis [ctb] (ORCID:\n    <https://orcid.org/0009-0006-3124-6056>),\n  Tobias Muetze [ctb] (ORCID: <https://orcid.org/0000-0002-4111-1941>)",
    "url": "https://www.rpact.org, https://www.rpact.com,\nhttps://docs.rpact.org, https://github.com/rpact-com/rpact,\nhttps://rpact.shinyapps.io/connect",
    "bug_reports": "https://github.com/rpact-com/rpact/issues",
    "repository": "https://cran.r-project.org/package=rpact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rpact Confirmatory Adaptive Clinical Trial Design and Analysis Design and analysis of confirmatory adaptive clinical trials with continuous, binary, and survival endpoints according to the methods described in the monograph by Wassmer and Brannath (2025) <doi:10.1007/978-3-031-89669-9>. This includes classical group sequential as well as multi-stage adaptive hypotheses tests that are based on the combination testing principle.  "
  },
  {
    "id": 1313,
    "package_name": "tepr",
    "title": "Transcription Elongation Profiling",
    "description": "\n    The general principle relies on calculating the cumulative signal of nascent\n    RNA sequencing over the gene body of any given gene or transcription unit.\n    'tepr' can identify transcription attenuation sites by comparing profile to\n    a null model which assumes uniform read density over the entirety of the\n    transcription unit. It can also identify increased or diminished\n    transcription attenuation by comparing two conditions. Besides rigorous\n    statistical testing and high sensitivity, a major feature of 'tepr' is its\n    ability to provide the elongation pattern of each individual gene, including\n    the position of the main attenuation point when such a phenomenon occurs.\n    Using 'tepr', users can visualize and refine genome-wide aggregated analyses\n    of elongation patterns to robustly identify effects specific to subsets of\n    genes. These metrics are suitable for internal comparisons (between genes in\n    each condition) and for studying elongation of the same gene in different\n    conditions or comparing it to a perfect theoretical uniform elongation. ",
    "version": "1.1.13",
    "maintainer": "Nicolas Descostes <nicolas.descostes@gmail.com>",
    "author": "Nicolas Descostes [cre],\n  Victor Billon [aut],\n  Gael Cristofari [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tepr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tepr Transcription Elongation Profiling \n    The general principle relies on calculating the cumulative signal of nascent\n    RNA sequencing over the gene body of any given gene or transcription unit.\n    'tepr' can identify transcription attenuation sites by comparing profile to\n    a null model which assumes uniform read density over the entirety of the\n    transcription unit. It can also identify increased or diminished\n    transcription attenuation by comparing two conditions. Besides rigorous\n    statistical testing and high sensitivity, a major feature of 'tepr' is its\n    ability to provide the elongation pattern of each individual gene, including\n    the position of the main attenuation point when such a phenomenon occurs.\n    Using 'tepr', users can visualize and refine genome-wide aggregated analyses\n    of elongation patterns to robustly identify effects specific to subsets of\n    genes. These metrics are suitable for internal comparisons (between genes in\n    each condition) and for studying elongation of the same gene in different\n    conditions or comparing it to a perfect theoretical uniform elongation.   "
  },
  {
    "id": 1439,
    "package_name": "weightedGCM",
    "title": "Weighted Generalised Covariance Measure Conditional Independence\nTest",
    "description": "A conditional independence test that can be applied both to\n    univariate and multivariate random variables. The test is based on a\n    weighted form of the sample covariance of the residuals after a\n    nonlinear regression on the conditioning variables. Details are\n    described in Scheidegger, Hoerrmann and Buehlmann (2022) \"The Weighted\n    Generalised Covariance Measure\" <http://jmlr.org/papers/v23/21-1328.html>.\n    The test is a generalisation of the Generalised Covariance Measure (GCM)\n    implemented in the R package 'GeneralisedCovarianceMeasure' by Jonas Peters and\n    Rajen D. Shah based on Shah and Peters (2020) \"The Hardness of\n    Conditional Independence Testing and the Generalised Covariance\n    Measure\" <doi:10.1214/19-AOS1857>.",
    "version": "0.1.1",
    "maintainer": "Cyrill Scheidegger <cyrill.scheidegger@stat.math.ethz.ch>",
    "author": "Cyrill Scheidegger [aut, cre, cph],\n  Julia Hoerrmann [ths],\n  Peter Buehlmann [ths],\n  Jonas Peters [ctb, cph] (Parts of the code are inspired by similar\n    functions from the R package 'GeneralisedCovarianceMeasure' by\n    Jonas Peters and Rajen D. Shah),\n  Rajen D. Shah [ctb, cph] (Parts of the code are inspired by similar\n    functions from the R package 'GeneralisedCovarianceMeasure' by\n    Jonas Peters and Rajen D. Shah)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=weightedGCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "weightedGCM Weighted Generalised Covariance Measure Conditional Independence\nTest A conditional independence test that can be applied both to\n    univariate and multivariate random variables. The test is based on a\n    weighted form of the sample covariance of the residuals after a\n    nonlinear regression on the conditioning variables. Details are\n    described in Scheidegger, Hoerrmann and Buehlmann (2022) \"The Weighted\n    Generalised Covariance Measure\" <http://jmlr.org/papers/v23/21-1328.html>.\n    The test is a generalisation of the Generalised Covariance Measure (GCM)\n    implemented in the R package 'GeneralisedCovarianceMeasure' by Jonas Peters and\n    Rajen D. Shah based on Shah and Peters (2020) \"The Hardness of\n    Conditional Independence Testing and the Generalised Covariance\n    Measure\" <doi:10.1214/19-AOS1857>.  "
  },
  {
    "id": 1501,
    "package_name": "ACV",
    "title": "Optimal Out-of-Sample Forecast Evaluation and Testing under\nStationarity",
    "description": "Package 'ACV' (short for Affine Cross-Validation) offers an improved time-series cross-validation loss estimator which utilizes both in-sample and out-of-sample forecasting performance via a carefully constructed affine weighting scheme. Under the assumption of stationarity, the estimator is the best linear unbiased estimator of the out-of-sample loss. Besides that, the package also offers improved versions of Diebold-Mariano and Ibragimov-Muller tests of equal predictive ability which deliver more power relative to their conventional counterparts. For more information, see the accompanying article Stanek (2021) <doi:10.2139/ssrn.3996166>.",
    "version": "1.0.2",
    "maintainer": "Filip Stanek <stanek.fi@gmail.com>",
    "author": "Filip Stanek [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ACV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ACV Optimal Out-of-Sample Forecast Evaluation and Testing under\nStationarity Package 'ACV' (short for Affine Cross-Validation) offers an improved time-series cross-validation loss estimator which utilizes both in-sample and out-of-sample forecasting performance via a carefully constructed affine weighting scheme. Under the assumption of stationarity, the estimator is the best linear unbiased estimator of the out-of-sample loss. Besides that, the package also offers improved versions of Diebold-Mariano and Ibragimov-Muller tests of equal predictive ability which deliver more power relative to their conventional counterparts. For more information, see the accompanying article Stanek (2021) <doi:10.2139/ssrn.3996166>.  "
  },
  {
    "id": 1505,
    "package_name": "ADDT",
    "title": "Analysis of Accelerated Destructive Degradation Test Data",
    "description": "Accelerated destructive degradation tests (ADDT) are often used to collect necessary data for assessing the long-term properties of polymeric materials. Based on the collected data, a thermal index (TI) is estimated. The TI can be useful for material rating and comparison. This package implements the traditional method based on the least-squares method, the parametric method based on maximum likelihood estimation, and the semiparametric method based on spline methods, and the corresponding methods for estimating TI for polymeric materials. The traditional approach is a two-step approach that is currently used in industrial standards, while the parametric method is widely used in the statistical literature. The semiparametric method is newly developed. Both the parametric and semiparametric approaches allow one to do statistical inference such as quantifying uncertainties in estimation, hypothesis testing, and predictions. Publicly available datasets are provided illustrations. More details can be found in Jin et al. (2017).",
    "version": "2.0",
    "maintainer": "Yili Hong <yilihong@vt.edu>",
    "author": "Yili Hong, Yimeng Xie, Zhongnan Jin, and Caleb King",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ADDT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ADDT Analysis of Accelerated Destructive Degradation Test Data Accelerated destructive degradation tests (ADDT) are often used to collect necessary data for assessing the long-term properties of polymeric materials. Based on the collected data, a thermal index (TI) is estimated. The TI can be useful for material rating and comparison. This package implements the traditional method based on the least-squares method, the parametric method based on maximum likelihood estimation, and the semiparametric method based on spline methods, and the corresponding methods for estimating TI for polymeric materials. The traditional approach is a two-step approach that is currently used in industrial standards, while the parametric method is widely used in the statistical literature. The semiparametric method is newly developed. Both the parametric and semiparametric approaches allow one to do statistical inference such as quantifying uncertainties in estimation, hypothesis testing, and predictions. Publicly available datasets are provided illustrations. More details can be found in Jin et al. (2017).  "
  },
  {
    "id": 1511,
    "package_name": "ADPF",
    "title": "Use Least Squares Polynomial Regression and Statistical Testing\nto Improve Savitzky-Golay",
    "description": "This function takes a vector or matrix of data and smooths\n    the data with an improved Savitzky Golay transform. The Savitzky-Golay\n    method for data smoothing and differentiation calculates convolution\n    weights using Gram polynomials that exactly reproduce the results of\n    least-squares polynomial regression. Use of the Savitzky-Golay\n    method requires specification of both filter length and\n    polynomial degree to calculate convolution weights. For maximum\n    smoothing of statistical noise in data, polynomials with\n    low degrees are desirable, while a high polynomial degree\n    is necessary for accurate reproduction of peaks in the data.\n    Extension of the least-squares regression formalism with\n    statistical testing of additional terms of polynomial degree\n    to a heuristically chosen minimum for each data window leads\n    to an adaptive-degree polynomial filter (ADPF). Based on noise\n    reduction for data that consist of pure noise and on signal\n    reproduction for data that is purely signal, ADPF performed\n    nearly as well as the optimally chosen fixed-degree\n    Savitzky-Golay filter and outperformed sub-optimally chosen\n    Savitzky-Golay filters. For synthetic data consisting of noise\n    and signal, ADPF outperformed both optimally chosen and\n    sub-optimally chosen fixed-degree Savitzky-Golay filters. See Barak, P. (1995) <doi:10.1021/ac00113a006> for more information.",
    "version": "0.0.1",
    "maintainer": "Samuel Kruse <samdkruse@gmail.com>",
    "author": "Phillip Barak [aut],\n  Samuel Kruse [cre, aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ADPF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ADPF Use Least Squares Polynomial Regression and Statistical Testing\nto Improve Savitzky-Golay This function takes a vector or matrix of data and smooths\n    the data with an improved Savitzky Golay transform. The Savitzky-Golay\n    method for data smoothing and differentiation calculates convolution\n    weights using Gram polynomials that exactly reproduce the results of\n    least-squares polynomial regression. Use of the Savitzky-Golay\n    method requires specification of both filter length and\n    polynomial degree to calculate convolution weights. For maximum\n    smoothing of statistical noise in data, polynomials with\n    low degrees are desirable, while a high polynomial degree\n    is necessary for accurate reproduction of peaks in the data.\n    Extension of the least-squares regression formalism with\n    statistical testing of additional terms of polynomial degree\n    to a heuristically chosen minimum for each data window leads\n    to an adaptive-degree polynomial filter (ADPF). Based on noise\n    reduction for data that consist of pure noise and on signal\n    reproduction for data that is purely signal, ADPF performed\n    nearly as well as the optimally chosen fixed-degree\n    Savitzky-Golay filter and outperformed sub-optimally chosen\n    Savitzky-Golay filters. For synthetic data consisting of noise\n    and signal, ADPF outperformed both optimally chosen and\n    sub-optimally chosen fixed-degree Savitzky-Golay filters. See Barak, P. (1995) <doi:10.1021/ac00113a006> for more information.  "
  },
  {
    "id": 1512,
    "package_name": "ADPclust",
    "title": "Fast Clustering Using Adaptive Density Peak Detection",
    "description": "An implementation of ADPclust clustering procedures (Fast\n    Clustering Using Adaptive Density Peak Detection). The work is built and\n    improved upon the idea of Rodriguez and Laio (2014)<DOI:10.1126/science.1242072>. \n    ADPclust clusters data by finding density peaks in a density-distance plot \n    generated from local multivariate Gaussian density estimation. It includes \n    an automatic centroids selection and parameter optimization algorithm, which \n    finds the number of clusters and cluster centroids by comparing average \n    silhouettes on a grid of testing clustering results; It also includes a user \n    interactive algorithm that allows the user to manually selects cluster \n    centroids from a two dimensional \"density-distance plot\". Here is the \n    research article associated with this package: \"Wang, Xiao-Feng, and \n    Yifan Xu (2015)<DOI:10.1177/0962280215609948> Fast clustering using adaptive \n    density peak detection.\" Statistical methods in medical research\". url:\n    http://smm.sagepub.com/content/early/2015/10/15/0962280215609948.abstract. ",
    "version": "0.7",
    "maintainer": "Yifan (Ethan) Xu <ethan.yifanxu@gmail.com>",
    "author": "Yifan (Ethan) Xu [aut, cre],\n  Xiao-Feng Wang [aut]",
    "url": "https://github.com/ethanyxu/ADPclust",
    "bug_reports": "https://github.com/ethanyxu/ADPclust/issues",
    "repository": "https://cran.r-project.org/package=ADPclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ADPclust Fast Clustering Using Adaptive Density Peak Detection An implementation of ADPclust clustering procedures (Fast\n    Clustering Using Adaptive Density Peak Detection). The work is built and\n    improved upon the idea of Rodriguez and Laio (2014)<DOI:10.1126/science.1242072>. \n    ADPclust clusters data by finding density peaks in a density-distance plot \n    generated from local multivariate Gaussian density estimation. It includes \n    an automatic centroids selection and parameter optimization algorithm, which \n    finds the number of clusters and cluster centroids by comparing average \n    silhouettes on a grid of testing clustering results; It also includes a user \n    interactive algorithm that allows the user to manually selects cluster \n    centroids from a two dimensional \"density-distance plot\". Here is the \n    research article associated with this package: \"Wang, Xiao-Feng, and \n    Yifan Xu (2015)<DOI:10.1177/0962280215609948> Fast clustering using adaptive \n    density peak detection.\" Statistical methods in medical research\". url:\n    http://smm.sagepub.com/content/early/2015/10/15/0962280215609948.abstract.   "
  },
  {
    "id": 1545,
    "package_name": "ALTopt",
    "title": "Optimal Experimental Designs for Accelerated Life Testing",
    "description": "Creates the optimal (D, U and I) designs for the accelerated life\n    testing with right censoring or interval censoring. It uses generalized \n    linear model (GLM) approach to derive the asymptotic variance-covariance \n    matrix of regression coefficients. The failure time distribution is assumed \n    to follow Weibull distribution with a known shape parameter and log-linear \n    link functions are used to model the relationship between failure time \n    parameters and stress variables. The acceleration model may have multiple \n    stress factors, although most ALTs involve only two or less stress factors. \n    ALTopt package also provides several plotting functions including contour plot,\n    Fraction of Use Space (FUS) plot and Variance Dispersion graphs of Use Space\n    (VDUS) plot. For more details, see Seo and Pan (2015) <doi:10.32614/RJ-2015-029>.",
    "version": "0.1.2",
    "maintainer": "Kangwon Seo <seoka@missouri.edu>",
    "author": "Kangwon Seo [aut, cre],\n  Rong Pan [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ALTopt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ALTopt Optimal Experimental Designs for Accelerated Life Testing Creates the optimal (D, U and I) designs for the accelerated life\n    testing with right censoring or interval censoring. It uses generalized \n    linear model (GLM) approach to derive the asymptotic variance-covariance \n    matrix of regression coefficients. The failure time distribution is assumed \n    to follow Weibull distribution with a known shape parameter and log-linear \n    link functions are used to model the relationship between failure time \n    parameters and stress variables. The acceleration model may have multiple \n    stress factors, although most ALTs involve only two or less stress factors. \n    ALTopt package also provides several plotting functions including contour plot,\n    Fraction of Use Space (FUS) plot and Variance Dispersion graphs of Use Space\n    (VDUS) plot. For more details, see Seo and Pan (2015) <doi:10.32614/RJ-2015-029>.  "
  },
  {
    "id": 1571,
    "package_name": "APFr",
    "title": "Multiple Testing Approach using Average Power Function (APF) and\nBayes FDR Robust Estimation",
    "description": "Implements a multiple testing approach to the\n    choice of a threshold gamma on the p-values using the\n    Average Power Function (APF) and Bayes False Discovery\n    Rate (FDR) robust estimation. Function apf_fdr() \n    estimates both quantities from either raw data or\n    p-values. Function apf_plot() produces smooth graphs \n    and tables of the relevant results. Details of the methods\n    can be found in Quatto P, Margaritella N, et al. (2019) \n    <doi:10.1177/0962280219844288>.",
    "version": "1.0.2",
    "maintainer": "Nicol\u00f2 Margaritella <N.Margaritella@sms.ed.ac.uk>",
    "author": "Nicol\u00f2 Margaritella [cre, aut],\n  Piero Quatto [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=APFr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "APFr Multiple Testing Approach using Average Power Function (APF) and\nBayes FDR Robust Estimation Implements a multiple testing approach to the\n    choice of a threshold gamma on the p-values using the\n    Average Power Function (APF) and Bayes False Discovery\n    Rate (FDR) robust estimation. Function apf_fdr() \n    estimates both quantities from either raw data or\n    p-values. Function apf_plot() produces smooth graphs \n    and tables of the relevant results. Details of the methods\n    can be found in Quatto P, Margaritella N, et al. (2019) \n    <doi:10.1177/0962280219844288>.  "
  },
  {
    "id": 1590,
    "package_name": "ARIMAANN",
    "title": "Time Series Forecasting using ARIMA-ANN Hybrid Model",
    "description": "Testing, Implementation, and Forecasting of the ARIMA-ANN hybrid model. The ARIMA-ANN hybrid model combines the distinct strengths of the Auto-Regressive Integrated Moving Average (ARIMA) model and the Artificial Neural Network (ANN) model for time series forecasting.For method details see Zhang, GP (2003) <doi:10.1016/S0925-2312(01)00702-0>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "author": "Ramasubramanian V. [aut, ctb],\n  Mrinmoy Ray [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ARIMAANN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ARIMAANN Time Series Forecasting using ARIMA-ANN Hybrid Model Testing, Implementation, and Forecasting of the ARIMA-ANN hybrid model. The ARIMA-ANN hybrid model combines the distinct strengths of the Auto-Regressive Integrated Moving Average (ARIMA) model and the Artificial Neural Network (ANN) model for time series forecasting.For method details see Zhang, GP (2003) <doi:10.1016/S0925-2312(01)00702-0>.  "
  },
  {
    "id": 1615,
    "package_name": "AUtests",
    "title": "Approximate Unconditional and Permutation Tests",
    "description": "Performs approximate unconditional and permutation testing for\n    2x2 contingency tables. Motivated by testing for disease association with rare\n    genetic variants in case-control studies. When variants are extremely rare,\n    these tests give better control of Type I error than standard tests.",
    "version": "0.99",
    "maintainer": "Arjun Sondhi <asondhi@uw.edu>",
    "author": "Arjun Sondhi, Ken Rice",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AUtests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AUtests Approximate Unconditional and Permutation Tests Performs approximate unconditional and permutation testing for\n    2x2 contingency tables. Motivated by testing for disease association with rare\n    genetic variants in case-control studies. When variants are extremely rare,\n    these tests give better control of Type I error than standard tests.  "
  },
  {
    "id": 1643,
    "package_name": "AdaptiveBoxplot",
    "title": "FDR(BH) Boxplot and FWER(Holm) Boxplot",
    "description": "Implements a framework for creating boxplots where the \n    whisker lengths are determined by formal multiple testing procedures, \n    making them adaptive to sample size and data characteristics. The function bh_boxplot()\n    generates boxplots that control the False Discovery \n    Rate (FDR) via the Benjamini-Hochberg procedure, and the function holm_boxplot() generates\n    boxplots that control the Family-Wise Error Rate (FWER) via the Holm procedure. \n    The methods are based on the framework in Gang, Lin, and Tong (2025) \n    <doi:10.48550/arXiv.2510.20259>. ",
    "version": "0.1.1",
    "maintainer": "Bowen Gang <gangbowen02@gmail.com>",
    "author": "Bowen Gang [aut, cre],\n  Hongmei Lin [aut],\n  Tiejun Tong [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AdaptiveBoxplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AdaptiveBoxplot FDR(BH) Boxplot and FWER(Holm) Boxplot Implements a framework for creating boxplots where the \n    whisker lengths are determined by formal multiple testing procedures, \n    making them adaptive to sample size and data characteristics. The function bh_boxplot()\n    generates boxplots that control the False Discovery \n    Rate (FDR) via the Benjamini-Hochberg procedure, and the function holm_boxplot() generates\n    boxplots that control the Family-Wise Error Rate (FWER) via the Holm procedure. \n    The methods are based on the framework in Gang, Lin, and Tong (2025) \n    <doi:10.48550/arXiv.2510.20259>.   "
  },
  {
    "id": 1690,
    "package_name": "Analitica",
    "title": "Exploratory Data Analysis, Group Comparison Tools, and Other\nProcedures",
    "description": "Provides a comprehensive set of tools for descriptive statistics,\n    graphical data exploration, outlier detection, homoscedasticity testing, and\n    multiple comparison procedures. Includes manual implementations of Levene's test,\n    Bartlett's test, and the Fligner-Killeen test, as well as post hoc comparison\n    methods such as Tukey, Scheff\u00e9, Games-Howell, Brunner-Munzel, and others.\n    This version introduces two new procedures: the Jonckheere-Terpstra trend test\n    and the Jarque-Bera test with Glinskiy's (2024) correction. Designed for use in\n    teaching, applied statistical analysis, and reproducible research. \n    Additionally you can find a post hoc Test Planner, which helps you to make a \n    decision on which procedure is most suitable.",
    "version": "2.2.0",
    "maintainer": "Carlos Jim\u00e9nez-Gallardo <carlos.jimenez@ufrontera.cl>",
    "author": "Carlos Jim\u00e9nez-Gallardo [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Analitica",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Analitica Exploratory Data Analysis, Group Comparison Tools, and Other\nProcedures Provides a comprehensive set of tools for descriptive statistics,\n    graphical data exploration, outlier detection, homoscedasticity testing, and\n    multiple comparison procedures. Includes manual implementations of Levene's test,\n    Bartlett's test, and the Fligner-Killeen test, as well as post hoc comparison\n    methods such as Tukey, Scheff\u00e9, Games-Howell, Brunner-Munzel, and others.\n    This version introduces two new procedures: the Jonckheere-Terpstra trend test\n    and the Jarque-Bera test with Glinskiy's (2024) correction. Designed for use in\n    teaching, applied statistical analysis, and reproducible research. \n    Additionally you can find a post hoc Test Planner, which helps you to make a \n    decision on which procedure is most suitable.  "
  },
  {
    "id": 1710,
    "package_name": "AovBay",
    "title": "Classic, Nonparametric and Bayesian One-Way Analysis of Variance\nPanel",
    "description": "It covers various approaches to analysis of variance, provides an assumption testing section in order to provide a decision diagram that allows selecting the most appropriate technique. It provides the classical analysis of variance, the nonparametric equivalent of Kruskal Wallis, and the Bayesian approach. These results are shown in an interactive shiny panel, which allows modifying the arguments of the tests, contains interactive graphics and presents automatic conclusions depending on the tests in order to contribute to the interpretation of these analyzes. 'AovBay' uses 'Stan' and 'FactorBayes' for Bayesian analysis and 'Highcharts' for interactive charts.",
    "version": "0.1.0",
    "maintainer": "Mauricio Rojas-Campuzano <maujroja@espol.edu.ec>",
    "author": "Mauricio Rojas-Campuzano [aut, cre, ctb],\n  Johny Pambabay-Calero [aut, ctb],\n  Sergio Bauz-Olvera [ctb],\n  Omar Ruiz-Barzola [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AovBay",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AovBay Classic, Nonparametric and Bayesian One-Way Analysis of Variance\nPanel It covers various approaches to analysis of variance, provides an assumption testing section in order to provide a decision diagram that allows selecting the most appropriate technique. It provides the classical analysis of variance, the nonparametric equivalent of Kruskal Wallis, and the Bayesian approach. These results are shown in an interactive shiny panel, which allows modifying the arguments of the tests, contains interactive graphics and presents automatic conclusions depending on the tests in order to contribute to the interpretation of these analyzes. 'AovBay' uses 'Stan' and 'FactorBayes' for Bayesian analysis and 'Highcharts' for interactive charts.  "
  },
  {
    "id": 1749,
    "package_name": "AutoStepwiseGLM",
    "title": "Builds Stepwise GLMs via Train and Test Approach",
    "description": "Randomly splits data into testing and training sets. Then, uses stepwise selection to fit numerous multiple regression models on the training data, and tests them on the test data. Returned for each model are plots comparing model Akaike Information Criterion (AIC), Pearson correlation coefficient (r) between the predicted and actual values, Mean Absolute Error (MAE), and R-Squared among the models. Each model is ranked relative to the other models by the model evaluation metrics (i.e., AIC, r, MAE, and R-Squared) and the model with the best mean ranking among the model evaluation metrics is returned. Model evaluation metric weights for AIC, r, MAE, and R-Squared are taken in as arguments as aic_wt, r_wt, mae_wt, and r_squ_wt, respectively. They are equally weighted as default but may be adjusted relative to each other if the user prefers one or more metrics to the others, Field, A. (2013, ISBN:978-1-4462-4918-5).",
    "version": "0.2.0",
    "maintainer": "Aaron England <aaron.england24@gmail.com>",
    "author": "Aaron England <aaron.england24@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AutoStepwiseGLM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AutoStepwiseGLM Builds Stepwise GLMs via Train and Test Approach Randomly splits data into testing and training sets. Then, uses stepwise selection to fit numerous multiple regression models on the training data, and tests them on the test data. Returned for each model are plots comparing model Akaike Information Criterion (AIC), Pearson correlation coefficient (r) between the predicted and actual values, Mean Absolute Error (MAE), and R-Squared among the models. Each model is ranked relative to the other models by the model evaluation metrics (i.e., AIC, r, MAE, and R-Squared) and the model with the best mean ranking among the model evaluation metrics is returned. Model evaluation metric weights for AIC, r, MAE, and R-Squared are taken in as arguments as aic_wt, r_wt, mae_wt, and r_squ_wt, respectively. They are equally weighted as default but may be adjusted relative to each other if the user prefers one or more metrics to the others, Field, A. (2013, ISBN:978-1-4462-4918-5).  "
  },
  {
    "id": 1780,
    "package_name": "BANAM",
    "title": "Bayesian Analysis of the Network Autocorrelation Model",
    "description": "The network autocorrelation model (NAM) can be used for studying the degree of social influence \n    regarding an outcome variable based on one or more known networks. \n    The degree of social influence is quantified via the network autocorrelation parameters. In case of a single\n    network, the Bayesian methods of Dittrich, Leenders, and Mulder\n    (2017) <DOI:10.1016/j.socnet.2016.09.002> and Dittrich, Leenders, and Mulder (2019)\n    <DOI:10.1177/0049124117729712> are implemented using a normal, flat, or independence  \n    Jeffreys prior for the network autocorrelation. In the case of multiple \n    networks, the Bayesian methods of Dittrich, Leenders, and Mulder (2020) \n    <DOI:10.1177/0081175020913899> are implemented using a multivariate normal prior for \n    the network autocorrelation parameters. Flat priors are implemented \n    for estimating the coefficients. For Bayesian testing of equality and order-constrained \n    hypotheses, the default Bayes factor of Gu, Mulder, and Hoijtink, (2018) \n    <DOI:10.1111/bmsp.12110> is used with the posterior mean and posterior covariance \n    matrix of the NAM parameters based on flat priors as input.",
    "version": "0.2.2",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "author": "Joris Mulder [aut, cre],\n  Dino Dittrich [aut, ctb],\n  Roger Leenders [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BANAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BANAM Bayesian Analysis of the Network Autocorrelation Model The network autocorrelation model (NAM) can be used for studying the degree of social influence \n    regarding an outcome variable based on one or more known networks. \n    The degree of social influence is quantified via the network autocorrelation parameters. In case of a single\n    network, the Bayesian methods of Dittrich, Leenders, and Mulder\n    (2017) <DOI:10.1016/j.socnet.2016.09.002> and Dittrich, Leenders, and Mulder (2019)\n    <DOI:10.1177/0049124117729712> are implemented using a normal, flat, or independence  \n    Jeffreys prior for the network autocorrelation. In the case of multiple \n    networks, the Bayesian methods of Dittrich, Leenders, and Mulder (2020) \n    <DOI:10.1177/0081175020913899> are implemented using a multivariate normal prior for \n    the network autocorrelation parameters. Flat priors are implemented \n    for estimating the coefficients. For Bayesian testing of equality and order-constrained \n    hypotheses, the default Bayes factor of Gu, Mulder, and Hoijtink, (2018) \n    <DOI:10.1111/bmsp.12110> is used with the posterior mean and posterior covariance \n    matrix of the NAM parameters based on flat priors as input.  "
  },
  {
    "id": 1799,
    "package_name": "BCD",
    "title": "Bivariate Distributions via Conditional Specification",
    "description": "Implementation of bivariate binomial, geometric, and Poisson distributions based on conditional specifications. The package also includes tools for data generation and goodness-of-fit testing for these three distribution families. For methodological details, see Ghosh, Marques, and Chakraborty (2025) <doi:10.1080/03610926.2024.2315294>, Ghosh, Marques, and Chakraborty (2023) <doi:10.1080/03610918.2021.2004419>, and Ghosh, Marques, and Chakraborty (2021) <doi:10.1080/02664763.2020.1793307>.",
    "version": "0.1.1",
    "maintainer": "Mina Norouzirad <mina.norouzirad@gmail.com>",
    "author": "Mina Norouzirad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0311-6888>),\n  Filipe J. Marques [aut] (ORCID:\n    <https://orcid.org/0000-0001-6453-6558>),\n  Indranil Ghosh [ctb] (ORCID: <https://orcid.org/0000-0001-7910-8919>),\n  FCT, I.P. [fnd] (under the scope of the projects UIDB/00297/2020 and\n    UIDP/00297/2020 (NOVAMath))",
    "url": "https://github.com/mnrzrad/BCD",
    "bug_reports": "https://github.com/mnrzrad/BCD/issues",
    "repository": "https://cran.r-project.org/package=BCD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BCD Bivariate Distributions via Conditional Specification Implementation of bivariate binomial, geometric, and Poisson distributions based on conditional specifications. The package also includes tools for data generation and goodness-of-fit testing for these three distribution families. For methodological details, see Ghosh, Marques, and Chakraborty (2025) <doi:10.1080/03610926.2024.2315294>, Ghosh, Marques, and Chakraborty (2023) <doi:10.1080/03610918.2021.2004419>, and Ghosh, Marques, and Chakraborty (2021) <doi:10.1080/02664763.2020.1793307>.  "
  },
  {
    "id": 1822,
    "package_name": "BET",
    "title": "Binary Expansion Testing",
    "description": "Nonparametric detection of nonuniformity and dependence with Binary Expansion Testing (BET). See Kai Zhang (2019) BET on Independence, Journal of the American Statistical Association, 114:528, 1620-1637, <DOI:10.1080/01621459.2018.1537921>, Kai Zhang, Wan Zhang, Zhigen Zhao, Wen Zhou. (2023). BEAUTY Powered BEAST, <doi:10.48550/arXiv.2103.00674>  and Wan Zhang, Zhigen Zhao, Michael Baiocchi, Yao Li, Kai Zhang. (2023) SorBET: A Fast and Powerful Algorithm to Test Dependence of Variables, Techinical report.",
    "version": "0.5.4",
    "maintainer": "Wan Zhang <wanz63@live.unc.edu>",
    "author": "Wan Zhang [aut, cre],\n  Zhigen Zhao [aut],\n  Michael Baiocchi [aut],\n  Kai Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BET Binary Expansion Testing Nonparametric detection of nonuniformity and dependence with Binary Expansion Testing (BET). See Kai Zhang (2019) BET on Independence, Journal of the American Statistical Association, 114:528, 1620-1637, <DOI:10.1080/01621459.2018.1537921>, Kai Zhang, Wan Zhang, Zhigen Zhao, Wen Zhou. (2023). BEAUTY Powered BEAST, <doi:10.48550/arXiv.2103.00674>  and Wan Zhang, Zhigen Zhao, Michael Baiocchi, Yao Li, Kai Zhang. (2023) SorBET: A Fast and Powerful Algorithm to Test Dependence of Variables, Techinical report.  "
  },
  {
    "id": 1828,
    "package_name": "BFpack",
    "title": "Flexible Bayes Factor Testing of Scientific Expectations",
    "description": "Implementation of default Bayes factors\n    for testing statistical hypotheses under various statistical models. The package is\n    intended for applied quantitative researchers in the\n    social and behavioral sciences, medical research,\n    and related fields. The Bayes factor tests can be\n    executed for statistical models such as \n    univariate and multivariate normal linear models,\n    correlation analysis, generalized linear models, special cases of \n    linear mixed models, survival models, relational\n    event models. Parameters that can be tested are\n    location parameters (e.g., group means, regression coefficients),\n    variances (e.g., group variances), and measures of \n    association (e.g,. polychoric/polyserial/biserial/tetrachoric/product\n    moments correlations), among others.\n    The statistical underpinnings are\n    described in\n    O'Hagan (1995) <DOI:10.1111/j.2517-6161.1995.tb02017.x>,\n    De Santis and Spezzaferri (2001) <DOI:10.1016/S0378-3758(00)00240-8>,\n    Mulder and Xin (2022) <DOI:10.1080/00273171.2021.1904809>,\n    Mulder and Gelissen (2019) <DOI:10.1080/02664763.2021.1992360>,\n    Mulder (2016) <DOI:10.1016/j.jmp.2014.09.004>,\n    Mulder and Fox (2019) <DOI:10.1214/18-BA1115>,\n    Mulder and Fox (2013) <DOI:10.1007/s11222-011-9295-3>,\n    Boeing-Messing, van Assen, Hofman, Hoijtink, and Mulder (2017) <DOI:10.1037/met0000116>,\n    Hoijtink, Mulder, van Lissa, and Gu (2018) <DOI:10.1037/met0000201>,\n    Gu, Mulder, and Hoijtink (2018) <DOI:10.1111/bmsp.12110>,\n    Hoijtink, Gu, and Mulder (2018) <DOI:10.1111/bmsp.12145>, and\n    Hoijtink, Gu, Mulder, and Rosseel (2018) <DOI:10.1037/met0000187>. When using the\n    packages, please refer to the package Mulder et al. (2021) <DOI:10.18637/jss.v100.i18>\n    and the relevant methodological papers.",
    "version": "1.5.0",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "author": "Joris Mulder [aut, cre],\n  Caspar van Lissa [aut, ctb],\n  Donald R. Williams [aut, ctb],\n  Xin Gu [aut, ctb],\n  Anton Olsson-Collentine [aut, ctb],\n  Florian Boeing-Messing [aut, ctb],\n  Jean-Paul Fox [aut, ctb],\n  Janosch Menke [ctb],\n  Robbie van Aert [ctb],\n  Barry Brown [ctb],\n  James Lovato [ctb],\n  Kathy Russell [ctb],\n  Lapack 3.8 [ctb],\n  Jack Dongarra [ctb],\n  Jim Bunch [ctb],\n  Cleve Moler [ctb],\n  Gilbert Stewart [ctb],\n  John Burkandt [ctb],\n  Ashwith Rego [ctb],\n  Alexander Godunov [ctb],\n  Alan Miller [ctb],\n  Jean-Pierre Moreau [ctb],\n  The R Core Team [cph]",
    "url": "https://github.com/jomulder/BFpack",
    "bug_reports": "https://github.com/jomulder/BFpack/issues",
    "repository": "https://cran.r-project.org/package=BFpack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BFpack Flexible Bayes Factor Testing of Scientific Expectations Implementation of default Bayes factors\n    for testing statistical hypotheses under various statistical models. The package is\n    intended for applied quantitative researchers in the\n    social and behavioral sciences, medical research,\n    and related fields. The Bayes factor tests can be\n    executed for statistical models such as \n    univariate and multivariate normal linear models,\n    correlation analysis, generalized linear models, special cases of \n    linear mixed models, survival models, relational\n    event models. Parameters that can be tested are\n    location parameters (e.g., group means, regression coefficients),\n    variances (e.g., group variances), and measures of \n    association (e.g,. polychoric/polyserial/biserial/tetrachoric/product\n    moments correlations), among others.\n    The statistical underpinnings are\n    described in\n    O'Hagan (1995) <DOI:10.1111/j.2517-6161.1995.tb02017.x>,\n    De Santis and Spezzaferri (2001) <DOI:10.1016/S0378-3758(00)00240-8>,\n    Mulder and Xin (2022) <DOI:10.1080/00273171.2021.1904809>,\n    Mulder and Gelissen (2019) <DOI:10.1080/02664763.2021.1992360>,\n    Mulder (2016) <DOI:10.1016/j.jmp.2014.09.004>,\n    Mulder and Fox (2019) <DOI:10.1214/18-BA1115>,\n    Mulder and Fox (2013) <DOI:10.1007/s11222-011-9295-3>,\n    Boeing-Messing, van Assen, Hofman, Hoijtink, and Mulder (2017) <DOI:10.1037/met0000116>,\n    Hoijtink, Mulder, van Lissa, and Gu (2018) <DOI:10.1037/met0000201>,\n    Gu, Mulder, and Hoijtink (2018) <DOI:10.1111/bmsp.12110>,\n    Hoijtink, Gu, and Mulder (2018) <DOI:10.1111/bmsp.12145>, and\n    Hoijtink, Gu, Mulder, and Rosseel (2018) <DOI:10.1037/met0000187>. When using the\n    packages, please refer to the package Mulder et al. (2021) <DOI:10.18637/jss.v100.i18>\n    and the relevant methodological papers.  "
  },
  {
    "id": 1832,
    "package_name": "BGGM",
    "title": "Bayesian Gaussian Graphical Models",
    "description": "Fit Bayesian Gaussian graphical models. The methods are separated into \n    two Bayesian approaches for inference: hypothesis testing and estimation. There are \n    extensions for confirmatory hypothesis testing, comparing Gaussian graphical models, \n    and node wise predictability. These methods were recently introduced in the Gaussian \n    graphical model literature, including \n    Williams (2019) <doi:10.31234/osf.io/x8dpr>, \n    Williams and Mulder (2019) <doi:10.31234/osf.io/ypxd8>,\n    Williams, Rast, Pericchi, and Mulder (2019) <doi:10.31234/osf.io/yt386>.",
    "version": "2.1.6",
    "maintainer": "Philippe Rast <rast.ph@gmail.com>",
    "author": "Donald Williams [aut],\n  Joris Mulder [aut],\n  Philippe Rast [aut, cre]",
    "url": "https://rast-lab.github.io/BGGM/",
    "bug_reports": "https://github.com/rast-lab/BGGM/issues",
    "repository": "https://cran.r-project.org/package=BGGM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BGGM Bayesian Gaussian Graphical Models Fit Bayesian Gaussian graphical models. The methods are separated into \n    two Bayesian approaches for inference: hypothesis testing and estimation. There are \n    extensions for confirmatory hypothesis testing, comparing Gaussian graphical models, \n    and node wise predictability. These methods were recently introduced in the Gaussian \n    graphical model literature, including \n    Williams (2019) <doi:10.31234/osf.io/x8dpr>, \n    Williams and Mulder (2019) <doi:10.31234/osf.io/ypxd8>,\n    Williams, Rast, Pericchi, and Mulder (2019) <doi:10.31234/osf.io/yt386>.  "
  },
  {
    "id": 1858,
    "package_name": "BLA",
    "title": "Boundary Line Analysis",
    "description": "Fits boundary line models to datasets as proposed by Webb\n    (1972) <doi:10.1080/00221589.1972.11514472> and makes statistical\n    inferences about their parameters. Provides additional tools for\n    testing datasets for evidence of boundary presence and selecting\n    initial starting values for model optimization prior to fitting the\n    boundary line models. It also includes tools for conducting post-hoc\n    analyses such as predicting boundary values and identifying the most\n    limiting factor (Miti, Milne, Giller, Lark (2024)\n    <doi:10.1016/j.fcr.2024.109365>). This ensures a comprehensive\n    analysis for datasets that exhibit upper boundary structures.",
    "version": "1.0.1",
    "maintainer": "Chawezi Miti <chawezi.miti@nottingham.ac.uk>",
    "author": "Chawezi Miti [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0001-7776-8366>),\n  Richard M Lark [aut] (ORCID: <https://orcid.org/0000-0003-2571-8521>),\n  Alice E Milne [aut] (ORCID: <https://orcid.org/0000-0002-4509-0578>),\n  Ken E Giller [aut] (ORCID: <https://orcid.org/0000-0002-5998-4652>),\n  Victor O Sadras [ctb],\n  University of Nottingham/Rothamsted Research [fnd]",
    "url": "https://chawezimiti.github.io/BLA/",
    "bug_reports": "https://github.com/chawezimiti/BLA/issues",
    "repository": "https://cran.r-project.org/package=BLA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BLA Boundary Line Analysis Fits boundary line models to datasets as proposed by Webb\n    (1972) <doi:10.1080/00221589.1972.11514472> and makes statistical\n    inferences about their parameters. Provides additional tools for\n    testing datasets for evidence of boundary presence and selecting\n    initial starting values for model optimization prior to fitting the\n    boundary line models. It also includes tools for conducting post-hoc\n    analyses such as predicting boundary values and identifying the most\n    limiting factor (Miti, Milne, Giller, Lark (2024)\n    <doi:10.1016/j.fcr.2024.109365>). This ensures a comprehensive\n    analysis for datasets that exhibit upper boundary structures.  "
  },
  {
    "id": 1886,
    "package_name": "BOIN",
    "title": "Bayesian Optimal INterval (BOIN) Design for Single-Agent and\nDrug- Combination Phase I Clinical Trials",
    "description": "The Bayesian optimal interval (BOIN) design is a novel phase I\n    clinical trial design for finding the maximum tolerated dose (MTD). It can be\n    used to design both single-agent and drug-combination trials. The BOIN design\n    is motivated by the top priority and concern of clinicians when testing a new\n    drug, which is to effectively treat patients and minimize the chance of exposing\n    them to subtherapeutic or overly toxic doses. The prominent advantage of the\n    BOIN design is that it achieves simplicity and superior performance at the same\n    time. The BOIN design is algorithm-based and can be implemented in a simple\n    way similar to the traditional 3+3 design. The BOIN design yields an average\n    performance that is comparable to that of the continual reassessment method\n    (CRM, one of the best model-based designs) in terms of selecting the MTD, but\n    has a substantially lower risk of assigning patients to subtherapeutic or overly\n    toxic doses. For tutorial, please check Yan et al. (2020) <doi:10.18637/jss.v094.i13>.",
    "version": "2.7.2",
    "maintainer": "Ying Yuan <yyuan@mdanderson.org>",
    "author": "Ying Yuan and Suyu Liu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BOIN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BOIN Bayesian Optimal INterval (BOIN) Design for Single-Agent and\nDrug- Combination Phase I Clinical Trials The Bayesian optimal interval (BOIN) design is a novel phase I\n    clinical trial design for finding the maximum tolerated dose (MTD). It can be\n    used to design both single-agent and drug-combination trials. The BOIN design\n    is motivated by the top priority and concern of clinicians when testing a new\n    drug, which is to effectively treat patients and minimize the chance of exposing\n    them to subtherapeutic or overly toxic doses. The prominent advantage of the\n    BOIN design is that it achieves simplicity and superior performance at the same\n    time. The BOIN design is algorithm-based and can be implemented in a simple\n    way similar to the traditional 3+3 design. The BOIN design yields an average\n    performance that is comparable to that of the continual reassessment method\n    (CRM, one of the best model-based designs) in terms of selecting the MTD, but\n    has a substantially lower risk of assigning patients to subtherapeutic or overly\n    toxic doses. For tutorial, please check Yan et al. (2020) <doi:10.18637/jss.v094.i13>.  "
  },
  {
    "id": 1938,
    "package_name": "Ball",
    "title": "Statistical Inference and Sure Independence Screening via Ball\nStatistics",
    "description": "Hypothesis tests and sure independence screening (SIS) procedure based on ball statistics, including ball divergence <doi:10.1214/17-AOS1579>, ball covariance <doi:10.1080/01621459.2018.1543600>, and ball correlation <doi:10.1080/01621459.2018.1462709>, are developed to analyze complex data in metric spaces, e.g, shape, directional, compositional and symmetric positive definite matrix data. The ball divergence and ball covariance based distribution-free tests are implemented to detecting distribution difference and association in metric spaces <doi:10.18637/jss.v097.i06>. Furthermore, several generic non-parametric feature selection procedures based on ball correlation, BCor-SIS and all of its variants, are implemented to tackle the challenge in the context of ultra high dimensional data. A fast implementation for large-scale multiple K-sample testing with ball divergence <doi: 10.1002/gepi.22423> is supported, which is particularly helpful for genome-wide association study.",
    "version": "1.3.13",
    "maintainer": "Jin Zhu <zhuj37@mail2.sysu.edu.cn>",
    "author": "Jin Zhu [aut, cre] (ORCID: <https://orcid.org/0000-0001-8550-5822>),\n  Wenliang Pan [aut],\n  Yuan Tian [aut],\n  Weinan Xiao [aut],\n  Chengfeng Liu [aut],\n  Ruihuang Liu [aut],\n  Yue Hu [aut],\n  Hongtu Zhu [aut],\n  Heping Zhang [aut],\n  Xueqin Wang [aut] (ORCID: <https://orcid.org/0000-0001-5205-9950>)",
    "url": "https://mamba413.github.io/Ball/, https://github.com/Mamba413/Ball",
    "bug_reports": "https://github.com/Mamba413/Ball/issues",
    "repository": "https://cran.r-project.org/package=Ball",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Ball Statistical Inference and Sure Independence Screening via Ball\nStatistics Hypothesis tests and sure independence screening (SIS) procedure based on ball statistics, including ball divergence <doi:10.1214/17-AOS1579>, ball covariance <doi:10.1080/01621459.2018.1543600>, and ball correlation <doi:10.1080/01621459.2018.1462709>, are developed to analyze complex data in metric spaces, e.g, shape, directional, compositional and symmetric positive definite matrix data. The ball divergence and ball covariance based distribution-free tests are implemented to detecting distribution difference and association in metric spaces <doi:10.18637/jss.v097.i06>. Furthermore, several generic non-parametric feature selection procedures based on ball correlation, BCor-SIS and all of its variants, are implemented to tackle the challenge in the context of ultra high dimensional data. A fast implementation for large-scale multiple K-sample testing with ball divergence <doi: 10.1002/gepi.22423> is supported, which is particularly helpful for genome-wide association study.  "
  },
  {
    "id": 1952,
    "package_name": "BayesAT",
    "title": "Bayesian Adaptive Trial",
    "description": "Bayesian adaptive trial algorithm implements multiple-stage interim analysis. Package includes data generating function, and Bayesian hypothesis testing function.",
    "version": "0.1.0",
    "maintainer": "Yuan Zhong <aqua.zhong@gmail.com>",
    "author": "Yuan Zhong [aut, cre],\n  Zeynep Baskurt [aut],\n  Wei Xu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesAT Bayesian Adaptive Trial Bayesian adaptive trial algorithm implements multiple-stage interim analysis. Package includes data generating function, and Bayesian hypothesis testing function.  "
  },
  {
    "id": 1968,
    "package_name": "BayesDissolution",
    "title": "Bayesian Models for Dissolution Testing",
    "description": "Fits Bayesian models (amongst others) to dissolution data sets that can be used for dissolution testing. The package was originally constructed to include only the Bayesian models outlined in Pourmohamad et al. (2022) <doi:10.1111/rssc.12535>. However, additional Bayesian and non-Bayesian models (based on bootstrapping and generalized pivotal quanties) have also been added. More models may be added over time.",
    "version": "0.2.1",
    "maintainer": "Tony Pourmohamad <tpourmohamad@gmail.com>",
    "author": "Tony Pourmohamad [aut, cre],\n  Steven Novick [aut],\n  Robert Richardson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesDissolution",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesDissolution Bayesian Models for Dissolution Testing Fits Bayesian models (amongst others) to dissolution data sets that can be used for dissolution testing. The package was originally constructed to include only the Bayesian models outlined in Pourmohamad et al. (2022) <doi:10.1111/rssc.12535>. However, additional Bayesian and non-Bayesian models (based on bootstrapping and generalized pivotal quanties) have also been added. More models may be added over time.  "
  },
  {
    "id": 1997,
    "package_name": "BayesPower",
    "title": "Sample Size and Power Calculation for Bayesian Testing with\nBayes Factor",
    "description": "The goal of 'BayesPower' is to provide tools for Bayesian sample size determination and power analysis across a range of common hypothesis testing scenarios using Bayes factors. The main function, BayesPower_BayesFactor(), launches an interactive 'shiny' application for performing these analyses. The application also provides command-line code for reproducibility. Details of the methods are described in the tutorial by Wong, Pawel, and Tendeiro (2025) <doi:10.31234/osf.io/pgdac_v1>.",
    "version": "1.0.1",
    "maintainer": "Tsz Keung Wong <t.k.wong3004@gmail.com>",
    "author": "Tsz Keung Wong [aut, cre],\n  Samuel Pawel [aut],\n  Jorge Tendeiro [aut]",
    "url": "",
    "bug_reports": "https://github.com/tkWong3004/BayesPower/issues",
    "repository": "https://cran.r-project.org/package=BayesPower",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesPower Sample Size and Power Calculation for Bayesian Testing with\nBayes Factor The goal of 'BayesPower' is to provide tools for Bayesian sample size determination and power analysis across a range of common hypothesis testing scenarios using Bayes factors. The main function, BayesPower_BayesFactor(), launches an interactive 'shiny' application for performing these analyses. The application also provides command-line code for reproducibility. Details of the methods are described in the tutorial by Wong, Pawel, and Tendeiro (2025) <doi:10.31234/osf.io/pgdac_v1>.  "
  },
  {
    "id": 2004,
    "package_name": "BayesS5",
    "title": "Bayesian Variable Selection Using Simplified Shotgun Stochastic\nSearch with Screening (S5)",
    "description": "In p >> n settings, full posterior sampling using existing Markov chain Monte\n    Carlo (MCMC) algorithms is highly inefficient and often not feasible from a practical\n    perspective. To overcome this problem, we propose a scalable stochastic search algorithm that is called the Simplified Shotgun Stochastic Search (S5) and aimed at rapidly explore interesting regions of model space and finding the maximum a posteriori(MAP) model. Also, the S5 provides an approximation of posterior probability of each model (including the marginal inclusion probabilities). This algorithm is a part of an article titled \"Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings\" (2018) by Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson and \"Nonlocal Functional Priors for Nonparametric Hypothesis Testing and High-dimensional Model Selection\" (2020+) by Minsuk Shin and Anirban Bhattacharya. ",
    "version": "1.41",
    "maintainer": "Minsuk Shin <minsuk000@gmail.com>",
    "author": "Minsuk Shin and Ruoxuan Tian",
    "url": "https://arxiv.org/abs/1507.07106v4",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesS5",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesS5 Bayesian Variable Selection Using Simplified Shotgun Stochastic\nSearch with Screening (S5) In p >> n settings, full posterior sampling using existing Markov chain Monte\n    Carlo (MCMC) algorithms is highly inefficient and often not feasible from a practical\n    perspective. To overcome this problem, we propose a scalable stochastic search algorithm that is called the Simplified Shotgun Stochastic Search (S5) and aimed at rapidly explore interesting regions of model space and finding the maximum a posteriori(MAP) model. Also, the S5 provides an approximation of posterior probability of each model (including the marginal inclusion probabilities). This algorithm is a part of an article titled \"Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings\" (2018) by Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson and \"Nonlocal Functional Priors for Nonparametric Hypothesis Testing and High-dimensional Model Selection\" (2020+) by Minsuk Shin and Anirban Bhattacharya.   "
  },
  {
    "id": 2014,
    "package_name": "BayesVarSel",
    "title": "Bayes Factors, Model Choice and Variable Selection in Linear\nModels",
    "description": "Bayes factors and posterior probabilities in Linear models, \n    aimed at provide a formal Bayesian answer to testing and variable \n    selection problems. ",
    "version": "2.4.5",
    "maintainer": "Gonzalo Garcia-Donato <gonzalo.garciadonato@uclm.es>",
    "author": "Gonzalo Garcia-Donato [aut, cre],\n  Anabel Forte [aut],\n  Carlos Vergara-Hern\u00e1ndez [ctb]",
    "url": "https://github.com/comodin19/BayesVarSel",
    "bug_reports": "https://github.com/comodin19/BayesVarSel/issues",
    "repository": "https://cran.r-project.org/package=BayesVarSel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesVarSel Bayes Factors, Model Choice and Variable Selection in Linear\nModels Bayes factors and posterior probabilities in Linear models, \n    aimed at provide a formal Bayesian answer to testing and variable \n    selection problems.   "
  },
  {
    "id": 2086,
    "package_name": "BioPETsurv",
    "title": "Biomarker Prognostic Enrichment Tool for Time-to-Event Trial",
    "description": "Prognostic Enrichment is a strategy of enriching a clinical trial for testing an intervention intended to prevent or delay an unwanted clinical event.  A prognostically enriched trial enrolls only patients who are more likely to experience the unwanted clinical event than the broader patient population (R. Temple (2010) <doi:10.1038/clpt.2010.233>). By testing the intervention in an enriched study population, the trial may be adequately powered with a smaller sample size, which can have both practical and ethical advantages.\n    This package provides tools to evaluate biomarkers for prognostic enrichment of clinical trials with survival/time-to-event outcomes.",
    "version": "0.1.0",
    "maintainer": "Si Cheng <chengsi@uw.edu>",
    "author": "Si Cheng [cre, aut],\n  Kathleen F. Kerr [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BioPETsurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BioPETsurv Biomarker Prognostic Enrichment Tool for Time-to-Event Trial Prognostic Enrichment is a strategy of enriching a clinical trial for testing an intervention intended to prevent or delay an unwanted clinical event.  A prognostically enriched trial enrolls only patients who are more likely to experience the unwanted clinical event than the broader patient population (R. Temple (2010) <doi:10.1038/clpt.2010.233>). By testing the intervention in an enriched study population, the trial may be adequately powered with a smaller sample size, which can have both practical and ethical advantages.\n    This package provides tools to evaluate biomarkers for prognostic enrichment of clinical trials with survival/time-to-event outcomes.  "
  },
  {
    "id": 2095,
    "package_name": "BioWorldR",
    "title": "A Curated Collection of Biodiversity and Species Datasets and\nUtilities",
    "description": "Provides a curated collection of biodiversity and species-related datasets (birds, plants, reptiles, turtles, mammals, bees, marine data and related biological measurements), together with small utilities to load and explore them.\n    The package gathers data sourced from public repositories (including Kaggle and well-known ecological/biological R packages) and standardizes access for researchers, educators, and data analysts working on biodiversity, biogeography, ecology and comparative biology.\n    It aims to simplify reproducible workflows by packaging commonly used example datasets and metadata so they can be easily inspected, visualized, and used for teaching, testing, and prototyping analyses.",
    "version": "0.1.0",
    "maintainer": "Juan David Monroy <monroyjuandavid773@gmail.com>",
    "author": "Juan David Monroy [aut, cre]",
    "url": "https://github.com/Monroy31039/BioWorld,\nhttps://Monroy31039.github.io/BioWorld/",
    "bug_reports": "https://github.com/Monroy31039/BioWorld/issues",
    "repository": "https://cran.r-project.org/package=BioWorldR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BioWorldR A Curated Collection of Biodiversity and Species Datasets and\nUtilities Provides a curated collection of biodiversity and species-related datasets (birds, plants, reptiles, turtles, mammals, bees, marine data and related biological measurements), together with small utilities to load and explore them.\n    The package gathers data sourced from public repositories (including Kaggle and well-known ecological/biological R packages) and standardizes access for researchers, educators, and data analysts working on biodiversity, biogeography, ecology and comparative biology.\n    It aims to simplify reproducible workflows by packaging commonly used example datasets and metadata so they can be easily inspected, visualized, and used for teaching, testing, and prototyping analyses.  "
  },
  {
    "id": 2105,
    "package_name": "BisRNA",
    "title": "Analysis of RNA Cytosine-5 Methylation",
    "description": "Bisulfite-treated RNA non-conversion in a set of samples is\n  analysed as follows : each sample's non-conversion distribution is\n  identified to a Poisson distribution. P-values adjusted for multiple\n  testing are calculated in each sample. Combined non-conversion P-values\n  and standard errors are calculated on the intersection of the set of\n  samples. For further details, see C Legrand, F Tuorto, M Hartmann, \n  R Liebers, D Jakob, M Helm and F Lyko (2017) <doi:10.1101/gr.210666.116>.",
    "version": "0.2.2",
    "maintainer": "Carine Legrand <c.legrand@dkfz.de>",
    "author": "C Legrand",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BisRNA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BisRNA Analysis of RNA Cytosine-5 Methylation Bisulfite-treated RNA non-conversion in a set of samples is\n  analysed as follows : each sample's non-conversion distribution is\n  identified to a Poisson distribution. P-values adjusted for multiple\n  testing are calculated in each sample. Combined non-conversion P-values\n  and standard errors are calculated on the intersection of the set of\n  samples. For further details, see C Legrand, F Tuorto, M Hartmann, \n  R Liebers, D Jakob, M Helm and F Lyko (2017) <doi:10.1101/gr.210666.116>.  "
  },
  {
    "id": 2126,
    "package_name": "BonEV",
    "title": "An Improved Multiple Testing Procedure for Controlling False\nDiscovery Rates",
    "description": "An improved multiple testing procedure for controlling false discovery rates which is developed based on the Bonferroni procedure with integrated estimates from the Benjamini-Hochberg procedure and the Storey's q-value procedure. It controls false discovery rates through controlling the expected number of false discoveries.",
    "version": "1.0",
    "maintainer": "Dongmei Li <dongmei_li@urmc.rochester.edu>",
    "author": "Dongmei Li",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BonEV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BonEV An Improved Multiple Testing Procedure for Controlling False\nDiscovery Rates An improved multiple testing procedure for controlling false discovery rates which is developed based on the Bonferroni procedure with integrated estimates from the Benjamini-Hochberg procedure and the Storey's q-value procedure. It controls false discovery rates through controlling the expected number of false discoveries.  "
  },
  {
    "id": 2164,
    "package_name": "Bvalue",
    "title": "B-Value and Empirical Equivalence Bound",
    "description": "Calculates B-value and empirical equivalence bound. B-value is defined as the maximum magnitude of a confidence interval; and the empirical equivalence bound is the minimum B-value at a certain level. A new two-stage procedure for hypothesis testing is proposed, where the first stage is conventional hypothesis testing and the second is an equivalence testing procedure using the introduced empirical equivalence bound. See Zhao et al. (2019) \"B-Value and Empirical Equivalence Bound: A New Procedure of Hypothesis Testing\" <arXiv:1912.13084> for details.",
    "version": "1.0",
    "maintainer": "Yi Zhao <zhaoyi1026@gmail.com>",
    "author": "Yi Zhao <zhaoyi1026@gmail.com>\n\t\tBrian Caffo <bcaffo@gmail.com>\n\t\tJoshua Ewen <ewen@kennedykrieger.org>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Bvalue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Bvalue B-Value and Empirical Equivalence Bound Calculates B-value and empirical equivalence bound. B-value is defined as the maximum magnitude of a confidence interval; and the empirical equivalence bound is the minimum B-value at a certain level. A new two-stage procedure for hypothesis testing is proposed, where the first stage is conventional hypothesis testing and the second is an equivalence testing procedure using the introduced empirical equivalence bound. See Zhao et al. (2019) \"B-Value and Empirical Equivalence Bound: A New Procedure of Hypothesis Testing\" <arXiv:1912.13084> for details.  "
  },
  {
    "id": 2179,
    "package_name": "CANE",
    "title": "Comprehensive Groups of Experiments Analysis for Numerous\nEnvironments",
    "description": "In many cases, experiments must be repeated across multiple seasons or locations to ensure applicability of findings. A single experiment conducted in one location and season may yield limited conclusions, as results can vary under different environmental conditions. In agricultural research, treatment \u00d7 location and treatment \u00d7 season interactions play a crucial role. Analyzing a series of experiments across diverse conditions allows for more generalized and reliable recommendations. The 'CANE' package facilitates the pooled analysis of experiments conducted over multiple years, seasons, or locations. It is designed to assess treatment interactions with environmental factors (such as location and season) using various experimental designs. The package supports pooled analysis of variance (ANOVA) for the following designs: (1) 'PooledCRD()': completely randomized design; (2) 'PooledRBD()': randomized block design; (3) 'PooledLSD()': Latin square design; (4) 'PooledSPD()': split plot design; and (5) 'PooledStPD()': strip plot design. Each function provides the following outputs: (i) Individual ANOVA tables based on independent analysis for each location or year; (ii) Testing of homogeneity of error variances among distinct locations using Bartlett\u2019s Chi-Square test; (iii) If Bartlett\u2019s test is significant, 'Aitken\u2019s' transformation, defined as the ratio of the response to the square root of the error mean square, is applied to the response variable; otherwise, the data is used as is; (iv) Combined analysis to obtain a pooled ANOVA table; (v) Multiple comparison tests, including Tukey's honestly significant difference (Tukey's HSD) test, Duncan\u2019s multiple range test (DMRT), and the least significant difference (LSD) test, for treatment comparisons. The statistical theory and steps of analysis of these designs are available in Dean et al. (2017)<doi:10.1007/978-3-319-52250-0> and Ru\u00edz et al. (2024)<doi:10.1007/978-3-031-65575-3>. By broadening the scope of experimental conclusions, 'CANE' enables researchers to derive robust, widely applicable recommendations. This package is particularly valuable in agricultural research, where accounting for treatment \u00d7 location and treatment \u00d7 season interactions is essential for ensuring the validity of findings across multiple settings.",
    "version": "0.1.1",
    "maintainer": "Vinayaka <vinayaka.b3vs@gmail.com>",
    "author": "Vinayaka [aut, cre] (ORCID: <https://orcid.org/0000-0001-5004-0084>),\n  T. Lakshmi Pathy [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-8940-7971>),\n  K. Gopalareddy [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-8825-6363>),\n  Shweta Kumari [aut, ctb] (ORCID:\n    <https://orcid.org/0009-0000-4377-2455>),\n  P. Murali [aut, ctb] (ORCID: <https://orcid.org/0000-0002-1149-7097>),\n  P. Govindaraj [aut, ctb],\n  P. Rama Chandra Prasad [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0001-5285-9827>),\n  L.N. Vinaykumar [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CANE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CANE Comprehensive Groups of Experiments Analysis for Numerous\nEnvironments In many cases, experiments must be repeated across multiple seasons or locations to ensure applicability of findings. A single experiment conducted in one location and season may yield limited conclusions, as results can vary under different environmental conditions. In agricultural research, treatment \u00d7 location and treatment \u00d7 season interactions play a crucial role. Analyzing a series of experiments across diverse conditions allows for more generalized and reliable recommendations. The 'CANE' package facilitates the pooled analysis of experiments conducted over multiple years, seasons, or locations. It is designed to assess treatment interactions with environmental factors (such as location and season) using various experimental designs. The package supports pooled analysis of variance (ANOVA) for the following designs: (1) 'PooledCRD()': completely randomized design; (2) 'PooledRBD()': randomized block design; (3) 'PooledLSD()': Latin square design; (4) 'PooledSPD()': split plot design; and (5) 'PooledStPD()': strip plot design. Each function provides the following outputs: (i) Individual ANOVA tables based on independent analysis for each location or year; (ii) Testing of homogeneity of error variances among distinct locations using Bartlett\u2019s Chi-Square test; (iii) If Bartlett\u2019s test is significant, 'Aitken\u2019s' transformation, defined as the ratio of the response to the square root of the error mean square, is applied to the response variable; otherwise, the data is used as is; (iv) Combined analysis to obtain a pooled ANOVA table; (v) Multiple comparison tests, including Tukey's honestly significant difference (Tukey's HSD) test, Duncan\u2019s multiple range test (DMRT), and the least significant difference (LSD) test, for treatment comparisons. The statistical theory and steps of analysis of these designs are available in Dean et al. (2017)<doi:10.1007/978-3-319-52250-0> and Ru\u00edz et al. (2024)<doi:10.1007/978-3-031-65575-3>. By broadening the scope of experimental conclusions, 'CANE' enables researchers to derive robust, widely applicable recommendations. This package is particularly valuable in agricultural research, where accounting for treatment \u00d7 location and treatment \u00d7 season interactions is essential for ensuring the validity of findings across multiple settings.  "
  },
  {
    "id": 2195,
    "package_name": "CAinterprTools",
    "title": "Graphical Aid in Correspondence Analysis Interpretation and\nSignificance Testings",
    "description": "Allows to plot a number of information related to the interpretation of Correspondence Analysis' results. It provides the facility to plot the contribution of rows and columns categories to the principal dimensions, the quality of points display on selected dimensions, the correlation of row and column categories to selected dimensions, etc. It also allows to assess which dimension(s) is important for the data structure interpretation by means of different statistics and tests. The package also offers the facility to plot the permuted distribution of the table total inertia as well as of the inertia accounted for by pairs of selected dimensions. Different facilities are also provided that aim to produce interpretation-oriented scatterplots. Reference: Alberti 2015 <doi:10.1016/j.softx.2015.07.001>.",
    "version": "1.1.0",
    "maintainer": "Gianmarco Alberti <gianmarcoalberti@gmail.com>",
    "author": "Gianmarco Alberti [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CAinterprTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CAinterprTools Graphical Aid in Correspondence Analysis Interpretation and\nSignificance Testings Allows to plot a number of information related to the interpretation of Correspondence Analysis' results. It provides the facility to plot the contribution of rows and columns categories to the principal dimensions, the quality of points display on selected dimensions, the correlation of row and column categories to selected dimensions, etc. It also allows to assess which dimension(s) is important for the data structure interpretation by means of different statistics and tests. The package also offers the facility to plot the permuted distribution of the table total inertia as well as of the inertia accounted for by pairs of selected dimensions. Different facilities are also provided that aim to produce interpretation-oriented scatterplots. Reference: Alberti 2015 <doi:10.1016/j.softx.2015.07.001>.  "
  },
  {
    "id": 2206,
    "package_name": "CBTF",
    "title": "Caught by the Fuzz! - A Minimalistic Fuzz-Test Runner",
    "description": "A simple runner for fuzz-testing functions in an R package's\n    public interface. Fuzz testing helps identify functions lacking sufficient\n    argument validation, and uncovers problematic inputs that, while valid by\n    function signature, may cause issues within the function body.",
    "version": "0.5.0",
    "maintainer": "Marco Colombo <mar.colombo13@gmail.com>",
    "author": "Marco Colombo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6672-0623>)",
    "url": "https://mcol.github.io/caught-by-the-fuzz/",
    "bug_reports": "https://github.com/mcol/caught-by-the-fuzz/issues",
    "repository": "https://cran.r-project.org/package=CBTF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CBTF Caught by the Fuzz! - A Minimalistic Fuzz-Test Runner A simple runner for fuzz-testing functions in an R package's\n    public interface. Fuzz testing helps identify functions lacking sufficient\n    argument validation, and uncovers problematic inputs that, while valid by\n    function signature, may cause issues within the function body.  "
  },
  {
    "id": 2211,
    "package_name": "CCI",
    "title": "Computational Test for Conditional Independence",
    "description": "Tool for performing computational testing for conditional independence between variables in a dataset. 'CCI' implements permutation in combination with Monte Carlo Cross-Validation in generating null distributions and test statistics. For more details see Computational Test for Conditional Independence (2024) <doi:10.3390/a17080323>.   ",
    "version": "0.3.4",
    "maintainer": "Christian B. H. Thorjussen <christianbern@gmail.com>",
    "author": "Christian B. H. Thorjussen [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-5006-6491>),\n  Kristian Hovde Liland [aut] (ORCID:\n    <https://orcid.org/0000-0001-6468-9423>)",
    "url": "https://github.com/khliland/CCI",
    "bug_reports": "https://github.com/khliland/CCI/issues",
    "repository": "https://cran.r-project.org/package=CCI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CCI Computational Test for Conditional Independence Tool for performing computational testing for conditional independence between variables in a dataset. 'CCI' implements permutation in combination with Monte Carlo Cross-Validation in generating null distributions and test statistics. For more details see Computational Test for Conditional Independence (2024) <doi:10.3390/a17080323>.     "
  },
  {
    "id": 2257,
    "package_name": "CIEE",
    "title": "Estimating and Testing Direct Effects in Directed Acyclic Graphs\nusing Estimating Equations",
    "description": "In many studies across different disciplines, detailed measures of the variables of interest are available. If assumptions can be made regarding the direction of effects between the assessed variables, this has to be considered in the analysis. The functions in this package implement the novel approach CIEE (causal inference using estimating equations; Konigorski et al., 2018, <DOI:10.1002/gepi.22107>) for estimating and testing the direct effect of an exposure variable on a primary outcome, while adjusting for indirect effects of the exposure on the primary outcome through a secondary intermediate outcome and potential factors influencing the secondary outcome. The underlying directed acyclic graph (DAG) of this considered model is described in the vignette. CIEE can be applied to studies in many different fields, and it is implemented here for the analysis of a continuous primary outcome and a time-to-event primary outcome subject to censoring. CIEE uses estimating equations to obtain estimates of the direct effect and robust sandwich standard error estimates. Then, a large-sample Wald-type test statistic is computed for testing the absence of the direct effect. Additionally, standard multiple regression, regression of residuals, and the structural equation modeling approach are implemented for comparison. ",
    "version": "0.1.1",
    "maintainer": "Stefan Konigorski <stefan.konigorski@gmail.com>",
    "author": "Stefan Konigorski [aut, cre],\n  Yildiz E. Yilmaz [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CIEE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CIEE Estimating and Testing Direct Effects in Directed Acyclic Graphs\nusing Estimating Equations In many studies across different disciplines, detailed measures of the variables of interest are available. If assumptions can be made regarding the direction of effects between the assessed variables, this has to be considered in the analysis. The functions in this package implement the novel approach CIEE (causal inference using estimating equations; Konigorski et al., 2018, <DOI:10.1002/gepi.22107>) for estimating and testing the direct effect of an exposure variable on a primary outcome, while adjusting for indirect effects of the exposure on the primary outcome through a secondary intermediate outcome and potential factors influencing the secondary outcome. The underlying directed acyclic graph (DAG) of this considered model is described in the vignette. CIEE can be applied to studies in many different fields, and it is implemented here for the analysis of a continuous primary outcome and a time-to-event primary outcome subject to censoring. CIEE uses estimating equations to obtain estimates of the direct effect and robust sandwich standard error estimates. Then, a large-sample Wald-type test statistic is computed for testing the absence of the direct effect. Additionally, standard multiple regression, regression of residuals, and the structural equation modeling approach are implemented for comparison.   "
  },
  {
    "id": 2266,
    "package_name": "CIS.DGLM",
    "title": "Covariates, Interaction, and Selection for DGLM",
    "description": "An implementation of double generalized linear model (DGLM) building with variable selection procedures and handling of interaction terms and other complex situations. We also provide a method of handling convergence issues within the dglm() function. The package offers a simulation function for generating simulated data for testing purposes and utilizes the forward stepwise variable selection procedure in model-building. It also provides a new custom bootstrap function for mean and standard deviation estimation and functions for building crossplots and squareplots from a data set.",
    "version": "0.1.0",
    "maintainer": "Yishi Wang <wangy@uncw.edu>",
    "author": "Ann Stapleton [aut],\n  Yishi Wang [aut, cre],\n  Kaitlyn Hohmeier [aut],\n  Jordan Tanley [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CIS.DGLM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CIS.DGLM Covariates, Interaction, and Selection for DGLM An implementation of double generalized linear model (DGLM) building with variable selection procedures and handling of interaction terms and other complex situations. We also provide a method of handling convergence issues within the dglm() function. The package offers a simulation function for generating simulated data for testing purposes and utilizes the forward stepwise variable selection procedure in model-building. It also provides a new custom bootstrap function for mean and standard deviation estimation and functions for building crossplots and squareplots from a data set.  "
  },
  {
    "id": 2272,
    "package_name": "CJAMP",
    "title": "Copula-Based Joint Analysis of Multiple Phenotypes",
    "description": "We provide a computationally efficient and robust implementation of the recently proposed C-JAMP (Copula-based Joint Analysis of Multiple Phenotypes) method (Konigorski et al., 2019, submitted). C-JAMP allows estimating and testing the association of one or multiple predictors on multiple outcomes in a joint model, and is implemented here with a focus on large-scale genome-wide association studies with two phenotypes. The use of copula functions allows modeling a wide range of multivariate dependencies between the phenotypes, and previous results are supporting that C-JAMP can increase the power of association studies to identify associated genetic variants in comparison to existing methods (Konigorski, Yilmaz, Pischon, 2016, <DOI:10.1186/s12919-016-0045-6>; Konigorski, Yilmaz, Bull, 2014, <DOI:10.1186/1753-6561-8-S1-S72>). In addition to the C-JAMP functions, functions are available to generate genetic and phenotypic data, to compute the minor allele frequency (MAF) of genetic markers, and to estimate the phenotypic variance explained by genetic markers.",
    "version": "0.1.1",
    "maintainer": "Stefan Konigorski <stefan.konigorski@gmail.com>",
    "author": "Stefan Konigorski [aut, cre],\n  Yildiz E. Yilmaz [ctb, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CJAMP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CJAMP Copula-Based Joint Analysis of Multiple Phenotypes We provide a computationally efficient and robust implementation of the recently proposed C-JAMP (Copula-based Joint Analysis of Multiple Phenotypes) method (Konigorski et al., 2019, submitted). C-JAMP allows estimating and testing the association of one or multiple predictors on multiple outcomes in a joint model, and is implemented here with a focus on large-scale genome-wide association studies with two phenotypes. The use of copula functions allows modeling a wide range of multivariate dependencies between the phenotypes, and previous results are supporting that C-JAMP can increase the power of association studies to identify associated genetic variants in comparison to existing methods (Konigorski, Yilmaz, Pischon, 2016, <DOI:10.1186/s12919-016-0045-6>; Konigorski, Yilmaz, Bull, 2014, <DOI:10.1186/1753-6561-8-S1-S72>). In addition to the C-JAMP functions, functions are available to generate genetic and phenotypic data, to compute the minor allele frequency (MAF) of genetic markers, and to estimate the phenotypic variance explained by genetic markers.  "
  },
  {
    "id": 2303,
    "package_name": "CNPS",
    "title": "Nonparametric Statistics",
    "description": "We unify various nonparametric hypothesis testing problems in a framework of permutation testing, enabling hypothesis testing on multi-sample, multidimensional data and contingency tables. Most of the functions available in the R environment to implement permutation tests are single functions constructed for specific test problems; to facilitate the use of the package, the package encapsulates similar tests in a categorized manner, greatly improving ease of use.\n    We will all provide functions for self-selected permutation scoring methods and self-selected p-value calculation methods (asymptotic, exact, and sampling). For two-sample tests, we will provide mean tests and estimate drift sizes; we will provide tests on variance; we will provide paired-sample tests; we will provide correlation coefficient tests under three measures. For multi-sample problems, we will provide both ordinary and ordered alternative test problems. For multidimensional data, we will implement multivariate means (including ordered alternatives) and multivariate pairwise tests based on four statistics; the components with significant differences are also calculated. For contingency tables, we will perform permutation chi-square test or ordered alternative.",
    "version": "1.0.0",
    "maintainer": "JiaSheng Zhang <zhangjiasheng0509@outlook.com>",
    "author": "JiaSheng Zhang [aut,cre] (<zhangjiasheng0509@outlook.com>),\n  SiWei Deng [aut],\n  Feng Yu [aut],\n  YangYang Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CNPS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CNPS Nonparametric Statistics We unify various nonparametric hypothesis testing problems in a framework of permutation testing, enabling hypothesis testing on multi-sample, multidimensional data and contingency tables. Most of the functions available in the R environment to implement permutation tests are single functions constructed for specific test problems; to facilitate the use of the package, the package encapsulates similar tests in a categorized manner, greatly improving ease of use.\n    We will all provide functions for self-selected permutation scoring methods and self-selected p-value calculation methods (asymptotic, exact, and sampling). For two-sample tests, we will provide mean tests and estimate drift sizes; we will provide tests on variance; we will provide paired-sample tests; we will provide correlation coefficient tests under three measures. For multi-sample problems, we will provide both ordinary and ordered alternative test problems. For multidimensional data, we will implement multivariate means (including ordered alternatives) and multivariate pairwise tests based on four statistics; the components with significant differences are also calculated. For contingency tables, we will perform permutation chi-square test or ordered alternative.  "
  },
  {
    "id": 2310,
    "package_name": "COINr",
    "title": "Composite Indicator Construction and Analysis",
    "description": "A comprehensive high-level package, for composite indicator construction and analysis. It is a \"development environment\"\n    for composite indicators and scoreboards, which includes utilities for construction (indicator selection, denomination, imputation,\n    data treatment, normalisation, weighting and aggregation) and analysis (multivariate analysis, correlation plotting, short cuts for\n    principal component analysis, global sensitivity analysis, and more). A composite indicator is completely encapsulated inside a single\n    hierarchical list called a \"coin\". This allows a fast and efficient work flow, as well as making quick copies, testing methodological variations and making comparisons.\n    It also includes many plotting options, both statistical (scatter plots, distribution plots) as well as for presenting results.",
    "version": "1.1.14",
    "maintainer": "William Becker <william.becker@bluefoxdata.eu>",
    "author": "William Becker [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6467-4472>)",
    "url": "https://bluefoxr.github.io/COINr/",
    "bug_reports": "https://github.com/bluefoxr/COINr/issues",
    "repository": "https://cran.r-project.org/package=COINr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "COINr Composite Indicator Construction and Analysis A comprehensive high-level package, for composite indicator construction and analysis. It is a \"development environment\"\n    for composite indicators and scoreboards, which includes utilities for construction (indicator selection, denomination, imputation,\n    data treatment, normalisation, weighting and aggregation) and analysis (multivariate analysis, correlation plotting, short cuts for\n    principal component analysis, global sensitivity analysis, and more). A composite indicator is completely encapsulated inside a single\n    hierarchical list called a \"coin\". This allows a fast and efficient work flow, as well as making quick copies, testing methodological variations and making comparisons.\n    It also includes many plotting options, both statistical (scatter plots, distribution plots) as well as for presenting results.  "
  },
  {
    "id": 2316,
    "package_name": "COMPoissonReg",
    "title": "Conway-Maxwell Poisson (COM-Poisson) Regression",
    "description": "Fit Conway-Maxwell Poisson (COM-Poisson or CMP) regression models\n    to count data (Sellers & Shmueli, 2010) <doi:10.1214/09-AOAS306>. The\n    package provides functions for model estimation, dispersion testing, and\n    diagnostics. Zero-inflated CMP regression (Sellers & Raim, 2016)\n    <doi:10.1016/j.csda.2016.01.007> is also supported.",
    "version": "0.8.1",
    "maintainer": "Andrew Raim <andrew.raim@gmail.com>",
    "author": "Kimberly Sellers <kfs7@georgetown.edu>\n\tThomas Lotze <thomas.lotze@thomaslotze.com>\n\tAndrew Raim <andrew.raim@gmail.com>",
    "url": "https://github.com/lotze/COMPoissonReg",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=COMPoissonReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "COMPoissonReg Conway-Maxwell Poisson (COM-Poisson) Regression Fit Conway-Maxwell Poisson (COM-Poisson or CMP) regression models\n    to count data (Sellers & Shmueli, 2010) <doi:10.1214/09-AOAS306>. The\n    package provides functions for model estimation, dispersion testing, and\n    diagnostics. Zero-inflated CMP regression (Sellers & Raim, 2016)\n    <doi:10.1016/j.csda.2016.01.007> is also supported.  "
  },
  {
    "id": 2345,
    "package_name": "CRTConjoint",
    "title": "Conditional Randomization Testing (CRT) Approach for Conjoint\nAnalysis",
    "description": "Computes p-value according to the CRT using the HierNet test statistic. For more details, see Ham, Imai, Janson (2022) \"Using Machine Learning to Test Causal Hypotheses in Conjoint Analysis\" <arXiv:2201.08343>.",
    "version": "0.1.0",
    "maintainer": "Dae Woong Ham <daewoongham@g.harvard.edu>",
    "author": "Dae Woong Ham [aut, cre],\n  Kosuke Imai [aut],\n  Lucas Janson [aut],\n  Jacob Bien [ctb, cph]",
    "url": "https://github.com/daewoongham97/CRTConjoint",
    "bug_reports": "https://github.com/daewoongham97/CRTConjoint/issues",
    "repository": "https://cran.r-project.org/package=CRTConjoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CRTConjoint Conditional Randomization Testing (CRT) Approach for Conjoint\nAnalysis Computes p-value according to the CRT using the HierNet test statistic. For more details, see Ham, Imai, Janson (2022) \"Using Machine Learning to Test Causal Hypotheses in Conjoint Analysis\" <arXiv:2201.08343>.  "
  },
  {
    "id": 2377,
    "package_name": "CVST",
    "title": "Fast Cross-Validation via Sequential Testing",
    "description": "The fast cross-validation via sequential testing (CVST) procedure is an improved cross-validation procedure which uses non-parametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating under-performing candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the capability of a full cross-validation. Additionally to the CVST the package contains an implementation of the ordinary k-fold cross-validation with a flexible and powerful set of helper objects and methods to handle the overall model selection process. The implementations of the Cochran's Q test with permutations and the sequential testing framework of Wald are generic and can therefore also be used in other contexts.",
    "version": "0.2-3",
    "maintainer": "Tammo Krueger <tammokrueger@googlemail.com>",
    "author": "Tammo Krueger, Mikio Braun",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CVST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CVST Fast Cross-Validation via Sequential Testing The fast cross-validation via sequential testing (CVST) procedure is an improved cross-validation procedure which uses non-parametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating under-performing candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the capability of a full cross-validation. Additionally to the CVST the package contains an implementation of the ordinary k-fold cross-validation with a flexible and powerful set of helper objects and methods to handle the overall model selection process. The implementations of the Cochran's Q test with permutations and the sequential testing framework of Wald are generic and can therefore also be used in other contexts.  "
  },
  {
    "id": 2442,
    "package_name": "ChangepointTesting",
    "title": "Change Point Estimation for Clustered Signals",
    "description": "A multiple testing procedure for clustered alternative hypotheses. \n    It is assumed that the p-values under the null hypotheses follow U(0,1) and \n    that the distributions of p-values from the alternative hypotheses are \n    stochastically smaller than U(0,1). By aggregating information, this \n    method is more sensitive to detecting signals of low magnitude than \n    standard methods. Additionally, sporadic small p-values appearing \n    within a null hypotheses sequence are avoided by averaging on the \n    neighboring p-values.",
    "version": "1.2",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "author": "Hongyuan Cao [aut],\n  Wei Biao Wu [aut],\n  Shannon T. Holloway [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ChangepointTesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ChangepointTesting Change Point Estimation for Clustered Signals A multiple testing procedure for clustered alternative hypotheses. \n    It is assumed that the p-values under the null hypotheses follow U(0,1) and \n    that the distributions of p-values from the alternative hypotheses are \n    stochastically smaller than U(0,1). By aggregating information, this \n    method is more sensitive to detecting signals of low magnitude than \n    standard methods. Additionally, sporadic small p-values appearing \n    within a null hypotheses sequence are avoided by averaging on the \n    neighboring p-values.  "
  },
  {
    "id": 2576,
    "package_name": "Compositional",
    "title": "Compositional Data Analysis",
    "description": "Regression, classification, contour plots, hypothesis testing and fitting of distributions for compositional data are some of the functions included. We further include functions for percentages (or proportions).\n             The standard textbook for such data is John Aitchison's (1986) \"The statistical analysis of compositional data\". Relevant papers include:\n\t\t\t       a) Tsagris M.T., Preston S. and Wood A.T.A. (2011). \"A data-based power transformation for compositional data\". Fourth International International Workshop on Compositional Data Analysis. <doi:10.48550/arXiv.1106.1451>.\n\t\t\t       b) Tsagris M. (2014). \"The k-NN algorithm for compositional data: a revised approach with and without zero values present\". Journal of Data Science, 12(3): 519--534. <doi:10.6339/JDS.201407_12(3).0008>.\n\t\t\t       c) Tsagris M. (2015). \"A novel, divergence based, regression for compositional data\". Proceedings of the 28th Panhellenic Statistics Conference, 15-18 April 2015, Athens, Greece, 430--444. <doi:10.48550/arXiv.1511.07600>.\n\t\t\t       d) Tsagris M. (2015). \"Regression analysis with compositional data containing zero values\". Chilean Journal of Statistics, 6(2): 47--57. <https://soche.cl/chjs/volumes/06/02/Tsagris(2015).pdf>.\n\t\t\t       e) Tsagris M., Preston S. and Wood A.T.A. (2016). \"Improved supervised classification for compositional data using the alpha-transformation\". Journal of Classification, 33(2): 243--261. <doi:10.1007/s00357-016-9207-5>. \n\t\t\t       f) Tsagris M., Preston S. and Wood A.T.A. (2017). \"Nonparametric hypothesis testing for equality of means on the simplex\". Journal of Statistical Computation and Simulation, 87(2): 406--422. <doi:10.1080/00949655.2016.1216554>. \n\t\t\t       g) Tsagris M. and Stewart C. (2018). \"A Dirichlet regression model for compositional data with zeros\". Lobachevskii Journal of Mathematics, 39(3): 398--412. <doi:10.1134/S1995080218030198>. \n\t\t\t       h) Alenazi A. (2019). \"Regression for compositional data with compositional data as predictor variables with or without zero values\". Journal of Data Science, 17(1): 219--238. <doi:10.6339/JDS.201901_17(1).0010>. \n\t\t         i) Tsagris M. and Stewart C. (2020). \"A folded model for compositional data analysis\". Australian and New Zealand Journal of Statistics, 62(2): 249--277. <doi:10.1111/anzs.12289>. \n\t\t         j) Alenazi A.A. (2022). \"f-divergence regression models for compositional data\". Pakistan Journal of Statistics and Operation Research, 18(4): 867--882. <doi:10.18187/pjsor.v18i4.3969>.\n\t\t         k) Tsagris M. and Stewart C. (2022). \"A Review of Flexible Transformations for Modeling Compositional Data\". In Advances and Innovations in Statistics and Data Science, pp. 225--234. <doi:10.1007/978-3-031-08329-7_10>.\n\t\t         l) Alenazi A. (2023). \"A review of compositional data analysis and recent advances\". Communications in Statistics--Theory and Methods, 52(16): 5535--5567. <doi:10.1080/03610926.2021.2014890>.\n\t\t\t       m) Tsagris M., Alenazi A. and Stewart C. (2023). \"Flexible non-parametric regression models for compositional response data with zeros\". Statistics and Computing, 33(106). <doi:10.1007/s11222-023-10277-5>. \n\t\t\t       n) Tsagris. M. (2025). \"Constrained least squares simplicial-simplicial regression\". Statistics and Computing, 35(27). <doi:10.1007/s11222-024-10560-z>.\n\t\t\t       o) Sevinc V. and Tsagris. M. (2024). \"Energy Based Equality of Distributions Testing for Compositional Data\". <doi:10.48550/arXiv.2412.05199>.\n\t\t\t       p) Tsagris M. (2025). \"Scalable approximation of the transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares\". <doi:10.48550/arXiv.2511.13296>.",
    "version": "8.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Giorgos Athineou [aut],\n  Abdulaziz Alenazi [ctb],\n  Christos Adam [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Compositional",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Compositional Compositional Data Analysis Regression, classification, contour plots, hypothesis testing and fitting of distributions for compositional data are some of the functions included. We further include functions for percentages (or proportions).\n             The standard textbook for such data is John Aitchison's (1986) \"The statistical analysis of compositional data\". Relevant papers include:\n\t\t\t       a) Tsagris M.T., Preston S. and Wood A.T.A. (2011). \"A data-based power transformation for compositional data\". Fourth International International Workshop on Compositional Data Analysis. <doi:10.48550/arXiv.1106.1451>.\n\t\t\t       b) Tsagris M. (2014). \"The k-NN algorithm for compositional data: a revised approach with and without zero values present\". Journal of Data Science, 12(3): 519--534. <doi:10.6339/JDS.201407_12(3).0008>.\n\t\t\t       c) Tsagris M. (2015). \"A novel, divergence based, regression for compositional data\". Proceedings of the 28th Panhellenic Statistics Conference, 15-18 April 2015, Athens, Greece, 430--444. <doi:10.48550/arXiv.1511.07600>.\n\t\t\t       d) Tsagris M. (2015). \"Regression analysis with compositional data containing zero values\". Chilean Journal of Statistics, 6(2): 47--57. <https://soche.cl/chjs/volumes/06/02/Tsagris(2015).pdf>.\n\t\t\t       e) Tsagris M., Preston S. and Wood A.T.A. (2016). \"Improved supervised classification for compositional data using the alpha-transformation\". Journal of Classification, 33(2): 243--261. <doi:10.1007/s00357-016-9207-5>. \n\t\t\t       f) Tsagris M., Preston S. and Wood A.T.A. (2017). \"Nonparametric hypothesis testing for equality of means on the simplex\". Journal of Statistical Computation and Simulation, 87(2): 406--422. <doi:10.1080/00949655.2016.1216554>. \n\t\t\t       g) Tsagris M. and Stewart C. (2018). \"A Dirichlet regression model for compositional data with zeros\". Lobachevskii Journal of Mathematics, 39(3): 398--412. <doi:10.1134/S1995080218030198>. \n\t\t\t       h) Alenazi A. (2019). \"Regression for compositional data with compositional data as predictor variables with or without zero values\". Journal of Data Science, 17(1): 219--238. <doi:10.6339/JDS.201901_17(1).0010>. \n\t\t         i) Tsagris M. and Stewart C. (2020). \"A folded model for compositional data analysis\". Australian and New Zealand Journal of Statistics, 62(2): 249--277. <doi:10.1111/anzs.12289>. \n\t\t         j) Alenazi A.A. (2022). \"f-divergence regression models for compositional data\". Pakistan Journal of Statistics and Operation Research, 18(4): 867--882. <doi:10.18187/pjsor.v18i4.3969>.\n\t\t         k) Tsagris M. and Stewart C. (2022). \"A Review of Flexible Transformations for Modeling Compositional Data\". In Advances and Innovations in Statistics and Data Science, pp. 225--234. <doi:10.1007/978-3-031-08329-7_10>.\n\t\t         l) Alenazi A. (2023). \"A review of compositional data analysis and recent advances\". Communications in Statistics--Theory and Methods, 52(16): 5535--5567. <doi:10.1080/03610926.2021.2014890>.\n\t\t\t       m) Tsagris M., Alenazi A. and Stewart C. (2023). \"Flexible non-parametric regression models for compositional response data with zeros\". Statistics and Computing, 33(106). <doi:10.1007/s11222-023-10277-5>. \n\t\t\t       n) Tsagris. M. (2025). \"Constrained least squares simplicial-simplicial regression\". Statistics and Computing, 35(27). <doi:10.1007/s11222-024-10560-z>.\n\t\t\t       o) Sevinc V. and Tsagris. M. (2024). \"Energy Based Equality of Distributions Testing for Compositional Data\". <doi:10.48550/arXiv.2412.05199>.\n\t\t\t       p) Tsagris M. (2025). \"Scalable approximation of the transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares\". <doi:10.48550/arXiv.2511.13296>.  "
  },
  {
    "id": 2589,
    "package_name": "ConcordanceTest",
    "title": "An Alternative to the Kruskal-Wallis Based on the Kendall Tau\nDistance",
    "description": "The Concordance Test is a non-parametric method for testing whether two o more samples originate from the same distribution. It extends the Kendall Tau correlation coefficient when there are only two groups. For details, see Alcaraz J., Anton-Sanchez L., Monge J.F. (2022) The Concordance Test, an Alternative to Kruskal-Wallis Based on the Kendall-tau Distance: An R Package. The R Journal 14, 26\u201353 <doi:10.32614/RJ-2022-039>.",
    "version": "1.0.3",
    "maintainer": "Laura Anton-Sanchez <l.anton@umh.es>",
    "author": "Javier Alcaraz [aut],\n  Laura Anton-Sanchez [aut, cre],\n  Juan Francisco Monge [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ConcordanceTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ConcordanceTest An Alternative to the Kruskal-Wallis Based on the Kendall Tau\nDistance The Concordance Test is a non-parametric method for testing whether two o more samples originate from the same distribution. It extends the Kendall Tau correlation coefficient when there are only two groups. For details, see Alcaraz J., Anton-Sanchez L., Monge J.F. (2022) The Concordance Test, an Alternative to Kruskal-Wallis Based on the Kendall-tau Distance: An R Package. The R Journal 14, 26\u201353 <doi:10.32614/RJ-2022-039>.  "
  },
  {
    "id": 2634,
    "package_name": "CoreMicrobiomeR",
    "title": "Identification of Core Microbiome",
    "description": "The Core Microbiome refers to the group of microorganisms that are consistently present in a particular environment, habitat, or host species. These microorganisms play a crucial role in the functioning and stability of that ecosystem. Identifying these microorganisms can contribute to the emerging field of personalized medicine. The 'CoreMicrobiomeR' is designed to facilitate the identification, statistical testing, and visualization of this group of microorganisms.This package offers three key functions to analyze and visualize microbial community data. This package has been developed based on the research papers published by Pereira et al.(2018) <doi:10.1186/s12864-018-4637-6> and Beule L, Karlovsky P. (2020) <doi:10.7717/peerj.9593>.",
    "version": "0.1.0",
    "maintainer": "Mohammad Samir Farooqi <samirfarooqi8@gmail.com>",
    "author": "Sorna A M [aut],\n  Mohammad Samir Farooqi [aut, cre],\n  Dwijesh Chandra Mishra [aut],\n  Krishna Kumar Chaturvedi [aut],\n  Anu Sharma [aut],\n  Prawin Arya [aut],\n  Sudhir Srivastava [aut],\n  Sharanbasappa [aut],\n  Girish Kumar Jha [aut],\n  Kabilan S [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CoreMicrobiomeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CoreMicrobiomeR Identification of Core Microbiome The Core Microbiome refers to the group of microorganisms that are consistently present in a particular environment, habitat, or host species. These microorganisms play a crucial role in the functioning and stability of that ecosystem. Identifying these microorganisms can contribute to the emerging field of personalized medicine. The 'CoreMicrobiomeR' is designed to facilitate the identification, statistical testing, and visualization of this group of microorganisms.This package offers three key functions to analyze and visualize microbial community data. This package has been developed based on the research papers published by Pereira et al.(2018) <doi:10.1186/s12864-018-4637-6> and Beule L, Karlovsky P. (2020) <doi:10.7717/peerj.9593>.  "
  },
  {
    "id": 2648,
    "package_name": "CovSel",
    "title": "Model-Free Covariate Selection",
    "description": "Model-free selection of covariates under unconfoundedness for situations where the parameter of interest is an average causal effect. This package is based on  model-free backward elimination algorithms proposed in de Luna, Waernbaum and Richardson (2011). Marginal co-ordinate hypothesis testing is used in situations where all covariates are continuous while kernel-based smoothing appropriate for mixed data is used otherwise.",
    "version": "1.2.2",
    "maintainer": "Jenny H\u00e4ggstr\u00f6m <jenny.haggstrom@umu.se>",
    "author": "Jenny H\u00e4ggstr\u00f6m [aut, cre],\n  Emma Persson [aut],\n  Sandy Weisberg [aut] (Author of functions originating from the package\n    dr version 3.0.10.)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CovSel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CovSel Model-Free Covariate Selection Model-free selection of covariates under unconfoundedness for situations where the parameter of interest is an average causal effect. This package is based on  model-free backward elimination algorithms proposed in de Luna, Waernbaum and Richardson (2011). Marginal co-ordinate hypothesis testing is used in situations where all covariates are continuous while kernel-based smoothing appropriate for mixed data is used otherwise.  "
  },
  {
    "id": 2654,
    "package_name": "CoxMK",
    "title": "A Model-X Knockoff Method for Genome-Wide Survival Association\nAnalysis",
    "description": "A genome-wide survival framework that integrates sequential conditional independent tuples and saddlepoint approximation method, to provide SNP-level false discovery rate control while improving power, particularly for biobank-scale survival analyses with low event rates. The method is based on model-X knockoffs as described in Barber and Candes (2015) <doi:10.1214/15-AOS1337> and fast survival analysis methods from Bi et al. (2020) <doi:10.1016/j.ajhg.2020.06.003>. A shrinkage algorithmic leveraging accelerates multiple knockoffs generation in large genetic cohorts. This CRAN version uses standard Cox regression for association testing. For enhanced performance on very large datasets, users may optionally install the 'SPACox' package from GitHub which provides saddlepoint approximation methods for survival analysis.",
    "version": "0.1.1",
    "maintainer": "Yang Chen <yangchen5@stu.scu.edu.cn>",
    "author": "Yang Chen [aut, cre],\n  Contributors [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CoxMK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CoxMK A Model-X Knockoff Method for Genome-Wide Survival Association\nAnalysis A genome-wide survival framework that integrates sequential conditional independent tuples and saddlepoint approximation method, to provide SNP-level false discovery rate control while improving power, particularly for biobank-scale survival analyses with low event rates. The method is based on model-X knockoffs as described in Barber and Candes (2015) <doi:10.1214/15-AOS1337> and fast survival analysis methods from Bi et al. (2020) <doi:10.1016/j.ajhg.2020.06.003>. A shrinkage algorithmic leveraging accelerates multiple knockoffs generation in large genetic cohorts. This CRAN version uses standard Cox regression for association testing. For enhanced performance on very large datasets, users may optionally install the 'SPACox' package from GitHub which provides saddlepoint approximation methods for survival analysis.  "
  },
  {
    "id": 2673,
    "package_name": "CsChange",
    "title": "Testing for Change in C-Statistic",
    "description": "Calculate the confidence interval and p value for change in C-statistic. The adjusted C-statistic is calculated by using formula as \"Somers' Dxy rank correlation\"/2+0.5. The confidence interval was calculated by using the bootstrap method. The p value was calculated by using the Z testing method. Please refer to the article of Peter Ganz et al. (2016) <doi:10.1001/jama.2016.5951>.",
    "version": "0.1.7",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "author": "Zhicheng Du, Yuantao Hao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CsChange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CsChange Testing for Change in C-Statistic Calculate the confidence interval and p value for change in C-statistic. The adjusted C-statistic is calculated by using formula as \"Somers' Dxy rank correlation\"/2+0.5. The confidence interval was calculated by using the bootstrap method. The p value was calculated by using the Z testing method. Please refer to the article of Peter Ganz et al. (2016) <doi:10.1001/jama.2016.5951>.  "
  },
  {
    "id": 2683,
    "package_name": "CvmortalityMult",
    "title": "Cross-Validation for Multi-Population Mortality Models",
    "description": "Implementation of cross-validation method for testing the forecasting accuracy of several multi-population mortality models. The family of multi-population includes several multi-population mortality models proposed through the actuarial and demography literature. The package includes functions for fitting and forecast the mortality rates of several populations. Additionally, we include functions for testing the forecasting accuracy of different multi-population models.\n  References, <https://journal.r-project.org/articles/RJ-2025-018/>.\n  Atance, D., Debon, A., and Navarro, E. (2020) <doi:10.3390/math8091550>.\n  Bergmeir, C. & Benitez, J.M. (2012) <doi:10.1016/j.ins.2011.12.028>.\n  Debon, A., Montes, F., & Martinez-Ruiz, F. (2011) <doi:10.1007/s13385-011-0043-z>.\n  Lee, R.D. & Carter, L.R. (1992) <doi:10.1080/01621459.1992.10475265>.\n  Russolillo, M., Giordano, G., & Haberman, S. (2011) <doi:10.1080/03461231003611933>.\n  Santolino, M. (2023) <doi:10.3390/risks11100170>.",
    "version": "1.1.1",
    "maintainer": "David Atance <david.atance@uah.es>",
    "author": "David Atance [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5860-0584>),\n  Ana Deb\u00f3n [aut] (ORCID: <https://orcid.org/0000-0002-5116-289X>)",
    "url": "https://github.com/davidAtance/CvmortalityMult",
    "bug_reports": "https://github.com/davidAtance/CvmortalityMult/issues",
    "repository": "https://cran.r-project.org/package=CvmortalityMult",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CvmortalityMult Cross-Validation for Multi-Population Mortality Models Implementation of cross-validation method for testing the forecasting accuracy of several multi-population mortality models. The family of multi-population includes several multi-population mortality models proposed through the actuarial and demography literature. The package includes functions for fitting and forecast the mortality rates of several populations. Additionally, we include functions for testing the forecasting accuracy of different multi-population models.\n  References, <https://journal.r-project.org/articles/RJ-2025-018/>.\n  Atance, D., Debon, A., and Navarro, E. (2020) <doi:10.3390/math8091550>.\n  Bergmeir, C. & Benitez, J.M. (2012) <doi:10.1016/j.ins.2011.12.028>.\n  Debon, A., Montes, F., & Martinez-Ruiz, F. (2011) <doi:10.1007/s13385-011-0043-z>.\n  Lee, R.D. & Carter, L.R. (1992) <doi:10.1080/01621459.1992.10475265>.\n  Russolillo, M., Giordano, G., & Haberman, S. (2011) <doi:10.1080/03461231003611933>.\n  Santolino, M. (2023) <doi:10.3390/risks11100170>.  "
  },
  {
    "id": 2686,
    "package_name": "CytoProfile",
    "title": "Cytokine Profiling Analysis Tool",
    "description": "Provides comprehensive cytokine profiling analysis through quality control using biologically meaningful cutoffs on raw cytokine measurements and by testing for distributional symmetry to recommend appropriate transformations. Offers exploratory data analysis with summary statistics, enhanced boxplots, and barplots, along with univariate and multivariate analytical capabilities for in-depth cytokine profiling such as Principal Component Analysis based on Andrzej Ma\u0107kiewicz and Waldemar Ratajczak (1993) <doi:10.1016/0098-3004(93)90090-R>, Sparse Partial Least Squares Discriminant Analysis based on L\u00ea Cao K-A, Boitard S, and Besse P (2011) <doi:10.1186/1471-2105-12-253>, Random Forest based on Breiman, L. (2001) <doi:10.1023/A:1010933404324>, and Extreme Gradient Boosting based on Tianqi Chen and Carlos Guestrin (2016) <doi:10.1145/2939672.2939785>.",
    "version": "0.2.3",
    "maintainer": "Shubh Saraswat <shubh.saraswat00@gmail.com>",
    "author": "Shubh Saraswat [cre, aut, cph] (ORCID:\n    <https://orcid.org/0009-0009-2359-1484>),\n  Xiaohua Douglas Zhang [aut] (ORCID:\n    <https://orcid.org/0000-0002-2486-7931>)",
    "url": "https://github.com/saraswatsh/CytoProfile,\nhttps://cytoprofile.cytokineprofile.org/",
    "bug_reports": "https://github.com/saraswatsh/CytoProfile/issues",
    "repository": "https://cran.r-project.org/package=CytoProfile",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CytoProfile Cytokine Profiling Analysis Tool Provides comprehensive cytokine profiling analysis through quality control using biologically meaningful cutoffs on raw cytokine measurements and by testing for distributional symmetry to recommend appropriate transformations. Offers exploratory data analysis with summary statistics, enhanced boxplots, and barplots, along with univariate and multivariate analytical capabilities for in-depth cytokine profiling such as Principal Component Analysis based on Andrzej Ma\u0107kiewicz and Waldemar Ratajczak (1993) <doi:10.1016/0098-3004(93)90090-R>, Sparse Partial Least Squares Discriminant Analysis based on L\u00ea Cao K-A, Boitard S, and Besse P (2011) <doi:10.1186/1471-2105-12-253>, Random Forest based on Breiman, L. (2001) <doi:10.1023/A:1010933404324>, and Extreme Gradient Boosting based on Tianqi Chen and Carlos Guestrin (2016) <doi:10.1145/2939672.2939785>.  "
  },
  {
    "id": 2712,
    "package_name": "DBItest",
    "title": "Testing DBI Backends",
    "description": "A helper that tests DBI back ends for conformity to the\n    interface.",
    "version": "1.8.2",
    "maintainer": "Kirill M\u00fcller <kirill@cynkra.com>",
    "author": "Kirill M\u00fcller [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1416-3412>),\n  RStudio [cph],\n  R Consortium [fnd]",
    "url": "https://dbitest.r-dbi.org, https://github.com/r-dbi/DBItest",
    "bug_reports": "https://github.com/r-dbi/DBItest/issues",
    "repository": "https://cran.r-project.org/package=DBItest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DBItest Testing DBI Backends A helper that tests DBI back ends for conformity to the\n    interface.  "
  },
  {
    "id": 2714,
    "package_name": "DBNMFrank",
    "title": "Rank Selection for Non-Negative Matrix Factorization",
    "description": "Given the non-negative data and its distribution, the package estimates the rank parameter for Non-negative Matrix Factorization. The method is based on hypothesis testing, using a deconvolved bootstrap distribution to assess the significance level accurately despite the large amount of optimization error. The distribution of the non-negative data can be either Normal distributed or Poisson distributed.",
    "version": "0.1.0",
    "maintainer": "Yun Cai <Yun.Cai@dal.ca>",
    "author": "Yun Cai [aut, cre],\n  Hong Gu [aut],\n  Tobias Kenney [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DBNMFrank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DBNMFrank Rank Selection for Non-Negative Matrix Factorization Given the non-negative data and its distribution, the package estimates the rank parameter for Non-negative Matrix Factorization. The method is based on hypothesis testing, using a deconvolved bootstrap distribution to assess the significance level accurately despite the large amount of optimization error. The distribution of the non-negative data can be either Normal distributed or Poisson distributed.  "
  },
  {
    "id": 2746,
    "package_name": "DEHOGT",
    "title": "Differentially Expressed Heterogeneous Overdispersion Gene Test\nfor Count Data",
    "description": "Implements a generalized linear model approach for detecting\n    differentially expressed genes across treatment groups in count data. The\n    package supports both quasi-Poisson and negative binomial models to handle\n    over-dispersion, ensuring robust identification of differential expression.\n    It allows for the inclusion of treatment effects and gene-wise covariates, \n    as well as normalization factors for accurate scaling across samples.\n    Additionally, it incorporates statistical significance testing with\n    options for p-value adjustment and log2 fold range thresholds,\n    making it suitable for RNA-seq analysis as described in by \n    Xu et al., (2024) <doi:10.1371/journal.pone.0300565>.",
    "version": "0.99.0",
    "maintainer": "Arlina Shen <ahshen24@berkeley.edu>",
    "author": "Qi Xu [aut],\n  Arlina Shen [cre] (ORCID: <https://orcid.org/0009-0008-5330-6659>),\n  Yubai Yuan [ctb],\n  Annie Qu [ctb]",
    "url": "https://github.com/ahshen26/DEHOGT",
    "bug_reports": "https://github.com/ahshen26/DEHOGT/issues",
    "repository": "https://cran.r-project.org/package=DEHOGT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DEHOGT Differentially Expressed Heterogeneous Overdispersion Gene Test\nfor Count Data Implements a generalized linear model approach for detecting\n    differentially expressed genes across treatment groups in count data. The\n    package supports both quasi-Poisson and negative binomial models to handle\n    over-dispersion, ensuring robust identification of differential expression.\n    It allows for the inclusion of treatment effects and gene-wise covariates, \n    as well as normalization factors for accurate scaling across samples.\n    Additionally, it incorporates statistical significance testing with\n    options for p-value adjustment and log2 fold range thresholds,\n    making it suitable for RNA-seq analysis as described in by \n    Xu et al., (2024) <doi:10.1371/journal.pone.0300565>.  "
  },
  {
    "id": 2764,
    "package_name": "DFBA",
    "title": "Distribution-Free Bayesian Analysis",
    "description": "A set of functions to perform distribution-free Bayesian analyses. \n             Included are Bayesian analogues to the frequentist Mann-Whitney U \n             test, the Wilcoxon Signed-Ranks test, Kendall's Tau Rank \n             Correlation Coefficient, Goodman and Kruskal's Gamma, McNemar's\n             Test, the binomial test, the sign test, the median test, as well as \n             distribution-free methods for testing contrasts among condition and \n             for computing Bayes factors for hypotheses. The package also\n             includes procedures to estimate the power of distribution-free\n             Bayesian tests based on data simulations using various probability \n             models for the data. The set of functions provide data analysts \n             with a set of Bayesian procedures that avoids requiring parametric \n             assumptions about measurement error and is robust to problem of \n             extreme outlier scores.",
    "version": "0.1.0",
    "maintainer": "Daniel H. Barch <daniel.barch@tufts.edu>",
    "author": "Daniel H. Barch [aut, cre],\n  Richard A. Chechile [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DFBA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DFBA Distribution-Free Bayesian Analysis A set of functions to perform distribution-free Bayesian analyses. \n             Included are Bayesian analogues to the frequentist Mann-Whitney U \n             test, the Wilcoxon Signed-Ranks test, Kendall's Tau Rank \n             Correlation Coefficient, Goodman and Kruskal's Gamma, McNemar's\n             Test, the binomial test, the sign test, the median test, as well as \n             distribution-free methods for testing contrasts among condition and \n             for computing Bayes factors for hypotheses. The package also\n             includes procedures to estimate the power of distribution-free\n             Bayesian tests based on data simulations using various probability \n             models for the data. The set of functions provide data analysts \n             with a set of Bayesian procedures that avoids requiring parametric \n             assumptions about measurement error and is robust to problem of \n             extreme outlier scores.  "
  },
  {
    "id": 2767,
    "package_name": "DGEAR",
    "title": "Differential Gene Expression Analysis with R",
    "description": "Analyses gene expression data derived from experiments to detect differentially expressed genes by employing the concept of majority voting with five different statistical models. It includes functions for differential expression analysis, significance testing, etc. It simplifies the process of uncovering meaningful patterns and trends within gene expression data, aiding researchers in downstream analysis. Boyer, R.S., Moore, J.S. (1991) <doi:10.1007/978-94-011-3488-0_5>.",
    "version": "0.1.4",
    "maintainer": "Koushik Bardhan <koushikbardhan2000@gmail.com>",
    "author": "Koushik Bardhan [aut, cre, ctb] (ORCID:\n    <https://orcid.org/0009-0002-8846-8347>),\n  Chiranjib Sarkar [aut, ths] (ORCID:\n    <https://orcid.org/0000-0003-1536-7449>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DGEAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DGEAR Differential Gene Expression Analysis with R Analyses gene expression data derived from experiments to detect differentially expressed genes by employing the concept of majority voting with five different statistical models. It includes functions for differential expression analysis, significance testing, etc. It simplifies the process of uncovering meaningful patterns and trends within gene expression data, aiding researchers in downstream analysis. Boyer, R.S., Moore, J.S. (1991) <doi:10.1007/978-94-011-3488-0_5>.  "
  },
  {
    "id": 2770,
    "package_name": "DGLMExtPois",
    "title": "Double Generalized Linear Models Extending Poisson Regression",
    "description": "Model estimation, dispersion testing and diagnosis of hyper-Poisson\n    Saez-Castillo, A.J. and Conde-Sanchez, A. (2013) \n    <doi:10.1016/j.csda.2012.12.009> and Conway-Maxwell-Poisson Huang, A. (2017)\n    regression models.",
    "version": "0.2.4",
    "maintainer": "Francisco Martinez <fmartin@ujaen.es>",
    "author": "Antonio Jose Saez-Castillo [aut],\n  Antonio Conde-Sanchez [aut],\n  Francisco Martinez [aut, cre]",
    "url": "https://github.com/franciscomartinezdelrio/DGLMExtPois",
    "bug_reports": "https://github.com/franciscomartinezdelrio/DGLMExtPois/issues",
    "repository": "https://cran.r-project.org/package=DGLMExtPois",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DGLMExtPois Double Generalized Linear Models Extending Poisson Regression Model estimation, dispersion testing and diagnosis of hyper-Poisson\n    Saez-Castillo, A.J. and Conde-Sanchez, A. (2013) \n    <doi:10.1016/j.csda.2012.12.009> and Conway-Maxwell-Poisson Huang, A. (2017)\n    regression models.  "
  },
  {
    "id": 2818,
    "package_name": "DNAseqtest",
    "title": "Generating and Testing DNA Sequences",
    "description": "Generates DNA sequences based on Markov model techniques for matched sequences. This can be generalized to several sequences. The sequences (taxa) are then arranged in an evolutionary tree (phylogenetic tree) depicting how taxa diverge from their common ancestors. This gives the tests and estimation methods for the parameters of different models. Standard phylogenetic methods assume stationarity, homogeneity and reversibility for the Markov processes, and  often impose further restrictions on the parameters.",
    "version": "1.0",
    "maintainer": "Hasinur Rahaman Khan <hasinurkhan@gmail.com>",
    "author": "Faisal Ababneh, John Robinson, Lars S Jermiin and Hasinur Rahaman Khan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DNAseqtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DNAseqtest Generating and Testing DNA Sequences Generates DNA sequences based on Markov model techniques for matched sequences. This can be generalized to several sequences. The sequences (taxa) are then arranged in an evolutionary tree (phylogenetic tree) depicting how taxa diverge from their common ancestors. This gives the tests and estimation methods for the parameters of different models. Standard phylogenetic methods assume stationarity, homogeneity and reversibility for the Markov processes, and  often impose further restrictions on the parameters.  "
  },
  {
    "id": 2822,
    "package_name": "DNetFinder",
    "title": "Estimating Differential Networks under Semiparametric Gaussian\nGraphical Models",
    "description": "Provides a modified hierarchical test (Liu (2017) <doi:10.1214/17-AOS1539>) for detecting the structural difference between two Semiparametric Gaussian graphical models. The multiple testing procedure asymptotically controls the false discovery rate (FDR) at a user-specified level. To construct the test statistic, a truncated estimator is used to approximate the transformation functions and two R functions including lassoGGM() and lassoNPN() are provided to compute the lasso estimates of the regression coefficients. ",
    "version": "1.1",
    "maintainer": "Qingyang Zhang <qz008@uark.edu>",
    "author": "Qingyang Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DNetFinder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DNetFinder Estimating Differential Networks under Semiparametric Gaussian\nGraphical Models Provides a modified hierarchical test (Liu (2017) <doi:10.1214/17-AOS1539>) for detecting the structural difference between two Semiparametric Gaussian graphical models. The multiple testing procedure asymptotically controls the false discovery rate (FDR) at a user-specified level. To construct the test statistic, a truncated estimator is used to approximate the transformation functions and two R functions including lassoGGM() and lassoNPN() are provided to compute the lasso estimates of the regression coefficients.   "
  },
  {
    "id": 2845,
    "package_name": "DRPT",
    "title": "Density Ratio Permutation Test",
    "description": "Implementation of the Density Ratio Permutation Test for testing the goodness-of-fit of a hypothesised ratio of two densities, as described in Bordino and Berrett (2025) <doi:10.48550/arXiv.2505.24529>.",
    "version": "1.1",
    "maintainer": "Alberto Bordino <alberto.bordino@warwick.ac.uk>",
    "author": "Alberto Bordino [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-1556-6973>),\n  Thomas B. Berrett [aut] (ORCID:\n    <https://orcid.org/0000-0002-2005-110X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DRPT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DRPT Density Ratio Permutation Test Implementation of the Density Ratio Permutation Test for testing the goodness-of-fit of a hypothesised ratio of two densities, as described in Bordino and Berrett (2025) <doi:10.48550/arXiv.2505.24529>.  "
  },
  {
    "id": 2912,
    "package_name": "DataSimilarity",
    "title": "Quantifying Similarity of Datasets and Multivariate Two- And\nk-Sample Testing",
    "description": "A collection of methods for quantifying the similarity of two or more datasets, many of which can be used for two- or k-sample testing. It provides newly implemented methods as well as wrapper functions for existing methods that enable calling many different methods in a unified framework. The methods were selected from the review and comparison of Stolte et al. (2024) <doi:10.1214/24-SS149>.",
    "version": "0.2.0",
    "maintainer": "Marieke Stolte <stolte@statistik.tu-dortmund.de>",
    "author": "Marieke Stolte [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0002-0711-6789>),\n  Luca Sauer [aut] (ORCID: <https://orcid.org/0009-0000-1086-023X>),\n  David Alvarez-Melis [ctb] (Original python implementation of OTDD,\n    <https://github.com/microsoft/otdd.git>),\n  Nabarun Deb [ctb] (Original implementation of rank-based Energy test\n    (DS), <https://github.com/NabarunD/MultiDistFree.git>),\n  Bodhisattva Sen [ctb] (Original implementation of rank-based Energy\n    test (DS), <https://github.com/NabarunD/MultiDistFree.git>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DataSimilarity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataSimilarity Quantifying Similarity of Datasets and Multivariate Two- And\nk-Sample Testing A collection of methods for quantifying the similarity of two or more datasets, many of which can be used for two- or k-sample testing. It provides newly implemented methods as well as wrapper functions for existing methods that enable calling many different methods in a unified framework. The methods were selected from the review and comparison of Stolte et al. (2024) <doi:10.1214/24-SS149>.  "
  },
  {
    "id": 2980,
    "package_name": "DiffXTables",
    "title": "Pattern Analysis Across Contingency Tables",
    "description": "Statistical hypothesis testing of pattern heterogeneity\n    via differences in underlying distributions across multiple\n    contingency tables. Five tests are included: the comparative\n    chi-squared test (Song et al. 2014) <doi:10.1093/nar/gku086>\n    (Zhang et al. 2015) <doi:10.1093/nar/gkv358>, the Sharma-Song\n    test (Sharma et al. 2021) <doi:10.1093/bioinformatics/btab240>, \n    the heterogeneity test, the marginal-change test (Sharma et al.\n    2020) <doi:10.1145/3388440.3412485>, and the strength test\n    (Sharma et al. 2020) <doi:10.1145/3388440.3412485>. Under the\n    null hypothesis that row and column variables are statistically\n    independent and joint distributions are equal, their test\n    statistics all follow an asymptotically chi-squared distribution.\n    A comprehensive type analysis categorizes the relation among the\n    contingency tables into type null, 0, 1, and 2 (Sharma et al.\n    2020) <doi:10.1145/3388440.3412485>. They can identify\n    heterogeneous patterns that differ in either the first order\n    (marginal) or the second order (differential departure from\n    independence). Second-order differences reveal more\n    fundamental changes than first-order differences across\n    heterogeneous patterns.",
    "version": "0.1.3",
    "maintainer": "Joe Song <joemsong@cs.nmsu.edu>",
    "author": "Ruby Sharma [aut] (ORCID: <https://orcid.org/0000-0001-7774-4065>),\n  Joe Song [aut, cre] (ORCID: <https://orcid.org/0000-0002-6883-6547>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiffXTables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiffXTables Pattern Analysis Across Contingency Tables Statistical hypothesis testing of pattern heterogeneity\n    via differences in underlying distributions across multiple\n    contingency tables. Five tests are included: the comparative\n    chi-squared test (Song et al. 2014) <doi:10.1093/nar/gku086>\n    (Zhang et al. 2015) <doi:10.1093/nar/gkv358>, the Sharma-Song\n    test (Sharma et al. 2021) <doi:10.1093/bioinformatics/btab240>, \n    the heterogeneity test, the marginal-change test (Sharma et al.\n    2020) <doi:10.1145/3388440.3412485>, and the strength test\n    (Sharma et al. 2020) <doi:10.1145/3388440.3412485>. Under the\n    null hypothesis that row and column variables are statistically\n    independent and joint distributions are equal, their test\n    statistics all follow an asymptotically chi-squared distribution.\n    A comprehensive type analysis categorizes the relation among the\n    contingency tables into type null, 0, 1, and 2 (Sharma et al.\n    2020) <doi:10.1145/3388440.3412485>. They can identify\n    heterogeneous patterns that differ in either the first order\n    (marginal) or the second order (differential departure from\n    independence). Second-order differences reveal more\n    fundamental changes than first-order differences across\n    heterogeneous patterns.  "
  },
  {
    "id": 2981,
    "package_name": "DigestiveDataSets",
    "title": "A Curated Collection of Digestive System and Gastrointestinal\nDisease Datasets",
    "description": "Provides an extensive and curated collection of datasets related to the digestive system, stomach, intestines, liver, pancreas, and associated diseases. \n    This package includes clinical trials, observational studies, experimental datasets, cohort data, and case series involving gastrointestinal disorders such as gastritis, ulcers, pancreatitis, liver cirrhosis, colon cancer, colorectal conditions, Helicobacter pylori infection, irritable bowel syndrome, intestinal infections, and post-surgical outcomes.\n    The datasets support educational, clinical, and research applications in gastroenterology, public health, epidemiology, and biomedical sciences. \n    Designed for researchers, clinicians, data scientists, students, and educators interested in digestive diseases, the package facilitates reproducible analysis, modeling, and hypothesis testing using real-world and historical data.",
    "version": "0.2.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/digestivedatasets,\nhttps://lightbluetitan.github.io/digestivedatasets/",
    "bug_reports": "https://github.com/lightbluetitan/digestivedatasets/issues",
    "repository": "https://cran.r-project.org/package=DigestiveDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DigestiveDataSets A Curated Collection of Digestive System and Gastrointestinal\nDisease Datasets Provides an extensive and curated collection of datasets related to the digestive system, stomach, intestines, liver, pancreas, and associated diseases. \n    This package includes clinical trials, observational studies, experimental datasets, cohort data, and case series involving gastrointestinal disorders such as gastritis, ulcers, pancreatitis, liver cirrhosis, colon cancer, colorectal conditions, Helicobacter pylori infection, irritable bowel syndrome, intestinal infections, and post-surgical outcomes.\n    The datasets support educational, clinical, and research applications in gastroenterology, public health, epidemiology, and biomedical sciences. \n    Designed for researchers, clinicians, data scientists, students, and educators interested in digestive diseases, the package facilitates reproducible analysis, modeling, and hypothesis testing using real-world and historical data.  "
  },
  {
    "id": 2987,
    "package_name": "Directional",
    "title": "A Collection of Functions for Directional Data Analysis",
    "description": "A collection of functions for directional data (including massive data, with millions of observations) analysis. \n             Hypothesis testing, discriminant and regression analysis, MLE of distributions and more are included. \n\t\t\t The standard textbook for such data is the \"Directional Statistics\" by Mardia, K. V. and Jupp, P. E. (2000). \n\t\t\t Other references include:\n\t\t\t a) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2018). \"An elliptically symmetric angular Gaussian distribution\". Statistics and Computing 28(3): 689-697. <doi:10.1007/s11222-017-9756-4>. \n\t\t\t b) Tsagris M. and Alenazi A. (2019). \"Comparison of discriminant analysis methods on the sphere\". Communications in Statistics: Case Studies, Data Analysis and Applications 5(4):467--491. <doi:10.1080/23737484.2019.1684854>. \n\t\t\t c) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2020). \"Spherical regression models with general covariates and anisotropic errors\". Statistics and Computing 30(1): 153--165. <doi:10.1007/s11222-019-09872-2>. \n\t\t\t d) Tsagris M. and Alenazi A. (2024). \"An investigation of hypothesis testing procedures for circular and spherical mean vectors\". Communications in Statistics-Simulation and Computation, 53(3): 1387--1408. <doi:10.1080/03610918.2022.2045499>. \n\t\t\t e) Yu Z. and Huang X. (2024). A new parameterization for elliptically symmetric angular Gaussian distributions of arbitrary dimension. Electronic Journal of Statistics, 18(1): 301--334. <doi:10.1214/23-EJS2210>. \n\t\t\t f) Tsagris M. and Alzeley O. (2025). \"Circular and spherical projected Cauchy distributions: A Novel Framework for Circular and Directional Data Modeling\". Australian & New Zealand Journal of Statistics, 67(1): 77--103. <doi:10.1111/anzs.12434>. \n\t\t\t g) Tsagris M., Papastamoulis P. and Kato S. (2025). \"Directional data analysis: spherical Cauchy or Poisson kernel-based distribution\". Statistics and Computing, 35:51. <doi:10.1007/s11222-025-10583-0>.",
    "version": "7.3",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Giorgos Athineou [aut],\n  Christos Adam [aut],\n  Zehao Yu [aut],\n  Anamul Sajib [ctb],\n  Eli Amson [ctb],\n  Micah J. Waldstein [ctb],\n  Panagiotis Papastamoulis [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Directional",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Directional A Collection of Functions for Directional Data Analysis A collection of functions for directional data (including massive data, with millions of observations) analysis. \n             Hypothesis testing, discriminant and regression analysis, MLE of distributions and more are included. \n\t\t\t The standard textbook for such data is the \"Directional Statistics\" by Mardia, K. V. and Jupp, P. E. (2000). \n\t\t\t Other references include:\n\t\t\t a) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2018). \"An elliptically symmetric angular Gaussian distribution\". Statistics and Computing 28(3): 689-697. <doi:10.1007/s11222-017-9756-4>. \n\t\t\t b) Tsagris M. and Alenazi A. (2019). \"Comparison of discriminant analysis methods on the sphere\". Communications in Statistics: Case Studies, Data Analysis and Applications 5(4):467--491. <doi:10.1080/23737484.2019.1684854>. \n\t\t\t c) Paine J.P., Preston S.P., Tsagris M. and Wood A.T.A. (2020). \"Spherical regression models with general covariates and anisotropic errors\". Statistics and Computing 30(1): 153--165. <doi:10.1007/s11222-019-09872-2>. \n\t\t\t d) Tsagris M. and Alenazi A. (2024). \"An investigation of hypothesis testing procedures for circular and spherical mean vectors\". Communications in Statistics-Simulation and Computation, 53(3): 1387--1408. <doi:10.1080/03610918.2022.2045499>. \n\t\t\t e) Yu Z. and Huang X. (2024). A new parameterization for elliptically symmetric angular Gaussian distributions of arbitrary dimension. Electronic Journal of Statistics, 18(1): 301--334. <doi:10.1214/23-EJS2210>. \n\t\t\t f) Tsagris M. and Alzeley O. (2025). \"Circular and spherical projected Cauchy distributions: A Novel Framework for Circular and Directional Data Modeling\". Australian & New Zealand Journal of Statistics, 67(1): 77--103. <doi:10.1111/anzs.12434>. \n\t\t\t g) Tsagris M., Papastamoulis P. and Kato S. (2025). \"Directional data analysis: spherical Cauchy or Poisson kernel-based distribution\". Statistics and Computing, 35:51. <doi:10.1007/s11222-025-10583-0>.  "
  },
  {
    "id": 2994,
    "package_name": "DiscreteDatasets",
    "title": "Example Data Sets for Use with Discrete Statistical Tests",
    "description": "Provides several data sets for use with discrete statistical tests\n  and discrete multiple testing procedures. Some of them are also available as a\n  four-column version, so that each row represents a 2x2 table.",
    "version": "0.1.2",
    "maintainer": "Florian Junge <diso.fbmn@h-da.de>",
    "author": "Christina Kihn [aut],\n  Sebastian D\u00f6hler [aut],\n  Florian Junge [cre, aut],\n  Lukas Klein [ctb]",
    "url": "https://github.com/DISOhda/DiscreteDatasets",
    "bug_reports": "https://github.com/DISOhda/DiscreteDatasets/issues",
    "repository": "https://cran.r-project.org/package=DiscreteDatasets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiscreteDatasets Example Data Sets for Use with Discrete Statistical Tests Provides several data sets for use with discrete statistical tests\n  and discrete multiple testing procedures. Some of them are also available as a\n  four-column version, so that each row represents a 2x2 table.  "
  },
  {
    "id": 2996,
    "package_name": "DiscreteFDR",
    "title": "FDR Based Multiple Testing Procedures with Adaptation for\nDiscrete Tests",
    "description": "Implementations of the multiple testing procedures for discrete\n  tests described in the paper D\u00f6hler, Durand and Roquain (2018) \"New FDR\n  bounds for discrete and heterogeneous tests\" <doi:10.1214/18-EJS1441>. The\n  main procedures of the paper (HSU and HSD), their adaptive counterparts\n  (AHSU and AHSD), and the HBR variant are available and are coded to take as\n  input the results of a test procedure from package 'DiscreteTests', or a set\n  of observed p-values and their discrete support under their nulls. A\n  shortcut function to obtain such p-values and supports is also provided,\n  along with a wrapper allowing to apply discrete procedures directly to data.",
    "version": "2.1.0",
    "maintainer": "Florian Junge <diso.fbmn@h-da.de>",
    "author": "Sebastian D\u00f6hler [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-0321-6355>),\n  Florian Junge [aut, ctb, cre] (ORCID:\n    <https://orcid.org/0009-0001-6856-6938>),\n  Guillermo Durand [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-4056-5631>),\n  Etienne Roquain [ctb],\n  Christina Kihn [ctb]",
    "url": "https://github.com/DISOhda/DiscreteFDR",
    "bug_reports": "https://github.com/DISOhda/DiscreteFDR/issues",
    "repository": "https://cran.r-project.org/package=DiscreteFDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiscreteFDR FDR Based Multiple Testing Procedures with Adaptation for\nDiscrete Tests Implementations of the multiple testing procedures for discrete\n  tests described in the paper D\u00f6hler, Durand and Roquain (2018) \"New FDR\n  bounds for discrete and heterogeneous tests\" <doi:10.1214/18-EJS1441>. The\n  main procedures of the paper (HSU and HSD), their adaptive counterparts\n  (AHSU and AHSD), and the HBR variant are available and are coded to take as\n  input the results of a test procedure from package 'DiscreteTests', or a set\n  of observed p-values and their discrete support under their nulls. A\n  shortcut function to obtain such p-values and supports is also provided,\n  along with a wrapper allowing to apply discrete procedures directly to data.  "
  },
  {
    "id": 2997,
    "package_name": "DiscreteFWER",
    "title": "FWER-Based Multiple Testing Procedures with Adaptation for\nDiscrete Tests",
    "description": "Implementations of several multiple testing procedures that control\n    the family-wise error rate (FWER) designed specifically for discrete tests.\n    Included are discrete adaptations of the Bonferroni, Holm, Hochberg and\n    \u0160id\u00e1k procedures as described in the papers D\u00f6hler (2010) \"Validation of\n    credit default probabilities using multiple-testing procedures\"\n    <doi:10.21314/JRMV.2010.062> and Zhu & Guo (2019) \"Family-Wise Error Rate\n    Controlling Procedures for Discrete Data\" \n    <doi:10.1080/19466315.2019.1654912>. The main procedures of this package\n    take as input the results of a test procedure from package 'DiscreteTests'\n    or a set of observed p-values and their discrete support under their nulls.\n    A shortcut function to apply discrete procedures directly to data is also\n    provided.",
    "version": "1.0.0",
    "maintainer": "Florian Junge <diso.fbmn@h-da.de>",
    "author": "Sebastian D\u00f6hler [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0002-0321-6355>),\n  Florian Junge [aut, ctb, cre] (ORCID:\n    <https://orcid.org/0009-0001-6856-6938>)",
    "url": "https://github.com/DISOhda/DiscreteFWER",
    "bug_reports": "https://github.com/DISOhda/DiscreteFWER/issues",
    "repository": "https://cran.r-project.org/package=DiscreteFWER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiscreteFWER FWER-Based Multiple Testing Procedures with Adaptation for\nDiscrete Tests Implementations of several multiple testing procedures that control\n    the family-wise error rate (FWER) designed specifically for discrete tests.\n    Included are discrete adaptations of the Bonferroni, Holm, Hochberg and\n    \u0160id\u00e1k procedures as described in the papers D\u00f6hler (2010) \"Validation of\n    credit default probabilities using multiple-testing procedures\"\n    <doi:10.21314/JRMV.2010.062> and Zhu & Guo (2019) \"Family-Wise Error Rate\n    Controlling Procedures for Discrete Data\" \n    <doi:10.1080/19466315.2019.1654912>. The main procedures of this package\n    take as input the results of a test procedure from package 'DiscreteTests'\n    or a set of observed p-values and their discrete support under their nulls.\n    A shortcut function to apply discrete procedures directly to data is also\n    provided.  "
  },
  {
    "id": 3001,
    "package_name": "DiscreteQvalue",
    "title": "Improved q-Values for Discrete Uniform and Homogeneous Tests",
    "description": "We consider a multiple testing procedure used in many modern applications which is the q-value method proposed by Storey and Tibshirani (2003), <doi:10.1073/pnas.1530509100>. The q-value method is based on the false discovery rate (FDR), hence versions of the q-value method can be defined depending on which estimator of the proportion of true null hypotheses, p0, is plugged in the FDR estimator. We implement the q-value method based on two classical pi0 estimators, and furthermore, we propose and implement three versions of the q-value method for homogeneous discrete uniform P-values based on pi0 estimators which take into account the discrete distribution of the P-values.",
    "version": "1.1",
    "maintainer": "Marta Cousido Rocha <martacousido@uvigo.es>",
    "author": "Marta Cousido Rocha [aut, cre],\n  Jos\u00e9 Carlos Soage Gonz\u00e1lez [ctr],\n  Jacobo de U\u00f1a \u00c1lvarez [aut, ths],\n  Sebastian D\u00f6hler [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DiscreteQvalue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DiscreteQvalue Improved q-Values for Discrete Uniform and Homogeneous Tests We consider a multiple testing procedure used in many modern applications which is the q-value method proposed by Storey and Tibshirani (2003), <doi:10.1073/pnas.1530509100>. The q-value method is based on the false discovery rate (FDR), hence versions of the q-value method can be defined depending on which estimator of the proportion of true null hypotheses, p0, is plugged in the FDR estimator. We implement the q-value method based on two classical pi0 estimators, and furthermore, we propose and implement three versions of the q-value method for homogeneous discrete uniform P-values based on pi0 estimators which take into account the discrete distribution of the P-values.  "
  },
  {
    "id": 3012,
    "package_name": "DistributionUtils",
    "title": "Distribution Utilities",
    "description": "Utilities are provided which are of use in the\n    packages I have developed for dealing with\n    distributions. Currently these packages are GeneralizedHyperbolic,\n    VarianceGamma, and SkewHyperbolic and NormalLaplace. Each of these\n    packages requires DistributionUtils. Functionality includes sample\n    skewness and kurtosis, log-histogram, tail plots, moments by\n    integration, changing the point about which a moment is\n    calculated, functions for testing distributions using inversion\n    tests and the Massart inequality. Also includes an implementation\n    of the incomplete Bessel K function.",
    "version": "0.6-2",
    "maintainer": "David Scott <d.scott@auckland.ac.nz>",
    "author": "David Scott [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DistributionUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DistributionUtils Distribution Utilities Utilities are provided which are of use in the\n    packages I have developed for dealing with\n    distributions. Currently these packages are GeneralizedHyperbolic,\n    VarianceGamma, and SkewHyperbolic and NormalLaplace. Each of these\n    packages requires DistributionUtils. Functionality includes sample\n    skewness and kurtosis, log-histogram, tail plots, moments by\n    integration, changing the point about which a moment is\n    calculated, functions for testing distributions using inversion\n    tests and the Massart inequality. Also includes an implementation\n    of the incomplete Bessel K function.  "
  },
  {
    "id": 3022,
    "package_name": "Docovt",
    "title": "Distributed Online Covariance Matrix Tests",
    "description": "Distributed Online Covariance Matrix Tests 'Docovt' is a powerful tool designed to efficiently process and analyze distributed datasets. It enables users to perform covariance matrix tests in an online, distributed manner, making it highly suitable for large-scale data analysis. By leveraging advanced computational techniques, 'Docovt' ensures robust and scalable solutions for statistical analysis, particularly in scenarios where data is dispersed across multiple nodes or sources. This package is ideal for researchers and practitioners working with high-dimensional data, providing a flexible and efficient framework for covariance matrix estimation and hypothesis testing. The philosophy of 'Docovt' is described in Guo G.(2025) <doi:10.1016/j.physa.2024.130308>.",
    "version": "0.3",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "author": "Guangbao Guo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4115-6218>),\n  Congfan Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Docovt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Docovt Distributed Online Covariance Matrix Tests Distributed Online Covariance Matrix Tests 'Docovt' is a powerful tool designed to efficiently process and analyze distributed datasets. It enables users to perform covariance matrix tests in an online, distributed manner, making it highly suitable for large-scale data analysis. By leveraging advanced computational techniques, 'Docovt' ensures robust and scalable solutions for statistical analysis, particularly in scenarios where data is dispersed across multiple nodes or sources. This package is ideal for researchers and practitioners working with high-dimensional data, providing a flexible and efficient framework for covariance matrix estimation and hypothesis testing. The philosophy of 'Docovt' is described in Guo G.(2025) <doi:10.1016/j.physa.2024.130308>.  "
  },
  {
    "id": 3024,
    "package_name": "Domean",
    "title": "Distributed Online Mean Tests",
    "description": "Distributed Online Mean Tests  is a powerful tool designed to efficiently process and analyze distributed datasets. It enables users to perform mean tests in an online, distributed manner, making it highly suitable for large-scale data analysis. By leveraging advanced computational techniques, 'Domean' ensures robust and scalable solutions for statistical analysis, particularly in scenarios where data is dispersed across multiple nodes or sources. This package is ideal for researchers and practitioners working with high-dimensional data, providing a flexible and efficient framework for mean  testing. The philosophy of 'Domean' is described in Guo G.(2025) <doi:10.1016/j.physa.2024.130308>.",
    "version": "0.1",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "author": "Guangbao Guo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4115-6218>),\n  Qianwen Liu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Domean",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Domean Distributed Online Mean Tests Distributed Online Mean Tests  is a powerful tool designed to efficiently process and analyze distributed datasets. It enables users to perform mean tests in an online, distributed manner, making it highly suitable for large-scale data analysis. By leveraging advanced computational techniques, 'Domean' ensures robust and scalable solutions for statistical analysis, particularly in scenarios where data is dispersed across multiple nodes or sources. This package is ideal for researchers and practitioners working with high-dimensional data, providing a flexible and efficient framework for mean  testing. The philosophy of 'Domean' is described in Guo G.(2025) <doi:10.1016/j.physa.2024.130308>.  "
  },
  {
    "id": 3030,
    "package_name": "Dowd",
    "title": "Functions Ported from 'MMR2' Toolbox Offered in Kevin Dowd's\nBook Measuring Market Risk",
    "description": "'Kevin Dowd's' book Measuring Market Risk is a widely read book \n          in the area of risk measurement by students and \n          practitioners alike. As he claims, 'MATLAB' indeed might have been the most \n          suitable language when he originally wrote the functions, but,\n          with growing popularity of R it is not entirely \n\t  valid. As 'Dowd's' code was not intended to be error free and were mainly \n\t  for reference, some functions in this package have inherited those \n\t  errors. An attempt will be made in future releases to identify and correct \n\t  them. 'Dowd's' original code can be downloaded from www.kevindowd.org/measuring-market-risk/. \n          It should be noted that 'Dowd' offers both\n          'MMR2' and 'MMR1' toolboxes. Only 'MMR2' was ported to R. 'MMR2' is more \n          recent version of 'MMR1' toolbox and they both have mostly similar \n          function. The toolbox mainly contains different parametric and non \n\t  parametric methods for measurement of market risk as well as \n\t  backtesting risk measurement methods.",
    "version": "0.12",
    "maintainer": "Dinesh Acharya <dines.acharya@gmail.com>",
    "author": "Dinesh Acharya <dines.acharya@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Dowd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Dowd Functions Ported from 'MMR2' Toolbox Offered in Kevin Dowd's\nBook Measuring Market Risk 'Kevin Dowd's' book Measuring Market Risk is a widely read book \n          in the area of risk measurement by students and \n          practitioners alike. As he claims, 'MATLAB' indeed might have been the most \n          suitable language when he originally wrote the functions, but,\n          with growing popularity of R it is not entirely \n\t  valid. As 'Dowd's' code was not intended to be error free and were mainly \n\t  for reference, some functions in this package have inherited those \n\t  errors. An attempt will be made in future releases to identify and correct \n\t  them. 'Dowd's' original code can be downloaded from www.kevindowd.org/measuring-market-risk/. \n          It should be noted that 'Dowd' offers both\n          'MMR2' and 'MMR1' toolboxes. Only 'MMR2' was ported to R. 'MMR2' is more \n          recent version of 'MMR1' toolbox and they both have mostly similar \n          function. The toolbox mainly contains different parametric and non \n\t  parametric methods for measurement of market risk as well as \n\t  backtesting risk measurement methods.  "
  },
  {
    "id": 3065,
    "package_name": "EBPRS",
    "title": "Derive Polygenic Risk Score Based on Emprical Bayes Theory",
    "description": "EB-PRS is a novel method that leverages information for effect sizes across all the markers to improve the prediction accuracy.  No parameter tuning is needed in the method, and no external information is needed. This R-package provides the calculation of polygenic risk scores from the given training summary statistics and testing data. We can use EB-PRS to extract main information, estimate Empirical Bayes parameters, derive polygenic risk scores for  each individual in testing data, and evaluate the PRS according to AUC and predictive r2. See Song et al. (2020) <doi:10.1371/journal.pcbi.1007565> for a detailed presentation of the method.",
    "version": "2.1.0",
    "maintainer": "Shuang Song <song-s19@mails.tsinghua.edu.cn>",
    "author": "Shuang Song [aut, cre], Wei Jiang [aut], Lin Hou [aut] and Hongyu Zhao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EBPRS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EBPRS Derive Polygenic Risk Score Based on Emprical Bayes Theory EB-PRS is a novel method that leverages information for effect sizes across all the markers to improve the prediction accuracy.  No parameter tuning is needed in the method, and no external information is needed. This R-package provides the calculation of polygenic risk scores from the given training summary statistics and testing data. We can use EB-PRS to extract main information, estimate Empirical Bayes parameters, derive polygenic risk scores for  each individual in testing data, and evaluate the PRS according to AUC and predictive r2. See Song et al. (2020) <doi:10.1371/journal.pcbi.1007565> for a detailed presentation of the method.  "
  },
  {
    "id": 3093,
    "package_name": "EFDR",
    "title": "Wavelet-Based Enhanced FDR for Detecting Signals from Complete\nor Incomplete Spatially Aggregated Data",
    "description": "Enhanced False Discovery Rate (EFDR) is a tool to detect anomalies\n    in an image. The image is first transformed into the wavelet domain in\n    order to decorrelate any noise components, following which the coefficients\n    at each resolution are standardised. Statistical tests (in a multiple\n    hypothesis testing setting) are then carried out to find the anomalies. The\n    power of EFDR exceeds that of standard FDR, which would carry out tests on\n    every wavelet coefficient: EFDR choose which wavelets to test based on a\n    criterion described in Shen et al. (2002). The package also provides\n    elementary tools to interpolate spatially irregular data onto a grid of the\n    required size. The work is based on Shen, X., Huang, H.-C., and Cressie, N.\n    'Nonparametric hypothesis testing for a spatial signal.' Journal of the\n    American Statistical Association 97.460 (2002): 1122-1140.",
    "version": "1.3",
    "maintainer": "Andrew Zammit-Mangion <andrewzm@gmail.com>",
    "author": "Andrew Zammit-Mangion [aut, cre],\n  Hsin-Cheng Huang [aut]",
    "url": "https://github.com/andrewzm/EFDR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EFDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EFDR Wavelet-Based Enhanced FDR for Detecting Signals from Complete\nor Incomplete Spatially Aggregated Data Enhanced False Discovery Rate (EFDR) is a tool to detect anomalies\n    in an image. The image is first transformed into the wavelet domain in\n    order to decorrelate any noise components, following which the coefficients\n    at each resolution are standardised. Statistical tests (in a multiple\n    hypothesis testing setting) are then carried out to find the anomalies. The\n    power of EFDR exceeds that of standard FDR, which would carry out tests on\n    every wavelet coefficient: EFDR choose which wavelets to test based on a\n    criterion described in Shen et al. (2002). The package also provides\n    elementary tools to interpolate spatially irregular data onto a grid of the\n    required size. The work is based on Shen, X., Huang, H.-C., and Cressie, N.\n    'Nonparametric hypothesis testing for a spatial signal.' Journal of the\n    American Statistical Association 97.460 (2002): 1122-1140.  "
  },
  {
    "id": 3152,
    "package_name": "EQUIVNONINF",
    "title": "Testing for Equivalence and Noninferiority",
    "description": "Making available in R the complete set of programs accompanying S. Wellek's (2010) monograph\n             ''Testing Statistical Hypotheses of Equivalence and Noninferiority. Second Edition''\n             (Chapman&Hall/CRC). ",
    "version": "1.0.2",
    "maintainer": "Stefan Wellek <stefan.wellek@zi-mannheim.de>",
    "author": "Stefan Wellek,\n        Peter Ziegler",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EQUIVNONINF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EQUIVNONINF Testing for Equivalence and Noninferiority Making available in R the complete set of programs accompanying S. Wellek's (2010) monograph\n             ''Testing Statistical Hypotheses of Equivalence and Noninferiority. Second Edition''\n             (Chapman&Hall/CRC).   "
  },
  {
    "id": 3155,
    "package_name": "ERP",
    "title": "Significance Analysis of Event-Related Potentials Data",
    "description": "Functions for signal detection and identification designed for Event-Related Potentials (ERP) data in a linear model framework. The functional F-test proposed in Causeur, Sheu, Perthame, Rufini (2018, submitted) for analysis of variance issues in ERP designs is implemented for signal detection (tests for mean difference among groups of curves in One-way ANOVA designs for example). Once an experimental effect is declared significant, identification of significant intervals is achieved by the multiple testing procedures reviewed and compared in Sheu, Perthame, Lee and Causeur (2016, <DOI:10.1214/15-AOAS888>). Some of the methods gathered in the package are the classical FDR- and FWER-controlling procedures, also available using function p.adjust. The package also implements the Guthrie-Buchwald procedure (Guthrie and Buchwald, 1991 <DOI:10.1111/j.1469-8986.1991.tb00417.x>), which accounts for the auto-correlation among t-tests to control erroneous detection of short intervals. The Adaptive Factor-Adjustment method is an extension of the method described in Causeur, Chu, Hsieh and Sheu (2012, <DOI:10.3758/s13428-012-0230-0>). It assumes a factor model for the correlation among tests and combines adaptively the estimation of the signal and the updating of the dependence modelling (see Sheu et al., 2016, <DOI:10.1214/15-AOAS888> for further details).",
    "version": "2.2",
    "maintainer": "David Causeur <david.causeur@agrocampus-ouest.fr>",
    "author": "David Causeur, Ching-Fan Sheu, Mei-Chen Chu, Flavia Rufini",
    "url": "http://erpinr.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ERP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ERP Significance Analysis of Event-Related Potentials Data Functions for signal detection and identification designed for Event-Related Potentials (ERP) data in a linear model framework. The functional F-test proposed in Causeur, Sheu, Perthame, Rufini (2018, submitted) for analysis of variance issues in ERP designs is implemented for signal detection (tests for mean difference among groups of curves in One-way ANOVA designs for example). Once an experimental effect is declared significant, identification of significant intervals is achieved by the multiple testing procedures reviewed and compared in Sheu, Perthame, Lee and Causeur (2016, <DOI:10.1214/15-AOAS888>). Some of the methods gathered in the package are the classical FDR- and FWER-controlling procedures, also available using function p.adjust. The package also implements the Guthrie-Buchwald procedure (Guthrie and Buchwald, 1991 <DOI:10.1111/j.1469-8986.1991.tb00417.x>), which accounts for the auto-correlation among t-tests to control erroneous detection of short intervals. The Adaptive Factor-Adjustment method is an extension of the method described in Causeur, Chu, Hsieh and Sheu (2012, <DOI:10.3758/s13428-012-0230-0>). It assumes a factor model for the correlation among tests and combines adaptively the estimation of the signal and the updating of the dependence modelling (see Sheu et al., 2016, <DOI:10.1214/15-AOAS888> for further details).  "
  },
  {
    "id": 3161,
    "package_name": "ESTER",
    "title": "Efficient Sequential Testing with Evidence Ratios",
    "description": "An implementation of sequential testing that uses evidence ratios\n    computed from the weights of a set of models. These weights correspond either\n    to Akaike weights computed from the Akaike Information Criterion (AIC) or the\n    Bayesian Information Criterion (BIC) and following Burnham & Anderson\n    (2004, <doi:10.1177/0049124104268644>) recommendations, or to pseudo-BMA\n    weights computed from the WAIC or the LOO-IC of models fitted\n    with 'brms' and following Yao et al. (2017, <arXiv:1704.02030v3>).",
    "version": "0.2.0",
    "maintainer": "Ladislas Nalborczyk <ladislas.nalborczyk@gmail.com>",
    "author": "Ladislas Nalborczyk [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7419-9855>)",
    "url": "https://github.com/lnalborczyk/ESTER",
    "bug_reports": "https://github.com/lnalborczyk/ESTER/issues",
    "repository": "https://cran.r-project.org/package=ESTER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ESTER Efficient Sequential Testing with Evidence Ratios An implementation of sequential testing that uses evidence ratios\n    computed from the weights of a set of models. These weights correspond either\n    to Akaike weights computed from the Akaike Information Criterion (AIC) or the\n    Bayesian Information Criterion (BIC) and following Burnham & Anderson\n    (2004, <doi:10.1177/0049124104268644>) recommendations, or to pseudo-BMA\n    weights computed from the WAIC or the LOO-IC of models fitted\n    with 'brms' and following Yao et al. (2017, <arXiv:1704.02030v3>).  "
  },
  {
    "id": 3240,
    "package_name": "EpiForsk",
    "title": "Code Sharing at the Department of Epidemiology Research at\nStatens Serum Institut",
    "description": "This is a collection of assorted functions and examples collected \n    from various projects. Currently we have functionalities for simplifying \n    overlapping time intervals, Charlson comorbidity score constructors for \n    Danish data, getting frequency for multiple variables, getting standardized\n    output from logistic and log-linear regressions, sibling design linear \n    regression functionalities a method for calculating the confidence intervals \n    for functions of parameters from a GLM, Bayes equivalent for hypothesis \n    testing with asymptotic Bayes factor, and several help functions for \n    generalized random forest analysis using 'grf'. ",
    "version": "0.2.0",
    "maintainer": "Kim Daniel Jakobsen <kija@ssi.dk>",
    "author": "Anders Husby [aut] (ORCID: <https://orcid.org/0000-0002-7634-8455>),\n  Anna Laksafoss [aut] (ORCID: <https://orcid.org/0000-0002-9898-2924>),\n  Emilia Myrup Thiesson [aut] (ORCID:\n    <https://orcid.org/0000-0001-6258-4177>),\n  Kim Daniel Jakobsen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0086-9980>),\n  Mikael Andersson [aut] (ORCID: <https://orcid.org/0000-0002-0114-2057>),\n  Klaus Rostgaard [aut] (ORCID: <https://orcid.org/0000-0001-6220-9414>)",
    "url": "https://github.com/Laksafoss/EpiForsk",
    "bug_reports": "https://github.com/Laksafoss/EpiForsk/issues",
    "repository": "https://cran.r-project.org/package=EpiForsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EpiForsk Code Sharing at the Department of Epidemiology Research at\nStatens Serum Institut This is a collection of assorted functions and examples collected \n    from various projects. Currently we have functionalities for simplifying \n    overlapping time intervals, Charlson comorbidity score constructors for \n    Danish data, getting frequency for multiple variables, getting standardized\n    output from logistic and log-linear regressions, sibling design linear \n    regression functionalities a method for calculating the confidence intervals \n    for functions of parameters from a GLM, Bayes equivalent for hypothesis \n    testing with asymptotic Bayes factor, and several help functions for \n    generalized random forest analysis using 'grf'.   "
  },
  {
    "id": 3256,
    "package_name": "Equalden.HD",
    "title": "Testing the Equality of a High Dimensional Set of Densities",
    "description": "The equality of a large number k of densities is tested by measuring the L2 distance between the corresponding kernel density estimators and the one based on the pooled sample. The test even works for sample sizes as small as 2.",
    "version": "1.2.1",
    "maintainer": "Marta Cousido Rocha <martacousido@uvigo.es>",
    "author": "Marta Cousido Rocha [aut, cre],\n  Jos\u00e9 Carlos Soage Gonz\u00e1lez [ctr],\n  Jacobo de U\u00f1a \u00c1lvarez [aut, ths],\n  Jeffrey D. Hart [aut],\n  Ivan Kojadinovic [cph],\n  A. Patton [cph],\n  C. Parmeter [cph],\n  J. Racine [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Equalden.HD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Equalden.HD Testing the Equality of a High Dimensional Set of Densities The equality of a large number k of densities is tested by measuring the L2 distance between the corresponding kernel density estimators and the one based on the pooled sample. The test even works for sample sizes as small as 2.  "
  },
  {
    "id": 3258,
    "package_name": "EquiTrends",
    "title": "Equivalence Testing for Pre-Trends in Difference-in-Differences\nDesigns",
    "description": "Testing for parallel trends is crucial in the Difference-in-Differences framework. To this end, this package performs equivalence testing in the context of Difference-in-Differences estimation. It allows users to test if pre-treatment trends in the treated group are \u201cequivalent\u201d to those in the control group. Here, \u201cequivalence\u201d means that rejection of the null hypothesis implies that a function of the pre-treatment placebo effects (maximum absolute, average or root mean squared value) does not exceed a pre-specified threshold below which trend differences are considered negligible. The package is based on the theory developed in Dette & Schumann (2024) <doi:10.1080/07350015.2024.2308121>. ",
    "version": "1.0.0",
    "maintainer": "Ties Bos <tc.bos@student.maastrichtuniversity.nl>",
    "author": "Ties Bos [aut, cre],\n  Martin Schumann [ctb]",
    "url": "https://github.com/TiesBos/EquiTrends",
    "bug_reports": "https://github.com/TiesBos/EquiTrends/issues",
    "repository": "https://cran.r-project.org/package=EquiTrends",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EquiTrends Equivalence Testing for Pre-Trends in Difference-in-Differences\nDesigns Testing for parallel trends is crucial in the Difference-in-Differences framework. To this end, this package performs equivalence testing in the context of Difference-in-Differences estimation. It allows users to test if pre-treatment trends in the treated group are \u201cequivalent\u201d to those in the control group. Here, \u201cequivalence\u201d means that rejection of the null hypothesis implies that a function of the pre-treatment placebo effects (maximum absolute, average or root mean squared value) does not exceed a pre-specified threshold below which trend differences are considered negligible. The package is based on the theory developed in Dette & Schumann (2024) <doi:10.1080/07350015.2024.2308121>.   "
  },
  {
    "id": 3262,
    "package_name": "EstimDiagnostics",
    "title": "Diagnostic Tools and Unit Tests for Statistical Estimators",
    "description": "Extension of 'testthat' package to make unit tests on empirical distributions of estimators and functions for diagnostics of their finite-sample performance.",
    "version": "0.0.3",
    "maintainer": "Dmitry Otryakhin <d.otryakhin.acad@protonmail.ch>",
    "author": "Dmitry Otryakhin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4700-7221>)",
    "url": "https://gitlab.com/Dmitry_Otryakhin/diagnostics-and-tests-for-statistical-estimators",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EstimDiagnostics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EstimDiagnostics Diagnostic Tools and Unit Tests for Statistical Estimators Extension of 'testthat' package to make unit tests on empirical distributions of estimators and functions for diagnostics of their finite-sample performance.  "
  },
  {
    "id": 3278,
    "package_name": "EventWinRatios",
    "title": "Event-Specific Win Ratios for Terminal and Non-Terminal Events",
    "description": "Provides several confidence interval and testing procedures using\n    event-specific win ratios for semi-competing risks data with non-terminal\n    and terminal events, as developed in Yang et al. (2021<doi:10.1002/sim.9266>). \n    Compared with conventional methods for survival data, these procedures are \n    designed to utilize more data for improved inference procedures with \n    semi-competing risks data. The event-specific win ratios were introduced in \n    Yang and Troendle (2021<doi:10.1177/1740774520972408>). In this package, \n    the event-specific win ratios and confidence intervals are obtained for each \n    event type, and several testing procedures are developed for the global null \n    of no treatment effect on either terminal or non-terminal events. Furthermore,\n    a test of proportional hazard assumptions, under which the event-specific win \n    ratios converge to the hazard ratios, and a test of equal hazard ratios are \n    provided. For summarizing the treatment effect on all events, confidence \n    intervals for linear combinations of the event-specific win ratios are available\n    using pre-determined or data-driven weights. Asymptotic properties of these \n    inference procedures are discussed in Yang et al (2021<doi:10.1002/sim.9266>). \n    Also, transformations are used to yield better control of the type one error \n    rates for moderately sized data sets.",
    "version": "1.0.0",
    "maintainer": "Daewoo Pak <dpak@yonsei.ac.kr>",
    "author": "Daewoo Pak [aut, cre],\n  Song Yang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EventWinRatios",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EventWinRatios Event-Specific Win Ratios for Terminal and Non-Terminal Events Provides several confidence interval and testing procedures using\n    event-specific win ratios for semi-competing risks data with non-terminal\n    and terminal events, as developed in Yang et al. (2021<doi:10.1002/sim.9266>). \n    Compared with conventional methods for survival data, these procedures are \n    designed to utilize more data for improved inference procedures with \n    semi-competing risks data. The event-specific win ratios were introduced in \n    Yang and Troendle (2021<doi:10.1177/1740774520972408>). In this package, \n    the event-specific win ratios and confidence intervals are obtained for each \n    event type, and several testing procedures are developed for the global null \n    of no treatment effect on either terminal or non-terminal events. Furthermore,\n    a test of proportional hazard assumptions, under which the event-specific win \n    ratios converge to the hazard ratios, and a test of equal hazard ratios are \n    provided. For summarizing the treatment effect on all events, confidence \n    intervals for linear combinations of the event-specific win ratios are available\n    using pre-determined or data-driven weights. Asymptotic properties of these \n    inference procedures are discussed in Yang et al (2021<doi:10.1002/sim.9266>). \n    Also, transformations are used to yield better control of the type one error \n    rates for moderately sized data sets.  "
  },
  {
    "id": 3295,
    "package_name": "ExactVaRTest",
    "title": "Exact Finite-Sample Value-at-Risk Back-Testing",
    "description": "Provides fast dynamic-programming algorithms in 'C++'/'Rcpp'\n    (with pure 'R' fallbacks) for the exact finite-sample distributions\n    and p-values of Christoffersen (1998) independence (IND) and\n    conditional-coverage (CC) VaR backtests. For completeness, it also\n    provides the exact unconditional-coverage (UC) test following\n    Kupiec (1995) via a closed-form binomial enumeration. See\n    Christoffersen (1998) <doi:10.2307/2527341> and Kupiec (1995)\n    <doi:10.3905/jod.1995.407942>.",
    "version": "0.1.3",
    "maintainer": "Yujian Chen <yjc4996@gmail.com>",
    "author": "Yujian Chen [aut, cre]",
    "url": "https://github.com/YujianCHEN219/ExactVaRTest",
    "bug_reports": "https://github.com/YujianCHEN219/ExactVaRTest/issues",
    "repository": "https://cran.r-project.org/package=ExactVaRTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExactVaRTest Exact Finite-Sample Value-at-Risk Back-Testing Provides fast dynamic-programming algorithms in 'C++'/'Rcpp'\n    (with pure 'R' fallbacks) for the exact finite-sample distributions\n    and p-values of Christoffersen (1998) independence (IND) and\n    conditional-coverage (CC) VaR backtests. For completeness, it also\n    provides the exact unconditional-coverage (UC) test following\n    Kupiec (1995) via a closed-form binomial enumeration. See\n    Christoffersen (1998) <doi:10.2307/2527341> and Kupiec (1995)\n    <doi:10.3905/jod.1995.407942>.  "
  },
  {
    "id": 3319,
    "package_name": "ExtremeRisks",
    "title": "Extreme Risk Measures",
    "description": "A set of procedures for estimating risks related to extreme events via risk measures such as Expectile, Value-at-Risk, etc. is provided. Estimation methods for univariate independent observations and temporal dependent observations are available. The methodology is extended to the case of independent multidimensional observations.  The statistical inference is performed through parametric and non-parametric estimators. Inferential procedures such as confidence intervals, confidence regions and hypothesis testing are obtained by exploiting the asymptotic theory. Adapts the methodologies derived in Padoan and Stupfler (2022) <doi:10.3150/21-BEJ1375>, Davison et al. (2023) <doi:10.1080/07350015.2022.2078332>, Daouia et al. (2018) <doi:10.1111/rssb.12254>, Drees (2000) <doi:10.1214/aoap/1019487617>, Drees (2003) <doi:10.3150/bj/1066223272>, de Haan and Ferreira (2006) <doi:10.1007/0-387-34471-3>, de Haan et al. (2016) <doi:10.1007/s00780-015-0287-6>, Padoan and Rizzelli (2024) <doi:10.3150/23-BEJ1668>, Daouia et al. (2024) <doi:10.3150/23-BEJ1632>.",
    "version": "0.0.5",
    "maintainer": "Simone Padoan <simone.padoan@unibocconi.it>",
    "author": "Simone Padoan [cre, aut],\n  Gilles Stupfler [aut],\n  Carlotta Pacifici [aut]",
    "url": "https://faculty.unibocconi.it/simonepadoan/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ExtremeRisks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ExtremeRisks Extreme Risk Measures A set of procedures for estimating risks related to extreme events via risk measures such as Expectile, Value-at-Risk, etc. is provided. Estimation methods for univariate independent observations and temporal dependent observations are available. The methodology is extended to the case of independent multidimensional observations.  The statistical inference is performed through parametric and non-parametric estimators. Inferential procedures such as confidence intervals, confidence regions and hypothesis testing are obtained by exploiting the asymptotic theory. Adapts the methodologies derived in Padoan and Stupfler (2022) <doi:10.3150/21-BEJ1375>, Davison et al. (2023) <doi:10.1080/07350015.2022.2078332>, Daouia et al. (2018) <doi:10.1111/rssb.12254>, Drees (2000) <doi:10.1214/aoap/1019487617>, Drees (2003) <doi:10.3150/bj/1066223272>, de Haan and Ferreira (2006) <doi:10.1007/0-387-34471-3>, de Haan et al. (2016) <doi:10.1007/s00780-015-0287-6>, Padoan and Rizzelli (2024) <doi:10.3150/23-BEJ1668>, Daouia et al. (2024) <doi:10.3150/23-BEJ1632>.  "
  },
  {
    "id": 3332,
    "package_name": "FAS",
    "title": "Factor-Augmented Sparse Regression Tuning-Free Testing",
    "description": "The 'FAS' package implements the bootstrap method for the tuning parameter selection and tuning-free inference on sparse regression coefficient vectors. Currently, the test could be applied to linear and factor-augmented sparse regressions, see Lederer & Vogt (2021, JMLR) <https://www.jmlr.org/papers/volume22/20-539/20-539.pdf> and Beyhum & Striaukas (2023) <arXiv:2307.13364>. ",
    "version": "1.0.0",
    "maintainer": "Jonas Striaukas <jonas.striaukas@gmail.com>",
    "author": "Jonas Striaukas [cre, aut],\n  Jad Beyhum [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FAS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FAS Factor-Augmented Sparse Regression Tuning-Free Testing The 'FAS' package implements the bootstrap method for the tuning parameter selection and tuning-free inference on sparse regression coefficient vectors. Currently, the test could be applied to linear and factor-augmented sparse regressions, see Lederer & Vogt (2021, JMLR) <https://www.jmlr.org/papers/volume22/20-539/20-539.pdf> and Beyhum & Striaukas (2023) <arXiv:2307.13364>.   "
  },
  {
    "id": 3347,
    "package_name": "FCVAR",
    "title": "Estimation and Inference for the Fractionally Cointegrated VAR",
    "description": "Estimation and inference using the Fractionally Cointegrated \n    Vector Autoregressive (VAR) model. It includes functions for model specification, \n    including lag selection and cointegration rank selection, as well as a comprehensive\n    set of options for hypothesis testing, including tests of hypotheses on the \n    cointegrating relations, the adjustment coefficients and the fractional \n    differencing parameters. \n    An article describing the FCVAR model with examples is available on the Webpage \n    <https://sites.google.com/view/mortennielsen/software>.",
    "version": "0.1.4",
    "maintainer": "Lealand Morin <lealand.morin@ucf.edu>",
    "author": "Lealand Morin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8539-1386>),\n  Morten Nielsen [aut] (ORCID: <https://orcid.org/0000-0002-1337-9844>),\n  Michal Popiel [aut]",
    "url": "https://github.com/LeeMorinUCF/FCVAR",
    "bug_reports": "https://github.com/LeeMorinUCF/FCVAR/issues",
    "repository": "https://cran.r-project.org/package=FCVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FCVAR Estimation and Inference for the Fractionally Cointegrated VAR Estimation and inference using the Fractionally Cointegrated \n    Vector Autoregressive (VAR) model. It includes functions for model specification, \n    including lag selection and cointegration rank selection, as well as a comprehensive\n    set of options for hypothesis testing, including tests of hypotheses on the \n    cointegrating relations, the adjustment coefficients and the fractional \n    differencing parameters. \n    An article describing the FCVAR model with examples is available on the Webpage \n    <https://sites.google.com/view/mortennielsen/software>.  "
  },
  {
    "id": 3349,
    "package_name": "FDOTT",
    "title": "Optimal Transport Based Testing in Factorial Design",
    "description": "Perform optimal transport based tests in factorial designs as introduced in Groppe et al. (2025) <doi:10.48550/arXiv.2509.13970> via the FDOTT() function. These tests are inspired by ANOVA and its nonparametric counterparts. They allow for testing linear relationships in factorial designs between finitely supported probability measures on a metric space. Such relationships include equality of all measures (no treatment effect), interaction effects between a number of factors, as well as main and simple factor effects.",
    "version": "0.1.0",
    "maintainer": "Michel Groppe <michel.groppe@uni-goettingen.de>",
    "author": "Michel Groppe [aut, cre],\n  Linus Niem\u00f6ller [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FDOTT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FDOTT Optimal Transport Based Testing in Factorial Design Perform optimal transport based tests in factorial designs as introduced in Groppe et al. (2025) <doi:10.48550/arXiv.2509.13970> via the FDOTT() function. These tests are inspired by ANOVA and its nonparametric counterparts. They allow for testing linear relationships in factorial designs between finitely supported probability measures on a metric space. Such relationships include equality of all measures (no treatment effect), interaction effects between a number of factors, as well as main and simple factor effects.  "
  },
  {
    "id": 3353,
    "package_name": "FDX",
    "title": "False Discovery Exceedance Controlling Multiple Testing\nProcedures",
    "description": "Multiple testing procedures for heterogeneous and discrete tests as\n    described in D\u00f6hler and Roquain (2020) <doi:10.1214/20-EJS1771>. The main \n    algorithms of the paper are available as continuous, discrete and weighted\n    versions. They take as input the results of a test procedure from package\n    'DiscreteTests', or a set of observed p-values and their discrete support\n    under their nulls. A shortcut function to obtain such p-values and supports\n    is also provided, along with wrappers allowing to apply discrete procedures\n    directly to data.",
    "version": "2.0.2",
    "maintainer": "Florian Junge <diso.fbmn@h-da.de>",
    "author": "Sebastian D\u00f6hler [aut] (ORCID: <https://orcid.org/0000-0002-0321-6355>),\n  Florian Junge [aut, cre] (ORCID:\n    <https://orcid.org/0009-0001-6856-6938>),\n  Etienne Roquain [ctb]",
    "url": "https://github.com/DISOhda/FDX",
    "bug_reports": "https://github.com/DISOhda/FDX/issues",
    "repository": "https://cran.r-project.org/package=FDX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FDX False Discovery Exceedance Controlling Multiple Testing\nProcedures Multiple testing procedures for heterogeneous and discrete tests as\n    described in D\u00f6hler and Roquain (2020) <doi:10.1214/20-EJS1771>. The main \n    algorithms of the paper are available as continuous, discrete and weighted\n    versions. They take as input the results of a test procedure from package\n    'DiscreteTests', or a set of observed p-values and their discrete support\n    under their nulls. A shortcut function to obtain such p-values and supports\n    is also provided, along with wrappers allowing to apply discrete procedures\n    directly to data.  "
  },
  {
    "id": 3399,
    "package_name": "FPV",
    "title": "Testing Hypotheses via Fuzzy P-Value in Fuzzy Environment",
    "description": "The main goal of this package is drawing the membership function of the fuzzy p-value which is defined as a fuzzy set on the unit interval for three following problems: (1) testing crisp hypotheses based on fuzzy data, see Filzmoser and Viertl (2004) <doi:10.1007/s001840300269>, (2) testing fuzzy hypotheses based on crisp data, see Parchami et al. (2010) <doi:10.1007/s00362-008-0133-4>, and (3) testing fuzzy hypotheses based on fuzzy data, see Parchami et al. (2012) <doi:10.1007/s00362-010-0353-2>. In all cases, the fuzziness of data or / and the fuzziness of the boundary of null fuzzy hypothesis transported via the p-value function and causes to produce the fuzzy p-value. If the p-value is fuzzy, it is more appropriate to consider a fuzzy significance level for the problem. Therefore, the comparison of the fuzzy p-value and the fuzzy significance level is evaluated by a fuzzy ranking method in this package.",
    "version": "0.5",
    "maintainer": "Abbas Parchami <parchami@uk.ac.ir>",
    "author": "Abbas Parchami (Department of Statistics, Faculty of Mathematics and Computer, Shahid Bahonar University of Kerman, Kerman, Iran)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FPV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FPV Testing Hypotheses via Fuzzy P-Value in Fuzzy Environment The main goal of this package is drawing the membership function of the fuzzy p-value which is defined as a fuzzy set on the unit interval for three following problems: (1) testing crisp hypotheses based on fuzzy data, see Filzmoser and Viertl (2004) <doi:10.1007/s001840300269>, (2) testing fuzzy hypotheses based on crisp data, see Parchami et al. (2010) <doi:10.1007/s00362-008-0133-4>, and (3) testing fuzzy hypotheses based on fuzzy data, see Parchami et al. (2012) <doi:10.1007/s00362-010-0353-2>. In all cases, the fuzziness of data or / and the fuzziness of the boundary of null fuzzy hypothesis transported via the p-value function and causes to produce the fuzzy p-value. If the p-value is fuzzy, it is more appropriate to consider a fuzzy significance level for the problem. Therefore, the comparison of the fuzzy p-value and the fuzzy significance level is evaluated by a fuzzy ranking method in this package.  "
  },
  {
    "id": 3407,
    "package_name": "FRESA.CAD",
    "title": "Feature Selection Algorithms for Computer Aided Diagnosis",
    "description": "Contains a set of utilities for building and testing statistical models (linear, logistic,ordinal or COX) for Computer Aided Diagnosis/Prognosis applications. Utilities include data adjustment, univariate analysis, model building, model-validation, longitudinal analysis, reporting and visualization.",
    "version": "3.4.8",
    "maintainer": "Jose Gerardo Tamez-Pena <jose.tamezpena@tec.mx>",
    "author": "Jose Gerardo Tamez-Pena, Antonio Martinez-Torteya, Israel Alanis and Jorge Orozco",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FRESA.CAD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FRESA.CAD Feature Selection Algorithms for Computer Aided Diagnosis Contains a set of utilities for building and testing statistical models (linear, logistic,ordinal or COX) for Computer Aided Diagnosis/Prognosis applications. Utilities include data adjustment, univariate analysis, model building, model-validation, longitudinal analysis, reporting and visualization.  "
  },
  {
    "id": 3422,
    "package_name": "FTSgof",
    "title": "White Noise and Goodness-of-Fit Tests for Functional Time Series",
    "description": "It offers comprehensive tools for the analysis of functional\n    time series data, focusing on white noise hypothesis testing and\n    goodness-of-fit evaluations, alongside functions for\n    simulating data and advanced visualization techniques, such as 3D\n    rainbow plots. These methods are described in Kokoszka, Rice, and Shang (2017)  <doi:10.1016/j.jmva.2017.08.004>, \n    Yeh, Rice, and Dubin (2023) <doi:10.1214/23-EJS2112>, Kim, Kokoszka, and Rice (2023) <doi:10.1214/23-ss143>, and \n    Rice, Wirjanto, and Zhao (2020) <doi:10.1111/jtsa.12532>.",
    "version": "1.0.0",
    "maintainer": "Mihyun Kim <mihyun.kim@mail.wvu.edu>",
    "author": "Mihyun Kim [aut, cre],\n  Chi-Kuang Yeh [aut] (ORCID: <https://orcid.org/0000-0001-7057-2096>),\n  Yuqian Zhao [aut],\n  Gregory Rice [ctb]",
    "url": "https://github.com/veritasmih/FTSgof",
    "bug_reports": "https://github.com/veritasmih/FTSgof/issues",
    "repository": "https://cran.r-project.org/package=FTSgof",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FTSgof White Noise and Goodness-of-Fit Tests for Functional Time Series It offers comprehensive tools for the analysis of functional\n    time series data, focusing on white noise hypothesis testing and\n    goodness-of-fit evaluations, alongside functions for\n    simulating data and advanced visualization techniques, such as 3D\n    rainbow plots. These methods are described in Kokoszka, Rice, and Shang (2017)  <doi:10.1016/j.jmva.2017.08.004>, \n    Yeh, Rice, and Dubin (2023) <doi:10.1214/23-EJS2112>, Kim, Kokoszka, and Rice (2023) <doi:10.1214/23-ss143>, and \n    Rice, Wirjanto, and Zhao (2020) <doi:10.1111/jtsa.12532>.  "
  },
  {
    "id": 3443,
    "package_name": "FarmTest",
    "title": "Factor-Adjusted Robust Multiple Testing",
    "description": "Performs robust multiple testing for means in the presence of known and unknown latent factors presented in Fan et al.(2019) \"FarmTest: Factor-Adjusted Robust Multiple Testing With Approximate False Discovery Control\" <doi:10.1080/01621459.2018.1527700>.\n             Implements a series of adaptive Huber methods combined with fast data-drive tuning schemes proposed in Ke et al.(2019) \"User-Friendly Covariance Estimation for Heavy-Tailed Distributions\" <doi:10.1214/19-STS711> to estimate model parameters and construct test statistics that are robust against heavy-tailed and/or asymmetric error distributions. \n             Extensions to two-sample simultaneous mean comparison problems are also included. \n             As by-products, this package contains functions that compute adaptive Huber mean, covariance and regression estimators that are of independent interest.",
    "version": "2.2.0",
    "maintainer": "Xiaoou Pan <xip024@ucsd.edu>",
    "author": "Xiaoou Pan [aut, cre],\n  Yuan Ke [aut],\n  Wen-Xin Zhou [aut]",
    "url": "https://github.com/XiaoouPan/FarmTest",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FarmTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FarmTest Factor-Adjusted Robust Multiple Testing Performs robust multiple testing for means in the presence of known and unknown latent factors presented in Fan et al.(2019) \"FarmTest: Factor-Adjusted Robust Multiple Testing With Approximate False Discovery Control\" <doi:10.1080/01621459.2018.1527700>.\n             Implements a series of adaptive Huber methods combined with fast data-drive tuning schemes proposed in Ke et al.(2019) \"User-Friendly Covariance Estimation for Heavy-Tailed Distributions\" <doi:10.1214/19-STS711> to estimate model parameters and construct test statistics that are robust against heavy-tailed and/or asymmetric error distributions. \n             Extensions to two-sample simultaneous mean comparison problems are also included. \n             As by-products, this package contains functions that compute adaptive Huber mean, covariance and regression estimators that are of independent interest.  "
  },
  {
    "id": 3453,
    "package_name": "FastKRR",
    "title": "Kernel Ridge Regression using 'RcppArmadillo'",
    "description": "Provides core computational operations in C++ via 'RcppArmadillo', enabling faster performance than pure R, improved numerical stability, and parallel execution with OpenMP where available. On systems without OpenMP support, the package automatically falls back to single-threaded execution with no user configuration required. For efficient model selection, it integrates with 'CVST' to provide sequential-testing cross-validation that identifies competitive hyperparameters without exhaustive grid search. The package offers a unified interface for exact kernel ridge regression and three scalable approximations\u2014Nystr\u00f6m, Pivoted Cholesky, and Random Fourier Features\u2014allowing analyses with substantially larger sample sizes than are feasible with exact KRR. It also integrates with the 'tidymodels' ecosystem via the 'parsnip' model specification 'krr_reg', and the S3 method tunable.krr_reg(). To understand the theoretical background, one can refer to Wainwright (2019) <doi:10.1017/9781108627771>.",
    "version": "0.1.2",
    "maintainer": "Kwan-Young Bak <kybak@sungshin.ac.kr>",
    "author": "Gyeongmin Kim [aut] (Sungshin Women's University),\n  Seyoung Lee [aut] (Sungshin Women's University),\n  Miyoung Jang [aut] (Sungshin Women's University),\n  Kwan-Young Bak [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4541-160X>, Sungshin Women's\n    University)",
    "url": "https://github.com/kybak90/FastKRR, https://www.tidymodels.org",
    "bug_reports": "https://github.com/kybak90/FastKRR/issues",
    "repository": "https://cran.r-project.org/package=FastKRR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FastKRR Kernel Ridge Regression using 'RcppArmadillo' Provides core computational operations in C++ via 'RcppArmadillo', enabling faster performance than pure R, improved numerical stability, and parallel execution with OpenMP where available. On systems without OpenMP support, the package automatically falls back to single-threaded execution with no user configuration required. For efficient model selection, it integrates with 'CVST' to provide sequential-testing cross-validation that identifies competitive hyperparameters without exhaustive grid search. The package offers a unified interface for exact kernel ridge regression and three scalable approximations\u2014Nystr\u00f6m, Pivoted Cholesky, and Random Fourier Features\u2014allowing analyses with substantially larger sample sizes than are feasible with exact KRR. It also integrates with the 'tidymodels' ecosystem via the 'parsnip' model specification 'krr_reg', and the S3 method tunable.krr_reg(). To understand the theoretical background, one can refer to Wainwright (2019) <doi:10.1017/9781108627771>.  "
  },
  {
    "id": 3490,
    "package_name": "FixSeqMTP",
    "title": "Fixed Sequence Multiple Testing Procedures",
    "description": "Several generalized / directional Fixed Sequence Multiple Testing\n    Procedures (FSMTPs) are developed for testing a sequence of pre-ordered\n    hypotheses while controlling the FWER, FDR and Directional Error (mdFWER).\n    All three FWER controlling generalized FSMTPs are designed under arbitrary\n    dependence, which allow any number of acceptances. Two FDR controlling\n    generalized FSMTPs are respectively designed under arbitrary dependence and\n    independence, which allow more but a given number of acceptances. Two mdFWER\n    controlling directional FSMTPs are respectively designed under arbitrary\n    dependence and independence, which can also make directional decisions based\n    on the signs of the test statistics. The main functions for each proposed\n    generalized / directional FSMTPs are designed to calculate adjusted p-values\n    and critical values, respectively. For users' convenience, the functions also\n    provide the output option for printing decision rules.",
    "version": "0.1.2",
    "maintainer": "Yalin Zhu <yalin.zhu@outlook.com>",
    "author": "Yalin Zhu, Wenge Guo",
    "url": "",
    "bug_reports": "https://github.com/allenzhuaz/FixSeqMTP/issues",
    "repository": "https://cran.r-project.org/package=FixSeqMTP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FixSeqMTP Fixed Sequence Multiple Testing Procedures Several generalized / directional Fixed Sequence Multiple Testing\n    Procedures (FSMTPs) are developed for testing a sequence of pre-ordered\n    hypotheses while controlling the FWER, FDR and Directional Error (mdFWER).\n    All three FWER controlling generalized FSMTPs are designed under arbitrary\n    dependence, which allow any number of acceptances. Two FDR controlling\n    generalized FSMTPs are respectively designed under arbitrary dependence and\n    independence, which allow more but a given number of acceptances. Two mdFWER\n    controlling directional FSMTPs are respectively designed under arbitrary\n    dependence and independence, which can also make directional decisions based\n    on the signs of the test statistics. The main functions for each proposed\n    generalized / directional FSMTPs are designed to calculate adjusted p-values\n    and critical values, respectively. For users' convenience, the functions also\n    provide the output option for printing decision rules.  "
  },
  {
    "id": 3515,
    "package_name": "ForeComp",
    "title": "Size-Power Tradeoff Visualization for Equal Predictive Ability\nof Two Forecasts",
    "description": "Offers a set of tools for visualizing and analyzing size and power properties of the test for equal predictive accuracy, the Diebold-Mariano test that is based on heteroskedasticity and autocorrelation-robust (HAR) inference. A typical HAR inference is involved with non-parametric estimation of the long-run variance, and one of its tuning parameters, the truncation parameter, trades off a size and power. Lazarus, Lewis, and Stock (2021)<doi:10.3982/ECTA15404> theoretically characterize the size-power frontier for the Gaussian multivariate location model. 'ForeComp' computes and visualizes the finite-sample size-power frontier of the Diebold-Mariano test based on fixed-b asymptotics together with the Bartlett kernel. To compute the finite-sample size and power, it works with the best approximating ARMA process to the given dataset. It informs the user how their choice of the truncation parameter performs and how robust the testing outcomes are.",
    "version": "0.9.0",
    "maintainer": "Minchul Shin <visiblehand@gmail.com>",
    "author": "Nathan Schor [aut],\n  Minchul Shin [aut, cre, cph]",
    "url": "https://github.com/mcmcs/ForeComp",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ForeComp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ForeComp Size-Power Tradeoff Visualization for Equal Predictive Ability\nof Two Forecasts Offers a set of tools for visualizing and analyzing size and power properties of the test for equal predictive accuracy, the Diebold-Mariano test that is based on heteroskedasticity and autocorrelation-robust (HAR) inference. A typical HAR inference is involved with non-parametric estimation of the long-run variance, and one of its tuning parameters, the truncation parameter, trades off a size and power. Lazarus, Lewis, and Stock (2021)<doi:10.3982/ECTA15404> theoretically characterize the size-power frontier for the Gaussian multivariate location model. 'ForeComp' computes and visualizes the finite-sample size-power frontier of the Diebold-Mariano test based on fixed-b asymptotics together with the Bartlett kernel. To compute the finite-sample size and power, it works with the best approximating ARMA process to the given dataset. It informs the user how their choice of the truncation parameter performs and how robust the testing outcomes are.  "
  },
  {
    "id": 3524,
    "package_name": "FormulR",
    "title": "Comprehensive Tools for Drug Formulation Analysis and\nVisualization",
    "description": "This presents a comprehensive set of tools for the analysis and visualization of drug formulation data. It includes functions for statistical analysis, regression modeling, hypothesis testing, and comparative analysis to assess the impact of formulation parameters on drug release and other critical attributes. Additionally, the package offers a variety of data visualization functions, such as scatterplots, histograms, and boxplots, to facilitate the interpretation of formulation data. With its focus on usability and efficiency, this package aims to streamline the drug formulation process and aid researchers in making informed decisions during formulation design and optimization.",
    "version": "1.0.0",
    "maintainer": "Oche Ambrose George <ocheab1@gmail.com>",
    "author": "Oche Ambrose George [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3979-6232>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FormulR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FormulR Comprehensive Tools for Drug Formulation Analysis and\nVisualization This presents a comprehensive set of tools for the analysis and visualization of drug formulation data. It includes functions for statistical analysis, regression modeling, hypothesis testing, and comparative analysis to assess the impact of formulation parameters on drug release and other critical attributes. Additionally, the package offers a variety of data visualization functions, such as scatterplots, histograms, and boxplots, to facilitate the interpretation of formulation data. With its focus on usability and efficiency, this package aims to streamline the drug formulation process and aid researchers in making informed decisions during formulation design and optimization.  "
  },
  {
    "id": 3533,
    "package_name": "FracFixR",
    "title": "Compositional Statistical Framework for RNA Fractionation\nAnalysis",
    "description": "A compositional statistical framework for absolute proportion \n    estimation between fractions in RNA sequencing data. 'FracFixR' addresses \n    the fundamental challenge in fractionated RNA-seq experiments where library \n    preparation and sequencing depth obscure the original proportions of RNA \n    fractions. It reconstructs original fraction proportions using non-negative \n    linear regression, estimates the \"lost\" unrecoverable fraction, corrects \n    individual transcript frequencies, and performs differential proportion \n    testing between conditions. Supports any RNA fractionation protocol including\n    polysome profiling, sub-cellular localization, and RNA-protein complex isolation.",
    "version": "1.0.0",
    "maintainer": "Alice Cleynen <alice.cleynen@cnrs.fr>",
    "author": "Alice Cleynen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8083-0204>),\n  Agin Ravindran [aut],\n  Nikolay Shirokikh [aut] (ORCID:\n    <https://orcid.org/0000-0001-8249-358X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FracFixR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FracFixR Compositional Statistical Framework for RNA Fractionation\nAnalysis A compositional statistical framework for absolute proportion \n    estimation between fractions in RNA sequencing data. 'FracFixR' addresses \n    the fundamental challenge in fractionated RNA-seq experiments where library \n    preparation and sequencing depth obscure the original proportions of RNA \n    fractions. It reconstructs original fraction proportions using non-negative \n    linear regression, estimates the \"lost\" unrecoverable fraction, corrects \n    individual transcript frequencies, and performs differential proportion \n    testing between conditions. Supports any RNA fractionation protocol including\n    polysome profiling, sub-cellular localization, and RNA-protein complex isolation.  "
  },
  {
    "id": 3541,
    "package_name": "FunChisq",
    "title": "Model-Free Functional Chi-Squared and Exact Tests",
    "description": "Statistical hypothesis testing methods for\n inferring model-free functional dependency using asymptotic\n chi-squared or exact distributions. Functional test\n statistics are asymmetric and functionally optimal, unique\n from other related statistics. Tests in this package reveal\n evidence for causality based on the causality-by-\n functionality principle. They include asymptotic functional\n chi-squared tests (Zhang & Song 2013) <doi:10.48550/arXiv.1311.2707>,\n an adapted functional chi-squared test (Kumar & Song 2022) \n <doi:10.1093/bioinformatics/btac206>, \n and an exact functional test (Zhong & Song 2019)\n <doi:10.1109/TCBB.2018.2809743> (Nguyen et al. 2020)\n <doi:10.24963/ijcai.2020/372>. The normalized functional\n chi-squared test was used by Best Performer 'NMSUSongLab'\n in HPN-DREAM (DREAM8) Breast Cancer Network Inference\n Challenges (Hill et al. 2016) <doi:10.1038/nmeth.3773>. A\n function index (Zhong & Song 2019)\n <doi:10.1186/s12920-019-0565-9> (Kumar et al. 2018)\n <doi:10.1109/BIBM.2018.8621502> derived from the\n functional test statistic offers a new effect size measure\n for the strength of functional dependency, a better\n alternative to conditional entropy in many aspects. For\n continuous data, these tests offer an advantage over\n regression analysis when a parametric functional form\n cannot be assumed; for categorical data, they provide a\n novel means to assess directional dependency not possible\n with symmetrical Pearson's chi-squared or Fisher's exact\n tests.",
    "version": "2.5.4",
    "maintainer": "Joe Song <joemsong@cs.nmsu.edu>",
    "author": "Yang Zhang [aut],\n  Hua Zhong [aut] (ORCID: <https://orcid.org/0000-0003-1962-2603>),\n  Hien Nguyen [aut] (ORCID: <https://orcid.org/0000-0002-7237-4752>),\n  Ruby Sharma [aut] (ORCID: <https://orcid.org/0000-0001-7774-4065>),\n  Sajal Kumar [aut] (ORCID: <https://orcid.org/0000-0003-0930-1582>),\n  Yiyi Li [aut] (ORCID: <https://orcid.org/0000-0001-8859-3987>),\n  Joe Song [aut, cre] (ORCID: <https://orcid.org/0000-0002-6883-6547>)",
    "url": "https://www.cs.nmsu.edu/~joemsong/publications/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FunChisq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FunChisq Model-Free Functional Chi-Squared and Exact Tests Statistical hypothesis testing methods for\n inferring model-free functional dependency using asymptotic\n chi-squared or exact distributions. Functional test\n statistics are asymmetric and functionally optimal, unique\n from other related statistics. Tests in this package reveal\n evidence for causality based on the causality-by-\n functionality principle. They include asymptotic functional\n chi-squared tests (Zhang & Song 2013) <doi:10.48550/arXiv.1311.2707>,\n an adapted functional chi-squared test (Kumar & Song 2022) \n <doi:10.1093/bioinformatics/btac206>, \n and an exact functional test (Zhong & Song 2019)\n <doi:10.1109/TCBB.2018.2809743> (Nguyen et al. 2020)\n <doi:10.24963/ijcai.2020/372>. The normalized functional\n chi-squared test was used by Best Performer 'NMSUSongLab'\n in HPN-DREAM (DREAM8) Breast Cancer Network Inference\n Challenges (Hill et al. 2016) <doi:10.1038/nmeth.3773>. A\n function index (Zhong & Song 2019)\n <doi:10.1186/s12920-019-0565-9> (Kumar et al. 2018)\n <doi:10.1109/BIBM.2018.8621502> derived from the\n functional test statistic offers a new effect size measure\n for the strength of functional dependency, a better\n alternative to conditional entropy in many aspects. For\n continuous data, these tests offer an advantage over\n regression analysis when a parametric functional form\n cannot be assumed; for categorical data, they provide a\n novel means to assess directional dependency not possible\n with symmetrical Pearson's chi-squared or Fisher's exact\n tests.  "
  },
  {
    "id": 3551,
    "package_name": "Fuzzy.p.value",
    "title": "Computing Fuzzy p-Value",
    "description": "The main goal of this package is drawing the membership function of the fuzzy p-value which is defined as a fuzzy set on the unit interval for three following problems: (1) testing crisp hypotheses based on fuzzy data, (2) testing fuzzy hypotheses based on crisp data, and (3) testing fuzzy hypotheses based on fuzzy data. In all cases, the fuzziness of data or/and the fuzziness of the boundary of null fuzzy hypothesis transported via the p-value function and causes to produce the fuzzy p-value. If the p-value is fuzzy, it is more appropriate to consider a fuzzy significance level for the problem. Therefore, the comparison of the fuzzy p-value and the fuzzy significance level is evaluated by a fuzzy ranking method in this package.",
    "version": "1.1",
    "maintainer": "Abbas Parchami <parchami@uk.ac.ir>",
    "author": "Abbas Parchami (Department of Statistics, Faculty of Mathematics and Computer, Shahid Bahonar University of Kerman, Kerman, Iran)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Fuzzy.p.value",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Fuzzy.p.value Computing Fuzzy p-Value The main goal of this package is drawing the membership function of the fuzzy p-value which is defined as a fuzzy set on the unit interval for three following problems: (1) testing crisp hypotheses based on fuzzy data, (2) testing fuzzy hypotheses based on crisp data, and (3) testing fuzzy hypotheses based on fuzzy data. In all cases, the fuzziness of data or/and the fuzziness of the boundary of null fuzzy hypothesis transported via the p-value function and causes to produce the fuzzy p-value. If the p-value is fuzzy, it is more appropriate to consider a fuzzy significance level for the problem. Therefore, the comparison of the fuzzy p-value and the fuzzy significance level is evaluated by a fuzzy ranking method in this package.  "
  },
  {
    "id": 3564,
    "package_name": "FuzzySTs",
    "title": "Fuzzy Statistical Tools",
    "description": "The main goal of this package is to present various fuzzy statistical tools. It intends to provide an implementation of the theoretical and empirical approaches presented in the book entitled \"The signed distance measure in fuzzy statistical analysis. Some theoretical, empirical and programming advances\" <doi: 10.1007/978-3-030-76916-1>. For the theoretical approaches, see Berkachy R. and Donze L. (2019) <doi:10.1007/978-3-030-03368-2_1>. For the empirical approaches, see Berkachy R. and Donze L. (2016) <ISBN: 978-989-758-201-1>). Important (non-exhaustive) implementation highlights of this package are as follows: (1) a numerical procedure to estimate the fuzzy difference and the fuzzy square. (2) two numerical methods of fuzzification. (3) a function performing different possibilities of distances, including the signed distance and the generalized signed distance for instance with all its properties. (4) numerical estimations of fuzzy statistical measures such as the variance, the moment, etc. (5) two methods of estimation of the bootstrap distribution of the likelihood ratio in the fuzzy context. (6) an estimation of a fuzzy confidence interval by the likelihood ratio method. (7) testing fuzzy hypotheses and/or fuzzy data by fuzzy confidence intervals in the Kwakernaak - Kruse and Meyer sense. (8) a general method to estimate the fuzzy p-value with fuzzy hypotheses and/or fuzzy data. (9) a method of estimation of global and individual evaluations of linguistic questionnaires. (10) numerical estimations of multi-ways analysis of variance models in the fuzzy context. The unbalance in the considered designs are also foreseen. ",
    "version": "0.4",
    "maintainer": "Redina Berkachy <redina.berkachy@hefr.ch>",
    "author": "Redina Berkachy [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7491-0416>),\n  Laurent Donze [aut] (ORCID: <https://orcid.org/0000-0003-3522-4672>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=FuzzySTs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FuzzySTs Fuzzy Statistical Tools The main goal of this package is to present various fuzzy statistical tools. It intends to provide an implementation of the theoretical and empirical approaches presented in the book entitled \"The signed distance measure in fuzzy statistical analysis. Some theoretical, empirical and programming advances\" <doi: 10.1007/978-3-030-76916-1>. For the theoretical approaches, see Berkachy R. and Donze L. (2019) <doi:10.1007/978-3-030-03368-2_1>. For the empirical approaches, see Berkachy R. and Donze L. (2016) <ISBN: 978-989-758-201-1>). Important (non-exhaustive) implementation highlights of this package are as follows: (1) a numerical procedure to estimate the fuzzy difference and the fuzzy square. (2) two numerical methods of fuzzification. (3) a function performing different possibilities of distances, including the signed distance and the generalized signed distance for instance with all its properties. (4) numerical estimations of fuzzy statistical measures such as the variance, the moment, etc. (5) two methods of estimation of the bootstrap distribution of the likelihood ratio in the fuzzy context. (6) an estimation of a fuzzy confidence interval by the likelihood ratio method. (7) testing fuzzy hypotheses and/or fuzzy data by fuzzy confidence intervals in the Kwakernaak - Kruse and Meyer sense. (8) a general method to estimate the fuzzy p-value with fuzzy hypotheses and/or fuzzy data. (9) a method of estimation of global and individual evaluations of linguistic questionnaires. (10) numerical estimations of multi-ways analysis of variance models in the fuzzy context. The unbalance in the considered designs are also foreseen.   "
  },
  {
    "id": 3612,
    "package_name": "GEC",
    "title": "Generalized Exponentiated Composite Distributions",
    "description": "Contains the framework of the estimation, sampling, and hypotheses testing for two special distributions (Exponentiated Exponential-Pareto and Exponentiated Inverse Gamma-Pareto) within the family of Generalized Exponentiated Composite distributions. The detailed explanation and the applications of these two distributions were introduced in Bowen Liu, Malwane M.A. Ananda (2022) <doi:10.1080/03610926.2022.2050399>, Bowen Liu, Malwane M.A. Ananda (2022) <doi:10.3390/math10111895>, and Bowen Liu, Malwane M.A. Ananda (2022) <doi:10.3390/app13010645>. ",
    "version": "0.1.0",
    "maintainer": "Bowen Liu <bowen.liu@umkc.edu>",
    "author": "Bowen Liu [aut, cre],\n  Malwane M.A. Ananda [aut],\n  Dwaipayan Mukhopadhyay [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GEC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GEC Generalized Exponentiated Composite Distributions Contains the framework of the estimation, sampling, and hypotheses testing for two special distributions (Exponentiated Exponential-Pareto and Exponentiated Inverse Gamma-Pareto) within the family of Generalized Exponentiated Composite distributions. The detailed explanation and the applications of these two distributions were introduced in Bowen Liu, Malwane M.A. Ananda (2022) <doi:10.1080/03610926.2022.2050399>, Bowen Liu, Malwane M.A. Ananda (2022) <doi:10.3390/math10111895>, and Bowen Liu, Malwane M.A. Ananda (2022) <doi:10.3390/app13010645>.   "
  },
  {
    "id": 3615,
    "package_name": "GEEmediate",
    "title": "Mediation Analysis for Generalized Linear Models Using the\nDifference Method",
    "description": "Causal mediation analysis for a single exposure/treatment and a\n    single mediator, both allowed to be either continuous or binary. The package\n    implements the difference method and provides point and interval estimates as\n    well as testing for the natural direct and indirect effects and the mediation\n    proportion. Nevo, Xiao and Spiegelman (2017) <doi:10.1515/ijb-2017-0006>.",
    "version": "1.1.4",
    "maintainer": "Daniel Nevo <danielnevo@gmail.com>",
    "author": "Daniel Nevo [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/daniel258/GEEMediate/issues",
    "repository": "https://cran.r-project.org/package=GEEmediate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GEEmediate Mediation Analysis for Generalized Linear Models Using the\nDifference Method Causal mediation analysis for a single exposure/treatment and a\n    single mediator, both allowed to be either continuous or binary. The package\n    implements the difference method and provides point and interval estimates as\n    well as testing for the natural direct and indirect effects and the mediation\n    proportion. Nevo, Xiao and Spiegelman (2017) <doi:10.1515/ijb-2017-0006>.  "
  },
  {
    "id": 3625,
    "package_name": "GET",
    "title": "Global Envelopes",
    "description": "Implementation of global envelopes for a set of general d-dimensional vectors T\n    in various applications. A 100(1-alpha)% global envelope is a band bounded by two\n    vectors such that the probability that T falls outside this envelope in any of the d\n    points is equal to alpha. Global means that the probability is controlled simultaneously\n    for all the d elements of the vectors. The global envelopes can be used for graphical\n    Monte Carlo and permutation tests where the test statistic is a multivariate vector or\n    function (e.g. goodness-of-fit testing for point patterns and random sets, functional\n    analysis of variance, functional general linear model, n-sample test of correspondence\n    of distribution functions), for central regions of functional or multivariate data (e.g.\n    outlier detection, functional boxplot) and for global confidence and prediction bands\n    (e.g. confidence band in polynomial regression, Bayesian posterior prediction). See\n    Myllym\u00e4ki and Mrkvi\u010dka (2024) <doi:10.18637/jss.v111.i03>,\n    Myllym\u00e4ki et al. (2017) <doi:10.1111/rssb.12172>,\n    Mrkvi\u010dka and Myllym\u00e4ki (2023) <doi:10.1007/s11222-023-10275-7>,\n    Mrkvi\u010dka et al. (2016) <doi:10.1016/j.spasta.2016.04.005>,\n    Mrkvi\u010dka et al. (2017) <doi:10.1007/s11222-016-9683-9>,\n    Mrkvi\u010dka et al. (2020) <doi:10.14736/kyb-2020-3-0432>,\n    Mrkvi\u010dka et al. (2021) <doi:10.1007/s11009-019-09756-y>,\n    Myllym\u00e4ki et al. (2021) <doi:10.1016/j.spasta.2020.100436>,\n    Mrkvi\u010dka et al. (2022) <doi:10.1002/sim.9236>,\n    Dai et al. (2022) <doi:10.5772/intechopen.100124>,\n    Dvo\u0159\u00e1k and Mrkvi\u010dka (2022) <doi:10.1007/s00180-021-01134-y>,\n    Mrkvi\u010dka et al. (2023) <doi:10.48550/arXiv.2309.04746>, and\n    Konstantinou et al. (2024) <doi: 10.1007/s00180-024-01569-z>.",
    "version": "1.0-7",
    "maintainer": "Mari Myllym\u00e4ki <mari.myllymaki@luke.fi>",
    "author": "Mari Myllym\u00e4ki [aut, cre],\n  Tom\u00e1\u0161 Mrkvi\u010dka [aut],\n  Mikko Kuronen [ctb],\n  Ji\u0159\u00ed Dvo\u0159\u00e1k [ctb],\n  Pavel Grabarnik [ctb],\n  Ute Hahn [ctb],\n  Michael Rost [ctb],\n  Henri Seijo [ctb]",
    "url": "https://github.com/myllym/GET",
    "bug_reports": "https://github.com/myllym/GET/issues",
    "repository": "https://cran.r-project.org/package=GET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GET Global Envelopes Implementation of global envelopes for a set of general d-dimensional vectors T\n    in various applications. A 100(1-alpha)% global envelope is a band bounded by two\n    vectors such that the probability that T falls outside this envelope in any of the d\n    points is equal to alpha. Global means that the probability is controlled simultaneously\n    for all the d elements of the vectors. The global envelopes can be used for graphical\n    Monte Carlo and permutation tests where the test statistic is a multivariate vector or\n    function (e.g. goodness-of-fit testing for point patterns and random sets, functional\n    analysis of variance, functional general linear model, n-sample test of correspondence\n    of distribution functions), for central regions of functional or multivariate data (e.g.\n    outlier detection, functional boxplot) and for global confidence and prediction bands\n    (e.g. confidence band in polynomial regression, Bayesian posterior prediction). See\n    Myllym\u00e4ki and Mrkvi\u010dka (2024) <doi:10.18637/jss.v111.i03>,\n    Myllym\u00e4ki et al. (2017) <doi:10.1111/rssb.12172>,\n    Mrkvi\u010dka and Myllym\u00e4ki (2023) <doi:10.1007/s11222-023-10275-7>,\n    Mrkvi\u010dka et al. (2016) <doi:10.1016/j.spasta.2016.04.005>,\n    Mrkvi\u010dka et al. (2017) <doi:10.1007/s11222-016-9683-9>,\n    Mrkvi\u010dka et al. (2020) <doi:10.14736/kyb-2020-3-0432>,\n    Mrkvi\u010dka et al. (2021) <doi:10.1007/s11009-019-09756-y>,\n    Myllym\u00e4ki et al. (2021) <doi:10.1016/j.spasta.2020.100436>,\n    Mrkvi\u010dka et al. (2022) <doi:10.1002/sim.9236>,\n    Dai et al. (2022) <doi:10.5772/intechopen.100124>,\n    Dvo\u0159\u00e1k and Mrkvi\u010dka (2022) <doi:10.1007/s00180-021-01134-y>,\n    Mrkvi\u010dka et al. (2023) <doi:10.48550/arXiv.2309.04746>, and\n    Konstantinou et al. (2024) <doi: 10.1007/s00180-024-01569-z>.  "
  },
  {
    "id": 3629,
    "package_name": "GEint",
    "title": "Misspecified Models for Gene-Environment Interaction",
    "description": "The first major functionality is to compute the bias in regression coefficients of misspecified linear gene-environment interaction models. The most generalized function for this objective is GE_bias(). However GE_bias() requires specification of many higher order moments of covariates in the model. If users are unsure about how to calculate/estimate these higher order moments, it may be easier to use GE_bias_normal_squaredmis(). This function places many more assumptions on the covariates (most notably that they are all jointly generated from a multivariate normal distribution) and is thus able to automatically calculate many of the higher order moments automatically, necessitating only that the user specify some covariances. There are also functions to solve for the bias through simulation and non-linear equation solvers; these can be used to check your work. Second major functionality is to implement the Bootstrap Inference with Correct Sandwich (BICS) testing procedure, which we have found to provide better finite-sample performance than other inference procedures for testing GxE interaction. More details on these functions are available in Sun, Carroll, Christiani, and Lin (2018) <doi:10.1111/biom.12813>.",
    "version": "1.1",
    "maintainer": "Ryan Sun <ryansun.work@gmail.com>",
    "author": "Ryan Sun [aut, cre],\n  Richard Barfield [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GEint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GEint Misspecified Models for Gene-Environment Interaction The first major functionality is to compute the bias in regression coefficients of misspecified linear gene-environment interaction models. The most generalized function for this objective is GE_bias(). However GE_bias() requires specification of many higher order moments of covariates in the model. If users are unsure about how to calculate/estimate these higher order moments, it may be easier to use GE_bias_normal_squaredmis(). This function places many more assumptions on the covariates (most notably that they are all jointly generated from a multivariate normal distribution) and is thus able to automatically calculate many of the higher order moments automatically, necessitating only that the user specify some covariances. There are also functions to solve for the bias through simulation and non-linear equation solvers; these can be used to check your work. Second major functionality is to implement the Bootstrap Inference with Correct Sandwich (BICS) testing procedure, which we have found to provide better finite-sample performance than other inference procedures for testing GxE interaction. More details on these functions are available in Sun, Carroll, Christiani, and Lin (2018) <doi:10.1111/biom.12813>.  "
  },
  {
    "id": 3633,
    "package_name": "GFDmcv",
    "title": "General Hypothesis Testing Problems for Multivariate\nCoefficients of Variation",
    "description": "Performs test procedures for general hypothesis testing problems for four multivariate coefficients of variation (Ditzhaus and Smaga, 2023 <arXiv:2301.12009>). We can verify the global hypothesis about equality as well as the particular hypotheses defined by contrasts, e.g., we can conduct post hoc tests. We also provide the simultaneous confidence intervals for contrasts.",
    "version": "0.1.0",
    "maintainer": "Lukasz Smaga <ls@amu.edu.pl>",
    "author": "Marc Ditzhaus [aut],\n  Lukasz Smaga [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GFDmcv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GFDmcv General Hypothesis Testing Problems for Multivariate\nCoefficients of Variation Performs test procedures for general hypothesis testing problems for four multivariate coefficients of variation (Ditzhaus and Smaga, 2023 <arXiv:2301.12009>). We can verify the global hypothesis about equality as well as the particular hypotheses defined by contrasts, e.g., we can conduct post hoc tests. We also provide the simultaneous confidence intervals for contrasts.  "
  },
  {
    "id": 3675,
    "package_name": "GLMaSPU",
    "title": "An Adaptive Test on High Dimensional Parameters in Generalized\nLinear Models",
    "description": "Several tests for high dimensional generalized linear models have been proposed recently. In this package, we implemented a new test called adaptive  sum of powered score (aSPU) for high dimensional generalized linear models, which is often more powerful than the existing methods in a wide scenarios. We also implemented permutation based version of several existing methods for research purpose. We recommend users use the aSPU test for their real testing problem. You can learn more about the tests implemented in the package via the following papers: 1. Pan, W., Kim, J., Zhang, Y., Shen, X. and Wei, P. (2014) <DOI:10.1534/genetics.114.165035> A powerful and adaptive association test for rare variants, Genetics, 197(4). 2. Guo, B., and Chen, S. X. (2016) <DOI:10.1111/rssb.12152>. Tests for high dimensional generalized linear models. Journal of the Royal Statistical Society: Series B. 3. Goeman, J. J., Van Houwelingen, H. C., and Finos, L. (2011) <DOI:10.1093/biomet/asr016>. Testing against a high-dimensional alternative in the generalized linear model: asymptotic type I error control. Biometrika, 98(2).",
    "version": "1.0",
    "maintainer": "Chong Wu <wuxx0845@umn.edu>",
    "author": "Chong Wu and Wei Pan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GLMaSPU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GLMaSPU An Adaptive Test on High Dimensional Parameters in Generalized\nLinear Models Several tests for high dimensional generalized linear models have been proposed recently. In this package, we implemented a new test called adaptive  sum of powered score (aSPU) for high dimensional generalized linear models, which is often more powerful than the existing methods in a wide scenarios. We also implemented permutation based version of several existing methods for research purpose. We recommend users use the aSPU test for their real testing problem. You can learn more about the tests implemented in the package via the following papers: 1. Pan, W., Kim, J., Zhang, Y., Shen, X. and Wei, P. (2014) <DOI:10.1534/genetics.114.165035> A powerful and adaptive association test for rare variants, Genetics, 197(4). 2. Guo, B., and Chen, S. X. (2016) <DOI:10.1111/rssb.12152>. Tests for high dimensional generalized linear models. Journal of the Royal Statistical Society: Series B. 3. Goeman, J. J., Van Houwelingen, H. C., and Finos, L. (2011) <DOI:10.1093/biomet/asr016>. Testing against a high-dimensional alternative in the generalized linear model: asymptotic type I error control. Biometrika, 98(2).  "
  },
  {
    "id": 3737,
    "package_name": "GROAN",
    "title": "Genomic Regression Workbench",
    "description": "Workbench for testing genomic regression accuracy on\n    (optionally noisy) phenotypes.",
    "version": "1.3.1",
    "maintainer": "Nelson Nazzicari <nelson.nazzicari@gmail.com>",
    "author": "Nelson Nazzicari & Filippo Biscarini",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GROAN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GROAN Genomic Regression Workbench Workbench for testing genomic regression accuracy on\n    (optionally noisy) phenotypes.  "
  },
  {
    "id": 3739,
    "package_name": "GRSxE",
    "title": "Testing Gene-Environment Interactions Through Genetic Risk\nScores",
    "description": "Statistical testing procedures for detecting\n  GxE (gene-environment) interactions. The main focus lies on\n  GRSxE interaction tests that aim at detecting GxE interactions\n  through GRS (genetic risk scores). Moreover, a novel testing\n  procedure based on bagging and OOB (out-of-bag) predictions is\n  implemented for incorporating all available observations at\n  both GRS construction and GxE testing (Lau et al., 2023,\n  <doi:10.1038/s41598-023-28172-4>).",
    "version": "1.0.1",
    "maintainer": "Michael Lau <michael.lau@hhu.de>",
    "author": "Michael Lau [aut, cre] (ORCID: <https://orcid.org/0000-0002-5327-8351>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GRSxE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GRSxE Testing Gene-Environment Interactions Through Genetic Risk\nScores Statistical testing procedures for detecting\n  GxE (gene-environment) interactions. The main focus lies on\n  GRSxE interaction tests that aim at detecting GxE interactions\n  through GRS (genetic risk scores). Moreover, a novel testing\n  procedure based on bagging and OOB (out-of-bag) predictions is\n  implemented for incorporating all available observations at\n  both GRS construction and GxE testing (Lau et al., 2023,\n  <doi:10.1038/s41598-023-28172-4>).  "
  },
  {
    "id": 3759,
    "package_name": "GTRT",
    "title": "Graph Theoretic Randomness Tests",
    "description": "A collection of functions for testing randomness (or mutual independence) in linear and circular data as proposed in Gehlot and Laha (2025a) <doi:10.48550/arXiv.2506.21157> and Gehlot and Laha (2025b) <doi:10.48550/arXiv.2506.23522>, respectively. ",
    "version": "0.1.0",
    "maintainer": "Shriya Gehlot <phd20shriyag@iima.ac.in>",
    "author": "Shriya Gehlot [aut, cre],\n  Arnab Kumar Laha [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GTRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GTRT Graph Theoretic Randomness Tests A collection of functions for testing randomness (or mutual independence) in linear and circular data as proposed in Gehlot and Laha (2025a) <doi:10.48550/arXiv.2506.21157> and Gehlot and Laha (2025b) <doi:10.48550/arXiv.2506.23522>, respectively.   "
  },
  {
    "id": 3765,
    "package_name": "GUniFrac",
    "title": "Generalized UniFrac Distances, Distance-Based Multivariate\nMethods and Feature-Based Univariate Methods for Microbiome\nData Analysis",
    "description": "A suite of methods for powerful and robust microbiome data analysis including data normalization, data simulation, community-level association testing and differential abundance analysis. It implements generalized UniFrac distances,  Geometric Mean of Pairwise Ratios (GMPR) normalization, semiparametric data simulator, distance-based statistical methods, and feature-based statistical methods. The distance-based statistical methods include three extensions of PERMANOVA: (1) PERMANOVA using the Freedman-Lane permutation scheme, (2) PERMANOVA omnibus test using multiple matrices, and  (3) analytical approach to approximating PERMANOVA p-value. Feature-based statistical methods include linear model-based methods for differential abundance analysis of zero-inflated high-dimensional compositional data. ",
    "version": "1.9",
    "maintainer": "Jun Chen <chen.jun2@mayo.edu>",
    "author": "Jun Chen [aut, cre],\n  Xianyang Zhang [aut],\n  Lu Yang [aut],\n  Lujun Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GUniFrac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GUniFrac Generalized UniFrac Distances, Distance-Based Multivariate\nMethods and Feature-Based Univariate Methods for Microbiome\nData Analysis A suite of methods for powerful and robust microbiome data analysis including data normalization, data simulation, community-level association testing and differential abundance analysis. It implements generalized UniFrac distances,  Geometric Mean of Pairwise Ratios (GMPR) normalization, semiparametric data simulator, distance-based statistical methods, and feature-based statistical methods. The distance-based statistical methods include three extensions of PERMANOVA: (1) PERMANOVA using the Freedman-Lane permutation scheme, (2) PERMANOVA omnibus test using multiple matrices, and  (3) analytical approach to approximating PERMANOVA p-value. Feature-based statistical methods include linear model-based methods for differential abundance analysis of zero-inflated high-dimensional compositional data.   "
  },
  {
    "id": 3768,
    "package_name": "GWASExactHW",
    "title": "Exact Hardy-Weinburg Testing for Genome Wide Association Studies",
    "description": "\n  Exact Hardy-Weinburg testing (using Fisher's test) for SNP genotypes\n  as typically obtained in a Genome Wide Association Study (GWAS).",
    "version": "1.2",
    "maintainer": "Stephanie Gogarten <sdmorris@uw.edu>",
    "author": "Ian Painter, University of Washington",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GWASExactHW",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GWASExactHW Exact Hardy-Weinburg Testing for Genome Wide Association Studies \n  Exact Hardy-Weinburg testing (using Fisher's test) for SNP genotypes\n  as typically obtained in a Genome Wide Association Study (GWAS).  "
  },
  {
    "id": 3808,
    "package_name": "GeneF",
    "title": "Package for Generalized F-Statistics",
    "description": "Implementation of several generalized F-statistics. The\n    current version includes a generalized F-statistic based on the\n    flexible isotonic/monotonic regression or order restricted hypothesis\n    testing. Based on: Y. Lai (2011) <doi:10.1371/journal.pone.0019754>.",
    "version": "1.0.1",
    "maintainer": "Yinglei Lai <ylai@gwu.edu>",
    "author": "Yinglei Lai [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GeneF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeneF Package for Generalized F-Statistics Implementation of several generalized F-statistics. The\n    current version includes a generalized F-statistic based on the\n    flexible isotonic/monotonic regression or order restricted hypothesis\n    testing. Based on: Y. Lai (2011) <doi:10.1371/journal.pone.0019754>.  "
  },
  {
    "id": 3815,
    "package_name": "GeneralisedCovarianceMeasure",
    "title": "Test for Conditional Independence Based on the Generalized\nCovariance Measure (GCM)",
    "description": "A statistical hypothesis test for conditional independence. It performs nonlinear regressions on the conditioning variable and then tests for a vanishing covariance between the resulting residuals. It can be applied to both univariate random variables and multivariate random vectors. Details of the method can be found in Rajen D. Shah and Jonas Peters: The Hardness of Conditional Independence Testing and the Generalised Covariance Measure, Annals of Statistics 48(3), 1514--1538, 2020.",
    "version": "0.2.0",
    "maintainer": "Jonas Peters <jonas.peters@math.ku.dk>",
    "author": "Jonas Peters and Rajen D. Shah",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GeneralisedCovarianceMeasure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeneralisedCovarianceMeasure Test for Conditional Independence Based on the Generalized\nCovariance Measure (GCM) A statistical hypothesis test for conditional independence. It performs nonlinear regressions on the conditioning variable and then tests for a vanishing covariance between the resulting residuals. It can be applied to both univariate random variables and multivariate random vectors. Details of the method can be found in Rajen D. Shah and Jonas Peters: The Hardness of Conditional Independence Testing and the Generalised Covariance Measure, Annals of Statistics 48(3), 1514--1538, 2020.  "
  },
  {
    "id": 3853,
    "package_name": "GillespieSSA2",
    "title": "Gillespie's Stochastic Simulation Algorithm for Impatient People",
    "description": "A fast, scalable, and versatile framework for\n    simulating large systems with Gillespie's Stochastic Simulation\n    Algorithm ('SSA').  This package is the spiritual successor to the\n    'GillespieSSA' package originally written by Mario Pineda-Krch.\n    Benefits of this package include major speed improvements (>100x),\n    easier to understand documentation, and many unit tests that try to\n    ensure the package works as intended. Cannoodt and Saelens et al. (2021) \n    <doi:10.1038/s41467-021-24152-2>.",
    "version": "0.3.0",
    "maintainer": "Robrecht Cannoodt <rcannood@gmail.com>",
    "author": "Robrecht Cannoodt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3641-729X>),\n  Wouter Saelens [aut] (ORCID: <https://orcid.org/0000-0002-7114-6248>)",
    "url": "https://rcannood.github.io/GillespieSSA2/,\nhttps://github.com/rcannood/GillespieSSA2",
    "bug_reports": "https://github.com/rcannood/GillespieSSA2/issues",
    "repository": "https://cran.r-project.org/package=GillespieSSA2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GillespieSSA2 Gillespie's Stochastic Simulation Algorithm for Impatient People A fast, scalable, and versatile framework for\n    simulating large systems with Gillespie's Stochastic Simulation\n    Algorithm ('SSA').  This package is the spiritual successor to the\n    'GillespieSSA' package originally written by Mario Pineda-Krch.\n    Benefits of this package include major speed improvements (>100x),\n    easier to understand documentation, and many unit tests that try to\n    ensure the package works as intended. Cannoodt and Saelens et al. (2021) \n    <doi:10.1038/s41467-021-24152-2>.  "
  },
  {
    "id": 3861,
    "package_name": "Glarmadillo",
    "title": "Solve the Graphical Lasso Problem with 'Armadillo'",
    "description": "Efficiently implements the Graphical Lasso algorithm,\n            utilizing the 'Armadillo' 'C++' library for rapid computation. This algorithm \n            introduces an L1 penalty to derive sparse inverse covariance matrices from \n            observations of multivariate normal distributions. Features include the \n            generation of random and structured sparse covariance matrices, beneficial \n            for simulations, statistical method testing, and educational purposes in \n            graphical modeling. A unique function for regularization parameter selection \n            based on predefined sparsity levels is also offered, catering to users with\n            specific sparsity requirements in their models. The methodology for sparse \n            inverse covariance estimation implemented in this package is based on the \n            work of Friedman, Hastie, and Tibshirani (2008) <doi:10.1093/biostatistics/kxm045>.",
    "version": "1.1.1",
    "maintainer": "Alessandro Meng <mengfangeng@ruc.edu.cn>",
    "author": "Alessandro Meng [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Glarmadillo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Glarmadillo Solve the Graphical Lasso Problem with 'Armadillo' Efficiently implements the Graphical Lasso algorithm,\n            utilizing the 'Armadillo' 'C++' library for rapid computation. This algorithm \n            introduces an L1 penalty to derive sparse inverse covariance matrices from \n            observations of multivariate normal distributions. Features include the \n            generation of random and structured sparse covariance matrices, beneficial \n            for simulations, statistical method testing, and educational purposes in \n            graphical modeling. A unique function for regularization parameter selection \n            based on predefined sparsity levels is also offered, catering to users with\n            specific sparsity requirements in their models. The methodology for sparse \n            inverse covariance estimation implemented in this package is based on the \n            work of Friedman, Hastie, and Tibshirani (2008) <doi:10.1093/biostatistics/kxm045>.  "
  },
  {
    "id": 3868,
    "package_name": "GoFKernel",
    "title": "Testing Goodness-of-Fit with the Kernel Density Estimator",
    "description": "Tests of goodness-of-fit based on a kernel smoothing of the data.\n    References:\n       Pav\u00eda (2015) <doi:10.18637/jss.v066.c01>.",
    "version": "2.1-3",
    "maintainer": "Jose M. Pav\u00eda <jose.m.pavia@uv.es>",
    "author": "Jose M. Pav\u00eda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0129-726X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GoFKernel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GoFKernel Testing Goodness-of-Fit with the Kernel Density Estimator Tests of goodness-of-fit based on a kernel smoothing of the data.\n    References:\n       Pav\u00eda (2015) <doi:10.18637/jss.v066.c01>.  "
  },
  {
    "id": 3871,
    "package_name": "GofKmt",
    "title": "Khmaladze Martingale Transformation Goodness-of-Fit Test",
    "description": "Consider a goodness-of-fit (GOF) problem of testing whether a random sample comes from one sample location-scale model where location and scale parameters are unknown. It is well known that Khmaladze martingale transformation method - which was proposed by Khmaladze (1981) <DOI:10.1137/1126027> - provides asymptotic distribution free test for the GOF problem. This package contains one function: KhmaladzeTrans(). In this version, KhmaladzeTrans() provides test statistic and critical value of GOF test for normal, Cauchy, and logistic distributions. This package used the main algorithm proposed by Kim (2020) <DOI:10.1007/s00180-020-00971-7> and tests for other distributions will be available at the later version.",
    "version": "2.2.0",
    "maintainer": "Jiwoong Kim <jwboys26@gmail.com>",
    "author": "Jiwoong Kim <jwboys26 at gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GofKmt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GofKmt Khmaladze Martingale Transformation Goodness-of-Fit Test Consider a goodness-of-fit (GOF) problem of testing whether a random sample comes from one sample location-scale model where location and scale parameters are unknown. It is well known that Khmaladze martingale transformation method - which was proposed by Khmaladze (1981) <DOI:10.1137/1126027> - provides asymptotic distribution free test for the GOF problem. This package contains one function: KhmaladzeTrans(). In this version, KhmaladzeTrans() provides test statistic and critical value of GOF test for normal, Cauchy, and logistic distributions. This package used the main algorithm proposed by Kim (2020) <DOI:10.1007/s00180-020-00971-7> and tests for other distributions will be available at the later version.  "
  },
  {
    "id": 3890,
    "package_name": "GreyModel",
    "title": "Fitting and Forecasting of Grey Model",
    "description": "Testing, Implementation and Forecasting of Grey Model (GM(1, 1)). For method details see Hsu, L. and Wang, C. (2007). <doi:10.1016/j.techfore.2006.02.005>. ",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "author": "Mrinmoy Ray [aut, cre],\n  Rajeev Ranjan kumar [aut, ctb],\n  K.N. Singh [ctb],\n  Ramasubramanian V. [ctb],\n  Kanchan Sinha [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GreyModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GreyModel Fitting and Forecasting of Grey Model Testing, Implementation and Forecasting of Grey Model (GM(1, 1)). For method details see Hsu, L. and Wang, C. (2007). <doi:10.1016/j.techfore.2006.02.005>.   "
  },
  {
    "id": 3896,
    "package_name": "GroupTest",
    "title": "Multiple Testing Procedure for Grouped Hypotheses",
    "description": "Contains functions for a two-stage multiple testing procedure for grouped hypothesis, aiming at controlling both the total posterior false discovery rate and within-group false discovery rate. ",
    "version": "1.0.1",
    "maintainer": "Zhigen Zhao <zhaozhg@temple.edu>",
    "author": "Zhigen Zhao",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GroupTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GroupTest Multiple Testing Procedure for Grouped Hypotheses Contains functions for a two-stage multiple testing procedure for grouped hypothesis, aiming at controlling both the total posterior false discovery rate and within-group false discovery rate.   "
  },
  {
    "id": 3916,
    "package_name": "HCTR",
    "title": "Higher Criticism Tuned Regression",
    "description": "A novel searching scheme for tuning parameter in high-dimensional \n             penalized regression. We propose a new estimate of the regularization\n             parameter based on an estimated lower bound of the proportion of false \n             null hypotheses (Meinshausen and Rice (2006) <doi:10.1214/009053605000000741>).\n             The bound is estimated by applying the empirical null distribution of the higher \n             criticism statistic, a second-level significance testing, which is constructed\n             by dependent p-values from a multi-split regression and aggregation method\n             (Jeng, Zhang and Tzeng (2019) <doi:10.1080/01621459.2018.1518236>). An estimate \n             of tuning parameter in penalized regression is decided corresponding to the lower \n             bound of the proportion of false null hypotheses. Different penalized \n             regression methods are provided in the multi-split algorithm. ",
    "version": "0.1.1",
    "maintainer": "Tao Jiang <tjiang8@ncsu.edu>",
    "author": "Tao Jiang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HCTR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HCTR Higher Criticism Tuned Regression A novel searching scheme for tuning parameter in high-dimensional \n             penalized regression. We propose a new estimate of the regularization\n             parameter based on an estimated lower bound of the proportion of false \n             null hypotheses (Meinshausen and Rice (2006) <doi:10.1214/009053605000000741>).\n             The bound is estimated by applying the empirical null distribution of the higher \n             criticism statistic, a second-level significance testing, which is constructed\n             by dependent p-values from a multi-split regression and aggregation method\n             (Jeng, Zhang and Tzeng (2019) <doi:10.1080/01621459.2018.1518236>). An estimate \n             of tuning parameter in penalized regression is decided corresponding to the lower \n             bound of the proportion of false null hypotheses. Different penalized \n             regression methods are provided in the multi-split algorithm.   "
  },
  {
    "id": 3927,
    "package_name": "HDLSSkST",
    "title": "Distribution-Free Exact High Dimensional Low Sample Size\nk-Sample Tests",
    "description": "Testing homogeneity of k multivariate distributions is a classical and challenging problem in\n             statistics, and this becomes even more challenging when the dimension of the data exceeds the sample size.\n             We construct some tests for this purpose which are exact level (size) alpha tests based on clustering. \n             These tests are easy to implement and distribution-free in finite sample situations. Under appropriate \n             regularity conditions, these tests have the consistency property in HDLSS asymptotic regime, where the \n             dimension of data grows to infinity while the sample size remains fixed. We also consider a multiscale \n             approach, where the results for different number of partitions are aggregated judiciously. Details are in \n             Biplab Paul, Shyamal K De and Anil K Ghosh (2021) <doi:10.1016/j.jmva.2021.104897>; Soham Sarkar and Anil K Ghosh (2019) \n             <doi:10.1109/TPAMI.2019.2912599>; William M Rand (1971) <doi:10.1080/01621459.1971.10482356>;  \n             Cyrus R Mehta and Nitin R Patel (1983) <doi:10.2307/2288652>; Joseph C Dunn (1973) \n             <doi:10.1080/01969727308546046>; Sture Holm (1979) <doi:10.2307/4615733>; \n             Yoav Benjamini and Yosef Hochberg (1995) <doi: 10.2307/2346101>.",
    "version": "2.1.0",
    "maintainer": "Biplab Paul <paul.biplab497@gmail.com>",
    "author": "Biplab Paul [aut, cre],\n  Shyamal K. De [aut],\n  Anil K. Ghosh [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HDLSSkST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDLSSkST Distribution-Free Exact High Dimensional Low Sample Size\nk-Sample Tests Testing homogeneity of k multivariate distributions is a classical and challenging problem in\n             statistics, and this becomes even more challenging when the dimension of the data exceeds the sample size.\n             We construct some tests for this purpose which are exact level (size) alpha tests based on clustering. \n             These tests are easy to implement and distribution-free in finite sample situations. Under appropriate \n             regularity conditions, these tests have the consistency property in HDLSS asymptotic regime, where the \n             dimension of data grows to infinity while the sample size remains fixed. We also consider a multiscale \n             approach, where the results for different number of partitions are aggregated judiciously. Details are in \n             Biplab Paul, Shyamal K De and Anil K Ghosh (2021) <doi:10.1016/j.jmva.2021.104897>; Soham Sarkar and Anil K Ghosh (2019) \n             <doi:10.1109/TPAMI.2019.2912599>; William M Rand (1971) <doi:10.1080/01621459.1971.10482356>;  \n             Cyrus R Mehta and Nitin R Patel (1983) <doi:10.2307/2288652>; Joseph C Dunn (1973) \n             <doi:10.1080/01969727308546046>; Sture Holm (1979) <doi:10.2307/4615733>; \n             Yoav Benjamini and Yosef Hochberg (1995) <doi: 10.2307/2346101>.  "
  },
  {
    "id": 3930,
    "package_name": "HDMT",
    "title": "A Multiple Testing Procedure for High-Dimensional Mediation\nHypotheses",
    "description": "A multiple-testing procedure for high-dimensional mediation hypotheses. Mediation analysis is of rising interest in epidemiology and clinical trials. Among existing methods for mediation analyses, the popular joint significance (JS) test yields an overly conservative type I error rate and therefore low power. In the R package 'HDMT' we implement a multiple-testing procedure that accurately controls the family-wise error rate (FWER) and the false discovery rate (FDR) when using JS for testing high-dimensional mediation hypotheses. The core of our procedure is based on estimating the proportions of three component null hypotheses and deriving the corresponding mixture distribution of null p-values. Results of the data examples include better-behaved quantile-quantile plots and improved detection of novel mediation relationships on the role of DNA methylation in genetic regulation of gene expression. With increasing interest in mediation by molecular intermediaries such as gene expression, the proposed method addresses an unmet methodological challenge. Methods used in the package refer to James Y. Dai, Janet L. Stanford & Michael LeBlanc (2020) <doi:10.1080/01621459.2020.1765785>.",
    "version": "1.0.5",
    "maintainer": "James Dai <jdai@fredhutch.org>",
    "author": "James Dai [aut, cre],\n  Xiaoyu Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HDMT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDMT A Multiple Testing Procedure for High-Dimensional Mediation\nHypotheses A multiple-testing procedure for high-dimensional mediation hypotheses. Mediation analysis is of rising interest in epidemiology and clinical trials. Among existing methods for mediation analyses, the popular joint significance (JS) test yields an overly conservative type I error rate and therefore low power. In the R package 'HDMT' we implement a multiple-testing procedure that accurately controls the family-wise error rate (FWER) and the false discovery rate (FDR) when using JS for testing high-dimensional mediation hypotheses. The core of our procedure is based on estimating the proportions of three component null hypotheses and deriving the corresponding mixture distribution of null p-values. Results of the data examples include better-behaved quantile-quantile plots and improved detection of novel mediation relationships on the role of DNA methylation in genetic regulation of gene expression. With increasing interest in mediation by molecular intermediaries such as gene expression, the proposed method addresses an unmet methodological challenge. Methods used in the package refer to James Y. Dai, Janet L. Stanford & Michael LeBlanc (2020) <doi:10.1080/01621459.2020.1765785>.  "
  },
  {
    "id": 3931,
    "package_name": "HDNRA",
    "title": "High-Dimensional Location Testing with Normal-Reference\nApproaches",
    "description": "We provide a collection of various classical tests and latest normal-reference tests for comparing high-dimensional mean vectors including two-sample and general linear hypothesis testing (GLHT) problem. Some existing tests for two-sample problem [see Bai, Zhidong, and Hewa Saranadasa.(1996) <https://www.jstor.org/stable/24306018>; Chen, Song Xi, and Ying-Li Qin.(2010) <doi:10.1214/09-aos716>; Srivastava, Muni S., and Meng Du.(2008) <doi:10.1016/j.jmva.2006.11.002>; Srivastava, Muni S., Shota Katayama, and Yutaka Kano.(2013)<doi:10.1016/j.jmva.2012.08.014>]. Normal-reference tests for two-sample problem [see Zhang, Jin-Ting, Jia Guo, Bu Zhou, and Ming-Yen Cheng.(2020) <doi:10.1080/01621459.2019.1604366>; Zhang, Jin-Ting, Bu Zhou, Jia Guo, and Tianming Zhu.(2021) <doi:10.1016/j.jspi.2020.11.008>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2020) <doi:10.1016/j.ecosta.2019.12.002>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2023) <doi:10.1080/02664763.2020.1834516>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1080/10485252.2021.2015768>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1007/s42519-021-00232-w>; Zhu, Tianming, Pengfei Wang, and Jin-Ting Zhang.(2023) <doi:10.1007/s00180-023-01433-6>]. Some existing tests for GLHT problem [see Fujikoshi, Yasunori, Tetsuto Himeno, and Hirofumi Wakaki.(2004) <doi:10.14490/jjss.34.19>; Srivastava, Muni S., and Yasunori Fujikoshi.(2006) <doi:10.1016/j.jmva.2005.08.010>; Yamada, Takayuki, and Muni S. Srivastava.(2012) <doi:10.1080/03610926.2011.581786>; Schott, James R.(2007) <doi:10.1016/j.jmva.2006.11.007>; Zhou, Bu, Jia Guo, and Jin-Ting Zhang.(2017) <doi:10.1016/j.jspi.2017.03.005>]. Normal-reference  tests for GLHT problem [see Zhang, Jin-Ting, Jia Guo, and Bu Zhou.(2017) <doi:10.1016/j.jmva.2017.01.002>; Zhang, Jin-Ting, Bu Zhou, and Jia Guo.(2022) <doi:10.1016/j.jmva.2021.104816>; Zhu, Tianming, Liang Zhang, and Jin-Ting Zhang.(2022) <doi:10.5705/ss.202020.0362>; Zhu, Tianming, and Jin-Ting Zhang.(2022) <doi:10.1007/s00180-021-01110-6>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1016/j.csda.2021.107385>].",
    "version": "2.0.1",
    "maintainer": "Pengfei Wang <nie23.wp8738@e.ntu.edu.sg>",
    "author": "Pengfei Wang [aut, cre],\n  Shuqi Luo [aut],\n  Tianming Zhu [aut],\n  Bu Zhou [aut]",
    "url": "https://nie23wp8738.github.io/HDNRA/",
    "bug_reports": "https://github.com/nie23wp8738/HDNRA/issues",
    "repository": "https://cran.r-project.org/package=HDNRA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HDNRA High-Dimensional Location Testing with Normal-Reference\nApproaches We provide a collection of various classical tests and latest normal-reference tests for comparing high-dimensional mean vectors including two-sample and general linear hypothesis testing (GLHT) problem. Some existing tests for two-sample problem [see Bai, Zhidong, and Hewa Saranadasa.(1996) <https://www.jstor.org/stable/24306018>; Chen, Song Xi, and Ying-Li Qin.(2010) <doi:10.1214/09-aos716>; Srivastava, Muni S., and Meng Du.(2008) <doi:10.1016/j.jmva.2006.11.002>; Srivastava, Muni S., Shota Katayama, and Yutaka Kano.(2013)<doi:10.1016/j.jmva.2012.08.014>]. Normal-reference tests for two-sample problem [see Zhang, Jin-Ting, Jia Guo, Bu Zhou, and Ming-Yen Cheng.(2020) <doi:10.1080/01621459.2019.1604366>; Zhang, Jin-Ting, Bu Zhou, Jia Guo, and Tianming Zhu.(2021) <doi:10.1016/j.jspi.2020.11.008>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2020) <doi:10.1016/j.ecosta.2019.12.002>; Zhang, Liang, Tianming Zhu, and Jin-Ting Zhang.(2023) <doi:10.1080/02664763.2020.1834516>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1080/10485252.2021.2015768>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1007/s42519-021-00232-w>; Zhu, Tianming, Pengfei Wang, and Jin-Ting Zhang.(2023) <doi:10.1007/s00180-023-01433-6>]. Some existing tests for GLHT problem [see Fujikoshi, Yasunori, Tetsuto Himeno, and Hirofumi Wakaki.(2004) <doi:10.14490/jjss.34.19>; Srivastava, Muni S., and Yasunori Fujikoshi.(2006) <doi:10.1016/j.jmva.2005.08.010>; Yamada, Takayuki, and Muni S. Srivastava.(2012) <doi:10.1080/03610926.2011.581786>; Schott, James R.(2007) <doi:10.1016/j.jmva.2006.11.007>; Zhou, Bu, Jia Guo, and Jin-Ting Zhang.(2017) <doi:10.1016/j.jspi.2017.03.005>]. Normal-reference  tests for GLHT problem [see Zhang, Jin-Ting, Jia Guo, and Bu Zhou.(2017) <doi:10.1016/j.jmva.2017.01.002>; Zhang, Jin-Ting, Bu Zhou, and Jia Guo.(2022) <doi:10.1016/j.jmva.2021.104816>; Zhu, Tianming, Liang Zhang, and Jin-Ting Zhang.(2022) <doi:10.5705/ss.202020.0362>; Zhu, Tianming, and Jin-Ting Zhang.(2022) <doi:10.1007/s00180-021-01110-6>; Zhang, Jin-Ting, and Tianming Zhu.(2022) <doi:10.1016/j.csda.2021.107385>].  "
  },
  {
    "id": 3956,
    "package_name": "HIViz",
    "title": "Interactive Dashboard for 'HIV' Data Visualization",
    "description": "An interactive 'Shiny' dashboard for visualizing and exploring key metrics related to HIV/AIDS, including prevalence, incidence, mortality, and treatment coverage. The dashboard is designed to work with a dataset containing specific columns with standardized names. These columns must be present in the input data for the app to function properly: year: Numeric year of the data (e.g. 2010, 2021); sex: Gender classification (e.g. Male, Female); age_group: Age bracket (e.g. 15\u201324, 25\u201334); hiv_prevalence: Estimated HIV prevalence percentage; hiv_incidence: Number of new HIV cases per year; aids_deaths: Total AIDS-related deaths; plhiv: Estimated number of people living with HIV; art_coverage: Percentage receiving antiretroviral therapy (ART); testing_coverage: HIV testing services coverage; causes: Description of likely HIV transmission cause (e.g. unprotected sex, drug use). The dataset structure must strictly follow this column naming convention for the dashboard to render correctly.",
    "version": "0.1.2",
    "maintainer": "Atefeh Rashidi Pour <rashidiatefeh98@gmail.com>",
    "author": "Atefeh Rashidi Pour [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-3834-3183>),\n  Marzieh Rashidipour [aut] (ORCID:\n    <https://orcid.org/0000-0003-4024-765X>)",
    "url": "https://github.com/Atefehrashidi/HIViz",
    "bug_reports": "https://github.com/Atefehrashidi/HIViz/issues",
    "repository": "https://cran.r-project.org/package=HIViz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HIViz Interactive Dashboard for 'HIV' Data Visualization An interactive 'Shiny' dashboard for visualizing and exploring key metrics related to HIV/AIDS, including prevalence, incidence, mortality, and treatment coverage. The dashboard is designed to work with a dataset containing specific columns with standardized names. These columns must be present in the input data for the app to function properly: year: Numeric year of the data (e.g. 2010, 2021); sex: Gender classification (e.g. Male, Female); age_group: Age bracket (e.g. 15\u201324, 25\u201334); hiv_prevalence: Estimated HIV prevalence percentage; hiv_incidence: Number of new HIV cases per year; aids_deaths: Total AIDS-related deaths; plhiv: Estimated number of people living with HIV; art_coverage: Percentage receiving antiretroviral therapy (ART); testing_coverage: HIV testing services coverage; causes: Description of likely HIV transmission cause (e.g. unprotected sex, drug use). The dataset structure must strictly follow this column naming convention for the dashboard to render correctly.  "
  },
  {
    "id": 3960,
    "package_name": "HLMdiag",
    "title": "Diagnostic Tools for Hierarchical (Multilevel) Linear Models",
    "description": "A suite of diagnostic tools for hierarchical\n    (multilevel) linear models. The tools include\n    not only leverage and traditional deletion diagnostics (Cook's\n    distance, covratio, covtrace, and MDFFITS) but also \n    convenience functions and graphics for residual analysis. Models\n    can be fit using either lmer in the 'lme4' package or lme in the 'nlme' package.",
    "version": "0.5.1",
    "maintainer": "Adam Loy <loyad01@gmail.com>",
    "author": "Adam Loy [cre, aut],\n  Jaylin Lowe [aut],\n  Jack Moran [aut]",
    "url": "https://github.com/aloy/HLMdiag",
    "bug_reports": "https://github.com/aloy/HLMdiag/issues",
    "repository": "https://cran.r-project.org/package=HLMdiag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HLMdiag Diagnostic Tools for Hierarchical (Multilevel) Linear Models A suite of diagnostic tools for hierarchical\n    (multilevel) linear models. The tools include\n    not only leverage and traditional deletion diagnostics (Cook's\n    distance, covratio, covtrace, and MDFFITS) but also \n    convenience functions and graphics for residual analysis. Models\n    can be fit using either lmer in the 'lme4' package or lme in the 'nlme' package.  "
  },
  {
    "id": 3972,
    "package_name": "HMP",
    "title": "Hypothesis Testing and Power Calculations for Comparing\nMetagenomic Samples from HMP",
    "description": "Using Dirichlet-Multinomial distribution to provide several functions for formal hypothesis testing, power and sample size calculations for human microbiome experiments.",
    "version": "2.0.1",
    "maintainer": "Berkley Shands <rpackages@biorankings.com>",
    "author": "Patricio S. La Rosa, Elena Deych, Sharina Carter, Berkley Shands, Dake Yang, William D. Shannon",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HMP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HMP Hypothesis Testing and Power Calculations for Comparing\nMetagenomic Samples from HMP Using Dirichlet-Multinomial distribution to provide several functions for formal hypothesis testing, power and sample size calculations for human microbiome experiments.  "
  },
  {
    "id": 3995,
    "package_name": "HTT",
    "title": "Hypothesis Testing Tree",
    "description": "A novel decision tree algorithm in the hypothesis testing framework. The algorithm examines the distribution difference between two child nodes over all possible binary partitions. The test statistic of the hypothesis testing is equivalent to the generalized energy distance, which enables the algorithm to be more powerful in detecting the complex structure, not only the mean difference. It is applicable for numeric, nominal, ordinal explanatory variables and the response in general metric space of strong negative type. The algorithm has superior performance compared to other tree models in type I error, power, prediction accuracy, and complexity.",
    "version": "0.1.2",
    "maintainer": "Jiaqi Hu <hujiaqi@mail.ustc.edu.cn>",
    "author": "Jiaqi Hu [cre, aut],\n  Zhe Gao [aut],\n  Bo Zhang [aut],\n  Xueqin Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HTT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HTT Hypothesis Testing Tree A novel decision tree algorithm in the hypothesis testing framework. The algorithm examines the distribution difference between two child nodes over all possible binary partitions. The test statistic of the hypothesis testing is equivalent to the generalized energy distance, which enables the algorithm to be more powerful in detecting the complex structure, not only the mean difference. It is applicable for numeric, nominal, ordinal explanatory variables and the response in general metric space of strong negative type. The algorithm has superior performance compared to other tree models in type I error, power, prediction accuracy, and complexity.  "
  },
  {
    "id": 3998,
    "package_name": "HWEintrinsic",
    "title": "Objective Bayesian Testing for the Hardy-Weinberg Equilibrium\nProblem",
    "description": "General (multi-allelic) Hardy-Weinberg equilibrium problem from an objective Bayesian testing standpoint. This aim is achieved through the identification of a class of priors specifically designed for this testing problem. A class of intrinsic priors under the full model is considered. This class is indexed by a tuning quantity, the training sample size, as discussed in Consonni, Moreno and Venturini (2010). These priors are objective, satisfy Savage's continuity condition and have proved to behave extremely well for many statistical testing problems.",
    "version": "1.2.3",
    "maintainer": "Sergio Venturini <sergio.venturini@unicatt.it>",
    "author": "Sergio Venturini",
    "url": "https://onlinelibrary.wiley.com/doi/10.1002/sim.4084/abstract",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HWEintrinsic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HWEintrinsic Objective Bayesian Testing for the Hardy-Weinberg Equilibrium\nProblem General (multi-allelic) Hardy-Weinberg equilibrium problem from an objective Bayesian testing standpoint. This aim is achieved through the identification of a class of priors specifically designed for this testing problem. A class of intrinsic priors under the full model is considered. This class is indexed by a tuning quantity, the training sample size, as discussed in Consonni, Moreno and Venturini (2010). These priors are objective, satisfy Savage's continuity condition and have proved to behave extremely well for many statistical testing problems.  "
  },
  {
    "id": 4007,
    "package_name": "Haplin",
    "title": "Analyzing Case-Parent Triad and/or Case-Control Data with SNP\nHaplotypes",
    "description": "Performs genetic association analyses of case-parent triad (trio) data with multiple markers. It can also incorporate complete or incomplete control triads, for instance independent control children. Estimation is based on haplotypes, for instance SNP haplotypes, even though phase is not known from the genetic data. 'Haplin' estimates relative risk (RR + conf.int.) and p-value associated with each haplotype. It uses maximum likelihood estimation to make optimal use of data from triads with missing genotypic data, for instance if some SNPs has not been typed for some individuals. 'Haplin' also allows estimation of effects of maternal haplotypes and parent-of-origin effects, particularly appropriate in perinatal epidemiology. 'Haplin' allows special models, like X-inactivation, to be fitted on the X-chromosome. A GxE analysis allows testing interactions between environment and all estimated genetic effects. The models were originally described in \"Gjessing HK and Lie RT. Case-parent triads: Estimating single- and double-dose effects of fetal and maternal disease gene haplotypes. Annals of Human Genetics (2006) 70, pp. 382-396\".",
    "version": "7.3.2",
    "maintainer": "Hakon K. Gjessing <hakon.gjessing@uib.no>",
    "author": "Hakon K. Gjessing [aut, cre],\n  Miriam Gjerdevik [ctb] (functions 'lineByLine', 'cbindFiles',\n    'rbindFiles', 'snpPower', 'snpSampleSize', 'hapSim', 'hapRun',\n    'hapPower', 'hapPowerAsymp', and 'hapRelEff'),\n  Julia Romanowska [ctb] (ORCID: <https://orcid.org/0000-0001-6733-1953>,\n    new data format, parallelisation, new documentation),\n  Oivind Skare [ctb] (TDT tests)",
    "url": "https://haplin.bitbucket.io",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Haplin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Haplin Analyzing Case-Parent Triad and/or Case-Control Data with SNP\nHaplotypes Performs genetic association analyses of case-parent triad (trio) data with multiple markers. It can also incorporate complete or incomplete control triads, for instance independent control children. Estimation is based on haplotypes, for instance SNP haplotypes, even though phase is not known from the genetic data. 'Haplin' estimates relative risk (RR + conf.int.) and p-value associated with each haplotype. It uses maximum likelihood estimation to make optimal use of data from triads with missing genotypic data, for instance if some SNPs has not been typed for some individuals. 'Haplin' also allows estimation of effects of maternal haplotypes and parent-of-origin effects, particularly appropriate in perinatal epidemiology. 'Haplin' allows special models, like X-inactivation, to be fitted on the X-chromosome. A GxE analysis allows testing interactions between environment and all estimated genetic effects. The models were originally described in \"Gjessing HK and Lie RT. Case-parent triads: Estimating single- and double-dose effects of fetal and maternal disease gene haplotypes. Annals of Human Genetics (2006) 70, pp. 382-396\".  "
  },
  {
    "id": 4011,
    "package_name": "HardyWeinberg",
    "title": "Statistical Tests and Graphics for Hardy-Weinberg Equilibrium",
    "description": "Contains tools for exploring Hardy-Weinberg equilibrium (Hardy, 1908;  Weinberg, 1908) for bi and multi-allelic genetic marker data. All classical tests (chi-square, exact, likelihood-ratio and permutation tests) with bi-allelic variants are included in the package, as well as functions for power computation and for the simulation of marker data under equilibrium and disequilibrium. Routines for dealing with markers on the X-chromosome are included (Graffelman & Weir, 2016) <doi:10.1038/hdy.2016.20>, including Bayesian procedures. Some exact and permutation procedures also work with multi-allelic variants. Special test procedures that jointly address Hardy-Weinberg equilibrium and equality of allele frequencies in both sexes are supplied, for the bi and multi-allelic case. Functions for testing equilibrium in the presence of missing data by using multiple imputation are also provided. Implements several graphics for exploring the equilibrium status of a large set of bi-allelic markers: ternary plots with acceptance regions, log-ratio plots and Q-Q plots. The functionality of the package is explained in detail in a related JSS paper <doi:10.18637/jss.v064.i03>. ",
    "version": "1.7.9",
    "maintainer": "Jan Graffelman <jan.graffelman@upc.edu>",
    "author": "Jan Graffelman [aut, cre],\n  Christopher Chang [ctb],\n  Xavi Puig [ctb],\n  Jan Wigginton [ctb],\n  Leonardo Ortoleva [ctb],\n  William R. Engels [ctb]",
    "url": "https://www.r-project.org, https://www-eio.upc.edu/~jan/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HardyWeinberg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HardyWeinberg Statistical Tests and Graphics for Hardy-Weinberg Equilibrium Contains tools for exploring Hardy-Weinberg equilibrium (Hardy, 1908;  Weinberg, 1908) for bi and multi-allelic genetic marker data. All classical tests (chi-square, exact, likelihood-ratio and permutation tests) with bi-allelic variants are included in the package, as well as functions for power computation and for the simulation of marker data under equilibrium and disequilibrium. Routines for dealing with markers on the X-chromosome are included (Graffelman & Weir, 2016) <doi:10.1038/hdy.2016.20>, including Bayesian procedures. Some exact and permutation procedures also work with multi-allelic variants. Special test procedures that jointly address Hardy-Weinberg equilibrium and equality of allele frequencies in both sexes are supplied, for the bi and multi-allelic case. Functions for testing equilibrium in the presence of missing data by using multiple imputation are also provided. Implements several graphics for exploring the equilibrium status of a large set of bi-allelic markers: ternary plots with acceptance regions, log-ratio plots and Q-Q plots. The functionality of the package is explained in detail in a related JSS paper <doi:10.18637/jss.v064.i03>.   "
  },
  {
    "id": 4028,
    "package_name": "HiCocietyExample",
    "title": "Example HiC and Two 'HiCociety' Outputs for Demonstration and\nTesting",
    "description": "\n  Provides an example HiC dataset and two examples of 'HiCociety' \n  outputs from a function named hic2community(). \n  The data are intended for demonstration purposes only and kept small \n  enough to be distributed via CRAN.",
    "version": "1.0.0",
    "maintainer": "Sora Yoon <sora.yoon@pennmedicine.upenn.edu>",
    "author": "Sora Yoon [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=HiCocietyExample",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "HiCocietyExample Example HiC and Two 'HiCociety' Outputs for Demonstration and\nTesting \n  Provides an example HiC dataset and two examples of 'HiCociety' \n  outputs from a function named hic2community(). \n  The data are intended for demonstration purposes only and kept small \n  enough to be distributed via CRAN.  "
  },
  {
    "id": 4092,
    "package_name": "ICRanks",
    "title": "Simultaneous Confidence Intervals for Ranks",
    "description": "Algorithms to construct simultaneous confidence intervals for\n    the ranks of means mu_1,...,mu_n based on an independent Gaussian sample\n    using multiple testing techniques.",
    "version": "3.2",
    "maintainer": "Diaa Al Mohamad <diaa.almohamad@gmail.com>",
    "author": "Diaa Al Mohamad [aut, cre],\n  Erik W. van Zwet [aut],\n  Jelle J. Goeman [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ICRanks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ICRanks Simultaneous Confidence Intervals for Ranks Algorithms to construct simultaneous confidence intervals for\n    the ranks of means mu_1,...,mu_n based on an independent Gaussian sample\n    using multiple testing techniques.  "
  },
  {
    "id": 4095,
    "package_name": "ICSKAT",
    "title": "Interval-Censored Sequence Kernel Association Test",
    "description": "Implements the Interval-Censored Sequence Kernel Association (ICSKAT) test for testing the association between interval-censored time-to-event outcomes and groups of single nucleotide polymorphisms (SNPs). Interval-censored time-to-event data occur when the event time is not known exactly but can be deduced to fall within a given interval. For example, some medical conditions like bone mineral density deficiency are generally only diagnosed at clinical visits. If a patient goes for clinical checkups yearly and is diagnosed at, say, age 30, then the onset of the deficiency is only known to fall between the date of their age 29 checkup and the date of the age 30 checkup. Interval-censored data include right- and left-censored data as special cases. This package also implements the interval-censored Burden test and the ICSKATO test, which is the optimal combination of the ICSKAT and Burden tests. Please see the vignette for a quickstart guide. The paper describing these methods is \" Inference for Set-Based Effects in Genetic Association Studies with Interval-Censored Outcomes\" by Sun R, Zhu L, Li Y, Yasui Y, & Robison L (Biometrics 2023, <doi:10.1111/biom.13636>).",
    "version": "0.3.0",
    "maintainer": "Ryan Sun <ryansun.work@gmail.com>",
    "author": "Ryan Sun [aut, cre],\n  Liang Zhu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ICSKAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ICSKAT Interval-Censored Sequence Kernel Association Test Implements the Interval-Censored Sequence Kernel Association (ICSKAT) test for testing the association between interval-censored time-to-event outcomes and groups of single nucleotide polymorphisms (SNPs). Interval-censored time-to-event data occur when the event time is not known exactly but can be deduced to fall within a given interval. For example, some medical conditions like bone mineral density deficiency are generally only diagnosed at clinical visits. If a patient goes for clinical checkups yearly and is diagnosed at, say, age 30, then the onset of the deficiency is only known to fall between the date of their age 29 checkup and the date of the age 30 checkup. Interval-censored data include right- and left-censored data as special cases. This package also implements the interval-censored Burden test and the ICSKATO test, which is the optimal combination of the ICSKAT and Burden tests. Please see the vignette for a quickstart guide. The paper describing these methods is \" Inference for Set-Based Effects in Genetic Association Studies with Interval-Censored Outcomes\" by Sun R, Zhu L, Li Y, Yasui Y, & Robison L (Biometrics 2023, <doi:10.1111/biom.13636>).  "
  },
  {
    "id": 4107,
    "package_name": "ICtest",
    "title": "Estimating and Testing the Number of Interesting Components in\nLinear Dimension Reduction",
    "description": "For different linear dimension reduction methods like principal components analysis (PCA), independent components analysis (ICA) and supervised linear dimension reduction tests and estimates for the number of interesting components (ICs) are provided.",
    "version": "0.3-6",
    "maintainer": "Klaus Nordhausen <klausnordhausenR@gmail.com>",
    "author": "Klaus Nordhausen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3758-8501>),\n  Hannu Oja [aut] (ORCID: <https://orcid.org/0000-0002-4945-5976>),\n  Katariina Perkonoja [aut] (ORCID:\n    <https://orcid.org/0000-0002-3812-0871>),\n  David E. Tyler [aut],\n  Joni Virta [aut] (ORCID: <https://orcid.org/0000-0002-2150-2769>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ICtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ICtest Estimating and Testing the Number of Interesting Components in\nLinear Dimension Reduction For different linear dimension reduction methods like principal components analysis (PCA), independent components analysis (ICA) and supervised linear dimension reduction tests and estimates for the number of interesting components (ICs) are provided.  "
  },
  {
    "id": 4143,
    "package_name": "ILS",
    "title": "Interlaboratory Study",
    "description": "It performs interlaboratory studies (ILS) to detect those\n    laboratories that provide non-consistent results when comparing to others. It\n    permits to work simultaneously with various testing materials, from standard\n    univariate, and functional data analysis (FDA) perspectives. The univariate\n    approach based on ASTM E691-08 consist of estimating the Mandel's h and\n    k statistics to identify those laboratories that provide more significant\n    different results, testing also the presence of outliers by Cochran and Grubbs\n    tests, Analysis of variance (ANOVA) techniques are provided (F and Tuckey\n    tests) to test differences in means corresponding to different laboratories per\n    each material. Taking into account the functional nature of data retrieved in\n    analytical chemistry, applied physics and engineering (spectra, thermograms,\n    etc.). ILS package provides a FDA approach for finding the Mandel's k and h\n    statistics distribution by smoothing bootstrap resampling.",
    "version": "0.3",
    "maintainer": "Miguel Flores <ma.flores@outlook.com>",
    "author": "Miguel Flores [aut, cre],\n  Ruben Fernandez [aut],\n  Salvador Naya [aut],\n  Javier Tarrio-Saavedra [aut]",
    "url": "https://github.com/mflores72000/ILS/",
    "bug_reports": "https://github.com/mflores72000/ILS/issues",
    "repository": "https://cran.r-project.org/package=ILS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ILS Interlaboratory Study It performs interlaboratory studies (ILS) to detect those\n    laboratories that provide non-consistent results when comparing to others. It\n    permits to work simultaneously with various testing materials, from standard\n    univariate, and functional data analysis (FDA) perspectives. The univariate\n    approach based on ASTM E691-08 consist of estimating the Mandel's h and\n    k statistics to identify those laboratories that provide more significant\n    different results, testing also the presence of outliers by Cochran and Grubbs\n    tests, Analysis of variance (ANOVA) techniques are provided (F and Tuckey\n    tests) to test differences in means corresponding to different laboratories per\n    each material. Taking into account the functional nature of data retrieved in\n    analytical chemistry, applied physics and engineering (spectra, thermograms,\n    etc.). ILS package provides a FDA approach for finding the Mandel's k and h\n    statistics distribution by smoothing bootstrap resampling.  "
  },
  {
    "id": 4212,
    "package_name": "IgAScores",
    "title": "Score Taxon-Level IgA Binding in IgA-Seq Experiments",
    "description": "Functions to calculate indices used to score immunoglobulin A (IgA) binding of bacteria in IgA sequencing (IgA-Seq) experiments.\n  This includes the original Kau and Palm indices and more recent methods as described in Jackson et al. (2020) <doi:10.1101/2020.08.19.257501>.\n  Additionally the package contains a function to simulate IgA-Seq data and an example experimental data set for method testing. ",
    "version": "0.1.2",
    "maintainer": "Matthew Jackson <mattjackson4@gmail.com>",
    "author": "Matthew Jackson [aut, cre, cph]",
    "url": "https://doi.org/10.1101/2020.08.19.257501",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IgAScores",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IgAScores Score Taxon-Level IgA Binding in IgA-Seq Experiments Functions to calculate indices used to score immunoglobulin A (IgA) binding of bacteria in IgA sequencing (IgA-Seq) experiments.\n  This includes the original Kau and Palm indices and more recent methods as described in Jackson et al. (2020) <doi:10.1101/2020.08.19.257501>.\n  Additionally the package contains a function to simulate IgA-Seq data and an example experimental data set for method testing.   "
  },
  {
    "id": 4231,
    "package_name": "IncomPair",
    "title": "Comparison of Means for the Incomplete Paired Data",
    "description": "Implements a variety of nonparametric and parametric methods that are commonly used when the data set is a mixture of paired observations and independent samples. The package also calculates and returns values of different tests with their corresponding p-values.\n    Bhoj, D. S. (1991) <doi:10.1002/bimj.4710330108> \"Testing equality of means in the presence of correlation and missing data\".\n    Dubnicka, S. R., Blair, R. C., and Hettmansperger, T. P. (2002) <doi:10.22237/jmasm/1020254460> \"Rank-based procedures for mixed paired and two-sample designs\".\n    Einsporn, R. L. and Habtzghi, D. (2013) <https://pdfs.semanticscholar.org/89a3/90bafeb2bc41ed4414533cfd5ab84a6b54b6.pdf> \"Combining paired and two-sample data using a permutation test\".\n    Ekbohm, G. (1976) <doi:10.1093/biomet/63.2.299> \"On comparing means in the paired case with incomplete data on both responses\".\n    Lin, P. E. and Stivers, L. E. (1974) <doi:10.1093/biomet/61.2.325> On difference of means with incomplete data\".\n    Maritz, J. S. (1995) <doi:10.1111/j.1467-842x.1995.tb00649.x> \"A permutation paired test allowing for missing values\".",
    "version": "0.1.0",
    "maintainer": "Desale Habtzghi <dhabtzgh@depaul.edu>",
    "author": "Desale Habtzghi [aut, cre],\n  Yilin Zhang [aut],\n  Richard Einsporn [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IncomPair",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IncomPair Comparison of Means for the Incomplete Paired Data Implements a variety of nonparametric and parametric methods that are commonly used when the data set is a mixture of paired observations and independent samples. The package also calculates and returns values of different tests with their corresponding p-values.\n    Bhoj, D. S. (1991) <doi:10.1002/bimj.4710330108> \"Testing equality of means in the presence of correlation and missing data\".\n    Dubnicka, S. R., Blair, R. C., and Hettmansperger, T. P. (2002) <doi:10.22237/jmasm/1020254460> \"Rank-based procedures for mixed paired and two-sample designs\".\n    Einsporn, R. L. and Habtzghi, D. (2013) <https://pdfs.semanticscholar.org/89a3/90bafeb2bc41ed4414533cfd5ab84a6b54b6.pdf> \"Combining paired and two-sample data using a permutation test\".\n    Ekbohm, G. (1976) <doi:10.1093/biomet/63.2.299> \"On comparing means in the paired case with incomplete data on both responses\".\n    Lin, P. E. and Stivers, L. E. (1974) <doi:10.1093/biomet/61.2.325> On difference of means with incomplete data\".\n    Maritz, J. S. (1995) <doi:10.1111/j.1467-842x.1995.tb00649.x> \"A permutation paired test allowing for missing values\".  "
  },
  {
    "id": 4293,
    "package_name": "JATSdecoder",
    "title": "A Metadata and Text Extraction and Manipulation Tool Set",
    "description": "Provides a function collection to extract metadata, sectioned text and study characteristics from scientific articles in 'NISO-JATS' format. Articles in PDF format can be converted to 'NISO-JATS' with the 'Content ExtRactor and MINEr' ('CERMINE', <https://github.com/CeON/CERMINE>). For convenience, two functions bundle the extraction heuristics: JATSdecoder() converts 'NISO-JATS'-tagged XML files to a structured list with elements title, author, journal, history, 'DOI', abstract, sectioned text and reference list. study.character() extracts multiple study characteristics like number of included studies, statistical methods used, alpha error, power, statistical results, correction method for multiple testing, software used. The function get.stats() extracts all statistical results from text and recomputes p-values for many standard test statistics. It performs a consistency check of the reported with the recalculated p-values. An estimation of the involved sample size is performed based on textual reports within the abstract and the reported degrees of freedom within statistical results. In addition, the package contains some useful functions to process text (text2sentences(), text2num(), ngram(), strsplit2(), grep2()). See B\u00f6schen, I. (2021) <doi:10.1007/s11192-021-04162-z> B\u00f6schen, I. (2021) <doi:10.1038/s41598-021-98782-3>, B\u00f6schen, I. (2023) <doi:10.1038/s41598-022-27085-y>, and B\u00f6schen, I. (2024) <doi:10.48550/arXiv.2408.07948>.",
    "version": "1.2.1",
    "maintainer": "Ingmar B\u00f6schen <ingmar.boeschen@uni-hamburg.de>",
    "author": "Ingmar B\u00f6schen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1159-3991>)",
    "url": "https://github.com/ingmarboeschen/JATSdecoder",
    "bug_reports": "https://github.com/ingmarboeschen/JATSdecoder/issues",
    "repository": "https://cran.r-project.org/package=JATSdecoder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "JATSdecoder A Metadata and Text Extraction and Manipulation Tool Set Provides a function collection to extract metadata, sectioned text and study characteristics from scientific articles in 'NISO-JATS' format. Articles in PDF format can be converted to 'NISO-JATS' with the 'Content ExtRactor and MINEr' ('CERMINE', <https://github.com/CeON/CERMINE>). For convenience, two functions bundle the extraction heuristics: JATSdecoder() converts 'NISO-JATS'-tagged XML files to a structured list with elements title, author, journal, history, 'DOI', abstract, sectioned text and reference list. study.character() extracts multiple study characteristics like number of included studies, statistical methods used, alpha error, power, statistical results, correction method for multiple testing, software used. The function get.stats() extracts all statistical results from text and recomputes p-values for many standard test statistics. It performs a consistency check of the reported with the recalculated p-values. An estimation of the involved sample size is performed based on textual reports within the abstract and the reported degrees of freedom within statistical results. In addition, the package contains some useful functions to process text (text2sentences(), text2num(), ngram(), strsplit2(), grep2()). See B\u00f6schen, I. (2021) <doi:10.1007/s11192-021-04162-z> B\u00f6schen, I. (2021) <doi:10.1038/s41598-021-98782-3>, B\u00f6schen, I. (2023) <doi:10.1038/s41598-022-27085-y>, and B\u00f6schen, I. (2024) <doi:10.48550/arXiv.2408.07948>.  "
  },
  {
    "id": 4352,
    "package_name": "KLINK",
    "title": "Kinship Analysis with Linked Markers",
    "description": "A 'shiny' application for forensic kinship testing, based on\n    the 'pedsuite' R packages. 'KLINK' is closely aligned with the (non-R)\n    software 'Familias' and 'FamLink', but offers several unique features,\n    including visualisations and automated report generation. The\n    calculation of likelihood ratios supports pairs of linked markers, and\n    all common mutation models.",
    "version": "1.1.0",
    "maintainer": "Magnus Dehli Vigeland <m.d.vigeland@medisin.uio.no>",
    "author": "Magnus Dehli Vigeland [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9134-4962>)",
    "url": "https://github.com/magnusdv/KLINK",
    "bug_reports": "https://github.com/magnusdv/KLINK/issues",
    "repository": "https://cran.r-project.org/package=KLINK",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KLINK Kinship Analysis with Linked Markers A 'shiny' application for forensic kinship testing, based on\n    the 'pedsuite' R packages. 'KLINK' is closely aligned with the (non-R)\n    software 'Familias' and 'FamLink', but offers several unique features,\n    including visualisations and automated report generation. The\n    calculation of likelihood ratios supports pairs of linked markers, and\n    all common mutation models.  "
  },
  {
    "id": 4394,
    "package_name": "KinMixLite",
    "title": "Inference About Relationships from DNA Mixtures",
    "description": "Methods for inference about/under complex relationships using\n  peak height data from DNA mixtures: the most basic example would be testing whether a contributor to \n  a mixture is the father of a child of known genotype. This provides most of the functionality\n  of the 'KinMix' package, but with some loss of efficiency and restriction on problem size,\n  as the latter uses 'RHugin' as the Bayes net engine, while this package uses 'gRain'.\n  The package implements the methods introduced in \n  Green, P. J. and Mortera, J. (2017) <doi:10.1016/j.fsigen.2017.02.001> and \n  Green, P. J. and Mortera, J. (2021) <doi:10.1111/rssc.12498>.",
    "version": "2.2.1",
    "maintainer": "Peter Green <P.J.Green@bristol.ac.uk>",
    "author": "Peter Green [aut, cre]",
    "url": "https://petergreen5678.github.io/research/software/kinmix.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=KinMixLite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KinMixLite Inference About Relationships from DNA Mixtures Methods for inference about/under complex relationships using\n  peak height data from DNA mixtures: the most basic example would be testing whether a contributor to \n  a mixture is the father of a child of known genotype. This provides most of the functionality\n  of the 'KinMix' package, but with some loss of efficiency and restriction on problem size,\n  as the latter uses 'RHugin' as the Bayes net engine, while this package uses 'gRain'.\n  The package implements the methods introduced in \n  Green, P. J. and Mortera, J. (2017) <doi:10.1016/j.fsigen.2017.02.001> and \n  Green, P. J. and Mortera, J. (2021) <doi:10.1111/rssc.12498>.  "
  },
  {
    "id": 4444,
    "package_name": "LDM",
    "title": "Testing Hypotheses About the Microbiome using the Linear\nDecomposition Model",
    "description": "A single analysis path that includes distance-based ordination, global tests of any effect of the microbiome, and tests of the effects of individual taxa with false-discovery-rate (FDR) control. It accommodates both continuous and discrete covariates as well as interaction terms to be tested either singly or in combination, allows for adjustment of confounding covariates, and uses permutation-based p-values that can control for sample correlations. It can be applied to transformed data, and an omnibus test can combine results from analyses conducted on different transformation scales. It can also be used for testing presence-absence associations based on infinite number of rarefaction replicates, testing mediation effects of the microbiome, analyzing censored time-to-event outcomes, and for compositional analysis by fitting linear models to centered-log-ratio taxa count data. ",
    "version": "6.0.1",
    "maintainer": "Yi-Juan Hu <yijuan.hu@emory.edu>",
    "author": "Yi-Juan Hu [aut, cre],\n  Glen A Satten [aut]",
    "url": "https://github.com/yijuanhu/LDM",
    "bug_reports": "https://github.com/yijuanhu/LDM/issues",
    "repository": "https://cran.r-project.org/package=LDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LDM Testing Hypotheses About the Microbiome using the Linear\nDecomposition Model A single analysis path that includes distance-based ordination, global tests of any effect of the microbiome, and tests of the effects of individual taxa with false-discovery-rate (FDR) control. It accommodates both continuous and discrete covariates as well as interaction terms to be tested either singly or in combination, allows for adjustment of confounding covariates, and uses permutation-based p-values that can control for sample correlations. It can be applied to transformed data, and an omnibus test can combine results from analyses conducted on different transformation scales. It can also be used for testing presence-absence associations based on infinite number of rarefaction replicates, testing mediation effects of the microbiome, analyzing censored time-to-event outcomes, and for compositional analysis by fitting linear models to centered-log-ratio taxa count data.   "
  },
  {
    "id": 4468,
    "package_name": "LLMTranslate",
    "title": "'shiny' App for TRAPD/ISPOR Survey Translation with LLMs",
    "description": "A 'shiny' application to automate forward and back survey translation\n    with optional reconciliation using large language models (LLMs). Supports\n    OpenAI (GPT), Google Gemini, and Anthropic Claude models. It follows\n    the TRAPD (Translation, Review, Adjudication, Pretesting, Documentation)\n    framework and ISPOR (International Society for Pharmacoeconomics and\n    Outcomes Research) recommendations. See Harkness et al. (2010)\n    <doi:10.1002/9780470609927.ch7> and Wild et al. (2005)\n    <doi:10.1111/j.1524-4733.2005.04054.x>.",
    "version": "0.2.0",
    "maintainer": "Jonas R. Kunst <jonas.r.kunst@bi.no>",
    "author": "Jonas R. Kunst [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LLMTranslate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LLMTranslate 'shiny' App for TRAPD/ISPOR Survey Translation with LLMs A 'shiny' application to automate forward and back survey translation\n    with optional reconciliation using large language models (LLMs). Supports\n    OpenAI (GPT), Google Gemini, and Anthropic Claude models. It follows\n    the TRAPD (Translation, Review, Adjudication, Pretesting, Documentation)\n    framework and ISPOR (International Society for Pharmacoeconomics and\n    Outcomes Research) recommendations. See Harkness et al. (2010)\n    <doi:10.1002/9780470609927.ch7> and Wild et al. (2005)\n    <doi:10.1111/j.1524-4733.2005.04054.x>.  "
  },
  {
    "id": 4482,
    "package_name": "LNPar",
    "title": "Estimation and Testing for a Lognormal-Pareto Mixture",
    "description": "Estimates a lognormal-Pareto mixture by means of the Expectation-Conditional-Maximization-Either algorithm and by maximizing the profile likelihood function. A likelihood ratio test for discriminating between lognormal and Pareto tail is also implemented. See Bee, M. (2022) <doi:10.1007/s11634-022-00497-4>.",
    "version": "1.1.2",
    "maintainer": "Marco Bee <marco.bee@unitn.it>",
    "author": "Marco Bee [aut, cre] (ORCID: <https://orcid.org/0000-0002-9579-3650>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LNPar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LNPar Estimation and Testing for a Lognormal-Pareto Mixture Estimates a lognormal-Pareto mixture by means of the Expectation-Conditional-Maximization-Either algorithm and by maximizing the profile likelihood function. A likelihood ratio test for discriminating between lognormal and Pareto tail is also implemented. See Bee, M. (2022) <doi:10.1007/s11634-022-00497-4>.  "
  },
  {
    "id": 4491,
    "package_name": "LPGraph",
    "title": "Nonparametric Smoothing of Laplacian Graph Spectra",
    "description": "A nonparametric method to approximate Laplacian graph spectra of a network with \n    ordered vertices. This provides a computationally efficient algorithm for obtaining an \n\taccurate and smooth estimate of the graph Laplacian basis. The approximation results can \n\tthen be used for tasks like change point detection, k-sample testing, and so on. The \n\tprimary reference is Mukhopadhyay, S. and Wang, K. (2018, Technical Report).",
    "version": "2.1",
    "maintainer": "Kaijun Wang <kaijun.wang@temple.edu>",
    "author": "Subhadeep Mukhopadhyay, Kaijun Wang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LPGraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LPGraph Nonparametric Smoothing of Laplacian Graph Spectra A nonparametric method to approximate Laplacian graph spectra of a network with \n    ordered vertices. This provides a computationally efficient algorithm for obtaining an \n\taccurate and smooth estimate of the graph Laplacian basis. The approximation results can \n\tthen be used for tasks like change point detection, k-sample testing, and so on. The \n\tprimary reference is Mukhopadhyay, S. and Wang, K. (2018, Technical Report).  "
  },
  {
    "id": 4498,
    "package_name": "LREP",
    "title": "Estimate and Test Exponential vs. Pareto Distributions",
    "description": "The programs were developed for estimation of parameters and testing exponential versus Pareto distribution during our work on hydrologic extremes. See Kozubowski, T.J., A.K. Panorska, F. Qeadan, and A. Gershunov (2007) <doi:10.1080/03610910802439121>, and Panorska, A.K., A. Gershunov, and T.J. Kozubowski (2007) <doi:10.1007/978-0-387-34918-3_26>.",
    "version": "0.1.1",
    "maintainer": "Jiqiang Wu <charles.wu@utah.edu>",
    "author": "Fares Qeadan [aut],\n  Jiqiang Wu [aut, cre],\n  Tomasz Kozubowski [aut],\n  Anna Panorska [aut]",
    "url": "",
    "bug_reports": "https://github.com/jiqiaingwu/LREP/issues",
    "repository": "https://cran.r-project.org/package=LREP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LREP Estimate and Test Exponential vs. Pareto Distributions The programs were developed for estimation of parameters and testing exponential versus Pareto distribution during our work on hydrologic extremes. See Kozubowski, T.J., A.K. Panorska, F. Qeadan, and A. Gershunov (2007) <doi:10.1080/03610910802439121>, and Panorska, A.K., A. Gershunov, and T.J. Kozubowski (2007) <doi:10.1007/978-0-387-34918-3_26>.  "
  },
  {
    "id": 4503,
    "package_name": "LS2Wstat",
    "title": "A Multiscale Test of Spatial Stationarity for LS2W Processes",
    "description": "Wavelet-based methods for testing stationarity and quadtree segmenting of images, see Taylor et al (2014) <doi:10.1080/00401706.2013.823890>.",
    "version": "2.1-5",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "author": "Sarah Taylor [aut],\n  Matt Nunes [aut, cre],\n  Idris Eckley [ctb, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LS2Wstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LS2Wstat A Multiscale Test of Spatial Stationarity for LS2W Processes Wavelet-based methods for testing stationarity and quadtree segmenting of images, see Taylor et al (2014) <doi:10.1080/00401706.2013.823890>.  "
  },
  {
    "id": 4548,
    "package_name": "Largevars",
    "title": "Testing Large VARs for the Presence of Cointegration",
    "description": "Conducts a cointegration test for high-dimensional vector autoregressions (VARs) of order k based on the large N,T asymptotics of Bykhovskaya and Gorin, 2022 (<doi:10.48550/arXiv.2202.07150>). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy-1 point process. This package contains simulated quantiles of the first ten partial sums of the Airy-1 point process that are precise up to the first three digits.",
    "version": "1.0.3",
    "maintainer": "Eszter Kiss <ekiss2803@gmail.com>",
    "author": "Anna Bykhovskaya [aut],\n  Vadim Gorin [aut],\n  Eszter Kiss [cre, aut]",
    "url": "https://github.com/eszter-kiss/Largevars",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Largevars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Largevars Testing Large VARs for the Presence of Cointegration Conducts a cointegration test for high-dimensional vector autoregressions (VARs) of order k based on the large N,T asymptotics of Bykhovskaya and Gorin, 2022 (<doi:10.48550/arXiv.2202.07150>). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy-1 point process. This package contains simulated quantiles of the first ten partial sums of the Airy-1 point process that are precise up to the first three digits.  "
  },
  {
    "id": 4560,
    "package_name": "LePage",
    "title": "LePage Type Tests",
    "description": "Location and scale hypothesis testing using the LePage test and variants of its as proposed by Hussain A. and Tsagris M. (2025), <doi:10.48550/arXiv.2509.19126>. ",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Abid Hussain [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LePage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LePage LePage Type Tests Location and scale hypothesis testing using the LePage test and variants of its as proposed by Hussain A. and Tsagris M. (2025), <doi:10.48550/arXiv.2509.19126>.   "
  },
  {
    "id": 4571,
    "package_name": "LearningStats",
    "title": "Elemental Descriptive and Inferential Statistics",
    "description": "Provides tools to teach students elemental statistics. The main topics covered are descriptive statistics, probability models (discrete and continuous variables) and statistical inference (confidence intervals and hypothesis tests). One of the main advantages of this package is that allows the user to read quite a variety of types of data files with one unique command. Moreover it includes shortcuts to simple but up-to-now not in R descriptive features such a complete frequency table or an histogram with the optimal number of intervals. Related to model distributions (both discrete and continuous), the package allows the student to easy plot the mass/density function, distribution function and quantile function just detailing as input arguments the known population parameters. The inference related tools are basically confidence interval and hypothesis testing. Having defined independent commands for these two tools makes it easier for the student to understand what the software is performing, and it also helps the student to have a better knowledge on which specific tool they need to use in each situation. Moreover, the hypothesis testing commands provide not only the numeric result on the screen but also a very intuitive graph (which includes the statistic distribution, the observed value of the statistic, the rejection area and the p-value) that is very useful for the student to visualise the process. The regression section includes up to now, a simple linear model, with one single command the student can obtain the numeric summary as well as the corresponding diagram with the adjusted regression model and a legend with basic information (formula of the adjusted model and R-squared).",
    "version": "0.1.0",
    "maintainer": "Mar\u00eda Isabel Borrajo-Garc\u00eda <mariaisabel.borrajo@usc.es>",
    "author": "Mar\u00eda Isabel Borrajo-Garc\u00eda [aut, cre],\n  Mercedes Conde-Amboage [aut],\n  Alejandra L\u00f3pez-P\u00e9rez [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LearningStats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LearningStats Elemental Descriptive and Inferential Statistics Provides tools to teach students elemental statistics. The main topics covered are descriptive statistics, probability models (discrete and continuous variables) and statistical inference (confidence intervals and hypothesis tests). One of the main advantages of this package is that allows the user to read quite a variety of types of data files with one unique command. Moreover it includes shortcuts to simple but up-to-now not in R descriptive features such a complete frequency table or an histogram with the optimal number of intervals. Related to model distributions (both discrete and continuous), the package allows the student to easy plot the mass/density function, distribution function and quantile function just detailing as input arguments the known population parameters. The inference related tools are basically confidence interval and hypothesis testing. Having defined independent commands for these two tools makes it easier for the student to understand what the software is performing, and it also helps the student to have a better knowledge on which specific tool they need to use in each situation. Moreover, the hypothesis testing commands provide not only the numeric result on the screen but also a very intuitive graph (which includes the statistic distribution, the observed value of the statistic, the rejection area and the p-value) that is very useful for the student to visualise the process. The regression section includes up to now, a simple linear model, with one single command the student can obtain the numeric summary as well as the corresponding diagram with the adjusted regression model and a legend with basic information (formula of the adjusted model and R-squared).  "
  },
  {
    "id": 4582,
    "package_name": "LiftTest",
    "title": "A Bootstrap Proportion Test for Brand Lift Testing",
    "description": "A bootstrap proportion test for Brand Lift Testing to quantify the effectiveness of online advertising. Methods of the bootstrap proportion test are presented in\n    Liu, Yu, Mao, Wu, Dyer (2023) <doi:10.1145/3583780.3615021>.",
    "version": "0.2.0",
    "maintainer": "Wanjun Liu <waliu@linkedin.com>",
    "author": "Wanjun Liu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=LiftTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LiftTest A Bootstrap Proportion Test for Brand Lift Testing A bootstrap proportion test for Brand Lift Testing to quantify the effectiveness of online advertising. Methods of the bootstrap proportion test are presented in\n    Liu, Yu, Mao, Wu, Dyer (2023) <doi:10.1145/3583780.3615021>.  "
  },
  {
    "id": 4658,
    "package_name": "MARSGWR",
    "title": "A Hybrid Spatial Model for Capturing Spatially Varying\nRelationships Between Variables in the Data",
    "description": "It is a hybrid spatial model that combines the strength of two widely used regression models, MARS (Multivariate Adaptive Regression Splines) and\n             GWR (Geographically Weighted Regression) to provide an effective approach for predicting a response variable at unknown locations. The MARS model\n             is used in the first step of the development of a hybrid model to identify the most important predictor variables that assist in predicting the response\n             variable. For method details see, Friedman, J.H. (1991). <DOI:10.1214/aos/1176347963>.The GWR model is then used to predict the response variable at \n             testing locations based on these selected variables that account for spatial variations in the relationships between the variables. This hybrid model \n             can improve the accuracy of the predictions compared to using an individual model alone.This developed hybrid spatial model can be useful particularly in \n             cases where the relationship between the response variable and predictor variables is complex and non-linear, and varies across locations.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <nobin.paul@icar.gov.in>",
    "author": "Nobin Chandra Paul [aut, cre, cph],\n  Anil Rai [aut],\n  Ankur Biswas [aut],\n  Tauqueer Ahmad [aut],\n  Dhananjay D. Nangare [aut],\n  Bhaskar B. Gaikwad [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MARSGWR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MARSGWR A Hybrid Spatial Model for Capturing Spatially Varying\nRelationships Between Variables in the Data It is a hybrid spatial model that combines the strength of two widely used regression models, MARS (Multivariate Adaptive Regression Splines) and\n             GWR (Geographically Weighted Regression) to provide an effective approach for predicting a response variable at unknown locations. The MARS model\n             is used in the first step of the development of a hybrid model to identify the most important predictor variables that assist in predicting the response\n             variable. For method details see, Friedman, J.H. (1991). <DOI:10.1214/aos/1176347963>.The GWR model is then used to predict the response variable at \n             testing locations based on these selected variables that account for spatial variations in the relationships between the variables. This hybrid model \n             can improve the accuracy of the predictions compared to using an individual model alone.This developed hybrid spatial model can be useful particularly in \n             cases where the relationship between the response variable and predictor variables is complex and non-linear, and varies across locations.  "
  },
  {
    "id": 4664,
    "package_name": "MAT",
    "title": "Multidimensional Adaptive Testing",
    "description": "Simulates Multidimensional Adaptive Testing using the multidimensional three-parameter logistic model as described in Segall (1996) <doi:10.1007/BF02294343>, van der Linden (1999) <doi:10.3102/10769986024004398>, Reckase (2009) <doi:10.1007/978-0-387-89976-3>, and Mulder & van der Linden (2009) <doi:10.1007/s11336-008-9097-5>.",
    "version": "2.3.2",
    "maintainer": "Seung W. Choi <schoi@austin.utexas.edu>",
    "author": "Seung W. Choi and David R. King",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MAT Multidimensional Adaptive Testing Simulates Multidimensional Adaptive Testing using the multidimensional three-parameter logistic model as described in Segall (1996) <doi:10.1007/BF02294343>, van der Linden (1999) <doi:10.3102/10769986024004398>, Reckase (2009) <doi:10.1007/978-0-387-89976-3>, and Mulder & van der Linden (2009) <doi:10.1007/s11336-008-9097-5>.  "
  },
  {
    "id": 4684,
    "package_name": "MCARtest",
    "title": "Optimal Nonparametric Testing of Missing Completely at Random",
    "description": "Provides functions for carrying out nonparametric hypothesis tests of the MCAR hypothesis based on the theory of Frechet classes and compatibility. Also gives functions for computing halfspace representations of the marginal polytope and related geometric objects.",
    "version": "1.3",
    "maintainer": "Thomas B. Berrett <tom.berrett@warwick.ac.uk>",
    "author": "Thomas B. Berrett [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2005-110X>),\n  Alberto Bordino [aut],\n  Danat Duisenbekov [aut],\n  Sean Jaffe [aut],\n  Richard J. Samworth [aut] (ORCID:\n    <https://orcid.org/0000-0003-2426-4679>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MCARtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MCARtest Optimal Nonparametric Testing of Missing Completely at Random Provides functions for carrying out nonparametric hypothesis tests of the MCAR hypothesis based on the theory of Frechet classes and compatibility. Also gives functions for computing halfspace representations of the marginal polytope and related geometric objects.  "
  },
  {
    "id": 4691,
    "package_name": "MCM",
    "title": "Estimating and Testing Intergenerational Social Mobility Effect",
    "description": "Estimate and test inter-generational social mobility effect on an outcome with cross-sectional or longitudinal data.",
    "version": "0.1.7",
    "maintainer": "Jiahui Xu <jpx5053@psu.edu>",
    "author": "Jiahui Xu [aut, cre],\n  Liying Luo [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MCM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MCM Estimating and Testing Intergenerational Social Mobility Effect Estimate and test inter-generational social mobility effect on an outcome with cross-sectional or longitudinal data.  "
  },
  {
    "id": 4712,
    "package_name": "MDCcure",
    "title": "Martingale Dependence Tools and Testing for Mixture Cure Models",
    "description": "Computes martingale difference correlation (MDC), martingale difference divergence, and their partial extensions to assess conditional mean dependence. The methods are based on Shao and Zhang (2014) <doi:10.1080/01621459.2014.887012>. Additionally, introduces a novel hypothesis test for evaluating covariate effects on the cure rate in mixture cure models, using MDC-based statistics. The methodology is described in Monroy-Castillo et al. (2025, manuscript submitted).",
    "version": "0.1.0",
    "maintainer": "Blanca Monroy-Castillo <blancamonroy.96@gmail.com>",
    "author": "Blanca Monroy-Castillo [aut, cre],\n  Amalia J\u00e1come [aut],\n  Ricardo Cao [aut],\n  Ingrid Van Keilegom [aut],\n  Ursula M\u00fcller [aut]",
    "url": "https://github.com/CastleMon/MDCcure",
    "bug_reports": "https://github.com/CastleMon/MDCcure/issues",
    "repository": "https://cran.r-project.org/package=MDCcure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MDCcure Martingale Dependence Tools and Testing for Mixture Cure Models Computes martingale difference correlation (MDC), martingale difference divergence, and their partial extensions to assess conditional mean dependence. The methods are based on Shao and Zhang (2014) <doi:10.1080/01621459.2014.887012>. Additionally, introduces a novel hypothesis test for evaluating covariate effects on the cure rate in mixture cure models, using MDC-based statistics. The methodology is described in Monroy-Castillo et al. (2025, manuscript submitted).  "
  },
  {
    "id": 4731,
    "package_name": "MEFM",
    "title": "Perform MEFM Estimation on Matrix Time Series",
    "description": "To perform main effect matrix factor model (MEFM) estimation for a given matrix time series as described in Lam and Cen (2024) <doi:10.48550/arXiv.2406.00128>. Estimation of traditional matrix factor models is also supported. Supplementary functions for testing MEFM over factor models are included.",
    "version": "0.1.1",
    "maintainer": "Zetai Cen <z.cen@lse.ac.uk>",
    "author": "Zetai Cen [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MEFM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MEFM Perform MEFM Estimation on Matrix Time Series To perform main effect matrix factor model (MEFM) estimation for a given matrix time series as described in Lam and Cen (2024) <doi:10.48550/arXiv.2406.00128>. Estimation of traditional matrix factor models is also supported. Supplementary functions for testing MEFM over factor models are included.  "
  },
  {
    "id": 4759,
    "package_name": "MHTdiscrete",
    "title": "Multiple Hypotheses Testing for Discrete Data",
    "description": "A comprehensive tool for almost all existing multiple testing\n    methods for discrete data. The package also provides some novel multiple testing\n    procedures controlling FWER/FDR for discrete data. Given discrete p-values\n    and their domains, the [method].p.adjust function returns adjusted p-values,\n    which can be used to compare with the nominal significant level alpha and make\n    decisions. For users' convenience, the functions also provide the output option \n    for printing decision rules.",
    "version": "1.0.1",
    "maintainer": "Yalin Zhu <yalin.zhu@outlook.com>",
    "author": "Yalin Zhu, Wenge Guo",
    "url": "https://allen.shinyapps.io/MTPs/",
    "bug_reports": "https://github.com/allenzhuaz/MHTdiscrete/issues",
    "repository": "https://cran.r-project.org/package=MHTdiscrete",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MHTdiscrete Multiple Hypotheses Testing for Discrete Data A comprehensive tool for almost all existing multiple testing\n    methods for discrete data. The package also provides some novel multiple testing\n    procedures controlling FWER/FDR for discrete data. Given discrete p-values\n    and their domains, the [method].p.adjust function returns adjusted p-values,\n    which can be used to compare with the nominal significant level alpha and make\n    decisions. For users' convenience, the functions also provide the output option \n    for printing decision rules.  "
  },
  {
    "id": 4760,
    "package_name": "MHTmult",
    "title": "Multiple Hypotheses Testing for Multiple Families/Groups\nStructure",
    "description": "A Comprehensive tool for almost all existing multiple testing\n    methods for multiple families. The package summarizes the existing methods for multiple families multiple testing procedures (MTPs) such as double FDR, group Benjamini-Hochberg (GBH) procedure and average FDR controlling procedure. The package also provides some novel multiple testing procedures using selective inference idea.",
    "version": "0.1.0",
    "maintainer": "Yalin Zhu <yalin.zhu@outlook.com>",
    "author": "Yalin Zhu, Wenge Guo",
    "url": "",
    "bug_reports": "https://github.com/allenzhuaz/MHTmult/issues",
    "repository": "https://cran.r-project.org/package=MHTmult",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MHTmult Multiple Hypotheses Testing for Multiple Families/Groups\nStructure A Comprehensive tool for almost all existing multiple testing\n    methods for multiple families. The package summarizes the existing methods for multiple families multiple testing procedures (MTPs) such as double FDR, group Benjamini-Hochberg (GBH) procedure and average FDR controlling procedure. The package also provides some novel multiple testing procedures using selective inference idea.  "
  },
  {
    "id": 4831,
    "package_name": "MNM",
    "title": "Multivariate Nonparametric Methods. An Approach Based on Spatial\nSigns and Ranks",
    "description": "Multivariate tests, estimates and methods based on the identity score, spatial sign score and spatial rank score are provided. The methods include one and c-sample problems, shape estimation and testing, linear regression and principal components. The methodology is described in Oja (2010) <doi:10.1007/978-1-4419-0468-3> and Nordhausen and Oja (2011) <doi:10.18637/jss.v043.i05>.  ",
    "version": "1.0-4",
    "maintainer": "Klaus Nordhausen <klausnordhausenR@gmail.com>",
    "author": "Klaus Nordhausen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3758-8501>),\n  Jyrki Mottonen [aut] (ORCID: <https://orcid.org/0000-0002-6270-2556>),\n  Hannu Oja [aut] (ORCID: <https://orcid.org/0000-0002-4945-5976>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MNM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MNM Multivariate Nonparametric Methods. An Approach Based on Spatial\nSigns and Ranks Multivariate tests, estimates and methods based on the identity score, spatial sign score and spatial rank score are provided. The methods include one and c-sample problems, shape estimation and testing, linear regression and principal components. The methodology is described in Oja (2010) <doi:10.1007/978-1-4419-0468-3> and Nordhausen and Oja (2011) <doi:10.18637/jss.v043.i05>.    "
  },
  {
    "id": 4834,
    "package_name": "MNormTest",
    "title": "Multivariate Normal Hypothesis Testing",
    "description": "Hypothesis testing of the parameters of multivariate normal distributions, including the testing of a single mean vector, two mean vectors, multiple mean vectors, a single covariance matrix, multiple covariance matrices, a mean and a covariance matrix simultaneously, and the testing of independence of multivariate normal random vectors. Huixuan, Gao (2005, ISBN:9787301078587), \"Applied Multivariate Statistical Analysis\".",
    "version": "1.1.1",
    "maintainer": "Xifeng Zhang <cnxifeng9819@163.com>",
    "author": "Xifeng Zhang [aut, cre] (ORCID:\n    <https://orcid.org/0009-0001-8878-3753>)",
    "url": "https://github.com/Astringency/MNormTest,\nhttps://astringency.github.io/MNormTest/,\nhttps://CRAN.R-project.org/package=MNormTest",
    "bug_reports": "https://github.com/Astringency/MNormTest/issues",
    "repository": "https://cran.r-project.org/package=MNormTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MNormTest Multivariate Normal Hypothesis Testing Hypothesis testing of the parameters of multivariate normal distributions, including the testing of a single mean vector, two mean vectors, multiple mean vectors, a single covariance matrix, multiple covariance matrices, a mean and a covariance matrix simultaneously, and the testing of independence of multivariate normal random vectors. Huixuan, Gao (2005, ISBN:9787301078587), \"Applied Multivariate Statistical Analysis\".  "
  },
  {
    "id": 4837,
    "package_name": "MOEADr",
    "title": "Component-Wise MOEA/D Implementation",
    "description": "Modular implementation of Multiobjective Evolutionary Algorithms \n              based on Decomposition (MOEA/D) [Zhang and Li (2007), \n              <DOI:10.1109/TEVC.2007.892759>] for quick assembling and \n              testing of new algorithmic components, as well as easy \n              replication of published MOEA/D proposals. The full framework is\n              documented in a paper published in the Journal of Statistical \n              Software [<doi:10.18637/jss.v092.i06>].",
    "version": "1.1.3",
    "maintainer": "Felipe Campelo <fcampelo@ufmg.br>",
    "author": "Felipe Campelo [aut, cre],\n  Lucas Batista [com],\n  Claus Aranha [aut]",
    "url": "https://fcampelo.github.io/MOEADr/",
    "bug_reports": "https://github.com/fcampelo/MOEADr/issues",
    "repository": "https://cran.r-project.org/package=MOEADr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MOEADr Component-Wise MOEA/D Implementation Modular implementation of Multiobjective Evolutionary Algorithms \n              based on Decomposition (MOEA/D) [Zhang and Li (2007), \n              <DOI:10.1109/TEVC.2007.892759>] for quick assembling and \n              testing of new algorithmic components, as well as easy \n              replication of published MOEA/D proposals. The full framework is\n              documented in a paper published in the Journal of Statistical \n              Software [<doi:10.18637/jss.v092.i06>].  "
  },
  {
    "id": 4840,
    "package_name": "MOM",
    "title": "Estimation and Testing of Hypothesis",
    "description": "A collection of functions to do some statistical inferences. On estimation, it has the function to get the method of moments estimates,\n            the sampling interval. In terms of testing it has function of doing most powerful test.",
    "version": "0.1.0",
    "maintainer": "Produymna Ghose Majumdar <ghosemajumdarproduymna@gmail.com>",
    "author": "Produymna Ghose Majumdar [aut, cre],\n  Sangbartta Banerjee [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MOM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MOM Estimation and Testing of Hypothesis A collection of functions to do some statistical inferences. On estimation, it has the function to get the method of moments estimates,\n            the sampling interval. In terms of testing it has function of doing most powerful test.  "
  },
  {
    "id": 4849,
    "package_name": "MPGE",
    "title": "A Two-Step Approach to Testing Overall Effect of\nGene-Environment Interaction for Multiple Phenotypes",
    "description": "Interaction between a genetic variant (e.g., a single nucleotide polymorphism) and an environmental variable (e.g., physical activity) can have a shared effect on multiple phenotypes (e.g., blood lipids). We implement a two-step method to test for an overall interaction effect on multiple phenotypes. In first step, the method tests for an overall marginal genetic association between the genetic variant and the multivariate phenotype. The genetic variants which show an evidence of marginal overall genetic effect in the first step are prioritized while testing for an overall gene-environment interaction effect in the second step. Methodology is available from: A Majumdar, KS Burch, S Sankararaman, B Pasaniuc, WJ Gauderman, JS Witte (2020) <doi:10.1101/2020.07.06.190256>.",
    "version": "1.0.0",
    "maintainer": "Arunabha Majumdar <statgen.arunabha@gmail.com>",
    "author": "Arunabha Majumdar [aut, cre],\n  Tanushree Haldar [aut]",
    "url": "https://github.com/ArunabhaCodes/MPGE",
    "bug_reports": "https://github.com/ArunabhaCodes/MPGE/issues",
    "repository": "https://cran.r-project.org/package=MPGE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MPGE A Two-Step Approach to Testing Overall Effect of\nGene-Environment Interaction for Multiple Phenotypes Interaction between a genetic variant (e.g., a single nucleotide polymorphism) and an environmental variable (e.g., physical activity) can have a shared effect on multiple phenotypes (e.g., blood lipids). We implement a two-step method to test for an overall interaction effect on multiple phenotypes. In first step, the method tests for an overall marginal genetic association between the genetic variant and the multivariate phenotype. The genetic variants which show an evidence of marginal overall genetic effect in the first step are prioritized while testing for an overall gene-environment interaction effect in the second step. Methodology is available from: A Majumdar, KS Burch, S Sankararaman, B Pasaniuc, WJ Gauderman, JS Witte (2020) <doi:10.1101/2020.07.06.190256>.  "
  },
  {
    "id": 4883,
    "package_name": "MSCsimtester",
    "title": "Tests of Multispecies Coalescent Gene Tree Simulator Output",
    "description": "Statistical tests for validating multispecies coalescent gene tree simulators, using pairwise distances and rooted triple counts. See Allman ES, Ba\u00f1os HD, Rhodes JA 2023. Testing multispecies coalescent simulators using summary statistics, IEEE/ACM Trans Comput Biol Bioinformat, 20(2):1613\u20131618. <doi:10.1109/TCBB.2022.3177956>.",
    "version": "1.1",
    "maintainer": "Elizabeth Allman <e.allman@alaska.edu>",
    "author": "Elizabeth Allman [aut, cre, cph],\n  Hector Banos [aut, cph],\n  John Rhodes [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MSCsimtester",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSCsimtester Tests of Multispecies Coalescent Gene Tree Simulator Output Statistical tests for validating multispecies coalescent gene tree simulators, using pairwise distances and rooted triple counts. See Allman ES, Ba\u00f1os HD, Rhodes JA 2023. Testing multispecies coalescent simulators using summary statistics, IEEE/ACM Trans Comput Biol Bioinformat, 20(2):1613\u20131618. <doi:10.1109/TCBB.2022.3177956>.  "
  },
  {
    "id": 4884,
    "package_name": "MSEtool",
    "title": "Management Strategy Evaluation Toolkit",
    "description": "Development, simulation testing, and implementation of management procedures for fisheries \n    (see Carruthers & Hordyk (2018) <doi:10.1111/2041-210X.13081>).",
    "version": "3.7.5",
    "maintainer": "Adrian Hordyk <adrian@bluematterscience.com>",
    "author": "Adrian Hordyk [aut, cre],\n  Quang Huynh [aut],\n  Tom Carruthers [aut],\n  Chris Grandin [ctb] (iSCAM functions)",
    "url": "https://msetool.openmse.com/",
    "bug_reports": "https://github.com/Blue-Matter/MSEtool/issues",
    "repository": "https://cran.r-project.org/package=MSEtool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSEtool Management Strategy Evaluation Toolkit Development, simulation testing, and implementation of management procedures for fisheries \n    (see Carruthers & Hordyk (2018) <doi:10.1111/2041-210X.13081>).  "
  },
  {
    "id": 4896,
    "package_name": "MSTest",
    "title": "Hypothesis Testing for Markov Switching Models",
    "description": "Implementation of hypothesis testing procedures described in Hansen (1992) <doi:10.1002/jae.3950070506>, Carrasco, Hu, & Ploberger (2014) <doi:10.3982/ECTA8609>, Dufour & Luger (2017) <doi:10.1080/07474938.2017.1307548>, and Rodriguez Rondon & Dufour (2024) <https://grodriguezrondon.com/files/RodriguezRondon_Dufour_2025_MonteCarlo_LikelihoodRatioTest_MarkovSwitchingModels_20251014.pdf> that can be used to identify the number of regimes in Markov switching models.",
    "version": "0.1.6",
    "maintainer": "Gabriel Rodriguez Rondon <gabriel.rodriguezrondon@mail.mcgill.ca>",
    "author": "Gabriel Rodriguez Rondon [cre, aut],\n  Jean-Marie Dufour [aut]",
    "url": "https://github.com/roga11/MSTest",
    "bug_reports": "https://github.com/roga11/MSTest/issues",
    "repository": "https://cran.r-project.org/package=MSTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MSTest Hypothesis Testing for Markov Switching Models Implementation of hypothesis testing procedures described in Hansen (1992) <doi:10.1002/jae.3950070506>, Carrasco, Hu, & Ploberger (2014) <doi:10.3982/ECTA8609>, Dufour & Luger (2017) <doi:10.1080/07474938.2017.1307548>, and Rodriguez Rondon & Dufour (2024) <https://grodriguezrondon.com/files/RodriguezRondon_Dufour_2025_MonteCarlo_LikelihoodRatioTest_MarkovSwitchingModels_20251014.pdf> that can be used to identify the number of regimes in Markov switching models.  "
  },
  {
    "id": 4915,
    "package_name": "MTest",
    "title": "A Procedure for Multicollinearity Testing using Bootstrap",
    "description": "Functions for detecting multicollinearity. This test gives statistical support to two of the most famous methods for detecting multicollinearity in applied work: Klein\u2019s rule and Variance Inflation Factor (VIF). See the URL for the papers associated with this package, as for instance, Morales-O\u00f1ate and Morales-O\u00f1ate (2015) <doi:10.33333/rp.vol51n2.05>.",
    "version": "1.0.4",
    "maintainer": "V\u00edctor Morales-O\u00f1ate <vmorales.ppb@gmail.com>",
    "author": "V\u00edctor Morales-O\u00f1ate [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1922-6571>),\n  Bol\u00edvar Morales-O\u00f1ate [aut] (ORCID:\n    <https://orcid.org/0000-0003-4980-8759>)",
    "url": "https://github.com/vmoprojs/MTest",
    "bug_reports": "https://github.com/vmoprojs/MTest/issues",
    "repository": "https://cran.r-project.org/package=MTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MTest A Procedure for Multicollinearity Testing using Bootstrap Functions for detecting multicollinearity. This test gives statistical support to two of the most famous methods for detecting multicollinearity in applied work: Klein\u2019s rule and Variance Inflation Factor (VIF). See the URL for the papers associated with this package, as for instance, Morales-O\u00f1ate and Morales-O\u00f1ate (2015) <doi:10.33333/rp.vol51n2.05>.  "
  },
  {
    "id": 4921,
    "package_name": "MVET",
    "title": "Multivariate Estimates and Tests",
    "description": "Multivariate estimation and testing, currently a package for testing parametric data. To deal with parametric data, various multivariate normality tests and outlier detection are performed and visualized using the 'ggplot2' package. Homogeneity tests for covariance matrices are also possible, as well as the Hotelling's T-square test and the multivariate analysis of variance test. We are exploring additional tests and visualization techniques, such as profile analysis and randomized complete block design, to be made available in the future and making them easily accessible to users.",
    "version": "0.1.0",
    "maintainer": "Yeonseok Choi <yeonseok3488@gmail.com>",
    "author": "Yeonseok Choi [aut, cre],\n  Yong-Seok Choi [ctb]",
    "url": "https://github.com/YeonSeok-Choi/MVET",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MVET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MVET Multivariate Estimates and Tests Multivariate estimation and testing, currently a package for testing parametric data. To deal with parametric data, various multivariate normality tests and outlier detection are performed and visualized using the 'ggplot2' package. Homogeneity tests for covariance matrices are also possible, as well as the Hotelling's T-square test and the multivariate analysis of variance test. We are exploring additional tests and visualization techniques, such as profile analysis and randomized complete block design, to be made available in the future and making them easily accessible to users.  "
  },
  {
    "id": 4930,
    "package_name": "MVT",
    "title": "Estimation and Testing for the Multivariate t-Distribution",
    "description": "Routines to perform estimation and inference under the multivariate\n  t-distribution <doi:10.1007/s10182-022-00468-2>. Currently, the following methodologies \n  are implemented: multivariate mean and covariance estimation, hypothesis testing \n  about equicorrelation and homogeneity of variances, the Wilson-Hilferty transformation, \n  QQ-plots with envelopes and random variate generation.",
    "version": "0.3-81",
    "maintainer": "Felipe Osorio <felipe.osorios@usm.cl>",
    "author": "Felipe Osorio [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4675-5201>)",
    "url": "http://mvt.mat.utfsm.cl/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MVT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MVT Estimation and Testing for the Multivariate t-Distribution Routines to perform estimation and inference under the multivariate\n  t-distribution <doi:10.1007/s10182-022-00468-2>. Currently, the following methodologies \n  are implemented: multivariate mean and covariance estimation, hypothesis testing \n  about equicorrelation and homogeneity of variances, the Wilson-Hilferty transformation, \n  QQ-plots with envelopes and random variate generation.  "
  },
  {
    "id": 4937,
    "package_name": "MacBehaviour",
    "title": "Behavioural Studies of Large Language Models",
    "description": "Efficient way to design and conduct psychological experiments for testing the performance of large language models. It simplifies the process of setting up experiments and data collection via language models\u2019 API, facilitating a smooth workflow for researchers in the field of machine behaviour.",
    "version": "1.2.8",
    "maintainer": "Xufeng Duan <dxfdxfdxf88@gmail.com>",
    "author": "Xufeng Duan [aut, cre],\n  Shixuan Li [aut],\n  Zhenguang Cai [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MacBehaviour",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MacBehaviour Behavioural Studies of Large Language Models Efficient way to design and conduct psychological experiments for testing the performance of large language models. It simplifies the process of setting up experiments and data collection via language models\u2019 API, facilitating a smooth workflow for researchers in the field of machine behaviour.  "
  },
  {
    "id": 4956,
    "package_name": "ManyTests",
    "title": "Multiple Testing Procedures of Cox (2011) and Wong and Cox\n(2007)",
    "description": "Performs the multiple testing procedures of Cox (2011) <doi:10.5170/CERN-2011-006> and Wong and Cox (2007) <doi:10.1080/02664760701240014>.",
    "version": "1.2",
    "maintainer": "Christiana Kartsonaki <christiana.kartsonaki@gmail.com>",
    "author": "Christiana Kartsonaki",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ManyTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ManyTests Multiple Testing Procedures of Cox (2011) and Wong and Cox\n(2007) Performs the multiple testing procedures of Cox (2011) <doi:10.5170/CERN-2011-006> and Wong and Cox (2007) <doi:10.1080/02664760701240014>.  "
  },
  {
    "id": 5047,
    "package_name": "MiSPU",
    "title": "Microbiome Based Sum of Powered Score (MiSPU) Tests",
    "description": "There is an increasing interest in investigating how the compositions of microbial communities are associated with human health and disease. In this package, we present a novel global testing method called aMiSPU, that is highly adaptive and thus high powered across various scenarios, alleviating the issue with the choice of a phylogenetic distance. Our simulations and real data analysis demonstrated that aMiSPU test was often more powerful than several competing methods while correctly controlling type I error rates.",
    "version": "1.0",
    "maintainer": "Chong Wu <wuxx0845@umn.edu>",
    "author": "Chong Wu, Wei Pan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MiSPU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MiSPU Microbiome Based Sum of Powered Score (MiSPU) Tests There is an increasing interest in investigating how the compositions of microbial communities are associated with human health and disease. In this package, we present a novel global testing method called aMiSPU, that is highly adaptive and thus high powered across various scenarios, alleviating the issue with the choice of a phylogenetic distance. Our simulations and real data analysis demonstrated that aMiSPU test was often more powerful than several competing methods while correctly controlling type I error rates.  "
  },
  {
    "id": 5060,
    "package_name": "MinEDfind",
    "title": "A Bayesian Design for Minimum Effective Dosing-Finding Trial",
    "description": "The nonparametric two-stage Bayesian adaptive design is a novel phase II clinical trial design for finding the minimum effective dose (MinED). This design is motivated by the top priority and concern of clinicians when testing a new drug, which is to effectively treat patients and minimize the chance of exposing them to subtherapeutic or overly toxic doses. It is used to design single-agent trials. ",
    "version": "0.1.3",
    "maintainer": "Chia-Wei Hsu <Chia-Wei.Hsu@stjude.org>",
    "author": "Chia-Wei Hsu, Fang Wang, Rongji Mu, Haitao Pan, Guoying Xu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MinEDfind",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MinEDfind A Bayesian Design for Minimum Effective Dosing-Finding Trial The nonparametric two-stage Bayesian adaptive design is a novel phase II clinical trial design for finding the minimum effective dose (MinED). This design is motivated by the top priority and concern of clinicians when testing a new drug, which is to effectively treat patients and minimize the chance of exposing them to subtherapeutic or overly toxic doses. It is used to design single-agent trials.   "
  },
  {
    "id": 5068,
    "package_name": "MissMech",
    "title": "Testing Homoscedasticity, Multivariate Normality, and Missing\nCompletely at Random",
    "description": "To test whether the missing data mechanism, in a set of incompletely observed data, is one of missing completely at random (MCAR). \n    For detailed description see Jamshidian, M. Jalal, S., and Jansen, C. (2014). \"MissMech: An R Package for Testing Homoscedasticity, Multivariate Normality, and Missing Completely at Random (MCAR)\", Journal of Statistical Software, 56(6), 1-31. <https://www.jstatsoft.org/v56/i06/> <doi:10.18637/jss.v056.i06>.",
    "version": "1.0.4",
    "maintainer": "Mao Kobayashi <kobamao.jp@gmail.com>",
    "author": "Mortaza Jamshidian [aut],\n  Siavash Jalal [aut],\n  Camden Jansen [aut],\n  Mao Kobayashi [cre]",
    "url": "https://github.com/indenkun/MissMech",
    "bug_reports": "https://github.com/indenkun/MissMech/issues",
    "repository": "https://cran.r-project.org/package=MissMech",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MissMech Testing Homoscedasticity, Multivariate Normality, and Missing\nCompletely at Random To test whether the missing data mechanism, in a set of incompletely observed data, is one of missing completely at random (MCAR). \n    For detailed description see Jamshidian, M. Jalal, S., and Jansen, C. (2014). \"MissMech: An R Package for Testing Homoscedasticity, Multivariate Normality, and Missing Completely at Random (MCAR)\", Journal of Statistical Software, 56(6), 1-31. <https://www.jstatsoft.org/v56/i06/> <doi:10.18637/jss.v056.i06>.  "
  },
  {
    "id": 5072,
    "package_name": "Missplot",
    "title": "Missing Plot Technique in Design of Experiment",
    "description": "A system for testing differential effects among treatments in case of Randomised Block Design and Latin Square Design when there is one missing observation. Methods for this process are as described in A.M.Gun,M.K.Gupta and B.Dasgupta(2019,ISBN:81-87567-81-3). ",
    "version": "0.1.0",
    "maintainer": "Shantanu Nayek <shantanuashis@gmail.com>",
    "author": "Shantanu Nayek [cre, aut],\n  Saheli Datta [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Missplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Missplot Missing Plot Technique in Design of Experiment A system for testing differential effects among treatments in case of Randomised Block Design and Latin Square Design when there is one missing observation. Methods for this process are as described in A.M.Gun,M.K.Gupta and B.Dasgupta(2019,ISBN:81-87567-81-3).   "
  },
  {
    "id": 5084,
    "package_name": "MixTwice",
    "title": "Large-Scale Hypothesis Testing by Variance Mixing",
    "description": "Implements large-scale hypothesis testing by variance mixing. It takes two statistics per testing unit -- an estimated effect and its associated squared standard error -- and fits a nonparametric, shape-constrained mixture separately on two latent parameters. It reports local false discovery rates (lfdr) and local false sign rates (lfsr). Manuscript describing algorithm of MixTwice: Zheng et al(2021) <doi: 10.1093/bioinformatics/btab162>.",
    "version": "2.0",
    "maintainer": "Zihao Zheng <zihao.zheng@wisc.edu>",
    "author": "Zihao Zheng and  Michael A.Newton",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MixTwice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MixTwice Large-Scale Hypothesis Testing by Variance Mixing Implements large-scale hypothesis testing by variance mixing. It takes two statistics per testing unit -- an estimated effect and its associated squared standard error -- and fits a nonparametric, shape-constrained mixture separately on two latent parameters. It reports local false discovery rates (lfdr) and local false sign rates (lfsr). Manuscript describing algorithm of MixTwice: Zheng et al(2021) <doi: 10.1093/bioinformatics/btab162>.  "
  },
  {
    "id": 5085,
    "package_name": "MixedIndTests",
    "title": "Tests of Randomness and Tests of Independence",
    "description": "Functions for testing randomness for a univariate time series with arbitrary distribution  (discrete, continuous, mixture of both types) and for testing  independence between random variables with arbitrary distributions. The test statistics are based on the multilinear empirical copula and multipliers are used to compute P-values. The test of independence between random variables appeared in  Genest, Ne\u0161lehov\u00e1, R\u00e9millard & Murphy (2019) and the test of randomness appeared in Nasri (2022).",
    "version": "1.2.0",
    "maintainer": "Bouchra R. Nasri <bouchra.nasri@umontreal.ca>",
    "author": "Bouchra R. Nasri [aut, cre, cph],\n  Bruno N Remillard [aut],\n  Johanna G Neslehova [aut],\n  Christian Genest [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MixedIndTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MixedIndTests Tests of Randomness and Tests of Independence Functions for testing randomness for a univariate time series with arbitrary distribution  (discrete, continuous, mixture of both types) and for testing  independence between random variables with arbitrary distributions. The test statistics are based on the multilinear empirical copula and multipliers are used to compute P-values. The test of independence between random variables appeared in  Genest, Ne\u0161lehov\u00e1, R\u00e9millard & Murphy (2019) and the test of randomness appeared in Nasri (2022).  "
  },
  {
    "id": 5119,
    "package_name": "MonteCarloSEM",
    "title": "Monte Carlo Data Simulation Package",
    "description": "Monte Carlo simulation allows testing different conditions given to the correct structural equation models. This package runs Monte Carlo simulations under different conditions (such as sample size or normality of data). Within the package data sets can be simulated and run based on the given model.\n  First, continuous and normal data sets are generated based on the given model. Later Fleishman's power method (1978) <DOI:10.1007/BF02293811> is used to add non-normality if exists. \n  When data generation is completed (or when generated data sets are given) model test can also be run.\n  Please cite as \"Or\u00e7an, F. (2021). MonteCarloSEM: An R Package to Simulate Data for SEM. International Journal of Assessment Tools in Education, 8 (3), 704-713.\"",
    "version": "0.0.8",
    "maintainer": "Fatih Orcan <fatihorcan84@gmail.com>",
    "author": "Fatih Orcan [aut, cre] (ORCID: <https://orcid.org/0000-0003-1727-0456>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MonteCarloSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MonteCarloSEM Monte Carlo Data Simulation Package Monte Carlo simulation allows testing different conditions given to the correct structural equation models. This package runs Monte Carlo simulations under different conditions (such as sample size or normality of data). Within the package data sets can be simulated and run based on the given model.\n  First, continuous and normal data sets are generated based on the given model. Later Fleishman's power method (1978) <DOI:10.1007/BF02293811> is used to add non-normality if exists. \n  When data generation is completed (or when generated data sets are given) model test can also be run.\n  Please cite as \"Or\u00e7an, F. (2021). MonteCarloSEM: An R Package to Simulate Data for SEM. International Journal of Assessment Tools in Education, 8 (3), 704-713.\"  "
  },
  {
    "id": 5137,
    "package_name": "MultANOVA",
    "title": "Analysis of Designed High-Dimensional Data using the\nComprehensive MultANOVA Framework",
    "description": "A comprehensive and computationally fast framework to analyze high dimensional data associated with an experimental design based on Multiple ANOVAs (MultANOVA). It includes testing the overall significance of terms in the model, post-hoc analyses of significant terms and variable selection. Details may be found in Mahieu, B., & Cariou, V. (2025). MultANOVA Followed by Post Hoc Analyses for Designed High\u2010Dimensional Data: A Comprehensive Framework That Outperforms ASCA, rMANOVA, and VASCA. Journal of Chemometrics, 39(7). <doi:10.1002/cem.70039>.",
    "version": "1.0.1",
    "maintainer": "Benjamin Mahieu <benjamin.mahieu@oniris-nantes.fr>",
    "author": "Benjamin Mahieu [aut, cre],\n  Veronique Cariou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultANOVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultANOVA Analysis of Designed High-Dimensional Data using the\nComprehensive MultANOVA Framework A comprehensive and computationally fast framework to analyze high dimensional data associated with an experimental design based on Multiple ANOVAs (MultANOVA). It includes testing the overall significance of terms in the model, post-hoc analyses of significant terms and variable selection. Details may be found in Mahieu, B., & Cariou, V. (2025). MultANOVA Followed by Post Hoc Analyses for Designed High\u2010Dimensional Data: A Comprehensive Framework That Outperforms ASCA, rMANOVA, and VASCA. Journal of Chemometrics, 39(7). <doi:10.1002/cem.70039>.  "
  },
  {
    "id": 5141,
    "package_name": "MultNonParam",
    "title": "Multivariate Nonparametric Methods",
    "description": "A collection of multivariate nonparametric methods, selected in part to support an MS level course in nonparametric statistical methods. Methods include adjustments for multiple comparisons, implementation of multivariate Mann-Whitney-Wilcoxon testing, inversion of these tests to produce a confidence region, some permutation tests for linear models, and some algorithms for calculating exact probabilities associated with one- and two- stage testing involving Mann-Whitney-Wilcoxon statistics.  Supported by grant NSF DMS 1712839.  See Kolassa and Seifu (2013) <doi:10.1016/j.acra.2013.03.006>.",
    "version": "1.3.9",
    "maintainer": "John E. Kolassa <kolassa@stat.rutgers.edu>",
    "author": "John E. Kolassa [cre],\n  Stephane Jankowski [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultNonParam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultNonParam Multivariate Nonparametric Methods A collection of multivariate nonparametric methods, selected in part to support an MS level course in nonparametric statistical methods. Methods include adjustments for multiple comparisons, implementation of multivariate Mann-Whitney-Wilcoxon testing, inversion of these tests to produce a confidence region, some permutation tests for linear models, and some algorithms for calculating exact probabilities associated with one- and two- stage testing involving Mann-Whitney-Wilcoxon statistics.  Supported by grant NSF DMS 1712839.  See Kolassa and Seifu (2013) <doi:10.1016/j.acra.2013.03.006>.  "
  },
  {
    "id": 5153,
    "package_name": "MultiGroupSequential",
    "title": "Group-Sequential Procedures with Multiple Hypotheses",
    "description": "It is often challenging to strongly control the family-wise type-1 error rate in the group-sequential trials with multiple endpoints (hypotheses). The inflation of type-1 error rate comes from two sources (S1) repeated testing individual hypothesis and (S2) simultaneous testing multiple hypotheses. The 'MultiGroupSequential' package is intended to help researchers to tackle this challenge. The procedures provided include the sequential procedures described in Luo and Quan (2023) <doi:10.1080/19466315.2023.2191989> and the graphical procedure proposed by Maurer and Bretz (2013) <doi:10.1080/19466315.2013.807748>. Luo and Quan (2013) describes three procedures, and the functions to implement these procedures are (1) seqgspgx() implements a sequential graphical procedure based on the group-sequential p-values; (2) seqgsphh() implements a sequential Hochberg/Hommel procedure based on the group-sequential p-values; and (3) seqqvalhh() implements a sequential Hochberg/Hommel procedure based on the q-values. In addition, seqmbgx() implements the sequential graphical procedure described in Maurer and Bretz (2013).",
    "version": "1.1.0",
    "maintainer": "Xiaodong Luo <Xiaodong.Luo@sanofi.com>",
    "author": "Xiaodong Luo [aut, cre],\n  Hui Quan [ctb],\n  Sanofi [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultiGroupSequential",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiGroupSequential Group-Sequential Procedures with Multiple Hypotheses It is often challenging to strongly control the family-wise type-1 error rate in the group-sequential trials with multiple endpoints (hypotheses). The inflation of type-1 error rate comes from two sources (S1) repeated testing individual hypothesis and (S2) simultaneous testing multiple hypotheses. The 'MultiGroupSequential' package is intended to help researchers to tackle this challenge. The procedures provided include the sequential procedures described in Luo and Quan (2023) <doi:10.1080/19466315.2023.2191989> and the graphical procedure proposed by Maurer and Bretz (2013) <doi:10.1080/19466315.2013.807748>. Luo and Quan (2013) describes three procedures, and the functions to implement these procedures are (1) seqgspgx() implements a sequential graphical procedure based on the group-sequential p-values; (2) seqgsphh() implements a sequential Hochberg/Hommel procedure based on the group-sequential p-values; and (3) seqqvalhh() implements a sequential Hochberg/Hommel procedure based on the q-values. In addition, seqmbgx() implements the sequential graphical procedure described in Maurer and Bretz (2013).  "
  },
  {
    "id": 5155,
    "package_name": "MultiKink",
    "title": "Estimation and Inference for Multi-Kink Quantile Regression",
    "description": "Estimation and inference for multiple kink quantile regression for longitudinal data and the i.i.d data. A bootstrap restarting iterative segmented quantile algorithm is proposed to estimate the multiple kink quantile regression model conditional on a given number of change points. The number of kinks is also allowed to be unknown. In such case, the backward elimination algorithm and the bootstrap restarting iterative segmented quantile algorithm are combined to select the number of change points based on a quantile BIC. For longitudinal data, we also develop the GEE estimator to incorporate the within-subject correlations.  A score-type based test statistic is also developed for testing the existence of kink effect. The package is based on the paper, ``Wei Zhong, Chuang Wan and Wenyang Zhang (2022). Estimation and inference for multikink quantile regression, JBES'' and ``Chuang Wan, Wei Zhong, Wenyang Zhang and Changliang Zou (2022). Multi-kink quantile regression for longitudinal data with application to progesterone data analysis, Biometrics\". ",
    "version": "0.2.0",
    "maintainer": "Chuang Wan <wanchuanghnu@126.com>",
    "author": "Chuang Wan [aut, cre],\n  Wei Zhong [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultiKink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultiKink Estimation and Inference for Multi-Kink Quantile Regression Estimation and inference for multiple kink quantile regression for longitudinal data and the i.i.d data. A bootstrap restarting iterative segmented quantile algorithm is proposed to estimate the multiple kink quantile regression model conditional on a given number of change points. The number of kinks is also allowed to be unknown. In such case, the backward elimination algorithm and the bootstrap restarting iterative segmented quantile algorithm are combined to select the number of change points based on a quantile BIC. For longitudinal data, we also develop the GEE estimator to incorporate the within-subject correlations.  A score-type based test statistic is also developed for testing the existence of kink effect. The package is based on the paper, ``Wei Zhong, Chuang Wan and Wenyang Zhang (2022). Estimation and inference for multikink quantile regression, JBES'' and ``Chuang Wan, Wei Zhong, Wenyang Zhang and Changliang Zou (2022). Multi-kink quantile regression for longitudinal data with application to progesterone data analysis, Biometrics\".   "
  },
  {
    "id": 5175,
    "package_name": "MultisiteMediation",
    "title": "Causal Mediation Analysis in Multisite Trials",
    "description": "Multisite causal mediation analysis using the methods proposed by Qin and Hong (2017) <doi:10.3102/1076998617694879>, Qin, Hong, Deutsch, and Bein (2019) <doi:10.1111/rssa.12446>, and Qin, Deutsch, and Hong (2021) <doi:10.1002/pam.22268>. It enables causal mediation analysis in multisite trials, in which individuals are assigned to a treatment or a control group at each site. It allows for estimation and hypothesis testing for not only the population average but also the between-site variance of direct and indirect effects transmitted through one single mediator or two concurrent (conditionally independent) mediators. This strategy conveniently relaxes the assumption of no treatment-by-mediator interaction while greatly simplifying the outcome model specification without invoking strong distributional assumptions. This package also provides a function that can further incorporate a sample weight and a nonresponse weight for multisite causal mediation analysis in the presence of complex sample and survey designs and non-random nonresponse, to enhance both the internal validity and external validity. The package also provides a weighting-based balance checking function for assessing the remaining overt bias.",
    "version": "0.0.4",
    "maintainer": "Xu Qin <xuqin@pitt.edu>",
    "author": "Xu Qin, Guanglei Hong, Jonah Deutsch, and Edward Bein",
    "url": "https://github.com/Xu-Qin/MultisiteMediation",
    "bug_reports": "https://github.com/Xu-Qin/MultisiteMediation/issues",
    "repository": "https://cran.r-project.org/package=MultisiteMediation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultisiteMediation Causal Mediation Analysis in Multisite Trials Multisite causal mediation analysis using the methods proposed by Qin and Hong (2017) <doi:10.3102/1076998617694879>, Qin, Hong, Deutsch, and Bein (2019) <doi:10.1111/rssa.12446>, and Qin, Deutsch, and Hong (2021) <doi:10.1002/pam.22268>. It enables causal mediation analysis in multisite trials, in which individuals are assigned to a treatment or a control group at each site. It allows for estimation and hypothesis testing for not only the population average but also the between-site variance of direct and indirect effects transmitted through one single mediator or two concurrent (conditionally independent) mediators. This strategy conveniently relaxes the assumption of no treatment-by-mediator interaction while greatly simplifying the outcome model specification without invoking strong distributional assumptions. This package also provides a function that can further incorporate a sample weight and a nonresponse weight for multisite causal mediation analysis in the presence of complex sample and survey designs and non-random nonresponse, to enhance both the internal validity and external validity. The package also provides a weighting-based balance checking function for assessing the remaining overt bias.  "
  },
  {
    "id": 5178,
    "package_name": "MultivariateTrendAnalysis",
    "title": "Univariate and Multivariate Trend Testing",
    "description": "With foundations on the work by Goutali and Chebana (2024) <doi:10.1016/j.envsoft.2024.106090>, this package contains various \n  univariate and multivariate trend tests. The main functions regard the Multivariate \n  Dependence Trend and Multivariate Overall Trend tests as proposed by Goutali \n  and Chebana (2024), as well as a plotting function that proves useful as a summary \n  and complement of the tests.\n  Although many packages and methods carry univariate tests, the Mann-Kendall \n  and Spearman's rho test implementations are included in the package with an \n  adapted version to hydrological formulation (e.g. as in Rao and Hamed 1998 <doi:10.1016/S0022-1694(97)00125-X> or Chebana 2022 <doi:10.1016/C2021-0-01317-1>).\n  For better understanding of the example use of the functions, three datasets \n  are included. These are synthetic data and shouldn't be used beyond that purpose.",
    "version": "0.1.3",
    "maintainer": "Dorsaf Goutali <dorsaf.goutali@inrs.ca>",
    "author": "Dorsaf Goutali [aut, cre],\n  Fateh Chebana [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MultivariateTrendAnalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MultivariateTrendAnalysis Univariate and Multivariate Trend Testing With foundations on the work by Goutali and Chebana (2024) <doi:10.1016/j.envsoft.2024.106090>, this package contains various \n  univariate and multivariate trend tests. The main functions regard the Multivariate \n  Dependence Trend and Multivariate Overall Trend tests as proposed by Goutali \n  and Chebana (2024), as well as a plotting function that proves useful as a summary \n  and complement of the tests.\n  Although many packages and methods carry univariate tests, the Mann-Kendall \n  and Spearman's rho test implementations are included in the package with an \n  adapted version to hydrological formulation (e.g. as in Rao and Hamed 1998 <doi:10.1016/S0022-1694(97)00125-X> or Chebana 2022 <doi:10.1016/C2021-0-01317-1>).\n  For better understanding of the example use of the functions, three datasets \n  are included. These are synthetic data and shouldn't be used beyond that purpose.  "
  },
  {
    "id": 5180,
    "package_name": "MulvariateRandomForestVarImp",
    "title": "Variable Importance Measures for Multivariate Random Forests",
    "description": "Calculates two sets of post-hoc variable importance measures for multivariate random forests. The first set of variable importance measures are given by the sum of mean split improvements for splits defined by feature j measured on user-defined examples (i.e., training or testing samples). The second set of importance measures are calculated on a per-outcome variable basis as the sum of mean absolute difference of node values for each split defined by feature j measured on user-defined examples (i.e., training or testing samples). The user can optionally threshold both sets of importance measures to include only splits that are statistically significant as measured using an F-test. ",
    "version": "0.0.2",
    "maintainer": "Dogonadze Nika <nika.dogonadze@toptal.com>",
    "author": "Sikdar Sharmistha [aut],\n  Hooker Giles [aut],\n  Kadiyali Vrinda [ctb],\n  Dogonadze Nika [cre]",
    "url": "https://github.com/Megatvini/VIM/",
    "bug_reports": "https://github.com/Megatvini/VIM/issues",
    "repository": "https://cran.r-project.org/package=MulvariateRandomForestVarImp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MulvariateRandomForestVarImp Variable Importance Measures for Multivariate Random Forests Calculates two sets of post-hoc variable importance measures for multivariate random forests. The first set of variable importance measures are given by the sum of mean split improvements for splits defined by feature j measured on user-defined examples (i.e., training or testing samples). The second set of importance measures are calculated on a per-outcome variable basis as the sum of mean absolute difference of node values for each split defined by feature j measured on user-defined examples (i.e., training or testing samples). The user can optionally threshold both sets of importance measures to include only splits that are statistically significant as measured using an F-test.   "
  },
  {
    "id": 5193,
    "package_name": "NAP",
    "title": "Non-Local Alternative Priors in Psychology",
    "description": "Conducts Bayesian Hypothesis tests of a point null hypothesis against a two-sided alternative\n             using Non-local Alternative Prior (NAP) for one- and two-sample z- and t-tests \n             (Pramanik and Johnson, 2022). Under the alternative, the NAP is assumed on the standardized \n             effects size in one-sample tests and on their differences in two-sample tests. The package \n             considers two types of NAP densities: (1) the normal moment prior, and (2) the composite alternative. \n             In fixed design tests, the functions calculate the Bayes factors and the expected weight of evidence\n             for varied effect size and sample size. The package also provides a sequential testing framework using the\n             Sequential Bayes Factor (SBF) design. The functions calculate the operating characteristics (OC)\n             and the average sample number (ASN), and also conducts sequential tests for a sequentially observed data.",
    "version": "1.1",
    "maintainer": "Sandipan Pramanik <sandy@stat.tamu.edu>",
    "author": "Sandipan Pramanik [aut, cre],\n  Valen E. Johnson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NAP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NAP Non-Local Alternative Priors in Psychology Conducts Bayesian Hypothesis tests of a point null hypothesis against a two-sided alternative\n             using Non-local Alternative Prior (NAP) for one- and two-sample z- and t-tests \n             (Pramanik and Johnson, 2022). Under the alternative, the NAP is assumed on the standardized \n             effects size in one-sample tests and on their differences in two-sample tests. The package \n             considers two types of NAP densities: (1) the normal moment prior, and (2) the composite alternative. \n             In fixed design tests, the functions calculate the Bayes factors and the expected weight of evidence\n             for varied effect size and sample size. The package also provides a sequential testing framework using the\n             Sequential Bayes Factor (SBF) design. The functions calculate the operating characteristics (OC)\n             and the average sample number (ASN), and also conducts sequential tests for a sequentially observed data.  "
  },
  {
    "id": 5230,
    "package_name": "NIPTeR",
    "title": "Fast and Accurate Trisomy Prediction in Non-Invasive Prenatal\nTesting",
    "description": "Fast and Accurate Trisomy Prediction in Non-Invasive Prenatal\n    Testing.",
    "version": "1.0.2",
    "maintainer": "Lennart Johansson <l.johansson@umcg.nl>",
    "author": "Dirk de Weerd, Lennart Johansson",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NIPTeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NIPTeR Fast and Accurate Trisomy Prediction in Non-Invasive Prenatal\nTesting Fast and Accurate Trisomy Prediction in Non-Invasive Prenatal\n    Testing.  "
  },
  {
    "id": 5231,
    "package_name": "NIRStat",
    "title": "Novel Statistical Methods for Studying Near-Infrared\nSpectroscopy (NIRS) Time Series Data",
    "description": "Provides transfusion-related differential tests on Near-infrared spectroscopy (NIRS) time series with detection limit, which contains two testing statistics: Mean Area Under the Curve (MAUC) and slope statistic. This package applied a penalized spline method within imputation setting. Testing is conducted by a nested permutation approach within imputation. Refer to Guo et al (2018) <doi:10.1177/0962280218786302> for further details.",
    "version": "1.1",
    "maintainer": "Yikai Wang <johnzon.wyk@gmail.com>",
    "author": "Yikai Wang [Emory University], Xiao Wang [ICF]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NIRStat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NIRStat Novel Statistical Methods for Studying Near-Infrared\nSpectroscopy (NIRS) Time Series Data Provides transfusion-related differential tests on Near-infrared spectroscopy (NIRS) time series with detection limit, which contains two testing statistics: Mean Area Under the Curve (MAUC) and slope statistic. This package applied a penalized spline method within imputation setting. Testing is conducted by a nested permutation approach within imputation. Refer to Guo et al (2018) <doi:10.1177/0962280218786302> for further details.  "
  },
  {
    "id": 5232,
    "package_name": "NISTnls",
    "title": "Nonlinear least squares examples from NIST",
    "description": "Datasets for testing nonlinear regression routines.",
    "version": "0.9-13",
    "maintainer": "Douglas Bates <bates@stat.wisc.edu>",
    "author": "original from National Institutes for Standards and Technology\n        (NIST) http://www.itl.nist.gov/div898/strd/nls/nls_main.shtml R\n        port by Douglas Bates <bates@stat.wisc.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NISTnls",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NISTnls Nonlinear least squares examples from NIST Datasets for testing nonlinear regression routines.  "
  },
  {
    "id": 5248,
    "package_name": "NMTox",
    "title": "Dose-Response Relationship Analysis of Nanomaterial Toxicity",
    "description": "Perform an exploration and a preliminary analysis on the dose-\n    response relationship of nanomaterial toxicity. Several functions are\n    provided for data exploration, including functions for creating a subset of\n    dataset, frequency tables and plots. Inference for order restricted dose-\n    response data is performed by testing the significance of monotonic\n    dose-response relationship, using Williams, Marcus, M, Modified M and \n    Likelihood ratio tests. Several methods of multiplicity adjustment \n    are also provided. Description of the methods can be found in <https://github.com/rahmasarina/dose-response-analysis/blob/main/Methodology.pdf>.",
    "version": "0.1.0",
    "maintainer": "Rahmasari Nur Azizah <rahma.azizah@uhasselt.be>",
    "author": "Rahmasari Nur Azizah [aut, cre],\n  Geert Verheyen [aut, ths],\n  Sabine Van Miert [aut, ths],\n  Ziv Shkedy [aut, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NMTox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NMTox Dose-Response Relationship Analysis of Nanomaterial Toxicity Perform an exploration and a preliminary analysis on the dose-\n    response relationship of nanomaterial toxicity. Several functions are\n    provided for data exploration, including functions for creating a subset of\n    dataset, frequency tables and plots. Inference for order restricted dose-\n    response data is performed by testing the significance of monotonic\n    dose-response relationship, using Williams, Marcus, M, Modified M and \n    Likelihood ratio tests. Several methods of multiplicity adjustment \n    are also provided. Description of the methods can be found in <https://github.com/rahmasarina/dose-response-analysis/blob/main/Methodology.pdf>.  "
  },
  {
    "id": 5274,
    "package_name": "NRejections",
    "title": "Metrics for Multiple Testing with Correlated Outcomes",
    "description": "Implements methods in Mathur and VanderWeele (in preparation) to characterize global evidence strength across W correlated ordinary least squares (OLS) hypothesis tests. Specifically, uses resampling to estimate a null interval for the total number of rejections in, for example, 95% of samples generated with no associations (the global null), the excess hits (the difference between the observed number of rejections and the upper limit of the null interval), and a test of the global null based on the number of rejections.",
    "version": "1.2.0",
    "maintainer": "Maya B. Mathur <mmathur@stanford.edu>",
    "author": "Maya B. Mathur, Tyler J. VanderWeele",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NRejections",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NRejections Metrics for Multiple Testing with Correlated Outcomes Implements methods in Mathur and VanderWeele (in preparation) to characterize global evidence strength across W correlated ordinary least squares (OLS) hypothesis tests. Specifically, uses resampling to estimate a null interval for the total number of rejections in, for example, 95% of samples generated with no associations (the global null), the excess hits (the difference between the observed number of rejections and the upper limit of the null interval), and a test of the global null based on the number of rejections.  "
  },
  {
    "id": 5314,
    "package_name": "NetRep",
    "title": "Permutation Testing Network Module Preservation Across Datasets",
    "description": "Functions for assessing the replication/preservation of a network \n  module's topology across datasets through permutation testing; Ritchie et al. \n  (2015) <doi: 10.1016/j.cels.2016.06.012>.",
    "version": "1.2.9",
    "maintainer": "Scott Ritchie <sritchie73@gmail.com>",
    "author": "Scott Ritchie [aut, cre] (0000-0002-8454-9548)",
    "url": "",
    "bug_reports": "https://github.com/sritchie73/NetRep/issues",
    "repository": "https://cran.r-project.org/package=NetRep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NetRep Permutation Testing Network Module Preservation Across Datasets Functions for assessing the replication/preservation of a network \n  module's topology across datasets through permutation testing; Ritchie et al. \n  (2015) <doi: 10.1016/j.cels.2016.06.012>.  "
  },
  {
    "id": 5331,
    "package_name": "NeuroDataSets",
    "title": "A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets",
    "description": "Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.",
    "version": "0.3.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "author": "Renzo Caceres Rossi [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-0744-854X>)",
    "url": "https://github.com/lightbluetitan/neurodatasets,\nhttps://lightbluetitan.github.io/neurodatasets/",
    "bug_reports": "https://github.com/lightbluetitan/neurodatasets/issues",
    "repository": "https://cran.r-project.org/package=NeuroDataSets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NeuroDataSets A Comprehensive Collection of Neuroscience and Brain-Related\nDatasets Offers a rich and diverse collection of datasets focused on the brain, nervous system, and related disorders. \n    The package includes clinical, experimental, neuroimaging, behavioral, cognitive, and simulated data on conditions such as \n    Parkinson's disease, Alzheimer's disease, dementia, epilepsy, schizophrenia, autism spectrum disorder, attention deficit, hyperactivity disorder, \n    Tourette's syndrome, traumatic brain injury, gliomas, migraines, headaches, sleep disorders, concussions, encephalitis, \n    subarachnoid hemorrhage, and mental health conditions. Datasets cover structural and functional brain data, cross-sectional and longitudinal \n    MRI imaging studies, neurotransmission, gene expression, cognitive performance, intelligence metrics, sleep deprivation effects, treatment outcomes, \n    brain-body relationships across species, neurological injury patterns, and acupuncture interventions. Data sources include peer-reviewed studies, \n    clinical trials, military health records, sports injury databases, and international comparative studies.\n    Designed for researchers, neuroscientists, clinicians, psychologists, data scientists, and students, this package facilitates exploratory data analysis, \n    statistical modeling, and hypothesis testing in neuroscience and neuroepidemiology.  "
  },
  {
    "id": 5348,
    "package_name": "NonParRolCor",
    "title": "a Non-Parametric Statistical Significance Test for Rolling\nWindow Correlation",
    "description": "Estimates and plots (as a single plot and as a heat map) the rolling window correlation coefficients between two time series and computes their statistical significance, which is carried out through a non-parametric computing-intensive method. This method addresses the effects due to the multiple testing (inflation of the Type I error) when the statistical significance is estimated for the rolling window correlation coefficients. The method is based on Monte Carlo simulations by permuting one of the variables (e.g., the dependent) under analysis and keeping fixed the other variable (e.g., the independent). We improve the computational efficiency of this method to reduce the computation time through parallel computing. The 'NonParRolCor' package also provides examples with synthetic and real-life environmental time series to exemplify its use. Methods derived from R. Telford (2013) <https://quantpalaeo.wordpress.com/2013/01/04/> and J.M. Polanco-Martinez and J.L. Lopez-Martinez (2021) <doi:10.1016/j.ecoinf.2021.101379>.",
    "version": "0.8.0",
    "maintainer": "Josue M. Polanco-Martinez <josue.m.polanco@gmail.com>",
    "author": "Josue M. Polanco-Martinez [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0001-7164-0185>),\n  Jose L. Lopez-Martinez [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2489-7559>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=NonParRolCor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NonParRolCor a Non-Parametric Statistical Significance Test for Rolling\nWindow Correlation Estimates and plots (as a single plot and as a heat map) the rolling window correlation coefficients between two time series and computes their statistical significance, which is carried out through a non-parametric computing-intensive method. This method addresses the effects due to the multiple testing (inflation of the Type I error) when the statistical significance is estimated for the rolling window correlation coefficients. The method is based on Monte Carlo simulations by permuting one of the variables (e.g., the dependent) under analysis and keeping fixed the other variable (e.g., the independent). We improve the computational efficiency of this method to reduce the computation time through parallel computing. The 'NonParRolCor' package also provides examples with synthetic and real-life environmental time series to exemplify its use. Methods derived from R. Telford (2013) <https://quantpalaeo.wordpress.com/2013/01/04/> and J.M. Polanco-Martinez and J.L. Lopez-Martinez (2021) <doi:10.1016/j.ecoinf.2021.101379>.  "
  },
  {
    "id": 5355,
    "package_name": "NormalityAssessment",
    "title": "A Graphical User Interface for Testing Normality Visually",
    "description": "Package including an interactive Shiny application for\n    testing normality visually.",
    "version": "0.1.1",
    "maintainer": "Christopher Casement <casementc@gmail.com>",
    "author": "Christopher Casement [cre, aut],\n  Laura McSweeney [aut]",
    "url": "https://github.com/ccasement/NormalityAssessment,\nhttps://CRAN.R-project.org/package=NormalityAssessment",
    "bug_reports": "https://github.com/ccasement/NormalityAssessment/issues",
    "repository": "https://cran.r-project.org/package=NormalityAssessment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "NormalityAssessment A Graphical User Interface for Testing Normality Visually Package including an interactive Shiny application for\n    testing normality visually.  "
  },
  {
    "id": 5381,
    "package_name": "OLCPM",
    "title": "Online Change Point Detection for Matrix-Valued Time Series",
    "description": "We provide two algorithms for monitoring change points with online matrix-valued time series, under the assumption of a two-way factor structure. The algorithms are based on different calculations of the second moment matrices. One is based on stacking the columns of matrix observations, while another is by a more delicate projected approach. A well-known fact is that, in the presence of a change point, a factor model can be rewritten as a model with a larger number of common factors. In turn, this entails that, in the presence of a change point, the number of spiked eigenvalues in the second moment matrix of the data increases. Based on this, we propose two families of procedures - one based on the fluctuations of partial sums, and one based on extreme value theory - to monitor whether the first non-spiked eigenvalue diverges after a point in time in the monitoring horizon, thereby indicating the presence of a change point. This package also provides some simple functions for detecting and removing outliers, imputing missing entries and testing moments. See more details in He et al. (2021)<doi:10.48550/arXiv.2112.13479>.",
    "version": "0.1.2",
    "maintainer": "Long Yu <fduyulong@163.com>",
    "author": "Yong He [aut],\n  Xinbing Kong [aut],\n  Lorenzo Trapani [aut],\n  Long Yu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OLCPM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OLCPM Online Change Point Detection for Matrix-Valued Time Series We provide two algorithms for monitoring change points with online matrix-valued time series, under the assumption of a two-way factor structure. The algorithms are based on different calculations of the second moment matrices. One is based on stacking the columns of matrix observations, while another is by a more delicate projected approach. A well-known fact is that, in the presence of a change point, a factor model can be rewritten as a model with a larger number of common factors. In turn, this entails that, in the presence of a change point, the number of spiked eigenvalues in the second moment matrix of the data increases. Based on this, we propose two families of procedures - one based on the fluctuations of partial sums, and one based on extreme value theory - to monitor whether the first non-spiked eigenvalue diverges after a point in time in the monitoring horizon, thereby indicating the presence of a change point. This package also provides some simple functions for detecting and removing outliers, imputing missing entries and testing moments. See more details in He et al. (2021)<doi:10.48550/arXiv.2112.13479>.  "
  },
  {
    "id": 5390,
    "package_name": "OPI",
    "title": "Open Perimetry Interface",
    "description": "Implementation of the Open Perimetry Interface (OPI) for simulating and controlling visual field machines using R. The OPI is a standard for interfacing with visual field testing machines (perimeters) first started as an open source project with support of Haag-Streit in 2010. It specifies basic functions that allow many visual field tests to be constructed. As of February 2022 it is fully implemented on the Haag-Streit Octopus 900 and 'CrewT ImoVifa' ('Topcon Tempo') with partial implementations on the Centervue Compass, Kowa AP 7000 and Android phones. It also has a cousin: the R package 'visualFields', which has tools for analysing and manipulating visual field data.",
    "version": "3.0.5",
    "maintainer": "Andrew Turpin <andrew.turpin@lei.org.au>",
    "author": "Andrew Turpin [cre, aut, cph] (ORCID: 0000-0003-2559-8769),\n  David Lawson [ctb, cph],\n  Ivan Marin-Franch [ctb, cph],\n  Matthias Muller [ctb],\n  Jonathan Denniss [ctb, cph],\n  Astrid Zeman [ctb],\n  Giovanni Montesano [ctb]",
    "url": "https://opi.lei.org.au/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OPI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OPI Open Perimetry Interface Implementation of the Open Perimetry Interface (OPI) for simulating and controlling visual field machines using R. The OPI is a standard for interfacing with visual field testing machines (perimeters) first started as an open source project with support of Haag-Streit in 2010. It specifies basic functions that allow many visual field tests to be constructed. As of February 2022 it is fully implemented on the Haag-Streit Octopus 900 and 'CrewT ImoVifa' ('Topcon Tempo') with partial implementations on the Centervue Compass, Kowa AP 7000 and Android phones. It also has a cousin: the R package 'visualFields', which has tools for analysing and manipulating visual field data.  "
  },
  {
    "id": 5395,
    "package_name": "OPTtesting",
    "title": "Optimal Testing",
    "description": "Optimal testing under general dependence. The R package implements procedures proposed in Wang, Han, and Tong (2022). The package includes parameter estimation procedures, the computation for the posterior probabilities, and the testing procedure.",
    "version": "1.0.0",
    "maintainer": "Lijia Wang <lijiawan@usc.edu>",
    "author": "Lijia Wang [aut, cre, cph],\n  Xu Han [aut],\n  Xin Tong [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OPTtesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OPTtesting Optimal Testing Optimal testing under general dependence. The R package implements procedures proposed in Wang, Han, and Tong (2022). The package includes parameter estimation procedures, the computation for the posterior probabilities, and the testing procedure.  "
  },
  {
    "id": 5422,
    "package_name": "OasisR",
    "title": "Outright Tool for the Analysis of Spatial Inequalities and\nSegregation",
    "description": "A comprehensive set of indexes and tests for social segregation analysis,\n              as described in Tivadar (2019) - 'OasisR': An R Package to Bring Some Order\n              to the World of Segregation Measurement <doi:10.18637/jss.v089.i07>.\n              The package  is the most complete existing tool and it clarifies\n              many ambiguities and errors regarding the definition of segregation\n              indices. Additionally, 'OasisR' introduces several resampling methods\n              that enable testing their statistical significance\n              (randomization tests, bootstrapping, and jackknife methods).",
    "version": "3.1.1",
    "maintainer": "Mihai Tivadar <mihai.tivadar@inrae.fr>",
    "author": "Mihai Tivadar [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OasisR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OasisR Outright Tool for the Analysis of Spatial Inequalities and\nSegregation A comprehensive set of indexes and tests for social segregation analysis,\n              as described in Tivadar (2019) - 'OasisR': An R Package to Bring Some Order\n              to the World of Segregation Measurement <doi:10.18637/jss.v089.i07>.\n              The package  is the most complete existing tool and it clarifies\n              many ambiguities and errors regarding the definition of segregation\n              indices. Additionally, 'OasisR' introduces several resampling methods\n              that enable testing their statistical significance\n              (randomization tests, bootstrapping, and jackknife methods).  "
  },
  {
    "id": 5483,
    "package_name": "OptSig",
    "title": "Optimal Level of Significance for Regression and Other\nStatistical Tests",
    "description": "The optimal level of significance is calculated based on a decision-theoretic approach. The optimal level is chosen so that the expected loss from hypothesis testing is minimized. A range of statistical tests are covered, including the test for the population mean, population proportion, and a linear restriction in a multiple regression model. \n             The details are covered in Kim and Choi (2020) <doi:10.1111/abac.12172>, and Kim (2021) <doi:10.1080/00031305.2020.1750484>. ",
    "version": "2.2",
    "maintainer": "Jae H. Kim <jaekim8080@gmail.com>",
    "author": "Jae H. Kim <jaekim8080@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=OptSig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "OptSig Optimal Level of Significance for Regression and Other\nStatistical Tests The optimal level of significance is calculated based on a decision-theoretic approach. The optimal level is chosen so that the expected loss from hypothesis testing is minimized. A range of statistical tests are covered, including the test for the population mean, population proportion, and a linear restriction in a multiple regression model. \n             The details are covered in Kim and Choi (2020) <doi:10.1111/abac.12172>, and Kim (2021) <doi:10.1080/00031305.2020.1750484>.   "
  },
  {
    "id": 5541,
    "package_name": "PBSmodelling",
    "title": "GUI Tools Made Easy: Interact with Models and Explore Data",
    "description": "Provides software to facilitate the design, testing, and operation\n   of computer models. It focuses particularly on tools that make it easy to\n   construct and edit a customized graphical user interface ('GUI'). Although our\n   simplified 'GUI' language depends heavily on the R interface to the 'Tcl/Tk'\n   package, a user does not need to know 'Tcl/Tk'. Examples illustrate models\n   built with other R packages, including 'PBSmapping', 'PBSddesolve', and 'BRugs'. \n   A complete user's guide 'PBSmodelling-UG.pdf' shows how to use this package\n   effectively.",
    "version": "2.70.2",
    "maintainer": "Nick Fisch <nick.fisch@dfo-mpo.gc.ca>",
    "author": "Jon T. Schnute [aut],\n  Alex Couture-Beil [aut],\n  Rowan Haigh [aut],\n  Nicholas Boers [aut],\n  Anisa Egeli [aut],\n  A. R. Kronlund [ctb],\n  Steve Martell [ctb],\n  Norm Olsen [ctb],\n  Nick Fisch [cre]",
    "url": "https://github.com/pbs-software/pbs-modelling",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PBSmodelling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PBSmodelling GUI Tools Made Easy: Interact with Models and Explore Data Provides software to facilitate the design, testing, and operation\n   of computer models. It focuses particularly on tools that make it easy to\n   construct and edit a customized graphical user interface ('GUI'). Although our\n   simplified 'GUI' language depends heavily on the R interface to the 'Tcl/Tk'\n   package, a user does not need to know 'Tcl/Tk'. Examples illustrate models\n   built with other R packages, including 'PBSmapping', 'PBSddesolve', and 'BRugs'. \n   A complete user's guide 'PBSmodelling-UG.pdf' shows how to use this package\n   effectively.  "
  },
  {
    "id": 5551,
    "package_name": "PCGII",
    "title": "Partial Correlation Graph with Information Incorporation",
    "description": "Large-scale gene expression studies allow gene network construction to uncover associations among genes. This package is developed for estimating and testing partial correlation graphs with prior information incorporated. ",
    "version": "1.1.2",
    "maintainer": "Hao Wang <haydo.wang@outlook.com>",
    "author": "Hao Wang [aut, cre],\n  Yumou Qiu [aut],\n  Peng Liu [aut]",
    "url": "https://haowang47.github.io/PCGII/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PCGII",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PCGII Partial Correlation Graph with Information Incorporation Large-scale gene expression studies allow gene network construction to uncover associations among genes. This package is developed for estimating and testing partial correlation graphs with prior information incorporated.   "
  },
  {
    "id": 5575,
    "package_name": "PDtoolkit",
    "title": "Collection of Tools for PD Rating Model Development and\nValidation",
    "description": "The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.",
    "version": "1.2.0",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "author": "Andrija Djurovic [aut, cre]",
    "url": "https://github.com/andrija-djurovic/PDtoolkit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PDtoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PDtoolkit Collection of Tools for PD Rating Model Development and\nValidation The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.  "
  },
  {
    "id": 5577,
    "package_name": "PEGroupTesting",
    "title": "Population Proportion Estimation using Group Testing",
    "description": "The population proportion using group testing can be estimated by different methods. Four functions including p.mle(), p.gart(), p.burrow() and p.order() are provided to implement four estimating methods including the maximum likelihood estimate, Gart's estimate, Burrow's estimate, and order statistic estimate.   ",
    "version": "1.0",
    "maintainer": "Qingyang Zhang <qz008@uark.edu>",
    "author": "Qingyang Zhang, Yanchuan Li",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PEGroupTesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PEGroupTesting Population Proportion Estimation using Group Testing The population proportion using group testing can be estimated by different methods. Four functions including p.mle(), p.gart(), p.burrow() and p.order() are provided to implement four estimating methods including the maximum likelihood estimate, Gart's estimate, Burrow's estimate, and order statistic estimate.     "
  },
  {
    "id": 5581,
    "package_name": "PEPBVS",
    "title": "Bayesian Variable Selection using Power-Expected-Posterior Prior",
    "description": "Performs Bayesian variable selection under normal linear\n          models for the data with the model parameters following as prior distributions either \n          the power-expected-posterior (PEP) or the intrinsic (a special case of the former)\n          (Fouskakis and Ntzoufras (2022) <doi: 10.1214/21-BA1288>,\n          Fouskakis and Ntzoufras (2020) <doi: 10.3390/econometrics8020017>).          \n          The prior distribution on model space is the uniform over all models\n          or the uniform on model dimension (a special case of the beta-binomial prior). \n          The selection is performed by either implementing a full enumeration \n          and evaluation of all possible models or using the Markov Chain \n          Monte Carlo Model Composition (MC3) algorithm (Madigan and York (1995) <doi: 10.2307/1403615>). \n          Complementary functions for hypothesis testing, estimation and \n          predictions under Bayesian model averaging, as well as, plotting and \n          printing the results are also provided. The results can be compared to the\n          ones obtained under other well-known priors on model parameters and model spaces.",
    "version": "2.2",
    "maintainer": "Konstantina Charmpi <xarmpi.kon@gmail.com>",
    "author": "Konstantina Charmpi [aut, cre],\n  Dimitris Fouskakis [aut],\n  Ioannis Ntzoufras [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PEPBVS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PEPBVS Bayesian Variable Selection using Power-Expected-Posterior Prior Performs Bayesian variable selection under normal linear\n          models for the data with the model parameters following as prior distributions either \n          the power-expected-posterior (PEP) or the intrinsic (a special case of the former)\n          (Fouskakis and Ntzoufras (2022) <doi: 10.1214/21-BA1288>,\n          Fouskakis and Ntzoufras (2020) <doi: 10.3390/econometrics8020017>).          \n          The prior distribution on model space is the uniform over all models\n          or the uniform on model dimension (a special case of the beta-binomial prior). \n          The selection is performed by either implementing a full enumeration \n          and evaluation of all possible models or using the Markov Chain \n          Monte Carlo Model Composition (MC3) algorithm (Madigan and York (1995) <doi: 10.2307/1403615>). \n          Complementary functions for hypothesis testing, estimation and \n          predictions under Bayesian model averaging, as well as, plotting and \n          printing the results are also provided. The results can be compared to the\n          ones obtained under other well-known priors on model parameters and model spaces.  "
  },
  {
    "id": 5585,
    "package_name": "PEkit",
    "title": "Partition Exchangeability Toolkit",
    "description": "Bayesian supervised predictive classifiers, hypothesis testing, and parametric estimation under Partition Exchangeability are implemented. The two classifiers presented are the marginal classifier (that assumes test data is i.i.d.) next to a more computationally costly but accurate simultaneous classifier (that finds a labelling for the entire test dataset at once based on simultanous use of all the test data to predict each label). We also provide the Maximum Likelihood Estimation (MLE) of the only underlying parameter of the partition exchangeability generative model as well as hypothesis testing statistics for equality of this parameter with a single value, alternative, or multiple samples. We present functions to simulate the sequences from Ewens Sampling Formula as the realisation of the Poisson-Dirichlet distribution and their respective probabilities.",
    "version": "1.0.0.1000",
    "maintainer": "Ali Amiryousefi <ali.amiryousefi@helsinki.fi>",
    "author": "Ville Kinnula [aut],\n  Jing Tang [ctb] (ORCID: <https://orcid.org/0000-0001-7480-7710>),\n  Ali Amiryousefi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6317-3860>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PEkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PEkit Partition Exchangeability Toolkit Bayesian supervised predictive classifiers, hypothesis testing, and parametric estimation under Partition Exchangeability are implemented. The two classifiers presented are the marginal classifier (that assumes test data is i.i.d.) next to a more computationally costly but accurate simultaneous classifier (that finds a labelling for the entire test dataset at once based on simultanous use of all the test data to predict each label). We also provide the Maximum Likelihood Estimation (MLE) of the only underlying parameter of the partition exchangeability generative model as well as hypothesis testing statistics for equality of this parameter with a single value, alternative, or multiple samples. We present functions to simulate the sequences from Ewens Sampling Formula as the realisation of the Poisson-Dirichlet distribution and their respective probabilities.  "
  },
  {
    "id": 5618,
    "package_name": "PLIS",
    "title": "Multiplicity Control using Pooled LIS Statistic",
    "description": "A multiple testing procedure for testing several groups of \n        hypotheses is implemented. Linear dependency among the hypotheses \n        within the same group is modeled by using hidden Markov Models. \n        It is noted that a smaller p value does not necessarily imply \n        more significance due to the dependency. A typical application is \n        to analyze genome wide association studies datasets, where SNPs \n        from the same chromosome are treated as a group and exhibit \n        strong linear genomic dependency. See Wei Z, Sun W, Wang K, \n        Hakonarson H (2009) <doi:10.1093/bioinformatics/btp476> for more details.",
    "version": "1.2",
    "maintainer": "Zhi Wei <zhiwei04@gmail.com>",
    "author": "Zhi Wei & Wenguang Sun",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PLIS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PLIS Multiplicity Control using Pooled LIS Statistic A multiple testing procedure for testing several groups of \n        hypotheses is implemented. Linear dependency among the hypotheses \n        within the same group is modeled by using hidden Markov Models. \n        It is noted that a smaller p value does not necessarily imply \n        more significance due to the dependency. A typical application is \n        to analyze genome wide association studies datasets, where SNPs \n        from the same chromosome are treated as a group and exhibit \n        strong linear genomic dependency. See Wei Z, Sun W, Wang K, \n        Hakonarson H (2009) <doi:10.1093/bioinformatics/btp476> for more details.  "
  },
  {
    "id": 5634,
    "package_name": "PNAR",
    "title": "Poisson Network Autoregressive Models",
    "description": "Quasi likelihood-based methods for estimating linear and log-linear Poisson Network Autoregression models with p lags and covariates. Tools for testing the linearity versus several non-linear alternatives. Tools for simulation of multivariate count distributions, from linear and non-linear PNAR models, by using a specific copula construction. References include: Armillotta, M. and K. Fokianos (2023). \"Nonlinear network autoregression\". Annals of Statistics, 51(6): 2526--2552. <doi:10.1214/23-AOS2345>. Armillotta, M. and K. Fokianos (2024). \"Count network autoregression\". Journal of Time Series Analysis, 45(4): 584--612. <doi:10.1111/jtsa.12728>. Armillotta, M., Tsagris, M. and Fokianos, K. (2024). \"Inference for Network Count Time Series with the R Package PNAR\". The R Journal, 15/4: 255--269. <doi:10.32614/RJ-2023-094>.",
    "version": "1.7",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Mirko Armillotta [aut, cph],\n  Konstantinos Fokianos [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PNAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PNAR Poisson Network Autoregressive Models Quasi likelihood-based methods for estimating linear and log-linear Poisson Network Autoregression models with p lags and covariates. Tools for testing the linearity versus several non-linear alternatives. Tools for simulation of multivariate count distributions, from linear and non-linear PNAR models, by using a specific copula construction. References include: Armillotta, M. and K. Fokianos (2023). \"Nonlinear network autoregression\". Annals of Statistics, 51(6): 2526--2552. <doi:10.1214/23-AOS2345>. Armillotta, M. and K. Fokianos (2024). \"Count network autoregression\". Journal of Time Series Analysis, 45(4): 584--612. <doi:10.1111/jtsa.12728>. Armillotta, M., Tsagris, M. and Fokianos, K. (2024). \"Inference for Network Count Time Series with the R Package PNAR\". The R Journal, 15/4: 255--269. <doi:10.32614/RJ-2023-094>.  "
  },
  {
    "id": 5635,
    "package_name": "PNC",
    "title": "Phylogenetic Niche Conservatism Analysis for Ecological\nCommunities",
    "description": "Provides functions for testing phylogenetic niche conservatism, a key prerequisite in community assembly studies. The package integrates global functional trait data across major taxonomic groups and implements methods such as Pagel's Lambda and Blomberg's K to quantify phylogenetic signals in ecological communities. Methods are described in M\u00fcnkem\u00fcller et al. (2012) <doi:10.1111/j.2041-210X.2012.00196.x>.",
    "version": "0.1.0",
    "maintainer": "Yan He <heyan@njfu.edu.cn>",
    "author": "Yan He [aut, cre],\n  Yu Xia [aut],\n  Rui Yang [aut],\n  Lingfeng Mao [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PNC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PNC Phylogenetic Niche Conservatism Analysis for Ecological\nCommunities Provides functions for testing phylogenetic niche conservatism, a key prerequisite in community assembly studies. The package integrates global functional trait data across major taxonomic groups and implements methods such as Pagel's Lambda and Blomberg's K to quantify phylogenetic signals in ecological communities. Methods are described in M\u00fcnkem\u00fcller et al. (2012) <doi:10.1111/j.2041-210X.2012.00196.x>.  "
  },
  {
    "id": 5650,
    "package_name": "POSTm",
    "title": "Phylogeny-Guided OTU-Specific Association Test for Microbiome\nData",
    "description": "Implements the Phylogeny-Guided Microbiome OTU-Specific Association \n    Test method, which boosts the testing power by adaptively borrowing \n    information from phylogenetically close OTUs (operational taxonomic units)\n    of the target OTU. This method\n    is built on a kernel machine regression framework and allows for flexible \n    modeling of complex microbiome effects, adjustments for covariates, and \n    can accommodate both continuous and binary outcomes. ",
    "version": "1.4",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "author": "Caizhi Huang [aut],\n  Jung-Ying Tzeng [aut],\n  Shannon T. Holloway [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=POSTm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "POSTm Phylogeny-Guided OTU-Specific Association Test for Microbiome\nData Implements the Phylogeny-Guided Microbiome OTU-Specific Association \n    Test method, which boosts the testing power by adaptively borrowing \n    information from phylogenetically close OTUs (operational taxonomic units)\n    of the target OTU. This method\n    is built on a kernel machine regression framework and allows for flexible \n    modeling of complex microbiome effects, adjustments for covariates, and \n    can accommodate both continuous and binary outcomes.   "
  },
  {
    "id": 5655,
    "package_name": "PPCDT",
    "title": "An Optimal Subset Selection for Distributed Hypothesis Testing",
    "description": "In the era of big data, data redundancy and distributed characteristics present novel challenges to data analysis. This package introduces a method for estimating optimal subsets of redundant distributed data, based on PPCDT (Conjunction of Power and P-value in Distributed Settings). Leveraging PPC technology, this approach can efficiently extract valuable information from redundant distributed data and determine the optimal subset. Experimental results demonstrate that this method not only enhances data quality and utilization efficiency but also assesses its performance effectively. The philosophy of the package is described in Guo G. (2020) <doi:10.1007/s00180-020-00974-4>.",
    "version": "0.2.0",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "author": "Guangbao Guo [aut, cre, cph],\n  Jiarui Li [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PPCDT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PPCDT An Optimal Subset Selection for Distributed Hypothesis Testing In the era of big data, data redundancy and distributed characteristics present novel challenges to data analysis. This package introduces a method for estimating optimal subsets of redundant distributed data, based on PPCDT (Conjunction of Power and P-value in Distributed Settings). Leveraging PPC technology, this approach can efficiently extract valuable information from redundant distributed data and determine the optimal subset. Experimental results demonstrate that this method not only enhances data quality and utilization efficiency but also assesses its performance effectively. The philosophy of the package is described in Guo G. (2020) <doi:10.1007/s00180-020-00974-4>.  "
  },
  {
    "id": 5674,
    "package_name": "PRISM.forecast",
    "title": "Penalized Regression with Inferred Seasonality Module -\nForecasting Unemployment Initial Claims using 'Google Trends'\nData",
    "description": "Implements Penalized Regression with Inferred Seasonality Module (PRISM) to generate forecast estimation of weekly unemployment initial claims using 'Google Trends' data. It includes required data and tools for backtesting the performance in 2007-2020.",
    "version": "0.2.1",
    "maintainer": "Dingdong Yi <ryan.ddyi@gmail.com>",
    "author": "Dingdong Yi [aut, cre],\n  Samuel Kou [aut],\n  Shaoyang Ning [aut]",
    "url": "https://github.com/ryanddyi/prism",
    "bug_reports": "https://github.com/ryanddyi/prism/issues",
    "repository": "https://cran.r-project.org/package=PRISM.forecast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PRISM.forecast Penalized Regression with Inferred Seasonality Module -\nForecasting Unemployment Initial Claims using 'Google Trends'\nData Implements Penalized Regression with Inferred Seasonality Module (PRISM) to generate forecast estimation of weekly unemployment initial claims using 'Google Trends' data. It includes required data and tools for backtesting the performance in 2007-2020.  "
  },
  {
    "id": 5675,
    "package_name": "PRISMA",
    "title": "Protocol Inspection and State Machine Analysis",
    "description": "Loads and processes huge text\n    corpora processed with the sally toolbox (<http://www.mlsec.org/sally/>).\n    sally acts as a very fast preprocessor which splits the text files into\n    tokens or n-grams. These output files can then be read with the PRISMA\n    package which applies testing-based token selection and has some\n    replicate-aware, highly tuned non-negative matrix factorization and\n    principal component analysis implementation which allows the processing of\n    very big data sets even on desktop machines.",
    "version": "0.2-7",
    "maintainer": "Tammo Krueger <tammokrueger@googlemail.com>",
    "author": "Tammo Krueger, Nicole Kraemer",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PRISMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PRISMA Protocol Inspection and State Machine Analysis Loads and processes huge text\n    corpora processed with the sally toolbox (<http://www.mlsec.org/sally/>).\n    sally acts as a very fast preprocessor which splits the text files into\n    tokens or n-grams. These output files can then be read with the PRISMA\n    package which applies testing-based token selection and has some\n    replicate-aware, highly tuned non-negative matrix factorization and\n    principal component analysis implementation which allows the processing of\n    very big data sets even on desktop machines.  "
  },
  {
    "id": 5721,
    "package_name": "PUMP",
    "title": "Power Under Multiplicity Project",
    "description": "Estimates power, minimum detectable effect size (MDES) and sample size requirements. The context is multilevel randomized experiments with multiple outcomes. The estimation takes into account the use of multiple testing procedures. Development of this package was supported by a grant from the Institute of Education Sciences (R305D170030). For a full package description, including a detailed technical appendix, see <doi:10.18637/jss.v108.i06>.",
    "version": "1.0.4",
    "maintainer": "Luke Miratrix <luke_miratrix@gse.harvard.edu>",
    "author": "Luke Miratrix [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0078-1906>),\n  Kristen Hunter [aut] (ORCID: <https://orcid.org/0000-0002-5678-4620>),\n  Zarni Htet [aut],\n  Kristin Porter [aut],\n  MDRC [cph],\n  Institute of Education Sciences [fnd]",
    "url": "https://github.com/MDRCNY/PUMP, https://mdrcny.github.io/PUMP/",
    "bug_reports": "https://github.com/MDRCNY/PUMP/issues",
    "repository": "https://cran.r-project.org/package=PUMP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PUMP Power Under Multiplicity Project Estimates power, minimum detectable effect size (MDES) and sample size requirements. The context is multilevel randomized experiments with multiple outcomes. The estimation takes into account the use of multiple testing procedures. Development of this package was supported by a grant from the Institute of Education Sciences (R305D170030). For a full package description, including a detailed technical appendix, see <doi:10.18637/jss.v108.i06>.  "
  },
  {
    "id": 5747,
    "package_name": "PakPMICS2018",
    "title": "Multiple Indicator Cluster Survey (MICS) 2017-18 Data for\nPunjab, Pakistan",
    "description": "Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of SDG monitoring, as the survey produces information on 32 global SDG indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using Probability Proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household. ",
    "version": "1.2.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5923-1714>)",
    "url": "https://myaseen208.com/PakPMICS2018/\nhttps://CRAN.R-project.org/package=PakPMICS2018\nhttps://github.com/myaseen208/PakPMICS2018",
    "bug_reports": "https://github.com/myaseen208/PakPMICS2018/issues",
    "repository": "https://cran.r-project.org/package=PakPMICS2018",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PakPMICS2018 Multiple Indicator Cluster Survey (MICS) 2017-18 Data for\nPunjab, Pakistan Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of SDG monitoring, as the survey produces information on 32 global SDG indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using Probability Proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household.   "
  },
  {
    "id": 5748,
    "package_name": "PakPMICS2018bh",
    "title": "Multiple Indicator Cluster Survey (MICS) 2017-18 Birth History\nof Children Questionnaire Data for Punjab, Pakistan",
    "description": "Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Household questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of SDG monitoring, as the survey produces information on 32 global SDG indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using Probability Proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).",
    "version": "0.1.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre]\n      , Muhammad Usman [ctb]",
    "url": "https://github.com/myaseen208/PakPMICS2018bh,\nhttps://myaseen208.github.io/PakPMICS2018bh/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PakPMICS2018bh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PakPMICS2018bh Multiple Indicator Cluster Survey (MICS) 2017-18 Birth History\nof Children Questionnaire Data for Punjab, Pakistan Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Household questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of SDG monitoring, as the survey produces information on 32 global SDG indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using Probability Proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).  "
  },
  {
    "id": 5749,
    "package_name": "PakPMICS2018fs",
    "title": "Multiple Indicator Cluster Survey (MICS) 2017-18 Children Age\n5-17 Questionnaire Data for Punjab, Pakistan",
    "description": "Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Children Age 5-17 questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).",
    "version": "0.1.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre], Muhammad Usman [ctb]",
    "url": "https://github.com/myaseen208/PakPMICS2018fs,\nhttps://myaseen208.github.io/PakPMICS2018fs/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PakPMICS2018fs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PakPMICS2018fs Multiple Indicator Cluster Survey (MICS) 2017-18 Children Age\n5-17 Questionnaire Data for Punjab, Pakistan Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Children Age 5-17 questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).  "
  },
  {
    "id": 5750,
    "package_name": "PakPMICS2018hh",
    "title": "Multiple Indicator Cluster Survey (MICS) 2017-18 Household\nQuestionnaire Data for Punjab, Pakistan",
    "description": "Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Household questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).",
    "version": "0.1.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre],\n  Muhammad Usman [ctb]",
    "url": "https://github.com/myaseen208/PakPMICS2018hh,\nhttps://myaseen208.github.io/PakPMICS2018hh/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PakPMICS2018hh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PakPMICS2018hh Multiple Indicator Cluster Survey (MICS) 2017-18 Household\nQuestionnaire Data for Punjab, Pakistan Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Household questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).  "
  },
  {
    "id": 5751,
    "package_name": "PakPMICS2018mm",
    "title": "Multiple Indicator Cluster Survey (MICS) 2017-18 Maternal\nMortality Questionnaire Data for Punjab, Pakistan",
    "description": "Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Maternal Mortality questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).",
    "version": "0.1.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre],\n  Muhammad Usman [ctb]",
    "url": "https://github.com/myaseen208/PakPMICS2018mm,\nhttps://myaseen208.github.io/PakPMICS2018mm/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PakPMICS2018mm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PakPMICS2018mm Multiple Indicator Cluster Survey (MICS) 2017-18 Maternal\nMortality Questionnaire Data for Punjab, Pakistan Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Maternal Mortality questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).  "
  },
  {
    "id": 5752,
    "package_name": "PakPMICS2018mn",
    "title": "Multiple Indicator Cluster Survey (MICS) 2017-18 Men\nQuestionnaire Data for Punjab, Pakistan",
    "description": "Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Men questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).",
    "version": "0.1.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "author": "Muhammad Yaseen [aut, cre],\n  Muhammad Usman [ctb]",
    "url": "https://github.com/myaseen208/PakPMICS2018mn,\nhttps://myaseen208.github.io/PakPMICS2018mn/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PakPMICS2018mn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PakPMICS2018mn Multiple Indicator Cluster Survey (MICS) 2017-18 Men\nQuestionnaire Data for Punjab, Pakistan Provides data set and function for exploration of Multiple Indicator Cluster Survey (MICS) 2017-18 Men questionnaire data for Punjab, Pakistan. The results of the present survey are critically important for the purposes of Sustainable Development Goals (SDGs) monitoring, as the survey produces information on 32 global Sustainable Development Goals (SDGs) indicators. The data was collected from 53,840 households selected at the second stage with systematic random sampling out of a sample of 2,692 clusters selected using probability proportional to size sampling. Six questionnaires were used in the survey: (1) a household questionnaire to collect basic demographic information on all de jure household members (usual residents), the household, and the dwelling; (2) a water quality testing questionnaire administered in three households in each cluster of the sample; (3) a questionnaire for individual women administered in each household to all women age 15-49 years; (4) a questionnaire for individual men administered in every second household to all men age 15-49 years; (5) an under-5 questionnaire, administered to mothers (or caretakers) of all children under 5 living in the household; and (6) a questionnaire for children age 5-17 years, administered to the mother (or caretaker) of one randomly selected child age 5-17 years living in the household (<http://www.mics.unicef.org/surveys>).  "
  },
  {
    "id": 5771,
    "package_name": "Path.Analysis",
    "title": "Path Coefficient Analysis",
    "description": "Facilitates the performance of several analyses, including simple and sequential path coefficient analysis, correlation estimate, drawing correlogram, Heatmap, and path diagram. When working with raw data, that includes one or more dependent variables along with one or more independent variables are available, the path coefficient analysis can be conducted. It allows for testing direct effects, which can be a vital indicator in path coefficient analysis. The process of preparing the dataset rule is explained in detail in the vignette file \"Path.Analysis_manual.Rmd\". You can find this in the folders labelled \"data\" and \"~/inst/extdata\". Also see: 1)the 'lavaan', 2)a sample of sequential path analysis in 'metan' suggested by Olivoto and L\u00facio (2020) <doi:10.1111/2041-210X.13384>, 3)the simple 'PATHSAS' macro written in 'SAS' by Cramer et al. (1999) <doi:10.1093/jhered/90.1.260>, and 4)the semPlot() function of 'OpenMx' as initial tools for conducting path coefficient analyses and SEM (Structural Equation Modeling). To gain a comprehensive understanding of path coefficient analysis, both in theory and practice, see a 'Minitab' macro developed by Arminian, A. in the paper by Arminian et al. (2008) <doi:10.1080/15427520802043182>.",
    "version": "0.1",
    "maintainer": "Ali Arminian <abeyran@gmail.com>",
    "author": "Ali Arminian [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-4749-6085>)",
    "url": "https://github.com/abeyran/Path.Analysis",
    "bug_reports": "https://github.com/abeyran/Path.Analysis/issues",
    "repository": "https://cran.r-project.org/package=Path.Analysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Path.Analysis Path Coefficient Analysis Facilitates the performance of several analyses, including simple and sequential path coefficient analysis, correlation estimate, drawing correlogram, Heatmap, and path diagram. When working with raw data, that includes one or more dependent variables along with one or more independent variables are available, the path coefficient analysis can be conducted. It allows for testing direct effects, which can be a vital indicator in path coefficient analysis. The process of preparing the dataset rule is explained in detail in the vignette file \"Path.Analysis_manual.Rmd\". You can find this in the folders labelled \"data\" and \"~/inst/extdata\". Also see: 1)the 'lavaan', 2)a sample of sequential path analysis in 'metan' suggested by Olivoto and L\u00facio (2020) <doi:10.1111/2041-210X.13384>, 3)the simple 'PATHSAS' macro written in 'SAS' by Cramer et al. (1999) <doi:10.1093/jhered/90.1.260>, and 4)the semPlot() function of 'OpenMx' as initial tools for conducting path coefficient analyses and SEM (Structural Equation Modeling). To gain a comprehensive understanding of path coefficient analysis, both in theory and practice, see a 'Minitab' macro developed by Arminian, A. in the paper by Arminian et al. (2008) <doi:10.1080/15427520802043182>.  "
  },
  {
    "id": 5789,
    "package_name": "PenguinR",
    "title": "A Comprehensive Collection of Penguin Datasets for Statistical\nAnalysis and Experimental Design",
    "description": "Offers a comprehensive collection of penguin-related datasets suitable for descriptive statistics, hypothesis testing, and experimental design.\n    Derived from open ecological and biological sources such as Palmer Station studies, the package integrates datasets covering adult morphology, clutch size, blood isotope composition, and heart rate.\n    It is designed for researchers, students, and educators to explore statistical methods including ANOVA, regression, multivariate analysis, and design of experiments in an accessible and reproducible context.",
    "version": "0.1.0",
    "maintainer": "Juan Pablo Vargas Perez <j.pablovargas340@gmail.com>",
    "author": "Juan Pablo Vargas Perez [aut, cre]",
    "url": "https://github.com/jpablovargas340/PenguinR,\nhttps://jpablovargas340.github.io/PenguinR/",
    "bug_reports": "https://github.com/jpablovargas340/PenguinR/issues",
    "repository": "https://cran.r-project.org/package=PenguinR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PenguinR A Comprehensive Collection of Penguin Datasets for Statistical\nAnalysis and Experimental Design Offers a comprehensive collection of penguin-related datasets suitable for descriptive statistics, hypothesis testing, and experimental design.\n    Derived from open ecological and biological sources such as Palmer Station studies, the package integrates datasets covering adult morphology, clutch size, blood isotope composition, and heart rate.\n    It is designed for researchers, students, and educators to explore statistical methods including ANOVA, regression, multivariate analysis, and design of experiments in an accessible and reproducible context.  "
  },
  {
    "id": 5795,
    "package_name": "PerRegMod",
    "title": "Fitting Periodic Coefficients Linear Regression Models",
    "description": "Provides tools for fitting periodic coefficients regression models to data where periodicity plays a crucial role. It allows users to model and analyze relationships between variables that exhibit cyclical or seasonal patterns, offering functions for estimating parameters and testing the periodicity of coefficients in linear regression models. For simple periodic coefficient regression model see Regui et al. (2024) <doi:10.1080/03610918.2024.2314662>.",
    "version": "4.4.3",
    "maintainer": "Slimane Regui <slimaneregui111997@gmail.com>",
    "author": "Slimane Regui [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3696-1300>),\n  Abdelhadi Akharif [aut],\n  Amal Mellouk [aut]",
    "url": "https://doi.org/10.1080/03610918.2024.2314662",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PerRegMod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PerRegMod Fitting Periodic Coefficients Linear Regression Models Provides tools for fitting periodic coefficients regression models to data where periodicity plays a crucial role. It allows users to model and analyze relationships between variables that exhibit cyclical or seasonal patterns, offering functions for estimating parameters and testing the periodicity of coefficients in linear regression models. For simple periodic coefficient regression model see Regui et al. (2024) <doi:10.1080/03610918.2024.2314662>.  "
  },
  {
    "id": 5801,
    "package_name": "PermCor",
    "title": "Robust Permutation Tests of Correlation Coefficients",
    "description": "\n    Provides tools for statistical testing of correlation coefficients through robust \n    permutation method and large sample approximation method. Tailored to different types \n    of correlation coefficients including Pearson correlation coefficient, weighted \n    Pearson correlation coefficient, Spearman correlation coefficient, and Lin's \n    concordance correlation coefficient.The robust permutation test controls type I error\n    under general scenarios when sample size is small and two variables are dependent but \n    uncorrelated. The large sample approximation test generally controls type I error when\n    the sample size is large (>200).",
    "version": "0.1.0",
    "maintainer": "Mengyu Fang <mengyu.fang@roswellpark.org>",
    "author": "Mengyu Fang [aut, cre],\n  Han Yu [aut],\n  Alan Hutson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PermCor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PermCor Robust Permutation Tests of Correlation Coefficients \n    Provides tools for statistical testing of correlation coefficients through robust \n    permutation method and large sample approximation method. Tailored to different types \n    of correlation coefficients including Pearson correlation coefficient, weighted \n    Pearson correlation coefficient, Spearman correlation coefficient, and Lin's \n    concordance correlation coefficient.The robust permutation test controls type I error\n    under general scenarios when sample size is small and two variables are dependent but \n    uncorrelated. The large sample approximation test generally controls type I error when\n    the sample size is large (>200).  "
  },
  {
    "id": 5851,
    "package_name": "PlotNormTest",
    "title": "Graphical Univariate/Multivariate Assessments for Normality\nAssumption",
    "description": "Graphical methods testing multivariate normality assumption. Methods including assessing score function, and moment generating functions,independent transformations and linear transformations. For more details see Tran (2024),\"Contributions to Multivariate Data Science: Assessment and Identification of Multivariate Distributions and Supervised Learning for Groups of Objects.\" , PhD thesis,\n  <https://our.oakland.edu/items/c8942577-2562-4d2f-8677-cb8ec0bf6234>. ",
    "version": "1.0.1",
    "maintainer": "Huong Tran <quynhhuong5335@gmail.com>",
    "author": "Huong Tran [aut, cre] (ORCID: <https://orcid.org/0000-0003-3828-3862>),\n  Ravindra Khattree [aut] (ORCID:\n    <https://orcid.org/0000-0002-9305-2365>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PlotNormTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PlotNormTest Graphical Univariate/Multivariate Assessments for Normality\nAssumption Graphical methods testing multivariate normality assumption. Methods including assessing score function, and moment generating functions,independent transformations and linear transformations. For more details see Tran (2024),\"Contributions to Multivariate Data Science: Assessment and Identification of Multivariate Distributions and Supervised Learning for Groups of Objects.\" , PhD thesis,\n  <https://our.oakland.edu/items/c8942577-2562-4d2f-8677-cb8ec0bf6234>.   "
  },
  {
    "id": 5883,
    "package_name": "PoolBal",
    "title": "Balancing Central and Marginal Rejection of Pooled p-Values",
    "description": "When using pooled p-values to adjust for multiple testing, there is an inherent balance that must be struck between rejection based on weak evidence spread among many tests and strong evidence in a few, explored in Salahub and Olford (2023) <arXiv:2310.16600>. This package provides functionality to compute marginal and central rejection levels and the centrality quotient for p-value pooling functions and provides implementations of the chi-squared quantile pooled p-value (described in Salahub and Oldford (2023)) and a proposal from Heard and Rubin-Delanchy (2018) <doi:10.1093/biomet/asx076> to control the quotient's value.",
    "version": "0.1-0",
    "maintainer": "Chris Salahub <chris.salahub@uwaterloo.ca>",
    "author": "Chris Salahub [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PoolBal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PoolBal Balancing Central and Marginal Rejection of Pooled p-Values When using pooled p-values to adjust for multiple testing, there is an inherent balance that must be struck between rejection based on weak evidence spread among many tests and strong evidence in a few, explored in Salahub and Olford (2023) <arXiv:2310.16600>. This package provides functionality to compute marginal and central rejection levels and the centrality quotient for p-value pooling functions and provides implementations of the chi-squared quantile pooled p-value (described in Salahub and Oldford (2023)) and a proposal from Heard and Rubin-Delanchy (2018) <doi:10.1093/biomet/asx076> to control the quotient's value.  "
  },
  {
    "id": 5898,
    "package_name": "PortfolioTesteR",
    "title": "Test Investment Strategies with English-Like Code",
    "description": "Design, backtest, and analyze portfolio strategies using simple,\n    English-like function chains. Includes technical indicators, flexible stock\n    selection, portfolio construction methods (equal weighting, signal weighting,\n    inverse volatility, hierarchical risk parity), and a compact backtesting\n    engine for portfolio returns, drawdowns, and summary metrics.",
    "version": "0.1.4",
    "maintainer": "Alberto Pallotta <pallottaalberto@gmail.com>",
    "author": "Alberto Pallotta [aut, cre]",
    "url": "https://github.com/AlbertoPallotta/PortfolioTesteR",
    "bug_reports": "https://github.com/AlbertoPallotta/PortfolioTesteR/issues",
    "repository": "https://cran.r-project.org/package=PortfolioTesteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PortfolioTesteR Test Investment Strategies with English-Like Code Design, backtest, and analyze portfolio strategies using simple,\n    English-like function chains. Includes technical indicators, flexible stock\n    selection, portfolio construction methods (equal weighting, signal weighting,\n    inverse volatility, hierarchical risk parity), and a compact backtesting\n    engine for portfolio returns, drawdowns, and summary metrics.  "
  },
  {
    "id": 5947,
    "package_name": "ProjectTemplate",
    "title": "Automates the Creation of New Statistical Analysis Projects",
    "description": "Provides functions to\n    automatically build a directory structure for a new R\n    project. Using this structure, 'ProjectTemplate'\n    automates data loading, preprocessing, library\n    importing and unit testing.",
    "version": "0.11.1",
    "maintainer": "Kenton White <jkentonwhite@gmail.com>",
    "author": "Aleksandar Blagotic [ctb],\n  Diego Valle-Jones [ctb],\n  Jeffrey Breen [ctb],\n  Joakim Lundborg [ctb],\n  John Myles White [aut, cph],\n  Josh Bode [ctb],\n  Kenton White [ctb, cre],\n  Kirill Mueller [ctb],\n  Matteo Redaelli [ctb],\n  Noah Lorang [ctb],\n  Patrick Schalk [ctb],\n  Dominik Schneider [ctb],\n  Gerold Hepp [ctb],\n  Zunaira Jamil [ctb],\n  Glen Falk [ctb]",
    "url": "http://projecttemplate.net",
    "bug_reports": "https://github.com/KentonWhite/ProjectTemplate/issues",
    "repository": "https://cran.r-project.org/package=ProjectTemplate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ProjectTemplate Automates the Creation of New Statistical Analysis Projects Provides functions to\n    automatically build a directory structure for a new R\n    project. Using this structure, 'ProjectTemplate'\n    automates data loading, preprocessing, library\n    importing and unit testing.  "
  },
  {
    "id": 5977,
    "package_name": "PytrendsLongitudinalR",
    "title": "Create Longitudinal Google Trends Data",
    "description": "'Google Trends' provides cross-sectional and time-series data on searches, but lacks readily available \n    longitudinal data. Researchers, who want to create longitudinal 'Google Trends' on their own, face practical challenges, such as normalized counts that make it difficult to combine \n    cross-sectional and time-series data and limitations in data formats and timelines that limit data \n    granularity over extended time periods. \n    This package addresses these issues and enables researchers to generate longitudinal 'Google Trends' data.  \n    This package is built on 'pytrends', a Python library that acts as the unofficial 'Google Trends API' to collect 'Google Trends' data. As long as the 'Google Trends API', 'pytrends' and all their dependencies are working, this package will work.\n    During testing, we noticed that for the same input (keyword, topic, data_format, timeline), the output index can vary from time to time. Besides, if the keyword is not very popular, then the resulting dataset will contain a lot of zeros, which will greatly affect the final result. While this package has no control over the accuracy or quality of 'Google Trends' data, once the data is created, this package coverts it to longitudinal data.  \n    In addition, the user may encounter a 429 Too Many Requests error when using cross_section() and time_series() to collect 'Google Trends' data. This error indicates that the user has exceeded the rate limits set by the 'Google Trends API'. For more information about the 'Google Trends API' - 'pytrends', visit <https://pypi.org/project/pytrends/>.",
    "version": "0.1.4",
    "maintainer": "Taeyong Park <taeyongp@andrew.cmu.edu>",
    "author": "Taeyong Park [cre, cph, aut],\n  Malika Dixit [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PytrendsLongitudinalR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PytrendsLongitudinalR Create Longitudinal Google Trends Data 'Google Trends' provides cross-sectional and time-series data on searches, but lacks readily available \n    longitudinal data. Researchers, who want to create longitudinal 'Google Trends' on their own, face practical challenges, such as normalized counts that make it difficult to combine \n    cross-sectional and time-series data and limitations in data formats and timelines that limit data \n    granularity over extended time periods. \n    This package addresses these issues and enables researchers to generate longitudinal 'Google Trends' data.  \n    This package is built on 'pytrends', a Python library that acts as the unofficial 'Google Trends API' to collect 'Google Trends' data. As long as the 'Google Trends API', 'pytrends' and all their dependencies are working, this package will work.\n    During testing, we noticed that for the same input (keyword, topic, data_format, timeline), the output index can vary from time to time. Besides, if the keyword is not very popular, then the resulting dataset will contain a lot of zeros, which will greatly affect the final result. While this package has no control over the accuracy or quality of 'Google Trends' data, once the data is created, this package coverts it to longitudinal data.  \n    In addition, the user may encounter a 429 Too Many Requests error when using cross_section() and time_series() to collect 'Google Trends' data. This error indicates that the user has exceeded the rate limits set by the 'Google Trends API'. For more information about the 'Google Trends API' - 'pytrends', visit <https://pypi.org/project/pytrends/>.  "
  },
  {
    "id": 6004,
    "package_name": "QR.break",
    "title": "Structural Breaks in Quantile Regression",
    "description": "Methods for detecting structural breaks, determining the\n    number of breaks, and estimating break locations in linear quantile\n    regression, using one or multiple quantiles, based on Qu (2008) and\n    Oka and Qu (2011).  Applicable to both time series and repeated\n    cross-sectional data. The main function is rq.break().\n\n        References for detailed theoretical and empirical explanations:\n\n        (1) Qu, Z. (2008).  \"Testing for Structural Change in Regression\n    Quantiles.\"  Journal of Econometrics, 146(1), 170-184\n    <doi:10.1016/j.jeconom.2008.08.006>\n\n        (2) Oka, T., and Qu, Z. (2011).  \"Estimating Structural Changes in\n    Regression Quantiles.\"  Journal of Econometrics, 162(2), 248-267\n    <doi:10.1016/j.jeconom.2011.01.005>.",
    "version": "1.0.2",
    "maintainer": "Zhongjun Qu <qu@bu.edu>",
    "author": "Zhongjun Qu [aut, cre],\n  Tatsushi Oka [aut],\n  Samuel Messer [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QR.break",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QR.break Structural Breaks in Quantile Regression Methods for detecting structural breaks, determining the\n    number of breaks, and estimating break locations in linear quantile\n    regression, using one or multiple quantiles, based on Qu (2008) and\n    Oka and Qu (2011).  Applicable to both time series and repeated\n    cross-sectional data. The main function is rq.break().\n\n        References for detailed theoretical and empirical explanations:\n\n        (1) Qu, Z. (2008).  \"Testing for Structural Change in Regression\n    Quantiles.\"  Journal of Econometrics, 146(1), 170-184\n    <doi:10.1016/j.jeconom.2008.08.006>\n\n        (2) Oka, T., and Qu, Z. (2011).  \"Estimating Structural Changes in\n    Regression Quantiles.\"  Journal of Econometrics, 162(2), 248-267\n    <doi:10.1016/j.jeconom.2011.01.005>.  "
  },
  {
    "id": 6010,
    "package_name": "QTE.RD",
    "title": "Quantile Treatment Effects under the Regression Discontinuity\nDesign",
    "description": "Provides comprehensive methods for testing, estimating, and conducting uniform inference on quantile treatment effects (QTEs) in sharp regression discontinuity (RD) designs, incorporating covariates and implementing robust bias correction methods of Qu, Yoon, Perron (2024) <doi:10.1162/rest_a_01168>.",
    "version": "1.2.0",
    "maintainer": "Jungmo Yoon <jmyoon@hanyang.ac.kr>",
    "author": "Zhongjun Qu [aut, cph],\n  Jungmo Yoon [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QTE.RD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QTE.RD Quantile Treatment Effects under the Regression Discontinuity\nDesign Provides comprehensive methods for testing, estimating, and conducting uniform inference on quantile treatment effects (QTEs) in sharp regression discontinuity (RD) designs, incorporating covariates and implementing robust bias correction methods of Qu, Yoon, Perron (2024) <doi:10.1162/rest_a_01168>.  "
  },
  {
    "id": 6037,
    "package_name": "QuantPsyc",
    "title": "Quantitative Psychology Tools",
    "description": "Contains tools useful for data screening, testing\n        moderation (Cohen et. al. 2003)<doi:10.4324/9780203774441>, \n        mediation (MacKinnon et. al. 2002)<doi:10.1037/1082-989x.7.1.83> \n        and estimating power (Murphy & Myors 2014)<ISBN:9781315773155>.",
    "version": "1.6",
    "maintainer": "Thomas D. Fletcher <t.d.fletcher05@gmail.com>",
    "author": "Thomas D. Fletcher",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QuantPsyc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QuantPsyc Quantitative Psychology Tools Contains tools useful for data screening, testing\n        moderation (Cohen et. al. 2003)<doi:10.4324/9780203774441>, \n        mediation (MacKinnon et. al. 2002)<doi:10.1037/1082-989x.7.1.83> \n        and estimating power (Murphy & Myors 2014)<ISBN:9781315773155>.  "
  },
  {
    "id": 6098,
    "package_name": "RARfreq",
    "title": "Response Adaptive Randomization with 'Frequentist' Approaches",
    "description": "Provides functions and command-line user interface to generate allocation sequence \n             by response-adaptive randomization for clinical trials. The package currently supports \n             two families of frequentist response-adaptive randomization procedures, Doubly Adaptive \n             Biased Coin Design ('DBCD') and Sequential Estimation-adjusted Urn Model ('SEU'), for \n             binary and normal endpoints. One-sided proportion (or mean) difference and Chi-square \n             (or 'ANOVA') hypothesis testing methods are also available in the package to  facilitate \n             the inference for treatment effect. Additionally, the package provides comprehensive and \n             efficient tools to allow one to evaluate and  compare the performance of randomization \n             procedures and tests based on various criteria. For example, plots for relationship among \n             assumed treatment effects,  sample size, and power are provided. Five allocation functions \n             for 'DBCD' and six addition rule functions for 'SEU' are implemented to target allocations \n             such as 'Neyman', 'Rosenberger' Rosenberger et al. (2001) \n             <doi:10.1111/j.0006-341X.2001.00909.x> and 'Urn' allocations.",
    "version": "0.1.5",
    "maintainer": "Xiu Huang <xiu.huang@aya.yale.edu>",
    "author": "Mengjia Yu [aut],\n  Xiu Huang [aut, cre],\n  Li Wang [aut],\n  Hongjian Zhu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RARfreq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RARfreq Response Adaptive Randomization with 'Frequentist' Approaches Provides functions and command-line user interface to generate allocation sequence \n             by response-adaptive randomization for clinical trials. The package currently supports \n             two families of frequentist response-adaptive randomization procedures, Doubly Adaptive \n             Biased Coin Design ('DBCD') and Sequential Estimation-adjusted Urn Model ('SEU'), for \n             binary and normal endpoints. One-sided proportion (or mean) difference and Chi-square \n             (or 'ANOVA') hypothesis testing methods are also available in the package to  facilitate \n             the inference for treatment effect. Additionally, the package provides comprehensive and \n             efficient tools to allow one to evaluate and  compare the performance of randomization \n             procedures and tests based on various criteria. For example, plots for relationship among \n             assumed treatment effects,  sample size, and power are provided. Five allocation functions \n             for 'DBCD' and six addition rule functions for 'SEU' are implemented to target allocations \n             such as 'Neyman', 'Rosenberger' Rosenberger et al. (2001) \n             <doi:10.1111/j.0006-341X.2001.00909.x> and 'Urn' allocations.  "
  },
  {
    "id": 6101,
    "package_name": "RATest",
    "title": "Randomization Tests",
    "description": "A collection of randomization tests, data sets and examples. The current version focuses on five testing problems and their implementation in empirical work. First, it facilitates the empirical researcher to test for particular hypotheses, such as comparisons of means, medians, and variances from k populations using robust permutation tests, which asymptotic validity holds under very weak assumptions, while retaining the exact rejection probability in finite samples when the underlying distributions are identical. Second, the description and implementation of a permutation test for testing the continuity assumption of the baseline covariates in the sharp regression discontinuity design (RDD) as in Canay and Kamat (2018) <https://goo.gl/UZFqt7>. More specifically, it allows the user to select a set of covariates and test the aforementioned hypothesis using a permutation test based on the Cramer-von Misses test statistic. Graphical inspection of the empirical CDF and histograms for the variables of interest is also supported in the package. Third, it provides the practitioner with an effortless implementation of a permutation test based on the martingale decomposition of the empirical process for testing for heterogeneous treatment effects in the presence of an estimated nuisance parameter as in Chung and Olivares (2021) <doi:10.1016/j.jeconom.2020.09.015>. Fourth, this version considers the two-sample goodness-of-fit testing problem under covariate adaptive randomization and implements a permutation test based on a prepivoted Kolmogorov-Smirnov test statistic. Lastly, it implements an asymptotically valid permutation test based on the quantile process for the hypothesis of constant quantile treatment effects in the presence of an estimated nuisance parameter.",
    "version": "0.1.10",
    "maintainer": "Mauricio Olivares <mau.olivarego@gmail.com>",
    "author": "Mauricio Olivares [aut, cre],\n  Ignacio Sarmiento-Barbieri [aut]",
    "url": "",
    "bug_reports": "https://github.com/ignaciomsarmiento/RATest/issues",
    "repository": "https://cran.r-project.org/package=RATest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RATest Randomization Tests A collection of randomization tests, data sets and examples. The current version focuses on five testing problems and their implementation in empirical work. First, it facilitates the empirical researcher to test for particular hypotheses, such as comparisons of means, medians, and variances from k populations using robust permutation tests, which asymptotic validity holds under very weak assumptions, while retaining the exact rejection probability in finite samples when the underlying distributions are identical. Second, the description and implementation of a permutation test for testing the continuity assumption of the baseline covariates in the sharp regression discontinuity design (RDD) as in Canay and Kamat (2018) <https://goo.gl/UZFqt7>. More specifically, it allows the user to select a set of covariates and test the aforementioned hypothesis using a permutation test based on the Cramer-von Misses test statistic. Graphical inspection of the empirical CDF and histograms for the variables of interest is also supported in the package. Third, it provides the practitioner with an effortless implementation of a permutation test based on the martingale decomposition of the empirical process for testing for heterogeneous treatment effects in the presence of an estimated nuisance parameter as in Chung and Olivares (2021) <doi:10.1016/j.jeconom.2020.09.015>. Fourth, this version considers the two-sample goodness-of-fit testing problem under covariate adaptive randomization and implements a permutation test based on a prepivoted Kolmogorov-Smirnov test statistic. Lastly, it implements an asymptotically valid permutation test based on the quantile process for the hypothesis of constant quantile treatment effects in the presence of an estimated nuisance parameter.  "
  },
  {
    "id": 6114,
    "package_name": "RBtest",
    "title": "Regression-Based Approach for Testing the Type of Missing Data",
    "description": "The regression-based (RB) approach is a method to test the missing data mechanism.\n\t\t\tThis package contains two functions that test the type of missing data (Missing Completely \n\t\t\tAt Random vs Missing At Random) on the basis of the RB approach. The first function applies \n\t\t\tthe RB approach independently on each variable with missing data, using the completely \n\t\t\tobserved variables only. The second function tests the missing data mechanism globally \n\t\t\t(on all variables with missing data) with the use of all available information. The \n\t\t\talgorithm is adapted both to continuous and categorical data. ",
    "version": "1.1",
    "maintainer": "Serguei Rouzinov <rouzinovs@gmail.com>",
    "author": "Serguei Rouzinov and Andr\u00e9 Berchtold",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RBtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RBtest Regression-Based Approach for Testing the Type of Missing Data The regression-based (RB) approach is a method to test the missing data mechanism.\n\t\t\tThis package contains two functions that test the type of missing data (Missing Completely \n\t\t\tAt Random vs Missing At Random) on the basis of the RB approach. The first function applies \n\t\t\tthe RB approach independently on each variable with missing data, using the completely \n\t\t\tobserved variables only. The second function tests the missing data mechanism globally \n\t\t\t(on all variables with missing data) with the use of all available information. The \n\t\t\talgorithm is adapted both to continuous and categorical data.   "
  },
  {
    "id": 6183,
    "package_name": "RESET",
    "title": "Reconstruction Set Test",
    "description": "Contains logic for sample-level variable set scoring using randomized reduced rank reconstruction error. \n  Frost, H. Robert (2023) \"Reconstruction Set Test (RESET): a computationally efficient method for \n  single sample gene set testing based on randomized reduced rank reconstruction error\" <doi:10.1101/2023.04.03.535366>.",
    "version": "1.0.0",
    "maintainer": "H. Robert Frost <rob.frost@dartmouth.edu>",
    "author": "H. Robert Frost [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RESET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RESET Reconstruction Set Test Contains logic for sample-level variable set scoring using randomized reduced rank reconstruction error. \n  Frost, H. Robert (2023) \"Reconstruction Set Test (RESET): a computationally efficient method for \n  single sample gene set testing based on randomized reduced rank reconstruction error\" <doi:10.1101/2023.04.03.535366>.  "
  },
  {
    "id": 6244,
    "package_name": "RInSp",
    "title": "R Individual Specialization",
    "description": "Functions to calculate several ecological indices of individual \n    and population niche width (Araujo's E, clustering and pairwise similarity \n    among individuals, IS, Petraitis' W, and Roughgarden's WIC/TNW) to assess \n    individual specialization based on data of resource use. Resource use can \n    be quantified by counts of categories, measures of mass or length, or \n    proportions. Monte Carlo resampling procedures are available for hypothesis \n    testing against multinomial null models.\n    Details are provided in Zaccarelli et al. (2013) <doi:10.1111/2041-210X.12079>\n    and associated references.",
    "version": "1.2.5",
    "maintainer": "Dr. Nicola Zaccarelli <nicola.zaccarelli@gmail.com>",
    "author": "Dr. Nicola Zaccarelli [aut, cre],\n  Giorgio Mancinelli [aut],\n  Dan Bolnick [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RInSp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RInSp R Individual Specialization Functions to calculate several ecological indices of individual \n    and population niche width (Araujo's E, clustering and pairwise similarity \n    among individuals, IS, Petraitis' W, and Roughgarden's WIC/TNW) to assess \n    individual specialization based on data of resource use. Resource use can \n    be quantified by counts of categories, measures of mass or length, or \n    proportions. Monte Carlo resampling procedures are available for hypothesis \n    testing against multinomial null models.\n    Details are provided in Zaccarelli et al. (2013) <doi:10.1111/2041-210X.12079>\n    and associated references.  "
  },
  {
    "id": 6252,
    "package_name": "RJafroc",
    "title": "Artificial Intelligence Systems and Observer Performance",
    "description": "Analyzing the performance of artificial intelligence\n (AI) systems/algorithms characterized by a 'search-and-report'\n strategy. Historically observer performance has dealt with\n measuring radiologists' performances in search tasks, e.g., searching\n for lesions in medical images and reporting them, but the implicit\n location information has been ignored. The implemented methods apply\n to analyzing the absolute and relative performances of AI systems,\n comparing AI performance to a group of human readers or optimizing the\n reporting threshold of an AI system. In addition to performing historical\n receiver operating receiver operating characteristic (ROC) analysis\n (localization information ignored), the software also performs\n free-response receiver operating characteristic (FROC)\n analysis, where lesion localization information is used. A book\n using the software has been published: Chakraborty DP: Observer\n Performance Methods for Diagnostic Imaging - Foundations, Modeling,\n and Applications with R-Based Examples, Taylor-Francis LLC; 2017:\n <https://www.routledge.com/Observer-Performance-Methods-for-Diagnostic-Imaging-Foundations-Modeling/Chakraborty/p/book/9781482214840>.\n Online updates to this book, which use the software, are at\n <https://dpc10ster.github.io/RJafrocQuickStart/>,\n <https://dpc10ster.github.io/RJafrocRocBook/> and at\n <https://dpc10ster.github.io/RJafrocFrocBook/>. Supported data\n collection paradigms are the ROC, FROC and the location ROC (LROC).\n ROC data consists of single ratings per images, where a rating is\n the perceived confidence level that the image is that of a diseased\n patient. An ROC curve is a plot of true positive fraction vs. false\n positive fraction. FROC data consists of a variable number (zero or\n more) of mark-rating pairs per image, where a mark is the location\n of a reported suspicious region and the rating is the confidence\n level that it is a real lesion. LROC data consists of a rating and a\n location of the most suspicious region, for every image. Four models\n of observer performance, and curve-fitting software, are implemented:\n the binormal model (BM), the contaminated binormal model (CBM), the\n correlated contaminated binormal model (CORCBM), and the radiological\n search model (RSM). Unlike the binormal model, CBM, CORCBM and RSM\n predict 'proper' ROC curves that do not inappropriately cross the\n chance diagonal. Additionally, RSM parameters are related to search\n performance (not measured in conventional ROC analysis) and\n classification performance. Search performance refers to finding\n lesions, i.e., true positives, while simultaneously not finding false\n positive locations. Classification performance measures the ability to\n distinguish between true and false positive locations. Knowing these\n separate performances allows principled optimization of reader or AI\n system performance. This package supersedes Windows JAFROC (jackknife\n alternative FROC) software V4.2.1,\n <https://github.com/dpc10ster/WindowsJafroc>. Package functions are\n organized as follows. Data file related function names are preceded\n by 'Df', curve fitting functions by 'Fit', included data sets by 'dataset',\n plotting functions by 'Plot', significance testing functions by 'St',\n sample size related functions by 'Ss', data simulation functions by\n 'Simulate' and utility functions by 'Util'. Implemented are figures of\n merit (FOMs) for quantifying performance and functions for visualizing\n empirical or fitted operating characteristics: e.g., ROC, FROC, alternative\n FROC (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study\n designs significance testing of reader-averaged FOM differences between\n modalities is implemented via either Dorfman-Berbaum-Metz or the\n Obuchowski-Rockette methods. Also implemented is single treatment analysis,\n which allows comparison of performance of a group of radiologists to a\n specified value, or comparison of AI to a group of radiologists interpreting\n the same cases. Crossed-modality analysis is implemented wherein there are\n two crossed treatment factors and the aim is to determined performance in\n each treatment factor averaged over all levels of the second factor. Sample\n size estimation tools are provided for ROC and FROC studies; these use\n estimates of the relevant variances from a pilot study to predict required\n numbers of readers and cases in a pivotal study to achieve the desired power.\n Utility and data file manipulation functions allow data to be read in any of\n the currently used input formats, including Excel, and the results of the\n analysis can be viewed in text or Excel output files. The methods are\n illustrated with several included datasets from the author's collaborations.\n This update includes improvements to the code, some as a result of\n user-reported bugs and new feature requests, and others discovered during\n ongoing testing and code simplification.",
    "version": "2.1.2",
    "maintainer": "Dev Chakraborty <dpc10ster@gmail.com>",
    "author": "Dev Chakraborty [cre, aut, cph],\n  Peter Phillips [ctb],\n  Xuetong Zhai [aut]",
    "url": "https://dpc10ster.github.io/RJafroc/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RJafroc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RJafroc Artificial Intelligence Systems and Observer Performance Analyzing the performance of artificial intelligence\n (AI) systems/algorithms characterized by a 'search-and-report'\n strategy. Historically observer performance has dealt with\n measuring radiologists' performances in search tasks, e.g., searching\n for lesions in medical images and reporting them, but the implicit\n location information has been ignored. The implemented methods apply\n to analyzing the absolute and relative performances of AI systems,\n comparing AI performance to a group of human readers or optimizing the\n reporting threshold of an AI system. In addition to performing historical\n receiver operating receiver operating characteristic (ROC) analysis\n (localization information ignored), the software also performs\n free-response receiver operating characteristic (FROC)\n analysis, where lesion localization information is used. A book\n using the software has been published: Chakraborty DP: Observer\n Performance Methods for Diagnostic Imaging - Foundations, Modeling,\n and Applications with R-Based Examples, Taylor-Francis LLC; 2017:\n <https://www.routledge.com/Observer-Performance-Methods-for-Diagnostic-Imaging-Foundations-Modeling/Chakraborty/p/book/9781482214840>.\n Online updates to this book, which use the software, are at\n <https://dpc10ster.github.io/RJafrocQuickStart/>,\n <https://dpc10ster.github.io/RJafrocRocBook/> and at\n <https://dpc10ster.github.io/RJafrocFrocBook/>. Supported data\n collection paradigms are the ROC, FROC and the location ROC (LROC).\n ROC data consists of single ratings per images, where a rating is\n the perceived confidence level that the image is that of a diseased\n patient. An ROC curve is a plot of true positive fraction vs. false\n positive fraction. FROC data consists of a variable number (zero or\n more) of mark-rating pairs per image, where a mark is the location\n of a reported suspicious region and the rating is the confidence\n level that it is a real lesion. LROC data consists of a rating and a\n location of the most suspicious region, for every image. Four models\n of observer performance, and curve-fitting software, are implemented:\n the binormal model (BM), the contaminated binormal model (CBM), the\n correlated contaminated binormal model (CORCBM), and the radiological\n search model (RSM). Unlike the binormal model, CBM, CORCBM and RSM\n predict 'proper' ROC curves that do not inappropriately cross the\n chance diagonal. Additionally, RSM parameters are related to search\n performance (not measured in conventional ROC analysis) and\n classification performance. Search performance refers to finding\n lesions, i.e., true positives, while simultaneously not finding false\n positive locations. Classification performance measures the ability to\n distinguish between true and false positive locations. Knowing these\n separate performances allows principled optimization of reader or AI\n system performance. This package supersedes Windows JAFROC (jackknife\n alternative FROC) software V4.2.1,\n <https://github.com/dpc10ster/WindowsJafroc>. Package functions are\n organized as follows. Data file related function names are preceded\n by 'Df', curve fitting functions by 'Fit', included data sets by 'dataset',\n plotting functions by 'Plot', significance testing functions by 'St',\n sample size related functions by 'Ss', data simulation functions by\n 'Simulate' and utility functions by 'Util'. Implemented are figures of\n merit (FOMs) for quantifying performance and functions for visualizing\n empirical or fitted operating characteristics: e.g., ROC, FROC, alternative\n FROC (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study\n designs significance testing of reader-averaged FOM differences between\n modalities is implemented via either Dorfman-Berbaum-Metz or the\n Obuchowski-Rockette methods. Also implemented is single treatment analysis,\n which allows comparison of performance of a group of radiologists to a\n specified value, or comparison of AI to a group of radiologists interpreting\n the same cases. Crossed-modality analysis is implemented wherein there are\n two crossed treatment factors and the aim is to determined performance in\n each treatment factor averaged over all levels of the second factor. Sample\n size estimation tools are provided for ROC and FROC studies; these use\n estimates of the relevant variances from a pilot study to predict required\n numbers of readers and cases in a pivotal study to achieve the desired power.\n Utility and data file manipulation functions allow data to be read in any of\n the currently used input formats, including Excel, and the results of the\n analysis can be viewed in text or Excel output files. The methods are\n illustrated with several included datasets from the author's collaborations.\n This update includes improvements to the code, some as a result of\n user-reported bugs and new feature requests, and others discovered during\n ongoing testing and code simplification.  "
  },
  {
    "id": 6262,
    "package_name": "RLRsim",
    "title": "Exact (Restricted) Likelihood Ratio Tests for Mixed and Additive\nModels",
    "description": "Rapid, simulation-based exact (restricted) likelihood ratio tests\n    for testing the presence of variance components/nonparametric terms for\n    models fit with nlme::lme(),lme4::lmer(), lmerTest::lmer(), gamm4::gamm4() and\n    mgcv::gamm().",
    "version": "3.1-9",
    "maintainer": "Fabian Scheipl <fabian.scheipl@stat.uni-muenchen.de>",
    "author": "Fabian Scheipl [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8172-3603>),\n  Ben Bolker [ctb] (ORCID: <https://orcid.org/0000-0002-2127-0443>)",
    "url": "https://github.com/fabian-s/RLRsim",
    "bug_reports": "https://github.com/fabian-s/RLRsim/issues",
    "repository": "https://cran.r-project.org/package=RLRsim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RLRsim Exact (Restricted) Likelihood Ratio Tests for Mixed and Additive\nModels Rapid, simulation-based exact (restricted) likelihood ratio tests\n    for testing the presence of variance components/nonparametric terms for\n    models fit with nlme::lme(),lme4::lmer(), lmerTest::lmer(), gamm4::gamm4() and\n    mgcv::gamm().  "
  },
  {
    "id": 6298,
    "package_name": "RMediation",
    "title": "Mediation Analysis Confidence Intervals",
    "description": "Computes confidence intervals for nonlinear functions of model \n    parameters (e.g., product of k coefficients) in single-level and multilevel \n    structural equation models. Methods include the distribution of the product, \n    Monte Carlo simulation, and bootstrap methods. It also performs the Model-Based \n    Constrained Optimization (MBCO) procedure for hypothesis testing of indirect \n    effects.\n    References:\n    Tofighi, D., and MacKinnon, D. P. (2011). RMediation: An R package for mediation \n    analysis confidence intervals. Behavior Research Methods, 43, 692-700. \n    <doi:10.3758/s13428-011-0076-x>;\n    Tofighi, D., and Kelley, K. (2020). Improved inference in mediation analysis: Introducing the model-based constrained optimization procedure. \n    Psychological Methods, 25(4), 496-515. <doi:10.1037/met0000259>;\n    Tofighi, D. (2020). Bootstrap Model-Based Constrained Optimization Tests of \n    Indirect Effects. Frontiers in Psychology, 10, 2989. \n    <doi:10.3389/fpsyg.2019.02989>.",
    "version": "1.3.0",
    "maintainer": "Davood Tofighi <dtofighi@gmail.com>",
    "author": "Davood Tofighi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8523-7776>)",
    "url": "https://data-wise.github.io/rmediation/,\nhttps://github.com/data-wise/rmediation",
    "bug_reports": "https://github.com/data-wise/rmediation/issues",
    "repository": "https://cran.r-project.org/package=RMediation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RMediation Mediation Analysis Confidence Intervals Computes confidence intervals for nonlinear functions of model \n    parameters (e.g., product of k coefficients) in single-level and multilevel \n    structural equation models. Methods include the distribution of the product, \n    Monte Carlo simulation, and bootstrap methods. It also performs the Model-Based \n    Constrained Optimization (MBCO) procedure for hypothesis testing of indirect \n    effects.\n    References:\n    Tofighi, D., and MacKinnon, D. P. (2011). RMediation: An R package for mediation \n    analysis confidence intervals. Behavior Research Methods, 43, 692-700. \n    <doi:10.3758/s13428-011-0076-x>;\n    Tofighi, D., and Kelley, K. (2020). Improved inference in mediation analysis: Introducing the model-based constrained optimization procedure. \n    Psychological Methods, 25(4), 496-515. <doi:10.1037/met0000259>;\n    Tofighi, D. (2020). Bootstrap Model-Based Constrained Optimization Tests of \n    Indirect Effects. Frontiers in Psychology, 10, 2989. \n    <doi:10.3389/fpsyg.2019.02989>.  "
  },
  {
    "id": 6311,
    "package_name": "RNOmni",
    "title": "Rank Normal Transformation Omnibus Test",
    "description": "Inverse normal transformation (INT) based genetic association testing. These tests are recommend for continuous traits with non-normally distributed residuals. INT-based tests robustly control the type I error in settings where standard linear regression does not, as when the residual distribution exhibits excess skew or kurtosis. Moreover, INT-based tests outperform standard linear regression in terms of power. These tests may be classified into two types. In direct INT (D-INT), the phenotype is itself transformed. In indirect INT (I-INT), phenotypic residuals are transformed. The omnibus test (O-INT) adaptively combines D-INT and I-INT into a single robust and statistically powerful approach. See McCaw ZR, Lane JM, Saxena R, Redline S, Lin X. \"Operating characteristics of the rank-based inverse normal transformation for quantitative trait analysis in genome-wide association studies\" <doi:10.1111/biom.13214>.",
    "version": "1.0.1.2",
    "maintainer": "Zachary McCaw <zmccaw@alumni.harvard.edu>",
    "author": "Zachary McCaw [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2006-9828>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RNOmni",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RNOmni Rank Normal Transformation Omnibus Test Inverse normal transformation (INT) based genetic association testing. These tests are recommend for continuous traits with non-normally distributed residuals. INT-based tests robustly control the type I error in settings where standard linear regression does not, as when the residual distribution exhibits excess skew or kurtosis. Moreover, INT-based tests outperform standard linear regression in terms of power. These tests may be classified into two types. In direct INT (D-INT), the phenotype is itself transformed. In indirect INT (I-INT), phenotypic residuals are transformed. The omnibus test (O-INT) adaptively combines D-INT and I-INT into a single robust and statistically powerful approach. See McCaw ZR, Lane JM, Saxena R, Redline S, Lin X. \"Operating characteristics of the rank-based inverse normal transformation for quantitative trait analysis in genome-wide association studies\" <doi:10.1111/biom.13214>.  "
  },
  {
    "id": 6335,
    "package_name": "ROKET",
    "title": "Optimal Transport-Based Kernel Regression",
    "description": "Perform optimal transport on somatic point mutations and kernel regression hypothesis testing by integrating pathway level similarities at the gene level (Little et al. (2023) <doi:10.1111/biom.13769>). The software implements balanced and unbalanced optimal transport and omnibus tests with 'C++' across a set of tumor samples and allows for multi-threading to decrease computational runtime.",
    "version": "1.0.0",
    "maintainer": "Paul Little <pllittle321@gmail.com>",
    "author": "Paul Little [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ROKET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ROKET Optimal Transport-Based Kernel Regression Perform optimal transport on somatic point mutations and kernel regression hypothesis testing by integrating pathway level similarities at the gene level (Little et al. (2023) <doi:10.1111/biom.13769>). The software implements balanced and unbalanced optimal transport and omnibus tests with 'C++' across a set of tumor samples and allows for multi-threading to decrease computational runtime.  "
  },
  {
    "id": 6374,
    "package_name": "RRI",
    "title": "Residual Randomization Inference for Regression Models",
    "description": "Testing and inference for regression models using residual randomization methods. The basis of inference is an invariance assumption on the regression errors, e.g., clustered errors, or doubly-clustered errors.",
    "version": "1.1",
    "maintainer": "Panos Toulis <panos.toulis@chicagobooth.edu>",
    "author": "Panos Toulis",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RRI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RRI Residual Randomization Inference for Regression Models Testing and inference for regression models using residual randomization methods. The basis of inference is an invariance assumption on the regression errors, e.g., clustered errors, or doubly-clustered errors.  "
  },
  {
    "id": 6388,
    "package_name": "RSAtools",
    "title": "Advanced Response Surface Analysis",
    "description": "Provides tools for response surface analysis, using a comparative framework that identifies best-fitting solutions across 37 families of polynomials. Many of these tools are based upon and extend the 'RSA' package, by testing a larger scope of polynomials (+27 families), more diverse response surface probing techniques (+acceleration points), more plots (+line of congruence, +line of incongruence, both with extrema), and other useful functions for exporting results.",
    "version": "0.1.2",
    "maintainer": "Fernando N\u00fa\u00f1ez-Regueiro <fernando.nr.france@gmail.com>",
    "author": "Fernando N\u00fa\u00f1ez-Regueiro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4784-2021>),\n  Jacques Juhel [aut] (ORCID: <https://orcid.org/0000-0002-3520-6012>),\n  Felix Sch\u00f6nbrodt [ctb] (ORCID: <https://orcid.org/0000-0002-8282-3910>),\n  Sarah Humberg [ctb] (ORCID: <https://orcid.org/0000-0002-7891-3622>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSAtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSAtools Advanced Response Surface Analysis Provides tools for response surface analysis, using a comparative framework that identifies best-fitting solutions across 37 families of polynomials. Many of these tools are based upon and extend the 'RSA' package, by testing a larger scope of polynomials (+27 families), more diverse response surface probing techniques (+acceleration points), more plots (+line of congruence, +line of incongruence, both with extrema), and other useful functions for exporting results.  "
  },
  {
    "id": 6391,
    "package_name": "RSCAT",
    "title": "Shadow-Test Approach to Computerized Adaptive Testing",
    "description": "As an advanced approach to computerized adaptive testing (CAT), \n  shadow testing (van der Linden(2005) <doi:10.1007/0-387-29054-0>) dynamically \n  assembles entire shadow tests as a part of \n  selecting items throughout the testing process.\n  Selecting items from shadow tests guarantees the compliance of all content \n  constraints defined by the blueprint. 'RSCAT' is an R package for the \n  shadow-test approach to CAT. The objective of \n  'RSCAT' is twofold: 1) Enhancing the effectiveness of shadow-test CAT simulation;\n  2) Contributing to the academic and scientific community for CAT research.\n  RSCAT is currently designed for dichotomous items based on the three-parameter logistic (3PL) model.",
    "version": "1.1.3",
    "maintainer": "Bingnan Jiang <bnjiangece@gmail.com>",
    "author": "Bingnan Jiang [aut, cre],\n  ACT, Inc. [cph]",
    "url": "",
    "bug_reports": "https://github.com/act-org/RSCAT/issues",
    "repository": "https://cran.r-project.org/package=RSCAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSCAT Shadow-Test Approach to Computerized Adaptive Testing As an advanced approach to computerized adaptive testing (CAT), \n  shadow testing (van der Linden(2005) <doi:10.1007/0-387-29054-0>) dynamically \n  assembles entire shadow tests as a part of \n  selecting items throughout the testing process.\n  Selecting items from shadow tests guarantees the compliance of all content \n  constraints defined by the blueprint. 'RSCAT' is an R package for the \n  shadow-test approach to CAT. The objective of \n  'RSCAT' is twofold: 1) Enhancing the effectiveness of shadow-test CAT simulation;\n  2) Contributing to the academic and scientific community for CAT research.\n  RSCAT is currently designed for dichotomous items based on the three-parameter logistic (3PL) model.  "
  },
  {
    "id": 6392,
    "package_name": "RSD",
    "title": "Compares Random Distributions using Stochastic Dominance",
    "description": "The Stochastic Dominance (SD) is the classical way of comparing two\n    random prospects, using their distribution functions. Almost Stochastic\n    Dominance (ASD) has also been developed to cover the SD failures due to \n    the extreme utility functions. This package focuses on classical and heuristic methods\n    for testing the first and second SD and ASD methods given the probability mass\n    function (PMF) of the random prospects. The goal is to apply these methods\n    easily, efficiently, and effectively on real-world datasets. For more\n    details see Hanoch and Levy (1969) <doi:10.2307/2296431>, Leshno and Levy\n    (2002) <doi:10.1287/mnsc.48.8.1074.169>, and Tzeng et al. (2012)\n    <doi:10.1287/mnsc.1120.1616>.",
    "version": "0.2.0",
    "maintainer": "Shayan Tohidi <shayant@iastate.edu>",
    "author": "Shayan Tohidi [aut, cre],\n  Sigurdur Olafsson [aut]",
    "url": "https://github.com/ShayanTohidi/RSD",
    "bug_reports": "https://github.com/ShayanTohidi/RSD/issues",
    "repository": "https://cran.r-project.org/package=RSD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSD Compares Random Distributions using Stochastic Dominance The Stochastic Dominance (SD) is the classical way of comparing two\n    random prospects, using their distribution functions. Almost Stochastic\n    Dominance (ASD) has also been developed to cover the SD failures due to \n    the extreme utility functions. This package focuses on classical and heuristic methods\n    for testing the first and second SD and ASD methods given the probability mass\n    function (PMF) of the random prospects. The goal is to apply these methods\n    easily, efficiently, and effectively on real-world datasets. For more\n    details see Hanoch and Levy (1969) <doi:10.2307/2296431>, Leshno and Levy\n    (2002) <doi:10.1287/mnsc.48.8.1074.169>, and Tzeng et al. (2012)\n    <doi:10.1287/mnsc.1120.1616>.  "
  },
  {
    "id": 6406,
    "package_name": "RSStest",
    "title": "Testing the Equality of Two Means Using RSS and MRSS",
    "description": "Testing the equality of two means using Ranked Set Sampling\n             and Median Ranked Set Sampling are provided under normal distribution. Data generation functions are also given RSS and MRSS. \n             Also, data generation functions are given under imperfect ranking data for Ranked Set Sampling and Median Ranked Set Sampling.\n             Ozdemir Y.A., Ebegil M., & Gokpinar F. (2019), <doi:10.1007/s40995-018-0558-0>\n             Ozdemir Y.A., Ebegil M., & Gokpinar F. (2017), <doi:10.1080/03610918.2016.1263736>.",
    "version": "1.0",
    "maintainer": "Fikri G\u00f6kp\u0131nar <fikri@gazi.edu.tr>",
    "author": "Fikri G\u00f6kp\u0131nar [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6310-8727>),\n  Meral Ebegil [aut] (ORCID: <https://orcid.org/0000-0003-4798-3422>),\n  Yaprak Arzu \u00d6zdemir [aut] (ORCID:\n    <https://orcid.org/0000-0003-3752-9744>),\n  Esra G\u00f6kp\u0131nar [aut] (ORCID: <https://orcid.org/0000-0003-2148-4940>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSStest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSStest Testing the Equality of Two Means Using RSS and MRSS Testing the equality of two means using Ranked Set Sampling\n             and Median Ranked Set Sampling are provided under normal distribution. Data generation functions are also given RSS and MRSS. \n             Also, data generation functions are given under imperfect ranking data for Ranked Set Sampling and Median Ranked Set Sampling.\n             Ozdemir Y.A., Ebegil M., & Gokpinar F. (2019), <doi:10.1007/s40995-018-0558-0>\n             Ozdemir Y.A., Ebegil M., & Gokpinar F. (2017), <doi:10.1080/03610918.2016.1263736>.  "
  },
  {
    "id": 6411,
    "package_name": "RSiena",
    "title": "Siena - Simulation Investigation for Empirical Network Analysis",
    "description": "The main purpose of this package is to perform simulation-based\n   estimation of stochastic actor-oriented models for longitudinal network\n   data collected as panel data. Dependent variables can be single or\n   multivariate networks, which can be directed, non-directed, or two-mode;\n   and associated actor variables.\n   There are also functions for testing parameters and checking goodness of fit.\n   An overview of these models is given in Snijders (2017),\n   <doi:10.1146/annurev-statistics-060116-054035>.",
    "version": "1.5.0",
    "maintainer": "Christian Steglich <c.e.g.steglich@rug.nl>",
    "author": "Tom A.B. Snijders [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-3157-4157>),\n  Ruth M. Ripley [aut],\n  Krists Boitmanis [aut, ctb],\n  Christian Steglich [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9097-0873>),\n  Johan Koskinen [ctb] (ORCID: <https://orcid.org/0000-0002-6860-325X>),\n  Nynke M.D. Niezink [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-4199-4841>),\n  Viviana Amati [aut, ctb] (ORCID:\n    <https://orcid.org/0000-0003-1190-1237>),\n  Christoph Stadtfeld [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2704-2134>),\n  James Hollway [ctb] (IHEID, ORCID:\n    <https://orcid.org/0000-0002-8361-9647>),\n  Per Block [ctb] (ORCID: <https://orcid.org/0000-0002-7583-2392>),\n  Robert Krause [ctb] (ORCID: <https://orcid.org/0000-0003-4288-4732>),\n  Charlotte Greenan [ctb],\n  Josh Lospinoso [ctb],\n  Michael Schweinberger [ctb] (ORCID:\n    <https://orcid.org/0000-0003-3649-5386>),\n  Mark Huisman [ctb] (ORCID: <https://orcid.org/0000-0002-9009-7859>),\n  Felix Schoenenberger [aut, ctb],\n  Mark Ortmann [ctb],\n  Marion Hoffman [ctb] (ORCID: <https://orcid.org/0000-0002-0741-7760>),\n  Robert Hellpap [ctb],\n  Alvaro Uzaheta [ctb] (ORCID: <https://orcid.org/0000-0003-4367-3670>),\n  Steffen Triebel [ctb]",
    "url": "https://www.stats.ox.ac.uk/~snijders/siena/",
    "bug_reports": "https://github.com/stocnet/rsiena/issues",
    "repository": "https://cran.r-project.org/package=RSiena",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSiena Siena - Simulation Investigation for Empirical Network Analysis The main purpose of this package is to perform simulation-based\n   estimation of stochastic actor-oriented models for longitudinal network\n   data collected as panel data. Dependent variables can be single or\n   multivariate networks, which can be directed, non-directed, or two-mode;\n   and associated actor variables.\n   There are also functions for testing parameters and checking goodness of fit.\n   An overview of these models is given in Snijders (2017),\n   <doi:10.1146/annurev-statistics-060116-054035>.  "
  },
  {
    "id": 6412,
    "package_name": "RSizeBiased",
    "title": "Hypothesis Testing Based on R-Size Biased Samples",
    "description": "Provides functions and examples for testing hypothesis about the population mean and variance on samples drawn by r-size biased sampling schemes.",
    "version": "0.1.0",
    "maintainer": "Dimitrios Bagkavos <dimitrios.bagkavos@gmail.com>",
    "author": "Dimitrios Bagkavos [aut, cre],\n  Polychronis Economou [aut],\n  Apostolos Batsidis [aut],\n  Gorgios Tzavelas [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RSizeBiased",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RSizeBiased Hypothesis Testing Based on R-Size Biased Samples Provides functions and examples for testing hypothesis about the population mean and variance on samples drawn by r-size biased sampling schemes.  "
  },
  {
    "id": 6430,
    "package_name": "RTSA",
    "title": "'Trial Sequential Analysis' for Error Control and Inference in\nSequential Meta-Analyses",
    "description": "Frequentist sequential meta-analysis based on \n    'Trial Sequential Analysis' (TSA) in programmed in Java by the Copenhagen \n    Trial Unit (CTU). The primary function is the calculation of group \n    sequential designs for meta-analysis to be used for planning and analysis of\n    both prospective and retrospective sequential meta-analyses to preserve \n    type-I-error control under sequential testing. 'RTSA' includes tools for \n    sample size and trial size calculation for meta-analysis and core \n    meta-analyses methods such as fixed-effect and random-effects models and\n    forest plots. TSA is described in Wetterslev et. al (2008) \n    <doi:10.1016/j.jclinepi.2007.03.013>. The methods for deriving the\n    group sequential designs are based on Jennison and Turnbull (1999,\n    ISBN:9780849303166).",
    "version": "0.2.2",
    "maintainer": "Anne Lyngholm Soerensen <lynganne@gmail.com>",
    "author": "Anne Lyngholm Soerensen [aut, cre, trl],\n  Markus Harboe Olsen [aut, ctr],\n  Theis Lange [ctr],\n  Christian Gluud [ctr]",
    "url": "https://github.com/AnneLyng/RTSA",
    "bug_reports": "https://github.com/AnneLyng/RTSA/issues",
    "repository": "https://cran.r-project.org/package=RTSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RTSA 'Trial Sequential Analysis' for Error Control and Inference in\nSequential Meta-Analyses Frequentist sequential meta-analysis based on \n    'Trial Sequential Analysis' (TSA) in programmed in Java by the Copenhagen \n    Trial Unit (CTU). The primary function is the calculation of group \n    sequential designs for meta-analysis to be used for planning and analysis of\n    both prospective and retrospective sequential meta-analyses to preserve \n    type-I-error control under sequential testing. 'RTSA' includes tools for \n    sample size and trial size calculation for meta-analysis and core \n    meta-analyses methods such as fixed-effect and random-effects models and\n    forest plots. TSA is described in Wetterslev et. al (2008) \n    <doi:10.1016/j.jclinepi.2007.03.013>. The methods for deriving the\n    group sequential designs are based on Jennison and Turnbull (1999,\n    ISBN:9780849303166).  "
  },
  {
    "id": 6436,
    "package_name": "RUnit",
    "title": "R Unit Test Framework",
    "description": "R functions implementing a standard Unit Testing\n        framework, with additional code inspection and report\n        generation tools.",
    "version": "0.4.33.1",
    "maintainer": "Roman Zenka <zenka.roman@mayo.edu>",
    "author": "Matthias Burger [aut],\n  Klaus Juenemann [aut],\n  Thomas Koenig [aut],\n  Roman Zenka [cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RUnit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RUnit R Unit Test Framework R functions implementing a standard Unit Testing\n        framework, with additional code inspection and report\n        generation tools.  "
  },
  {
    "id": 6437,
    "package_name": "RVAideMemoire",
    "title": "Testing and Plotting Procedures for Biostatistics",
    "description": "Contains miscellaneous functions useful in biostatistics, mostly univariate and multivariate testing procedures with a special emphasis on permutation tests. Many functions intend to simplify user's life by shortening existing procedures or by implementing plotting functions that can be used with as many methods from different packages as possible.",
    "version": "0.9-83-12",
    "maintainer": "Maxime HERVE <maxime.herve@univ-rennes.fr>",
    "author": "Maxime HERVE [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RVAideMemoire",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RVAideMemoire Testing and Plotting Procedures for Biostatistics Contains miscellaneous functions useful in biostatistics, mostly univariate and multivariate testing procedures with a special emphasis on permutation tests. Many functions intend to simplify user's life by shortening existing procedures or by implementing plotting functions that can be used with as many methods from different packages as possible.  "
  },
  {
    "id": 6474,
    "package_name": "RandomProjectionTest",
    "title": "Two-Sample Test in High Dimensions using Random Projection",
    "description": "Performs the random projection test (Lopes et al., (2011) <doi:10.48550/arXiv.1108.2401>) for the one-sample and two-sample hypothesis testing problem for equality of means in the high dimensional setting. We are interested in detecting the mean vector in the one-sample problem or the difference between mean vectors in the two-sample problem.",
    "version": "0.1.4",
    "maintainer": "Juan Ortiz Author <juan.ortiz1alonso@gmail.com>",
    "author": "Juan Ortiz Author [aut, cre, cph, rev]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RandomProjectionTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RandomProjectionTest Two-Sample Test in High Dimensions using Random Projection Performs the random projection test (Lopes et al., (2011) <doi:10.48550/arXiv.1108.2401>) for the one-sample and two-sample hypothesis testing problem for equality of means in the high dimensional setting. We are interested in detecting the mean vector in the one-sample problem or the difference between mean vectors in the two-sample problem.  "
  },
  {
    "id": 6489,
    "package_name": "RareComb",
    "title": "Combinatorial and Statistical Analyses of Rare Events",
    "description": "A custom implementation of the apriori algorithm and binomial tests to identify combinations of features (genes, variants etc) significantly enriched for simultaneous mutations/events from sparse Boolean input, see Vijay Kumar Pounraja, Santhosh Girirajan (2021). Version 1.1 includes a minor adjustment to the number of combinations to be considered for multiple testing correction. This updated version is more conservative in its approach and hence more selective. <doi:10.1101/2021.10.01.462832>.",
    "version": "1.1",
    "maintainer": "Vijay Kumar Pounraja <vijaykumar.mp@gmail.com>",
    "author": "Vijay Kumar Pounraja [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9710-189X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RareComb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RareComb Combinatorial and Statistical Analyses of Rare Events A custom implementation of the apriori algorithm and binomial tests to identify combinations of features (genes, variants etc) significantly enriched for simultaneous mutations/events from sparse Boolean input, see Vijay Kumar Pounraja, Santhosh Girirajan (2021). Version 1.1 includes a minor adjustment to the number of combinations to be considered for multiple testing correction. This updated version is more conservative in its approach and hence more selective. <doi:10.1101/2021.10.01.462832>.  "
  },
  {
    "id": 6546,
    "package_name": "RcmdrPlugin.survival",
    "title": "R Commander Plug-in for the 'survival' Package",
    "description": "An R Commander plug-in for the survival\n  package, with dialogs for Cox models, parametric survival regression models,\n  estimation of survival curves, and testing for differences in survival\n  curves, along with data-management facilities and a variety of tests, \n  diagnostics and graphs.",
    "version": "1.3-2",
    "maintainer": "John Fox <jfox@mcmaster.ca>",
    "author": "John Fox",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RcmdrPlugin.survival",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RcmdrPlugin.survival R Commander Plug-in for the 'survival' Package An R Commander plug-in for the survival\n  package, with dialogs for Cox models, parametric survival regression models,\n  estimation of survival curves, and testing for differences in survival\n  curves, along with data-management facilities and a variety of tests, \n  diagnostics and graphs.  "
  },
  {
    "id": 6670,
    "package_name": "Renvlp",
    "title": "Computing Envelope Estimators",
    "description": "Provides a general routine, envMU, which allows  estimation of the M envelope of span(U) given root n consistent estimators of M and U. The routine envMU does not presume a model.  This package implements response envelopes,  partial response envelopes,  envelopes in the predictor space,  heteroscedastic envelopes,  simultaneous envelopes,  scaled response envelopes,  scaled envelopes in the predictor space,  groupwise envelopes, weighted envelopes,  envelopes in logistic regression, envelopes in Poisson regression envelopes in function-on-function linear regression, envelope-based Partial Partial Least Squares,  envelopes with non-constant error covariance, envelopes with t-distributed errors, reduced rank envelopes and reduced rank envelopes with non-constant error covariance. For each of these model-based routines the package provides inference tools including bootstrap, cross validation, estimation and prediction, hypothesis testing on coefficients are included except for weighted envelopes. Tools for selection of dimension include AIC, BIC and likelihood ratio testing.   Background is available at Cook, R. D., Forzani, L. and Su, Z. (2016) <doi:10.1016/j.jmva.2016.05.006>. Optimization is based on a clockwise coordinate descent algorithm.",
    "version": "3.4.5",
    "maintainer": "Minji Lee <minjilee101@gmail.com>",
    "author": "Minji Lee, Zhihua Su",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Renvlp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Renvlp Computing Envelope Estimators Provides a general routine, envMU, which allows  estimation of the M envelope of span(U) given root n consistent estimators of M and U. The routine envMU does not presume a model.  This package implements response envelopes,  partial response envelopes,  envelopes in the predictor space,  heteroscedastic envelopes,  simultaneous envelopes,  scaled response envelopes,  scaled envelopes in the predictor space,  groupwise envelopes, weighted envelopes,  envelopes in logistic regression, envelopes in Poisson regression envelopes in function-on-function linear regression, envelope-based Partial Partial Least Squares,  envelopes with non-constant error covariance, envelopes with t-distributed errors, reduced rank envelopes and reduced rank envelopes with non-constant error covariance. For each of these model-based routines the package provides inference tools including bootstrap, cross validation, estimation and prediction, hypothesis testing on coefficients are included except for weighted envelopes. Tools for selection of dimension include AIC, BIC and likelihood ratio testing.   Background is available at Cook, R. D., Forzani, L. and Su, Z. (2016) <doi:10.1016/j.jmva.2016.05.006>. Optimization is based on a clockwise coordinate descent algorithm.  "
  },
  {
    "id": 6715,
    "package_name": "Riemann",
    "title": "Learning with Data on Riemannian Manifolds",
    "description": "We provide a variety of algorithms for manifold-valued data, including Fr\u00e9chet summaries, hypothesis testing, clustering, visualization, and other learning tasks. See Bhattacharya and Bhattacharya (2012) <doi:10.1017/CBO9781139094764> for general exposition to statistics on manifolds.",
    "version": "0.1.6",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "author": "Kisung You [aut, cre] (ORCID: <https://orcid.org/0000-0002-8584-459X>)",
    "url": "https://www.kisungyou.com/Riemann/",
    "bug_reports": "https://github.com/kisungyou/Riemann/issues",
    "repository": "https://cran.r-project.org/package=Riemann",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Riemann Learning with Data on Riemannian Manifolds We provide a variety of algorithms for manifold-valued data, including Fr\u00e9chet summaries, hypothesis testing, clustering, visualization, and other learning tasks. See Bhattacharya and Bhattacharya (2012) <doi:10.1017/CBO9781139094764> for general exposition to statistics on manifolds.  "
  },
  {
    "id": 6724,
    "package_name": "Rita",
    "title": "Automated Transformations, Normality Testing, and Reporting",
    "description": "\n    Automated performance of common transformations used to fulfill parametric\n    assumptions of normality and identification of the best performing method \n    for the user. Output for various normality tests (Thode, 2002) corresponding \n    to the best performing method and a descriptive statistical report of the \n    input data in its original units (5-number summary and mathematical moments) \n    are also presented. Lastly, the Rankit, an empirical normal quantile transformation \n    (ENQT) (Soloman & Sawilowsky, 2009), is provided to accommodate non-standard \n    use cases and facilitate adoption.  \n    <DOI: 10.1201/9780203910894>.\n    <DOI: 10.22237/jmasm/1257034080>.",
    "version": "1.2.0",
    "maintainer": "Daniel Mattei <DMattei@live.com>",
    "author": "Daniel Mattei [aut, cre],\n  John Ruscio [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rita",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rita Automated Transformations, Normality Testing, and Reporting \n    Automated performance of common transformations used to fulfill parametric\n    assumptions of normality and identification of the best performing method \n    for the user. Output for various normality tests (Thode, 2002) corresponding \n    to the best performing method and a descriptive statistical report of the \n    input data in its original units (5-number summary and mathematical moments) \n    are also presented. Lastly, the Rankit, an empirical normal quantile transformation \n    (ENQT) (Soloman & Sawilowsky, 2009), is provided to accommodate non-standard \n    use cases and facilitate adoption.  \n    <DOI: 10.1201/9780203910894>.\n    <DOI: 10.22237/jmasm/1257034080>.  "
  },
  {
    "id": 6750,
    "package_name": "Rmst",
    "title": "Computerized Adaptive Multistage Testing",
    "description": "Assemble the panels of computerized adaptive multistage testing by the \n    bottom-up and the top-down approach, and simulate the administration of the assembled \n    panels. The full documentation and tutorials are at <https://github.com/xluo11/Rmst>.\n    Reference: Luo and Kim (2018) <doi:10.1111/jedm.12174>.",
    "version": "0.0.3",
    "maintainer": "Xiao Luo <xluo1986@gmail.com>",
    "author": "Xiao Luo [aut, cre]",
    "url": "https://github.com/xluo11/Rmst",
    "bug_reports": "https://github.com/xluo11/Rmst/issues",
    "repository": "https://cran.r-project.org/package=Rmst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rmst Computerized Adaptive Multistage Testing Assemble the panels of computerized adaptive multistage testing by the \n    bottom-up and the top-down approach, and simulate the administration of the assembled \n    panels. The full documentation and tutorials are at <https://github.com/xluo11/Rmst>.\n    Reference: Luo and Kim (2018) <doi:10.1111/jedm.12174>.  "
  },
  {
    "id": 6773,
    "package_name": "RobustANOVA",
    "title": "Robust One-Way ANOVA Tests under Heteroscedasticity and\nNonnormality",
    "description": "Robust tests (RW, RPB and RGF) are provided for testing the equality of several long-tailed symmetric (LTS) means when the variances are unknown and arbitrary. RW, RPB and RGF tests are robust versions of Welch's F test proposed by Welch (1951) <doi:10.2307/2332579>, parametric bootstrap test proposed by Krishnamoorthy et. al (2007) <doi:10.1016/j.csda.2006.09.039>; and generalized F test proposed by Weerahandi (1995) <doi:10.2307/2532947>;, respectively. These tests are based on the modified maximum likelihood (MML) estimators proposed by Tiku(1967, 1968) <doi:10.2307/2333859>, <doi:10.1080/01621459.1968.11009228>. ",
    "version": "0.3.0",
    "maintainer": "Gamze Guven <gamzeguven@ogu.edu.tr>",
    "author": "Gamze Guven [aut, cre],\n  Sukru Acitas [aut],\n  Birdal Senoglu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RobustANOVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RobustANOVA Robust One-Way ANOVA Tests under Heteroscedasticity and\nNonnormality Robust tests (RW, RPB and RGF) are provided for testing the equality of several long-tailed symmetric (LTS) means when the variances are unknown and arbitrary. RW, RPB and RGF tests are robust versions of Welch's F test proposed by Welch (1951) <doi:10.2307/2332579>, parametric bootstrap test proposed by Krishnamoorthy et. al (2007) <doi:10.1016/j.csda.2006.09.039>; and generalized F test proposed by Weerahandi (1995) <doi:10.2307/2532947>;, respectively. These tests are based on the modified maximum likelihood (MML) estimators proposed by Tiku(1967, 1968) <doi:10.2307/2333859>, <doi:10.1080/01621459.1968.11009228>.   "
  },
  {
    "id": 6775,
    "package_name": "RobustBF",
    "title": "Robust Solution to the Behrens-Fisher Problem",
    "description": "Robust tests (RW and RF) are provided for testing the equality of two long-tailed symmetric (LTS) means when the variances are unknown and arbitrary. RW test is a robust version of Welch's two sample t test and the RF is a robust fiducial based test. The RW and RF tests are proposed using the adaptive modified maximum likelihood (AMML) estimators derived by Tiku and Surucu (2009) <doi:10.1016/j.spl.2008.12.001> and Donmez (2010) <https://open.metu.edu.tr/bitstream/handle/11511/19440/index.pdf>.",
    "version": "0.2.0",
    "maintainer": "Gamze Guven <gamzeguven@ogu.edu.tr>",
    "author": "Gamze Guven [aut, cre],\n  Sukru Acitas [aut],\n  Hatice Samkar [aut],\n  Birdal Senoglu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RobustBF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RobustBF Robust Solution to the Behrens-Fisher Problem Robust tests (RW and RF) are provided for testing the equality of two long-tailed symmetric (LTS) means when the variances are unknown and arbitrary. RW test is a robust version of Welch's two sample t test and the RF is a robust fiducial based test. The RW and RF tests are proposed using the adaptive modified maximum likelihood (AMML) estimators derived by Tiku and Surucu (2009) <doi:10.1016/j.spl.2008.12.001> and Donmez (2010) <https://open.metu.edu.tr/bitstream/handle/11511/19440/index.pdf>.  "
  },
  {
    "id": 6795,
    "package_name": "Rosenbrock",
    "title": "Extended Rosenbrock-Type Densities for Markov Chain Monte Carlo\n(MCMC) Sampler Benchmarking",
    "description": "New Markov chain Monte Carlo (MCMC) samplers new to be thoroughly tested\n    and their performance accurately assessed. This requires densities\n    that offer challenging properties to the novel sampling algorithms.\n    One such popular problem is the Rosenbrock function. However, while its\n    shape lends itself well to a benchmark problem, no codified multivariate expansion\n    of the density exists. We have developed an extension to this class of distributions\n    and supplied densities and direct sampler functions to assess the performance\n    of novel MCMC algorithms. The functions are introduced in \"An n-dimensional Rosenbrock \n    Distribution for MCMC Testing\" by Pagani, Wiegand and Nadarajah (2019) <arXiv:1903.09556>.",
    "version": "0.1.0",
    "maintainer": "Martin Wiegand <Martin.Wiegand@mrc-bsu.cam.ac.uk>",
    "author": "Martin Wiegand",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rosenbrock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rosenbrock Extended Rosenbrock-Type Densities for Markov Chain Monte Carlo\n(MCMC) Sampler Benchmarking New Markov chain Monte Carlo (MCMC) samplers new to be thoroughly tested\n    and their performance accurately assessed. This requires densities\n    that offer challenging properties to the novel sampling algorithms.\n    One such popular problem is the Rosenbrock function. However, while its\n    shape lends itself well to a benchmark problem, no codified multivariate expansion\n    of the density exists. We have developed an extension to this class of distributions\n    and supplied densities and direct sampler functions to assess the performance\n    of novel MCMC algorithms. The functions are introduced in \"An n-dimensional Rosenbrock \n    Distribution for MCMC Testing\" by Pagani, Wiegand and Nadarajah (2019) <arXiv:1903.09556>.  "
  },
  {
    "id": 6955,
    "package_name": "SESraster",
    "title": "Raster Randomization for Null Hypothesis Testing",
    "description": "Randomization of presence/absence species distribution raster\n    data with or without including spatial structure for calculating\n    standardized effect sizes and testing null hypothesis. The\n    randomization algorithms are based on classical algorithms for\n    matrices (Gotelli 2000, <doi:10.2307/177478>) implemented for raster\n    data.",
    "version": "0.7.1",
    "maintainer": "Neander Marcel Heming <neanderh@yahoo.com.br>",
    "author": "Neander Marcel Heming [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2461-5045>),\n  Fl\u00e1vio M. M. Mota [aut] (ORCID:\n    <https://orcid.org/0000-0002-0308-7151>),\n  Gabriela Alves-Ferreira [aut] (ORCID:\n    <https://orcid.org/0000-0001-5661-3381>)",
    "url": "https://CRAN.R-project.org/package=SESraster,\nhttps://github.com/HemingNM/SESraster,\nhttps://hemingnm.github.io/SESraster/",
    "bug_reports": "https://github.com/HemingNM/SESraster/issues",
    "repository": "https://cran.r-project.org/package=SESraster",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SESraster Raster Randomization for Null Hypothesis Testing Randomization of presence/absence species distribution raster\n    data with or without including spatial structure for calculating\n    standardized effect sizes and testing null hypothesis. The\n    randomization algorithms are based on classical algorithms for\n    matrices (Gotelli 2000, <doi:10.2307/177478>) implemented for raster\n    data.  "
  },
  {
    "id": 6975,
    "package_name": "SHT",
    "title": "Statistical Hypothesis Testing Toolbox",
    "description": "We provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) <doi:10.1007/0-387-27605-X>.",
    "version": "0.1.9",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "author": "Kyoungjae Lee [aut],\n  Lizhen Lin [aut],\n  Kisung You [aut, cre] (ORCID: <https://orcid.org/0000-0002-8584-459X>)",
    "url": "https://www.kisungyou.com/SHT/",
    "bug_reports": "https://github.com/kisungyou/SHT/issues",
    "repository": "https://cran.r-project.org/package=SHT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SHT Statistical Hypothesis Testing Toolbox We provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) <doi:10.1007/0-387-27605-X>.  "
  },
  {
    "id": 6998,
    "package_name": "SIRE",
    "title": "Finding Feedback Effects in SEM and Testing for Their\nSignificance",
    "description": "Provides two main functionalities.\n    1 - Given a system of simultaneous equation,\n    it decomposes  the matrix of coefficients weighting the endogenous variables \n    into three submatrices: one includes the subset of coefficients that have a causal nature\n    in the model, two include the subset of coefficients that have a interdependent nature\n    in the model, either at systematic level or induced by the correlation between error terms.\n    2 - Given a decomposed model,\n    it tests for the significance of the interdependent relationships acting in the system, \n    via Maximum likelihood and Wald test, which can be built starting from the function output.\n    For theoretical reference see Faliva (1992) <doi:10.1007/BF02589085> and \n    Faliva and Zoia (1994) <doi:10.1007/BF02589041>.",
    "version": "1.1.0",
    "maintainer": "Gianmarco Vacca <gianmarco.vacca@unicatt.it>",
    "author": "Gianmarco Vacca [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SIRE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SIRE Finding Feedback Effects in SEM and Testing for Their\nSignificance Provides two main functionalities.\n    1 - Given a system of simultaneous equation,\n    it decomposes  the matrix of coefficients weighting the endogenous variables \n    into three submatrices: one includes the subset of coefficients that have a causal nature\n    in the model, two include the subset of coefficients that have a interdependent nature\n    in the model, either at systematic level or induced by the correlation between error terms.\n    2 - Given a decomposed model,\n    it tests for the significance of the interdependent relationships acting in the system, \n    via Maximum likelihood and Wald test, which can be built starting from the function output.\n    For theoretical reference see Faliva (1992) <doi:10.1007/BF02589085> and \n    Faliva and Zoia (1994) <doi:10.1007/BF02589041>.  "
  },
  {
    "id": 7005,
    "package_name": "SK4FGA",
    "title": "Scott-Knott for Forensic Glass Analysis",
    "description": "In forensics, it is common and effective practice to analyse glass fragments from the scene and suspects \n  to gain evidence of placing a suspect at the crime scene. \n  This kind of analysis involves comparing the physical and chemical attributes of glass fragments that exist on both the \n  person and at the crime scene, and assessing the significance in a likeness that they share. \n  The package implements the Scott-Knott Modification 2 algorithm (SKM2) (Christopher M. Triggs and \n  James M. Curran and John S. Buckleton and Kevan A.J. Walsh (1997) <doi:10.1016/S0379-0738(96)02037-3> \n  \"The grouping problem in forensic glass analysis: a divisive approach\", Forensic Science International, 85(1), 1--14)\n  for small sample glass fragment analysis using the refractive index (ri) of a set of glass samples. \n  It also includes an experimental multivariate analog to the Scott-Knott algorithm for similar analysis on glass samples\n  with multiple chemical concentration variables and multiple samples of the same item; testing against the Hotellings T^2 \n  distribution (J.M. Curran and C.M. Triggs and J.R. Almirall and J.S. Buckleton and K.A.J. Walsh (1997) \n  <doi:10.1016/S1355-0306(97)72197-X> \"The interpretation of elemental composition measurements from forensic \n  glass evidence\", Science & Justice, 37(4), 241--244).",
    "version": "0.1.1",
    "maintainer": "Toby Hayward <tobyhayward13@gmail.com>",
    "author": "Toby Hayward [aut, cre] (Main developer and maintainer of the package.),\n  James Curran [aut, ctb] (Supervised and contributed to the development\n    of the package.),\n  Lewis Kendall-Jones [ctb] (Wrote and supported the development of the\n    C++ code.)",
    "url": "https://github.com/tobyhayward13/SCI118UOA_ForensicGlassAnalysis",
    "bug_reports": "https://github.com/tobyhayward13/SCI118UOA_ForensicGlassAnalysis/issues",
    "repository": "https://cran.r-project.org/package=SK4FGA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SK4FGA Scott-Knott for Forensic Glass Analysis In forensics, it is common and effective practice to analyse glass fragments from the scene and suspects \n  to gain evidence of placing a suspect at the crime scene. \n  This kind of analysis involves comparing the physical and chemical attributes of glass fragments that exist on both the \n  person and at the crime scene, and assessing the significance in a likeness that they share. \n  The package implements the Scott-Knott Modification 2 algorithm (SKM2) (Christopher M. Triggs and \n  James M. Curran and John S. Buckleton and Kevan A.J. Walsh (1997) <doi:10.1016/S0379-0738(96)02037-3> \n  \"The grouping problem in forensic glass analysis: a divisive approach\", Forensic Science International, 85(1), 1--14)\n  for small sample glass fragment analysis using the refractive index (ri) of a set of glass samples. \n  It also includes an experimental multivariate analog to the Scott-Knott algorithm for similar analysis on glass samples\n  with multiple chemical concentration variables and multiple samples of the same item; testing against the Hotellings T^2 \n  distribution (J.M. Curran and C.M. Triggs and J.R. Almirall and J.S. Buckleton and K.A.J. Walsh (1997) \n  <doi:10.1016/S1355-0306(97)72197-X> \"The interpretation of elemental composition measurements from forensic \n  glass evidence\", Science & Justice, 37(4), 241--244).  "
  },
  {
    "id": 7008,
    "package_name": "SKNN",
    "title": "A Super K-Nearest Neighbor (SKNN) Classification Algorithm",
    "description": "It's a Super K-Nearest Neighbor(SKNN) classification method with using kernel density to describe weight of the distance between a training observation and the testing sample. Comparison of performance between SKNN and KNN shows that SKNN is significantly superior to KNN.",
    "version": "4.1.2",
    "maintainer": "Yarong Yang <Yi.YA_yaya@hotmail.com>",
    "author": "Yarong Yang [aut, cre],\n  Nader Ebrahimi [ctb],\n  Yoram Rubin [ctb],\n  Jacob Zhang [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SKNN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SKNN A Super K-Nearest Neighbor (SKNN) Classification Algorithm It's a Super K-Nearest Neighbor(SKNN) classification method with using kernel density to describe weight of the distance between a training observation and the testing sample. Comparison of performance between SKNN and KNN shows that SKNN is significantly superior to KNN.  "
  },
  {
    "id": 7047,
    "package_name": "SMUT",
    "title": "Multi-SNP Mediation Intersection-Union Test",
    "description": "Testing the mediation effect of multiple SNPs on an outcome through a mediator.",
    "version": "1.1",
    "maintainer": "Wujuan Zhong <zhongwujuan@gmail.com>",
    "author": "Wujuan Zhong",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SMUT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SMUT Multi-SNP Mediation Intersection-Union Test Testing the mediation effect of multiple SNPs on an outcome through a mediator.  "
  },
  {
    "id": 7098,
    "package_name": "SPRT",
    "title": "Sequential Probability Ratio Test (SPRT) Method",
    "description": "Provides functions to perform the Sequential Probability Ratio Test (SPRT) \n  for hypothesis testing in Binomial, Poisson and Normal distributions. \n  The package allows users to specify Type I and Type II error probabilities, \n  decision thresholds, and compare null and alternative hypotheses sequentially \n  as data accumulate. It includes visualization tools for plotting the likelihood \n  ratio path and decision boundaries, making it easier to interpret results. \n  The methods are based on Wald (1945) <doi:10.1214/aoms/1177731118>, \n  who introduced the SPRT as one of the earliest and most powerful sequential \n  analysis techniques. This package is useful in quality control, clinical trials, \n  and other applications requiring early decision-making.The term 'SPRT' is an abbreviation and used intentionally.",
    "version": "1.1.0",
    "maintainer": "Huchesh Budihal <hhbudihal17@gmail.com>",
    "author": "Huchesh Budihal [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPRT Sequential Probability Ratio Test (SPRT) Method Provides functions to perform the Sequential Probability Ratio Test (SPRT) \n  for hypothesis testing in Binomial, Poisson and Normal distributions. \n  The package allows users to specify Type I and Type II error probabilities, \n  decision thresholds, and compare null and alternative hypotheses sequentially \n  as data accumulate. It includes visualization tools for plotting the likelihood \n  ratio path and decision boundaries, making it easier to interpret results. \n  The methods are based on Wald (1945) <doi:10.1214/aoms/1177731118>, \n  who introduced the SPRT as one of the earliest and most powerful sequential \n  analysis techniques. This package is useful in quality control, clinical trials, \n  and other applications requiring early decision-making.The term 'SPRT' is an abbreviation and used intentionally.  "
  },
  {
    "id": 7102,
    "package_name": "SPlit",
    "title": "Split a Dataset for Training and Testing",
    "description": "Procedure to optimally split a dataset for training and testing. \n    'SPlit' is based on the method of support points, which is independent of modeling methods.\n    Please see Joseph and Vakayil (2021) <doi:10.1080/00401706.2021.1921037> for details.\n    This work is supported by U.S. National Science Foundation grant DMREF-1921873.",
    "version": "1.2",
    "maintainer": "Akhil Vakayil <akhilv@gatech.edu>",
    "author": "Akhil Vakayil [aut, cre],\n  Roshan Joseph [aut, ths],\n  Simon Mak [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SPlit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SPlit Split a Dataset for Training and Testing Procedure to optimally split a dataset for training and testing. \n    'SPlit' is based on the method of support points, which is independent of modeling methods.\n    Please see Joseph and Vakayil (2021) <doi:10.1080/00401706.2021.1921037> for details.\n    This work is supported by U.S. National Science Foundation grant DMREF-1921873.  "
  },
  {
    "id": 7112,
    "package_name": "SQUIRE",
    "title": "Statistical Quality-Assured Integrated Response Estimation",
    "description": "Provides systematic geometry-adaptive parameter optimization with \n    statistical validation for experimental biological data. Combines ANOVA-based \n    validation with systematic constraint configuration testing (log-scale, \n    positive domain, Euclidean) through T,P,E testing. Only proceeds with \n    parameter optimization when statistically significant biological effects \n    are detected, preventing over-fitting to noise. Uses 'GALAHAD' trust region methods with constraint projection from Conn et al. (2000) \n    <doi:10.1137/S1052623497325107>, ANOVA-based validation following Fisher \n    (1925) <doi:10.1007/978-1-4612-4380-9_6>, and effect size calculations \n    per Cohen (1988, ISBN:0805802835). Designed for structured experimental \n    data including kinetic curves, dose-response studies, and treatment \n    comparisons where appropriate parameter constraints and statistical \n    justification are important for meaningful biological interpretation. \n    Developed at the Minnesota Center for Prion Research and Outreach at \n    the University of Minnesota.",
    "version": "1.0.1",
    "maintainer": "Richard A. Feiss <feiss026@umn.edu>",
    "author": "Richard A. Feiss [aut, cre] (ORCID:\n    <https://orcid.org/0009-0008-0409-6042>)",
    "url": "https://github.com/RFeissIV/SQUIRE",
    "bug_reports": "https://github.com/RFeissIV/SQUIRE/issues",
    "repository": "https://cran.r-project.org/package=SQUIRE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SQUIRE Statistical Quality-Assured Integrated Response Estimation Provides systematic geometry-adaptive parameter optimization with \n    statistical validation for experimental biological data. Combines ANOVA-based \n    validation with systematic constraint configuration testing (log-scale, \n    positive domain, Euclidean) through T,P,E testing. Only proceeds with \n    parameter optimization when statistically significant biological effects \n    are detected, preventing over-fitting to noise. Uses 'GALAHAD' trust region methods with constraint projection from Conn et al. (2000) \n    <doi:10.1137/S1052623497325107>, ANOVA-based validation following Fisher \n    (1925) <doi:10.1007/978-1-4612-4380-9_6>, and effect size calculations \n    per Cohen (1988, ISBN:0805802835). Designed for structured experimental \n    data including kinetic curves, dose-response studies, and treatment \n    comparisons where appropriate parameter constraints and statistical \n    justification are important for meaningful biological interpretation. \n    Developed at the Minnesota Center for Prion Research and Outreach at \n    the University of Minnesota.  "
  },
  {
    "id": 7161,
    "package_name": "SUMO",
    "title": "Generating Multi-Omics Datasets for Testing and Benchmarking",
    "description": "Provides tools to simulate multi-omics datasets with predefined signal structures. The generated data can be used for testing, validating, and benchmarking integrative analysis methods such as factor models and clustering approaches. This version includes enhanced signal customization, visualization tools (scatter, histogram, 3D), MOFA-based analysis pipelines, PowerPoint export, and statistical profiling of datasets. Designed for both method development and teaching, SUMO supports real and synthetic data pipelines with interpretable outputs. Tini, Giulia, et al (2019) <doi:10.1093/bib/bbx167>.",
    "version": "1.2.3",
    "maintainer": "Bernard Isekah Osang'ir <Bernard.Osangir@sckcen.be>",
    "author": "Bernard Isekah Osang'ir [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5557-3602>),\n  Ziv Shkedy [ctb],\n  Surya Gupta [ctb],\n  J\u00fcrgen Claesen [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SUMO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SUMO Generating Multi-Omics Datasets for Testing and Benchmarking Provides tools to simulate multi-omics datasets with predefined signal structures. The generated data can be used for testing, validating, and benchmarking integrative analysis methods such as factor models and clustering approaches. This version includes enhanced signal customization, visualization tools (scatter, histogram, 3D), MOFA-based analysis pipelines, PowerPoint export, and statistical profiling of datasets. Designed for both method development and teaching, SUMO supports real and synthetic data pipelines with interpretable outputs. Tini, Giulia, et al (2019) <doi:10.1093/bib/bbx167>.  "
  },
  {
    "id": 7171,
    "package_name": "SWIM",
    "title": "Scenario Weights for Importance Measurement",
    "description": "An efficient sensitivity analysis for stochastic models based on \n    Monte Carlo samples. Provides weights on simulated scenarios from a \n    stochastic model, such that stressed random variables fulfil given \n    probabilistic constraints (e.g. specified values for risk measures), \n    under the new scenario weights. Scenario weights are selected by \n    constrained minimisation of the relative entropy to the baseline model. \n    The 'SWIM' package is based on Pesenti S.M., Millossovich P., Tsanakas A. (2019)\n    \"Reverse Sensitivity Testing: What does it take to break the model\" \n    <openaccess.city.ac.uk/id/eprint/18896/> and Pesenti S.M. (2021) \n    \"Reverse Sensitivity Analysis for Risk Modelling\" <https://www.ssrn.com/abstract=3878879>.",
    "version": "1.0.0",
    "maintainer": "Silvana M. Pesenti <swimpackage@gmail.com>",
    "author": "Silvana M. Pesenti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6661-6970>),\n  Alberto Bettini [aut],\n  Pietro Millossovich [aut] (ORCID:\n    <https://orcid.org/0000-0001-8269-7507>),\n  Andreas Tsanakas [aut] (ORCID: <https://orcid.org/0000-0003-4552-5532>),\n  Zhuomin Mao [ctb],\n  Kent Wu [ctb]",
    "url": "https://github.com/spesenti/SWIM,\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3515274,\nhttps://utstat.toronto.edu/pesenti/?page_id=138",
    "bug_reports": "https://github.com/spesenti/SWIM/issues",
    "repository": "https://cran.r-project.org/package=SWIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SWIM Scenario Weights for Importance Measurement An efficient sensitivity analysis for stochastic models based on \n    Monte Carlo samples. Provides weights on simulated scenarios from a \n    stochastic model, such that stressed random variables fulfil given \n    probabilistic constraints (e.g. specified values for risk measures), \n    under the new scenario weights. Scenario weights are selected by \n    constrained minimisation of the relative entropy to the baseline model. \n    The 'SWIM' package is based on Pesenti S.M., Millossovich P., Tsanakas A. (2019)\n    \"Reverse Sensitivity Testing: What does it take to break the model\" \n    <openaccess.city.ac.uk/id/eprint/18896/> and Pesenti S.M. (2021) \n    \"Reverse Sensitivity Analysis for Risk Modelling\" <https://www.ssrn.com/abstract=3878879>.  "
  },
  {
    "id": 7177,
    "package_name": "SampleSize4ClinicalTrials",
    "title": "Sample Size Calculation for the Comparison of Means or\nProportions in Phase III Clinical Trials",
    "description": "There are four categories of Phase III clinical trials according to different research goals, including (1) Testing for equality, (2) Superiority trial, (3) Non-inferiority trial, and (4) Equivalence trial. This package aims to help researchers to calculate sample size when comparing means or proportions in Phase III clinical trials with different research goals.",
    "version": "0.2.3",
    "maintainer": "Hongchao Qi <hcqi1992@gmail.com>",
    "author": "Hongchao Qi, Fang Zhu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SampleSize4ClinicalTrials",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SampleSize4ClinicalTrials Sample Size Calculation for the Comparison of Means or\nProportions in Phase III Clinical Trials There are four categories of Phase III clinical trials according to different research goals, including (1) Testing for equality, (2) Superiority trial, (3) Non-inferiority trial, and (4) Equivalence trial. This package aims to help researchers to calculate sample size when comparing means or proportions in Phase III clinical trials with different research goals.  "
  },
  {
    "id": 7206,
    "package_name": "SeedVigorIndex",
    "title": "Seed Vigor Index",
    "description": "Seed vigor is defined as the sum total of those properties of the seed which determine the level of activity and performance of the seed or seed lot during germination and seedling emergence.  Testing for vigor becomes more important for carryover seeds, especially if seeds were stored under unknown conditions or under unfavorable storage conditions. Seed vigor testing is also used as indicator of the storage potential of a seed lot and in ranking various seed lots with different qualities. The vigour index is calculated using the equation given by (Ling et al. 2014) <doi:10.1038/srep05859>.",
    "version": "0.1.0",
    "maintainer": "Tanuj Misra <tanujmisra102@gmail.com>",
    "author": "Sudip Kumar Dutta [aut, cph],\n  Tanuj Misra [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SeedVigorIndex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SeedVigorIndex Seed Vigor Index Seed vigor is defined as the sum total of those properties of the seed which determine the level of activity and performance of the seed or seed lot during germination and seedling emergence.  Testing for vigor becomes more important for carryover seeds, especially if seeds were stored under unknown conditions or under unfavorable storage conditions. Seed vigor testing is also used as indicator of the storage potential of a seed lot and in ranking various seed lots with different qualities. The vigour index is calculated using the equation given by (Ling et al. 2014) <doi:10.1038/srep05859>.  "
  },
  {
    "id": 7236,
    "package_name": "SeqFeatR",
    "title": "A Tool to Associate FASTA Sequences and Features",
    "description": "Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>.",
    "version": "0.3.1",
    "maintainer": "Bettina Budeus <bettina.budeus@stud.uni-due.de>",
    "author": "Bettina Budeus",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SeqFeatR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SeqFeatR A Tool to Associate FASTA Sequences and Features Provides user friendly methods for the identification of sequence patterns that are statistically significantly associated with a property of the sequence. For instance, SeqFeatR allows to identify viral immune escape mutations for hosts of given HLA types. The underlying statistical method is Fisher's exact test, with appropriate corrections for multiple testing, or Bayes. Patterns may be point mutations or n-tuple of mutations. SeqFeatR offers several ways to visualize the results of the statistical analyses, see Budeus (2016) <doi:10.1371/journal.pone.0146409>.  "
  },
  {
    "id": 7244,
    "package_name": "SetTest",
    "title": "Group Testing Procedures for Signal Detection and\nGoodness-of-Fit",
    "description": "It provides cumulative distribution function (CDF),\n    quantile, p-value, statistical power calculator and random number generator\n    for a collection of group-testing procedures, including the Higher Criticism\n    tests, the one-sided Kolmogorov-Smirnov tests, the one-sided Berk-Jones tests,\n    the one-sided phi-divergence tests, etc. The input are a group of p-values.\n    The null hypothesis is that they are i.i.d. Uniform(0,1). In the context of\n    signal detection, the null hypothesis means no signals. In the context of the\n    goodness-of-fit testing, which contrasts a group of i.i.d. random variables to\n    a given continuous distribution, the input p-values can be obtained by the CDF\n    transformation. The null hypothesis means that these random variables follow the\n    given distribution. For reference, see [1]Hong Zhang, Jiashun Jin and Zheyang Wu. \n    \"Distributions and power of optimal signal-detection statistics in finite case\",\n    IEEE Transactions on Signal Processing (2020) 68, 1021-1033; [2] Hong Zhang and Zheyang Wu.\n    \"The general goodness-of-fit tests for correlated data\", Computational Statistics & \n    Data Analysis (2022) 167, 107379.",
    "version": "0.3.0",
    "maintainer": "Hong Zhang <hzhang@wpi.edu>",
    "author": "Hong Zhang and Zheyang Wu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SetTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SetTest Group Testing Procedures for Signal Detection and\nGoodness-of-Fit It provides cumulative distribution function (CDF),\n    quantile, p-value, statistical power calculator and random number generator\n    for a collection of group-testing procedures, including the Higher Criticism\n    tests, the one-sided Kolmogorov-Smirnov tests, the one-sided Berk-Jones tests,\n    the one-sided phi-divergence tests, etc. The input are a group of p-values.\n    The null hypothesis is that they are i.i.d. Uniform(0,1). In the context of\n    signal detection, the null hypothesis means no signals. In the context of the\n    goodness-of-fit testing, which contrasts a group of i.i.d. random variables to\n    a given continuous distribution, the input p-values can be obtained by the CDF\n    transformation. The null hypothesis means that these random variables follow the\n    given distribution. For reference, see [1]Hong Zhang, Jiashun Jin and Zheyang Wu. \n    \"Distributions and power of optimal signal-detection statistics in finite case\",\n    IEEE Transactions on Signal Processing (2020) 68, 1021-1033; [2] Hong Zhang and Zheyang Wu.\n    \"The general goodness-of-fit tests for correlated data\", Computational Statistics & \n    Data Analysis (2022) 167, 107379.  "
  },
  {
    "id": 7278,
    "package_name": "SignifReg",
    "title": "Consistent Significance Controlled Variable Selection in\nGeneralized Linear Regression",
    "description": "Provides significance controlled variable selection algorithms with different directions (forward, backward, stepwise) based on diverse criteria (AIC, BIC, adjusted r-square, PRESS, or p-value). The algorithm selects a final model with only significant variables defined as those with significant p-values after multiple testing correction such as Bonferroni, False Discovery Rate, etc. See Zambom and Kim (2018) <doi:10.1002/sta4.210>.",
    "version": "4.3",
    "maintainer": "Adriano Zanin Zambom <adriano.zambom@csun.edu>",
    "author": "Jongwook Kim, Adriano Zanin Zambom",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SignifReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SignifReg Consistent Significance Controlled Variable Selection in\nGeneralized Linear Regression Provides significance controlled variable selection algorithms with different directions (forward, backward, stepwise) based on diverse criteria (AIC, BIC, adjusted r-square, PRESS, or p-value). The algorithm selects a final model with only significant variables defined as those with significant p-values after multiple testing correction such as Bonferroni, False Discovery Rate, etc. See Zambom and Kim (2018) <doi:10.1002/sta4.210>.  "
  },
  {
    "id": 7288,
    "package_name": "SimDissolution",
    "title": "Modeling and Assessing Similarity of Drug Dissolutions Profiles",
    "description": "Implementation of a model-based bootstrap approach for testing whether two formulations are similar. The package provides a function for fitting a pharmacokinetic model to time-concentration data and comparing the results for all five candidate models regarding the Residual Sum of Squares (RSS). The candidate set contains a First order, Hixson-Crowell, Higuchi, Weibull and a logistic model. The assessment of similarity implemented in this package is performed regarding the maximum deviation of the profiles. See Moellenhoff et al. (2018) <doi:10.1002/sim.7689> for details.",
    "version": "0.1.0",
    "maintainer": "Kathrin Moellenhoff <kathrin.moellenhoff@rub.de>",
    "author": "Kathrin Moellenhoff",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimDissolution",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimDissolution Modeling and Assessing Similarity of Drug Dissolutions Profiles Implementation of a model-based bootstrap approach for testing whether two formulations are similar. The package provides a function for fitting a pharmacokinetic model to time-concentration data and comparing the results for all five candidate models regarding the Residual Sum of Squares (RSS). The candidate set contains a First order, Hixson-Crowell, Higuchi, Weibull and a logistic model. The assessment of similarity implemented in this package is performed regarding the maximum deviation of the profiles. See Moellenhoff et al. (2018) <doi:10.1002/sim.7689> for details.  "
  },
  {
    "id": 7292,
    "package_name": "SimIndep",
    "title": "WISE: a Weighted Similarity Aggregation Test for Serial\nIndependence",
    "description": "A fast implementation of the weighted information similarity aggregation (WISE) \n    test for detecting serial dependence, particularly suited for high-dimensional and \n    non-Euclidean time series. Includes functions for constructing similarity matrices \n    and conducting hypothesis testing. Users can use different similarity \n    measures and define their own weighting schemes. For more details see Q Zhu, M Liu, \n    Y Han, D Zhou (2025) <doi:10.48550/arXiv.2509.05678>.",
    "version": "0.1.2",
    "maintainer": "Qihua Zhu <zhuqihua@u.nus.edu>",
    "author": "Qihua Zhu [aut, cre],\n  Mingshuo Liu [aut],\n  Yuefeng Han [aut],\n  Doudou Zhou [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimIndep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimIndep WISE: a Weighted Similarity Aggregation Test for Serial\nIndependence A fast implementation of the weighted information similarity aggregation (WISE) \n    test for detecting serial dependence, particularly suited for high-dimensional and \n    non-Euclidean time series. Includes functions for constructing similarity matrices \n    and conducting hypothesis testing. Users can use different similarity \n    measures and define their own weighting schemes. For more details see Q Zhu, M Liu, \n    Y Han, D Zhou (2025) <doi:10.48550/arXiv.2509.05678>.  "
  },
  {
    "id": 7301,
    "package_name": "SimSeq",
    "title": "Nonparametric Simulation of RNA-Seq Data",
    "description": "RNA sequencing analysis methods are often derived by relying on hypothetical parametric models for read counts that are not likely to be precisely satisfied in practice. Methods are often tested by analyzing data that have been simulated according to the assumed model. This testing strategy can result in an overly optimistic view of the performance of an RNA-seq analysis method. We develop a data-based simulation algorithm for RNA-seq data. The vector of read counts simulated for a given experimental unit has a joint distribution that closely matches the distribution of a source RNA-seq dataset provided by the user. Users control the proportion of genes simulated to be differentially expressed (DE) and can provide a vector of weights to control the distribution of effect sizes. The algorithm requires a matrix of RNA-seq read counts with large sample sizes in at least two treatment groups. Many datasets are available that fit this standard.",
    "version": "1.4.0",
    "maintainer": "Samuel Benidt <sgbenidt@gmail.com>",
    "author": "Samuel Benidt",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimSeq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimSeq Nonparametric Simulation of RNA-Seq Data RNA sequencing analysis methods are often derived by relying on hypothetical parametric models for read counts that are not likely to be precisely satisfied in practice. Methods are often tested by analyzing data that have been simulated according to the assumed model. This testing strategy can result in an overly optimistic view of the performance of an RNA-seq analysis method. We develop a data-based simulation algorithm for RNA-seq data. The vector of read counts simulated for a given experimental unit has a joint distribution that closely matches the distribution of a source RNA-seq dataset provided by the user. Users control the proportion of genes simulated to be differentially expressed (DE) and can provide a vector of weights to control the distribution of effect sizes. The algorithm requires a matrix of RNA-seq read counts with large sample sizes in at least two treatment groups. Many datasets are available that fit this standard.  "
  },
  {
    "id": 7304,
    "package_name": "SimTOST",
    "title": "Sample Size Estimation for Bio-Equivalence Trials Through\nSimulation",
    "description": "\n    Sample size estimation for bio-equivalence trials is supported through a simulation-based approach \n    that extends the Two One-Sided Tests (TOST) procedure. The methodology provides flexibility in \n    hypothesis testing, accommodates multiple treatment comparisons, and accounts for correlated endpoints. \n    Users can model complex trial scenarios, including parallel and crossover designs, intra-subject variability, \n    and different equivalence margins. Monte Carlo simulations enable accurate estimation of power and type I error \n    rates, ensuring well-calibrated study designs. The statistical framework builds on established methods for \n    equivalence testing and multiple hypothesis testing in bio-equivalence studies, as described in Schuirmann (1987) \n    <doi:10.1007/BF01068419>, Mielke et al. (2018) <doi:10.1080/19466315.2017.1371071>, Shieh (2022) \n    <doi:10.1371/journal.pone.0269128>, and Sozu et al. (2015) <doi:10.1007/978-3-319-22005-5>. \n    Comprehensive documentation and vignettes guide users through implementation and interpretation of results.",
    "version": "1.0.2",
    "maintainer": "Thomas Debray <tdebray@fromdatatowisdom.com>",
    "author": "Thomas Debray [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1790-2719>),\n  Johanna Munoz [aut],\n  Dewi Amaliah [ctb],\n  Wei Wei [ctb],\n  Marian Mitroiu [ctb],\n  Scott McDonald [ctb],\n  Biogen Inc [cph, fnd]",
    "url": "https://smartdata-analysis-and-statistics.github.io/SimTOST/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SimTOST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SimTOST Sample Size Estimation for Bio-Equivalence Trials Through\nSimulation \n    Sample size estimation for bio-equivalence trials is supported through a simulation-based approach \n    that extends the Two One-Sided Tests (TOST) procedure. The methodology provides flexibility in \n    hypothesis testing, accommodates multiple treatment comparisons, and accounts for correlated endpoints. \n    Users can model complex trial scenarios, including parallel and crossover designs, intra-subject variability, \n    and different equivalence margins. Monte Carlo simulations enable accurate estimation of power and type I error \n    rates, ensuring well-calibrated study designs. The statistical framework builds on established methods for \n    equivalence testing and multiple hypothesis testing in bio-equivalence studies, as described in Schuirmann (1987) \n    <doi:10.1007/BF01068419>, Mielke et al. (2018) <doi:10.1080/19466315.2017.1371071>, Shieh (2022) \n    <doi:10.1371/journal.pone.0269128>, and Sozu et al. (2015) <doi:10.1007/978-3-319-22005-5>. \n    Comprehensive documentation and vignettes guide users through implementation and interpretation of results.  "
  },
  {
    "id": 7350,
    "package_name": "SoilTesting",
    "title": "Organic Carbon and Plant Available Nutrient Contents in Soil",
    "description": "Testing of soil for the contents of organic carbon, and available macro- and micro-nutrients is a crucial part of soil fertility assessment. This package computes some routinely tested soil properties viz. organic carbon (C), total nitrogen (N), available N, mineral N, available phosphorus (P), available potassium (K), available iron (Fe), available zinc (Zn), available manganese (Mn), available copper (Cu), and available nickel (Ni) in soil based on laboratory analysis data obtained by most commonly followed protocols. Besides, it can also draw standard curves based on absorption/emission vs. concentration data, and give out unknown concentrations from absorption/emission readings.",
    "version": "0.1.0",
    "maintainer": "Bappa Das <bappa.iari.1989@gmail.com>",
    "author": "Bappa Das [aut, cre] (ORCID: <https://orcid.org/0000-0003-1286-1492>),\n  Debarup Das [aut, ctb] (ORCID: <https://orcid.org/0000-0002-0706-3392>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SoilTesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SoilTesting Organic Carbon and Plant Available Nutrient Contents in Soil Testing of soil for the contents of organic carbon, and available macro- and micro-nutrients is a crucial part of soil fertility assessment. This package computes some routinely tested soil properties viz. organic carbon (C), total nitrogen (N), available N, mineral N, available phosphorus (P), available potassium (K), available iron (Fe), available zinc (Zn), available manganese (Mn), available copper (Cu), and available nickel (Ni) in soil based on laboratory analysis data obtained by most commonly followed protocols. Besides, it can also draw standard curves based on absorption/emission vs. concentration data, and give out unknown concentrations from absorption/emission readings.  "
  },
  {
    "id": 7355,
    "package_name": "SongEvo",
    "title": "An Individual-Based Model of Bird Song Evolution",
    "description": "Simulates the cultural evolution of quantitative traits of bird song. 'SongEvo' is an individual- (agent-) based model. 'SongEvo' is spatially-explicit and can be parameterized with, and tested against, measured song data. Functions are available for model implementation, sensitivity analyses, parameter optimization, model validation, and hypothesis testing. ",
    "version": "1.0.0",
    "maintainer": "Raymond Danner <dannerR@uncw.edu>",
    "author": "Raymond Danner [aut, cre],\n  Elizabeth Derryberry [aut],\n  Graham Derryberry [aut],\n  Julie Danner [aut],\n  David Luther [aut]",
    "url": "",
    "bug_reports": "https://github.com/raydanner/SongEvo/issues",
    "repository": "https://cran.r-project.org/package=SongEvo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SongEvo An Individual-Based Model of Bird Song Evolution Simulates the cultural evolution of quantitative traits of bird song. 'SongEvo' is an individual- (agent-) based model. 'SongEvo' is spatially-explicit and can be parameterized with, and tested against, measured song data. Functions are available for model implementation, sensitivity analyses, parameter optimization, model validation, and hypothesis testing.   "
  },
  {
    "id": 7401,
    "package_name": "SpatialPack",
    "title": "Tools for Assessment the Association Between Two Spatial\nProcesses",
    "description": "Tools to assess the association between two spatial processes. Currently,\n  several methodologies are implemented: A modified t-test to perform hypothesis testing\n  about the independence between the processes, a suitable nonparametric correlation\n  coefficient, the codispersion coefficient, and an F test for assessing the multiple\n  correlation between one spatial process and several others. Functions for image\n  processing and computing the spatial association between images are also provided.\n  Functions contained in the package are intended to accompany Vallejos, R., Osorio, F.,\n  Bevilacqua, M. (2020). Spatial Relationships Between Two Georeferenced Variables:\n  With Applications in R. Springer, Cham <doi:10.1007/978-3-030-56681-4>.",
    "version": "0.4-1",
    "maintainer": "Felipe Osorio <felipe.osorios@usm.cl>",
    "author": "Felipe Osorio [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4675-5201>),\n  Ronny Vallejos [aut] (ORCID: <https://orcid.org/0000-0001-5519-0946>),\n  Francisco Cuevas [ctb],\n  Diego Mancilla [ctb]",
    "url": "http://spatialpack.mat.utfsm.cl",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpatialPack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpatialPack Tools for Assessment the Association Between Two Spatial\nProcesses Tools to assess the association between two spatial processes. Currently,\n  several methodologies are implemented: A modified t-test to perform hypothesis testing\n  about the independence between the processes, a suitable nonparametric correlation\n  coefficient, the codispersion coefficient, and an F test for assessing the multiple\n  correlation between one spatial process and several others. Functions for image\n  processing and computing the spatial association between images are also provided.\n  Functions contained in the package are intended to accompany Vallejos, R., Osorio, F.,\n  Bevilacqua, M. (2020). Spatial Relationships Between Two Georeferenced Variables:\n  With Applications in R. Springer, Cham <doi:10.1007/978-3-030-56681-4>.  "
  },
  {
    "id": 7412,
    "package_name": "SpecHelpers",
    "title": "Spectroscopy Related Utilities",
    "description": "Utility functions for spectroscopy. 1. Functions to simulate\n    spectra for use in teaching or testing. 2. Functions to process files created by\n    'LoggerPro' and 'SpectraSuite' software.",
    "version": "0.3.2",
    "maintainer": "Bryan A. Hanson <hanson@depauw.edu>",
    "author": "Bryan A. Hanson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3536-8246>)",
    "url": "https://github.com/bryanhanson/SpecHelpers",
    "bug_reports": "https://github.com/bryanhanson/SpecHelpers/issues",
    "repository": "https://cran.r-project.org/package=SpecHelpers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpecHelpers Spectroscopy Related Utilities Utility functions for spectroscopy. 1. Functions to simulate\n    spectra for use in teaching or testing. 2. Functions to process files created by\n    'LoggerPro' and 'SpectraSuite' software.  "
  },
  {
    "id": 7431,
    "package_name": "SpuriousMemory",
    "title": "Testing True Long Memory Against Spurious Long Memory",
    "description": "Implements a test for distinguishing between true long memory\n    and spurious long memory. Reference: Qu, Z. (2011). \"A Test Against\n    Spurious Long Memory.\" Journal of Business & Economic Statistics,\n    29(3), 423\u2013438. <doi:10.1198/jbes.2010.09153>.",
    "version": "1.0.0",
    "maintainer": "Zhongjun Qu <qu@bu.edu>",
    "author": "Zhongjun Qu [aut, cre],\n  Cheolju Kim [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SpuriousMemory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SpuriousMemory Testing True Long Memory Against Spurious Long Memory Implements a test for distinguishing between true long memory\n    and spurious long memory. Reference: Qu, Z. (2011). \"A Test Against\n    Spurious Long Memory.\" Journal of Business & Economic Statistics,\n    29(3), 423\u2013438. <doi:10.1198/jbes.2010.09153>.  "
  },
  {
    "id": 7470,
    "package_name": "StepwiseTest",
    "title": "Multiple Testing Method to Control Generalized Family-Wise Error\nRate and False Discovery Proportion",
    "description": "Collection of stepwise procedures to conduct multiple hypotheses testing. The details of the stepwise algorithm can be found in Romano and Wolf (2007) <DOI:10.1214/009053606000001622> and Hsu, Kuan, and Yen (2014) <DOI:10.1093/jjfinec/nbu014>.",
    "version": "1.0",
    "maintainer": "Kendro Vincent <vincent.kendro@gmail.com>",
    "author": "Yu-Chin Hsu and Kendro Vincent",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=StepwiseTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "StepwiseTest Multiple Testing Method to Control Generalized Family-Wise Error\nRate and False Discovery Proportion Collection of stepwise procedures to conduct multiple hypotheses testing. The details of the stepwise algorithm can be found in Romano and Wolf (2007) <DOI:10.1214/009053606000001622> and Hsu, Kuan, and Yen (2014) <DOI:10.1093/jjfinec/nbu014>.  "
  },
  {
    "id": 7500,
    "package_name": "SubgrpID",
    "title": "Patient Subgroup Identification for Clinical Drug Development",
    "description": "Implementation of Sequential BATTing (bootstrapping and aggregating of thresholds from trees) for developing threshold-based multivariate (prognostic/predictive) biomarker signatures. Variable selection is automatically built-in. Final signatures are returned with interaction plots for predictive signatures. Cross-validation performance evaluation and testing dataset results are also output. Detail algorithms are described in Huang et al (2017) <doi:10.1002/sim.7236>.",
    "version": "0.12",
    "maintainer": "Xin Huang <xin.huang@abbvie.com>",
    "author": "Xin Huang [aut, cre, cph],\n  Yan Sun [aut],\n  Saptarshi Chatterjee [aut],\n  Paul Trow [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SubgrpID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SubgrpID Patient Subgroup Identification for Clinical Drug Development Implementation of Sequential BATTing (bootstrapping and aggregating of thresholds from trees) for developing threshold-based multivariate (prognostic/predictive) biomarker signatures. Variable selection is automatically built-in. Final signatures are returned with interaction plots for predictive signatures. Cross-validation performance evaluation and testing dataset results are also output. Detail algorithms are described in Huang et al (2017) <doi:10.1002/sim.7236>.  "
  },
  {
    "id": 7523,
    "package_name": "SurrogateParadoxTest",
    "title": "Empirical Testing of Surrogate Paradox Assumptions",
    "description": "Provides functions to nonparametrically assess assumptions necessary to prevent the surrogate paradox through hypothesis tests of stochastic dominance, monotonicity of regression functions, and non-negative residual treatment effects. More details are available in Hsiao et al 2025 (under review). A tutorial for this package can be found at <https://laylaparast.com/home/SurrogateParadoxTest.html>.",
    "version": "2.0",
    "maintainer": "Emily Hsiao <ehsiao@utexas.edu>",
    "author": "Emily Hsiao [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SurrogateParadoxTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SurrogateParadoxTest Empirical Testing of Surrogate Paradox Assumptions Provides functions to nonparametrically assess assumptions necessary to prevent the surrogate paradox through hypothesis tests of stochastic dominance, monotonicity of regression functions, and non-negative residual treatment effects. More details are available in Hsiao et al 2025 (under review). A tutorial for this package can be found at <https://laylaparast.com/home/SurrogateParadoxTest.html>.  "
  },
  {
    "id": 7526,
    "package_name": "SurrogateSeq",
    "title": "Group Sequential Testing of a Treatment Effect Using a Surrogate\nMarker",
    "description": "Provides functions to implement group sequential procedures that allow for early stopping to declare efficacy using a surrogate marker and the possibility of futility stopping.   More details are available in: Parast, L. and Bartroff, J (2024) <doi:10.1093/biomtc/ujae108>. A tutorial for this package can be found at <https://laylaparast.com/home/SurrogateSeq.html>.",
    "version": "1.0",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "author": "Layla Parast [aut, cre],\n  Jay Bartroff [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SurrogateSeq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SurrogateSeq Group Sequential Testing of a Treatment Effect Using a Surrogate\nMarker Provides functions to implement group sequential procedures that allow for early stopping to declare efficacy using a surrogate marker and the possibility of futility stopping.   More details are available in: Parast, L. and Bartroff, J (2024) <doi:10.1093/biomtc/ujae108>. A tutorial for this package can be found at <https://laylaparast.com/home/SurrogateSeq.html>.  "
  },
  {
    "id": 7527,
    "package_name": "SurrogateTest",
    "title": "Early Testing for a Treatment Effect using Surrogate Marker\nInformation",
    "description": "Provides functions to test for a treatment effect in terms of the difference in survival between a treatment group and a control group using surrogate marker information obtained at some early time point in a time-to-event outcome setting. Nonparametric kernel estimation is used to estimate the test statistic and perturbation resampling is used for variance estimation. More details will be available in the future in: Parast L, Cai T, Tian L (2019) ``Using a Surrogate Marker for Early Testing of a Treatment Effect\" Biometrics, 75(4):1253-1263. <doi:10.1111/biom.13067>.",
    "version": "1.3",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "author": "Layla Parast",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SurrogateTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SurrogateTest Early Testing for a Treatment Effect using Surrogate Marker\nInformation Provides functions to test for a treatment effect in terms of the difference in survival between a treatment group and a control group using surrogate marker information obtained at some early time point in a time-to-event outcome setting. Nonparametric kernel estimation is used to estimate the test statistic and perturbation resampling is used for variance estimation. More details will be available in the future in: Parast L, Cai T, Tian L (2019) ``Using a Surrogate Marker for Early Testing of a Treatment Effect\" Biometrics, 75(4):1253-1263. <doi:10.1111/biom.13067>.  "
  },
  {
    "id": 7540,
    "package_name": "SurvTrunc",
    "title": "Analysis of Doubly Truncated Data",
    "description": "Package performs Cox regression and survival distribution function estimation when the survival times are subject to double truncation. In case that the survival and truncation times are quasi-independent, the estimation procedure for each method involves inverse probability weighting, where the weights correspond to the inverse of the selection probabilities and are estimated using the survival times and truncation times only. A test for checking this independence assumption is also included in this package. The functions available in this package for Cox regression, survival distribution function estimation, and testing independence under double truncation are based on the following methods, respectively: Rennert and Xie (2018) <doi:10.1111/biom.12809>, Shen (2010) <doi:10.1007/s10463-008-0192-2>, Martin and Betensky (2005) <doi:10.1198/016214504000001538>. When the survival times are dependent on at least one of the truncation times, an EM algorithm is employed to obtain point estimates for the regression coefficients. The standard errors are calculated using the bootstrap method. See Rennert and Xie (2022) <doi:10.1111/biom.13451>. Both the independent and dependent cases assume no censoring is present in the data. Please contact Lior Rennert <liorr@clemson.edu> for questions regarding function coxDT and Yidan Shi <yidan.shi@pennmedicine.upenn.edu> for questions regarding function coxDTdep.  ",
    "version": "0.2.0",
    "maintainer": "Lior Rennert <liorr@clemson.edu>",
    "author": "Lior Rennert <liorr@clemson.edu> and Yidan Shi <yidan.shi@pennmedicine.upenn.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SurvTrunc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SurvTrunc Analysis of Doubly Truncated Data Package performs Cox regression and survival distribution function estimation when the survival times are subject to double truncation. In case that the survival and truncation times are quasi-independent, the estimation procedure for each method involves inverse probability weighting, where the weights correspond to the inverse of the selection probabilities and are estimated using the survival times and truncation times only. A test for checking this independence assumption is also included in this package. The functions available in this package for Cox regression, survival distribution function estimation, and testing independence under double truncation are based on the following methods, respectively: Rennert and Xie (2018) <doi:10.1111/biom.12809>, Shen (2010) <doi:10.1007/s10463-008-0192-2>, Martin and Betensky (2005) <doi:10.1198/016214504000001538>. When the survival times are dependent on at least one of the truncation times, an EM algorithm is employed to obtain point estimates for the regression coefficients. The standard errors are calculated using the bootstrap method. See Rennert and Xie (2022) <doi:10.1111/biom.13451>. Both the independent and dependent cases assume no censoring is present in the data. Please contact Lior Rennert <liorr@clemson.edu> for questions regarding function coxDT and Yidan Shi <yidan.shi@pennmedicine.upenn.edu> for questions regarding function coxDTdep.    "
  },
  {
    "id": 7552,
    "package_name": "SyScSelection",
    "title": "Systematic Scenario Selection for Stress Testing",
    "description": "Quasi-Monte-Carlo algorithm for systematic generation of shock scenarios from an arbitrary multivariate elliptical distribution. The algorithm selects a systematic mesh of arbitrary fineness that approximately evenly covers an isoprobability ellipsoid in d dimensions (Flood, Mark D. & Korenko, George G. (2013) <doi:10.1080/14697688.2014.926018>).\n  This package is the 'R' analogy to the 'Matlab' code published by Flood & Korenko in above-mentioned paper.",
    "version": "1.0.2",
    "maintainer": "Merlin Kopfmann <mghncd+cran@posteo.jp>",
    "author": "Merlin Kopfmann",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=SyScSelection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SyScSelection Systematic Scenario Selection for Stress Testing Quasi-Monte-Carlo algorithm for systematic generation of shock scenarios from an arbitrary multivariate elliptical distribution. The algorithm selects a systematic mesh of arbitrary fineness that approximately evenly covers an isoprobability ellipsoid in d dimensions (Flood, Mark D. & Korenko, George G. (2013) <doi:10.1080/14697688.2014.926018>).\n  This package is the 'R' analogy to the 'Matlab' code published by Flood & Korenko in above-mentioned paper.  "
  },
  {
    "id": 7590,
    "package_name": "TDAkit",
    "title": "Toolkit for Topological Data Analysis",
    "description": "Topological data analysis studies structure and shape of the data using topological features. We provide a variety of algorithms to learn with persistent homology of the data based on functional summaries for clustering, hypothesis testing, visualization, and others. We refer to Wasserman (2018) <doi:10.1146/annurev-statistics-031017-100045> for a statistical perspective on the topic. ",
    "version": "0.1.3",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "author": "Kisung You [aut, cre] (ORCID: <https://orcid.org/0000-0002-8584-459X>),\n  Byeongsu Yu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TDAkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TDAkit Toolkit for Topological Data Analysis Topological data analysis studies structure and shape of the data using topological features. We provide a variety of algorithms to learn with persistent homology of the data based on functional summaries for clustering, hypothesis testing, visualization, and others. We refer to Wasserman (2018) <doi:10.1146/annurev-statistics-031017-100045> for a statistical perspective on the topic.   "
  },
  {
    "id": 7612,
    "package_name": "TGST",
    "title": "Targeted Gold Standard Testing",
    "description": "Functions for implementing the targeted gold standard (GS) testing. You provide the true disease or treatment failure status and the risk score, tell 'TGST' the availability of GS tests and which method to use, and it returns the optimal tripartite rules. Please refer to Liu et al. (2013) <doi:10.1080/01621459.2013.810149> for more details.",
    "version": "1.0",
    "maintainer": "Yizhen Xu <yizhen_xu@alumni.brown.edu>",
    "author": "Yizhen Xu [aut, cre],\n  Tao Liu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TGST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TGST Targeted Gold Standard Testing Functions for implementing the targeted gold standard (GS) testing. You provide the true disease or treatment failure status and the risk score, tell 'TGST' the availability of GS tests and which method to use, and it returns the optimal tripartite rules. Please refer to Liu et al. (2013) <doi:10.1080/01621459.2013.810149> for more details.  "
  },
  {
    "id": 7614,
    "package_name": "THETASVM",
    "title": "Time Series Forecasting using THETA-SVM Hybrid Model",
    "description": "Testing, Implementation, and Forecasting of the THETA-SVM hybrid model. The THETA-SVM hybrid model combines the distinct strengths of the THETA model and the Support Vector Machine (SVM) model for time series forecasting.For method details see Bhattacharyya et al. (2022) <doi:10.1007/s11071-021-07099-3>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "author": "Fasila K. P. [aut, ctb],\n  Mrinmoy Ray [aut, cre],\n  Rajeev Ranjan Kumar [aut, ctb],\n  K. N. Singh [aut, ctb],\n  Amrender Kumar [aut, ctb],\n  Santosha Rathod [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=THETASVM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "THETASVM Time Series Forecasting using THETA-SVM Hybrid Model Testing, Implementation, and Forecasting of the THETA-SVM hybrid model. The THETA-SVM hybrid model combines the distinct strengths of the THETA model and the Support Vector Machine (SVM) model for time series forecasting.For method details see Bhattacharyya et al. (2022) <doi:10.1007/s11071-021-07099-3>.  "
  },
  {
    "id": 7630,
    "package_name": "TOHM",
    "title": "Testing One Hypothesis Multiple Times",
    "description": "Approximations of global p-values when testing hypothesis in presence of non-identifiable nuisance parameters. The method relies on the Euler characteristic heuristic and the expected Euler characteristic is efficiently computed by  in Algeri and van Dyk (2018) <arXiv:1803.03858>. ",
    "version": "1.4",
    "maintainer": "Sara Algeri <salgeri@umn.edu>",
    "author": "Sara Algeri",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TOHM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TOHM Testing One Hypothesis Multiple Times Approximations of global p-values when testing hypothesis in presence of non-identifiable nuisance parameters. The method relies on the Euler characteristic heuristic and the expected Euler characteristic is efficiently computed by  in Algeri and van Dyk (2018) <arXiv:1803.03858>.   "
  },
  {
    "id": 7632,
    "package_name": "TOSI",
    "title": "Two-Directional Simultaneous Inference for High-Dimensional\nModels",
    "description": "A general framework of two directional simultaneous inference\n    is provided for high-dimensional as well as the fixed dimensional models with manifest\n    variable or latent variable structure, such as high-dimensional mean models, high-\n    dimensional sparse regression models, and high-dimensional latent factors models.\n    It is making the simultaneous inference on a set of parameters from two directions,\n    one is testing whether the estimated zero parameters indeed are zero and the other is\n    testing whether there exists zero in the parameter set of non-zero. More details can be \n    referred to Wei Liu, et al. (2022) <doi:10.48550/arXiv.2012.11100>.",
    "version": "0.3.0",
    "maintainer": "Wei Liu <weiliu@smail.swufe.edu.cn>",
    "author": "Wei Liu [aut, cre],\n  Huazhen Lin [aut]",
    "url": "https://github.com/feiyoung/TOSI",
    "bug_reports": "https://github.com/feiyoung/TOSI/issues",
    "repository": "https://cran.r-project.org/package=TOSI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TOSI Two-Directional Simultaneous Inference for High-Dimensional\nModels A general framework of two directional simultaneous inference\n    is provided for high-dimensional as well as the fixed dimensional models with manifest\n    variable or latent variable structure, such as high-dimensional mean models, high-\n    dimensional sparse regression models, and high-dimensional latent factors models.\n    It is making the simultaneous inference on a set of parameters from two directions,\n    one is testing whether the estimated zero parameters indeed are zero and the other is\n    testing whether there exists zero in the parameter set of non-zero. More details can be \n    referred to Wei Liu, et al. (2022) <doi:10.48550/arXiv.2012.11100>.  "
  },
  {
    "id": 7633,
    "package_name": "TOSTER",
    "title": "Two One-Sided Tests (TOST) Equivalence Testing",
    "description": "Two one-sided tests (TOST) procedure to test equivalence for t-tests, correlations, differences between proportions, and meta-analyses, including power analysis for t-tests and correlations. Allows you to specify equivalence bounds in raw scale units or in terms of effect sizes. See: Lakens (2017) <doi:10.1177/1948550617697177>.",
    "version": "0.8.6",
    "maintainer": "Aaron Caldwell <arcaldwell49@gmail.com>",
    "author": "Daniel Lakens [aut],\n  Aaron Caldwell [aut, cre]",
    "url": "https://aaroncaldwell.us/TOSTERpkg/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TOSTER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TOSTER Two One-Sided Tests (TOST) Equivalence Testing Two one-sided tests (TOST) procedure to test equivalence for t-tests, correlations, differences between proportions, and meta-analyses, including power analysis for t-tests and correlations. Allows you to specify equivalence bounds in raw scale units or in terms of effect sizes. See: Lakens (2017) <doi:10.1177/1948550617697177>.  "
  },
  {
    "id": 7635,
    "package_name": "TPAC",
    "title": "Tissue-Adjusted Pathway Analysis of Cancer (TPAC)",
    "description": "Contains logic for single sample gene set testing of cancer transcriptomic data with adjustment for normal tissue-specificity.\n  Frost, H. Robert (2023) \"Tissue-adjusted pathway analysis of cancer (TPAC)\" <doi:10.1101/2022.03.17.484779>.",
    "version": "0.3.0",
    "maintainer": "H. Robert Frost <rob.frost@dartmouth.edu>",
    "author": "H. Robert Frost [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TPAC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TPAC Tissue-Adjusted Pathway Analysis of Cancer (TPAC) Contains logic for single sample gene set testing of cancer transcriptomic data with adjustment for normal tissue-specificity.\n  Frost, H. Robert (2023) \"Tissue-adjusted pathway analysis of cancer (TPAC)\" <doi:10.1101/2022.03.17.484779>.  "
  },
  {
    "id": 7676,
    "package_name": "TSS.RESTREND",
    "title": "Time Series Segmentation of Residual Trends",
    "description": "Time Series Segmented Residual Trends is a method for the automated detection of land degradation from remotely sensed vegetation and climate datasets. TSS-RESTREND incorporates aspects of two existing degradation detection methods: RESTREND which is used to control for climate variability, and BFAST which is used to look for structural changes in the ecosystem. The full details of the testing and justification of the TSS-RESTREND method (version 0.1.02) are published in Burrell et al., (2017). <doi:10.1016/j.rse.2017.05.018>. The changes to the method introduced in version 0.2.03 focus on the inclusion  of temperature as an additional climate variable. This allows for land  degradation assessment in temperature limited drylands. A paper that details this work is currently under review. There are also a number of bug fixes and speed improvements. Version 0.3.0 introduces additional attribution for eCO2,  climate change and climate variability the details of which are in press in Burrell et al., (2020).  The version under active development and additional example scripts showing  how the package can be applied can be found at <https://github.com/ArdenB/TSSRESTREND>. ",
    "version": "0.3.1",
    "maintainer": "Arden Burrell <arden.burrell@gmail.com>",
    "author": "Arden Burrell [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TSS.RESTREND",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TSS.RESTREND Time Series Segmentation of Residual Trends Time Series Segmented Residual Trends is a method for the automated detection of land degradation from remotely sensed vegetation and climate datasets. TSS-RESTREND incorporates aspects of two existing degradation detection methods: RESTREND which is used to control for climate variability, and BFAST which is used to look for structural changes in the ecosystem. The full details of the testing and justification of the TSS-RESTREND method (version 0.1.02) are published in Burrell et al., (2017). <doi:10.1016/j.rse.2017.05.018>. The changes to the method introduced in version 0.2.03 focus on the inclusion  of temperature as an additional climate variable. This allows for land  degradation assessment in temperature limited drylands. A paper that details this work is currently under review. There are also a number of bug fixes and speed improvements. Version 0.3.0 introduces additional attribution for eCO2,  climate change and climate variability the details of which are in press in Burrell et al., (2020).  The version under active development and additional example scripts showing  how the package can be applied can be found at <https://github.com/ArdenB/TSSRESTREND>.   "
  },
  {
    "id": 7703,
    "package_name": "TableHC",
    "title": "Higher Criticism Test of Two Frequency Counts Tables",
    "description": "Higher Criticism (HC) test between two frequency tables. Test is based on an adaptation of the Tukey-Donoho-Jin HC statistic to testing frequency tables described in Kipnis (2019) <arXiv:1911.01208>. ",
    "version": "0.1.2",
    "maintainer": "Alon Kipnis <kipnisal@stanford.edu>",
    "author": "Alon Kipnis <kipnisal@stanford.edu>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TableHC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TableHC Higher Criticism Test of Two Frequency Counts Tables Higher Criticism (HC) test between two frequency tables. Test is based on an adaptation of the Tukey-Donoho-Jin HC statistic to testing frequency tables described in Kipnis (2019) <arXiv:1911.01208>.   "
  },
  {
    "id": 7712,
    "package_name": "TauStar",
    "title": "Efficient Computation and Testing of the Bergsma-Dassios Sign\nCovariance",
    "description": "Computes the t* statistic corresponding to the tau* population\n          coefficient introduced by Bergsma and Dassios (2014) <DOI:10.3150/13-BEJ514>\n          and does so in O(n^2) time following the algorithm of Heller and\n          Heller (2016) <DOI:10.48550/arXiv.1605.08732> building off of the work of Weihs,\n          Drton, and Leung (2016) <DOI:10.1007/s00180-015-0639-x>. Also allows for\n          independence testing using the asymptotic distribution of t* as described by\n          Nandy, Weihs, and Drton (2016) <DOI:10.1214/16-EJS1166>.",
    "version": "1.1.8",
    "maintainer": "Julian D. Karch <j.d.karch@fsw.leidenuniv.nl>",
    "author": "Luca Weihs [aut],\n  Emin Martinian [ctb] (Created the red-black tree library included in\n    package.),\n  Julian D. Karch [cre] (ORCID: <https://orcid.org/0000-0002-1625-2822>)",
    "url": "https://github.com/karchjd/TauStar",
    "bug_reports": "https://github.com/karchjd/TauStar/issues",
    "repository": "https://cran.r-project.org/package=TauStar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TauStar Efficient Computation and Testing of the Bergsma-Dassios Sign\nCovariance Computes the t* statistic corresponding to the tau* population\n          coefficient introduced by Bergsma and Dassios (2014) <DOI:10.3150/13-BEJ514>\n          and does so in O(n^2) time following the algorithm of Heller and\n          Heller (2016) <DOI:10.48550/arXiv.1605.08732> building off of the work of Weihs,\n          Drton, and Leung (2016) <DOI:10.1007/s00180-015-0639-x>. Also allows for\n          independence testing using the asymptotic distribution of t* as described by\n          Nandy, Weihs, and Drton (2016) <DOI:10.1214/16-EJS1166>.  "
  },
  {
    "id": 7734,
    "package_name": "TestCor",
    "title": "FWER and FDR Controlling Procedures for Multiple Correlation\nTests",
    "description": "\n       Different multiple testing procedures for correlation tests are implemented. These procedures were shown to theoretically control asymptotically the Family Wise Error Rate (Roux (2018) <https://tel.archives-ouvertes.fr/tel-01971574v1>) or the False Discovery Rate (Cai & Liu (2016) <doi:10.1080/01621459.2014.999157>). The package gather four test statistics used in correlation testing, four FWER procedures with either single step or stepdown versions, and four FDR procedures.",
    "version": "0.0.2.2",
    "maintainer": "Gannaz Irene <irene.gannaz@insa-lyon.fr>",
    "author": "Gannaz Irene [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TestCor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TestCor FWER and FDR Controlling Procedures for Multiple Correlation\nTests \n       Different multiple testing procedures for correlation tests are implemented. These procedures were shown to theoretically control asymptotically the Family Wise Error Rate (Roux (2018) <https://tel.archives-ouvertes.fr/tel-01971574v1>) or the False Discovery Rate (Cai & Liu (2016) <doi:10.1080/01621459.2014.999157>). The package gather four test statistics used in correlation testing, four FWER procedures with either single step or stepdown versions, and four FDR procedures.  "
  },
  {
    "id": 7740,
    "package_name": "TestGenerator",
    "title": "Integration Unit Tests for Pharmacoepidemiological Studies",
    "description": "An R interface to load testing data in the 'OMOP' Common Data Model ('CDM'). An input file, csv or xlsx, can be converted to a 'CDMConnector' object. This object can be used to execute and test studies that use the 'CDM' <https://www.ohdsi.org/data-standardization/>.",
    "version": "0.4.0",
    "maintainer": "Ger Inberg <g.inberg@erasmusmc.nl>",
    "author": "Cesar Barboza [aut] (ORCID: <https://orcid.org/0009-0002-4453-3071>),\n  Ioanna Nika [aut],\n  Ger Inberg [aut, cre] (ORCID: <https://orcid.org/0000-0001-8993-8748>),\n  Adam Black [aut] (ORCID: <https://orcid.org/0000-0001-5576-8701>)",
    "url": "https://github.com/darwin-eu/TestGenerator,\nhttps://darwin-eu.github.io/TestGenerator/",
    "bug_reports": "https://github.com/darwin-eu/TestGenerator/issues",
    "repository": "https://cran.r-project.org/package=TestGenerator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TestGenerator Integration Unit Tests for Pharmacoepidemiological Studies An R interface to load testing data in the 'OMOP' Common Data Model ('CDM'). An input file, csv or xlsx, can be converted to a 'CDMConnector' object. This object can be used to execute and test studies that use the 'CDM' <https://www.ohdsi.org/data-standardization/>.  "
  },
  {
    "id": 7741,
    "package_name": "TestIndVars",
    "title": "Testing the Independence of Variables for Specific Covariance\nStructures",
    "description": "Test the nullity of covariances, in a set of variables, using a simple univariate procedure. See Marques, Diago, Norouzirad, Bispo (2023) <doi:10.1002/mma.9130>.",
    "version": "0.1.0",
    "maintainer": "Mina Norouzirad <mina.norouzirad@gmail.com>",
    "author": "Filipe J. Marques [aut] (ORCID:\n    <https://orcid.org/0000-0001-6453-6558>),\n  Mina Norouzirad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0311-6888>),\n  Joana Diogo [ctb],\n  Regina Bispo [ctb] (ORCID: <https://orcid.org/0000-0002-6723-2557>),\n  FCT, I.P. [fnd] (under the scope of the projects UIDB/00297/2020 and\n    UIDP/00297/2020 (NovaMath))",
    "url": "https://github.com/mnrzrad/TestIndVars",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TestIndVars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TestIndVars Testing the Independence of Variables for Specific Covariance\nStructures Test the nullity of covariances, in a set of variables, using a simple univariate procedure. See Marques, Diago, Norouzirad, Bispo (2023) <doi:10.1002/mma.9130>.  "
  },
  {
    "id": 7744,
    "package_name": "TestsSymmetry",
    "title": "Tests for Symmetry when the Center of Symmetry is Unknown",
    "description": "Provides functionality of a statistical testing implementation whether a dataset comes from a symmetric distribution when the center of symmetry is unknown, including Wilcoxon test and sign test procedure. In addition, sample size determination for both tests is provided. The Wilcoxon test procedure is described in Vexler et al. (2023) <https://www.sciencedirect.com/science/article/abs/pii/S0167947323000579>, and the sign test is outlined in Gastwirth (1971) <https://www.jstor.org/stable/2284233>. ",
    "version": "1.0.0",
    "maintainer": "Jiaojiao Zhou <woerge99@gmail.com>",
    "author": "Jiaojiao Zhou [aut, cre],\n  Xinyu Gao [aut],\n  Albert Vexler [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TestsSymmetry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TestsSymmetry Tests for Symmetry when the Center of Symmetry is Unknown Provides functionality of a statistical testing implementation whether a dataset comes from a symmetric distribution when the center of symmetry is unknown, including Wilcoxon test and sign test procedure. In addition, sample size determination for both tests is provided. The Wilcoxon test procedure is described in Vexler et al. (2023) <https://www.sciencedirect.com/science/article/abs/pii/S0167947323000579>, and the sign test is outlined in Gastwirth (1971) <https://www.jstor.org/stable/2284233>.   "
  },
  {
    "id": 7795,
    "package_name": "Trading",
    "title": "Trade Objects, Advanced Correlation & Beta Estimates, Betting\nStrategies",
    "description": "Contains performance analysis metrics of track records including entropy-based\n            correlation and dynamic beta based on a state/space algorithm. The normalized sample entropy method\n            has been implemented which produces accurate entropy estimation even on smaller datasets.\n            On a separate stream, trades from the five major assets classes and also\n            functionality to use pricing curves, rating tables, Credit Support Annex and add-on tables. The\n            implementation follows an object oriented logic whereby each trade inherits from\n            more abstract classes while also the curves/tables are objects. Furthermore, odds calculators\n            and P&L back-testing functionality has been implemented for the most widely used betting/trading\n            strategies including martingale, 'DAlembert', 'Labouchere' and Fibonacci. Back testing has also been included for the 'EuroMillions',\n      \t\t\tthe 'EuroJackpot', the UK Lotto, the Set For Life and the UK 'ThunderBall' lotteries.\n      \t\t\tFurthermore, some basic functionality about climate risk has been included. ",
    "version": "3.2",
    "maintainer": "Tasos Grivas <info@openriskcalculator.com>",
    "author": "Tasos Grivas [aut, cre]",
    "url": "https://openriskcalculator.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Trading",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Trading Trade Objects, Advanced Correlation & Beta Estimates, Betting\nStrategies Contains performance analysis metrics of track records including entropy-based\n            correlation and dynamic beta based on a state/space algorithm. The normalized sample entropy method\n            has been implemented which produces accurate entropy estimation even on smaller datasets.\n            On a separate stream, trades from the five major assets classes and also\n            functionality to use pricing curves, rating tables, Credit Support Annex and add-on tables. The\n            implementation follows an object oriented logic whereby each trade inherits from\n            more abstract classes while also the curves/tables are objects. Furthermore, odds calculators\n            and P&L back-testing functionality has been implemented for the most widely used betting/trading\n            strategies including martingale, 'DAlembert', 'Labouchere' and Fibonacci. Back testing has also been included for the 'EuroMillions',\n      \t\t\tthe 'EuroJackpot', the UK Lotto, the Set For Life and the UK 'ThunderBall' lotteries.\n      \t\t\tFurthermore, some basic functionality about climate risk has been included.   "
  },
  {
    "id": 7810,
    "package_name": "TreeDimensionTest",
    "title": "Trajectory Presence and Heterogeneity in Multivariate Data",
    "description": "Testing for trajectory presence and heterogeneity on\n multivariate data. Two statistical methods (Tenha & Song 2022) \n <doi:10.1371/journal.pcbi.1009829> are implemented. The tree dimension\n test quantifies the statistical evidence for trajectory presence. The\n subset specificity measure summarizes pattern heterogeneity using the\n minimum subtree cover. There is no user tunable parameters for either\n method. Examples are included to illustrate how to use the methods on \n single-cell data for studying gene and pathway expression dynamics and\n pathway expression specificity.",
    "version": "0.0.2",
    "maintainer": "Joe Song <joemsong@cs.nmsu.edu>",
    "author": "Lovemore Tenha [aut] (ORCID: <https://orcid.org/0000-0001-9705-2023>),\n  Joe Song [aut, cre] (ORCID: <https://orcid.org/0000-0002-6883-6547>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TreeDimensionTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TreeDimensionTest Trajectory Presence and Heterogeneity in Multivariate Data Testing for trajectory presence and heterogeneity on\n multivariate data. Two statistical methods (Tenha & Song 2022) \n <doi:10.1371/journal.pcbi.1009829> are implemented. The tree dimension\n test quantifies the statistical evidence for trajectory presence. The\n subset specificity measure summarizes pattern heterogeneity using the\n minimum subtree cover. There is no user tunable parameters for either\n method. Examples are included to illustrate how to use the methods on \n single-cell data for studying gene and pathway expression dynamics and\n pathway expression specificity.  "
  },
  {
    "id": 7813,
    "package_name": "TreeOrderTests",
    "title": "Tests for Tree Ordered Alternatives in One-Way ANOVA",
    "description": "Implements a likelihood ratio test and two pairwise standardized mean difference tests for testing equality of means against tree ordered alternatives in one-way ANOVA. The null hypothesis assumes all group means are equal, while the alternative assumes the control mean is less than or equal to each treatment mean with at least one strict inequality. Inputs are a list of numeric vectors (groups) and a significance level; outputs include the test statistic, critical value, and decision. Methods described in \"Testing Against Tree Ordered Alternatives in One-way ANOVA\" <doi:10.48550/arXiv.2507.17229>.",
    "version": "0.1.0",
    "maintainer": "Subha Halder <sb.halder123456@gmail.com>",
    "author": "Subha Halder [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TreeOrderTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TreeOrderTests Tests for Tree Ordered Alternatives in One-Way ANOVA Implements a likelihood ratio test and two pairwise standardized mean difference tests for testing equality of means against tree ordered alternatives in one-way ANOVA. The null hypothesis assumes all group means are equal, while the alternative assumes the control mean is less than or equal to each treatment mean with at least one strict inequality. Inputs are a list of numeric vectors (groups) and a significance level; outputs include the test statistic, critical value, and decision. Methods described in \"Testing Against Tree Ordered Alternatives in One-way ANOVA\" <doi:10.48550/arXiv.2507.17229>.  "
  },
  {
    "id": 7829,
    "package_name": "TrialSimulator",
    "title": "Clinical Trial Simulator",
    "description": "Simulate phase II and/or phase III clinical trials. It supports various types of endpoints and adaptive strategies. Tools for carrying out graphical testing procedure and combination test under group sequential design are also provided. ",
    "version": "1.3.0",
    "maintainer": "Han Zhang <zhangh.ustc@gmail.com>",
    "author": "Han Zhang [cre, aut]",
    "url": "https://zhangh12.github.io/TrialSimulator/",
    "bug_reports": "https://github.com/zhangh12/TrialSimulator/issues",
    "repository": "https://cran.r-project.org/package=TrialSimulator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TrialSimulator Clinical Trial Simulator Simulate phase II and/or phase III clinical trials. It supports various types of endpoints and adaptive strategies. Tools for carrying out graphical testing procedure and combination test under group sequential design are also provided.   "
  },
  {
    "id": 7870,
    "package_name": "UNPaC",
    "title": "Non-Parametric Cluster Significance Testing with Reference to a\nUnimodal Null Distribution",
    "description": "Assess the significance of identified clusters and estimates the true number of clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution which preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation and a Gaussian copula framework. A dimension reduction strategy and sparse covariance estimation optimize this method for the high-dimensional, low-sample size setting. This method is described in Helgeson, Vock, and Bair (2021) <doi:10.1111/biom.13376>.",
    "version": "1.1.1",
    "maintainer": "Erika S. Helgeson <helge@umn.edu>",
    "author": "Erika S. Helgeson, David Vock, and Eric Bair",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UNPaC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UNPaC Non-Parametric Cluster Significance Testing with Reference to a\nUnimodal Null Distribution Assess the significance of identified clusters and estimates the true number of clusters by comparing the explained variation due to the clustering from the original data to that produced by clustering a unimodal reference distribution which preserves the covariance structure in the data. The reference distribution is generated using kernel density estimation and a Gaussian copula framework. A dimension reduction strategy and sparse covariance estimation optimize this method for the high-dimensional, low-sample size setting. This method is described in Helgeson, Vock, and Bair (2021) <doi:10.1111/biom.13376>.  "
  },
  {
    "id": 7891,
    "package_name": "UniExactFunTest",
    "title": "Uniform Exact Functional Tests for Contingency Tables",
    "description": "Testing whether two discrete variables have a functional\n  relationship under null distributions where the two variables are\n  statistically independent with fixed marginal counts.\n  The fast enumeration algorithm was based on (Nguyen et al. 2020) <doi:10.24963/ijcai.2020/372>.",
    "version": "1.0.1",
    "maintainer": "Yiyi Li <gtarex@nmsu.edu>",
    "author": "Yiyi Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-8859-3987>),\n  Joe Song [aut] (ORCID: <https://orcid.org/0000-0002-6883-6547>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=UniExactFunTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "UniExactFunTest Uniform Exact Functional Tests for Contingency Tables Testing whether two discrete variables have a functional\n  relationship under null distributions where the two variables are\n  statistically independent with fixed marginal counts.\n  The fast enumeration algorithm was based on (Nguyen et al. 2020) <doi:10.24963/ijcai.2020/372>.  "
  },
  {
    "id": 7909,
    "package_name": "VALIDICLUST",
    "title": "VALID Inference for Clusters Separation Testing",
    "description": "Given a partition resulting from any clustering algorithm, the implemented tests allow valid post-clustering inference by testing if a given variable significantly separates two of the estimated clusters. \n             Methods are detailed in: Hivert B, Agniel D, Thiebaut R & Hejblum BP (2022). \n             \"Post-clustering difference testing: valid inference and practical considerations\", <arXiv:2210.13172>.",
    "version": "0.1.0",
    "maintainer": "Benjamin Hivert <benjamin.hivert@u-bordeaux.fr>",
    "author": "Benjamin Hivert",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VALIDICLUST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VALIDICLUST VALID Inference for Clusters Separation Testing Given a partition resulting from any clustering algorithm, the implemented tests allow valid post-clustering inference by testing if a given variable significantly separates two of the estimated clusters. \n             Methods are detailed in: Hivert B, Agniel D, Thiebaut R & Hejblum BP (2022). \n             \"Post-clustering difference testing: valid inference and practical considerations\", <arXiv:2210.13172>.  "
  },
  {
    "id": 7911,
    "package_name": "VAR.etp",
    "title": "VAR Modelling: Estimation, Testing, and Prediction",
    "description": "A collection of the functions for estimation, hypothesis testing, prediction for stationary vector autoregressive models.",
    "version": "1.1",
    "maintainer": "Jae H. Kim <jaekim8080@gmail.com>",
    "author": "Jae. H. Kim",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VAR.etp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VAR.etp VAR Modelling: Estimation, Testing, and Prediction A collection of the functions for estimation, hypothesis testing, prediction for stationary vector autoregressive models.  "
  },
  {
    "id": 7989,
    "package_name": "VizTest",
    "title": "Optimal Confidence Intervals for Visual Testing",
    "description": "Identifies the optimal confidence level to represent the results of a set of pairwise tests as suggested by Armstrong and Poirier (2025) <doi:10.1017/pan.2024.24>.  ",
    "version": "0.6",
    "maintainer": "Dave Armstrong <davearmstrong.ps@gmail.com>",
    "author": "Dave Armstrong [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9358-2489>),\n  William Poirier [aut] (ORCID: <https://orcid.org/0000-0002-3274-1351>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=VizTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "VizTest Optimal Confidence Intervals for Visual Testing Identifies the optimal confidence level to represent the results of a set of pairwise tests as suggested by Armstrong and Poirier (2025) <doi:10.1017/pan.2024.24>.    "
  },
  {
    "id": 8045,
    "package_name": "WaveletRF",
    "title": "Wavelet-RF Hybrid Model for Time Series Forecasting",
    "description": "The Wavelet Decomposition followed by Random Forest Regression (RF) models have been applied for time series forecasting. The maximum overlap discrete wavelet transform (MODWT) algorithm was chosen as it works for any length of the series. The series is first divided into training and testing sets. In each of the wavelet decomposed series, the  supervised machine learning approach namely random forest was employed to train the model. This package also provides accuracy metrics in the form of Root Mean Square Error (RMSE) and Mean Absolute Prediction Error (MAPE). This package is based on the algorithm of Ding et al. (2021) <DOI: 10.1007/s11356-020-12298-3>.",
    "version": "0.1.0",
    "maintainer": "Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "author": "Ranjit Kumar Paul [aut, cre],\n  Md Yeasin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WaveletRF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WaveletRF Wavelet-RF Hybrid Model for Time Series Forecasting The Wavelet Decomposition followed by Random Forest Regression (RF) models have been applied for time series forecasting. The maximum overlap discrete wavelet transform (MODWT) algorithm was chosen as it works for any length of the series. The series is first divided into training and testing sets. In each of the wavelet decomposed series, the  supervised machine learning approach namely random forest was employed to train the model. This package also provides accuracy metrics in the form of Root Mean Square Error (RMSE) and Mean Absolute Prediction Error (MAPE). This package is based on the algorithm of Ding et al. (2021) <DOI: 10.1007/s11356-020-12298-3>.  "
  },
  {
    "id": 8053,
    "package_name": "WeatherSentiment",
    "title": "Comprehensive Analysis of Tweet Sentiments and Weather Data",
    "description": "A comprehensive suite of functions for processing,\n  analyzing, and visualizing textual data from tweets is offered.  \n  Users can clean tweets, analyze their sentiments, visualize data, \n  and examine the correlation between sentiments and environmental \n  data such as weather conditions. Main features include text processing,  \n  sentiment analysis, data visualization, correlation analysis, and \n  synthetic data generation. Text processing involves cleaning and preparing  \n  tweets by removing textual noise and irrelevant words. Sentiment analysis \n  extracts and accurately analyzes sentiments from tweet texts using advanced \n  algorithms. Data visualization creates various charts like word clouds\n  and sentiment polarity graphs for visual representation of data. Correlation \n  analysis examines and calculates the correlation between tweet sentiments \n  and environmental variables such as weather conditions. Additionally, \n  random tweets can be generated for testing and evaluating the performance \n  of analyses, empowering users to effectively analyze and interpret 'Twitter' \n  data for research and commercial purposes.",
    "version": "1.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Andriette Bekker [aut],\n  Mohammad Arashi [aut],\n  Leila Marvian Mashhad [aut, cre],\n  Priyanka Nagar [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WeatherSentiment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WeatherSentiment Comprehensive Analysis of Tweet Sentiments and Weather Data A comprehensive suite of functions for processing,\n  analyzing, and visualizing textual data from tweets is offered.  \n  Users can clean tweets, analyze their sentiments, visualize data, \n  and examine the correlation between sentiments and environmental \n  data such as weather conditions. Main features include text processing,  \n  sentiment analysis, data visualization, correlation analysis, and \n  synthetic data generation. Text processing involves cleaning and preparing  \n  tweets by removing textual noise and irrelevant words. Sentiment analysis \n  extracts and accurately analyzes sentiments from tweet texts using advanced \n  algorithms. Data visualization creates various charts like word clouds\n  and sentiment polarity graphs for visual representation of data. Correlation \n  analysis examines and calculates the correlation between tweet sentiments \n  and environmental variables such as weather conditions. Additionally, \n  random tweets can be generated for testing and evaluating the performance \n  of analyses, empowering users to effectively analyze and interpret 'Twitter' \n  data for research and commercial purposes.  "
  },
  {
    "id": 8060,
    "package_name": "WeibullR.ALT",
    "title": "Accelerated Life Testing Using 'WeibullR'",
    "description": "Graphical data analysis of accelerated life tests. Methods derived from  Wayne Nelson (1990, ISBN: 9780471522775), William Q. Meeker and  Lois A. Escobar (1998, ISBN: 1-471-14328-6).",
    "version": "0.7.2",
    "maintainer": "Jacob Ormerod <jake@openreliability.org>",
    "author": "David Silkworth [aut],\n  Jacob Ormerod [cre],\n  OpenReliability.org [cph]",
    "url": "http://www.openreliability.org/weibull-r-weibull-analysis-on-r/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WeibullR.ALT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WeibullR.ALT Accelerated Life Testing Using 'WeibullR' Graphical data analysis of accelerated life tests. Methods derived from  Wayne Nelson (1990, ISBN: 9780471522775), William Q. Meeker and  Lois A. Escobar (1998, ISBN: 1-471-14328-6).  "
  },
  {
    "id": 8067,
    "package_name": "WeightedPortTest",
    "title": "Weighted Portmanteau Tests for Time Series Goodness-of-Fit",
    "description": "An implementation of the Weighted Portmanteau Tests described\n      in \"New Weighted Portmanteau Statistics for Time Series Goodness-of-Fit Testing\"\n      published by the Journal of the American Statistical Association, Volume 107, \n      Issue 498, pages 777-787, 2012.",
    "version": "1.1",
    "maintainer": "Thomas J. Fisher <fishert4@miamioh.edu>",
    "author": "Thomas J. Fisher [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5885-7646>),\n  Colin M. Gallagher [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WeightedPortTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WeightedPortTest Weighted Portmanteau Tests for Time Series Goodness-of-Fit An implementation of the Weighted Portmanteau Tests described\n      in \"New Weighted Portmanteau Statistics for Time Series Goodness-of-Fit Testing\"\n      published by the Journal of the American Statistical Association, Volume 107, \n      Issue 498, pages 777-787, 2012.  "
  },
  {
    "id": 8071,
    "package_name": "WhatIf",
    "title": "Software for Evaluating Counterfactuals",
    "description": "Inferences about counterfactuals are essential for prediction,\n      answering what if questions, and estimating causal effects.\n      However, when the counterfactuals posed are too far from the data at\n      hand, conclusions drawn from well-specified statistical analyses\n      become based largely on speculation hidden in convenient modeling\n      assumptions that few would be willing to defend. Unfortunately,\n      standard statistical approaches assume the veracity of the model\n      rather than revealing the degree of model-dependence, which makes this\n      problem hard to detect. WhatIf offers easy-to-apply methods to\n      evaluate counterfactuals that do not require sensitivity testing over\n      specified classes of models. If an analysis fails the tests offered\n      here, then we know that substantive inferences will be sensitive to at\n      least some modeling choices that are not based on empirical evidence,\n      no matter what method of inference one chooses to use. WhatIf\n      implements the methods for evaluating counterfactuals discussed in\n      Gary King and Langche Zeng, 2006, \"The Dangers of Extreme\n      Counterfactuals,\" Political Analysis 14 (2) <DOI:10.1093/pan/mpj004>; \n      and Gary King and Langche Zeng, 2007, \"When Can History Be Our Guide? The \n      Pitfalls of Counterfactual Inference,\" International Studies \n      Quarterly 51 (March) <DOI:10.1111/j.1468-2478.2007.00445.x>.",
    "version": "1.5-10",
    "maintainer": "Soubhik Barari <soubhikbarari@gmail.com>",
    "author": "Heather Stoll <hstoll@polsci.ucsb.edu>, Gary King\n        <king@harvard.edu>, Langche Zeng <zeng@ucsd.edu>,\n        Christopher Gandrud <zelig.zee@gmail.com>, Ben Sabath",
    "url": "https://gking.harvard.edu/whatif",
    "bug_reports": "https://github.com/IQSS/WhatIf/issues",
    "repository": "https://cran.r-project.org/package=WhatIf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WhatIf Software for Evaluating Counterfactuals Inferences about counterfactuals are essential for prediction,\n      answering what if questions, and estimating causal effects.\n      However, when the counterfactuals posed are too far from the data at\n      hand, conclusions drawn from well-specified statistical analyses\n      become based largely on speculation hidden in convenient modeling\n      assumptions that few would be willing to defend. Unfortunately,\n      standard statistical approaches assume the veracity of the model\n      rather than revealing the degree of model-dependence, which makes this\n      problem hard to detect. WhatIf offers easy-to-apply methods to\n      evaluate counterfactuals that do not require sensitivity testing over\n      specified classes of models. If an analysis fails the tests offered\n      here, then we know that substantive inferences will be sensitive to at\n      least some modeling choices that are not based on empirical evidence,\n      no matter what method of inference one chooses to use. WhatIf\n      implements the methods for evaluating counterfactuals discussed in\n      Gary King and Langche Zeng, 2006, \"The Dangers of Extreme\n      Counterfactuals,\" Political Analysis 14 (2) <DOI:10.1093/pan/mpj004>; \n      and Gary King and Langche Zeng, 2007, \"When Can History Be Our Guide? The \n      Pitfalls of Counterfactual Inference,\" International Studies \n      Quarterly 51 (March) <DOI:10.1111/j.1468-2478.2007.00445.x>.  "
  },
  {
    "id": 8112,
    "package_name": "YPInterimTesting",
    "title": "Interim Monitoring Using Adaptively Weighted Log-Rank Test in\nClinical Trials",
    "description": "For any spending function specified by the user, this \n    package provides corresponding boundaries for interim testing using\n    the adaptively weighted log-rank test developed by Yang and Prentice\n    (2010 <doi:10.1111/j.1541-0420.2009.01243.x>). \n    The package uses a re-sampling method to obtain stopping boundaries \n    at the interim looks.The output consists of stopping boundaries \n    and observed values of the test statistics at the interim looks, \n    along with nominal p-values defined as the probability of the test \n    exceeding the specific observed test statistic value or critical \n    value, regardless of the test behavior at other looks. \n    The asymptotic validity of the stopping boundaries is established\n    in Yang (2018 <doi:10.1002/sim.7958>).",
    "version": "1.0.3",
    "maintainer": "Daewoo Pak <heavyrain.pak@gmail.com>",
    "author": "Daewoo Pak and Song Yang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=YPInterimTesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "YPInterimTesting Interim Monitoring Using Adaptively Weighted Log-Rank Test in\nClinical Trials For any spending function specified by the user, this \n    package provides corresponding boundaries for interim testing using\n    the adaptively weighted log-rank test developed by Yang and Prentice\n    (2010 <doi:10.1111/j.1541-0420.2009.01243.x>). \n    The package uses a re-sampling method to obtain stopping boundaries \n    at the interim looks.The output consists of stopping boundaries \n    and observed values of the test statistics at the interim looks, \n    along with nominal p-values defined as the probability of the test \n    exceeding the specific observed test statistic value or critical \n    value, regardless of the test behavior at other looks. \n    The asymptotic validity of the stopping boundaries is established\n    in Yang (2018 <doi:10.1002/sim.7958>).  "
  },
  {
    "id": 8125,
    "package_name": "ZIBR",
    "title": "A Zero-Inflated Beta Random Effect Model",
    "description": "A two-part zero-inflated Beta regression model with random \n    effects (ZIBR) for testing the association between microbial abundance \n    and clinical covariates for longitudinal microbiome data. Eric Z. Chen \n    and Hongzhe Li (2016) <doi:10.1093/bioinformatics/btw308>.",
    "version": "1.0.2",
    "maintainer": "Charlie Bushman <ctbushman@gmail.com>",
    "author": "Eric Zhang Chen [aut, cph],\n  Charlie Bushman [cre]",
    "url": "https://github.com/PennChopMicrobiomeProgram/ZIBR",
    "bug_reports": "https://github.com/PennChopMicrobiomeProgram/ZIBR/issues",
    "repository": "https://cran.r-project.org/package=ZIBR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIBR A Zero-Inflated Beta Random Effect Model A two-part zero-inflated Beta regression model with random \n    effects (ZIBR) for testing the association between microbial abundance \n    and clinical covariates for longitudinal microbiome data. Eric Z. Chen \n    and Hongzhe Li (2016) <doi:10.1093/bioinformatics/btw308>.  "
  },
  {
    "id": 8134,
    "package_name": "ZIPG",
    "title": "Zero-Inflated Poisson-Gamma Regression",
    "description": "We provide a flexible Zero-inflated Poisson-Gamma Model (ZIPG) by connecting both the mean abundance and the variability to different covariates, and build valid statistical inference procedures for both parameter estimation and hypothesis testing. These functions can be used to analyze microbiome count data with zero-inflation and overdispersion. The model is discussed in Jiang et al (2023) <doi:10.1080/01621459.2022.2151447>.",
    "version": "1.1",
    "maintainer": "Roulan Jiang <roulan2000@gmail.com>",
    "author": "Roulan Jiang [aut, cre],\n  Tianying Wang [aut]",
    "url": "https://github.com/roulan2000/ZIPG",
    "bug_reports": "https://github.com/roulan2000/ZIPG/issues",
    "repository": "https://cran.r-project.org/package=ZIPG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ZIPG Zero-Inflated Poisson-Gamma Regression We provide a flexible Zero-inflated Poisson-Gamma Model (ZIPG) by connecting both the mean abundance and the variability to different covariates, and build valid statistical inference procedures for both parameter estimation and hypothesis testing. These functions can be used to analyze microbiome count data with zero-inflation and overdispersion. The model is discussed in Jiang et al (2023) <doi:10.1080/01621459.2022.2151447>.  "
  },
  {
    "id": 8144,
    "package_name": "aIc",
    "title": "Testing for Compositional Pathologies in Datasets",
    "description": "A set of tests for compositional pathologies. Tests for coherence of correlations with aIc.coherent() as suggested by (Erb et al. (2020) <doi:10.1016/j.acags.2020.100026>),  compositional dominance of distance with aIc.dominant(), compositional perturbation invariance with aIc.perturb() as suggested by (Aitchison (1992) <doi:10.1007/BF00891269>) and singularity of the covariation matrix with aIc.singular(). Currently tests five data transformations: prop, clr, TMM, TMMwsp, and RLE from the R packages 'ALDEx2', 'edgeR' and 'DESeq2' (Fernandes et al (2014) <doi:10.1186/2049-2618-2-15>, Anders et al. (2013)<doi:10.1038/nprot.2013.099>).",
    "version": "1.0",
    "maintainer": "Greg Gloor <ggloor@uwo.ca>",
    "author": "Greg Gloor",
    "url": "https://github.com/ggloor/aIc",
    "bug_reports": "https://github.com/ggloor/aIc/issues",
    "repository": "https://cran.r-project.org/package=aIc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aIc Testing for Compositional Pathologies in Datasets A set of tests for compositional pathologies. Tests for coherence of correlations with aIc.coherent() as suggested by (Erb et al. (2020) <doi:10.1016/j.acags.2020.100026>),  compositional dominance of distance with aIc.dominant(), compositional perturbation invariance with aIc.perturb() as suggested by (Aitchison (1992) <doi:10.1007/BF00891269>) and singularity of the covariation matrix with aIc.singular(). Currently tests five data transformations: prop, clr, TMM, TMMwsp, and RLE from the R packages 'ALDEx2', 'edgeR' and 'DESeq2' (Fernandes et al (2014) <doi:10.1186/2049-2618-2-15>, Anders et al. (2013)<doi:10.1038/nprot.2013.099>).  "
  },
  {
    "id": 8147,
    "package_name": "aMNLFA",
    "title": "Automated Moderated Nonlinear Factor Analysis Using 'M-plus'",
    "description": "Automated generation, running, and interpretation of moderated nonlinear factor analysis models for obtaining scores from observed variables, using the method described by Gottfredson and colleagues (2019) <doi:10.1016/j.addbeh.2018.10.031>. This package creates M-plus input files which may be run iteratively to test two different types of covariate effects on items: (1) latent variable impact (both mean and variance); and (2) differential  item functioning. After sequentially testing for all effects, it also creates a final model by including all significant effects after adjusting for multiple comparisons. Finally, the package creates a scoring model which uses the final values of parameter estimates to generate latent variable scores. \\n\\n This package generates TEMPLATES for M-plus inputs, which can and should be inspected, altered, and run by the user. In addition to being presented without warranty of any kind, the package is provided under the assumption that everyone who uses it is reading, interpreting, understanding, and altering every M-plus input and output file. There is no one right way to implement moderated nonlinear factor analysis, and this package exists solely to save users time as they generate M-plus syntax according to their own judgment.",
    "version": "1.1.2",
    "maintainer": "Veronica Cole <colev@wfu.edu>",
    "author": "Veronica Cole [aut, cre],\n  Nisha Gottfredson [aut],\n  Michael Giordano [aut],\n  Isabella Stallworthy [aut],\n  Meriah DeJoseph [aut],\n  Robin Sifre [aut],\n  Tim Janssen [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aMNLFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aMNLFA Automated Moderated Nonlinear Factor Analysis Using 'M-plus' Automated generation, running, and interpretation of moderated nonlinear factor analysis models for obtaining scores from observed variables, using the method described by Gottfredson and colleagues (2019) <doi:10.1016/j.addbeh.2018.10.031>. This package creates M-plus input files which may be run iteratively to test two different types of covariate effects on items: (1) latent variable impact (both mean and variance); and (2) differential  item functioning. After sequentially testing for all effects, it also creates a final model by including all significant effects after adjusting for multiple comparisons. Finally, the package creates a scoring model which uses the final values of parameter estimates to generate latent variable scores. \\n\\n This package generates TEMPLATES for M-plus inputs, which can and should be inspected, altered, and run by the user. In addition to being presented without warranty of any kind, the package is provided under the assumption that everyone who uses it is reading, interpreting, understanding, and altering every M-plus input and output file. There is no one right way to implement moderated nonlinear factor analysis, and this package exists solely to save users time as they generate M-plus syntax according to their own judgment.  "
  },
  {
    "id": 8153,
    "package_name": "aSPU",
    "title": "Adaptive Sum of Powered Score Test",
    "description": "R codes for the (adaptive) Sum of Powered Score ('SPU' and 'aSPU')\n    tests, inverse variance weighted Sum of Powered score ('SPUw' and 'aSPUw') tests\n    and gene-based and some pathway based association tests (Pathway based Sum of\n    Powered Score tests ('SPUpath'), adaptive 'SPUpath' ('aSPUpath') test, 'GEEaSPU'\n    test for multiple traits - single 'SNP' (single nucleotide polymorphism)\n    association in generalized estimation equations, 'MTaSPUs' test for multiple\n    traits - single 'SNP' association with Genome Wide Association Studies ('GWAS')\n    summary statistics, Gene-based Association Test that uses an extended 'Simes'\n    procedure ('GATES'), Hybrid Set-based Test ('HYST') and extended version\n    of 'GATES' test for pathway-based association testing ('GATES-Simes'). ).\n    The tests can be used with genetic and other data sets with covariates. The\n    response variable is binary or quantitative. Summary; (1) Single trait-'SNP' set\n    association with individual-level data ('aSPU', 'aSPUw', 'aSPUr'), (2) Single trait-'SNP'\n    set association with summary statistics ('aSPUs'), (3) Single trait-pathway\n    association with individual-level data ('aSPUpath'), (4) Single trait-pathway\n    association with summary statistics ('aSPUsPath'), (5) Multiple traits-single\n    'SNP' association with individual-level data ('GEEaSPU'), (6) Multiple traits-\n    single 'SNP' association with summary statistics ('MTaSPUs'), (7) Multiple traits-'SNP' set association with summary statistics('MTaSPUsSet'), (8) Multiple traits-pathway association with summary statistics('MTaSPUsSetPath').",
    "version": "1.50",
    "maintainer": "Il-Youp Kwak <ikwak2@cau.ac.kr>",
    "author": "Il-Youp Kwak and others (See Author(s) in each function manual)",
    "url": "https://github.com/ikwak2/aSPU",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aSPU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aSPU Adaptive Sum of Powered Score Test R codes for the (adaptive) Sum of Powered Score ('SPU' and 'aSPU')\n    tests, inverse variance weighted Sum of Powered score ('SPUw' and 'aSPUw') tests\n    and gene-based and some pathway based association tests (Pathway based Sum of\n    Powered Score tests ('SPUpath'), adaptive 'SPUpath' ('aSPUpath') test, 'GEEaSPU'\n    test for multiple traits - single 'SNP' (single nucleotide polymorphism)\n    association in generalized estimation equations, 'MTaSPUs' test for multiple\n    traits - single 'SNP' association with Genome Wide Association Studies ('GWAS')\n    summary statistics, Gene-based Association Test that uses an extended 'Simes'\n    procedure ('GATES'), Hybrid Set-based Test ('HYST') and extended version\n    of 'GATES' test for pathway-based association testing ('GATES-Simes'). ).\n    The tests can be used with genetic and other data sets with covariates. The\n    response variable is binary or quantitative. Summary; (1) Single trait-'SNP' set\n    association with individual-level data ('aSPU', 'aSPUw', 'aSPUr'), (2) Single trait-'SNP'\n    set association with summary statistics ('aSPUs'), (3) Single trait-pathway\n    association with individual-level data ('aSPUpath'), (4) Single trait-pathway\n    association with summary statistics ('aSPUsPath'), (5) Multiple traits-single\n    'SNP' association with individual-level data ('GEEaSPU'), (6) Multiple traits-\n    single 'SNP' association with summary statistics ('MTaSPUs'), (7) Multiple traits-'SNP' set association with summary statistics('MTaSPUsSet'), (8) Multiple traits-pathway association with summary statistics('MTaSPUsSetPath').  "
  },
  {
    "id": 8154,
    "package_name": "aTSA",
    "title": "Alternative Time Series Analysis",
    "description": "Contains some tools for testing, analyzing time series data and\n    fitting popular time series models such as ARIMA, Moving Average and Holt\n    Winters, etc. Most functions also provide nice and clear outputs like SAS\n    does, such as identify, estimate and forecast, which are the same statements\n    in PROC ARIMA in SAS.",
    "version": "3.1.2.1",
    "maintainer": "Debin Qiu <debinqiu@uga.edu>",
    "author": "Debin Qiu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=aTSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aTSA Alternative Time Series Analysis Contains some tools for testing, analyzing time series data and\n    fitting popular time series models such as ARIMA, Moving Average and Holt\n    Winters, etc. Most functions also provide nice and clear outputs like SAS\n    does, such as identify, estimate and forecast, which are the same statements\n    in PROC ARIMA in SAS.  "
  },
  {
    "id": 8184,
    "package_name": "abtest",
    "title": "Bayesian A/B Testing",
    "description": "Provides functions for Bayesian A/B testing including prior elicitation\n    options based on Kass and Vaidyanathan (1992) <doi:10.1111/j.2517-6161.1992.tb01868.x>. \n    Gronau, Raj K. N., & Wagenmakers (2021) <doi:10.18637/jss.v100.i17>.",
    "version": "1.0.1",
    "maintainer": "Quentin F. Gronau <Quentin.F.Gronau@gmail.com>",
    "author": "Quentin F. Gronau [aut, cre],\n  Akash Raj [ctb],\n  Eric-Jan Wagenmakers [ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=abtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "abtest Bayesian A/B Testing Provides functions for Bayesian A/B testing including prior elicitation\n    options based on Kass and Vaidyanathan (1992) <doi:10.1111/j.2517-6161.1992.tb01868.x>. \n    Gronau, Raj K. N., & Wagenmakers (2021) <doi:10.18637/jss.v100.i17>.  "
  },
  {
    "id": 8201,
    "package_name": "acdcR",
    "title": "Agro-Climatic Data by County",
    "description": "The functions are designed to calculate the most widely-used county-level variables in \n  agricultural production or agricultural-climatic and weather analyses. To operate some functions \n  in this package needs download of the bulk PRISM raster. See the examples, testing versions and \n  more details from: <https://github.com/ysd2004/acdcR>.",
    "version": "1.0.0",
    "maintainer": "Seong D. Yun <seong.yun@msstate.edu>",
    "author": "Seong D. Yun [aut, cre]",
    "url": "https://github.com/ysd2004/acdcR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=acdcR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acdcR Agro-Climatic Data by County The functions are designed to calculate the most widely-used county-level variables in \n  agricultural production or agricultural-climatic and weather analyses. To operate some functions \n  in this package needs download of the bulk PRISM raster. See the examples, testing versions and \n  more details from: <https://github.com/ysd2004/acdcR>.  "
  },
  {
    "id": 8252,
    "package_name": "adaptMT",
    "title": "Adaptive P-Value Thresholding for Multiple Hypothesis Testing\nwith Side Information",
    "description": "Implementation of adaptive p-value thresholding (AdaPT), including both a framework that allows the user to specify any \n  algorithm to learn local false discovery rate and a pool of convenient functions that implement specific \n  algorithms. See Lei, Lihua and Fithian, William (2016) <arXiv:1609.06035>.",
    "version": "1.0.0",
    "maintainer": "Lihua Lei <lihua.lei@berkeley.edu>",
    "author": "Lihua Lei [aut, cre]",
    "url": "https://arxiv.org/abs/1609.06035,\nhttps://github.com/lihualei71/adaptMT",
    "bug_reports": "https://github.com/lihualei71/adaptMT/issues",
    "repository": "https://cran.r-project.org/package=adaptMT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adaptMT Adaptive P-Value Thresholding for Multiple Hypothesis Testing\nwith Side Information Implementation of adaptive p-value thresholding (AdaPT), including both a framework that allows the user to specify any \n  algorithm to learn local false discovery rate and a pool of convenient functions that implement specific \n  algorithms. See Lei, Lihua and Fithian, William (2016) <arXiv:1609.06035>.  "
  },
  {
    "id": 8261,
    "package_name": "adbcdrivermanager",
    "title": "'Arrow' Database Connectivity ('ADBC') Driver Manager",
    "description": "Provides a developer-facing interface to 'Arrow' Database\n  Connectivity ('ADBC') for the purposes of driver development, driver\n  testing, and building high-level database interfaces for users. 'ADBC'\n  <https://arrow.apache.org/adbc/> is an API standard for database access\n  libraries that uses 'Arrow' for result sets and query parameters.",
    "version": "0.21.0",
    "maintainer": "Dewey Dunnington <dewey@dunnington.ca>",
    "author": "Dewey Dunnington [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-4582>),\n  Apache Arrow [aut, cph],\n  Apache Software Foundation [cph]",
    "url": "https://arrow.apache.org/adbc/current/r/adbcdrivermanager/,\nhttps://github.com/apache/arrow-adbc",
    "bug_reports": "https://github.com/apache/arrow-adbc/issues",
    "repository": "https://cran.r-project.org/package=adbcdrivermanager",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adbcdrivermanager 'Arrow' Database Connectivity ('ADBC') Driver Manager Provides a developer-facing interface to 'Arrow' Database\n  Connectivity ('ADBC') for the purposes of driver development, driver\n  testing, and building high-level database interfaces for users. 'ADBC'\n  <https://arrow.apache.org/adbc/> is an API standard for database access\n  libraries that uses 'Arrow' for result sets and query parameters.  "
  },
  {
    "id": 8304,
    "package_name": "admiral.test",
    "title": "Test Data for the 'admiral' Package",
    "description": "A set of Study Data Tabulation Model (SDTM) datasets from the Clinical\n    Data Interchange Standards Consortium (CDISC) pilot project used for testing\n    and developing Analysis Data Model (ADaM) derivations inside the 'admiral'\n    package.",
    "version": "0.7.0",
    "maintainer": "Ben Straub <ben.x.straub@gsk.com>",
    "author": "Ben Straub [aut, cre],\n  Stefan Bundfuss [aut],\n  Gordon Miller [aut],\n  Thomas Neitmann [aut],\n  Syed Mubasheer [aut],\n  Ross Farrugia [ctb],\n  Shan Lee [ctb],\n  Gopi Vegesna [ctb],\n  Sophie Shapcott [ctb],\n  Mahdi About [ctb],\n  Antonio Rodr\u00edguez [ctb],\n  Claudia Carlucci [ctb],\n  Annie Yang [ctb],\n  Tamara Senior [ctb],\n  F. Hoffmann-La Roche AG [cph, fnd],\n  GlaxoSmithKline LLC [cph, fnd],\n  Clinical Data Interchange Standards Consortium [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=admiral.test",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiral.test Test Data for the 'admiral' Package A set of Study Data Tabulation Model (SDTM) datasets from the Clinical\n    Data Interchange Standards Consortium (CDISC) pilot project used for testing\n    and developing Analysis Data Model (ADaM) derivations inside the 'admiral'\n    package.  "
  },
  {
    "id": 8305,
    "package_name": "admiraldev",
    "title": "Utility Functions and Development Tools for the Admiral Package\nFamily",
    "description": "Utility functions to check data, variables and conditions for\n    functions used in 'admiral' and 'admiral' extension packages.\n    Additional utility helper functions to assist developers with\n    maintaining documentation, testing and general upkeep of 'admiral' and\n    'admiral' extension packages.",
    "version": "1.3.1",
    "maintainer": "Ben Straub <ben.x.straub@gsk.com>",
    "author": "Ben Straub [aut, cre],\n  Stefan Bundfuss [aut] (ORCID: <https://orcid.org/0009-0005-0027-1198>),\n  Jeffrey Dickinson [aut],\n  Ross Farrugia [aut],\n  Fanny Gautier [aut],\n  Edoardo Mancini [aut] (ORCID: <https://orcid.org/0009-0006-4899-8641>),\n  Gordon Miller [aut],\n  Daniel Sjoberg [aut] (ORCID: <https://orcid.org/0000-0003-0862-2018>),\n  Stefan Thoma [aut] (ORCID: <https://orcid.org/0000-0002-5553-9252>),\n  F. Hoffmann-La Roche AG [cph, fnd],\n  GlaxoSmithKline LLC [cph, fnd]",
    "url": "https://pharmaverse.github.io/admiraldev/,\nhttps://github.com/pharmaverse/admiraldev/",
    "bug_reports": "https://github.com/pharmaverse/admiraldev/issues",
    "repository": "https://cran.r-project.org/package=admiraldev",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "admiraldev Utility Functions and Development Tools for the Admiral Package\nFamily Utility functions to check data, variables and conditions for\n    functions used in 'admiral' and 'admiral' extension packages.\n    Additional utility helper functions to assist developers with\n    maintaining documentation, testing and general upkeep of 'admiral' and\n    'admiral' extension packages.  "
  },
  {
    "id": 8328,
    "package_name": "adventr",
    "title": "Interactive R Tutorials to Accompany Field (2016), \"An Adventure\nin Statistics\"",
    "description": "Interactive 'R' tutorials written using 'learnr' for Field (2016), \"An Adventure in Statistics\", <ISBN:9781446210451>.\n    Topics include general workflow in 'R' and 'Rstudio', the 'R' environment and 'tidyverse', summarizing data, model fitting, central tendency, \n    visualising data using 'ggplot2', inferential statistics and robust estimation, hypothesis testing, the general linear model, comparing means,\n    repeated measures designs, factorial designs, multilevel models, growth models, and generalized linear models (logistic regression).",
    "version": "0.1.8",
    "maintainer": "Andy Field <andyf@sussex.ac.uk>",
    "author": "Andy Field [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=adventr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "adventr Interactive R Tutorials to Accompany Field (2016), \"An Adventure\nin Statistics\" Interactive 'R' tutorials written using 'learnr' for Field (2016), \"An Adventure in Statistics\", <ISBN:9781446210451>.\n    Topics include general workflow in 'R' and 'Rstudio', the 'R' environment and 'tidyverse', summarizing data, model fitting, central tendency, \n    visualising data using 'ggplot2', inferential statistics and robust estimation, hypothesis testing, the general linear model, comparing means,\n    repeated measures designs, factorial designs, multilevel models, growth models, and generalized linear models (logistic regression).  "
  },
  {
    "id": 8445,
    "package_name": "alpha.correction.bh",
    "title": "Benjamini-Hochberg Alpha Correction",
    "description": "Provides the alpha-adjustment correction from \"Benjamini, Y., & Hochberg, Y. (1995) <doi:10.1111/j.2517-6161.1995.tb02031.x> Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300\". For researchers interested in using the exact mathematical formulas and procedures as used in the original paper.",
    "version": "0.0.2",
    "maintainer": "Michael Mogessie <mogessie@upenn.edu>",
    "author": "Michael Mogessie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6769-5941>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=alpha.correction.bh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "alpha.correction.bh Benjamini-Hochberg Alpha Correction Provides the alpha-adjustment correction from \"Benjamini, Y., & Hochberg, Y. (1995) <doi:10.1111/j.2517-6161.1995.tb02031.x> Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1), 289-300\". For researchers interested in using the exact mathematical formulas and procedures as used in the original paper.  "
  },
  {
    "id": 8482,
    "package_name": "amp",
    "title": "Statistical Test for the Multivariate Point Null Hypotheses",
    "description": "A testing framework for testing the multivariate point null hypothesis. \n    A testing framework described in Elder et al. (2022) <arXiv:2203.01897> to test the multivariate point null hypothesis.  After the user selects a parameter of interest and defines the assumed data generating mechanism, this information should be encoded in functions for the parameter estimator and its corresponding influence curve. Some parameter and data generating mechanism combinations have codings in this package, and are explained in detail in the article.",
    "version": "1.0.0",
    "maintainer": "Adam Elder <shmelder@gmail.com>",
    "author": "Adam Elder [aut, cre] (ORCID: <https://orcid.org/0000-0003-1665-2639>),\n  Marco Carone [ths] (ORCID: <https://orcid.org/0000-0003-2106-0953>),\n  Alex Luedtke [ths] (ORCID: <https://orcid.org/0000-0002-9936-3236>)",
    "url": "",
    "bug_reports": "https://github.com/adam-s-elder/amp/issues",
    "repository": "https://cran.r-project.org/package=amp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amp Statistical Test for the Multivariate Point Null Hypotheses A testing framework for testing the multivariate point null hypothesis. \n    A testing framework described in Elder et al. (2022) <arXiv:2203.01897> to test the multivariate point null hypothesis.  After the user selects a parameter of interest and defines the assumed data generating mechanism, this information should be encoded in functions for the parameter estimator and its corresponding influence curve. Some parameter and data generating mechanism combinations have codings in this package, and are explained in detail in the article.  "
  },
  {
    "id": 8485,
    "package_name": "amregtest",
    "title": "Runs Allelematch Regression Tests",
    "description": "Automates regression testing of package 'allelematch'. Over\n    2500 tests covers all functions in 'allelematch', reproduces the\n    examples from the documentation and includes negative tests. The\n    implementation is based on 'testthat'.",
    "version": "1.0.5",
    "maintainer": "Torvald Staxler <torvald.staxler@telia.com>",
    "author": "Department of Wildlife, Fish and Environmental Studies at Swedish\n    University of Agricultural Sciences [cph],\n  G\u00f6ran Spong [cph] (Senior Lecturer at the Department of Wildlife, Fish\n    and Environmental Studies),\n  Paul Galpern [ctb] (Author of the excellent package 'allelematch' from\n    which the five amExample data files have been copied under MIT\n    license),\n  Torvald Staxler [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=amregtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "amregtest Runs Allelematch Regression Tests Automates regression testing of package 'allelematch'. Over\n    2500 tests covers all functions in 'allelematch', reproduces the\n    examples from the documentation and includes negative tests. The\n    implementation is based on 'testthat'.  "
  },
  {
    "id": 8605,
    "package_name": "archivist",
    "title": "Tools for Storing, Restoring and Searching for R Objects",
    "description": "Data exploration and modelling is a process in which a lot of data\n    artifacts are produced. Artifacts like: subsets, data aggregates, plots,\n    statistical models, different versions of data sets and different versions\n    of results. The more projects we work with the more artifacts are produced\n    and the harder it is to manage these artifacts. Archivist helps to store\n    and manage artifacts created in R. Archivist allows you to store selected\n    artifacts as a binary files together with their metadata and relations.\n    Archivist allows to share artifacts with others, either through shared\n    folder or github. Archivist allows to look for already created artifacts by\n    using it's class, name, date of the creation or other properties. Makes it\n    easy to restore such artifacts. Archivist allows to check if new artifact\n    is the exact copy that was produced some time ago. That might be useful\n    either for testing or caching.",
    "version": "2.3.8",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemyslaw Biecek [aut, cre],\n  Marcin Kosinski [aut],\n  Witold Chodor [ctb]",
    "url": "https://pbiecek.github.io/archivist/",
    "bug_reports": "https://github.com/pbiecek/archivist/issues",
    "repository": "https://cran.r-project.org/package=archivist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "archivist Tools for Storing, Restoring and Searching for R Objects Data exploration and modelling is a process in which a lot of data\n    artifacts are produced. Artifacts like: subsets, data aggregates, plots,\n    statistical models, different versions of data sets and different versions\n    of results. The more projects we work with the more artifacts are produced\n    and the harder it is to manage these artifacts. Archivist helps to store\n    and manage artifacts created in R. Archivist allows you to store selected\n    artifacts as a binary files together with their metadata and relations.\n    Archivist allows to share artifacts with others, either through shared\n    folder or github. Archivist allows to look for already created artifacts by\n    using it's class, name, date of the creation or other properties. Makes it\n    easy to restore such artifacts. Archivist allows to check if new artifact\n    is the exact copy that was produced some time ago. That might be useful\n    either for testing or caching.  "
  },
  {
    "id": 8671,
    "package_name": "ashr",
    "title": "Methods for Adaptive Shrinkage, using Empirical Bayes",
    "description": "The R package 'ashr' implements an Empirical Bayes\n    approach for large-scale hypothesis testing and false discovery\n    rate (FDR) estimation based on the methods proposed in\n    M. Stephens, 2016, \"False discovery rates: a new deal\",\n    <DOI:10.1093/biostatistics/kxw041>. These methods can be applied\n    whenever two sets of summary statistics---estimated effects and\n    standard errors---are available, just as 'qvalue' can be applied\n    to previously computed p-values. Two main interfaces are\n    provided: ash(), which is more user-friendly; and ash.workhorse(),\n    which has more options and is geared toward advanced users. The\n    ash() and ash.workhorse() also provides a flexible modeling\n    interface that can accommodate a variety of likelihoods (e.g.,\n    normal, Poisson) and mixture priors (e.g., uniform, normal).",
    "version": "2.2-63",
    "maintainer": "Peter Carbonetto <pcarbo@uchicago.edu>",
    "author": "Matthew Stephens [aut],\n  Peter Carbonetto [aut, cre],\n  Chaoxing Dai [ctb],\n  David Gerard [aut],\n  Mengyin Lu [aut],\n  Lei Sun [aut],\n  Jason Willwerscheid [aut],\n  Nan Xiao [aut],\n  Mazon Zeng [ctb]",
    "url": "https://github.com/stephens999/ashr",
    "bug_reports": "https://github.com/stephens999/ashr/issues",
    "repository": "https://cran.r-project.org/package=ashr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ashr Methods for Adaptive Shrinkage, using Empirical Bayes The R package 'ashr' implements an Empirical Bayes\n    approach for large-scale hypothesis testing and false discovery\n    rate (FDR) estimation based on the methods proposed in\n    M. Stephens, 2016, \"False discovery rates: a new deal\",\n    <DOI:10.1093/biostatistics/kxw041>. These methods can be applied\n    whenever two sets of summary statistics---estimated effects and\n    standard errors---are available, just as 'qvalue' can be applied\n    to previously computed p-values. Two main interfaces are\n    provided: ash(), which is more user-friendly; and ash.workhorse(),\n    which has more options and is geared toward advanced users. The\n    ash() and ash.workhorse() also provides a flexible modeling\n    interface that can accommodate a variety of likelihoods (e.g.,\n    normal, Poisson) and mixture priors (e.g., uniform, normal).  "
  },
  {
    "id": 8698,
    "package_name": "astrochron",
    "title": "A Computational Tool for Astrochronology",
    "description": "Routines for astrochronologic testing, astronomical time scale construction, and time series analysis <doi:10.1016/j.earscirev.2018.11.015>. Also included are a range of statistical analysis and modeling routines that are relevant to time scale development and paleoclimate analysis.",
    "version": "1.5",
    "maintainer": "Stephen Meyers <smeyers@geology.wisc.edu>",
    "author": "Stephen Meyers [aut, cre],\n  Alberto Malinverno [ctb],\n  Linda Hinnov [ctb],\n  Christian Zeeden [ctb],\n  Huaran Liu [ctb],\n  Vincent Moron [ctb],\n  Michel Crucifix [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=astrochron",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "astrochron A Computational Tool for Astrochronology Routines for astrochronologic testing, astronomical time scale construction, and time series analysis <doi:10.1016/j.earscirev.2018.11.015>. Also included are a range of statistical analysis and modeling routines that are relevant to time scale development and paleoclimate analysis.  "
  },
  {
    "id": 8707,
    "package_name": "atRisk",
    "title": "At-Risk",
    "description": "The at-Risk (aR) approach is based on a two-step parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the aR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al. (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.",
    "version": "0.2.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "author": "Quentin Lajaunie [aut, cre],\n  Guillaume Flament [aut, ctb],\n  Christophe Hurlin [aut],\n  Souzan Kazemi [rev]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=atRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "atRisk At-Risk The at-Risk (aR) approach is based on a two-step parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the aR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al. (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.  "
  },
  {
    "id": 8724,
    "package_name": "auctestr",
    "title": "Statistical Testing for AUC Data",
    "description": "Performs statistical testing to compare predictive \n\tmodels based on multiple observations of the A' statistic (also known as \n\tArea Under the Receiver Operating Characteristic Curve, or AUC). \n\tSpecifically, it implements a testing method based on the equivalence \n\tbetween the A' statistic and the Wilcoxon statistic. \n\tFor more information, see Hanley and McNeil (1982) <doi:10.1148/radiology.143.1.7063747>. ",
    "version": "1.0.0",
    "maintainer": "Josh Gardner <jpgard@umich.edu>",
    "author": "Josh Gardner [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=auctestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "auctestr Statistical Testing for AUC Data Performs statistical testing to compare predictive \n\tmodels based on multiple observations of the A' statistic (also known as \n\tArea Under the Receiver Operating Characteristic Curve, or AUC). \n\tSpecifically, it implements a testing method based on the equivalence \n\tbetween the A' statistic and the Wilcoxon statistic. \n\tFor more information, see Hanley and McNeil (1982) <doi:10.1148/radiology.143.1.7063747>.   "
  },
  {
    "id": 8815,
    "package_name": "bacistool",
    "title": "Bayesian Classification and Information Sharing (BaCIS) Tool for\nthe Design of Multi-Group Phase II Clinical Trials",
    "description": "Provides the design of multi-group phase\n    II clinical trials with binary outcomes using the hierarchical Bayesian\n    classification and information sharing (BaCIS) model. Subgroups are classified\n    into two clusters on the basis of their outcomes mimicking the hypothesis\n    testing framework. Subsequently, information sharing takes place within\n    subgroups in the same cluster, rather than across all subgroups. This method can\n    be applied to the design and analysis of multi-group clinical trials with binary\n    outcomes. Reference: Nan Chen and J. Jack Lee (2019) <doi:10.1002/bimj.201700275>.",
    "version": "1.0.0",
    "maintainer": "J. Jack Lee <jjlee@mdanderson.org>",
    "author": "Nan Chen and J. Jack Lee",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bacistool",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bacistool Bayesian Classification and Information Sharing (BaCIS) Tool for\nthe Design of Multi-Group Phase II Clinical Trials Provides the design of multi-group phase\n    II clinical trials with binary outcomes using the hierarchical Bayesian\n    classification and information sharing (BaCIS) model. Subgroups are classified\n    into two clusters on the basis of their outcomes mimicking the hypothesis\n    testing framework. Subsequently, information sharing takes place within\n    subgroups in the same cluster, rather than across all subgroups. This method can\n    be applied to the design and analysis of multi-group clinical trials with binary\n    outcomes. Reference: Nan Chen and J. Jack Lee (2019) <doi:10.1002/bimj.201700275>.  "
  },
  {
    "id": 9106,
    "package_name": "bigPLSR",
    "title": "Partial Least Squares Regression Models with Big Matrices",
    "description": "\n  Fast partial least squares (PLS) for dense and out-of-core data.\n  Provides SIMPLS (straightforward implementation of a statistically inspired \n  modification of the PLS method) and NIPALS (non-linear iterative partial least-squares) solvers, \n  plus kernel-style PLS variants ('kernelpls' and 'widekernelpls') with parity to 'pls'. Optimized for\n  'bigmemory'-backed matrices with streamed cross-products and chunked BLAS (Basic Linear Algebra Subprograms)\n  (XtX/XtY and XXt/YX), optional file-backed score sinks, and deterministic\n  testing helpers. Includes an auto-selection strategy that chooses between\n  XtX SIMPLS, XXt (wide) SIMPLS, and NIPALS based on (n, p) and a configurable\n  memory budget. About the package, Bertrand and Maumy (2023) <https://hal.science/hal-05352069>, \n  and  <https://hal.science/hal-05352061> highlighted fitting and cross-validating \n  PLS regression models to big data. For more details about some of the techniques \n  featured in the package, Dayal and MacGregor (1997) \n  <doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM435%3E3.0.CO;2-%23>,\n  Rosipal & Trejo (2001) <https://www.jmlr.org/papers/v2/rosipal01a.html>,\n  Tenenhaus, Viennet, and Saporta (2007) <doi:10.1016/j.csda.2007.01.004>,\n  Rosipal (2004) <doi:10.1007/978-3-540-45167-9_17>,\n  Rosipal (2019) <https://ieeexplore.ieee.org/document/8616346>,\n  Song, Wang, and Bai (2024) <doi:10.1016/j.chemolab.2024.105238>.\n  Includes kernel logistic PLS with 'C++'-accelerated alternating iteratively \n  reweighted least squares (IRLS) updates, streamed reproducing kernel Hilbert space (RKHS) \n  solvers with reusable centering statistics, and bootstrap\n  diagnostics with graphical summaries for coefficients, scores, and\n  cross-validation workflows, alongside dedicated plotting utilities for\n  individuals, variables, ellipses, and biplots.\n  The streaming backend uses far less memory and keeps memory bounded across data sizes.\n\tFor PLS1, streaming is often fast enough while preserving a small memory footprint; \n\tfor PLS2 it remains competitive with a bounded footprint.\n\tOn small problems that fit comfortably in RAM (random-access memory), dense in-memory \n\tsolvers are slightly faster; the crossover occurs as n or p grow and the Gram/cross-product cost dominates.",
    "version": "0.7.2",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "author": "Frederic Bertrand [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-0837-8281>),\n  Myriam Maumy [aut] (ORCID: <https://orcid.org/0000-0002-4615-1512>)",
    "url": "https://fbertran.github.io/bigPLSR/,\nhttps://github.com/fbertran/bigPLSR",
    "bug_reports": "https://github.com/fbertran/bigPLSR/issues",
    "repository": "https://cran.r-project.org/package=bigPLSR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigPLSR Partial Least Squares Regression Models with Big Matrices \n  Fast partial least squares (PLS) for dense and out-of-core data.\n  Provides SIMPLS (straightforward implementation of a statistically inspired \n  modification of the PLS method) and NIPALS (non-linear iterative partial least-squares) solvers, \n  plus kernel-style PLS variants ('kernelpls' and 'widekernelpls') with parity to 'pls'. Optimized for\n  'bigmemory'-backed matrices with streamed cross-products and chunked BLAS (Basic Linear Algebra Subprograms)\n  (XtX/XtY and XXt/YX), optional file-backed score sinks, and deterministic\n  testing helpers. Includes an auto-selection strategy that chooses between\n  XtX SIMPLS, XXt (wide) SIMPLS, and NIPALS based on (n, p) and a configurable\n  memory budget. About the package, Bertrand and Maumy (2023) <https://hal.science/hal-05352069>, \n  and  <https://hal.science/hal-05352061> highlighted fitting and cross-validating \n  PLS regression models to big data. For more details about some of the techniques \n  featured in the package, Dayal and MacGregor (1997) \n  <doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM435%3E3.0.CO;2-%23>,\n  Rosipal & Trejo (2001) <https://www.jmlr.org/papers/v2/rosipal01a.html>,\n  Tenenhaus, Viennet, and Saporta (2007) <doi:10.1016/j.csda.2007.01.004>,\n  Rosipal (2004) <doi:10.1007/978-3-540-45167-9_17>,\n  Rosipal (2019) <https://ieeexplore.ieee.org/document/8616346>,\n  Song, Wang, and Bai (2024) <doi:10.1016/j.chemolab.2024.105238>.\n  Includes kernel logistic PLS with 'C++'-accelerated alternating iteratively \n  reweighted least squares (IRLS) updates, streamed reproducing kernel Hilbert space (RKHS) \n  solvers with reusable centering statistics, and bootstrap\n  diagnostics with graphical summaries for coefficients, scores, and\n  cross-validation workflows, alongside dedicated plotting utilities for\n  individuals, variables, ellipses, and biplots.\n  The streaming backend uses far less memory and keeps memory bounded across data sizes.\n\tFor PLS1, streaming is often fast enough while preserving a small memory footprint; \n\tfor PLS2 it remains competitive with a bounded footprint.\n\tOn small problems that fit comfortably in RAM (random-access memory), dense in-memory \n\tsolvers are slightly faster; the crossover occurs as n or p grow and the Gram/cross-product cost dominates.  "
  },
  {
    "id": 9145,
    "package_name": "binGroup",
    "title": "Evaluation and Experimental Design for Binomial Group Testing",
    "description": "Methods for estimation and hypothesis testing of proportions\n in group testing designs: methods for estimating a proportion in a single\n        population (assuming sensitivity and specificity equal to 1 in designs\n        with equal group sizes), as well as hypothesis tests and\n        functions for experimental design for this situation. For\n        estimating one proportion or the difference of proportions, a\n        number of confidence interval methods are included, which can\n        deal with various different pool sizes. Further, regression\n        methods are implemented for simple pooling and matrix pooling\n        designs.\n        Methods for identification of positive items in group\n        testing designs: Optimal testing configurations can be found \n        for hierarchical and array-based algorithms. Operating \n        characteristics can be calculated for testing configurations \n        across a wide variety of situations.",
    "version": "2.2-3",
    "maintainer": "Frank Schaarschmidt <schaarschmidt@biostat.uni-hannover.de>",
    "author": "Boan Zhang [aut],\n  Christopher Bilder [aut],\n  Brad Biggerstaff [aut],\n  Frank Schaarschmidt [aut, cre],\n  Brianna Hitt [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=binGroup",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binGroup Evaluation and Experimental Design for Binomial Group Testing Methods for estimation and hypothesis testing of proportions\n in group testing designs: methods for estimating a proportion in a single\n        population (assuming sensitivity and specificity equal to 1 in designs\n        with equal group sizes), as well as hypothesis tests and\n        functions for experimental design for this situation. For\n        estimating one proportion or the difference of proportions, a\n        number of confidence interval methods are included, which can\n        deal with various different pool sizes. Further, regression\n        methods are implemented for simple pooling and matrix pooling\n        designs.\n        Methods for identification of positive items in group\n        testing designs: Optimal testing configurations can be found \n        for hierarchical and array-based algorithms. Operating \n        characteristics can be calculated for testing configurations \n        across a wide variety of situations.  "
  },
  {
    "id": 9146,
    "package_name": "binGroup2",
    "title": "Identification and Estimation using Group Testing",
    "description": "Methods for the group testing identification problem: 1) Operating \n    characteristics (e.g., expected number of tests) for commonly used \n    hierarchical and array-based algorithms, and 2) Optimal testing \n    configurations for these same algorithms. Methods for the group testing \n    estimation problem: 1) Estimation and inference procedures for an overall \n    prevalence, and 2) Regression modeling for commonly used hierarchical and \n    array-based algorithms. ",
    "version": "1.3.3",
    "maintainer": "Brianna Hitt <brianna.hitt@afacademy.af.edu>",
    "author": "Brianna Hitt [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0645-0067>),\n  Christopher Bilder [aut] (ORCID:\n    <https://orcid.org/0000-0002-2848-8576>),\n  Frank Schaarschmidt [aut] (ORCID:\n    <https://orcid.org/0000-0002-6599-3803>),\n  Brad Biggerstaff [aut] (ORCID: <https://orcid.org/0000-0002-3105-3530>),\n  Christopher McMahan [aut] (ORCID:\n    <https://orcid.org/0000-0001-5056-9615>),\n  Joshua Tebbs [aut] (ORCID: <https://orcid.org/0000-0002-6762-7241>),\n  Boan Zhang [ctb],\n  Michael Black [ctb],\n  Peijie Hou [ctb],\n  Peng Chen [ctb],\n  Minh Nguyen [ctb]",
    "url": "https://github.com/bdhitt/binGroup2,\nhttps://bdhitt.github.io/binGroup2/",
    "bug_reports": "https://github.com/bdhitt/binGroup2/issues",
    "repository": "https://cran.r-project.org/package=binGroup2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binGroup2 Identification and Estimation using Group Testing Methods for the group testing identification problem: 1) Operating \n    characteristics (e.g., expected number of tests) for commonly used \n    hierarchical and array-based algorithms, and 2) Optimal testing \n    configurations for these same algorithms. Methods for the group testing \n    estimation problem: 1) Estimation and inference procedures for an overall \n    prevalence, and 2) Regression modeling for commonly used hierarchical and \n    array-based algorithms.   "
  },
  {
    "id": 9173,
    "package_name": "binsreg",
    "title": "Binscatter Estimation and Inference",
    "description": "Provides tools for statistical analysis using the binscatter methods developed by Cattaneo, Crump, Farrell and Feng (2024a) <doi:10.48550/arXiv.1902.09608>, Cattaneo, Crump, Farrell and Feng (2024b) <https://nppackages.github.io/references/Cattaneo-Crump-Farrell-Feng_2024_NonlinearBinscatter.pdf> and Cattaneo, Crump, Farrell and Feng (2024c) <doi:10.48550/arXiv.1902.09615>. Binscatter provides a flexible way of describing the relationship between two variables based on partitioning/binning of the independent variable of interest. binsreg(), binsqreg() and binsglm() implement binscatter least squares regression, quantile regression and generalized linear regression respectively, with particular focus on constructing binned scatter plots. They also implement robust (pointwise and uniform) inference of regression functions and derivatives thereof. binstest() implements hypothesis testing procedures for parametric functional forms of and nonparametric shape restrictions on the regression function. binspwc() implements hypothesis testing procedures for pairwise group comparison of binscatter estimators. binsregselect() implements data-driven procedures for selecting the number of bins for binscatter estimation. All the commands allow for covariate adjustment, smoothness restrictions and clustering.",
    "version": "1.1",
    "maintainer": "Yingjie Feng <fengyingjiepku@gmail.com>",
    "author": "Matias D. Cattaneo, Richard K. Crump, Max H. Farrell, Yingjie Feng",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=binsreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "binsreg Binscatter Estimation and Inference Provides tools for statistical analysis using the binscatter methods developed by Cattaneo, Crump, Farrell and Feng (2024a) <doi:10.48550/arXiv.1902.09608>, Cattaneo, Crump, Farrell and Feng (2024b) <https://nppackages.github.io/references/Cattaneo-Crump-Farrell-Feng_2024_NonlinearBinscatter.pdf> and Cattaneo, Crump, Farrell and Feng (2024c) <doi:10.48550/arXiv.1902.09615>. Binscatter provides a flexible way of describing the relationship between two variables based on partitioning/binning of the independent variable of interest. binsreg(), binsqreg() and binsglm() implement binscatter least squares regression, quantile regression and generalized linear regression respectively, with particular focus on constructing binned scatter plots. They also implement robust (pointwise and uniform) inference of regression functions and derivatives thereof. binstest() implements hypothesis testing procedures for parametric functional forms of and nonparametric shape restrictions on the regression function. binspwc() implements hypothesis testing procedures for pairwise group comparison of binscatter estimators. binsregselect() implements data-driven procedures for selecting the number of bins for binscatter estimation. All the commands allow for covariate adjustment, smoothness restrictions and clustering.  "
  },
  {
    "id": 9200,
    "package_name": "biosensors.usc",
    "title": "Distributional Data Analysis Techniques for Biosensor Data",
    "description": "Unified and user-friendly framework for using new \n             distributional representations of biosensors data in different statistical modeling \n             tasks: regression models, hypothesis testing, cluster analysis, visualization, and \n             descriptive analysis. Distributional representations are a functional extension of \n             compositional time-range metrics and we have used them successfully so far in modeling \n             glucose profiles and accelerometer data. However, these functional representations can \n             be used to represent any biosensor data such as ECG or medical imaging such as fMRI.\n             Matabuena M, Petersen A, Vidal JC, Gude F. \"Glucodensities: A new representation of \n             glucose profiles using distributional data analysis\" (2021) \n             <doi:10.1177/0962280221998064>.",
    "version": "1.0",
    "maintainer": "Juan C. Vidal <juan.vidal@usc.es>",
    "author": "Juan C. Vidal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8682-6772>),\n  Marcos Matabuena [aut] (ORCID: <https://orcid.org/0000-0003-3841-4447>),\n  Marta Karas [ctb] (ORCID: <https://orcid.org/0000-0001-5889-3970>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=biosensors.usc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "biosensors.usc Distributional Data Analysis Techniques for Biosensor Data Unified and user-friendly framework for using new \n             distributional representations of biosensors data in different statistical modeling \n             tasks: regression models, hypothesis testing, cluster analysis, visualization, and \n             descriptive analysis. Distributional representations are a functional extension of \n             compositional time-range metrics and we have used them successfully so far in modeling \n             glucose profiles and accelerometer data. However, these functional representations can \n             be used to represent any biosensor data such as ECG or medical imaging such as fMRI.\n             Matabuena M, Petersen A, Vidal JC, Gude F. \"Glucodensities: A new representation of \n             glucose profiles using distributional data analysis\" (2021) \n             <doi:10.1177/0962280221998064>.  "
  },
  {
    "id": 9222,
    "package_name": "bispdep",
    "title": "Statistical Tools for Bivariate Spatial Dependence Analysis",
    "description": "A collection of functions to test spatial autocorrelation between variables, including Moran I, Geary C and Getis G together with scatter plots, functions for mapping and identifying clusters and outliers, functions associated with the moments of the previous statistics that will allow testing whether there is bivariate spatial autocorrelation, and a function that allows identifying (visualizing neighbours) on the map, the neighbors of any region once the scheme of the spatial weights matrix has been established.",
    "version": "1.0-2",
    "maintainer": "Carlos Melo <cmelo@udistrital.edu.co>",
    "author": "Carlos Melo [aut, cre] (ORCID: <https://orcid.org/0000-0002-5598-1913>),\n  Oscar Melo [ctb] (ORCID: <https://orcid.org/0000-0002-0296-4511>),\n  Sandra Melo [ctb] (ORCID: <https://orcid.org/0000-0002-4875-7657>)",
    "url": "https://github.com/carlosm77/bispdep",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bispdep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bispdep Statistical Tools for Bivariate Spatial Dependence Analysis A collection of functions to test spatial autocorrelation between variables, including Moran I, Geary C and Getis G together with scatter plots, functions for mapping and identifying clusters and outliers, functions associated with the moments of the previous statistics that will allow testing whether there is bivariate spatial autocorrelation, and a function that allows identifying (visualizing neighbours) on the map, the neighbors of any region once the scheme of the spatial weights matrix has been established.  "
  },
  {
    "id": 9237,
    "package_name": "bizicount",
    "title": "Bivariate Zero-Inflated Count Models Using Copulas",
    "description": "Maximum likelihood estimation of copula-based zero-inflated \n    (and non-inflated) Poisson and negative binomial count models, based on the \n    article <doi:10.18637/jss.v109.i01>. Supports Frank and Gaussian copulas. \n    Allows for mixed margins (e.g., one margin Poisson, the other zero-inflated \n    negative binomial), and several marginal link functions. Built-in methods for \n    publication-quality tables using 'texreg', post-estimation diagnostics using \n    'DHARMa', and testing for marginal zero-modification via <doi:10.1177/0962280217749991>. \n    For information on copula regression for count data, see Genest and Ne\u0161lehov\u00e1 (2007) \n    <doi:10.1017/S0515036100014963> as well as Nikoloulopoulos (2013) <doi:10.1007/978-3-642-35407-6_11>. \n    For information on zero-inflated count regression generally, see Lambert (1992) \n    <https://www.jstor.org/stable/1269547>. The author acknowledges \n    support by NSF DMS-1925119 and DMS-212324.",
    "version": "1.3.4",
    "maintainer": "John Niehaus <jniehaus2257@gmail.com>",
    "author": "John Niehaus [aut, cre]",
    "url": "https://github.com/jmniehaus/bizicount",
    "bug_reports": "https://github.com/jmniehaus/bizicount/issues",
    "repository": "https://cran.r-project.org/package=bizicount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bizicount Bivariate Zero-Inflated Count Models Using Copulas Maximum likelihood estimation of copula-based zero-inflated \n    (and non-inflated) Poisson and negative binomial count models, based on the \n    article <doi:10.18637/jss.v109.i01>. Supports Frank and Gaussian copulas. \n    Allows for mixed margins (e.g., one margin Poisson, the other zero-inflated \n    negative binomial), and several marginal link functions. Built-in methods for \n    publication-quality tables using 'texreg', post-estimation diagnostics using \n    'DHARMa', and testing for marginal zero-modification via <doi:10.1177/0962280217749991>. \n    For information on copula regression for count data, see Genest and Ne\u0161lehov\u00e1 (2007) \n    <doi:10.1017/S0515036100014963> as well as Nikoloulopoulos (2013) <doi:10.1007/978-3-642-35407-6_11>. \n    For information on zero-inflated count regression generally, see Lambert (1992) \n    <https://www.jstor.org/stable/1269547>. The author acknowledges \n    support by NSF DMS-1925119 and DMS-212324.  "
  },
  {
    "id": 9258,
    "package_name": "blockCV",
    "title": "Spatial and Environmental Blocking for K-Fold and LOO\nCross-Validation",
    "description": "Creating spatially or environmentally separated folds for cross-validation to provide a robust error estimation in spatially structured environments; Investigating and visualising the effective range of spatial autocorrelation in continuous raster covariates and point samples to find an initial realistic distance band to separate training and testing datasets spatially described in Valavi, R. et al. (2019) <doi:10.1111/2041-210X.13107>.",
    "version": "3.2-0",
    "maintainer": "Roozbeh Valavi <valavi.r@gmail.com>",
    "author": "Roozbeh Valavi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2495-5277>),\n  Jane Elith [aut],\n  Jos\u00e9 Lahoz-Monfort [aut],\n  Ian Flint [aut],\n  Gurutzeta Guillera-Arroita [aut]",
    "url": "https://github.com/rvalavi/blockCV",
    "bug_reports": "https://github.com/rvalavi/blockCV/issues",
    "repository": "https://cran.r-project.org/package=blockCV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blockCV Spatial and Environmental Blocking for K-Fold and LOO\nCross-Validation Creating spatially or environmentally separated folds for cross-validation to provide a robust error estimation in spatially structured environments; Investigating and visualising the effective range of spatial autocorrelation in continuous raster covariates and point samples to find an initial realistic distance band to separate training and testing datasets spatially described in Valavi, R. et al. (2019) <doi:10.1111/2041-210X.13107>.  "
  },
  {
    "id": 9286,
    "package_name": "bmet",
    "title": "Bayesian Multigroup Equivalence Testing",
    "description": "Calculates the necessary quantities to perform Bayesian multigroup equivalence testing. \n             Currently the package includes the Bayesian models and equivalence criteria outlined in Pourmohamad and Lee (2023) \n             <doi:10.1002/sta4.645>, but more models and equivalence testing features may be added over time.",
    "version": "0.1.0",
    "maintainer": "Tony Pourmohamad <tpourmohamad@gmail.com>",
    "author": "Tony Pourmohamad [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bmet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bmet Bayesian Multigroup Equivalence Testing Calculates the necessary quantities to perform Bayesian multigroup equivalence testing. \n             Currently the package includes the Bayesian models and equivalence criteria outlined in Pourmohamad and Lee (2023) \n             <doi:10.1002/sta4.645>, but more models and equivalence testing features may be added over time.  "
  },
  {
    "id": 9298,
    "package_name": "bnlearn",
    "title": "Bayesian Network Structure Learning, Parameter Learning and\nInference",
    "description": "Bayesian network structure learning, parameter learning and inference.\n  This package implements constraint-based (PC, GS, IAMB, Inter-IAMB, Fast-IAMB, MMPC,\n  Hiton-PC, HPC), pairwise (ARACNE and Chow-Liu), score-based (Hill-Climbing and Tabu\n  Search) and hybrid (MMHC, RSMAX2, H2PC) structure learning algorithms for discrete,\n  Gaussian and conditional Gaussian networks, along with many score functions and\n  conditional independence tests.\n  The Naive Bayes and the Tree-Augmented Naive Bayes (TAN) classifiers are also implemented.\n  Some utility functions (model comparison and manipulation, random data generation, arc\n  orientation testing, simple and advanced plots) are included, as well as support for\n  parameter estimation (maximum likelihood and Bayesian) and inference, conditional\n  probability queries, cross-validation, bootstrap and model averaging.\n  Development snapshots with the latest bugfixes are available from <https://www.bnlearn.com/>.",
    "version": "5.1",
    "maintainer": "Marco Scutari <scutari@bnlearn.com>",
    "author": "Marco Scutari [aut, cre],\n  Tomi Silander [ctb]",
    "url": "https://www.bnlearn.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bnlearn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bnlearn Bayesian Network Structure Learning, Parameter Learning and\nInference Bayesian network structure learning, parameter learning and inference.\n  This package implements constraint-based (PC, GS, IAMB, Inter-IAMB, Fast-IAMB, MMPC,\n  Hiton-PC, HPC), pairwise (ARACNE and Chow-Liu), score-based (Hill-Climbing and Tabu\n  Search) and hybrid (MMHC, RSMAX2, H2PC) structure learning algorithms for discrete,\n  Gaussian and conditional Gaussian networks, along with many score functions and\n  conditional independence tests.\n  The Naive Bayes and the Tree-Augmented Naive Bayes (TAN) classifiers are also implemented.\n  Some utility functions (model comparison and manipulation, random data generation, arc\n  orientation testing, simple and advanced plots) are included, as well as support for\n  parameter estimation (maximum likelihood and Bayesian) and inference, conditional\n  probability queries, cross-validation, bootstrap and model averaging.\n  Development snapshots with the latest bugfixes are available from <https://www.bnlearn.com/>.  "
  },
  {
    "id": 9303,
    "package_name": "bnpMTP",
    "title": "Bayesian Nonparametric Sensitivity Analysis of Multiple Testing\nProcedures for p Values",
    "description": "Bayesian Nonparametric sensitivity analysis of multiple testing procedures for p values with arbitrary dependencies, based on the Dirichlet process prior distribution.",
    "version": "1.0.0",
    "maintainer": "George Karabatsos <gkarabatsos1@gmail.com>",
    "author": "George Karabatsos [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4243-2285>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bnpMTP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bnpMTP Bayesian Nonparametric Sensitivity Analysis of Multiple Testing\nProcedures for p Values Bayesian Nonparametric sensitivity analysis of multiple testing procedures for p values with arbitrary dependencies, based on the Dirichlet process prior distribution.  "
  },
  {
    "id": 9309,
    "package_name": "bodenmiller",
    "title": "Profiling of Peripheral Blood Mononuclear Cells using CyTOF",
    "description": "This data package contains a subset of the Bodenmiller et al, Nat Biotech 2012 dataset for testing single cell, high dimensional analysis and visualization methods.",
    "version": "0.1.1",
    "maintainer": "Yann Abraham <yann.abraham@gmail.com>",
    "author": "Yann Abraham [aut, cre],\n  Bernd Bodenmiller [aut]",
    "url": "https://github.com/yannabraham/bodenmiller",
    "bug_reports": "https://github.com/yannabraham/bodenmiller/issues",
    "repository": "https://cran.r-project.org/package=bodenmiller",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bodenmiller Profiling of Peripheral Blood Mononuclear Cells using CyTOF This data package contains a subset of the Bodenmiller et al, Nat Biotech 2012 dataset for testing single cell, high dimensional analysis and visualization methods.  "
  },
  {
    "id": 9333,
    "package_name": "bootCT",
    "title": "Bootstrapping the ARDL Tests for Cointegration",
    "description": "The bootstrap ARDL tests for cointegration is the main functionality of this package. It also acts as a wrapper of the most commond ARDL testing procedures for cointegration: the bound tests of Pesaran, Shin and Smith (PSS; 2001 - <doi:10.1002/jae.616>) and the asymptotic test on the independent variables of Sam, McNown and Goh (SMG: 2019 - <doi:10.1016/j.econmod.2018.11.001>). Bootstrap and bound tests are performed under both the conditional and unconditional ARDL models.",
    "version": "2.1.0",
    "maintainer": "Gianmarco Vacca <gianmarco.vacca@unicatt.it>",
    "author": "Gianmarco Vacca, Stefano Bertelli",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bootCT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bootCT Bootstrapping the ARDL Tests for Cointegration The bootstrap ARDL tests for cointegration is the main functionality of this package. It also acts as a wrapper of the most commond ARDL testing procedures for cointegration: the bound tests of Pesaran, Shin and Smith (PSS; 2001 - <doi:10.1002/jae.616>) and the asymptotic test on the independent variables of Sam, McNown and Goh (SMG: 2019 - <doi:10.1016/j.econmod.2018.11.001>). Bootstrap and bound tests are performed under both the conditional and unconditional ARDL models.  "
  },
  {
    "id": 9347,
    "package_name": "bootruin",
    "title": "A Bootstrap Test for the Probability of Ruin in the Classical\nRisk Process",
    "description": "We provide a framework for testing the probability of ruin in the classical (compound Poisson) risk process. It also includes some procedures for assessing and comparing the performance between the bootstrap test and the test using asymptotic normality.",
    "version": "1.2-4",
    "maintainer": "Benjamin Baumgartner <benjamin@baumgrt.com>",
    "author": "Benjamin Baumgartner <benjamin@baumgrt.com>, Riccardo Gatto <gatto@stat.unibe.ch>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=bootruin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bootruin A Bootstrap Test for the Probability of Ruin in the Classical\nRisk Process We provide a framework for testing the probability of ruin in the classical (compound Poisson) risk process. It also includes some procedures for assessing and comparing the performance between the bootstrap test and the test using asymptotic normality.  "
  },
  {
    "id": 9362,
    "package_name": "box.linters",
    "title": "Linters for 'box' Modules",
    "description": "Static code analysis of 'box' modules.\n  The package enhances code quality by providing linters that check for common issues,\n  enforce best practices, and ensure consistent coding standards.",
    "version": "0.10.7",
    "maintainer": "Ricardo Rodrigo Basa <opensource+rodrigo@appsilon.com>",
    "author": "Ricardo Rodrigo Basa [aut, cre],\n  Jakub Nowicki [aut],\n  Mateusz Ko\u0142oma\u0144ski [ctb],\n  Guilherme Vituri [ctb],\n  Appsilon Sp. z o.o. [cph]",
    "url": "https://appsilon.github.io/box.linters/,\nhttps://github.com/Appsilon/box.linters",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=box.linters",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "box.linters Linters for 'box' Modules Static code analysis of 'box' modules.\n  The package enhances code quality by providing linters that check for common issues,\n  enforce best practices, and ensure consistent coding standards.  "
  },
  {
    "id": 9410,
    "package_name": "breathtestcore",
    "title": "Core Functions to Read and Fit 13c Time Series from Breath Tests",
    "description": "Reads several formats of 13C data (IRIS/Wagner,\n    BreathID) and CSV.  Creates artificial sample data for testing.  Fits\n    Maes/Ghoos, Bluck-Coward self-correcting formula using 'nls', 'nlme'.\n    Methods to fit breath test curves with Bayesian Stan methods are\n    refactored to package 'breathteststan'. For a Shiny GUI, see package\n    'dmenne/breathtestshiny' on github.",
    "version": "0.8.10",
    "maintainer": "Dieter Menne <dieter.menne@menne-biomed.de>",
    "author": "Dieter Menne [aut, cre],\n  Menne Biomed Consulting Tuebingen [cph],\n  Benjamin Misselwitz [fnd],\n  Mark Fox [fnd],\n  Andreas Steingoetter [dtc],\n  University Hospital of Zurich, Dep. Gastroenterology [fnd, dtc]",
    "url": "https://github.com/dmenne/breathtestcore,",
    "bug_reports": "https://github.com/dmenne/breathtestcore/issues",
    "repository": "https://cran.r-project.org/package=breathtestcore",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "breathtestcore Core Functions to Read and Fit 13c Time Series from Breath Tests Reads several formats of 13C data (IRIS/Wagner,\n    BreathID) and CSV.  Creates artificial sample data for testing.  Fits\n    Maes/Ghoos, Bluck-Coward self-correcting formula using 'nls', 'nlme'.\n    Methods to fit breath test curves with Bayesian Stan methods are\n    refactored to package 'breathteststan'. For a Shiny GUI, see package\n    'dmenne/breathtestshiny' on github.  "
  },
  {
    "id": 9411,
    "package_name": "breathteststan",
    "title": "Stan-Based Fit to Gastric Emptying Curves",
    "description": "Stan-based curve-fitting function\n  for use with package 'breathtestcore' by the same author.\n  Stan functions are refactored here for easier testing.",
    "version": "0.8.9",
    "maintainer": "Dieter Menne <dieter.menne@menne-biomed.de>",
    "author": "Dieter Menne [aut, cre],\n  Menne Biomed Consulting Tuebingen [cph],\n  Benjamin Misselwitz [fnd],\n  Mark Fox [fnd],\n  University Hospital of Zurich, Dep. Gastroenterology [fnd, dtc]",
    "url": "https://github.com/dmenne/breathteststan,\nhttps://dmenne.github.io/breathteststan/",
    "bug_reports": "https://github.com/dmenne/breathteststan/issues",
    "repository": "https://cran.r-project.org/package=breathteststan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "breathteststan Stan-Based Fit to Gastric Emptying Curves Stan-based curve-fitting function\n  for use with package 'breathtestcore' by the same author.\n  Stan functions are refactored here for easier testing.  "
  },
  {
    "id": 9555,
    "package_name": "calibrationband",
    "title": "Calibration Bands",
    "description": "Package to assess the calibration of probabilistic classifiers using confidence bands for monotonic functions. Besides testing the classical goodness-of-fit null hypothesis of perfect calibration, the confidence bands calculated within that package facilitate inverted goodness-of-fit tests whose rejection allows for a sought-after conclusion of a sufficiently well-calibrated model. The package creates flexible graphical tools to perform these tests.  For construction details see also Dimitriadis, D\u00fcmbgen, Henzi, Puke, Ziegel (2022) <arXiv:2203.04065>. ",
    "version": "0.2.1",
    "maintainer": "Marius Puke <marius.puke@uni-hohenheim.de>",
    "author": "Timo Dimitriadis [aut],\n  Alexander Henzi [aut],\n  Marius Puke [aut, cre]",
    "url": "https://github.com/marius-cp/calibrationband,\nhttps://marius-cp.github.io/calibrationband/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=calibrationband",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "calibrationband Calibration Bands Package to assess the calibration of probabilistic classifiers using confidence bands for monotonic functions. Besides testing the classical goodness-of-fit null hypothesis of perfect calibration, the confidence bands calculated within that package facilitate inverted goodness-of-fit tests whose rejection allows for a sought-after conclusion of a sufficiently well-calibrated model. The package creates flexible graphical tools to perform these tests.  For construction details see also Dimitriadis, D\u00fcmbgen, Henzi, Puke, Ziegel (2022) <arXiv:2203.04065>.   "
  },
  {
    "id": 9563,
    "package_name": "calms",
    "title": "Comprehensive Analysis of Latent Means",
    "description": "Provides a Shiny application to conduct comprehensive analysis of latent means including the examination of group equivalency, propensity score analysis, \n measurement invariance analysis, and assessment of latent mean differences of \n equivalent groups with invariant data. Group equivalency and propensity score analyses are implemented using the 'MatchIt' package [Ho et al. (2011) <doi:10.18637/jss.v042.i08>], \n ensuring robust control for covariates. Structural equation modeling and invariance testing rely heavily on the 'lavaan' package [Rosseel (2012) <doi:10.18637/jss.v048.i02>], \n providing a flexible and powerful modeling framework. The application also integrates modified functions from Hammack-Brown et al. (2021) <doi:10.1002/hrdq.21452> \n to support factor ratio testing and the list-and-delete procedure. ",
    "version": "1.0-1",
    "maintainer": "Kim Nimon <kim.nimon@gmail.com>",
    "author": "Kim Nimon [aut, cre],\n  Julia Fulmore [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=calms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "calms Comprehensive Analysis of Latent Means Provides a Shiny application to conduct comprehensive analysis of latent means including the examination of group equivalency, propensity score analysis, \n measurement invariance analysis, and assessment of latent mean differences of \n equivalent groups with invariant data. Group equivalency and propensity score analyses are implemented using the 'MatchIt' package [Ho et al. (2011) <doi:10.18637/jss.v042.i08>], \n ensuring robust control for covariates. Structural equation modeling and invariance testing rely heavily on the 'lavaan' package [Rosseel (2012) <doi:10.18637/jss.v048.i02>], \n providing a flexible and powerful modeling framework. The application also integrates modified functions from Hammack-Brown et al. (2021) <doi:10.1002/hrdq.21452> \n to support factor ratio testing and the list-and-delete procedure.   "
  },
  {
    "id": 9584,
    "package_name": "canvasXpress.data",
    "title": "Datasets for the 'canvasXpress' Package",
    "description": "Contains the prepared data that is needed for the 'shiny' application examples in the \n    'canvasXpress' package.  This package also includes datasets used for automated 'testthat' tests.\n    Scotto L, Narayan G, Nandula SV, Arias-Pulido H et al. (2008) <doi:10.1002/gcc.20577>.\n    Davis S, Meltzer PS (2007) <doi:10.1093/bioinformatics/btm254>.",
    "version": "1.34.2",
    "maintainer": "Connie Brett <connie@aggregate-genius.com>",
    "author": "Isaac Neuhaus [aut],\n  Connie Brett [aut, cre],\n  Ger Inberg [aut]",
    "url": "https://github.com/neuhausi/canvasXpress.data",
    "bug_reports": "https://github.com/neuhausi/canvasXpress.data/issues",
    "repository": "https://cran.r-project.org/package=canvasXpress.data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "canvasXpress.data Datasets for the 'canvasXpress' Package Contains the prepared data that is needed for the 'shiny' application examples in the \n    'canvasXpress' package.  This package also includes datasets used for automated 'testthat' tests.\n    Scotto L, Narayan G, Nandula SV, Arias-Pulido H et al. (2008) <doi:10.1002/gcc.20577>.\n    Davis S, Meltzer PS (2007) <doi:10.1093/bioinformatics/btm254>.  "
  },
  {
    "id": 9599,
    "package_name": "carat",
    "title": "Covariate-Adaptive Randomization for Clinical Trials",
    "description": "Provides functions and command-line user interface to generate allocation sequence by covariate-adaptive randomization for clinical trials. The package currently supports six covariate-adaptive randomization procedures. Three hypothesis testing methods that are valid and robust under covariate-adaptive randomization are also available in the package to facilitate the inference for treatment effect under the included randomization procedures. Additionally, the package provides comprehensive and efficient tools to allow one to evaluate and compare the performance of randomization procedures and tests based on various criteria. See Ma W, Ye X, Tu F, and Hu F (2023) <doi: 10.18637/jss.v107.i02> for details. ",
    "version": "2.2.1",
    "maintainer": "Xiaoqing Ye <ye_xiaoq@163.com>",
    "author": "Fuyi Tu [aut],\n  Xiaoqing Ye [aut, cre],\n  Wei Ma [aut, ths],\n  Feifang Hu [aut, ths]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=carat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "carat Covariate-Adaptive Randomization for Clinical Trials Provides functions and command-line user interface to generate allocation sequence by covariate-adaptive randomization for clinical trials. The package currently supports six covariate-adaptive randomization procedures. Three hypothesis testing methods that are valid and robust under covariate-adaptive randomization are also available in the package to facilitate the inference for treatment effect under the included randomization procedures. Additionally, the package provides comprehensive and efficient tools to allow one to evaluate and compare the performance of randomization procedures and tests based on various criteria. See Ma W, Ye X, Tu F, and Hu F (2023) <doi: 10.18637/jss.v107.i02> for details.   "
  },
  {
    "id": 9610,
    "package_name": "careless",
    "title": "Procedures for Computing Indices of Careless Responding",
    "description": "When taking online surveys, participants sometimes respond to items\n    without regard to their content. These types of responses, referred to as \n    careless or insufficient effort responding, constitute significant problems \n    for data quality, leading to distortions in data analysis and hypothesis \n    testing, such as spurious correlations. The 'R' package 'careless' provides \n    solutions designed to detect such careless / insufficient effort responses \n    by allowing easy calculation of indices proposed in the literature. It \n    currently supports the calculation of longstring, even-odd consistency, \n    psychometric synonyms/antonyms, Mahalanobis distance, and intra-individual \n    response variability (also termed inter-item standard deviation). \n    For a review of these methods, see Curran (2016) <doi:10.1016/j.jesp.2015.07.006>.",
    "version": "1.2.2",
    "maintainer": "Richard Yentes <ryentes@gmail.com>",
    "author": "Richard Yentes [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-6767-8065>),\n  Francisco Wilhelm [aut]",
    "url": "https://github.com/ryentes/careless/",
    "bug_reports": "https://github.com/ryentes/careless/issues",
    "repository": "https://cran.r-project.org/package=careless",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "careless Procedures for Computing Indices of Careless Responding When taking online surveys, participants sometimes respond to items\n    without regard to their content. These types of responses, referred to as \n    careless or insufficient effort responding, constitute significant problems \n    for data quality, leading to distortions in data analysis and hypothesis \n    testing, such as spurious correlations. The 'R' package 'careless' provides \n    solutions designed to detect such careless / insufficient effort responses \n    by allowing easy calculation of indices proposed in the literature. It \n    currently supports the calculation of longstring, even-odd consistency, \n    psychometric synonyms/antonyms, Mahalanobis distance, and intra-individual \n    response variability (also termed inter-item standard deviation). \n    For a review of these methods, see Curran (2016) <doi:10.1016/j.jesp.2015.07.006>.  "
  },
  {
    "id": 9642,
    "package_name": "catR",
    "title": "Generation of IRT Response Patterns under Computerized Adaptive\nTesting",
    "description": "Provides routines for the generation of response patterns under unidimensional dichotomous and polytomous computerized adaptive testing (CAT) framework. It holds many standard functions to estimate ability, select the first item(s) to administer and optimally select the next item, as well as several stopping rules. Options to control for item exposure and content balancing are also available (Magis and Barrada (2017) <doi:10.18637/jss.v076.c01>).",
    "version": "3.17",
    "maintainer": "Cheng Hua <chuabest@gmail.com>",
    "author": "David Magis (U Liege, Belgium), Gilles Raiche (UQAM, Canada), Juan Ramon Barrada (U Zaragoza, Spain)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=catR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catR Generation of IRT Response Patterns under Computerized Adaptive\nTesting Provides routines for the generation of response patterns under unidimensional dichotomous and polytomous computerized adaptive testing (CAT) framework. It holds many standard functions to estimate ability, select the first item(s) to administer and optimally select the next item, as well as several stopping rules. Options to control for item exposure and content balancing are also available (Magis and Barrada (2017) <doi:10.18637/jss.v076.c01>).  "
  },
  {
    "id": 9643,
    "package_name": "catSurv",
    "title": "Computerized Adaptive Testing for Survey Research",
    "description": "Provides methods of computerized adaptive testing for survey researchers.  See Montgomery and Rossiter (2020) <doi:10.1093/jssam/smz027>. Includes functionality for data fit with the classic item response methods including the latent trait model, the Birnbaum three parameter model, the graded response, and the generalized partial credit model.  Additionally, includes several ability parameter estimation and item selection routines.  During item selection, all calculations are done in compiled C++ code.",
    "version": "1.6.0",
    "maintainer": "Erin Rossiter <erossite@nd.edu>",
    "author": "Jacob Montgomery [aut],\n  Erin Rossiter [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/erossiter/catSurv/issues",
    "repository": "https://cran.r-project.org/package=catSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catSurv Computerized Adaptive Testing for Survey Research Provides methods of computerized adaptive testing for survey researchers.  See Montgomery and Rossiter (2020) <doi:10.1093/jssam/smz027>. Includes functionality for data fit with the classic item response methods including the latent trait model, the Birnbaum three parameter model, the graded response, and the generalized partial credit model.  Additionally, includes several ability parameter estimation and item selection routines.  During item selection, all calculations are done in compiled C++ code.  "
  },
  {
    "id": 9648,
    "package_name": "catcont",
    "title": "Test, Identify, Select and Mutate Categorical or Continuous\nValues",
    "description": "Methods and utilities for testing, identifying, selecting and  \n    mutating objects as categorical or continous types. These functions work on both \n    atomic vectors as well as recursive objects: data.frames, data.tables, \n    tibbles, lists, etc.. ",
    "version": "0.5.0",
    "maintainer": "Christopher Brown <chris.brown@decisionpatterns.com>",
    "author": "Christopher Brown [aut, cre],\n  Decision Patterns [cph]",
    "url": "https://github.com/decisionpatterns/catcont\nhttp://www.decisionpatterns.com",
    "bug_reports": "https://github.com/decisionpatterns/catcont/issues",
    "repository": "https://cran.r-project.org/package=catcont",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "catcont Test, Identify, Select and Mutate Categorical or Continuous\nValues Methods and utilities for testing, identifying, selecting and  \n    mutating objects as categorical or continous types. These functions work on both \n    atomic vectors as well as recursive objects: data.frames, data.tables, \n    tibbles, lists, etc..   "
  },
  {
    "id": 9724,
    "package_name": "cdcatR",
    "title": "Cognitive Diagnostic Computerized Adaptive Testing",
    "description": "Provides a set of functions for conducting cognitive diagnostic computerized adaptive testing applications (Chen, 2009) <DOI:10.1007/s11336-009-9123-2>). It includes different item selection rules such us the global discrimination index (Kaplan, de la Torre, and Barrada (2015) <DOI:10.1177/0146621614554650>) and the nonparametric selection method (Chang, Chiu, and Tsai (2019) <DOI:10.1177/0146621618813113>), as well as several stopping rules. Functions for generating item banks and responses are also provided. To guide item bank calibration, model comparison at the item level can be conducted using the two-step likelihood ratio test statistic by Sorrel, de la Torre, Abad and Olea (2017) <DOI:10.1027/1614-2241/a000131>.",
    "version": "1.0.6",
    "maintainer": "Miguel A. Sorrel <miguel.sorrel@uam.es>",
    "author": "Miguel A. Sorrel [aut, cre, cph],\n  Pablo N\u00e1jera [aut, cph],\n  Francisco J. Abad [aut, cph]",
    "url": "https://github.com/miguel-sorrel/cdcatR",
    "bug_reports": "https://github.com/miguel-sorrel/cdcatR/issues",
    "repository": "https://cran.r-project.org/package=cdcatR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cdcatR Cognitive Diagnostic Computerized Adaptive Testing Provides a set of functions for conducting cognitive diagnostic computerized adaptive testing applications (Chen, 2009) <DOI:10.1007/s11336-009-9123-2>). It includes different item selection rules such us the global discrimination index (Kaplan, de la Torre, and Barrada (2015) <DOI:10.1177/0146621614554650>) and the nonparametric selection method (Chang, Chiu, and Tsai (2019) <DOI:10.1177/0146621618813113>), as well as several stopping rules. Functions for generating item banks and responses are also provided. To guide item bank calibration, model comparison at the item level can be conducted using the two-step likelihood ratio test statistic by Sorrel, de la Torre, Abad and Olea (2017) <DOI:10.1027/1614-2241/a000131>.  "
  },
  {
    "id": 9834,
    "package_name": "checkthat",
    "title": "Intuitive Unit Testing Tools for Data Manipulation",
    "description": "Provides a lightweight data validation and testing toolkit for R. \n    Its guiding philosophy is that adding code-based data checks to users' \n    existing workflow should be both quick and intuitive. The suite of \n    functions included therefore mirror the common data checks many users \n    already perform by hand or by eye. Additionally, the 'checkthat' package is \n    optimized to work within 'tidyverse' data manipulation pipelines.",
    "version": "0.1.0",
    "maintainer": "Ian Cero <ian_cero@urmc.rochester.edu>",
    "author": "Ian Cero [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2862-0450>)",
    "url": "https://github.com/iancero/checkthat,\nhttps://iancero.github.io/checkthat/",
    "bug_reports": "https://github.com/iancero/checkthat/issues",
    "repository": "https://cran.r-project.org/package=checkthat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "checkthat Intuitive Unit Testing Tools for Data Manipulation Provides a lightweight data validation and testing toolkit for R. \n    Its guiding philosophy is that adding code-based data checks to users' \n    existing workflow should be both quick and intuitive. The suite of \n    functions included therefore mirror the common data checks many users \n    already perform by hand or by eye. Additionally, the 'checkthat' package is \n    optimized to work within 'tidyverse' data manipulation pipelines.  "
  },
  {
    "id": 9845,
    "package_name": "cherry",
    "title": "Multiple Testing Methods for Exploratory Research",
    "description": "Provides an alternative approach to multiple testing\n        by calculating a simultaneous upper confidence bounds for the\n        number of true null hypotheses among any subset of the hypotheses of interest, \n        using the methods of Goeman and Solari (2011) <doi:10.1214/11-STS356>. ",
    "version": "0.6-15",
    "maintainer": "Jelle Goeman <j.j.goeman@lumc.nl>",
    "author": "Jelle Goeman [aut, cre],\n  Aldo Solari [aut],\n  Rosa Meijer [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cherry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cherry Multiple Testing Methods for Exploratory Research Provides an alternative approach to multiple testing\n        by calculating a simultaneous upper confidence bounds for the\n        number of true null hypotheses among any subset of the hypotheses of interest, \n        using the methods of Goeman and Solari (2011) <doi:10.1214/11-STS356>.   "
  },
  {
    "id": 9865,
    "package_name": "chngpt",
    "title": "Estimation and Hypothesis Testing for Threshold Regression",
    "description": "Threshold regression models are also called two-phase regression, broken-stick regression, split-point regression, structural change models, and regression kink models, with and without interaction terms. Methods for both continuous and discontinuous threshold models are included, but the support for the former is much greater. This package is described in Fong, Huang, Gilbert and Permar (2017) <DOI:10.1186/s12859-017-1863-x> and the package vignette.",
    "version": "2024.11-15",
    "maintainer": "Youyi Fong <youyifong@gmail.com>",
    "author": "Youyi Fong [cre],\n  Qianqian Chen [aut],\n  Shuangcheng Hua [aut],\n  Hyunju Son [aut],\n  Adam Elder [aut],\n  Tao Yang [aut],\n  Zonglin He [aut],\n  Simone Giannerini [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=chngpt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "chngpt Estimation and Hypothesis Testing for Threshold Regression Threshold regression models are also called two-phase regression, broken-stick regression, split-point regression, structural change models, and regression kink models, with and without interaction terms. Methods for both continuous and discontinuous threshold models are included, but the support for the former is much greater. This package is described in Fong, Huang, Gilbert and Permar (2017) <DOI:10.1186/s12859-017-1863-x> and the package vignette.  "
  },
  {
    "id": 9922,
    "package_name": "cit",
    "title": "Causal Inference Test",
    "description": "A likelihood-based hypothesis testing approach is implemented for\n  assessing causal mediation. Described in Millstein, Chen, and Breton (2016),\n  <DOI:10.1093/bioinformatics/btw135>, it could be used to test for mediation\n  of a known causal association between a DNA variant, the 'instrumental variable',\n  and a clinical outcome or phenotype by gene expression or DNA methylation, the\n  potential mediator. Another example would be testing mediation of the effect\n  of a drug on a clinical outcome by the molecular target. The hypothesis test\n  generates a p-value or permutation-based FDR value with confidence intervals\n  to quantify uncertainty in the causal inference. The outcome can be represented\n  by either a continuous or binary variable, the potential mediator is continuous,\n  and the instrumental variable can be continuous or binary and is not limited to\n  a single variable but may be a design matrix representing multiple variables.",
    "version": "2.3.2",
    "maintainer": "Joshua Millstein <joshua.millstein@usc.edu>",
    "author": "Joshua Millstein [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7961-8943>)",
    "url": "https://github.com/USCbiostats/cit",
    "bug_reports": "https://github.com/USCbiostats/cit/issues",
    "repository": "https://cran.r-project.org/package=cit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cit Causal Inference Test A likelihood-based hypothesis testing approach is implemented for\n  assessing causal mediation. Described in Millstein, Chen, and Breton (2016),\n  <DOI:10.1093/bioinformatics/btw135>, it could be used to test for mediation\n  of a known causal association between a DNA variant, the 'instrumental variable',\n  and a clinical outcome or phenotype by gene expression or DNA methylation, the\n  potential mediator. Another example would be testing mediation of the effect\n  of a drug on a clinical outcome by the molecular target. The hypothesis test\n  generates a p-value or permutation-based FDR value with confidence intervals\n  to quantify uncertainty in the causal inference. The outcome can be represented\n  by either a continuous or binary variable, the potential mediator is continuous,\n  and the instrumental variable can be continuous or binary and is not limited to\n  a single variable but may be a design matrix representing multiple variables.  "
  },
  {
    "id": 9940,
    "package_name": "clampSeg",
    "title": "Idealisation of Patch Clamp Recordings",
    "description": "Implements the model-free multiscale idealisation approaches: Jump-Segmentation by MUltiResolution Filter (JSMURF), Hotz et al. (2013) <doi:10.1109/TNB.2013.2284063>, JUmp Local dEconvolution Segmentation filter (JULES), Pein et al. (2018) <doi:10.1109/TNB.2018.2845126>, and Heterogeneous Idealization by Local testing and DEconvolution (HILDE), Pein et al. (2021) <doi:10.1109/TNB.2020.3031202>. Further details on how to use them are given in the accompanying vignette.",
    "version": "1.2-0",
    "maintainer": "Pein Florian <f.pein@lancaster.ac.uk>",
    "author": "Pein Florian [aut, cre],\n  Timo Aspelmeier [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clampSeg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clampSeg Idealisation of Patch Clamp Recordings Implements the model-free multiscale idealisation approaches: Jump-Segmentation by MUltiResolution Filter (JSMURF), Hotz et al. (2013) <doi:10.1109/TNB.2013.2284063>, JUmp Local dEconvolution Segmentation filter (JULES), Pein et al. (2018) <doi:10.1109/TNB.2018.2845126>, and Heterogeneous Idealization by Local testing and DEconvolution (HILDE), Pein et al. (2021) <doi:10.1109/TNB.2020.3031202>. Further details on how to use them are given in the accompanying vignette.  "
  },
  {
    "id": 9953,
    "package_name": "cld",
    "title": "Create Compact Letter Display (CLD) for Statistical Comparisons",
    "description": "Creates compact letter displays (CLDs) for pairwise comparisons \n    from statistical post-hoc tests. Groups sharing the same letter are not \n    significantly different from each other. Supports multiple input formats \n    including results from 'stats' pairwise tests, 'DescTools', 'PMCMRplus', \n    'rstatix', symmetric matrices of p-values, and data frames. Provides a \n    consistent interface for visualizing statistical groupings across different \n    testing frameworks.",
    "version": "0.0.1",
    "maintainer": "Vilmantas Gegzna <GegznaV@gmail.com>",
    "author": "Vilmantas Gegzna [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9500-5167>)",
    "url": "https://gegznav.github.io/cld/",
    "bug_reports": "https://github.com/GegznaV/cld/issues",
    "repository": "https://cran.r-project.org/package=cld",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cld Create Compact Letter Display (CLD) for Statistical Comparisons Creates compact letter displays (CLDs) for pairwise comparisons \n    from statistical post-hoc tests. Groups sharing the same letter are not \n    significantly different from each other. Supports multiple input formats \n    including results from 'stats' pairwise tests, 'DescTools', 'PMCMRplus', \n    'rstatix', symmetric matrices of p-values, and data frames. Provides a \n    consistent interface for visualizing statistical groupings across different \n    testing frameworks.  "
  },
  {
    "id": 9962,
    "package_name": "cleanr",
    "title": "Helps You to Code Cleaner",
    "description": "Check your R code for some of the most common\n    layout flaws.  Many tried to teach us how to write code less dreadful,\n    be it implicitly as B. W. Kernighan and D. M. Ritchie (1988)\n    <ISBN:0-13-110362-8> in 'The C Programming Language' did, be it\n    explicitly as R.C. Martin (2008) <ISBN:0-13-235088-2> in 'Clean Code:\n    A Handbook of Agile Software Craftsmanship' did.  So we should check\n    our code for files too long or wide, functions with too many lines,\n    too wide lines, too many arguments or too many levels of nesting.\n    Note: This is not a static code analyzer like pylint or the like.\n    Checkout <https://cran.r-project.org/package=lintr> instead.",
    "version": "1.4.0",
    "maintainer": "Andreas Dominik Cullmann <fvafrcu@mailbox.org>",
    "author": "Andreas Dominik Cullmann [aut, cre]",
    "url": "https://gitlab.com/fvafrcu/cleanr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cleanr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cleanr Helps You to Code Cleaner Check your R code for some of the most common\n    layout flaws.  Many tried to teach us how to write code less dreadful,\n    be it implicitly as B. W. Kernighan and D. M. Ritchie (1988)\n    <ISBN:0-13-110362-8> in 'The C Programming Language' did, be it\n    explicitly as R.C. Martin (2008) <ISBN:0-13-235088-2> in 'Clean Code:\n    A Handbook of Agile Software Craftsmanship' did.  So we should check\n    our code for files too long or wide, functions with too many lines,\n    too wide lines, too many arguments or too many levels of nesting.\n    Note: This is not a static code analyzer like pylint or the like.\n    Checkout <https://cran.r-project.org/package=lintr> instead.  "
  },
  {
    "id": 9989,
    "package_name": "clinical",
    "title": "Analysis of Clinical Data",
    "description": "A collection of tools to easily analyze clinical data, including functions for correlation analysis, \n  and statistical testing. The package facilitates the integration of clinical metadata with other omics layers, \n  enabling exploration of quantitative variables. It also includes the utility for frequency matching samples across \n  a dataset based on patient variables.",
    "version": "0.1",
    "maintainer": "Stefano Cacciatore <tkcaccia@gmail.com>",
    "author": "Stefano Cacciatore [aut, trl, cre] (ORCID:\n    <https://orcid.org/0000-0001-7052-7156>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clinical",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinical Analysis of Clinical Data A collection of tools to easily analyze clinical data, including functions for correlation analysis, \n  and statistical testing. The package facilitates the integration of clinical metadata with other omics layers, \n  enabling exploration of quantitative variables. It also includes the utility for frequency matching samples across \n  a dataset based on patient variables.  "
  },
  {
    "id": 9998,
    "package_name": "clintrialx",
    "title": "Connect and Work with Clinical Trials Data Sources",
    "description": "Are you spending too much time fetching and managing clinical trial data? Struggling with complex queries and bulk data extraction? What if you could simplify this process with just a few lines of code? Introducing 'clintrialx' - Fetch clinical trial data from sources like 'ClinicalTrials.gov' <https://clinicaltrials.gov/> and the 'Clinical Trials Transformation Initiative - Access to Aggregate Content of ClinicalTrials.gov' database <https://aact.ctti-clinicaltrials.org/>, supporting pagination and bulk downloads. Also, you can generate HTML reports based on the data obtained from the sources!",
    "version": "0.1.1",
    "maintainer": "Indraneel Chakraborty <hello.indraneel@gmail.com>",
    "author": "Indraneel Chakraborty [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6958-8269>)",
    "url": "http://www.indraneelchakraborty.com/clintrialx/,\nhttps://github.com/ineelhere/clintrialx",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=clintrialx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clintrialx Connect and Work with Clinical Trials Data Sources Are you spending too much time fetching and managing clinical trial data? Struggling with complex queries and bulk data extraction? What if you could simplify this process with just a few lines of code? Introducing 'clintrialx' - Fetch clinical trial data from sources like 'ClinicalTrials.gov' <https://clinicaltrials.gov/> and the 'Clinical Trials Transformation Initiative - Access to Aggregate Content of ClinicalTrials.gov' database <https://aact.ctti-clinicaltrials.org/>, supporting pagination and bulk downloads. Also, you can generate HTML reports based on the data obtained from the sources!  "
  },
  {
    "id": 10019,
    "package_name": "clubSandwich",
    "title": "Cluster-Robust (Sandwich) Variance Estimators with Small-Sample\nCorrections",
    "description": "Provides several cluster-robust variance estimators (i.e.,\n    sandwich estimators) for ordinary and weighted least squares linear regression\n    models, including the bias-reduced linearization estimator introduced by Bell\n    and McCaffrey (2002) \n    <https://www150.statcan.gc.ca/n1/pub/12-001-x/2002002/article/9058-eng.pdf> and \n    developed further by Pustejovsky and Tipton (2017) \n    <DOI:10.1080/07350015.2016.1247004>. The package includes functions for estimating\n    the variance- covariance matrix and for testing single- and multiple-\n    contrast hypotheses based on Wald test statistics. Tests of single regression\n    coefficients use Satterthwaite or saddle-point corrections. Tests of multiple-\n    contrast hypotheses use an approximation to Hotelling's T-squared distribution.\n    Methods are provided for a variety of fitted models, including lm() and mlm\n    objects, glm(), geeglm() (from package 'geepack'), lm_robust() and lm_lin() \n    (from package 'estimatr'), ivreg() (from package 'AER'), ivreg() (from package 'ivreg' when \n    estimated by ordinary least squares), plm() (from package 'plm'), gls() and \n    lme() (from 'nlme'), lmer() (from `lme4`), robu() (from 'robumeta'), and rma.uni() \n    and rma.mv() (from 'metafor').",
    "version": "0.6.1",
    "maintainer": "James E. Pustejovsky <jepusto@gmail.com>",
    "author": "James E. Pustejovsky [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0591-9465>),\n  Samuel Pekofsky [ctb],\n  Jingru Zhang [ctb]",
    "url": "http://jepusto.github.io/clubSandwich/",
    "bug_reports": "https://github.com/jepusto/clubSandwich/issues",
    "repository": "https://cran.r-project.org/package=clubSandwich",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clubSandwich Cluster-Robust (Sandwich) Variance Estimators with Small-Sample\nCorrections Provides several cluster-robust variance estimators (i.e.,\n    sandwich estimators) for ordinary and weighted least squares linear regression\n    models, including the bias-reduced linearization estimator introduced by Bell\n    and McCaffrey (2002) \n    <https://www150.statcan.gc.ca/n1/pub/12-001-x/2002002/article/9058-eng.pdf> and \n    developed further by Pustejovsky and Tipton (2017) \n    <DOI:10.1080/07350015.2016.1247004>. The package includes functions for estimating\n    the variance- covariance matrix and for testing single- and multiple-\n    contrast hypotheses based on Wald test statistics. Tests of single regression\n    coefficients use Satterthwaite or saddle-point corrections. Tests of multiple-\n    contrast hypotheses use an approximation to Hotelling's T-squared distribution.\n    Methods are provided for a variety of fitted models, including lm() and mlm\n    objects, glm(), geeglm() (from package 'geepack'), lm_robust() and lm_lin() \n    (from package 'estimatr'), ivreg() (from package 'AER'), ivreg() (from package 'ivreg' when \n    estimated by ordinary least squares), plm() (from package 'plm'), gls() and \n    lme() (from 'nlme'), lmer() (from `lme4`), robu() (from 'robumeta'), and rma.uni() \n    and rma.mv() (from 'metafor').  "
  },
  {
    "id": 10081,
    "package_name": "cmprsk",
    "title": "Subdistribution Analysis of Competing Risks",
    "description": "Estimation, testing and regression modeling of\n subdistribution functions in competing risks, as described in Gray\n (1988), A class of K-sample tests for comparing the cumulative\n incidence of a competing risk, Ann. Stat. 16:1141-1154\n <DOI:10.1214/aos/1176350951>, and Fine JP and\n Gray RJ (1999), A proportional hazards model for the subdistribution\n of a competing risk, JASA, 94:496-509, <DOI:10.1080/01621459.1999.10474144>.",
    "version": "2.2-12",
    "maintainer": "Bob Gray <gray@jimmy.harvard.edu>",
    "author": "Bob Gray <gray@jimmy.harvard.edu>",
    "url": "https://www.R-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cmprsk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmprsk Subdistribution Analysis of Competing Risks Estimation, testing and regression modeling of\n subdistribution functions in competing risks, as described in Gray\n (1988), A class of K-sample tests for comparing the cumulative\n incidence of a competing risk, Ann. Stat. 16:1141-1154\n <DOI:10.1214/aos/1176350951>, and Fine JP and\n Gray RJ (1999), A proportional hazards model for the subdistribution\n of a competing risk, JASA, 94:496-509, <DOI:10.1080/01621459.1999.10474144>.  "
  },
  {
    "id": 10082,
    "package_name": "cmprskQR",
    "title": "Analysis of Competing Risks Using Quantile Regressions",
    "description": "Estimation, testing and regression modeling of\n subdistribution functions in competing risks using quantile regressions,\n as described in Peng and Fine (2009) <DOI:10.1198/jasa.2009.tm08228>.",
    "version": "0.9.2",
    "maintainer": "Stephan Dlugosz <stephan.dlugosz@googlemail.com>",
    "author": "Stephan Dlugosz [aut, cre], \n        Limin Peng [aut],\n        Ruosha Li [aut],\n        Shuolin Shi [ctb]",
    "url": "https://bitbucket.org/sdlugosz/cmprskqr",
    "bug_reports": "https://bitbucket.org/sdlugosz/cmprskqr/issues",
    "repository": "https://cran.r-project.org/package=cmprskQR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmprskQR Analysis of Competing Risks Using Quantile Regressions Estimation, testing and regression modeling of\n subdistribution functions in competing risks using quantile regressions,\n as described in Peng and Fine (2009) <DOI:10.1198/jasa.2009.tm08228>.  "
  },
  {
    "id": 10090,
    "package_name": "cmstatr",
    "title": "Statistical Methods for Composite Material Data",
    "description": "An implementation of the statistical methods commonly\n  used for advanced composite materials in aerospace applications.\n  This package focuses on calculating basis values (lower tolerance\n  bounds) for material strength properties, as well as performing the\n  associated diagnostic tests. This package provides functions for\n  calculating basis values assuming several different distributions,\n  as well as providing functions for non-parametric methods of computing\n  basis values. Functions are also provided for testing the hypothesis\n  that there is no difference between strength and modulus data from an\n  alternate sample and that from a \"qualification\" or \"baseline\" sample.\n  For a discussion of these statistical methods and their use, see the\n  Composite Materials Handbook, Volume 1 (2012, ISBN: 978-0-7680-7811-4).\n  Additional details about this package are available in the paper by\n  Kloppenborg (2020, <doi:10.21105/joss.02265>).",
    "version": "0.10.0",
    "maintainer": "Stefan Kloppenborg <stefan@kloppenborg.ca>",
    "author": "Stefan Kloppenborg [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1908-5214>),\n  Billy Cheng [ctb],\n  Ally Fraser [ctb],\n  Jeffrey Borlik [ctb],\n  Brice Langston [ctb],\n  Comtek Advanced Structures, Ltd. [fnd]",
    "url": "https://www.cmstatr.net/, https://github.com/cmstatr/cmstatr",
    "bug_reports": "https://github.com/cmstatr/cmstatr/issues",
    "repository": "https://cran.r-project.org/package=cmstatr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cmstatr Statistical Methods for Composite Material Data An implementation of the statistical methods commonly\n  used for advanced composite materials in aerospace applications.\n  This package focuses on calculating basis values (lower tolerance\n  bounds) for material strength properties, as well as performing the\n  associated diagnostic tests. This package provides functions for\n  calculating basis values assuming several different distributions,\n  as well as providing functions for non-parametric methods of computing\n  basis values. Functions are also provided for testing the hypothesis\n  that there is no difference between strength and modulus data from an\n  alternate sample and that from a \"qualification\" or \"baseline\" sample.\n  For a discussion of these statistical methods and their use, see the\n  Composite Materials Handbook, Volume 1 (2012, ISBN: 978-0-7680-7811-4).\n  Additional details about this package are available in the paper by\n  Kloppenborg (2020, <doi:10.21105/joss.02265>).  "
  },
  {
    "id": 10136,
    "package_name": "codewhere",
    "title": "Find the Location of an R Package's Code",
    "description": "Find the location of the code for an R package based on the package's name or string representation.\n    Checks on 'CRAN' based on information in the 'URL' field or 'BioConductor' and 'GitHub' based on constructing \n    a URL, and verifies all paths via testing for a successful response. This can be useful when automating static \n    code analysis based on a list of package names, and similar tasks.",
    "version": "0.1.1",
    "maintainer": "Nic Crane <thisisnic@gmail.com>",
    "author": "Nic Crane [aut, cre, cph]",
    "url": "https://thisisnic.github.io/codewhere/,\nhttps://github.com/thisisnic/codewhere/",
    "bug_reports": "https://github.com/thisisnic/codewhere/issues",
    "repository": "https://cran.r-project.org/package=codewhere",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "codewhere Find the Location of an R Package's Code Find the location of the code for an R package based on the package's name or string representation.\n    Checks on 'CRAN' based on information in the 'URL' field or 'BioConductor' and 'GitHub' based on constructing \n    a URL, and verifies all paths via testing for a successful response. This can be useful when automating static \n    code analysis based on a list of package names, and similar tasks.  "
  },
  {
    "id": 10147,
    "package_name": "cofad",
    "title": "Contrast Analyses for Factorial Designs",
    "description": "Contrast analysis for factorial designs provides an\n    alternative to the traditional ANOVA approach, offering the distinct\n    advantage of testing targeted hypotheses. The foundation of this\n    package is primarily rooted in the works of Rosenthal, Rosnow, and\n    Rubin (2000, ISBN: 978-0521659802) as well as Sedlmeier and Renkewitz\n    (2018, ISBN: 978-3868943214).",
    "version": "0.3.3",
    "maintainer": "Johannes Titz <johannes.titz@gmail.com>",
    "author": "Johannes Titz [aut, cre],\n  Markus Burkhardt [aut],\n  Mirka Henninger [ctb],\n  Simone Malejka [ctb]",
    "url": "https://github.com/johannes-titz/cofad",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cofad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cofad Contrast Analyses for Factorial Designs Contrast analysis for factorial designs provides an\n    alternative to the traditional ANOVA approach, offering the distinct\n    advantage of testing targeted hypotheses. The foundation of this\n    package is primarily rooted in the works of Rosenthal, Rosnow, and\n    Rubin (2000, ISBN: 978-0521659802) as well as Sedlmeier and Renkewitz\n    (2018, ISBN: 978-3868943214).  "
  },
  {
    "id": 10152,
    "package_name": "cogirt",
    "title": "Cognitive Testing Using Item Response Theory",
    "description": "Psychometrically analyze latent individual differences related to tasks, interventions, or maturational/aging effects in the context of experimental or longitudinal cognitive research using methods first described by Thomas et al. (2020) <doi:10.1177/0013164420919898>.",
    "version": "1.0.0",
    "maintainer": "Michael Thomas <michael.l.thomas@colostate.edu>",
    "author": "Michael Thomas [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cogirt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cogirt Cognitive Testing Using Item Response Theory Psychometrically analyze latent individual differences related to tasks, interventions, or maturational/aging effects in the context of experimental or longitudinal cognitive research using methods first described by Thomas et al. (2020) <doi:10.1177/0013164420919898>.  "
  },
  {
    "id": 10179,
    "package_name": "colocPropTest",
    "title": "Proportional Testing for Colocalisation Analysis",
    "description": "Colocalisation analysis tests whether two traits share a causal\n  genetic variant in a specified genomic region. Proportional testing for\n  colocalisation has been previously proposed\n  [Wallace (2013) <doi:10.1002/gepi.21765>], but is reimplemented here to\n  overcome barriers to its adoption. Its use is complementary to the fine-\n  mapping based colocalisation method in the 'coloc' package, and may be used in\n  particular to identify false \"H3\" conclusions in 'coloc'.",
    "version": "0.9.3",
    "maintainer": "Chris Wallace <cew54@cam.ac.uk>",
    "author": "Chris Wallace [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9755-1703>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=colocPropTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "colocPropTest Proportional Testing for Colocalisation Analysis Colocalisation analysis tests whether two traits share a causal\n  genetic variant in a specified genomic region. Proportional testing for\n  colocalisation has been previously proposed\n  [Wallace (2013) <doi:10.1002/gepi.21765>], but is reimplemented here to\n  overcome barriers to its adoption. Its use is complementary to the fine-\n  mapping based colocalisation method in the 'coloc' package, and may be used in\n  particular to identify false \"H3\" conclusions in 'coloc'.  "
  },
  {
    "id": 10216,
    "package_name": "combcoint",
    "title": "A Joint Test-Statistic for the Null of Non-Cointegration",
    "description": "Implements a joint cointegration testing approach that combines Engle-Granger, Johansen maximum eigenvalue, Boswijk, and Banerjee tests into a unified test-statistic for the null of non-cointegration. Also see Bayer and Hanck (2013) <doi:10.1111/j.1467-9892.2012.00814.x>.",
    "version": "0.2.0",
    "maintainer": "Janine Langerbein <janine.langerbein@icloud.com>",
    "author": "Janine Langerbein [aut, cre, cph],\n  Jens Klenke [aut] (ORCID: <https://orcid.org/0000-0001-6292-3968>)",
    "url": "https://github.com/Janine-Langerbein/combcoint",
    "bug_reports": "https://github.com/Janine-Langerbein/combcoint/issues",
    "repository": "https://cran.r-project.org/package=combcoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "combcoint A Joint Test-Statistic for the Null of Non-Cointegration Implements a joint cointegration testing approach that combines Engle-Granger, Johansen maximum eigenvalue, Boswijk, and Banerjee tests into a unified test-statistic for the null of non-cointegration. Also see Bayer and Hanck (2013) <doi:10.1111/j.1467-9892.2012.00814.x>.  "
  },
  {
    "id": 10217,
    "package_name": "combinIT",
    "title": "A Combined Interaction Test for Unreplicated Two-Way Tables",
    "description": "There are several non-functional-form-based interaction tests for testing interaction in unreplicated two-way layouts. However, no single test can detect all patterns of possible interaction and the tests are sensitive to a particular pattern of interaction. This package combines six non-functional-form-based interaction tests for testing additivity. These six tests were proposed by Boik (1993) <doi:10.1080/02664769300000004>, Piepho (1994), Kharrati-Kopaei and Sadooghi-Alvandi (2007) <doi:10.1080/03610920701386851>, Franck et al. (2013) <doi:10.1016/j.csda.2013.05.002>, Malik et al. (2016) <doi:10.1080/03610918.2013.870196> and Kharrati-Kopaei and Miller (2016) <doi:10.1080/00949655.2015.1057821>. The p-values of these six tests are combined by Bonferroni, Sidak, Jacobi polynomial expansion, and the Gaussian copula methods to provide researchers with a testing approach which leverages many existing methods to detect disparate forms of non-additivity. This package is based on the following published paper: Shenavari and Kharrati-Kopaei (2018) \"A Method for Testing Additivity in Unreplicated Two-Way Layouts Based on Combining Multiple Interaction Tests\". In addition, several sentences in help files or descriptions were copied from that paper.",
    "version": "2.0.1",
    "maintainer": "Hossein Haghbin <haghbin@pgu.ac.ir>",
    "author": "Zahra Shenavari [aut],\n  Hossein Haghbin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8416-2354>),\n  Mahmood Kharrati-Kopaei [aut] (ORCID:\n    <https://orcid.org/0000-0001-5555-253X>),\n  Seyed Morteza Najibi [aut]",
    "url": "https://github.com/haghbinh/combinIT",
    "bug_reports": "https://github.com/haghbinh/combinIT/issues",
    "repository": "https://cran.r-project.org/package=combinIT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "combinIT A Combined Interaction Test for Unreplicated Two-Way Tables There are several non-functional-form-based interaction tests for testing interaction in unreplicated two-way layouts. However, no single test can detect all patterns of possible interaction and the tests are sensitive to a particular pattern of interaction. This package combines six non-functional-form-based interaction tests for testing additivity. These six tests were proposed by Boik (1993) <doi:10.1080/02664769300000004>, Piepho (1994), Kharrati-Kopaei and Sadooghi-Alvandi (2007) <doi:10.1080/03610920701386851>, Franck et al. (2013) <doi:10.1016/j.csda.2013.05.002>, Malik et al. (2016) <doi:10.1080/03610918.2013.870196> and Kharrati-Kopaei and Miller (2016) <doi:10.1080/00949655.2015.1057821>. The p-values of these six tests are combined by Bonferroni, Sidak, Jacobi polynomial expansion, and the Gaussian copula methods to provide researchers with a testing approach which leverages many existing methods to detect disparate forms of non-additivity. This package is based on the following published paper: Shenavari and Kharrati-Kopaei (2018) \"A Method for Testing Additivity in Unreplicated Two-Way Layouts Based on Combining Multiple Interaction Tests\". In addition, several sentences in help files or descriptions were copied from that paper.  "
  },
  {
    "id": 10226,
    "package_name": "comets",
    "title": "Covariance Measure Tests for Conditional Independence",
    "description": "Covariance measure tests for conditional independence testing\n    against conditional covariance and nonlinear conditional mean alternatives.\n    The package implements versions of the generalised covariance measure test\n    (Shah and Peters, 2020, <doi:10.1214/19-aos1857>) and projected covariance\n    measure test (Lundborg et al., 2023, <doi:10.1214/24-AOS2447>). The\n    tram-GCM test, for censored responses, is implemented including the Cox\n    model and survival forests (Kook et al., 2024,\n    <doi:10.1080/01621459.2024.2395588>). Application examples to variable\n    significance testing and modality selection can be found in Kook and\n    Lundborg (2024, <doi:10.1093/bib/bbae475>).",
    "version": "0.2-2",
    "maintainer": "Lucas Kook <lucasheinrich.kook@gmail.com>",
    "author": "Lucas Kook [aut, cre] (ORCID: <https://orcid.org/0000-0002-7546-7356>),\n  Anton Rask Lundborg [ctb]",
    "url": "https://github.com/LucasKook/comets",
    "bug_reports": "https://github.com/LucasKook/comets/issues",
    "repository": "https://cran.r-project.org/package=comets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "comets Covariance Measure Tests for Conditional Independence Covariance measure tests for conditional independence testing\n    against conditional covariance and nonlinear conditional mean alternatives.\n    The package implements versions of the generalised covariance measure test\n    (Shah and Peters, 2020, <doi:10.1214/19-aos1857>) and projected covariance\n    measure test (Lundborg et al., 2023, <doi:10.1214/24-AOS2447>). The\n    tram-GCM test, for censored responses, is implemented including the Cox\n    model and survival forests (Kook et al., 2024,\n    <doi:10.1080/01621459.2024.2395588>). Application examples to variable\n    significance testing and modality selection can be found in Kook and\n    Lundborg (2024, <doi:10.1093/bib/bbae475>).  "
  },
  {
    "id": 10296,
    "package_name": "confSAM",
    "title": "Estimates and Bounds for the False Discovery Proportion, by\nPermutation",
    "description": "For multiple testing.\n    Computes estimates and confidence bounds for the\n    False Discovery Proportion (FDP), the fraction of false positives among\n    all rejected hypotheses.\n    The methods in the package use permutations of the data. Doing so, they\n    take into account the dependence structure in the data.",
    "version": "0.2",
    "maintainer": "Jesse Hemerik <j.b.a.hemerik@lumc.nl>",
    "author": "Jesse Hemerik and Jelle Goeman",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=confSAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "confSAM Estimates and Bounds for the False Discovery Proportion, by\nPermutation For multiple testing.\n    Computes estimates and confidence bounds for the\n    False Discovery Proportion (FDP), the fraction of false positives among\n    all rejected hypotheses.\n    The methods in the package use permutations of the data. Doing so, they\n    take into account the dependence structure in the data.  "
  },
  {
    "id": 10317,
    "package_name": "conjurer",
    "title": "A Parametric Method for Generating Synthetic Data",
    "description": "Generates synthetic data distributions to enable testing various modelling techniques in ways that real data does not allow. Noise can be added in a controlled manner such that the data seems real. This methodology is generic and therefore benefits both the academic and industrial research.",
    "version": "1.7.1",
    "maintainer": "Sidharth Macherla <msidharthrasik@gmail.com>",
    "author": "Sidharth Macherla [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4825-2026>)",
    "url": "https://www.foyi.co.nz/posts/documentation/documentationconjurer/",
    "bug_reports": "https://github.com/SidharthMacherla/conjurer/issues",
    "repository": "https://cran.r-project.org/package=conjurer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "conjurer A Parametric Method for Generating Synthetic Data Generates synthetic data distributions to enable testing various modelling techniques in ways that real data does not allow. Noise can be added in a controlled manner such that the data seems real. This methodology is generic and therefore benefits both the academic and industrial research.  "
  },
  {
    "id": 10392,
    "package_name": "corTESTsrd",
    "title": "Significance Testing of Rank Cross-Correlations under SRD",
    "description": "Significance test of Spearman's Rho or Kendall's Tau \n             between short-range dependent random variables.",
    "version": "1.0-0",
    "maintainer": "David Lun <lun@hydro.tuwien.ac.at>",
    "author": "David Lun [aut, cre],\n  Svenja Fischer [aut],\n  Alberto Viglione [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corTESTsrd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corTESTsrd Significance Testing of Rank Cross-Correlations under SRD Significance test of Spearman's Rho or Kendall's Tau \n             between short-range dependent random variables.  "
  },
  {
    "id": 10418,
    "package_name": "corrMCT",
    "title": "Correlated Weighted Hochberg",
    "description": "Perform additional multiple testing procedure methods to p.adjust(), \n  such as weighted Hochberg (Tamhane, A. C., & Liu, L., 2008) <doi:10.1093/biomet/asn018>, \n  ICC adjusted Bonferroni method (Shi, Q., Pavey, E. S., & Carter, R. E., 2012)\n  <doi:10.1002/pst.1514> and a new correlation corrected weighted Hochberg for correlated \n  endpoints.",
    "version": "0.2.0",
    "maintainer": "Xin-Wei Huang <xinweihuangstat@gmail.com>",
    "author": "Xin-Wei Huang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4238-3081>),\n  Jia Hua [ctb],\n  Bhramori Banerjee [ctb],\n  Xuelong Wang [ctb],\n  Qing Li [ctb],\n  Merck & Co., Inc [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corrMCT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corrMCT Correlated Weighted Hochberg Perform additional multiple testing procedure methods to p.adjust(), \n  such as weighted Hochberg (Tamhane, A. C., & Liu, L., 2008) <doi:10.1093/biomet/asn018>, \n  ICC adjusted Bonferroni method (Shi, Q., Pavey, E. S., & Carter, R. E., 2012)\n  <doi:10.1002/pst.1514> and a new correlation corrected weighted Hochberg for correlated \n  endpoints.  "
  },
  {
    "id": 10427,
    "package_name": "corrfuns",
    "title": "Correlation Coefficient Related Functions",
    "description": "Many correlation coefficient related functions are offered, such as correlations, partial correlations and hypothesis testing using asymptotic tests and computer intensive methods (bootstrap and permutation). References include Mardia K.V., Kent J.T. and Bibby J.M. (1979). \"Multivariate Analysis\". ISBN: 978-0124712522. London: Academic Press and Owen A. B. (2001). \"Empirical likelihood\". Chapman and Hall/CRC Press. ISBN: 9781584880714.",
    "version": "1.2",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corrfuns",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corrfuns Correlation Coefficient Related Functions Many correlation coefficient related functions are offered, such as correlations, partial correlations and hypothesis testing using asymptotic tests and computer intensive methods (bootstrap and permutation). References include Mardia K.V., Kent J.T. and Bibby J.M. (1979). \"Multivariate Analysis\". ISBN: 978-0124712522. London: Academic Press and Owen A. B. (2001). \"Empirical likelihood\". Chapman and Hall/CRC Press. ISBN: 9781584880714.  "
  },
  {
    "id": 10452,
    "package_name": "countdata",
    "title": "The Beta-Binomial Test for Count Data",
    "description": "The beta-binomial test is used for significance analysis of independent samples by Pham et al. (2010) <doi:10.1093/bioinformatics/btp677>. The inverted beta-binomial test is used for paired sample testing, e.g. pre-treatment and post-treatment data, by Pham and Jimenez (2012) <doi:10.1093/bioinformatics/bts394>. ",
    "version": "1.3",
    "maintainer": "Thang Pham <t.pham@amsterdamumc.nl>",
    "author": "Thang Pham [aut, cre, cph, ctb] (ORCID:\n    <https://orcid.org/0000-0003-0333-2492>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=countdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "countdata The Beta-Binomial Test for Count Data The beta-binomial test is used for significance analysis of independent samples by Pham et al. (2010) <doi:10.1093/bioinformatics/btp677>. The inverted beta-binomial test is used for paired sample testing, e.g. pre-treatment and post-treatment data, by Pham and Jimenez (2012) <doi:10.1093/bioinformatics/bts394>.   "
  },
  {
    "id": 10459,
    "package_name": "countsplit",
    "title": "Splitting a Count Matrix into Independent Folds",
    "description": "Implements the count splitting methodology from Neufeld et al. (2022) <doi:10.1093/biostatistics/kxac047> and Neufeld et al. (2023) <arXiv:2307.12985>. Intended for turning a matrix of single-cell RNA sequencing counts, or similar count datasets, into independent folds that can be used for training/testing or cross validation. Assumes that the entries in the matrix are from a Poisson or a negative binomial distribution.",
    "version": "4.0.0",
    "maintainer": "Anna Neufeld <aneufeld@fredhutch.org>",
    "author": "Anna Neufeld [aut, cre, cph],\n  Mischko Heming [ctb],\n  Joshua Popp [ctb]",
    "url": "https://github.com/anna-neufeld/countsplit",
    "bug_reports": "https://github.com/anna-neufeld/countsplit/issues",
    "repository": "https://cran.r-project.org/package=countsplit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "countsplit Splitting a Count Matrix into Independent Folds Implements the count splitting methodology from Neufeld et al. (2022) <doi:10.1093/biostatistics/kxac047> and Neufeld et al. (2023) <arXiv:2307.12985>. Intended for turning a matrix of single-cell RNA sequencing counts, or similar count datasets, into independent folds that can be used for training/testing or cross validation. Assumes that the entries in the matrix are from a Poisson or a negative binomial distribution.  "
  },
  {
    "id": 10460,
    "package_name": "countts",
    "title": "Thomson Sampling for Zero-Inflated Count Outcomes",
    "description": "A specialized tool is designed for assessing contextual bandit algorithms, particularly those aimed at handling overdispersed and zero-inflated count data. It offers a simulated testing environment that includes various models like Poisson, Overdispersed Poisson, Zero-inflated Poisson, and Zero-inflated Overdispersed Poisson. The package is capable of executing five specific algorithms: Linear Thompson sampling with log transformation on the outcome, Thompson sampling Poisson, Thompson sampling Negative Binomial, Thompson sampling Zero-inflated Poisson, and Thompson sampling Zero-inflated Negative Binomial. Additionally, it can generate regret plots to evaluate the performance of contextual bandit algorithms. This package is based on the algorithms by Liu et al. (2023) <arXiv:2311.14359>.",
    "version": "0.1.0",
    "maintainer": "Tanujit Chakraborty <tanujitisi@gmail.com>",
    "author": "Xueqing Liu [aut],\n  Nina Deliu [aut],\n  Tanujit Chakraborty [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-3479-2187>),\n  Lauren Bell [aut],\n  Bibhas Chakraborty [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=countts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "countts Thomson Sampling for Zero-Inflated Count Outcomes A specialized tool is designed for assessing contextual bandit algorithms, particularly those aimed at handling overdispersed and zero-inflated count data. It offers a simulated testing environment that includes various models like Poisson, Overdispersed Poisson, Zero-inflated Poisson, and Zero-inflated Overdispersed Poisson. The package is capable of executing five specific algorithms: Linear Thompson sampling with log transformation on the outcome, Thompson sampling Poisson, Thompson sampling Negative Binomial, Thompson sampling Zero-inflated Poisson, and Thompson sampling Zero-inflated Negative Binomial. Additionally, it can generate regret plots to evaluate the performance of contextual bandit algorithms. This package is based on the algorithms by Liu et al. (2023) <arXiv:2311.14359>.  "
  },
  {
    "id": 10465,
    "package_name": "covTestR",
    "title": "Covariance Matrix Tests",
    "description": "Testing functions for Covariance Matrices. These tests include high-dimension homogeneity of covariance\n  matrix testing described by Schott (2007) <doi:10.1016/j.csda.2007.03.004> and high-dimensional one-sample tests of \n  covariance matrix structure described by Fisher, et al. (2010) <doi:10.1016/j.jmva.2010.07.004>. Covariance matrix\n  tests use C++ to speed performance and allow larger data sets.",
    "version": "0.1.4",
    "maintainer": "Ben Barnard <ben_barnard@outlook.com>",
    "author": "Ben Barnard [aut, cre],\n  Dean Young [aut]",
    "url": "https://covtestr.bearstatistics.com",
    "bug_reports": "https://github.com/BenBarnard/covTestR/issues",
    "repository": "https://cran.r-project.org/package=covTestR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covTestR Covariance Matrix Tests Testing functions for Covariance Matrices. These tests include high-dimension homogeneity of covariance\n  matrix testing described by Schott (2007) <doi:10.1016/j.csda.2007.03.004> and high-dimensional one-sample tests of \n  covariance matrix structure described by Fisher, et al. (2010) <doi:10.1016/j.jmva.2010.07.004>. Covariance matrix\n  tests use C++ to speed performance and allow larger data sets.  "
  },
  {
    "id": 10469,
    "package_name": "covatest",
    "title": "Tests on Properties of Space-Time Covariance Functions",
    "description": "Tests on properties of space-time covariance functions.\n    Tests on symmetry, separability and for assessing \n    different forms of non-separability are available. Moreover tests on \n    some classes of covariance functions, such that the classes of \n    product-sum models, Gneiting models and integrated product models have \n    been provided.  It is the companion R package to the papers of  \n    Cappello, C., De Iaco, S., Posa, D., 2018, Testing the type of non-separability \n    and some classes of space-time covariance function models <doi:10.1007/s00477-017-1472-2>\n    and Cappello, C., De Iaco, S., Posa, D., 2020, covatest: an R package for\n    selecting a class of space-time covariance functions <doi:10.18637/jss.v094.i01>.",
    "version": "1.2.4",
    "maintainer": "Sandra De Iaco <sandra.deiaco@unisalento.it>",
    "author": "Sandra De Iaco [aut, cre],\n  Claudia Cappello [aut],\n  Donato Posa [aut],\n  Sabrina Maggio [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=covatest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covatest Tests on Properties of Space-Time Covariance Functions Tests on properties of space-time covariance functions.\n    Tests on symmetry, separability and for assessing \n    different forms of non-separability are available. Moreover tests on \n    some classes of covariance functions, such that the classes of \n    product-sum models, Gneiting models and integrated product models have \n    been provided.  It is the companion R package to the papers of  \n    Cappello, C., De Iaco, S., Posa, D., 2018, Testing the type of non-separability \n    and some classes of space-time covariance function models <doi:10.1007/s00477-017-1472-2>\n    and Cappello, C., De Iaco, S., Posa, D., 2020, covatest: an R package for\n    selecting a class of space-time covariance functions <doi:10.18637/jss.v094.i01>.  "
  },
  {
    "id": 10473,
    "package_name": "covercorr",
    "title": "Coverage Correlation Coefficient and Testing for Independence",
    "description": "Computes the coverage correlation coefficient introduced in <doi:10.48550/arXiv.2508.06402> , a statistical measure that quantifies dependence between two random vectors by computing the union volume of data-centered hypercubes in a uniform space.",
    "version": "1.0.0",
    "maintainer": "Tengyao Wang <t.wang59@lse.ac.uk>",
    "author": "Tengyao Wang [aut, cre],\n  Mona Azadkia [aut, ctb],\n  Xuzhi Yang [aut, ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=covercorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covercorr Coverage Correlation Coefficient and Testing for Independence Computes the coverage correlation coefficient introduced in <doi:10.48550/arXiv.2508.06402> , a statistical measure that quantifies dependence between two random vectors by computing the union volume of data-centered hypercubes in a uniform space.  "
  },
  {
    "id": 10480,
    "package_name": "covid19india",
    "title": "Pulling Clean Data from Covid19india.org",
    "description": "Pull raw and pre-cleaned versions of national and state-level \n    COVID-19 time-series data from covid19india.org <https://www.covid19india.org>. \n    Easily obtain and merge case count data, testing data, and vaccine data. \n    Also assists in calculating the time-varying effective reproduction number \n    with sensible parameters for COVID-19.",
    "version": "0.1.4",
    "maintainer": "Max Salvatore <mmsalva@umich.edu>",
    "author": "Max Salvatore [aut, cre],\n  Michael Kleinsasser [aut]",
    "url": "https://github.com/maxsal/covid19india",
    "bug_reports": "https://github.com/maxsal/covid19india/issues",
    "repository": "https://cran.r-project.org/package=covid19india",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covid19india Pulling Clean Data from Covid19india.org Pull raw and pre-cleaned versions of national and state-level \n    COVID-19 time-series data from covid19india.org <https://www.covid19india.org>. \n    Easily obtain and merge case count data, testing data, and vaccine data. \n    Also assists in calculating the time-varying effective reproduction number \n    with sensible parameters for COVID-19.  "
  },
  {
    "id": 10491,
    "package_name": "covsep",
    "title": "Tests for Determining if the Covariance Structure of\n2-Dimensional Data is Separable",
    "description": "Functions for testing if the covariance structure of 2-dimensional data\n    (e.g. samples of surfaces X_i = X_i(s,t)) is separable, i.e. if covariance(X) =\n    C_1 x C_2.\n    A complete descriptions of the implemented tests can be found in the paper\n    Aston, John A. D.; Pigoli, Davide; Tavakoli, Shahin. Tests for separability in\n    nonparametric covariance operators of random surfaces. Ann. Statist. 45 (2017),\n    no. 4, 1431--1461. <doi:10.1214/16-AOS1495> <https://projecteuclid.org/euclid.aos/1498636862> <arXiv:1505.02023>.",
    "version": "1.1.0",
    "maintainer": "Shahin Tavakoli <s.tavakoli@warwick.ac.uk>",
    "author": "Shahin Tavakoli [aut, cre],\n  Davide Pigoli [ctb],\n  John Aston [ctb]",
    "url": "http://arxiv.org/abs/1505.02023",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=covsep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covsep Tests for Determining if the Covariance Structure of\n2-Dimensional Data is Separable Functions for testing if the covariance structure of 2-dimensional data\n    (e.g. samples of surfaces X_i = X_i(s,t)) is separable, i.e. if covariance(X) =\n    C_1 x C_2.\n    A complete descriptions of the implemented tests can be found in the paper\n    Aston, John A. D.; Pigoli, Davide; Tavakoli, Shahin. Tests for separability in\n    nonparametric covariance operators of random surfaces. Ann. Statist. 45 (2017),\n    no. 4, 1431--1461. <doi:10.1214/16-AOS1495> <https://projecteuclid.org/euclid.aos/1498636862> <arXiv:1505.02023>.  "
  },
  {
    "id": 10493,
    "package_name": "covtracer",
    "title": "Contextualizing Tests",
    "description": "\n    Dissects a package environment or 'covr' coverage object in order to cross\n    reference tested code with the lines that are evaluated, as well as linking\n    those evaluated lines to the documentation that they are described within.\n    Connecting these three pieces of information provides a mechanism of \n\tlinking tests to documented behaviors.",
    "version": "0.0.1",
    "maintainer": "Doug Kelkhoff <doug.kelkhoff@gmail.com>",
    "author": "Doug Kelkhoff [cre, aut] (ORCID:\n    <https://orcid.org/0009-0003-7845-4061>),\n  Szymon Maksymiuk [aut] (ORCID: <https://orcid.org/0000-0002-3120-1601>),\n  Andrew McNeil [aut],\n  F. Hoffmann-La Roche AG [cph, fnd]",
    "url": "https://github.com/genentech/covtracer",
    "bug_reports": "https://github.com/genentech/covtracer/issues",
    "repository": "https://cran.r-project.org/package=covtracer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "covtracer Contextualizing Tests \n    Dissects a package environment or 'covr' coverage object in order to cross\n    reference tested code with the lines that are evaluated, as well as linking\n    those evaluated lines to the documentation that they are described within.\n    Connecting these three pieces of information provides a mechanism of \n\tlinking tests to documented behaviors.  "
  },
  {
    "id": 10620,
    "package_name": "cryptoQuotes",
    "title": "Open Access to Cryptocurrency Market Data, Sentiment Indicators\nand Interactive Charts",
    "description": "\n  This high-level API client provides open access to cryptocurrency market data, sentiment indicators, and interactive charting tools. \n  The data is sourced from major cryptocurrency exchanges via 'curl' and returned in 'xts'-format. The data comes in open, high, low, and close (OHLC) format with flexible granularity, ranging from seconds to months. \n  This flexibility makes it ideal for developing and backtesting trading strategies or conducting detailed market analysis.",
    "version": "1.3.3",
    "maintainer": "Serkan Korkmaz <serkor1@duck.com>",
    "author": "Serkan Korkmaz [cre, aut, ctb, cph] (ORCID:\n    <https://orcid.org/0000-0002-5052-0982>),\n  Jonas Cuzulan Hirani [ctb] (ORCID:\n    <https://orcid.org/0000-0002-9512-1993>)",
    "url": "https://serkor1.github.io/cryptoQuotes/,\nhttps://github.com/serkor1/cryptoQuotes",
    "bug_reports": "https://github.com/serkor1/cryptoQuotes/issues",
    "repository": "https://cran.r-project.org/package=cryptoQuotes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cryptoQuotes Open Access to Cryptocurrency Market Data, Sentiment Indicators\nand Interactive Charts \n  This high-level API client provides open access to cryptocurrency market data, sentiment indicators, and interactive charting tools. \n  The data is sourced from major cryptocurrency exchanges via 'curl' and returned in 'xts'-format. The data comes in open, high, low, and close (OHLC) format with flexible granularity, ranging from seconds to months. \n  This flexibility makes it ideal for developing and backtesting trading strategies or conducting detailed market analysis.  "
  },
  {
    "id": 10634,
    "package_name": "csmGmm",
    "title": "Conditionally Symmetric Multidimensional Gaussian Mixture Model",
    "description": "Implements the conditionally symmetric multidimensional Gaussian mixture model (csmGmm) for large-scale testing of composite null hypotheses in genetic association applications such as mediation analysis, pleiotropy analysis, and replication analysis. In such analyses, we typically have J sets of K test statistics where K is a small number (e.g. 2 or 3) and J is large (e.g. 1 million). For each one of the J sets, we want to know if we can reject all K individual nulls. Please see the vignette for a quickstart guide. The paper describing these methods is \"Testing a Large Number of Composite Null Hypotheses Using Conditionally Symmetric Multidimensional Gaussian Mixtures in Genome-Wide Studies\" by Sun R, McCaw Z, & Lin X (Journal of the American Statistical Association 2025, <doi:10.1080/01621459.2024.2422124>).",
    "version": "0.4.0",
    "maintainer": "Ryan Sun <ryansun.work@gmail.com>",
    "author": "Ryan Sun [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=csmGmm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csmGmm Conditionally Symmetric Multidimensional Gaussian Mixture Model Implements the conditionally symmetric multidimensional Gaussian mixture model (csmGmm) for large-scale testing of composite null hypotheses in genetic association applications such as mediation analysis, pleiotropy analysis, and replication analysis. In such analyses, we typically have J sets of K test statistics where K is a small number (e.g. 2 or 3) and J is large (e.g. 1 million). For each one of the J sets, we want to know if we can reject all K individual nulls. Please see the vignette for a quickstart guide. The paper describing these methods is \"Testing a Large Number of Composite Null Hypotheses Using Conditionally Symmetric Multidimensional Gaussian Mixtures in Genome-Wide Studies\" by Sun R, McCaw Z, & Lin X (Journal of the American Statistical Association 2025, <doi:10.1080/01621459.2024.2422124>).  "
  },
  {
    "id": 10657,
    "package_name": "ctgt",
    "title": "Closed Testing with Globaltest for Pathway Analysis",
    "description": "A shortcut procedure is proposed to implement closed testing for large-scale multiple testings, especially with the global test. This shortcut is asymptotically equivalent to closed testing and post hoc. Users could detect any possible sets of features or pathways with family-wise error rate controlled. The global test is powerful to detect associations between a group of features and an outcome of interest. ",
    "version": "2.0.1",
    "maintainer": "Ningning Xu <ningningxu312@gmail.com>",
    "author": "Ningning Xu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ctgt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ctgt Closed Testing with Globaltest for Pathway Analysis A shortcut procedure is proposed to implement closed testing for large-scale multiple testings, especially with the global test. This shortcut is asymptotically equivalent to closed testing and post hoc. Users could detect any possible sets of features or pathways with family-wise error rate controlled. The global test is powerful to detect associations between a group of features and an outcome of interest.   "
  },
  {
    "id": 10687,
    "package_name": "cucumber",
    "title": "Behavior-Driven Development for R",
    "description": "Write executable specifications in a natural language that describes how your code should behave.\n    Write specifications in feature files using 'Gherkin' language and execute them using functions implemented in R.\n    Use them as an extension to your 'testthat' tests to provide a high level description of how your code works.",
    "version": "2.1.1",
    "maintainer": "Jakub Sobolewski <jakupsob@gmail.com>",
    "author": "Jakub Sobolewski [aut, cre]",
    "url": "https://github.com/jakubsob/cucumber,\nhttps://jakubsobolewski.com/cucumber/",
    "bug_reports": "https://github.com/jakubsob/cucumber/issues",
    "repository": "https://cran.r-project.org/package=cucumber",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cucumber Behavior-Driven Development for R Write executable specifications in a natural language that describes how your code should behave.\n    Write specifications in feature files using 'Gherkin' language and execute them using functions implemented in R.\n    Use them as an extension to your 'testthat' tests to provide a high level description of how your code works.  "
  },
  {
    "id": 10728,
    "package_name": "cvequality",
    "title": "Tests for the Equality of Coefficients of Variation from\nMultiple Groups",
    "description": "Contains functions for testing for significant differences between multiple coefficients of variation. Includes Feltz and Miller's (1996) <DOI:10.1002/(SICI)1097-0258(19960330)15:6%3C647::AID-SIM184%3E3.0.CO;2-P> asymptotic test and Krishnamoorthy and Lee's (2014) <DOI:10.1007/s00180-013-0445-2> modified signed-likelihood ratio test. See the vignette for more, including full details of citations.",
    "version": "0.2.0",
    "maintainer": "Ben Marwick <benmarwick@gmail.com>",
    "author": "Ben Marwick [aut, cre],\n  Kalimuthu Krishnamoorthy [aut]",
    "url": "https://github.com/benmarwick/cvequality",
    "bug_reports": "https://github.com/benmarwick/cvequality/issues",
    "repository": "https://cran.r-project.org/package=cvequality",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvequality Tests for the Equality of Coefficients of Variation from\nMultiple Groups Contains functions for testing for significant differences between multiple coefficients of variation. Includes Feltz and Miller's (1996) <DOI:10.1002/(SICI)1097-0258(19960330)15:6%3C647::AID-SIM184%3E3.0.CO;2-P> asymptotic test and Krishnamoorthy and Lee's (2014) <DOI:10.1007/s00180-013-0445-2> modified signed-likelihood ratio test. See the vignette for more, including full details of citations.  "
  },
  {
    "id": 10730,
    "package_name": "cvmdisc",
    "title": "Cramer von Mises Tests for Discrete or Grouped Distributions",
    "description": "Implements Cramer-von Mises Statistics for testing fit to (1) fully specified discrete distributions as described in Choulakian, Lockhart and Stephens (1994) <doi:10.2307/3315828> (2) discrete distributions with unknown parameters that must be estimated from the sample data, see Spinelli & Stephens (1997) <doi:10.2307/3315735> and Lockhart, Spinelli and Stephens (2007) <doi:10.1002/cjs.5550350111> (3) grouped continuous distributions with Unknown Parameters, see Spinelli (2001) <doi:10.2307/3316040>. Maximum likelihood estimation (MLE) is used to estimate the parameters. The package computes the Cramer-von Mises Statistics, Anderson-Darling Statistics and the Watson-Stephens Statistics and their p-values.",
    "version": "0.1.0",
    "maintainer": "Shaun Zheng Sun <Shaun.Sun@ufv.ca>",
    "author": "Shaun Zheng Sun [aut, cre],\n  Dillon Duncan [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cvmdisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cvmdisc Cramer von Mises Tests for Discrete or Grouped Distributions Implements Cramer-von Mises Statistics for testing fit to (1) fully specified discrete distributions as described in Choulakian, Lockhart and Stephens (1994) <doi:10.2307/3315828> (2) discrete distributions with unknown parameters that must be estimated from the sample data, see Spinelli & Stephens (1997) <doi:10.2307/3315735> and Lockhart, Spinelli and Stephens (2007) <doi:10.1002/cjs.5550350111> (3) grouped continuous distributions with Unknown Parameters, see Spinelli (2001) <doi:10.2307/3316040>. Maximum likelihood estimation (MLE) is used to estimate the parameters. The package computes the Cramer-von Mises Statistics, Anderson-Darling Statistics and the Watson-Stephens Statistics and their p-values.  "
  },
  {
    "id": 10760,
    "package_name": "dCovTS",
    "title": "Distance Covariance and Correlation for Time Series Analysis",
    "description": "Computing and plotting the distance covariance and correlation function of a univariate or a multivariate time series. Both versions of biased and unbiased estimators of distance covariance and correlation are provided. Test statistics for testing pairwise independence are also implemented. Some data sets are also included. References include: \n\t\t\t       a) Edelmann Dominic, Fokianos Konstantinos and Pitsillou Maria (2019). 'An Updated Literature Review of Distance Correlation and Its Applications to Time Series'. International Statistical Review, 87(2): 237--262. <doi:10.1111/insr.12294>.\n             b) Fokianos Konstantinos and Pitsillou Maria (2018). 'Testing independence for multivariate time series via the auto-distance correlation matrix'. Biometrika, 105(2): 337--352. <doi:10.1093/biomet/asx082>.\n             c) Fokianos Konstantinos and Pitsillou Maria (2017). 'Consistent testing for pairwise dependence in time series'. Technometrics, 59(2): 262--270. <doi:10.1080/00401706.2016.1156024>.\n             d) Pitsillou Maria and Fokianos Konstantinos (2016). 'dCovTS: Distance Covariance/Correlation for Time Series'. R Journal, 8(2):324-340. <doi:10.32614/RJ-2016-049>.",
    "version": "1.4",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Maria Pitsillou [aut, cph],\n  Konstantinos Fokianos [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dCovTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dCovTS Distance Covariance and Correlation for Time Series Analysis Computing and plotting the distance covariance and correlation function of a univariate or a multivariate time series. Both versions of biased and unbiased estimators of distance covariance and correlation are provided. Test statistics for testing pairwise independence are also implemented. Some data sets are also included. References include: \n\t\t\t       a) Edelmann Dominic, Fokianos Konstantinos and Pitsillou Maria (2019). 'An Updated Literature Review of Distance Correlation and Its Applications to Time Series'. International Statistical Review, 87(2): 237--262. <doi:10.1111/insr.12294>.\n             b) Fokianos Konstantinos and Pitsillou Maria (2018). 'Testing independence for multivariate time series via the auto-distance correlation matrix'. Biometrika, 105(2): 337--352. <doi:10.1093/biomet/asx082>.\n             c) Fokianos Konstantinos and Pitsillou Maria (2017). 'Consistent testing for pairwise dependence in time series'. Technometrics, 59(2): 262--270. <doi:10.1080/00401706.2016.1156024>.\n             d) Pitsillou Maria and Fokianos Konstantinos (2016). 'dCovTS: Distance Covariance/Correlation for Time Series'. R Journal, 8(2):324-340. <doi:10.32614/RJ-2016-049>.  "
  },
  {
    "id": 10762,
    "package_name": "dHSIC",
    "title": "Independence Testing via Hilbert Schmidt Independence Criterion",
    "description": "Contains an implementation of the\n\t     d-variable Hilbert Schmidt independence criterion\n\t     and several hypothesis tests based on it, as described\n\t     in Pfister et al. (2017) <doi:10.1111/rssb.12235>.",
    "version": "2.1",
    "maintainer": "Niklas Pfister <pfister@stat.math.ethz.ch>",
    "author": "Niklas Pfister and Jonas Peters",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dHSIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dHSIC Independence Testing via Hilbert Schmidt Independence Criterion Contains an implementation of the\n\t     d-variable Hilbert Schmidt independence criterion\n\t     and several hypothesis tests based on it, as described\n\t     in Pfister et al. (2017) <doi:10.1111/rssb.12235>.  "
  },
  {
    "id": 10769,
    "package_name": "dSTEM",
    "title": "Multiple Testing of Local Extrema for Detection of Change Points",
    "description": "Simultaneously detect the number and locations of change points in piecewise linear models under stationary Gaussian noise allowing autocorrelated random noise. The core idea is to transform the problem of detecting change points into the detection of local extrema (local maxima and local minima)through kernel smoothing and differentiation of the data sequence, see Cheng et al. (2020) <doi:10.1214/20-EJS1751>. A low-computational and fast algorithm call 'dSTEM' is introduced to detect change points based on the 'STEM' algorithm in D. Cheng and A. Schwartzman (2017) <doi:10.1214/16-AOS1458>.",
    "version": "2.0-1",
    "maintainer": "Zhibing He <zhibingh@asu.edu>",
    "author": "Zhibing He <zhibingh@asu.edu>",
    "url": "https://doi.org/10.1214/20-EJS1751,\nhttps://doi.org/10.1214/16-AOS1458",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dSTEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dSTEM Multiple Testing of Local Extrema for Detection of Change Points Simultaneously detect the number and locations of change points in piecewise linear models under stationary Gaussian noise allowing autocorrelated random noise. The core idea is to transform the problem of detecting change points into the detection of local extrema (local maxima and local minima)through kernel smoothing and differentiation of the data sequence, see Cheng et al. (2020) <doi:10.1214/20-EJS1751>. A low-computational and fast algorithm call 'dSTEM' is introduced to detect change points based on the 'STEM' algorithm in D. Cheng and A. Schwartzman (2017) <doi:10.1214/16-AOS1458>.  "
  },
  {
    "id": 10773,
    "package_name": "dabestr",
    "title": "Data Analysis using Bootstrap-Coupled Estimation",
    "description": "Data Analysis using Bootstrap-Coupled ESTimation.  Estimation\n    statistics is a simple framework that avoids the pitfalls of\n    significance testing. It uses familiar statistical concepts: means,\n    mean differences, and error bars. More importantly, it focuses on the\n    effect size of one's experiment/intervention, as opposed to a false\n    dichotomy engendered by P values.  An estimation plot has two key\n    features: 1. It presents all datapoints as a swarmplot, which orders\n    each point to display the underlying distribution.  2. It presents the\n    effect size as a bootstrap 95% confidence interval on a separate but\n    aligned axes.  Estimation plots are introduced in Ho et al., Nature\n    Methods 2019, 1548-7105.  <doi:10.1038/s41592-019-0470-3>.  The\n    free-to-view PDF is located at\n    <https://www.nature.com/articles/s41592-019-0470-3.epdf?author_access_token=Euy6APITxsYA3huBKOFBvNRgN0jAjWel9jnR3ZoTv0Pr6zJiJ3AA5aH4989gOJS_dajtNr1Wt17D0fh-t4GFcvqwMYN03qb8C33na_UrCUcGrt-Z0J9aPL6TPSbOxIC-pbHWKUDo2XsUOr3hQmlRew%3D%3D>.",
    "version": "2025.3.15",
    "maintainer": "Yishan Mai <maiyishan@u.duke.nus.edu>",
    "author": "Joses W. Ho [aut] (ORCID: <https://orcid.org/0000-0002-9186-6322>),\n  Kah Seng Lian [aut],\n  Ana Rosa Castillo [aut],\n  Zhuoyu Wang [aut],\n  Jun Yang Liao [aut],\n  Felicia Low [aut],\n  Tayfun Tumkaya [aut] (ORCID: <https://orcid.org/0000-0001-8425-3360>),\n  Jonathan Anns [ctb] (ORCID: <https://orcid.org/0009-0005-8349-4986>),\n  Yishan Mai [cre, ctb] (ORCID: <https://orcid.org/0000-0002-7199-380X>),\n  Sangyu Xu [ctb] (ORCID: <https://orcid.org/0000-0002-4927-9204>),\n  Zinan Lu [ctb],\n  Hyungwon Choi [ctb] (ORCID: <https://orcid.org/0000-0002-6687-3088>),\n  Adam Claridge-Chang [ctb] (ORCID:\n    <https://orcid.org/0000-0002-4583-3650>),\n  ACCLAB [cph, fnd]",
    "url": "https://github.com/ACCLAB/dabestr,\nhttps://acclab.github.io/dabestr/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dabestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dabestr Data Analysis using Bootstrap-Coupled Estimation Data Analysis using Bootstrap-Coupled ESTimation.  Estimation\n    statistics is a simple framework that avoids the pitfalls of\n    significance testing. It uses familiar statistical concepts: means,\n    mean differences, and error bars. More importantly, it focuses on the\n    effect size of one's experiment/intervention, as opposed to a false\n    dichotomy engendered by P values.  An estimation plot has two key\n    features: 1. It presents all datapoints as a swarmplot, which orders\n    each point to display the underlying distribution.  2. It presents the\n    effect size as a bootstrap 95% confidence interval on a separate but\n    aligned axes.  Estimation plots are introduced in Ho et al., Nature\n    Methods 2019, 1548-7105.  <doi:10.1038/s41592-019-0470-3>.  The\n    free-to-view PDF is located at\n    <https://www.nature.com/articles/s41592-019-0470-3.epdf?author_access_token=Euy6APITxsYA3huBKOFBvNRgN0jAjWel9jnR3ZoTv0Pr6zJiJ3AA5aH4989gOJS_dajtNr1Wt17D0fh-t4GFcvqwMYN03qb8C33na_UrCUcGrt-Z0J9aPL6TPSbOxIC-pbHWKUDo2XsUOr3hQmlRew%3D%3D>.  "
  },
  {
    "id": 10855,
    "package_name": "datastat",
    "title": "Dataset for Statistical Analysis",
    "description": "Data are essential in statistical analysis.\n    This data package consists of four datasets for descriptive statistics, two datasets for statistical hypothesis testing, and two datasets for regression analysis.\n    All of the datasets are based on Rattanalertnusorn, A. (2024) <https://www.researchgate.net/publication/371944275_porkaermxarlaeakarprayuktchingan_R_and_its_applications>.   ",
    "version": "0.1.0",
    "maintainer": "Atchanut Rattanalertnusorn <atchanut_r@rmutt.ac.th>",
    "author": "Atchanut Rattanalertnusorn [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=datastat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datastat Dataset for Statistical Analysis Data are essential in statistical analysis.\n    This data package consists of four datasets for descriptive statistics, two datasets for statistical hypothesis testing, and two datasets for regression analysis.\n    All of the datasets are based on Rattanalertnusorn, A. (2024) <https://www.researchgate.net/publication/371944275_porkaermxarlaeakarprayuktchingan_R_and_its_applications>.     "
  },
  {
    "id": 10907,
    "package_name": "dcmdata",
    "title": "Data Sets for Diagnostic Classification Modeling",
    "description": "Access data sets for demonstrating or testing diagnostic\n    classification models. Simulated data sets can be used to compare estimated\n    model output to true data-generating values. Real data sets can be used to\n    demonstrate real-world applications of diagnostic models.",
    "version": "0.1.0",
    "maintainer": "W. Jake Thompson <wjakethompson@gmail.com>",
    "author": "W. Jake Thompson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7339-0300>),\n  University of Kansas [cph],\n  Institute of Education Sciences [fnd]",
    "url": "https://dcmdata.r-dcm.org, https://github.com/r-dcm/dcmdata",
    "bug_reports": "https://github.com/r-dcm/dcmdata/issues",
    "repository": "https://cran.r-project.org/package=dcmdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dcmdata Data Sets for Diagnostic Classification Modeling Access data sets for demonstrating or testing diagnostic\n    classification models. Simulated data sets can be used to compare estimated\n    model output to true data-generating values. Real data sets can be used to\n    demonstrate real-world applications of diagnostic models.  "
  },
  {
    "id": 10925,
    "package_name": "ddpca",
    "title": "Diagonally Dominant Principal Component Analysis",
    "description": "Efficient procedures for fitting the DD-PCA (Ke et al., 2019, <arXiv:1906.00051>)  by decomposing a large covariance matrix into a low-rank matrix plus a diagonally dominant matrix. The implementation of DD-PCA includes the convex approach using the Alternating Direction Method of Multipliers (ADMM) and the non-convex approach using the iterative projection algorithm. Applications of DD-PCA to large covariance matrix estimation and global multiple testing are also included in this package. ",
    "version": "1.1",
    "maintainer": "Fan Yang <fyang1@uchicago.edu>",
    "author": "Tracy Ke [aut],\n  Lingzhou Xue [aut],\n  Fan Yang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ddpca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ddpca Diagonally Dominant Principal Component Analysis Efficient procedures for fitting the DD-PCA (Ke et al., 2019, <arXiv:1906.00051>)  by decomposing a large covariance matrix into a low-rank matrix plus a diagonally dominant matrix. The implementation of DD-PCA includes the convex approach using the Alternating Direction Method of Multipliers (ADMM) and the non-convex approach using the iterative projection algorithm. Applications of DD-PCA to large covariance matrix estimation and global multiple testing are also included in this package.   "
  },
  {
    "id": 10928,
    "package_name": "ddst",
    "title": "Data Driven Smooth Tests",
    "description": "Smooth testing of goodness of fit. These tests are data\n    driven (alternative hypothesis is dynamically selected based on data). In this\n    package you will find various tests for exponent, Gaussian, Gumbel and uniform\n    distribution.",
    "version": "1.4",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "author": "Przemyslaw Biecek (R code), Teresa Ledwina (support,\n    descriptions)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ddst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ddst Data Driven Smooth Tests Smooth testing of goodness of fit. These tests are data\n    driven (alternative hypothesis is dynamically selected based on data). In this\n    package you will find various tests for exponent, Gaussian, Gumbel and uniform\n    distribution.  "
  },
  {
    "id": 10937,
    "package_name": "debar",
    "title": "A Post-Clustering Denoiser for COI-5P Barcode Data",
    "description": "The 'debar' sequence processing pipeline is designed for denoising high throughput \n    sequencing data for the animal DNA barcode marker cytochrome c oxidase I (COI). The package \n    is designed to detect and correct insertion and deletion errors within sequencer outputs. \n    This is accomplished through comparison of input sequences against a profile hidden Markov \n    model (PHMM) using the Viterbi algorithm (for algorithm details see Durbin et al. 1998, \n    ISBN: 9780521629713). Inserted base pairs are removed and deleted base pairs are accounted \n    for through the introduction of a placeholder character. Since the PHMM is a probabilistic \n    representation of the COI barcode, corrections are not always perfect. For this reason \n    'debar' censors base pairs adjacent to reported indel sites, turning them into placeholder \n    characters (default is 7 base pairs in either direction, this feature can be disabled).\n    Testing has shown that this censorship results in the correct sequence length being restored, \n    and erroneous base pairs being masked the vast majority of the time (>95%). ",
    "version": "0.1.1",
    "maintainer": "Cameron M. Nugent <camnugent@gmail.com>",
    "author": "Cameron M. Nugent",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=debar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "debar A Post-Clustering Denoiser for COI-5P Barcode Data The 'debar' sequence processing pipeline is designed for denoising high throughput \n    sequencing data for the animal DNA barcode marker cytochrome c oxidase I (COI). The package \n    is designed to detect and correct insertion and deletion errors within sequencer outputs. \n    This is accomplished through comparison of input sequences against a profile hidden Markov \n    model (PHMM) using the Viterbi algorithm (for algorithm details see Durbin et al. 1998, \n    ISBN: 9780521629713). Inserted base pairs are removed and deleted base pairs are accounted \n    for through the introduction of a placeholder character. Since the PHMM is a probabilistic \n    representation of the COI barcode, corrections are not always perfect. For this reason \n    'debar' censors base pairs adjacent to reported indel sites, turning them into placeholder \n    characters (default is 7 base pairs in either direction, this feature can be disabled).\n    Testing has shown that this censorship results in the correct sequence length being restored, \n    and erroneous base pairs being masked the vast majority of the time (>95%).   "
  },
  {
    "id": 10994,
    "package_name": "deltatest",
    "title": "Statistical Hypothesis Testing Using the Delta Method",
    "description": "Statistical hypothesis testing using the Delta method as\n    proposed by Deng et al. (2018) <doi:10.1145/3219819.3219919>. This\n    method replaces the standard variance estimation formula in the Z-test\n    with an approximate formula derived via the Delta method, which can\n    account for within-user correlation.",
    "version": "0.1.0",
    "maintainer": "Koji Makiyama <hoxo.smile@gmail.com>",
    "author": "Koji Makiyama [aut, cre, cph],\n  Shinichi Takayanagi [med]",
    "url": "https://github.com/hoxo-m/deltatest",
    "bug_reports": "https://github.com/hoxo-m/deltatest/issues",
    "repository": "https://cran.r-project.org/package=deltatest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deltatest Statistical Hypothesis Testing Using the Delta Method Statistical hypothesis testing using the Delta method as\n    proposed by Deng et al. (2018) <doi:10.1145/3219819.3219919>. This\n    method replaces the standard variance estimation formula in the Z-test\n    with an approximate formula derived via the Delta method, which can\n    account for within-user correlation.  "
  },
  {
    "id": 11020,
    "package_name": "densityratio",
    "title": "Distribution Comparison Through Density Ratio Estimation",
    "description": "Fast, flexible and user-friendly tools for distribution comparison\n  through direct density ratio estimation. The estimated density ratio can be \n  used for covariate shift adjustment, outlier-detection, change-point detection,\n  classification and evaluation of synthetic data quality. The package implements\n  multiple non-parametric estimation techniques (unconstrained least-squares\n  importance fitting, ulsif(), Kullback-Leibler importance estimation procedure,\n  kliep(), spectral density ratio estimation, spectral(), kernel mean matching,\n  kmm(), and least-squares hetero-distributional subspace search, lhss()).\n  with automatic tuning of hyperparameters. Helper functions are available for\n  two-sample testing and visualizing the density ratios. For an overview on \n  density ratio estimation, see Sugiyama et al. (2012) <doi:10.1017/CBO9781139035613>\n  for a general overview, and the help files for references on the specific \n  estimation techniques.",
    "version": "0.2.2",
    "maintainer": "Thom Volker <thombenjaminvolker@gmail.com>",
    "author": "Thom Volker [aut, cre] (ORCID: <https://orcid.org/0000-0002-2408-7820>),\n  Carlos Gonzalez Poses [ctb],\n  Erik-Jan van Kesteren [ctb]",
    "url": "https://thomvolker.github.io/densityratio/,\nhttps://github.com/thomvolker/densityratio",
    "bug_reports": "https://github.com/thomvolker/densityratio/issues",
    "repository": "https://cran.r-project.org/package=densityratio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "densityratio Distribution Comparison Through Density Ratio Estimation Fast, flexible and user-friendly tools for distribution comparison\n  through direct density ratio estimation. The estimated density ratio can be \n  used for covariate shift adjustment, outlier-detection, change-point detection,\n  classification and evaluation of synthetic data quality. The package implements\n  multiple non-parametric estimation techniques (unconstrained least-squares\n  importance fitting, ulsif(), Kullback-Leibler importance estimation procedure,\n  kliep(), spectral density ratio estimation, spectral(), kernel mean matching,\n  kmm(), and least-squares hetero-distributional subspace search, lhss()).\n  with automatic tuning of hyperparameters. Helper functions are available for\n  two-sample testing and visualizing the density ratios. For an overview on \n  density ratio estimation, see Sugiyama et al. (2012) <doi:10.1017/CBO9781139035613>\n  for a general overview, and the help files for references on the specific \n  estimation techniques.  "
  },
  {
    "id": 11022,
    "package_name": "denstest",
    "title": "Density Equality Testing",
    "description": "Methods for testing the equality between groups of estimated\n    density functions. The package implements FDET (Fourier-based Density Equality\n    Testing) and MDET (Moment-based Density Equality Testing), two new approaches\n    introduced by the author. Both methods extend an earlier testing approach\n    by Delicado (2007), \"Functional k-sample problem when data are density functions\"\n    <doi:10.1007/s00180-007-0047-y>, which is referred to as DET (Density Equality\n    Testing) in this package for clarity. FDET compares groups of densities\n    based on their global shape using Fourier transforms, while MDET tests for\n    differences in distributional moments. All methods are described in Anarat,\n    Krutmann and Schwender (2025), \"Testing for Differences in Extrinsic Skin Aging\n    Based on Density Functions\" (Submitted).",
    "version": "1.0.0",
    "maintainer": "Akin Anarat <akin.anarat@hhu.de>",
    "author": "Akin Anarat [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=denstest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "denstest Density Equality Testing Methods for testing the equality between groups of estimated\n    density functions. The package implements FDET (Fourier-based Density Equality\n    Testing) and MDET (Moment-based Density Equality Testing), two new approaches\n    introduced by the author. Both methods extend an earlier testing approach\n    by Delicado (2007), \"Functional k-sample problem when data are density functions\"\n    <doi:10.1007/s00180-007-0047-y>, which is referred to as DET (Density Equality\n    Testing) in this package for clarity. FDET compares groups of densities\n    based on their global shape using Fourier transforms, while MDET tests for\n    differences in distributional moments. All methods are described in Anarat,\n    Krutmann and Schwender (2025), \"Testing for Differences in Extrinsic Skin Aging\n    Based on Density Functions\" (Submitted).  "
  },
  {
    "id": 11025,
    "package_name": "denvax",
    "title": "Simple Dengue Test and Vaccinate Cost Thresholds",
    "description": "Provides the mathematical model described by \"Serostatus Testing & Dengue Vaccine Cost-Benefit Thresholds\"\n    in <doi:10.1098/rsif.2019.0234>.  Using the functions in the package,\n    that analysis can be repeated using sample life histories, either synthesized from local seroprevalence data\n    using other functions in this package (as in the manuscript) or from some other source.\n    The package provides a vignette which walks through the analysis in the publication, as well as a function\n    to generate a project skeleton for such an analysis.",
    "version": "0.1.2",
    "maintainer": "Carl A. B. Pearson <carl.pearson@lshtm.ac.uk>",
    "author": "Carl A. B. Pearson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0701-7860>),\n  Kaja M. Abbas [aut] (ORCID: <https://orcid.org/0000-0003-0563-1576>),\n  Samuel Clifford [aut] (ORCID: <https://orcid.org/0000-0002-3774-3882>),\n  Stefan Flasche [aut] (ORCID: <https://orcid.org/0000-0002-5808-2606>),\n  Thomas J. Hladish [aut] (ORCID:\n    <https://orcid.org/0000-0003-1819-6235>)",
    "url": "https://gitlab.com/cabp_LSHTM/denvax",
    "bug_reports": "https://gitlab.com/cabp_LSHTM/denvax/issues",
    "repository": "https://cran.r-project.org/package=denvax",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "denvax Simple Dengue Test and Vaccinate Cost Thresholds Provides the mathematical model described by \"Serostatus Testing & Dengue Vaccine Cost-Benefit Thresholds\"\n    in <doi:10.1098/rsif.2019.0234>.  Using the functions in the package,\n    that analysis can be repeated using sample life histories, either synthesized from local seroprevalence data\n    using other functions in this package (as in the manuscript) or from some other source.\n    The package provides a vignette which walks through the analysis in the publication, as well as a function\n    to generate a project skeleton for such an analysis.  "
  },
  {
    "id": 11029,
    "package_name": "depend.truncation",
    "title": "Statistical Methods for the Analysis of Dependently Truncated\nData",
    "description": "Estimation and testing methods for dependently truncated data.\n Semi-parametric methods are based on Emura et al. (2011)<Stat Sinica 21:349-67>, Emura & Wang (2012)<doi:10.1016/j.jmva.2012.03.012>,\n and Emura & Murotani (2015)<doi:10.1007/s11749-015-0432-8>.\n Parametric approaches are based on Emura & Konno (2012)<doi:10.1007/s00362-014-0626-2> and Emura & Pan (2017)<doi:10.1007/s00362-017-0947-z>.\n A regression approach is based on Emura & Wang (2016)<doi:10.1007/s10463-015-0526-9>. Quasi-independence tests are based on Emura & Wang (2010)<doi:10.1016/j.jmva.2009.07.006>.\n Right-truncated data for Japanese male centenarians are given by Emura & Murotani (2015)<doi:10.1007/s11749-015-0432-8>.",
    "version": "3.0",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "author": "Takeshi Emura",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=depend.truncation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "depend.truncation Statistical Methods for the Analysis of Dependently Truncated\nData Estimation and testing methods for dependently truncated data.\n Semi-parametric methods are based on Emura et al. (2011)<Stat Sinica 21:349-67>, Emura & Wang (2012)<doi:10.1016/j.jmva.2012.03.012>,\n and Emura & Murotani (2015)<doi:10.1007/s11749-015-0432-8>.\n Parametric approaches are based on Emura & Konno (2012)<doi:10.1007/s00362-014-0626-2> and Emura & Pan (2017)<doi:10.1007/s00362-017-0947-z>.\n A regression approach is based on Emura & Wang (2016)<doi:10.1007/s10463-015-0526-9>. Quasi-independence tests are based on Emura & Wang (2010)<doi:10.1016/j.jmva.2009.07.006>.\n Right-truncated data for Japanese male centenarians are given by Emura & Murotani (2015)<doi:10.1007/s11749-015-0432-8>.  "
  },
  {
    "id": 11089,
    "package_name": "dfmirroR",
    "title": "Simulate a Data Frame Mirroring an Input and Produce Shareable\nSimulation Code",
    "description": "The 'dfmirroR' package allows users to input a data frame, simulate some number of observations based on specified columns of that data frame, and then outputs a string that contains the code to re-create the simulation. The goal is to both provide workable test data sets and provide users with the information they need to set up reproducible examples with team members. This package was created out of a need to share examples in cases where data are private and where a full data frame is not needed for testing or coordinating.",
    "version": "2.2.0",
    "maintainer": "Jacob Patterson-Stein <jacobpstein@gmail.com>",
    "author": "Jacob Patterson-Stein [aut, cre]",
    "url": "https://github.com/jacobpstein/dfmirroR",
    "bug_reports": "https://github.com/jacobpstein/dfmirroR/issues",
    "repository": "https://cran.r-project.org/package=dfmirroR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dfmirroR Simulate a Data Frame Mirroring an Input and Produce Shareable\nSimulation Code The 'dfmirroR' package allows users to input a data frame, simulate some number of observations based on specified columns of that data frame, and then outputs a string that contains the code to re-create the simulation. The goal is to both provide workable test data sets and provide users with the information they need to set up reproducible examples with team members. This package was created out of a need to share examples in cases where data are private and where a full data frame is not needed for testing or coordinating.  "
  },
  {
    "id": 11123,
    "package_name": "diceR",
    "title": "Diverse Cluster Ensemble in R",
    "description": "Performs cluster analysis using an ensemble clustering\n    framework, Chiu & Talhouk (2018) <doi:10.1186/s12859-017-1996-y>.\n    Results from a diverse set of algorithms are pooled together using\n    methods such as majority voting, K-Modes, LinkCluE, and CSPA. There\n    are options to compare cluster assignments across algorithms using\n    internal and external indices, visualizations such as heatmaps, and\n    significance testing for the existence of clusters.",
    "version": "3.1.0",
    "maintainer": "Derek Chiu <dchiu@bccrc.ca>",
    "author": "Derek Chiu [aut, cre],\n  Aline Talhouk [aut],\n  Johnson Liu [ctb, com]",
    "url": "https://github.com/AlineTalhouk/diceR/,\nhttps://alinetalhouk.github.io/diceR/",
    "bug_reports": "https://github.com/AlineTalhouk/diceR/issues",
    "repository": "https://cran.r-project.org/package=diceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diceR Diverse Cluster Ensemble in R Performs cluster analysis using an ensemble clustering\n    framework, Chiu & Talhouk (2018) <doi:10.1186/s12859-017-1996-y>.\n    Results from a diverse set of algorithms are pooled together using\n    methods such as majority voting, K-Modes, LinkCluE, and CSPA. There\n    are options to compare cluster assignments across algorithms using\n    internal and external indices, visualizations such as heatmaps, and\n    significance testing for the existence of clusters.  "
  },
  {
    "id": 11196,
    "package_name": "discovr",
    "title": "Interactive Tutorials and Data for \"Discovering Statistics Using\nR and RStudio\"",
    "description": "Interactive 'R' tutorials and datasets for the textbook Field\n    (2026), \"Discovering Statistics Using R and RStudio\",\n    <https://www.discovr.rocks/>.  Interactive tutorials cover general\n    workflow in 'R' and 'RStudio', summarizing data, visualizing data,\n    fitting models and bias, correlation, the general linear model (GLM),\n    moderation, mediation, missing values, comparing means using the GLM\n    (analysis of variance), comparing adjusted means (analysis of covariance), factorial\n    designs, repeated measures designs, exploratory factor analysis (EFA).\n    There are no functions, only datasets and interactive tutorials.",
    "version": "0.2.2",
    "maintainer": "Andy Field <andyf@sussex.ac.uk>",
    "author": "Andy Field [aut, cre, cph]",
    "url": "https://www.discovr.rocks,\nhttps://github.com/profandyfield/discovr",
    "bug_reports": "https://github.com/profandyfield/discovr/issues",
    "repository": "https://cran.r-project.org/package=discovr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "discovr Interactive Tutorials and Data for \"Discovering Statistics Using\nR and RStudio\" Interactive 'R' tutorials and datasets for the textbook Field\n    (2026), \"Discovering Statistics Using R and RStudio\",\n    <https://www.discovr.rocks/>.  Interactive tutorials cover general\n    workflow in 'R' and 'RStudio', summarizing data, visualizing data,\n    fitting models and bias, correlation, the general linear model (GLM),\n    moderation, mediation, missing values, comparing means using the GLM\n    (analysis of variance), comparing adjusted means (analysis of covariance), factorial\n    designs, repeated measures designs, exploratory factor analysis (EFA).\n    There are no functions, only datasets and interactive tutorials.  "
  },
  {
    "id": 11211,
    "package_name": "disposables",
    "title": "Create Disposable R Packages for Testing",
    "description": "Create disposable R packages for testing.\n    You can create, install and load multiple R packages with a single\n    function call, and then unload, uninstall and destroy them with another\n    function call. This is handy when testing how some R code or an R package\n    behaves with respect to other packages.",
    "version": "1.0.3",
    "maintainer": "G\u00e1bor Cs\u00e1rdi <csardi.gabor@gmail.com>",
    "author": "G\u00e1bor Cs\u00e1rdi",
    "url": "https://github.com/gaborcsardi/disposables",
    "bug_reports": "https://github.com/gaborcsardi/disposables/issues",
    "repository": "https://cran.r-project.org/package=disposables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "disposables Create Disposable R Packages for Testing Create disposable R packages for testing.\n    You can create, install and load multiple R packages with a single\n    function call, and then unload, uninstall and destroy them with another\n    function call. This is handy when testing how some R code or an R package\n    behaves with respect to other packages.  "
  },
  {
    "id": 11222,
    "package_name": "distcomp",
    "title": "Computations over Distributed Data without Aggregation",
    "description": "Implementing algorithms and fitting models when sites (possibly remote) share\n  computation summaries rather than actual data over HTTP with a master R process (using\n  'opencpu', for example). A stratified Cox model and a singular value decomposition are\n  provided. The former makes direct use of code from the R 'survival' package. (That is,\n  the underlying Cox model code is derived from that in the R 'survival' package.)\n  Sites may provide data via several means: CSV files, Redcap API, etc. An extensible\n  design allows for new methods to be added in the future and includes facilities\n  for local prototyping and testing. Web applications are provided (via 'shiny') for\n  the implemented methods to help in designing and deploying the computations.",
    "version": "1.3-4",
    "maintainer": "Balasubramanian Narasimhan <naras@stat.Stanford.EDU>",
    "author": "Balasubramanian Narasimhan [aut, cre],\n  Marina Bendersky [aut],\n  Sam Gross [aut],\n  Terry M. Therneau [ctb],\n  Thomas Lumley [ctb]",
    "url": "http://dx.doi.org/10.18637/jss.v077.i13",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=distcomp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "distcomp Computations over Distributed Data without Aggregation Implementing algorithms and fitting models when sites (possibly remote) share\n  computation summaries rather than actual data over HTTP with a master R process (using\n  'opencpu', for example). A stratified Cox model and a singular value decomposition are\n  provided. The former makes direct use of code from the R 'survival' package. (That is,\n  the underlying Cox model code is derived from that in the R 'survival' package.)\n  Sites may provide data via several means: CSV files, Redcap API, etc. An extensible\n  design allows for new methods to be added in the future and includes facilities\n  for local prototyping and testing. Web applications are provided (via 'shiny') for\n  the implemented methods to help in designing and deploying the computations.  "
  },
  {
    "id": 11226,
    "package_name": "distfreereg",
    "title": "Distribution-Free Goodness-of-Fit Testing for Regression",
    "description": "Implements the distribution-free goodness-of-fit regression test\n  for the mean structure of parametric models introduced in Khmaladze (2021)\n  <doi:10.1007/s10463-021-00786-3>. The test is implemented for general\n  functions with minimal distributional assumptions as well as common models\n  (e.g., lm, glm) with the usual assumptions.",
    "version": "1.1",
    "maintainer": "Jesse Miller <mill9116@umn.edu>",
    "author": "Jesse Miller [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-9465-7461>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=distfreereg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "distfreereg Distribution-Free Goodness-of-Fit Testing for Regression Implements the distribution-free goodness-of-fit regression test\n  for the mean structure of parametric models introduced in Khmaladze (2021)\n  <doi:10.1007/s10463-021-00786-3>. The test is implemented for general\n  functions with minimal distributional assumptions as well as common models\n  (e.g., lm, glm) with the usual assumptions.  "
  },
  {
    "id": 11243,
    "package_name": "divDyn",
    "title": "Diversity Dynamics using Fossil Sampling Data",
    "description": "Functions to describe sampling and diversity dynamics of fossil occurrence datasets (e.g. from the Paleobiology Database). The package includes methods to calculate range- and occurrence-based metrics of taxonomic richness, extinction and origination rates, along with traditional sampling measures. A powerful subsampling tool is also included that implements frequently used sampling standardization methods in a multiple bin-framework. The plotting of time series and the occurrence data can be simplified by the functions incorporated in the package, as well as other calculations, such as environmental affinities and extinction selectivity testing. Details can be found in: Kocsis, A.T.; Reddin, C.J.; Alroy, J. and Kiessling, W. (2019) <doi:10.1101/423780>.",
    "version": "0.8.3",
    "maintainer": "Adam T. Kocsis <adam.t.kocsis@gmail.com>",
    "author": "Adam T. Kocsis [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9028-665X>),\n  John Alroy [aut] (ORCID: <https://orcid.org/0000-0002-9882-2111>),\n  Carl J. Reddin [aut] (ORCID: <https://orcid.org/0000-0001-5930-1164>),\n  Wolfgang Kiessling [aut] (ORCID:\n    <https://orcid.org/0000-0002-1088-2014>),\n  Deutsche Forschungsgemeinschaft [fnd],\n  FAU GeoZentrum Nordbayern [fnd]",
    "url": "",
    "bug_reports": "https://github.com/divDyn/r-package/issues",
    "repository": "https://cran.r-project.org/package=divDyn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "divDyn Diversity Dynamics using Fossil Sampling Data Functions to describe sampling and diversity dynamics of fossil occurrence datasets (e.g. from the Paleobiology Database). The package includes methods to calculate range- and occurrence-based metrics of taxonomic richness, extinction and origination rates, along with traditional sampling measures. A powerful subsampling tool is also included that implements frequently used sampling standardization methods in a multiple bin-framework. The plotting of time series and the occurrence data can be simplified by the functions incorporated in the package, as well as other calculations, such as environmental affinities and extinction selectivity testing. Details can be found in: Kocsis, A.T.; Reddin, C.J.; Alroy, J. and Kiessling, W. (2019) <doi:10.1101/423780>.  "
  },
  {
    "id": 11247,
    "package_name": "diverge",
    "title": "Evolutionary Trait Divergence Between Sister Species and Other\nPaired Lineages",
    "description": "Compares the fit of alternative models of continuous trait differentiation between sister species and other paired lineages. Differences in trait means between two lineages arise as they diverge from a common ancestor, and alternative processes of evolutionary divergence are expected to leave unique signatures in the distribution of trait differentiation in datasets comprised of many lineage pairs. Models include approximations of divergent selection, drift, and stabilizing selection. A variety of model extensions facilitate the testing of process-to-pattern hypotheses. Users supply trait data and divergence times for each lineage pair. The fit of alternative models is compared in a likelihood framework.",
    "version": "2.0.6",
    "maintainer": "Sean A. S. Anderson <sean.as.anderson@gmail.com>",
    "author": "Sean A. S. Anderson and Jason T. Weir",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=diverge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "diverge Evolutionary Trait Divergence Between Sister Species and Other\nPaired Lineages Compares the fit of alternative models of continuous trait differentiation between sister species and other paired lineages. Differences in trait means between two lineages arise as they diverge from a common ancestor, and alternative processes of evolutionary divergence are expected to leave unique signatures in the distribution of trait differentiation in datasets comprised of many lineage pairs. Models include approximations of divergent selection, drift, and stabilizing selection. A variety of model extensions facilitate the testing of process-to-pattern hypotheses. Users supply trait data and divergence times for each lineage pair. The fit of alternative models is compared in a likelihood framework.  "
  },
  {
    "id": 11312,
    "package_name": "doctest",
    "title": "Generate Tests from Examples Using 'roxygen' and 'testthat'",
    "description": "Creates 'testthat' tests from 'roxygen' examples using simple tags.",
    "version": "0.3.0",
    "maintainer": "David Hugh-Jones <davidhughjones@gmail.com>",
    "author": "David Hugh-Jones [aut, cre]",
    "url": "https://hughjonesd.github.io/doctest/",
    "bug_reports": "https://github.com/hughjonesd/doctest/issues",
    "repository": "https://cran.r-project.org/package=doctest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "doctest Generate Tests from Examples Using 'roxygen' and 'testthat' Creates 'testthat' tests from 'roxygen' examples using simple tags.  "
  },
  {
    "id": 11350,
    "package_name": "downsize",
    "title": "A Tool to Downsize Large Analysis Projects for Testing",
    "description": "Toggles the test and production versions of a large\n  data analysis project.",
    "version": "0.2.3",
    "maintainer": "William Michael Landau <will.landau@gmail.com>",
    "author": "William Michael Landau [aut, cph, cre]",
    "url": "https://github.com/wlandau/downsize",
    "bug_reports": "https://github.com/wlandau/downsize/issues",
    "repository": "https://cran.r-project.org/package=downsize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "downsize A Tool to Downsize Large Analysis Projects for Testing Toggles the test and production versions of a large\n  data analysis project.  "
  },
  {
    "id": 11352,
    "package_name": "dparser",
    "title": "Port of 'Dparser' Package",
    "description": "A Scannerless GLR parser/parser generator.  Note that GLR standing for \"generalized LR\", where L stands for \"left-to-right\" and\n   R stands for \"rightmost (derivation)\".  For more information see <https://en.wikipedia.org/wiki/GLR_parser>. This parser is based on the Tomita\n   (1987) algorithm. (Paper can be found at <https://aclanthology.org/P84-1073.pdf>).\n   The original 'dparser' package documentation can be found at <https://dparser.sourceforge.net/>.  This allows you to add mini-languages to R (like\n   rxode2's ODE mini-language Wang, Hallow, and James 2015 <DOI:10.1002/psp4.12052>) or to parse other languages like 'NONMEM' to automatically translate\n   them to R code.   To use this in your code, add a LinkingTo dparser in your DESCRIPTION file and instead of using #include <dparse.h> use\n   #include <dparser.h>.  This also provides a R-based port of the make_dparser <https://dparser.sourceforge.net/d/make_dparser.cat> command called\n   mkdparser().  Additionally you can parse an arbitrary grammar within R using the dparse() function, which works on most OSes and is mainly for grammar\n   testing.  The fastest parsing, of course, occurs at the C level, and is suggested.",
    "version": "1.3.1-13",
    "maintainer": "Matthew Fidler <matthew.fidler@gmail.com>",
    "author": "Matthew Fidler [aut, cre],\n  John Plevyak [aut, cph]",
    "url": "https://nlmixr2.github.io/dparser-R/,\nhttps://github.com/nlmixr2/dparser-R/",
    "bug_reports": "https://github.com/nlmixr2/dparser-R/issues/",
    "repository": "https://cran.r-project.org/package=dparser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dparser Port of 'Dparser' Package A Scannerless GLR parser/parser generator.  Note that GLR standing for \"generalized LR\", where L stands for \"left-to-right\" and\n   R stands for \"rightmost (derivation)\".  For more information see <https://en.wikipedia.org/wiki/GLR_parser>. This parser is based on the Tomita\n   (1987) algorithm. (Paper can be found at <https://aclanthology.org/P84-1073.pdf>).\n   The original 'dparser' package documentation can be found at <https://dparser.sourceforge.net/>.  This allows you to add mini-languages to R (like\n   rxode2's ODE mini-language Wang, Hallow, and James 2015 <DOI:10.1002/psp4.12052>) or to parse other languages like 'NONMEM' to automatically translate\n   them to R code.   To use this in your code, add a LinkingTo dparser in your DESCRIPTION file and instead of using #include <dparse.h> use\n   #include <dparser.h>.  This also provides a R-based port of the make_dparser <https://dparser.sourceforge.net/d/make_dparser.cat> command called\n   mkdparser().  Additionally you can parse an arbitrary grammar within R using the dparse() function, which works on most OSes and is mainly for grammar\n   testing.  The fastest parsing, of course, occurs at the C level, and is suggested.  "
  },
  {
    "id": 11396,
    "package_name": "droptest",
    "title": "Simulates LOX Drop Testing",
    "description": "Generates simulated data representing the LOX drop testing\n    process (also known as impact testing). A simulated process allows for\n    accelerated study of test behavior. Functions are provided to simulate\n    trials, test series, and groups of test series. Functions for creating plots\n    specific to this process are also included. Test attributes and criteria can\n    be set arbitrarily. This work is not endorsed by or affiliated with NASA.\n    See \"ASTM G86-17, Standard Test Method for Determining Ignition Sensitivity\n    of Materials to Mechanical Impact in Ambient Liquid Oxygen and Pressurized\n    Liquid and Gaseous Oxygen Environments\" <doi:10.1520/G0086-17>.",
    "version": "0.1.3",
    "maintainer": "Chad Ross <chad.ross@gmail.com>",
    "author": "Chad Ross [aut, cre]",
    "url": "https://github.com/chadr/droptest",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=droptest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "droptest Simulates LOX Drop Testing Generates simulated data representing the LOX drop testing\n    process (also known as impact testing). A simulated process allows for\n    accelerated study of test behavior. Functions are provided to simulate\n    trials, test series, and groups of test series. Functions for creating plots\n    specific to this process are also included. Test attributes and criteria can\n    be set arbitrarily. This work is not endorsed by or affiliated with NASA.\n    See \"ASTM G86-17, Standard Test Method for Determining Ignition Sensitivity\n    of Materials to Mechanical Impact in Ambient Liquid Oxygen and Pressurized\n    Liquid and Gaseous Oxygen Environments\" <doi:10.1520/G0086-17>.  "
  },
  {
    "id": 11417,
    "package_name": "dslice",
    "title": "Dynamic Slicing",
    "description": "Dynamic slicing is a method designed for dependency detection between a categorical variable and a continuous variable. It could be applied for non-parametric hypothesis testing and gene set enrichment analysis.",
    "version": "1.2.2",
    "maintainer": "Chao Ye <yechao1009@gmail.com>",
    "author": "Chao Ye [aut, cre],\n  Bo Jiang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dslice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dslice Dynamic Slicing Dynamic slicing is a method designed for dependency detection between a categorical variable and a continuous variable. It could be applied for non-parametric hypothesis testing and gene set enrichment analysis.  "
  },
  {
    "id": 11486,
    "package_name": "dynamac",
    "title": "Dynamic Simulation and Testing for Single-Equation ARDL Models",
    "description": "While autoregressive distributed lag (ARDL) models allow for extremely flexible dynamics, interpreting substantive significance of complex lag structures remains difficult. This package is designed to assist users in dynamically simulating and plotting the results of various ARDL models. It also contains post-estimation diagnostics, including a test for cointegration when estimating the error-correction variant of the autoregressive distributed lag model (Pesaran, Shin, and Smith 2001 <doi:10.1002/jae.616>).",
    "version": "0.1.12",
    "maintainer": "Soren Jordan <sorenjordanpols@gmail.com>",
    "author": "Soren Jordan [aut, cre, cph],\n  Andrew Q. Philips [aut]",
    "url": "https://github.com/andyphilips/dynamac/",
    "bug_reports": "https://github.com/andyphilips/dynamac/issues",
    "repository": "https://cran.r-project.org/package=dynamac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dynamac Dynamic Simulation and Testing for Single-Equation ARDL Models While autoregressive distributed lag (ARDL) models allow for extremely flexible dynamics, interpreting substantive significance of complex lag structures remains difficult. This package is designed to assist users in dynamically simulating and plotting the results of various ARDL models. It also contains post-estimation diagnostics, including a test for cointegration when estimating the error-correction variant of the autoregressive distributed lag model (Pesaran, Shin, and Smith 2001 <doi:10.1002/jae.616>).  "
  },
  {
    "id": 11493,
    "package_name": "dyngen",
    "title": "A Multi-Modal Simulator for Spearheading Single-Cell Omics\nAnalyses",
    "description": "A novel, multi-modal simulation engine for\n    studying dynamic cellular processes at single-cell resolution. 'dyngen'\n    is more flexible than current single-cell simulation engines. It\n    allows better method development and benchmarking, thereby stimulating\n    development and testing of novel computational methods. Cannoodt et\n    al. (2021) <doi:10.1038/s41467-021-24152-2>.",
    "version": "1.0.5",
    "maintainer": "Robrecht Cannoodt <rcannood@gmail.com>",
    "author": "Robrecht Cannoodt [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3641-729X>),\n  Wouter Saelens [aut] (ORCID: <https://orcid.org/0000-0002-7114-6248>)",
    "url": "https://dyngen.dynverse.org, https://github.com/dynverse/dyngen",
    "bug_reports": "https://github.com/dynverse/dyngen/issues",
    "repository": "https://cran.r-project.org/package=dyngen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dyngen A Multi-Modal Simulator for Spearheading Single-Cell Omics\nAnalyses A novel, multi-modal simulation engine for\n    studying dynamic cellular processes at single-cell resolution. 'dyngen'\n    is more flexible than current single-cell simulation engines. It\n    allows better method development and benchmarking, thereby stimulating\n    development and testing of novel computational methods. Cannoodt et\n    al. (2021) <doi:10.1038/s41467-021-24152-2>.  "
  },
  {
    "id": 11576,
    "package_name": "eba",
    "title": "Elimination-by-Aspects Models",
    "description": "Fitting and testing multi-attribute probabilistic choice\n  models, especially the Bradley-Terry-Luce (BTL) model (Bradley &\n  Terry, 1952 <doi:10.1093/biomet/39.3-4.324>; Luce, 1959),\n  elimination-by-aspects (EBA) models (Tversky, 1972 <doi:10.1037/h0032955>),\n  and preference tree (Pretree) models (Tversky & Sattath, 1979\n  <doi:10.1037/0033-295X.86.6.542>).",
    "version": "1.10-1",
    "maintainer": "Florian Wickelmaier <wickelmaier@web.de>",
    "author": "Florian Wickelmaier [aut, cre]",
    "url": "https://www.mathpsy.uni-tuebingen.de/wickelmaier/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eba Elimination-by-Aspects Models Fitting and testing multi-attribute probabilistic choice\n  models, especially the Bradley-Terry-Luce (BTL) model (Bradley &\n  Terry, 1952 <doi:10.1093/biomet/39.3-4.324>; Luce, 1959),\n  elimination-by-aspects (EBA) models (Tversky, 1972 <doi:10.1037/h0032955>),\n  and preference tree (Pretree) models (Tversky & Sattath, 1979\n  <doi:10.1037/0033-295X.86.6.542>).  "
  },
  {
    "id": 11607,
    "package_name": "eco",
    "title": "Ecological Inference in 2x2 Tables",
    "description": "Implements the Bayesian and likelihood methods proposed in Imai, Lu, and Strauss (2008 <doi:10.1093/pan/mpm017>) and (2011 <doi:10.18637/jss.v042.i05>) for ecological inference in 2 by 2 tables as well as the method of bounds introduced by Duncan and Davis (1953).  The package fits both parametric and nonparametric models using either the Expectation-Maximization algorithms (for likelihood models) or the Markov chain Monte Carlo algorithms (for Bayesian models).  For all models, the individual-level data can be directly incorporated into the estimation whenever such data are available. Along with in-sample and out-of-sample predictions, the package also provides a functionality which allows one to quantify the effect of data aggregation on parameter estimation and hypothesis testing under the parametric likelihood models.",
    "version": "4.0-6",
    "maintainer": "Kosuke Imai <imai@Harvard.Edu>",
    "author": "Kosuke Imai [aut, cre],\n  Ying Lu [aut],\n  Aaron Strauss [aut],\n  Hubert Jin [ctb]",
    "url": "https://github.com/kosukeimai/eco",
    "bug_reports": "https://github.com/kosukeimai/eco/issues",
    "repository": "https://cran.r-project.org/package=eco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eco Ecological Inference in 2x2 Tables Implements the Bayesian and likelihood methods proposed in Imai, Lu, and Strauss (2008 <doi:10.1093/pan/mpm017>) and (2011 <doi:10.18637/jss.v042.i05>) for ecological inference in 2 by 2 tables as well as the method of bounds introduced by Duncan and Davis (1953).  The package fits both parametric and nonparametric models using either the Expectation-Maximization algorithms (for likelihood models) or the Markov chain Monte Carlo algorithms (for Bayesian models).  For all models, the individual-level data can be directly incorporated into the estimation whenever such data are available. Along with in-sample and out-of-sample predictions, the package also provides a functionality which allows one to quantify the effect of data aggregation on parameter estimation and hypothesis testing under the parametric likelihood models.  "
  },
  {
    "id": 11626,
    "package_name": "ecopower",
    "title": "Power Estimates and Equivalence Testing for Multivariate Data",
    "description": "Estimates power by simulation for multivariate\n    abundance data to be used for sample size estimates. Multivariate\n    equivalence testing by simulation from a Gaussian copula model.\n    The package also provides functions for parameterising multivariate effect\n    sizes and simulating multivariate abundance data jointly. The discrete\n    Gaussian copula approach is described in\n    Popovic et al. (2018) <doi:10.1016/j.jmva.2017.12.002>.",
    "version": "0.2.0",
    "maintainer": "Michelle Lim <michelle.lim@unsw.edu.au>",
    "author": "Ben Maslen [aut],\n  Michelle Lim [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ecopower",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ecopower Power Estimates and Equivalence Testing for Multivariate Data Estimates power by simulation for multivariate\n    abundance data to be used for sample size estimates. Multivariate\n    equivalence testing by simulation from a Gaussian copula model.\n    The package also provides functions for parameterising multivariate effect\n    sizes and simulating multivariate abundance data jointly. The discrete\n    Gaussian copula approach is described in\n    Popovic et al. (2018) <doi:10.1016/j.jmva.2017.12.002>.  "
  },
  {
    "id": 11648,
    "package_name": "ectotemp",
    "title": "Quantitative Estimates of Small Ectotherm Temperature Regulation\nEffectiveness",
    "description": "Easy and rapid quantitative estimation of small terrestrial ectotherm temperature regulation effectiveness in R. ectotemp is built on classical formulas that evaluate temperature regulation by means of various indices, inaugurated by Hertz et al. (1993) <doi: 10.1086/285573>. Options for bootstrapping and permutation testing are included to test hypotheses about divergence between organisms, species or populations.",
    "version": "0.2.0",
    "maintainer": "Wouter Beukema <wouter.beukema@gmail.com>",
    "author": "Wouter Beukema [aut, cre]",
    "url": "https://CRAN.R-project.org/package=ectotemp",
    "bug_reports": "https://github.com/wouterbeukema/ectotemp/issues",
    "repository": "https://cran.r-project.org/package=ectotemp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ectotemp Quantitative Estimates of Small Ectotherm Temperature Regulation\nEffectiveness Easy and rapid quantitative estimation of small terrestrial ectotherm temperature regulation effectiveness in R. ectotemp is built on classical formulas that evaluate temperature regulation by means of various indices, inaugurated by Hertz et al. (1993) <doi: 10.1086/285573>. Options for bootstrapping and permutation testing are included to test hypotheses about divergence between organisms, species or populations.  "
  },
  {
    "id": 11741,
    "package_name": "elgbd",
    "title": "Empirical Likelihood for General Block Designs",
    "description": "Performs hypothesis testing for general block designs with\n    empirical likelihood. The core computational routines are implemented\n    using the 'Eigen' 'C++' library and 'RcppEigen' interface, with\n    'OpenMP' for parallel computation. Details of the methods are given in\n    Kim, MacEachern, and Peruggia (2023)\n    <doi:10.1080/10485252.2023.2206919>. This work was supported by the\n    U.S. National Science Foundation under Grants No.  SES-1921523 and\n    DMS-2015552.",
    "version": "0.9.0",
    "maintainer": "Eunseop Kim <markean@pm.me>",
    "author": "Eunseop Kim [aut, cph, cre],\n  Steven MacEachern [ctb, ths],\n  Mario Peruggia [ctb, ths]",
    "url": "https://github.com/markean/elgbd",
    "bug_reports": "https://github.com/markean/elgbd/issues",
    "repository": "https://cran.r-project.org/package=elgbd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "elgbd Empirical Likelihood for General Block Designs Performs hypothesis testing for general block designs with\n    empirical likelihood. The core computational routines are implemented\n    using the 'Eigen' 'C++' library and 'RcppEigen' interface, with\n    'OpenMP' for parallel computation. Details of the methods are given in\n    Kim, MacEachern, and Peruggia (2023)\n    <doi:10.1080/10485252.2023.2206919>. This work was supported by the\n    U.S. National Science Foundation under Grants No.  SES-1921523 and\n    DMS-2015552.  "
  },
  {
    "id": 11744,
    "package_name": "elitism",
    "title": "Equipment for Logarithmic and Linear Time Stepwise Multiple\nHypothesis Testing",
    "description": "Recently many new p-value based multiple test procedures have been proposed, and these new methods are more powerful than the widely used Hochberg procedure. These procedures strongly control the familywise error rate (FWER). This is a comprehensive collection of p-value based FWER-control stepwise multiple test procedures, including six procedure families and thirty multiple test procedures. In this collection, the conservative Hochberg procedure, linear time Hommel procedures, asymptotic Rom procedure, Gou-Tamhane-Xi-Rom procedures, and Quick procedures are all developed in recent five years since 2014. The package name \"elitism\" is an acronym of \"e\"quipment for \"l\"ogarithmic and l\"i\"near \"ti\"me \"s\"tepwise \"m\"ultiple hypothesis testing.\n    See Gou, J. (2022), \"Quick multiple test procedures and p-value adjustments\", Statistics in Biopharmaceutical Research 14(4), 636-650.",
    "version": "1.1.1",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing (Zoe) Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=elitism",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "elitism Equipment for Logarithmic and Linear Time Stepwise Multiple\nHypothesis Testing Recently many new p-value based multiple test procedures have been proposed, and these new methods are more powerful than the widely used Hochberg procedure. These procedures strongly control the familywise error rate (FWER). This is a comprehensive collection of p-value based FWER-control stepwise multiple test procedures, including six procedure families and thirty multiple test procedures. In this collection, the conservative Hochberg procedure, linear time Hommel procedures, asymptotic Rom procedure, Gou-Tamhane-Xi-Rom procedures, and Quick procedures are all developed in recent five years since 2014. The package name \"elitism\" is an acronym of \"e\"quipment for \"l\"ogarithmic and l\"i\"near \"ti\"me \"s\"tepwise \"m\"ultiple hypothesis testing.\n    See Gou, J. (2022), \"Quick multiple test procedures and p-value adjustments\", Statistics in Biopharmaceutical Research 14(4), 636-650.  "
  },
  {
    "id": 11792,
    "package_name": "energy",
    "title": "E-Statistics: Multivariate Inference via the Energy of Data",
    "description": "E-statistics (energy) tests and statistics for multivariate and univariate inference,\n             including distance correlation, one-sample, two-sample, and multi-sample tests for\n             comparing multivariate distributions, are implemented. Measuring and testing\n             multivariate independence based on distance correlation, partial distance correlation,\n             multivariate goodness-of-fit tests, k-groups and hierarchical clustering based on energy \n             distance, testing for multivariate normality, distance components (disco) for non-parametric \n             analysis of structured data, and other energy statistics/methods are implemented.",
    "version": "1.7-12",
    "maintainer": "Maria Rizzo <mrizzo@bgsu.edu>",
    "author": "Maria Rizzo [aut, cre],\n  Gabor Szekely [aut]",
    "url": "https://github.com/mariarizzo/energy",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=energy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "energy E-Statistics: Multivariate Inference via the Energy of Data E-statistics (energy) tests and statistics for multivariate and univariate inference,\n             including distance correlation, one-sample, two-sample, and multi-sample tests for\n             comparing multivariate distributions, are implemented. Measuring and testing\n             multivariate independence based on distance correlation, partial distance correlation,\n             multivariate goodness-of-fit tests, k-groups and hierarchical clustering based on energy \n             distance, testing for multivariate normality, distance components (disco) for non-parametric \n             analysis of structured data, and other energy statistics/methods are implemented.  "
  },
  {
    "id": 11875,
    "package_name": "epsiwal",
    "title": "Exact Post Selection Inference with Applications to the Lasso",
    "description": "Implements the conditional estimation procedure of\n  Lee, Sun, Sun and Taylor (2016) <doi:10.1214/15-AOS1371>.\n  This procedure allows hypothesis testing on the mean of\n  a normal random vector subject to linear constraints.",
    "version": "0.1.0",
    "maintainer": "Steven E. Pav <shabbychef@gmail.com>",
    "author": "Steven E. Pav [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4197-6195>)",
    "url": "https://github.com/shabbychef/epsiwal",
    "bug_reports": "https://github.com/shabbychef/epsiwal/issues",
    "repository": "https://cran.r-project.org/package=epsiwal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "epsiwal Exact Post Selection Inference with Applications to the Lasso Implements the conditional estimation procedure of\n  Lee, Sun, Sun and Taylor (2016) <doi:10.1214/15-AOS1371>.\n  This procedure allows hypothesis testing on the mean of\n  a normal random vector subject to linear constraints.  "
  },
  {
    "id": 11880,
    "package_name": "eqtesting",
    "title": "Equivalence Testing Functions",
    "description": "Contains several functions for equivalence testing and practical significance testing. First, the tsti() command provides an automatic computation of three-sided testing results for a given estimate, standard error, and region of practical equivalence. For details, see Goeman, Solari, & Stijnen (2010) <doi:10.1002/sim.4002> and Isager & Fitzgerald (2024) <doi:10.31234/osf.io/8y925>. Second, the lddtest() command performs logarithmic density discontinuity equivalence testing for regression discontinuity designs. For reference, see Fitzgerald (2025) <doi:10.31222/osf.io/2dgrp_v1>. ",
    "version": "0.1.1",
    "maintainer": "Jack Fitzgerald <j.f.fitzgerald@vu.nl>",
    "author": "Jack Fitzgerald [aut, cre],\n  Peder Isager [ctb]",
    "url": "https://github.com/jack-fitzgerald/eqtesting",
    "bug_reports": "https://github.com/jack-fitzgerald/eqtesting/issues",
    "repository": "https://cran.r-project.org/package=eqtesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eqtesting Equivalence Testing Functions Contains several functions for equivalence testing and practical significance testing. First, the tsti() command provides an automatic computation of three-sided testing results for a given estimate, standard error, and region of practical equivalence. For details, see Goeman, Solari, & Stijnen (2010) <doi:10.1002/sim.4002> and Isager & Fitzgerald (2024) <doi:10.31234/osf.io/8y925>. Second, the lddtest() command performs logarithmic density discontinuity equivalence testing for regression discontinuity designs. For reference, see Fitzgerald (2025) <doi:10.31222/osf.io/2dgrp_v1>.   "
  },
  {
    "id": 11881,
    "package_name": "equalCovs",
    "title": "Testing the Equality of Two Covariance Matrices",
    "description": "Tests the equality of two covariance matrices, used in paper \"Two sample tests for high dimensional covariance matrices.\" Li and Chen (2012) <arXiv:1206.0917>.",
    "version": "1.0",
    "maintainer": "Jun Li <jli49@kent.edu>",
    "author": "Jun Li [aut, cre], \n        Song Xi Chen [aut],\n        Lingjun Li [ctb],\n        Clay James [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=equalCovs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "equalCovs Testing the Equality of Two Covariance Matrices Tests the equality of two covariance matrices, used in paper \"Two sample tests for high dimensional covariance matrices.\" Li and Chen (2012) <arXiv:1206.0917>.  "
  },
  {
    "id": 11882,
    "package_name": "equaltestMI",
    "title": "Examine Measurement Invariance via Equivalence Testing and\nProjection Method",
    "description": "Functions for examining measurement invariance via equivalence testing are included in this package. The traditionally used RMSEA (Root Mean Square Error of Approximation) cutoff values are adjusted based on simulation results. In addition, a projection-based method is implemented to test the equality of latent factor means across groups without assuming the equality of intercepts. For more information, see Yuan, K. H., & Chan, W. (2016) <doi:10.1037/met0000080>, Deng, L., & Yuan, K. H. (2016) <doi:10.1007/s11336-015-9491-8>, and Jiang, G., Mai, Y., & Yuan, K. H. (2017) <doi:10.3389/fpsyg.2017.01823>. ",
    "version": "0.6.1",
    "maintainer": "Ge Jiang <gejiang2@illinois.edu>",
    "author": "Ge Jiang [aut, cre],\n  Yujiao Mai [aut],\n  Ke-Hai Yuan [ctb]",
    "url": "",
    "bug_reports": "https://github.com/gabriellajg/equaltestMI/issues",
    "repository": "https://cran.r-project.org/package=equaltestMI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "equaltestMI Examine Measurement Invariance via Equivalence Testing and\nProjection Method Functions for examining measurement invariance via equivalence testing are included in this package. The traditionally used RMSEA (Root Mean Square Error of Approximation) cutoff values are adjusted based on simulation results. In addition, a projection-based method is implemented to test the equality of latent factor means across groups without assuming the equality of intercepts. For more information, see Yuan, K. H., & Chan, W. (2016) <doi:10.1037/met0000080>, Deng, L., & Yuan, K. H. (2016) <doi:10.1007/s11336-015-9491-8>, and Jiang, G., Mai, Y., & Yuan, K. H. (2017) <doi:10.3389/fpsyg.2017.01823>.   "
  },
  {
    "id": 11919,
    "package_name": "esback",
    "title": "Expected Shortfall Backtesting",
    "description": "Implementations of the expected shortfall backtests of Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    as well as other well known backtests from the literature. Can be used to assess the correctness of forecasts of the \n    expected shortfall risk measure which is e.g. used in the banking and finance industry for quantifying the market risk \n    of investments. A special feature of the backtests of  Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    is that they only require forecasts of  the expected shortfall, which is in striking contrast to all other existing \n    backtests, making them particularly attractive for practitioners.",
    "version": "0.3.1",
    "maintainer": "Sebastian Bayer <sebastian.bayer@uni-konstanz.de>",
    "author": "Sebastian Bayer [aut, cre],\n  Timo Dimitriadis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=esback",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "esback Expected Shortfall Backtesting Implementations of the expected shortfall backtests of Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    as well as other well known backtests from the literature. Can be used to assess the correctness of forecasts of the \n    expected shortfall risk measure which is e.g. used in the banking and finance industry for quantifying the market risk \n    of investments. A special feature of the backtests of  Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    is that they only require forecasts of  the expected shortfall, which is in striking contrast to all other existing \n    backtests, making them particularly attractive for practitioners.  "
  },
  {
    "id": 11958,
    "package_name": "etsi",
    "title": "Efficient Testing Using Surrogate Information",
    "description": "Provides functions for treatment effect estimation, hypothesis testing, and future study design for settings where the surrogate is used in place of the primary outcome for individuals for whom the surrogate is valid, and the primary outcome is purposefully measured in the remaining patients. More details are available in: Knowlton, R., Parast, L. (2024) ``Efficient Testing Using Surrogate Information,\" Biometrical Journal, 67(6): e70086,  <doi:10.1002/bimj.70086>. A tutorial for this package can be found at <https://www.laylaparast.com/etsi>.",
    "version": "1.0",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "author": "Rebecca Knowlton [aut],\n  Layla Parast [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=etsi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "etsi Efficient Testing Using Surrogate Information Provides functions for treatment effect estimation, hypothesis testing, and future study design for settings where the surrogate is used in place of the primary outcome for individuals for whom the surrogate is valid, and the primary outcome is purposefully measured in the remaining patients. More details are available in: Knowlton, R., Parast, L. (2024) ``Efficient Testing Using Surrogate Information,\" Biometrical Journal, 67(6): e70086,  <doi:10.1002/bimj.70086>. A tutorial for this package can be found at <https://www.laylaparast.com/etsi>.  "
  },
  {
    "id": 11971,
    "package_name": "eva",
    "title": "Extreme Value Analysis with Goodness-of-Fit Testing",
    "description": "Goodness-of-fit tests for selection of r in the r-largest order\n    statistics (GEVr) model. Goodness-of-fit tests for threshold selection in the\n    Generalized Pareto distribution (GPD). Random number generation and density functions\n    for the GEVr distribution. Profile likelihood for return level estimation\n    using the GEVr and Generalized Pareto distributions. P-value adjustments for\n    sequential, multiple testing error control. Non-stationary fitting of GEVr and\n    GPD.\n    Bader, B., Yan, J. & Zhang, X. (2016) <doi:10.1007/s11222-016-9697-3>.\n    Bader, B., Yan, J. & Zhang, X. (2018) <doi:10.1214/17-AOAS1092>.",
    "version": "0.2.6",
    "maintainer": "Brian Bader <bbader.stat@gmail.com>",
    "author": "Brian Bader [aut, cre],\n  Jun Yan [ctb]",
    "url": "https://github.com/brianbader/eva_package",
    "bug_reports": "https://github.com/brianbader/eva_package/issues",
    "repository": "https://cran.r-project.org/package=eva",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eva Extreme Value Analysis with Goodness-of-Fit Testing Goodness-of-fit tests for selection of r in the r-largest order\n    statistics (GEVr) model. Goodness-of-fit tests for threshold selection in the\n    Generalized Pareto distribution (GPD). Random number generation and density functions\n    for the GEVr distribution. Profile likelihood for return level estimation\n    using the GEVr and Generalized Pareto distributions. P-value adjustments for\n    sequential, multiple testing error control. Non-stationary fitting of GEVr and\n    GPD.\n    Bader, B., Yan, J. & Zhang, X. (2016) <doi:10.1007/s11222-016-9697-3>.\n    Bader, B., Yan, J. & Zhang, X. (2018) <doi:10.1214/17-AOAS1092>.  "
  },
  {
    "id": 11990,
    "package_name": "eventstudyr",
    "title": "Estimation and Visualization of Linear Panel Event Studies",
    "description": "Estimates linear panel event study models. Plots coefficients following the recommendations in Freyaldenhoven et al. (2021) <doi:10.3386/w29170>. Includes sup-t bands, testing for key hypotheses, least wiggly path through the Wald region. Allows instrumental variables estimation following Freyaldenhoven et al. (2019) <doi:10.1257/aer.20180609>.",
    "version": "1.1.4",
    "maintainer": "Santiago Hermo <santiago.hermo@monash.edu>",
    "author": "Simon Freyaldenhoven [aut],\n  Christian Hansen [aut],\n  Jorge P\u00e9rez P\u00e9rez [aut],\n  Jesse Shapiro [aut],\n  Veli Andirin [aut],\n  Richard Calvo [aut],\n  Santiago Hermo [aut, cre],\n  Nathan Schor [aut],\n  Emily Wang [aut],\n  JMSLab [cph],\n  Ryan Kessler [cph]",
    "url": "https://github.com/JMSLab/eventstudyr",
    "bug_reports": "https://github.com/JMSLab/eventstudyr/issues",
    "repository": "https://cran.r-project.org/package=eventstudyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eventstudyr Estimation and Visualization of Linear Panel Event Studies Estimates linear panel event study models. Plots coefficients following the recommendations in Freyaldenhoven et al. (2021) <doi:10.3386/w29170>. Includes sup-t bands, testing for key hypotheses, least wiggly path through the Wald region. Allows instrumental variables estimation following Freyaldenhoven et al. (2019) <doi:10.1257/aer.20180609>.  "
  },
  {
    "id": 11994,
    "package_name": "evidence",
    "title": "Analysis of Scientific Evidence Using Bayesian and Likelihood\nMethods",
    "description": "Bayesian (and some likelihoodist) functions as alternatives to hypothesis-testing functions in R base using a user interface patterned after those of R's hypothesis testing functions. See McElreath (2016, ISBN: 978-1-4822-5344-3), Gelman and Hill (2007, ISBN: 0-521-68689-X) (new edition in preparation) and Albert (2009, ISBN: 978-0-387-71384-7) for good introductions to Bayesian analysis and Pawitan (2002, ISBN: 0-19-850765-8) for the Likelihood approach.  The functions in the package also make extensive use of graphical displays for data exploration and model comparison.",
    "version": "0.8.10",
    "maintainer": "Robert van Hulst <rvhulst@ubishops.ca>",
    "author": "Robert van Hulst",
    "url": "",
    "bug_reports": "https://github.com/rvhulst/evidence/",
    "repository": "https://cran.r-project.org/package=evidence",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evidence Analysis of Scientific Evidence Using Bayesian and Likelihood\nMethods Bayesian (and some likelihoodist) functions as alternatives to hypothesis-testing functions in R base using a user interface patterned after those of R's hypothesis testing functions. See McElreath (2016, ISBN: 978-1-4822-5344-3), Gelman and Hill (2007, ISBN: 0-521-68689-X) (new edition in preparation) and Albert (2009, ISBN: 978-0-387-71384-7) for good introductions to Bayesian analysis and Pawitan (2002, ISBN: 0-19-850765-8) for the Likelihood approach.  The functions in the package also make extensive use of graphical displays for data exploration and model comparison.  "
  },
  {
    "id": 12009,
    "package_name": "evolved",
    "title": "Open Software for Teaching Evolutionary Biology at Multiple\nScales Through Virtual Inquiries",
    "description": "\"Evolutionary Virtual Education\" - 'evolved' - provides multiple tools to help educators (especially at the graduate level or in advanced undergraduate level courses) apply inquiry-based learning in general evolution classes. In particular, the tools provided include functions that simulate evolutionary processes (e.g., genetic drift, natural selection within a single locus) or concepts (e.g. Hardy-Weinberg equilibrium, phylogenetic distribution of traits). More than only simulating, the package also provides tools for students to analyze (e.g., measuring, testing, visualizing) datasets with characteristics that are common to many fields related to evolutionary biology. Importantly, the package is heavily oriented towards providing tools for inquiry-based learning - where students follow scientific practices to actively construct knowledge. For additional details, see package's vignettes.",
    "version": "1.0.0",
    "maintainer": "Matheus Januario <januarioml.eco@gmail.com>",
    "author": "Matheus Januario [aut, cph, cre] (ORCID:\n    <https://orcid.org/0000-0002-6480-7095>),\n  Jennifer Auler [aut] (ORCID: <https://orcid.org/0000-0001-7576-9058>),\n  Andressa Viol [aut],\n  Daniel Rabosky [aut]",
    "url": "<https://github.com/Auler-J/evolved>",
    "bug_reports": "https://github.com/Auler-J/evolved/issues",
    "repository": "https://cran.r-project.org/package=evolved",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "evolved Open Software for Teaching Evolutionary Biology at Multiple\nScales Through Virtual Inquiries \"Evolutionary Virtual Education\" - 'evolved' - provides multiple tools to help educators (especially at the graduate level or in advanced undergraduate level courses) apply inquiry-based learning in general evolution classes. In particular, the tools provided include functions that simulate evolutionary processes (e.g., genetic drift, natural selection within a single locus) or concepts (e.g. Hardy-Weinberg equilibrium, phylogenetic distribution of traits). More than only simulating, the package also provides tools for students to analyze (e.g., measuring, testing, visualizing) datasets with characteristics that are common to many fields related to evolutionary biology. Importantly, the package is heavily oriented towards providing tools for inquiry-based learning - where students follow scientific practices to actively construct knowledge. For additional details, see package's vignettes.  "
  },
  {
    "id": 12027,
    "package_name": "exampletestr",
    "title": "Help for Writing Unit Tests Based on Function Examples",
    "description": "Take the examples written in your documentation of functions\n    and use them to create shells (skeletons which must be manually\n    completed by the user) of test files to be tested with the 'testthat'\n    package. Sort of like python 'doctests' for R.",
    "version": "1.7.3",
    "maintainer": "Rory Nolan <rorynoolan@gmail.com>",
    "author": "Rory Nolan [aut, cre] (ORCID: <https://orcid.org/0000-0002-5239-4043>),\n  Sergi Padilla-Parra [ths] (ORCID:\n    <https://orcid.org/0000-0002-8010-9481>),\n  Thomas Quinn [rev] (ORCID: <https://orcid.org/0000-0003-0286-6329>),\n  Laurent Gatto [rev] (ORCID: <https://orcid.org/0000-0002-1520-2268>)",
    "url": "https://rorynolan.github.io/exampletestr/,\nhttps://github.com/rorynolan/exampletestr#readme",
    "bug_reports": "https://github.com/rorynolan/exampletestr/issues",
    "repository": "https://cran.r-project.org/package=exampletestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exampletestr Help for Writing Unit Tests Based on Function Examples Take the examples written in your documentation of functions\n    and use them to create shells (skeletons which must be manually\n    completed by the user) of test files to be tested with the 'testthat'\n    package. Sort of like python 'doctests' for R.  "
  },
  {
    "id": 12057,
    "package_name": "expirest",
    "title": "Expiry Estimation Procedures",
    "description": "The Australian Regulatory Guidelines for Prescription\n    Medicines (ARGPM), guidance on \"Stability testing for prescription\n    medicines\", recommends to predict the shelf life of chemically derived\n    medicines from stability data by taking the worst case situation at batch\n    release into account. Consequently, if a change over time is observed,\n    a release limit needs to be specified. Finding a release limit and the\n    associated shelf life is supported, as well as the standard approach\n    that is recommended by guidance Q1E \"Evaluation of stability data\"\n    from the International Council for Harmonisation (ICH).",
    "version": "0.1.7",
    "maintainer": "Pius Dahinden <pius.dahinden@tillotts.com>",
    "author": "Pius Dahinden [aut, cre],\n  Tillotts Pharma AG [cph, fnd]",
    "url": "https://github.com/piusdahinden/expirest",
    "bug_reports": "https://github.com/piusdahinden/expirest/issues",
    "repository": "https://cran.r-project.org/package=expirest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "expirest Expiry Estimation Procedures The Australian Regulatory Guidelines for Prescription\n    Medicines (ARGPM), guidance on \"Stability testing for prescription\n    medicines\", recommends to predict the shelf life of chemically derived\n    medicines from stability data by taking the worst case situation at batch\n    release into account. Consequently, if a change over time is observed,\n    a release limit needs to be specified. Finding a release limit and the\n    associated shelf life is supported, as well as the standard approach\n    that is recommended by guidance Q1E \"Evaluation of stability data\"\n    from the International Council for Harmonisation (ICH).  "
  },
  {
    "id": 12066,
    "package_name": "expss",
    "title": "Tables, Labels and Some Useful Functions from Spreadsheets and\n'SPSS' Statistics",
    "description": "Package computes and displays tables with support for 'SPSS'-style \n        labels, multiple and nested banners, weights, multiple-response variables \n        and significance testing. There are facilities for nice output of tables \n        in 'knitr', 'Shiny', '*.xlsx' files, R and 'Jupyter' notebooks. Methods \n        for labelled variables add value labels support to base R functions and to \n        some functions from other packages. Additionally, the package brings \n        popular data transformation functions from 'SPSS' Statistics and 'Excel': \n        'RECODE', 'COUNT', 'COUNTIF', 'VLOOKUP' and etc. \n        These functions are very useful for data processing in marketing research \n        surveys. Package intended to help people to move data \n        processing from 'Excel' and 'SPSS' to R.",
    "version": "0.11.7",
    "maintainer": "Gregory Demin <gdemin@gmail.com>",
    "author": "Gregory Demin [aut, cre],\n  Sebastian Jeworutzki [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2671-5253>),\n  Dan Chaltiel [ctb],\n  John Williams [ctb],\n  Tom Elliott [ctb]",
    "url": "https://gdemin.github.io/expss/",
    "bug_reports": "https://github.com/gdemin/expss/issues",
    "repository": "https://cran.r-project.org/package=expss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "expss Tables, Labels and Some Useful Functions from Spreadsheets and\n'SPSS' Statistics Package computes and displays tables with support for 'SPSS'-style \n        labels, multiple and nested banners, weights, multiple-response variables \n        and significance testing. There are facilities for nice output of tables \n        in 'knitr', 'Shiny', '*.xlsx' files, R and 'Jupyter' notebooks. Methods \n        for labelled variables add value labels support to base R functions and to \n        some functions from other packages. Additionally, the package brings \n        popular data transformation functions from 'SPSS' Statistics and 'Excel': \n        'RECODE', 'COUNT', 'COUNTIF', 'VLOOKUP' and etc. \n        These functions are very useful for data processing in marketing research \n        surveys. Package intended to help people to move data \n        processing from 'Excel' and 'SPSS' to R.  "
  },
  {
    "id": 12093,
    "package_name": "exuber",
    "title": "Econometric Analysis of Explosive Time Series",
    "description": "Testing for and dating periods of explosive\n    dynamics (exuberance) in time series using the univariate and panel\n    recursive unit root tests proposed by Phillips et al. (2015)\n    <doi:10.1111/iere.12132> and Pavlidis et al. (2016)\n    <doi:10.1007/s11146-015-9531-2>.The recursive least-squares\n    algorithm utilizes the matrix inversion lemma to avoid matrix\n    inversion which results in significant speed improvements. Simulation\n    of a variety of periodically-collapsing bubble processes. Details can be \n    found in Vasilopoulos et al. (2022) <doi:10.18637/jss.v103.i10>.",
    "version": "1.1.0",
    "maintainer": "Kostas Vasilopoulos <k.vasilopoulo@gmail.com>",
    "author": "Kostas Vasilopoulos [cre, aut],\n  Efthymios Pavlidis [aut],\n  Enrique Mart\u00ednez-Garc\u00eda [aut],\n  Simon Spavound [aut]",
    "url": "https://kvasilopoulos.github.io/exuber/,\nhttps://github.com/kvasilopoulos/exuber",
    "bug_reports": "https://github.com/kvasilopoulos/exuber/issues",
    "repository": "https://cran.r-project.org/package=exuber",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "exuber Econometric Analysis of Explosive Time Series Testing for and dating periods of explosive\n    dynamics (exuberance) in time series using the univariate and panel\n    recursive unit root tests proposed by Phillips et al. (2015)\n    <doi:10.1111/iere.12132> and Pavlidis et al. (2016)\n    <doi:10.1007/s11146-015-9531-2>.The recursive least-squares\n    algorithm utilizes the matrix inversion lemma to avoid matrix\n    inversion which results in significant speed improvements. Simulation\n    of a variety of periodically-collapsing bubble processes. Details can be \n    found in Vasilopoulos et al. (2022) <doi:10.18637/jss.v103.i10>.  "
  },
  {
    "id": 12106,
    "package_name": "ezCutoffs",
    "title": "Fit Measure Cutoffs in SEM",
    "description": "Calculate cutoff values for model fit measures used in structural equation modeling (SEM) by simulating and testing data sets (cf. Hu & Bentler, 1999 <doi:10.1080/10705519909540118>) with the same parameters (population model, number of observations, etc.) as the model under consideration.",
    "version": "1.0.2",
    "maintainer": "Bjarne Schmalbach <bjarne.schmalbach@gmail.com>",
    "author": "Bjarne Schmalbach [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6853-412X>),\n  Julien Patrick Irmer [aut],\n  Martin Schultze [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ezCutoffs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ezCutoffs Fit Measure Cutoffs in SEM Calculate cutoff values for model fit measures used in structural equation modeling (SEM) by simulating and testing data sets (cf. Hu & Bentler, 1999 <doi:10.1080/10705519909540118>) with the same parameters (population model, number of observations, etc.) as the model under consideration.  "
  },
  {
    "id": 12115,
    "package_name": "fANCOVA",
    "title": "Nonparametric Analysis of Covariance",
    "description": "A collection of R functions to perform nonparametric \n    analysis of covariance for regression curves or surfaces. \n    Testing the equality or parallelism of nonparametric curves \n    or surfaces is equivalent to analysis of variance (ANOVA) or \n    analysis of covariance (ANCOVA) for one-sample functional data. \n    Three different testing methods are available in the package, \n    including one based on L-2 distance, one based on an ANOVA \n    statistic, and one based on variance estimators.",
    "version": "0.6-1",
    "maintainer": "Xiaofeng Wang <wangx6@ccf.org>",
    "author": "Xiaofeng Wang [aut, cre],\n  Xinge Ji [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fANCOVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fANCOVA Nonparametric Analysis of Covariance A collection of R functions to perform nonparametric \n    analysis of covariance for regression curves or surfaces. \n    Testing the equality or parallelism of nonparametric curves \n    or surfaces is equivalent to analysis of variance (ANOVA) or \n    analysis of covariance (ANCOVA) for one-sample functional data. \n    Three different testing methods are available in the package, \n    including one based on L-2 distance, one based on an ANOVA \n    statistic, and one based on variance estimators.  "
  },
  {
    "id": 12116,
    "package_name": "fBasics",
    "title": "Rmetrics - Markets and Basic Statistics",
    "description": "Provides a collection of functions to \n    explore and to investigate basic properties of financial returns \n    and related quantities.\n    The covered fields include techniques of explorative data analysis\n    and the investigation of distributional properties, including\n    parameter estimation and hypothesis testing. Even more there are\n    several utility functions for data handling and management.",
    "version": "4052.98",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "author": "Diethelm Wuertz [aut] (original code),\n  Tobias Setz [aut],\n  Yohan Chalabi [aut],\n  Martin Maechler [ctb] (ORCID: <https://orcid.org/0000-0002-8685-9910>),\n  CRAN Team [ctb],\n  Georgi N. Boshnakov [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-2839-346X>)",
    "url": "https://geobosh.github.io/fBasicsDoc/ (doc),\nhttps://r-forge.r-project.org/scm/viewvc.php/pkg/fBasics/?root=rmetrics\n(devel), https://www.rmetrics.org",
    "bug_reports": "https://r-forge.r-project.org/projects/rmetrics",
    "repository": "https://cran.r-project.org/package=fBasics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fBasics Rmetrics - Markets and Basic Statistics Provides a collection of functions to \n    explore and to investigate basic properties of financial returns \n    and related quantities.\n    The covered fields include techniques of explorative data analysis\n    and the investigation of distributional properties, including\n    parameter estimation and hypothesis testing. Even more there are\n    several utility functions for data handling and management.  "
  },
  {
    "id": 12121,
    "package_name": "fEGarch",
    "title": "SM/LM EGARCH & GARCH, VaR/ES Backtesting & Dual LM Extensions",
    "description": "Implement and fit a variety of short-memory (SM) and long-memory\n  (LM) models from a very broad family of exponential generalized autoregressive\n  conditional heteroskedasticity (EGARCH) models, such as a MEGARCH (modified\n  EGARCH), FIEGARCH (fractionally integrated EGARCH), FIMLog-GARCH (fractionally\n  integrated modulus Log-GARCH), and more. The FIMLog-GARCH as part of the\n  EGARCH family is discussed in Feng et al. (2023)\n  <https://econpapers.repec.org/paper/pdnciepap/156.htm>. For convenience and\n  the purpose of comparison, a variety of other popular SM and LM GARCH-type\n  models, like an APARCH model, a fractionally integrated\n  APARCH (FIAPARCH) model, standard GARCH and fractionally integrated GARCH\n  (FIGARCH) models, GJR-GARCH and FIGJR-GARCH models, TGARCH and FITGARCH\n  models, are implemented as well as dual models with simultaneous modelling of\n  the mean, including dual long-memory models with a fractionally integrated\n  autoregressive moving average (FARIMA) model in the mean and a long-memory\n  model in the variance, and semiparametric volatility model extensions.\n  Parametric models and parametric model parts are fitted through\n  quasi-maximum-likelihood estimation.\n  Furthermore, common forecasting and backtesting functions for value-at-risk\n  (VaR) and expected shortfall (ES) based on the package's models are provided.",
    "version": "1.0.3",
    "maintainer": "Dominik Schulz <dominik.schulz@uni-paderborn.de>",
    "author": "Dominik Schulz [aut, cre] (Paderborn University, Germany),\n  Yuanhua Feng [aut] (Paderborn University, Germany),\n  Christian Peitz [aut] (Financial Intelligence Unit (German Government)),\n  Oliver Kojo Ayensu [aut] (Paderborn University, Germany),\n  Thomas Gries [ctb] (Paderborn University, Germany),\n  Sikandar Siddiqui [ctb] (Deloitte Audit Analytics GmbH, Frankfurt,\n    Germany),\n  Shujie Li [ctb] (Paderborn University, Germany)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fEGarch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fEGarch SM/LM EGARCH & GARCH, VaR/ES Backtesting & Dual LM Extensions Implement and fit a variety of short-memory (SM) and long-memory\n  (LM) models from a very broad family of exponential generalized autoregressive\n  conditional heteroskedasticity (EGARCH) models, such as a MEGARCH (modified\n  EGARCH), FIEGARCH (fractionally integrated EGARCH), FIMLog-GARCH (fractionally\n  integrated modulus Log-GARCH), and more. The FIMLog-GARCH as part of the\n  EGARCH family is discussed in Feng et al. (2023)\n  <https://econpapers.repec.org/paper/pdnciepap/156.htm>. For convenience and\n  the purpose of comparison, a variety of other popular SM and LM GARCH-type\n  models, like an APARCH model, a fractionally integrated\n  APARCH (FIAPARCH) model, standard GARCH and fractionally integrated GARCH\n  (FIGARCH) models, GJR-GARCH and FIGJR-GARCH models, TGARCH and FITGARCH\n  models, are implemented as well as dual models with simultaneous modelling of\n  the mean, including dual long-memory models with a fractionally integrated\n  autoregressive moving average (FARIMA) model in the mean and a long-memory\n  model in the variance, and semiparametric volatility model extensions.\n  Parametric models and parametric model parts are fitted through\n  quasi-maximum-likelihood estimation.\n  Furthermore, common forecasting and backtesting functions for value-at-risk\n  (VaR) and expected shortfall (ES) based on the package's models are provided.  "
  },
  {
    "id": 12127,
    "package_name": "fNonlinear",
    "title": "Rmetrics - Nonlinear and Chaotic Time Series Modelling",
    "description": "Provides a collection of functions for testing various aspects of\n\tunivariate time series including independence and neglected\n\tnonlinearities. Further provides functions to investigate the chaotic\n\tbehavior of time series processes and to simulate different types of chaotic\n\ttime series maps.",
    "version": "4041.82",
    "maintainer": "Paul Smith <paul@waternumbers.co.uk>",
    "author": "Diethelm Wuertz [aut],\n  Tobias Setz [aut],\n  Yohan Chalabi [aut],\n  Paul Smith [cre]",
    "url": "https://www.rmetrics.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fNonlinear",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fNonlinear Rmetrics - Nonlinear and Chaotic Time Series Modelling Provides a collection of functions for testing various aspects of\n\tunivariate time series including independence and neglected\n\tnonlinearities. Further provides functions to investigate the chaotic\n\tbehavior of time series processes and to simulate different types of chaotic\n\ttime series maps.  "
  },
  {
    "id": 12128,
    "package_name": "fPASS",
    "title": "Power and Sample Size for Projection Test under Repeated\nMeasures",
    "description": "Computes the power and sample size (PASS) required to test for the\n    difference in the mean function between two groups under a repeatedly measured longitudinal \n    or sparse functional design. See the manuscript by Koner and Luo (2023) <https://salilkoner.github.io/assets/PASS_manuscript.pdf> \n    for details of the PASS formula and computational details. The details of the testing\n    procedure for univariate and multivariate response are presented in\n    Wang (2021) <doi:10.1214/21-EJS1802> and Koner and Luo (2023) \n    <arXiv:2302.05612> respectively. ",
    "version": "1.0.0",
    "maintainer": "Salil Koner <salil.koner@duke.edu>",
    "author": "Salil Koner [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1952-4210>),\n  Sheng Luo [ctb, fnd]",
    "url": "https://github.com/SalilKoner/fPASS",
    "bug_reports": "https://github.com/SalilKoner/fPASS/issues",
    "repository": "https://cran.r-project.org/package=fPASS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fPASS Power and Sample Size for Projection Test under Repeated\nMeasures Computes the power and sample size (PASS) required to test for the\n    difference in the mean function between two groups under a repeatedly measured longitudinal \n    or sparse functional design. See the manuscript by Koner and Luo (2023) <https://salilkoner.github.io/assets/PASS_manuscript.pdf> \n    for details of the PASS formula and computational details. The details of the testing\n    procedure for univariate and multivariate response are presented in\n    Wang (2021) <doi:10.1214/21-EJS1802> and Koner and Luo (2023) \n    <arXiv:2302.05612> respectively.   "
  },
  {
    "id": 12152,
    "package_name": "facmodTS",
    "title": "Time Series Factor Models for Asset Returns",
    "description": "Supports teaching methods of estimating and testing time series\n    factor models for use in robust portfolio construction and analysis. Unique\n    in providing not only classical least squares, but also modern robust model\n    fitting methods which are not much influenced by outliers. Includes\n    returns and risk decompositions, with user choice of  standard deviation,\n    value-at-risk, and expected shortfall risk measures. \"Robust Statistics\n    Theory and Methods (with R)\", R. A. Maronna, R. D. Martin, V. J. Yohai, \n    M. Salibian-Barrera (2019) <doi:10.1002/9781119214656>.",
    "version": "1.0",
    "maintainer": "Doug Martin <martinrd3d@gmail.com>",
    "author": "Doug Martin [cre, aut],\n  Eric Zivot [aut],\n  Sangeetha Srinivasan [aut],\n  Avinash Acharya [ctb],\n  Yi-An Chen [ctb],\n  Kirk Li [ctb],\n  Lingjie Yi [ctb],\n  Justin Shea [ctb],\n  Mido Shammaa [ctb],\n  Jon Spinney [ctb]",
    "url": "https://github.com/robustport/facmodTS",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=facmodTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "facmodTS Time Series Factor Models for Asset Returns Supports teaching methods of estimating and testing time series\n    factor models for use in robust portfolio construction and analysis. Unique\n    in providing not only classical least squares, but also modern robust model\n    fitting methods which are not much influenced by outliers. Includes\n    returns and risk decompositions, with user choice of  standard deviation,\n    value-at-risk, and expected shortfall risk measures. \"Robust Statistics\n    Theory and Methods (with R)\", R. A. Maronna, R. D. Martin, V. J. Yohai, \n    M. Salibian-Barrera (2019) <doi:10.1002/9781119214656>.  "
  },
  {
    "id": 12161,
    "package_name": "factorial2x2",
    "title": "Design and Analysis of a 2x2 Factorial Trial",
    "description": "Used for the design and analysis of a 2x2 factorial trial for\n    a time-to-event endpoint.  It performs power calculations and significance\n    testing as well as providing estimates of the relevant hazard ratios and the \n    corresponding 95% confidence intervals.  Important reference papers include\n    Slud EV. (1994) <https://www.ncbi.nlm.nih.gov/pubmed/8086609>\n    Lin DY, Gong J, Gallo P, Bunn PH, Couper D. (2016) <DOI:10.1111/biom.12507>\n    Leifer ES, Troendle JF, Kolecki A, Follmann DA. (2020)\n    <https://github.com/EricSLeifer/factorial2x2/blob/master/Leifer%20et%20al.%20paper.pdf>.",
    "version": "0.2.0",
    "maintainer": "Eric Leifer <Eric.Leifer@gmail.com>",
    "author": "Eric Leifer and James Troendle   ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=factorial2x2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "factorial2x2 Design and Analysis of a 2x2 Factorial Trial Used for the design and analysis of a 2x2 factorial trial for\n    a time-to-event endpoint.  It performs power calculations and significance\n    testing as well as providing estimates of the relevant hazard ratios and the \n    corresponding 95% confidence intervals.  Important reference papers include\n    Slud EV. (1994) <https://www.ncbi.nlm.nih.gov/pubmed/8086609>\n    Lin DY, Gong J, Gallo P, Bunn PH, Couper D. (2016) <DOI:10.1111/biom.12507>\n    Leifer ES, Troendle JF, Kolecki A, Follmann DA. (2020)\n    <https://github.com/EricSLeifer/factorial2x2/blob/master/Leifer%20et%20al.%20paper.pdf>.  "
  },
  {
    "id": 12173,
    "package_name": "fairadapt",
    "title": "Fair Data Adaptation with Quantile Preservation",
    "description": "An implementation of the fair data adaptation with quantile\n    preservation described in Plecko & Meinshausen (JMLR 2020, 21(242), 1-44).\n    The adaptation procedure uses the specified causal graph to pre-process the\n    given training and testing data in such a way to remove the bias caused by\n    the protected attribute. The procedure uses tree ensembles for quantile\n    regression. Instructions for using the methods are further elaborated in \n    the corresponding JSS manuscript, see <doi:10.18637/jss.v110.i04>.",
    "version": "1.0.0",
    "maintainer": "Drago Plecko <www.plecko@gmail.com>",
    "author": "Drago Plecko [aut, cre],\n  Nicolas Bennett [aut]",
    "url": "https://github.com/dplecko/fairadapt",
    "bug_reports": "https://github.com/dplecko/fairadapt/issues",
    "repository": "https://cran.r-project.org/package=fairadapt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fairadapt Fair Data Adaptation with Quantile Preservation An implementation of the fair data adaptation with quantile\n    preservation described in Plecko & Meinshausen (JMLR 2020, 21(242), 1-44).\n    The adaptation procedure uses the specified causal graph to pre-process the\n    given training and testing data in such a way to remove the bias caused by\n    the protected attribute. The procedure uses tree ensembles for quantile\n    regression. Instructions for using the methods are further elaborated in \n    the corresponding JSS manuscript, see <doi:10.18637/jss.v110.i04>.  "
  },
  {
    "id": 12186,
    "package_name": "familial",
    "title": "Statistical Tests of Familial Hypotheses",
    "description": "Provides functionality for testing familial hypotheses. Supports testing centers \n    belonging to the Huber family. Testing is carried out using the Bayesian bootstrap. One- and \n    two-sample tests are supported, as are directional tests. Methods for visualizing output are \n    provided.",
    "version": "1.0.7",
    "maintainer": "Ryan Thompson <ryan.thompson-1@uts.edu.au>",
    "author": "Ryan Thompson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9002-0448>)",
    "url": "https://github.com/ryan-thompson/familial",
    "bug_reports": "https://github.com/ryan-thompson/familial/issues",
    "repository": "https://cran.r-project.org/package=familial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "familial Statistical Tests of Familial Hypotheses Provides functionality for testing familial hypotheses. Supports testing centers \n    belonging to the Huber family. Testing is carried out using the Bayesian bootstrap. One- and \n    two-sample tests are supported, as are directional tests. Methods for visualizing output are \n    provided.  "
  },
  {
    "id": 12262,
    "package_name": "fastrerandomize",
    "title": "Hardware-Accelerated Rerandomization for Improved Balance",
    "description": "Provides hardware-accelerated tools for performing rerandomization\n    and randomization testing in experimental research. Using a 'JAX' backend, the\n    package enables exact rerandomization inference even for large experiments\n    with hundreds of billions of possible randomizations. Key functionalities\n    include generating pools of acceptable rerandomizations based on covariate\n    balance, conducting exact randomization tests, and performing pre-analysis\n    evaluations to determine optimal rerandomization acceptance thresholds. The\n    package supports various hardware acceleration frameworks including 'CPU',\n    'CUDA', and 'METAL', making it versatile across accelerated computing environments. This\n    allows researchers to efficiently implement stringent rerandomization designs and\n    conduct valid inference even with large sample sizes. The package is partly based on Jerzak and Goldstein (2023) <doi:10.48550/arXiv.2310.00861>. ",
    "version": "0.2",
    "maintainer": "Connor Jerzak <connor.jerzak@gmail.com>",
    "author": "Fucheng Warren Zhu [aut] (ORCID:\n    <https://orcid.org/0009-0001-5692-7572>),\n  Aniket Sachin Kamat [aut] (ORCID:\n    <https://orcid.org/0009-0003-6411-1084>),\n  Connor Jerzak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1914-8905>),\n  Rebecca Goldstein [aut] (ORCID:\n    <https://orcid.org/0000-0002-9944-8440>)",
    "url": "https://github.com/cjerzak/fastrerandomize-software",
    "bug_reports": "https://github.com/cjerzak/fastrerandomize-software/issues",
    "repository": "https://cran.r-project.org/package=fastrerandomize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastrerandomize Hardware-Accelerated Rerandomization for Improved Balance Provides hardware-accelerated tools for performing rerandomization\n    and randomization testing in experimental research. Using a 'JAX' backend, the\n    package enables exact rerandomization inference even for large experiments\n    with hundreds of billions of possible randomizations. Key functionalities\n    include generating pools of acceptable rerandomizations based on covariate\n    balance, conducting exact randomization tests, and performing pre-analysis\n    evaluations to determine optimal rerandomization acceptance thresholds. The\n    package supports various hardware acceleration frameworks including 'CPU',\n    'CUDA', and 'METAL', making it versatile across accelerated computing environments. This\n    allows researchers to efficiently implement stringent rerandomization designs and\n    conduct valid inference even with large sample sizes. The package is partly based on Jerzak and Goldstein (2023) <doi:10.48550/arXiv.2310.00861>.   "
  },
  {
    "id": 12281,
    "package_name": "fbnet",
    "title": "Forensic Bayesian Networks",
    "description": "Open-source package for computing likelihood ratios in kinship testing and human identification cases. It has the core function of the software GENis, developed by Fundaci\u00f3n Sadosky. It relies on a Bayesian Networks framework and is particularly well suited to efficiently perform large-size queries against databases of missing individuals.",
    "version": "1.0.4",
    "maintainer": "Franco Marsico <franco.lmarsico@gmail.com>",
    "author": "Franco Marsico [aut, cre],\n  Ariel Chernomoretz [aut]",
    "url": "https://marsicofl.github.io/fbnet/,\nhttps://github.com/MarsicoFL/fbnet",
    "bug_reports": "https://github.com/MarsicoFL/fbnet/issues",
    "repository": "https://cran.r-project.org/package=fbnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fbnet Forensic Bayesian Networks Open-source package for computing likelihood ratios in kinship testing and human identification cases. It has the core function of the software GENis, developed by Fundaci\u00f3n Sadosky. It relies on a Bayesian Networks framework and is particularly well suited to efficiently perform large-size queries against databases of missing individuals.  "
  },
  {
    "id": 12283,
    "package_name": "fbst",
    "title": "The Full Bayesian Evidence Test, Full Bayesian Significance Test\nand the e-Value",
    "description": "Provides access to a range of functions for computing and visualizing the Full Bayesian Significance Test (FBST) and the e-value for testing a sharp hypothesis against its alternative, and the Full Bayesian Evidence Test (FBET) and the (generalized) Bayesian evidence value for testing a composite (or interval) hypothesis against its alternative. The methods are widely applicable as long as a posterior MCMC sample is available.",
    "version": "2.2",
    "maintainer": "Riko Kelter <riko_k@gmx.de>",
    "author": "Riko Kelter",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fbst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fbst The Full Bayesian Evidence Test, Full Bayesian Significance Test\nand the e-Value Provides access to a range of functions for computing and visualizing the Full Bayesian Significance Test (FBST) and the e-value for testing a sharp hypothesis against its alternative, and the Full Bayesian Evidence Test (FBET) and the (generalized) Bayesian evidence value for testing a composite (or interval) hypothesis against its alternative. The methods are widely applicable as long as a posterior MCMC sample is available.  "
  },
  {
    "id": 12298,
    "package_name": "fdANOVA",
    "title": "Analysis of Variance for Univariate and Multivariate Functional\nData",
    "description": "Performs analysis of variance testing procedures for univariate and multivariate functional data (Cuesta-Albertos and Febrero-Bande (2010) <doi:10.1007/s11749-010-0185-3>, Gorecki and Smaga (2015) <doi:10.1007/s00180-015-0555-0>, Gorecki and Smaga (2017) <doi:10.1080/02664763.2016.1247791>, Zhang et al. (2018) <doi:10.1016/j.csda.2018.05.004>).",
    "version": "0.1.2",
    "maintainer": "Lukasz Smaga <ls@amu.edu.pl>",
    "author": "Tomasz Gorecki, Lukasz Smaga",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fdANOVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdANOVA Analysis of Variance for Univariate and Multivariate Functional\nData Performs analysis of variance testing procedures for univariate and multivariate functional data (Cuesta-Albertos and Febrero-Bande (2010) <doi:10.1007/s11749-010-0185-3>, Gorecki and Smaga (2015) <doi:10.1007/s00180-015-0555-0>, Gorecki and Smaga (2017) <doi:10.1080/02664763.2016.1247791>, Zhang et al. (2018) <doi:10.1016/j.csda.2018.05.004>).  "
  },
  {
    "id": 12299,
    "package_name": "fdWasserstein",
    "title": "Application of Optimal Transport to Functional Data Analysis",
    "description": "These functions were developed to support statistical analysis on functional covariance operators.\n  The package contains functions to:\n  - compute 2-Wasserstein distances between Gaussian Processes as in\n    Masarotto, Panaretos & Zemel (2019) <doi:10.1007/s13171-018-0130-1>;\n  - compute the Wasserstein barycenter (Frechet mean) as in Masarotto,\n    Panaretos & Zemel (2019) <doi:10.1007/s13171-018-0130-1>;\n  - perform analysis of variance testing procedures for functional\n    covariances and tangent space principal component analysis of\n    covariance operators as in Masarotto, Panaretos & Zemel (2022)\n    <arXiv:2212.04797>.\n  - perform a soft-clustering based on the Wasserstein distance where\n    functional data are classified based on their covariance structure\n    as in Masarotto & Masarotto (2023) <doi:10.1111/sjos.12692>.",
    "version": "1.0",
    "maintainer": "Valentina Masarotto <v.masarotto@math.leidenuniv.nl>",
    "author": "Valentina Masarotto [aut, cph, cre],\n  Guido Masarotto [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fdWasserstein",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdWasserstein Application of Optimal Transport to Functional Data Analysis These functions were developed to support statistical analysis on functional covariance operators.\n  The package contains functions to:\n  - compute 2-Wasserstein distances between Gaussian Processes as in\n    Masarotto, Panaretos & Zemel (2019) <doi:10.1007/s13171-018-0130-1>;\n  - compute the Wasserstein barycenter (Frechet mean) as in Masarotto,\n    Panaretos & Zemel (2019) <doi:10.1007/s13171-018-0130-1>;\n  - perform analysis of variance testing procedures for functional\n    covariances and tangent space principal component analysis of\n    covariance operators as in Masarotto, Panaretos & Zemel (2022)\n    <arXiv:2212.04797>.\n  - perform a soft-clustering based on the Wasserstein distance where\n    functional data are classified based on their covariance structure\n    as in Masarotto & Masarotto (2023) <doi:10.1111/sjos.12692>.  "
  },
  {
    "id": 12315,
    "package_name": "fdatest",
    "title": "Interval Testing Procedure for Functional Data",
    "description": "Implementation of the Interval Testing Procedure for functional data in different frameworks (i.e., one or two-population frameworks, functional linear models) by means of different basis expansions (i.e., B-spline, Fourier, and phase-amplitude Fourier). The current version of the package requires functional data evaluated on a uniform grid; it automatically projects each function on a chosen functional basis; it performs the entire family of multivariate tests; and, finally, it provides the matrix of the p-values of the previous tests and the vector of the corrected p-values. The functional basis, the coupled or uncoupled scenario, and the kind of test can be chosen by the user. The package provides also a plotting function creating a graphical output of the procedure: the p-value heat-map, the plot of the corrected p-values, and the plot of the functional data.",
    "version": "2.1.1",
    "maintainer": "Alessia Pini <alessia.pini@polimi.it>",
    "author": "Alessia Pini, Simone Vantini",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fdatest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdatest Interval Testing Procedure for Functional Data Implementation of the Interval Testing Procedure for functional data in different frameworks (i.e., one or two-population frameworks, functional linear models) by means of different basis expansions (i.e., B-spline, Fourier, and phase-amplitude Fourier). The current version of the package requires functional data evaluated on a uniform grid; it automatically projects each function on a chosen functional basis; it performs the entire family of multivariate tests; and, finally, it provides the matrix of the p-values of the previous tests and the vector of the corrected p-values. The functional basis, the coupled or uncoupled scenario, and the kind of test can be chosen by the user. The package provides also a plotting function creating a graphical output of the procedure: the p-value heat-map, the plot of the corrected p-values, and the plot of the functional data.  "
  },
  {
    "id": 12321,
    "package_name": "fdrDiscreteNull",
    "title": "False Discovery Rate Procedures Under Discrete and Heterogeneous\nNull Distributions",
    "description": "It is known that current false discovery rate (FDR) procedures can be very conservative when applied to multiple testing in the discrete paradigm where p-values (and test statistics) have discrete and heterogeneous null distributions. This package implements more powerful weighted or adaptive FDR procedures for FDR control and estimation in the discrete paradigm. The package takes in the original data set rather than just the p-values in order to carry out the adjustments for discreteness and heterogeneity of p-value distributions. The package implements methods for two types of test statistics and their p-values: (a) binomial test on if two independent Poisson distributions have the same means, (b) Fisher's exact test on if the conditional distribution is the same as the marginal distribution for two binomial distributions, or on if two independent binomial distributions have the same probabilities of success.",
    "version": "1.4",
    "maintainer": "Xiongzhi Chen <xiongzhi.chen@wsu.edu>",
    "author": "Xiongzhi Chen and Rebecca W. Doerge <rwdoerge@andrew.cmu.edu>",
    "url": "http://math.wsu.edu/faculty/xchen/welcome.php",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fdrDiscreteNull",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fdrDiscreteNull False Discovery Rate Procedures Under Discrete and Heterogeneous\nNull Distributions It is known that current false discovery rate (FDR) procedures can be very conservative when applied to multiple testing in the discrete paradigm where p-values (and test statistics) have discrete and heterogeneous null distributions. This package implements more powerful weighted or adaptive FDR procedures for FDR control and estimation in the discrete paradigm. The package takes in the original data set rather than just the p-values in order to carry out the adjustments for discreteness and heterogeneity of p-value distributions. The package implements methods for two types of test statistics and their p-values: (a) binomial test on if two independent Poisson distributions have the same means, (b) Fisher's exact test on if the conditional distribution is the same as the marginal distribution for two binomial distributions, or on if two independent binomial distributions have the same probabilities of success.  "
  },
  {
    "id": 12358,
    "package_name": "ffmanova",
    "title": "Fifty-Fifty MANOVA",
    "description": "General linear modeling with multiple responses (MANCOVA). An overall p-value for each model term is calculated by the 50-50 MANOVA method by Langsrud (2002) <doi:10.1111/1467-9884.00320>, which handles collinear responses. Rotation testing, described by Langsrud (2005) <doi:10.1007/s11222-005-4789-5>, is used to compute adjusted single response p-values according to familywise error rates and false discovery rates (FDR). The approach to FDR is described in the appendix of Moen et al. (2005) <doi:10.1128/AEM.71.4.2086-2094.2005>. Unbalanced designs are handled by Type II sums of squares as argued in Langsrud (2003) <doi:10.1023/A:1023260610025>. Furthermore, the Type II philosophy is extended to continuous design variables as described in Langsrud et al. (2007) <doi:10.1080/02664760701594246>. This means that the method is invariant to scale changes and that common pitfalls are avoided.",
    "version": "1.1.2",
    "maintainer": "\u00d8yvind Langsrud <oyl@ssb.no>",
    "author": "\u00d8yvind Langsrud [aut, cre],\n  Bj\u00f8rn-Helge Mevik [aut]",
    "url": "https://github.com/olangsrud/ffmanova",
    "bug_reports": "https://github.com/olangsrud/ffmanova/issues",
    "repository": "https://cran.r-project.org/package=ffmanova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ffmanova Fifty-Fifty MANOVA General linear modeling with multiple responses (MANCOVA). An overall p-value for each model term is calculated by the 50-50 MANOVA method by Langsrud (2002) <doi:10.1111/1467-9884.00320>, which handles collinear responses. Rotation testing, described by Langsrud (2005) <doi:10.1007/s11222-005-4789-5>, is used to compute adjusted single response p-values according to familywise error rates and false discovery rates (FDR). The approach to FDR is described in the appendix of Moen et al. (2005) <doi:10.1128/AEM.71.4.2086-2094.2005>. Unbalanced designs are handled by Type II sums of squares as argued in Langsrud (2003) <doi:10.1023/A:1023260610025>. Furthermore, the Type II philosophy is extended to continuous design variables as described in Langsrud et al. (2007) <doi:10.1080/02664760701594246>. This means that the method is invariant to scale changes and that common pitfalls are avoided.  "
  },
  {
    "id": 12359,
    "package_name": "ffp",
    "title": "Fully Flexible Probabilities for Stress Testing and Portfolio\nConstruction",
    "description": "Implements numerical entropy-pooling for portfolio construction and \n    scenario analysis as described in Meucci, Attilio (2008) and Meucci, Attilio (2010) \n    <doi:10.2139/ssrn.1696802>.",
    "version": "0.2.2",
    "maintainer": "Bernardo Reckziegel <bernardo_cse@hotmail.com>",
    "author": "Bernardo Reckziegel [aut, cre]",
    "url": "https://github.com/Reckziegel/FFP",
    "bug_reports": "https://github.com/Reckziegel/FFP/issues",
    "repository": "https://cran.r-project.org/package=ffp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ffp Fully Flexible Probabilities for Stress Testing and Portfolio\nConstruction Implements numerical entropy-pooling for portfolio construction and \n    scenario analysis as described in Meucci, Attilio (2008) and Meucci, Attilio (2010) \n    <doi:10.2139/ssrn.1696802>.  "
  },
  {
    "id": 12452,
    "package_name": "fitdistcp",
    "title": "Distribution Fitting with Calibrating Priors for Commonly Used\nDistributions",
    "description": "Generates predictive distributions based on calibrating priors\n    for various commonly used statistical models, including models with \n    predictors. Routines for densities, probabilities, quantiles, random\n    deviates and the parameter posterior are provided. The predictions are\n    generated from the Bayesian prediction integral, with priors chosen to\n    give good reliability (also known as calibration). For homogeneous models,\n    the prior is set to the right Haar prior, giving predictions which are\n    exactly reliable. As a result, in repeated testing, the frequencies of\n    out-of-sample outcomes and the probabilities from the predictions agree.\n    For other models, the prior is chosen to give good reliability. Where\n    possible, the Bayesian prediction integral is solved exactly. Where exact\n    solutions are not possible, the Bayesian prediction integral is solved\n    using the Datta-Mukerjee-Ghosh-Sweeting (DMGS) asymptotic expansion.\n    Optionally, the prediction integral can also be solved using posterior\n    samples generated using Paul Northrop's ratio of uniforms sampling package\n    ('rust'). Results are also generated based on maximum likelihood, for\n    comparison purposes. Various model selection diagnostics and testing\n    routines are included. Based on \"Reducing reliability bias in assessments\n    of extreme weather risk using calibrating priors\", Jewson, S., Sweeting, T.\n    and Jewson, L. (2024); <doi:10.5194/ascmo-11-1-2025>.",
    "version": "0.2.3",
    "maintainer": "Stephen Jewson <stephen.jewson@gmail.com>",
    "author": "Stephen Jewson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6011-6262>)",
    "url": "https://www.fitdistcp.info",
    "bug_reports": "https://github.com/stephenjewson/fitdistcp/issues",
    "repository": "https://cran.r-project.org/package=fitdistcp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fitdistcp Distribution Fitting with Calibrating Priors for Commonly Used\nDistributions Generates predictive distributions based on calibrating priors\n    for various commonly used statistical models, including models with \n    predictors. Routines for densities, probabilities, quantiles, random\n    deviates and the parameter posterior are provided. The predictions are\n    generated from the Bayesian prediction integral, with priors chosen to\n    give good reliability (also known as calibration). For homogeneous models,\n    the prior is set to the right Haar prior, giving predictions which are\n    exactly reliable. As a result, in repeated testing, the frequencies of\n    out-of-sample outcomes and the probabilities from the predictions agree.\n    For other models, the prior is chosen to give good reliability. Where\n    possible, the Bayesian prediction integral is solved exactly. Where exact\n    solutions are not possible, the Bayesian prediction integral is solved\n    using the Datta-Mukerjee-Ghosh-Sweeting (DMGS) asymptotic expansion.\n    Optionally, the prediction integral can also be solved using posterior\n    samples generated using Paul Northrop's ratio of uniforms sampling package\n    ('rust'). Results are also generated based on maximum likelihood, for\n    comparison purposes. Various model selection diagnostics and testing\n    routines are included. Based on \"Reducing reliability bias in assessments\n    of extreme weather risk using calibrating priors\", Jewson, S., Sweeting, T.\n    and Jewson, L. (2024); <doi:10.5194/ascmo-11-1-2025>.  "
  },
  {
    "id": 12490,
    "package_name": "flex",
    "title": "Fuzzy Linear Squares Estimation with Explicit Formula (FLEX)",
    "description": "The FLEX method, developed by Yoon and Choi (2013) <doi:10.1007/978-3-642-33042-1_21>, performs least squares estimation for fuzzy predictors and outcomes, generating crisp regression coefficients by minimizing the distance between observed and predicted outcomes. It also provides functions for fuzzifying data and inference tasks, including significance testing, fit indices, and confidence interval estimation.",
    "version": "0.1.0",
    "maintainer": "Chaewon Lee <chaewon.lee@unc.edu>",
    "author": "Chaewon Lee [aut, cre],\n  Jin Hee Yoon [ctb]",
    "url": "",
    "bug_reports": "https://github.com/cwlee-quantpsych/flex/issues",
    "repository": "https://cran.r-project.org/package=flex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flex Fuzzy Linear Squares Estimation with Explicit Formula (FLEX) The FLEX method, developed by Yoon and Choi (2013) <doi:10.1007/978-3-642-33042-1_21>, performs least squares estimation for fuzzy predictors and outcomes, generating crisp regression coefficients by minimizing the distance between observed and predicted outcomes. It also provides functions for fuzzifying data and inference tasks, including significance testing, fit indices, and confidence interval estimation.  "
  },
  {
    "id": 12515,
    "package_name": "flip",
    "title": "Multivariate Permutation Tests",
    "description": "It implements many univariate and multivariate permutation (and\n    rotation) tests. Allowed tests: the t one and two samples, ANOVA, linear\n    models, Chi Squared test, rank tests (i.e. Wilcoxon, Mann-Whitney,\n    Kruskal-Wallis), Sign test and Mc Nemar. Test on Linear Models are\n    performed also in presence of covariates (i.e. nuisance parameters).\n    The permutation and the rotation methods to get the null distribution of\n    the test statistics are available. It also implements methods for\n    multiplicity control such as Westfall & Young minP procedure and Closed\n    Testing (Marcus, 1976) and k-FWER. Moreover, it allows to test for fixed\n    effects in mixed effects models.",
    "version": "2.5.1",
    "maintainer": "Livio Finos <livio.finos@unipd.it>",
    "author": "Livio Finos [cre, aut] (ORCID: <https://orcid.org/0000-0003-3181-8078>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=flip",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flip Multivariate Permutation Tests It implements many univariate and multivariate permutation (and\n    rotation) tests. Allowed tests: the t one and two samples, ANOVA, linear\n    models, Chi Squared test, rank tests (i.e. Wilcoxon, Mann-Whitney,\n    Kruskal-Wallis), Sign test and Mc Nemar. Test on Linear Models are\n    performed also in presence of covariates (i.e. nuisance parameters).\n    The permutation and the rotation methods to get the null distribution of\n    the test statistics are available. It also implements methods for\n    multiplicity control such as Westfall & Young minP procedure and Closed\n    Testing (Marcus, 1976) and k-FWER. Moreover, it allows to test for fixed\n    effects in mixed effects models.  "
  },
  {
    "id": 12520,
    "package_name": "flipr",
    "title": "Flexible Inference via Permutations in R",
    "description": "A flexible permutation framework for making \n    inference such as point estimation, confidence \n    intervals or hypothesis testing, on any kind of data, \n    be it univariate, multivariate, or more complex such \n    as network-valued data, topological data, functional \n    data or density-valued data.",
    "version": "0.3.3",
    "maintainer": "Aymeric Stamm <aymeric.stamm@cnrs.fr>",
    "author": "Alessia Pini [aut],\n  Aymeric Stamm [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8725-3654>),\n  Simone Vantini [aut],\n  Juliette Chiapello [ctb]",
    "url": "https://LMJL-Alea.github.io/flipr/,\nhttps://github.com/LMJL-Alea/flipr/",
    "bug_reports": "https://github.com/LMJL-Alea/flipr/issues/",
    "repository": "https://cran.r-project.org/package=flipr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flipr Flexible Inference via Permutations in R A flexible permutation framework for making \n    inference such as point estimation, confidence \n    intervals or hypothesis testing, on any kind of data, \n    be it univariate, multivariate, or more complex such \n    as network-valued data, topological data, functional \n    data or density-valued data.  "
  },
  {
    "id": 12521,
    "package_name": "flipscores",
    "title": "Robust Score Testing in GLMs, by Sign-Flip Contributions",
    "description": "Provides robust tests for testing in GLMs, by sign-flipping score contributions. The tests are robust against overdispersion, heteroscedasticity and, in some cases, ignored nuisance variables. See Hemerik, Goeman and Finos (2020) <doi:10.1111/rssb.12369>.",
    "version": "1.3.2",
    "maintainer": "Livio Finos <livio.finos@unipd.it>",
    "author": "Livio Finos [cre, aut] (ORCID: <https://orcid.org/0000-0003-3181-8078>),\n  Jelle J. Goeman [ctb],\n  Jesse Hemerik [ctb],\n  Riccardo De Santis [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=flipscores",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flipscores Robust Score Testing in GLMs, by Sign-Flip Contributions Provides robust tests for testing in GLMs, by sign-flipping score contributions. The tests are robust against overdispersion, heteroscedasticity and, in some cases, ignored nuisance variables. See Hemerik, Goeman and Finos (2020) <doi:10.1111/rssb.12369>.  "
  },
  {
    "id": 12583,
    "package_name": "foreSIGHT",
    "title": "Systems Insights from Generation of Hydroclimatic Timeseries",
    "description": "A tool to create hydroclimate scenarios, stress test systems and visualize system performance in scenario-neutral climate change impact assessments. Scenario-neutral approaches 'stress-test' the performance of a modelled system by applying a wide range of plausible hydroclimate conditions (see Brown & Wilby (2012) <doi:10.1029/2012EO410001> and Prudhomme et al. (2010) <doi:10.1016/j.jhydrol.2010.06.043>). These approaches allow the identification of hydroclimatic variables that affect the vulnerability of a system to hydroclimate variation and change. This tool enables the generation of perturbed time series using a range of approaches including simple scaling of observed time series (e.g. Culley et al. (2016) <doi:10.1002/2015WR018253>) and stochastic simulation of perturbed time series via an inverse approach (see Guo et al. (2018) <doi:10.1016/j.jhydrol.2016.03.025>). It incorporates 'Richardson-type' weather generator model configurations documented in Richardson (1981) <doi:10.1029/WR017i001p00182>, Richardson and Wright (1984), as well as latent variable type model configurations documented in Bennett et al. (2018) <doi:10.1016/j.jhydrol.2016.12.043>, Rasmussen (2013) <doi:10.1002/wrcr.20164>, Bennett et al. (2019) <doi:10.5194/hess-23-4783-2019> to generate hydroclimate variables on a daily basis (e.g. precipitation, temperature, potential evapotranspiration) and allows a variety of different hydroclimate variable properties, herein called attributes, to be perturbed. Options are included for the easy integration of existing system models both internally in R and externally for seamless 'stress-testing'. A suite of visualization options for the results of a scenario-neutral analysis (e.g. plotting performance spaces and overlaying climate projection information) are also included. Version 1.0 of this package is described in Bennett et al. (2021) <doi:10.1016/j.envsoft.2021.104999>. As further developments in scenario-neutral approaches occur the tool will be updated to incorporate these advances.",
    "version": "2.0.0",
    "maintainer": "David McInerney <david.mcinerney@adelaide.edu.au>",
    "author": "Bree Bennett [aut] (ORCID: <https://orcid.org/0000-0002-2131-088X>),\n  David McInerney [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4876-8281>),\n  Sam Culley [aut] (ORCID: <https://orcid.org/0000-0003-4798-8522>),\n  Anjana Devanand [aut] (ORCID: <https://orcid.org/0000-0001-9422-3894>),\n  Seth Westra [aut] (ORCID: <https://orcid.org/0000-0003-4023-6061>),\n  Danlu Guo [ctb] (ORCID: <https://orcid.org/0000-0003-1083-1214>),\n  Holger Maier [ths] (ORCID: <https://orcid.org/0000-0002-0277-6887>)",
    "url": "",
    "bug_reports": "https://github.com/ClimateAnalytics/foreSIGHT/issues",
    "repository": "https://cran.r-project.org/package=foreSIGHT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "foreSIGHT Systems Insights from Generation of Hydroclimatic Timeseries A tool to create hydroclimate scenarios, stress test systems and visualize system performance in scenario-neutral climate change impact assessments. Scenario-neutral approaches 'stress-test' the performance of a modelled system by applying a wide range of plausible hydroclimate conditions (see Brown & Wilby (2012) <doi:10.1029/2012EO410001> and Prudhomme et al. (2010) <doi:10.1016/j.jhydrol.2010.06.043>). These approaches allow the identification of hydroclimatic variables that affect the vulnerability of a system to hydroclimate variation and change. This tool enables the generation of perturbed time series using a range of approaches including simple scaling of observed time series (e.g. Culley et al. (2016) <doi:10.1002/2015WR018253>) and stochastic simulation of perturbed time series via an inverse approach (see Guo et al. (2018) <doi:10.1016/j.jhydrol.2016.03.025>). It incorporates 'Richardson-type' weather generator model configurations documented in Richardson (1981) <doi:10.1029/WR017i001p00182>, Richardson and Wright (1984), as well as latent variable type model configurations documented in Bennett et al. (2018) <doi:10.1016/j.jhydrol.2016.12.043>, Rasmussen (2013) <doi:10.1002/wrcr.20164>, Bennett et al. (2019) <doi:10.5194/hess-23-4783-2019> to generate hydroclimate variables on a daily basis (e.g. precipitation, temperature, potential evapotranspiration) and allows a variety of different hydroclimate variable properties, herein called attributes, to be perturbed. Options are included for the easy integration of existing system models both internally in R and externally for seamless 'stress-testing'. A suite of visualization options for the results of a scenario-neutral analysis (e.g. plotting performance spaces and overlaying climate projection information) are also included. Version 1.0 of this package is described in Bennett et al. (2021) <doi:10.1016/j.envsoft.2021.104999>. As further developments in scenario-neutral approaches occur the tool will be updated to incorporate these advances.  "
  },
  {
    "id": 12626,
    "package_name": "forrel",
    "title": "Forensic Pedigree Analysis and Relatedness Inference",
    "description": "Forensic applications of pedigree analysis, including\n    likelihood ratios for relationship testing, general relatedness\n    inference, marker simulation, and power analysis. 'forrel' is part of\n    the 'pedsuite', a collection of packages for pedigree analysis,\n    further described in the book 'Pedigree Analysis in R' (Vigeland,\n    2021, ISBN:9780128244302). Several functions deal specifically with\n    power analysis in missing person cases, implementing methods described\n    in Vigeland et al. (2020) <doi:10.1016/j.fsigen.2020.102376>. Data\n    import from the 'Familias' software (Egeland et al. (2000)\n    <doi:10.1016/S0379-0738(00)00147-X>) is supported through the\n    'pedFamilias' package.",
    "version": "1.8.1",
    "maintainer": "Magnus Dehli Vigeland <m.d.vigeland@medisin.uio.no>",
    "author": "Magnus Dehli Vigeland [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9134-4962>),\n  Thore Egeland [ctb]",
    "url": "https://github.com/magnusdv/forrel",
    "bug_reports": "https://github.com/magnusdv/forrel/issues",
    "repository": "https://cran.r-project.org/package=forrel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "forrel Forensic Pedigree Analysis and Relatedness Inference Forensic applications of pedigree analysis, including\n    likelihood ratios for relationship testing, general relatedness\n    inference, marker simulation, and power analysis. 'forrel' is part of\n    the 'pedsuite', a collection of packages for pedigree analysis,\n    further described in the book 'Pedigree Analysis in R' (Vigeland,\n    2021, ISBN:9780128244302). Several functions deal specifically with\n    power analysis in missing person cases, implementing methods described\n    in Vigeland et al. (2020) <doi:10.1016/j.fsigen.2020.102376>. Data\n    import from the 'Familias' software (Egeland et al. (2000)\n    <doi:10.1016/S0379-0738(00)00147-X>) is supported through the\n    'pedFamilias' package.  "
  },
  {
    "id": 12699,
    "package_name": "freqpcr",
    "title": "Estimates Allele Frequency on qPCR DeltaDeltaCq from Bulk\nSamples",
    "description": "Interval estimation of the population allele frequency from qPCR analysis based on the restriction enzyme digestion (RED)-DeltaDeltaCq method (Osakabe et al. 2017, <doi:10.1016/j.pestbp.2017.04.003>), as well as general DeltaDeltaCq analysis. Compatible with the Cq measurement of DNA extracted from multiple individuals at once, so called \"group-testing\", this model assumes that the quantity of DNA extracted from an individual organism follows a gamma distribution. Therefore, the point estimate is robust regarding the uncertainty of the DNA yield.",
    "version": "0.4.0",
    "maintainer": "Masaaki Sudo <masaaki@sudori.info>",
    "author": "Masaaki Sudo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9834-9857>)",
    "url": "https://github.com/sudoms/freqpcr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=freqpcr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "freqpcr Estimates Allele Frequency on qPCR DeltaDeltaCq from Bulk\nSamples Interval estimation of the population allele frequency from qPCR analysis based on the restriction enzyme digestion (RED)-DeltaDeltaCq method (Osakabe et al. 2017, <doi:10.1016/j.pestbp.2017.04.003>), as well as general DeltaDeltaCq analysis. Compatible with the Cq measurement of DNA extracted from multiple individuals at once, so called \"group-testing\", this model assumes that the quantity of DNA extracted from an individual organism follows a gamma distribution. Therefore, the point estimate is robust regarding the uncertainty of the DNA yield.  "
  },
  {
    "id": 12738,
    "package_name": "ftsa",
    "title": "Functional Time Series Analysis",
    "description": "Functions for visualizing, modeling, forecasting and hypothesis testing of functional time series.",
    "version": "6.6",
    "maintainer": "Han Lin Shang <hanlin.shang@mq.edu.au>",
    "author": "Rob Hyndman [aut] (ORCID: <https://orcid.org/0000-0002-2140-5352>),\n  Han Lin Shang [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-1769-6430>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ftsa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ftsa Functional Time Series Analysis Functions for visualizing, modeling, forecasting and hypothesis testing of functional time series.  "
  },
  {
    "id": 12755,
    "package_name": "funStatTest",
    "title": "Statistical Testing for Functional Data",
    "description": "Implementation of two sample comparison procedures based on\n    median-based statistical tests for functional data, introduced in\n    Smida et al (2022) <doi:10.1080/10485252.2022.2064997>.  Other\n    competitive state-of-the-art approaches proposed by Chakraborty and\n    Chaudhuri (2015) <doi:10.1093/biomet/asu072>, Horvath et al (2013)\n    <doi:10.1111/j.1467-9868.2012.01032.x> or Cuevas et al (2004)\n    <doi:10.1016/j.csda.2003.10.021> are also included in the package, as\n    well as procedures to run test result comparisons and power analysis\n    using simulations.",
    "version": "1.0.3",
    "maintainer": "Ghislain Durif <gd.dev@libertymail.net>",
    "author": "Zaineb Smida [aut] (ORCID: <https://orcid.org/0000-0002-9974-299X>),\n  Ghislain Durif [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2567-1401>),\n  Lionel Cucala [aut]",
    "url": "https://plmlab.math.cnrs.fr/gdurif/funStatTest/,https://gdurif.pages.math.cnrs.fr/funStatTest/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=funStatTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "funStatTest Statistical Testing for Functional Data Implementation of two sample comparison procedures based on\n    median-based statistical tests for functional data, introduced in\n    Smida et al (2022) <doi:10.1080/10485252.2022.2064997>.  Other\n    competitive state-of-the-art approaches proposed by Chakraborty and\n    Chaudhuri (2015) <doi:10.1093/biomet/asu072>, Horvath et al (2013)\n    <doi:10.1111/j.1467-9868.2012.01032.x> or Cuevas et al (2004)\n    <doi:10.1016/j.csda.2003.10.021> are also included in the package, as\n    well as procedures to run test result comparisons and power analysis\n    using simulations.  "
  },
  {
    "id": 12809,
    "package_name": "fxregime",
    "title": "Exchange Rate Regime Analysis",
    "description": "Exchange rate regression and structural change tools\n             for estimating, testing, dating, and monitoring\n\t     (de facto) exchange rate regimes.",
    "version": "1.0-4",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Ajay Shah [ctb],\n  Ila Patnaik [ctb],\n  Anmol Sethy [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fxregime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fxregime Exchange Rate Regime Analysis Exchange rate regression and structural change tools\n             for estimating, testing, dating, and monitoring\n\t     (de facto) exchange rate regimes.  "
  },
  {
    "id": 12818,
    "package_name": "gJLS2",
    "title": "A Generalized Joint Location and Scale Framework for Association\nTesting",
    "description": "An update to the Joint Location-Scale (JLS) testing framework that identifies associated SNPs, gene-sets and pathways with main and/or interaction effects on quantitative traits (Soave et al., 2015; <doi:10.1016/j.ajhg.2015.05.015>). The JLS method simultaneously tests the null hypothesis of equal mean and equal variance across genotypes, by aggregating association evidence from the individual location/mean-only and scale/variance-only tests using Fisher's method. The generalized joint location-scale (gJLS) framework has been developed to deal specifically with sample correlation and group uncertainty (Soave and Sun, 2017; <doi:10.1111/biom.12651>). The current release: gJLS2, include additional functionalities that enable analyses of X-chromosome genotype data through novel methods for location (Chen et al., 2021; <doi:10.1002/gepi.22422>) and scale (Deng et al., 2019; <doi:10.1002/gepi.22247>).",
    "version": "0.2.0",
    "maintainer": "Wei Deng <deng@utstat.toronto.edu>",
    "author": "Wei Deng [aut, cre],\n  Lei Sun [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gJLS2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gJLS2 A Generalized Joint Location and Scale Framework for Association\nTesting An update to the Joint Location-Scale (JLS) testing framework that identifies associated SNPs, gene-sets and pathways with main and/or interaction effects on quantitative traits (Soave et al., 2015; <doi:10.1016/j.ajhg.2015.05.015>). The JLS method simultaneously tests the null hypothesis of equal mean and equal variance across genotypes, by aggregating association evidence from the individual location/mean-only and scale/variance-only tests using Fisher's method. The generalized joint location-scale (gJLS) framework has been developed to deal specifically with sample correlation and group uncertainty (Soave and Sun, 2017; <doi:10.1111/biom.12651>). The current release: gJLS2, include additional functionalities that enable analyses of X-chromosome genotype data through novel methods for location (Chen et al., 2021; <doi:10.1002/gepi.22422>) and scale (Deng et al., 2019; <doi:10.1002/gepi.22247>).  "
  },
  {
    "id": 12822,
    "package_name": "gMCPLite",
    "title": "Lightweight Graph Based Multiple Comparison Procedures",
    "description": "A lightweight fork of 'gMCP' with functions for graphical\n    described multiple test procedures introduced in\n    Bretz et al. (2009) <doi:10.1002/sim.3495> and\n    Bretz et al. (2011) <doi:10.1002/bimj.201000239>.\n    Implements a flexible function using 'ggplot2' to create\n    multiplicity graph visualizations.\n    Contains instructions of multiplicity graph and graphical testing for\n    group sequential design, described in\n    Maurer and Bretz (2013) <doi:10.1080/19466315.2013.807748>,\n    with necessary unit testing using 'testthat'.",
    "version": "0.1.6",
    "maintainer": "Nan Xiao <nan.xiao1@merck.com>",
    "author": "Yalin Zhu [aut] (ORCID: <https://orcid.org/0000-0003-3830-8660>),\n  Yilong Zhang [aut],\n  Xuan Deng [aut],\n  Keaven Anderson [aut],\n  Nan Xiao [aut, cre] (ORCID: <https://orcid.org/0000-0002-0250-5673>),\n  Kornelius Rohmeyer [ctb] (gMCP author),\n  Florian Klinglmueller [ctb] (gMCP author),\n  gMCP project contributors [cph] (gMCP package),\n  Merck & Co., Inc., Rahway, NJ, USA and its affiliates [cph]",
    "url": "https://merck.github.io/gMCPLite/,\nhttps://github.com/Merck/gMCPLite",
    "bug_reports": "https://github.com/Merck/gMCPLite/issues",
    "repository": "https://cran.r-project.org/package=gMCPLite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gMCPLite Lightweight Graph Based Multiple Comparison Procedures A lightweight fork of 'gMCP' with functions for graphical\n    described multiple test procedures introduced in\n    Bretz et al. (2009) <doi:10.1002/sim.3495> and\n    Bretz et al. (2011) <doi:10.1002/bimj.201000239>.\n    Implements a flexible function using 'ggplot2' to create\n    multiplicity graph visualizations.\n    Contains instructions of multiplicity graph and graphical testing for\n    group sequential design, described in\n    Maurer and Bretz (2013) <doi:10.1080/19466315.2013.807748>,\n    with necessary unit testing using 'testthat'.  "
  },
  {
    "id": 12828,
    "package_name": "gRbase",
    "title": "A Package for Graphical Modelling in R",
    "description": "The 'gRbase' package provides graphical modelling features\n    used by e.g. the packages 'gRain', 'gRim' and 'gRc'. 'gRbase' implements\n    graph algorithms including (i) maximum cardinality search (for marked\n    and unmarked graphs).\n    (ii) moralization, (iii) triangulation, (iv) creation of junction tree.\n    'gRbase' facilitates array operations,\n    'gRbase' implements functions for testing for conditional independence.\n    'gRbase' illustrates how hierarchical log-linear models may be\n    implemented and describes concept of graphical meta\n    data. \n    The facilities of the package are documented in the book by H\u00f8jsgaard,\n    Edwards and Lauritzen (2012,\n    <doi:10.1007/978-1-4614-2299-0>) and in the paper by \n    Dethlefsen and H\u00f8jsgaard, (2005, <doi:10.18637/jss.v014.i17>).\n    Please see 'citation(\"gRbase\")' for citation details. ",
    "version": "2.0.3",
    "maintainer": "S\u00f8ren H\u00f8jsgaard <sorenh@math.aau.dk>",
    "author": "S\u00f8ren H\u00f8jsgaard [aut, cre]",
    "url": "https://people.math.aau.dk/~sorenh/software/gR/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gRbase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gRbase A Package for Graphical Modelling in R The 'gRbase' package provides graphical modelling features\n    used by e.g. the packages 'gRain', 'gRim' and 'gRc'. 'gRbase' implements\n    graph algorithms including (i) maximum cardinality search (for marked\n    and unmarked graphs).\n    (ii) moralization, (iii) triangulation, (iv) creation of junction tree.\n    'gRbase' facilitates array operations,\n    'gRbase' implements functions for testing for conditional independence.\n    'gRbase' illustrates how hierarchical log-linear models may be\n    implemented and describes concept of graphical meta\n    data. \n    The facilities of the package are documented in the book by H\u00f8jsgaard,\n    Edwards and Lauritzen (2012,\n    <doi:10.1007/978-1-4614-2299-0>) and in the paper by \n    Dethlefsen and H\u00f8jsgaard, (2005, <doi:10.18637/jss.v014.i17>).\n    Please see 'citation(\"gRbase\")' for citation details.   "
  },
  {
    "id": 12833,
    "package_name": "gTests",
    "title": "Graph-Based Two-Sample Tests",
    "description": "Four graph-based tests are provided for testing whether two samples are from the same distribution.  It works for both continuous data and discrete data.",
    "version": "0.2",
    "maintainer": "Hao Chen <hxchen@ucdavis.edu>",
    "author": "Hao Chen and Jingru Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gTests Graph-Based Two-Sample Tests Four graph-based tests are provided for testing whether two samples are from the same distribution.  It works for both continuous data and discrete data.  "
  },
  {
    "id": 12834,
    "package_name": "gTestsMulti",
    "title": "New Graph-Based Multi-Sample Tests",
    "description": "New multi-sample tests for testing whether multiple samples are from the same distribution. They work well particularly for high-dimensional data.\n    Song, H. and Chen, H. (2022) \n    <arXiv:2205.13787>.",
    "version": "0.1.1",
    "maintainer": "Hoseung Song <hosong@ucdavis.edu>",
    "author": "Hoseung Song [aut, cre],\n  Hao Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gTestsMulti",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gTestsMulti New Graph-Based Multi-Sample Tests New multi-sample tests for testing whether multiple samples are from the same distribution. They work well particularly for high-dimensional data.\n    Song, H. and Chen, H. (2022) \n    <arXiv:2205.13787>.  "
  },
  {
    "id": 12839,
    "package_name": "gaawr2",
    "title": "Genetic Association Analysis",
    "description": "It gathers information, meta-data and scripts in a two-part Henry-Stewart talk by\n Zhao (2009, <doi:10.69645/DCRY5578>), which showcases analysis in aspects such as testing of polymorphic\n variant(s) for Hardy-Weinberg equilibrium, association with trait using genetic and statistical models\n as well as Bayesian implementation, power calculation in study design and genetic annotation. It also\n covers R integration with the Linux environment, GitHub, package creation and web applications.",
    "version": "0.0.3",
    "maintainer": "Jing Hua Zhao <jinghuazhao@hotmail.com>",
    "author": "Jing Hua Zhao [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1463-5870>, ORCID:\n    <https://orcid.org/0000-0003-4930-3582>),\n  Benjamin Altmann [ctb],\n  Brian Ripley [ctb]",
    "url": "https://jinghuazhao.github.io/gaawr2/,\nhttps://github.com/jinghuazhao/gaawr2",
    "bug_reports": "https://github.com/jinghuazhao/gaawr2/issues",
    "repository": "https://cran.r-project.org/package=gaawr2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gaawr2 Genetic Association Analysis It gathers information, meta-data and scripts in a two-part Henry-Stewart talk by\n Zhao (2009, <doi:10.69645/DCRY5578>), which showcases analysis in aspects such as testing of polymorphic\n variant(s) for Hardy-Weinberg equilibrium, association with trait using genetic and statistical models\n as well as Bayesian implementation, power calculation in study design and genetic annotation. It also\n covers R integration with the Linux environment, GitHub, package creation and web applications.  "
  },
  {
    "id": 12852,
    "package_name": "gallery",
    "title": "Generate Test Matrices for Numerical Experiments",
    "description": "Generates a variety of structured test matrices commonly used in numerical linear algebra and computational experiments. Includes well-known matrices for benchmarking and testing the performance, stability, and accuracy of linear algebra algorithms. Inspired by 'MATLAB' 'gallery' functions.",
    "version": "1.0.0",
    "maintainer": "Thomas Hsiao <thomas.hsiao@emory.edu>",
    "author": "Thomas Hsiao [aut, cre] (ORCID:\n    <https://orcid.org/0009-0005-7848-7992>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gallery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gallery Generate Test Matrices for Numerical Experiments Generates a variety of structured test matrices commonly used in numerical linear algebra and computational experiments. Includes well-known matrices for benchmarking and testing the performance, stability, and accuracy of linear algebra algorithms. Inspired by 'MATLAB' 'gallery' functions.  "
  },
  {
    "id": 12883,
    "package_name": "gammi",
    "title": "Generalized Additive Mixed Model Interface",
    "description": "An interface for fitting generalized additive models (GAMs) and generalized additive mixed models (GAMMs) using the 'lme4' package as the computational engine, as described in Helwig (2024) <doi:10.3390/stats7010003>. Supports default and formula methods for model specification, additive and tensor product splines for capturing nonlinear effects, and automatic determination of spline type based on the class of each predictor. Includes an S3 plot method for visualizing the (nonlinear) model terms, an S3 predict method for forming predictions from a fit model, and an S3 summary method for conducting significance testing using the Bayesian interpretation of a smoothing spline.",
    "version": "0.2",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "author": "Nathaniel E. Helwig [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gammi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gammi Generalized Additive Mixed Model Interface An interface for fitting generalized additive models (GAMs) and generalized additive mixed models (GAMMs) using the 'lme4' package as the computational engine, as described in Helwig (2024) <doi:10.3390/stats7010003>. Supports default and formula methods for model specification, additive and tensor product splines for capturing nonlinear effects, and automatic determination of spline type based on the class of each predictor. Includes an S3 plot method for visualizing the (nonlinear) model terms, an S3 predict method for forming predictions from a fit model, and an S3 summary method for conducting significance testing using the Bayesian interpretation of a smoothing spline.  "
  },
  {
    "id": 12911,
    "package_name": "gausscov",
    "title": "The Gaussian Covariate Method for Variable Selection",
    "description": "The standard linear regression theory whether frequentist or Bayesian is based on an 'assumed (revealed?) truth' (John Tukey) attitude to models. This is reflected in the language of statistical inference which involves a concept of truth, for example confidence intervals, hypothesis testing and consistency. The motivation behind this package was to remove the word true from the theory and practice of linear regression and to replace it by approximation. The approximations considered are the least squares approximations. An approximation is called valid if it contains no irrelevant covariates. This is operationalized using the concept of a Gaussian P-value which is the probability that pure Gaussian noise is better in term of least squares than the covariate. The precise definition given in the paper  \"An Approximation Based Theory of Linear Regression\".  Only four simple equations are required. Moreover the Gaussian P-values can be simply derived from standard F P-values. Furthermore they are exact and valid whatever the data in contrast F P-values are only valid for specially designed simulations. A valid approximation is one where all the Gaussian P-values are less than a threshold p0 specified by the statistician, in this package with the default value 0.01. This approximations approach is not only much simpler it is overwhelmingly better than the standard model based approach. The will be demonstrated using high dimensional regression and vector autoregression real data sets. The goal is to find valid approximations. The search function is f1st which is a greedy forward selection procedure which results in either just one or no approximations which may however not be valid. If the size is less than than a threshold with default value 21 then an all subset procedure is called which returns the best valid subset. A good default start is f1st(y,x,kmn=15) The best function for returning multiple approximations is f3st which repeatedly calls f1st. For more information see the papers: L. Davies and L. Duembgen, \"Covariate Selection Based on a Model-free Approach to Linear Regression with Exact Probabilities\", <doi:10.48550/arXiv.2202.01553>, L. Davies, \"An Approximation Based Theory of Linear Regression\", 2024, <doi:10.48550/arXiv.2402.09858>.",
    "version": "1.1.8",
    "maintainer": "Laurie Davies <pldavies44@cantab.net>",
    "author": "Laurie Davies [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gausscov",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gausscov The Gaussian Covariate Method for Variable Selection The standard linear regression theory whether frequentist or Bayesian is based on an 'assumed (revealed?) truth' (John Tukey) attitude to models. This is reflected in the language of statistical inference which involves a concept of truth, for example confidence intervals, hypothesis testing and consistency. The motivation behind this package was to remove the word true from the theory and practice of linear regression and to replace it by approximation. The approximations considered are the least squares approximations. An approximation is called valid if it contains no irrelevant covariates. This is operationalized using the concept of a Gaussian P-value which is the probability that pure Gaussian noise is better in term of least squares than the covariate. The precise definition given in the paper  \"An Approximation Based Theory of Linear Regression\".  Only four simple equations are required. Moreover the Gaussian P-values can be simply derived from standard F P-values. Furthermore they are exact and valid whatever the data in contrast F P-values are only valid for specially designed simulations. A valid approximation is one where all the Gaussian P-values are less than a threshold p0 specified by the statistician, in this package with the default value 0.01. This approximations approach is not only much simpler it is overwhelmingly better than the standard model based approach. The will be demonstrated using high dimensional regression and vector autoregression real data sets. The goal is to find valid approximations. The search function is f1st which is a greedy forward selection procedure which results in either just one or no approximations which may however not be valid. If the size is less than than a threshold with default value 21 then an all subset procedure is called which returns the best valid subset. A good default start is f1st(y,x,kmn=15) The best function for returning multiple approximations is f3st which repeatedly calls f1st. For more information see the papers: L. Davies and L. Duembgen, \"Covariate Selection Based on a Model-free Approach to Linear Regression with Exact Probabilities\", <doi:10.48550/arXiv.2202.01553>, L. Davies, \"An Approximation Based Theory of Linear Regression\", 2024, <doi:10.48550/arXiv.2402.09858>.  "
  },
  {
    "id": 12950,
    "package_name": "gdiff",
    "title": "Graphical Difference Testing",
    "description": "Functions for performing graphical difference testing.     \n             Differences are generated between raster images.\n             Comparisons can be performed between different package\n             versions and between different R versions.",
    "version": "0.2-5",
    "maintainer": "Paul Murrell <paul@stat.auckland.ac.nz>",
    "author": "Paul Murrell",
    "url": "https://github.com/pmur002/,\nhttps://stattech.wordpress.fos.auckland.ac.nz/2020/01/06/2020-01-visual-testing-for-graphics-in-r/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gdiff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdiff Graphical Difference Testing Functions for performing graphical difference testing.     \n             Differences are generated between raster images.\n             Comparisons can be performed between different package\n             versions and between different R versions.  "
  },
  {
    "id": 12951,
    "package_name": "gdim",
    "title": "Estimate Graph Dimension using Cross-Validated Eigenvalues",
    "description": "Cross-validated eigenvalues are estimated by\n    splitting a graph into two parts, the training and the test graph.\n    The training graph is used to estimate eigenvectors, and\n    the test graph is used to evaluate the correlation between the training\n    eigenvectors and the eigenvectors of the test graph.\n    The correlations follow a simple central limit theorem that can\n    be used to estimate graph dimension via hypothesis testing, see\n    Chen et al. (2021) <doi:10.48550/arXiv.2108.03336> for details.",
    "version": "0.1.1",
    "maintainer": "Alex Hayes <alexpghayes@gmail.com>",
    "author": "Fan Chen [aut] (ORCID: <https://orcid.org/0000-0003-4508-6023>),\n  Alex Hayes [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-4985-5160>),\n  Karl Rohe [aut]",
    "url": "https://github.com/RoheLab/gdim, https://rohelab.github.io/gdim/",
    "bug_reports": "https://github.com/RoheLab/gdim/issues",
    "repository": "https://cran.r-project.org/package=gdim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdim Estimate Graph Dimension using Cross-Validated Eigenvalues Cross-validated eigenvalues are estimated by\n    splitting a graph into two parts, the training and the test graph.\n    The training graph is used to estimate eigenvectors, and\n    the test graph is used to evaluate the correlation between the training\n    eigenvectors and the eigenvectors of the test graph.\n    The correlations follow a simple central limit theorem that can\n    be used to estimate graph dimension via hypothesis testing, see\n    Chen et al. (2021) <doi:10.48550/arXiv.2108.03336> for details.  "
  },
  {
    "id": 13018,
    "package_name": "genetics",
    "title": "Population Genetics",
    "description": "Classes and methods for handling genetic data. Includes\n        classes to represent genotypes and haplotypes at single markers\n        up to multiple markers on multiple chromosomes. Function\n        include allele frequencies, flagging homo/heterozygotes,\n        flagging carriers of certain alleles, estimating and testing\n        for Hardy-Weinberg disequilibrium, estimating and testing for\n        linkage disequilibrium, ...",
    "version": "1.3.8.1.3",
    "maintainer": "Gregory Warnes <greg@warnes.net>",
    "author": "Gregory Warnes, with contributions from Gregor Gorjanc,\n        Friedrich Leisch, and Michael Man.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=genetics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genetics Population Genetics Classes and methods for handling genetic data. Includes\n        classes to represent genotypes and haplotypes at single markers\n        up to multiple markers on multiple chromosomes. Function\n        include allele frequencies, flagging homo/heterozygotes,\n        flagging carriers of certain alleles, estimating and testing\n        for Hardy-Weinberg disequilibrium, estimating and testing for\n        linkage disequilibrium, ...  "
  },
  {
    "id": 13029,
    "package_name": "genodds",
    "title": "Generalised Odds Ratios",
    "description": "\n  Calculates Agresti's generalized odds ratios.\n  For a randomly selected pair of observations\n  from two groups, calculates the odds that\n  the second group will have a higher scoring outcome\n  than that of the first group.\n  Package provides hypothesis testing for if this odds\n  ratio is significantly different to 1 (equal chance).",
    "version": "1.1.2",
    "maintainer": "Hannah Johns <htjohns@gmail.com>",
    "author": "Hannah Johns [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2135-0504>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=genodds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genodds Generalised Odds Ratios \n  Calculates Agresti's generalized odds ratios.\n  For a randomly selected pair of observations\n  from two groups, calculates the odds that\n  the second group will have a higher scoring outcome\n  than that of the first group.\n  Package provides hypothesis testing for if this odds\n  ratio is significantly different to 1 (equal chance).  "
  },
  {
    "id": 13032,
    "package_name": "genomicper",
    "title": "Circular Genomic Permutation using Genome Wide Association\np-Values",
    "description": "Circular genomic permutation approach uses genome wide association studies (GWAS) results to establish the significance of pathway/gene-set associations whilst accounting for genomic structure(Cabrera et al (2012) <doi:10.1534/g3.112.002618>). All single nucleotide polymorphisms (SNPs) in the GWAS are placed in a 'circular genome' according to their location. Then the complete set of SNP association p-values are permuted by rotation with respect to the SNPs' genomic locations. Two testing frameworks are available: permutations at the gene level, and permutations at the SNP level. The permutation at the gene level uses Fisher's combination test to calculate a single gene p-value, followed by the hypergeometric test. The SNP count methodology maps each SNP to pathways/gene-sets and calculates the proportion of SNPs for the real and the permutated datasets above a pre-defined threshold. Genomicper requires a matrix of GWAS association p-values and SNPs annotation to genes. Pathways can be obtained from within the package or can be provided by the user.",
    "version": "1.7",
    "maintainer": "Claudia P Cabrera <c.cabrera@qmul.ac.uk>",
    "author": "Claudia P Cabrera [aut, cre],\n  Pau Navarro [aut],\n  Chris S Haley [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=genomicper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genomicper Circular Genomic Permutation using Genome Wide Association\np-Values Circular genomic permutation approach uses genome wide association studies (GWAS) results to establish the significance of pathway/gene-set associations whilst accounting for genomic structure(Cabrera et al (2012) <doi:10.1534/g3.112.002618>). All single nucleotide polymorphisms (SNPs) in the GWAS are placed in a 'circular genome' according to their location. Then the complete set of SNP association p-values are permuted by rotation with respect to the SNPs' genomic locations. Two testing frameworks are available: permutations at the gene level, and permutations at the SNP level. The permutation at the gene level uses Fisher's combination test to calculate a single gene p-value, followed by the hypergeometric test. The SNP count methodology maps each SNP to pathways/gene-sets and calculates the proportion of SNPs for the real and the permutated datasets above a pre-defined threshold. Genomicper requires a matrix of GWAS association p-values and SNPs annotation to genes. Pathways can be obtained from within the package or can be provided by the user.  "
  },
  {
    "id": 13106,
    "package_name": "geovol",
    "title": "Geopolitical Volatility (GEOVOL) Modelling",
    "description": "Simulation, estimation and testing for geopolitical volatility (GEOVOL) based on the global common volatility model of Engle and Campos-Martins (2023) <doi:10.1016/j.jfineco.2022.09.009>. GEOVOL is modelled as a latent multiplicative volatility factor with heterogeneous factor loadings. Estimation is carried out as a maximization-maximization procedure, where GEOVOL and the GEOVOL loadings are estimated iteratively until convergence.",
    "version": "1.1",
    "maintainer": "Susana Campos-Martins <scmartins@ucp.pt>",
    "author": "Susana Campos-Martins [aut, cre]",
    "url": "https://sites.google.com/site/susanacamposmartins/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geovol",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geovol Geopolitical Volatility (GEOVOL) Modelling Simulation, estimation and testing for geopolitical volatility (GEOVOL) based on the global common volatility model of Engle and Campos-Martins (2023) <doi:10.1016/j.jfineco.2022.09.009>. GEOVOL is modelled as a latent multiplicative volatility factor with heterogeneous factor loadings. Estimation is carried out as a maximization-maximization procedure, where GEOVOL and the GEOVOL loadings are estimated iteratively until convergence.  "
  },
  {
    "id": 13128,
    "package_name": "gets",
    "title": "General-to-Specific (GETS) Modelling and Indicator Saturation\nMethods",
    "description": "Automated General-to-Specific (GETS) modelling of the mean and variance of a regression, and indicator saturation methods for detecting and testing for structural breaks in the mean, see Pretis, Reade and Sucarrat (2018) <doi:10.18637/jss.v086.i03> for an overview of the package. In advanced use, the estimator and diagnostics tests can be fully user-specified, see Sucarrat (2021) <doi:10.32614/RJ-2021-024>.",
    "version": "0.38",
    "maintainer": "Genaro Sucarrat <genaro.sucarrat@bi.no>",
    "author": "Genaro Sucarrat [aut, cre], Felix Pretis [aut], James Reade [aut], Jonas Kurle [ctb], Moritz Schwarz [ctb]",
    "url": "https://CRAN.R-project.org/package=gets,\nhttp://www.sucarrat.net/R/gets/",
    "bug_reports": "https://github.com/gsucarrat/gets/issues",
    "repository": "https://cran.r-project.org/package=gets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gets General-to-Specific (GETS) Modelling and Indicator Saturation\nMethods Automated General-to-Specific (GETS) modelling of the mean and variance of a regression, and indicator saturation methods for detecting and testing for structural breaks in the mean, see Pretis, Reade and Sucarrat (2018) <doi:10.18637/jss.v086.i03> for an overview of the package. In advanced use, the estimator and diagnostics tests can be fully user-specified, see Sucarrat (2021) <doi:10.32614/RJ-2021-024>.  "
  },
  {
    "id": 13248,
    "package_name": "ggm",
    "title": "Graphical Markov Models with Mixed Graphs",
    "description": "Provides functions for defining \n    mixed graphs containing three types of edges, directed, \n    undirected and bi-directed, with possibly multiple edges.\n    These graphs are useful because they capture fundamental\n    independence structures in multivariate distributions\n    and in the induced distributions after marginalization \n    and conditioning.\n    The package is especially concerned with Gaussian graphical\n    models for\n    (i) ML estimation for directed acyclic graphs, undirected and \n    bi-directed graphs and ancestral graph models\n    (ii) testing several conditional independencies\n    (iii) checking global identification of DAG Gaussian models\n    with one latent variable\n    (iv) testing Markov equivalences and generating Markov \n    equivalent graphs of specific types.",
    "version": "2.5.2",
    "maintainer": "Giovanni M. Marchetti <giovanni.marchetti@unifi.it>",
    "author": "Giovanni M. Marchetti [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4413-9349>),\n  Mathias Drton [aut] (ORCID: <https://orcid.org/0000-0001-5614-3025>),\n  Kayvan Sadeghi [aut] (ORCID: <https://orcid.org/0000-0001-7314-744X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ggm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ggm Graphical Markov Models with Mixed Graphs Provides functions for defining \n    mixed graphs containing three types of edges, directed, \n    undirected and bi-directed, with possibly multiple edges.\n    These graphs are useful because they capture fundamental\n    independence structures in multivariate distributions\n    and in the induced distributions after marginalization \n    and conditioning.\n    The package is especially concerned with Gaussian graphical\n    models for\n    (i) ML estimation for directed acyclic graphs, undirected and \n    bi-directed graphs and ancestral graph models\n    (ii) testing several conditional independencies\n    (iii) checking global identification of DAG Gaussian models\n    with one latent variable\n    (iv) testing Markov equivalences and generating Markov \n    equivalent graphs of specific types.  "
  },
  {
    "id": 13361,
    "package_name": "ghcm",
    "title": "Functional Conditional Independence Testing with the GHCM",
    "description": "A statistical hypothesis test for conditional independence.\n    Given residuals from a sufficiently powerful regression, it tests whether \n    the covariance of the residuals is vanishing. It can be applied to both\n    discretely-observed functional data and multivariate data. \n    Details of the method can be found in Anton Rask Lundborg, Rajen D. Shah and Jonas\n    Peters (2022) <doi:10.1111/rssb.12544>.",
    "version": "3.0.1",
    "maintainer": "Anton Rask Lundborg <arl@math.ku.dk>",
    "author": "Anton Rask Lundborg [aut, cre],\n  Rajen D. Shah [aut],\n  Jonas Peters [aut]",
    "url": "https://github.com/arlundborg/ghcm",
    "bug_reports": "https://github.com/arlundborg/ghcm/issues",
    "repository": "https://cran.r-project.org/package=ghcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ghcm Functional Conditional Independence Testing with the GHCM A statistical hypothesis test for conditional independence.\n    Given residuals from a sufficiently powerful regression, it tests whether \n    the covariance of the residuals is vanishing. It can be applied to both\n    discretely-observed functional data and multivariate data. \n    Details of the method can be found in Anton Rask Lundborg, Rajen D. Shah and Jonas\n    Peters (2022) <doi:10.1111/rssb.12544>.  "
  },
  {
    "id": 13401,
    "package_name": "glarma",
    "title": "Generalized Linear Autoregressive Moving Average Models",
    "description": "Functions are provided for estimation, testing, diagnostic checking and forecasting of generalized linear autoregressive moving average (GLARMA) models for discrete valued time series with regression variables.  These are a class of observation driven non-linear non-Gaussian state space models. The state vector consists of a linear regression component plus an observation driven component consisting of an autoregressive-moving average (ARMA) filter of past predictive residuals. Currently three distributions (Poisson, negative binomial and binomial) can be used for the response series. Three options (Pearson, score-type and unscaled) for the residuals in the observation driven component are available. Estimation is via maximum likelihood (conditional on initializing values for the ARMA process) optimized using Fisher scoring or Newton Raphson iterative methods. Likelihood ratio and Wald tests for the observation driven component allow testing for serial dependence in generalized linear model settings. Graphical diagnostics including model fits, autocorrelation functions and probability integral transform residuals are included in the package. Several standard data sets are included in the package.",
    "version": "1.7-1",
    "maintainer": "William T.M. Dunsmuir <w.dunsmuir@unsw.edu.au>",
    "author": "William T.M. Dunsmuir [aut, cre],\n  Cenanning Li [aut],\n  David J. Scott [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glarma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glarma Generalized Linear Autoregressive Moving Average Models Functions are provided for estimation, testing, diagnostic checking and forecasting of generalized linear autoregressive moving average (GLARMA) models for discrete valued time series with regression variables.  These are a class of observation driven non-linear non-Gaussian state space models. The state vector consists of a linear regression component plus an observation driven component consisting of an autoregressive-moving average (ARMA) filter of past predictive residuals. Currently three distributions (Poisson, negative binomial and binomial) can be used for the response series. Three options (Pearson, score-type and unscaled) for the residuals in the observation driven component are available. Estimation is via maximum likelihood (conditional on initializing values for the ARMA process) optimized using Fisher scoring or Newton Raphson iterative methods. Likelihood ratio and Wald tests for the observation driven component allow testing for serial dependence in generalized linear model settings. Graphical diagnostics including model fits, autocorrelation functions and probability integral transform residuals are included in the package. Several standard data sets are included in the package.  "
  },
  {
    "id": 13460,
    "package_name": "glogis",
    "title": "Fitting and Testing Generalized Logistic Distributions",
    "description": "Tools for the generalized logistic distribution (Type I,\n             also known as skew-logistic distribution), encompassing\n\t     basic distribution functions (p, q, d, r, score), maximum\n\t     likelihood estimation, and structural change methods.",
    "version": "1.0-2",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Thomas Windberger [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=glogis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "glogis Fitting and Testing Generalized Logistic Distributions Tools for the generalized logistic distribution (Type I,\n             also known as skew-logistic distribution), encompassing\n\t     basic distribution functions (p, q, d, r, score), maximum\n\t     likelihood estimation, and structural change methods.  "
  },
  {
    "id": 13512,
    "package_name": "gofIG",
    "title": "Goodness-of-Fit Tests for the Inverse Gaussian Distribution",
    "description": "We implement various tests for the composite hypothesis of\n    testing the fit to the family of inverse Gaussian distributions. \n    Included are methods presented by Allison, J.S., Betsch, S., Ebner, B., and Visagie, I.J.H. (2022) <doi:10.48550/arXiv.1910.14119>, \n    as well as two tests from Henze and Klar (2002) <doi:10.1023/A:1022442506681>.\n    Additionally, the package implements a test proposed by Baringhaus and Gaigall (2015) <doi:10.1016/j.jmva.2015.05.013>. \n    For each test a parametric bootstrap procedure is implemented.",
    "version": "1.0",
    "maintainer": "Bruno Ebner <bruno.ebner@kit.edu>",
    "author": "Bruno Ebner [aut, cre],\n  Jaco Visagie [aut],\n  Steffen Betsch [aut],\n  James Allison [aut],\n  Lucas Iglesias [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gofIG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gofIG Goodness-of-Fit Tests for the Inverse Gaussian Distribution We implement various tests for the composite hypothesis of\n    testing the fit to the family of inverse Gaussian distributions. \n    Included are methods presented by Allison, J.S., Betsch, S., Ebner, B., and Visagie, I.J.H. (2022) <doi:10.48550/arXiv.1910.14119>, \n    as well as two tests from Henze and Klar (2002) <doi:10.1023/A:1022442506681>.\n    Additionally, the package implements a test proposed by Baringhaus and Gaigall (2015) <doi:10.1016/j.jmva.2015.05.013>. \n    For each test a parametric bootstrap procedure is implemented.  "
  },
  {
    "id": 13517,
    "package_name": "gofgamma",
    "title": "Goodness-of-Fit Tests for the Gamma Distribution",
    "description": "We implement various classical tests for the composite hypothesis of testing the fit to the family of gamma distributions as the Kolmogorov-Smirnov test, the Cramer-von Mises test, the Anderson Darling test and the Watson test. For each test a parametric bootstrap procedure is implemented, as considered in Henze, Meintanis & Ebner (2012) <doi:10.1080/03610926.2010.542851>. The recent procedures presented in Henze, Meintanis & Ebner (2012) <doi:10.1080/03610926.2010.542851> and Betsch & Ebner (2019) <doi:10.1007/s00184-019-00708-7> are implemented. Estimation of parameters of the gamma law are implemented using the method of Bhattacharya (2001) <doi:10.1080/00949650108812100>. ",
    "version": "1.0",
    "maintainer": "Bruno Ebner <bruno.ebner@kit.edu>",
    "author": "Lucas Butsch [aut],\n  Bruno Ebner [aut, cre],\n  Steffen Betsch [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gofgamma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gofgamma Goodness-of-Fit Tests for the Gamma Distribution We implement various classical tests for the composite hypothesis of testing the fit to the family of gamma distributions as the Kolmogorov-Smirnov test, the Cramer-von Mises test, the Anderson Darling test and the Watson test. For each test a parametric bootstrap procedure is implemented, as considered in Henze, Meintanis & Ebner (2012) <doi:10.1080/03610926.2010.542851>. The recent procedures presented in Henze, Meintanis & Ebner (2012) <doi:10.1080/03610926.2010.542851> and Betsch & Ebner (2019) <doi:10.1007/s00184-019-00708-7> are implemented. Estimation of parameters of the gamma law are implemented using the method of Bhattacharya (2001) <doi:10.1080/00949650108812100>.   "
  },
  {
    "id": 13626,
    "package_name": "greekLetters",
    "title": "Routines for Writing Greek Letters and Mathematical Symbols on\nthe 'RStudio' and 'RGui'",
    "description": "An implementation of functions to display Greek letters on the 'RStudio' (include subscript and superscript indexes) and 'RGui' (without subscripts and only with superscript 1, 2 or 3; because 'RGui' doesn't support printing the corresponding Unicode characters as a string: all subscripts ranging from 0 to 9 and superscripts equal to 0, 4, 5, 6, 7, 8 or 9). The functions in this package do not work properly on the R console. Characters are used via Unicode and encoded as UTF-8 to ensure that they can be viewed on all operating systems. Other characters related to mathematics are included, such as the infinity symbol. All this accessible from very simple commands. This is a package that can be used for teaching purposes, the statistical notation for hypothesis testing can be written from this package and so it is possible to build a course from the 'swirlify' package. Another utility of this package is to create new summary functions that contain the functional form of the model adjusted with the Greek letters, thus making the transition from statistical theory to practice easier. In addition, it is a natural extension of the 'clisymbols' package.",
    "version": "1.0.4",
    "maintainer": "Kevin Allan Sales Rodrigues <kevin.asr@outlook.com>",
    "author": "Kevin Allan Sales Rodrigues [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4925-5883>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=greekLetters",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "greekLetters Routines for Writing Greek Letters and Mathematical Symbols on\nthe 'RStudio' and 'RGui' An implementation of functions to display Greek letters on the 'RStudio' (include subscript and superscript indexes) and 'RGui' (without subscripts and only with superscript 1, 2 or 3; because 'RGui' doesn't support printing the corresponding Unicode characters as a string: all subscripts ranging from 0 to 9 and superscripts equal to 0, 4, 5, 6, 7, 8 or 9). The functions in this package do not work properly on the R console. Characters are used via Unicode and encoded as UTF-8 to ensure that they can be viewed on all operating systems. Other characters related to mathematics are included, such as the infinity symbol. All this accessible from very simple commands. This is a package that can be used for teaching purposes, the statistical notation for hypothesis testing can be written from this package and so it is possible to build a course from the 'swirlify' package. Another utility of this package is to create new summary functions that contain the functional form of the model adjusted with the Greek letters, thus making the transition from statistical theory to practice easier. In addition, it is a natural extension of the 'clisymbols' package.  "
  },
  {
    "id": 13654,
    "package_name": "grizbayr",
    "title": "Bayesian Inference for A|B and Bandit Marketing Tests",
    "description": "Uses simple Bayesian conjugate prior update rules to calculate \n    the win probability of each option, value remaining in the test, and \n    percent lift over the baseline for various marketing objectives.\n    References: \n    Fink, Daniel (1997) \"A Compendium of Conjugate Priors\" <https://www.johndcook.com/CompendiumOfConjugatePriors.pdf>.\n    Stucchio, Chris (2015) \"Bayesian A/B Testing at VWO\" <https://vwo.com/downloads/VWO_SmartStats_technical_whitepaper.pdf>.",
    "version": "1.3.5",
    "maintainer": "Ryan Angi <angi.ryan@gmail.com>",
    "author": "Ryan Angi",
    "url": "https://github.com/rangi513/grizbayr",
    "bug_reports": "https://github.com/rangi513/grizbayr/issues",
    "repository": "https://cran.r-project.org/package=grizbayr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grizbayr Bayesian Inference for A|B and Bandit Marketing Tests Uses simple Bayesian conjugate prior update rules to calculate \n    the win probability of each option, value remaining in the test, and \n    percent lift over the baseline for various marketing objectives.\n    References: \n    Fink, Daniel (1997) \"A Compendium of Conjugate Priors\" <https://www.johndcook.com/CompendiumOfConjugatePriors.pdf>.\n    Stucchio, Chris (2015) \"Bayesian A/B Testing at VWO\" <https://vwo.com/downloads/VWO_SmartStats_technical_whitepaper.pdf>.  "
  },
  {
    "id": 13660,
    "package_name": "groqR",
    "title": "A Coding Assistant using the Fast AI Inference 'Groq'",
    "description": "A comprehensive suite of functions and 'RStudio' Add-ins leveraging the capabilities of open-source Large Language Models (LLMs) to support R developers. These functions offer a range of utilities, including text rewriting, translation, and general query capabilities. Additionally, the programming-focused functions provide assistance with debugging, translating, commenting, documenting, and unit testing code, as well as suggesting variable and function names, thereby streamlining the development process.",
    "version": "0.0.3",
    "maintainer": "Gabriel Kaiser <quantresearch.gk@gmail.com>",
    "author": "Gabriel Kaiser [aut, cre]",
    "url": "https://github.com/GabrielKaiserQFin/groqR",
    "bug_reports": "https://github.com/GabrielKaiserQFin/groqR/issues",
    "repository": "https://cran.r-project.org/package=groqR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "groqR A Coding Assistant using the Fast AI Inference 'Groq' A comprehensive suite of functions and 'RStudio' Add-ins leveraging the capabilities of open-source Large Language Models (LLMs) to support R developers. These functions offer a range of utilities, including text rewriting, translation, and general query capabilities. Additionally, the programming-focused functions provide assistance with debugging, translating, commenting, documenting, and unit testing code, as well as suggesting variable and function names, thereby streamlining the development process.  "
  },
  {
    "id": 13663,
    "package_name": "groupTesting",
    "title": "Simulating and Modeling Group (Pooled) Testing Data",
    "description": "Provides an expectation-maximization (EM) algorithm using the approach introduced in Warasi (2023) <doi:10.1080/03610918.2021.2009867>. The EM algorithm can be used to estimate the prevalence (overall proportion) of a disease and to estimate a binary regression model from among the class of generalized linear models based on group testing data. The estimation framework we consider offers a flexible and general approach; i.e., its application is not limited to any specific group testing protocol. Consequently, the EM algorithm can model data arising from simple pooling as well as advanced pooling such as hierarchical testing, array testing, and quality control pooling. Also, provided are functions that can be used to conduct the Wald tests described in Buse (1982) <doi:10.1080/00031305.1982.10482817> and to simulate the group testing data described in Kim et al. (2007) <doi:10.1111/j.1541-0420.2007.00817.x>. We offer a function to compute relative efficiency measures, which can be used to optimize the maximum likelihood estimator of disease prevalence.",
    "version": "1.3.0",
    "maintainer": "Md S. Warasi <msarker@radford.edu>",
    "author": "Md S. Warasi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=groupTesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "groupTesting Simulating and Modeling Group (Pooled) Testing Data Provides an expectation-maximization (EM) algorithm using the approach introduced in Warasi (2023) <doi:10.1080/03610918.2021.2009867>. The EM algorithm can be used to estimate the prevalence (overall proportion) of a disease and to estimate a binary regression model from among the class of generalized linear models based on group testing data. The estimation framework we consider offers a flexible and general approach; i.e., its application is not limited to any specific group testing protocol. Consequently, the EM algorithm can model data arising from simple pooling as well as advanced pooling such as hierarchical testing, array testing, and quality control pooling. Also, provided are functions that can be used to conduct the Wald tests described in Buse (1982) <doi:10.1080/00031305.1982.10482817> and to simulate the group testing data described in Kim et al. (2007) <doi:10.1111/j.1541-0420.2007.00817.x>. We offer a function to compute relative efficiency measures, which can be used to optimize the maximum likelihood estimator of disease prevalence.  "
  },
  {
    "id": 13692,
    "package_name": "grwat",
    "title": "River Hydrograph Separation and Analysis",
    "description": "River hydrograph separation and daily runoff time series analysis. Provides\n  various filters to separate baseflow and quickflow. Implements advanced separation \n  technique by Rets et al. (2022) <doi:10.1134/S0097807822010146> which involves \n  meteorological data to reveal genetic components of the runoff: ground, rain, thaw \n  and spring (seasonal thaw). High-performance C++17 computation, annually aggregated \n  variables, statistical testing and numerous plotting functions for high-quality \n  visualization.",
    "version": "0.1",
    "maintainer": "Timofey Samsonov <tsamsonov@geogr.msu.ru>",
    "author": "Timofey Samsonov [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5994-0302>),\n  Ekaterina Rets [ctb] (ORCID: <https://orcid.org/0000-0002-4505-1173>),\n  Maria Kireeva [ctb] (ORCID: <https://orcid.org/0000-0002-8285-9761>)",
    "url": "https://github.com/tsamsonov/grwat,\nhttps://tsamsonov.github.io/grwat/",
    "bug_reports": "https://github.com/tsamsonov/grwat/issues",
    "repository": "https://cran.r-project.org/package=grwat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "grwat River Hydrograph Separation and Analysis River hydrograph separation and daily runoff time series analysis. Provides\n  various filters to separate baseflow and quickflow. Implements advanced separation \n  technique by Rets et al. (2022) <doi:10.1134/S0097807822010146> which involves \n  meteorological data to reveal genetic components of the runoff: ground, rain, thaw \n  and spring (seasonal thaw). High-performance C++17 computation, annually aggregated \n  variables, statistical testing and numerous plotting functions for high-quality \n  visualization.  "
  },
  {
    "id": 13762,
    "package_name": "gwid",
    "title": "Genome-Wide Identity-by-Descent",
    "description": "Methods and tools for the analysis of Genome Wide \n    Identity-by-Descent ('gwid') mapping data, focusing on testing whether there \n    is a higher occurrence of Identity-By-Descent (IBD) segments around potential causal variants \n    in cases compared to controls, which is crucial for identifying rare \n    variants. To enhance its analytical power, 'gwid' incorporates a Sliding \n    Window Approach, allowing for the detection and analysis of signals from \n    multiple Single Nucleotide Polymorphisms (SNPs).",
    "version": "0.3.0",
    "maintainer": "Soroush Mahmoudiandehkordi <soroushmdg@gmail.com>",
    "author": "Soroush Mahmoudiandehkordi [aut, cre],\n  Steven J Schrodi [aut],\n  Mehdi Maadooliat [aut]",
    "url": "https://github.com/soroushmdg/gwid,\nhttps://soroushmdg.github.io/gwid/",
    "bug_reports": "https://github.com/soroushmdg/gwid/issues",
    "repository": "https://cran.r-project.org/package=gwid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gwid Genome-Wide Identity-by-Descent Methods and tools for the analysis of Genome Wide \n    Identity-by-Descent ('gwid') mapping data, focusing on testing whether there \n    is a higher occurrence of Identity-By-Descent (IBD) segments around potential causal variants \n    in cases compared to controls, which is crucial for identifying rare \n    variants. To enhance its analytical power, 'gwid' incorporates a Sliding \n    Window Approach, allowing for the detection and analysis of signals from \n    multiple Single Nucleotide Polymorphisms (SNPs).  "
  },
  {
    "id": 13811,
    "package_name": "harmonicmeanp",
    "title": "Harmonic Mean p-Values and Model Averaging by Mean Maximum\nLikelihood",
    "description": "The harmonic mean p-value (HMP) test combines p-values and corrects for multiple testing while controlling the strong-sense family-wise error rate. It is more powerful than common alternatives including Bonferroni and Simes procedures when combining large proportions of all the p-values, at the cost of slightly lower power when combining small proportions of all the p-values. It is more stringent than controlling the false discovery rate, and possesses theoretical robustness to positive correlations between tests and unequal weights. It is a multi-level test in the sense that a superset of one or more significant tests is certain to be significant and conversely when the superset is non-significant, the constituent tests are certain to be non-significant. It is based on MAMML (model averaging by mean maximum likelihood), a frequentist analogue to Bayesian model averaging, and is theoretically grounded in generalized central limit theorem. For detailed examples type vignette(\"harmonicmeanp\") after installation. Version 3.0 addresses errors in versions 1.0 and 2.0 that led function p.hmp to control the familywise error rate only in the weak sense, rather than the strong sense as intended.",
    "version": "3.0.1",
    "maintainer": "Daniel Wilson <hmp.R.package@gmail.com>",
    "author": "Daniel J. Wilson",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=harmonicmeanp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "harmonicmeanp Harmonic Mean p-Values and Model Averaging by Mean Maximum\nLikelihood The harmonic mean p-value (HMP) test combines p-values and corrects for multiple testing while controlling the strong-sense family-wise error rate. It is more powerful than common alternatives including Bonferroni and Simes procedures when combining large proportions of all the p-values, at the cost of slightly lower power when combining small proportions of all the p-values. It is more stringent than controlling the false discovery rate, and possesses theoretical robustness to positive correlations between tests and unequal weights. It is a multi-level test in the sense that a superset of one or more significant tests is certain to be significant and conversely when the superset is non-significant, the constituent tests are certain to be non-significant. It is based on MAMML (model averaging by mean maximum likelihood), a frequentist analogue to Bayesian model averaging, and is theoretically grounded in generalized central limit theorem. For detailed examples type vignette(\"harmonicmeanp\") after installation. Version 3.0 addresses errors in versions 1.0 and 2.0 that led function p.hmp to control the familywise error rate only in the weak sense, rather than the strong sense as intended.  "
  },
  {
    "id": 13859,
    "package_name": "hdiVAR",
    "title": "Statistical Inference for Noisy Vector Autoregression",
    "description": "The model is high-dimensional vector autoregression with measurement error, also known as linear gaussian state-space model. Provable sparse expectation-maximization algorithm is provided for the estimation of transition matrix and noise variances. Global and simultaneous testings are implemented for transition matrix with false discovery rate control. For more information, see the accompanying paper: Lyu, X., Kang, J., & Li, L. (2023). \"Statistical inference for high-dimensional vector autoregression with measurement error\", Statistica Sinica.",
    "version": "1.0.2",
    "maintainer": "Xiang Lyu <xianglyu.public@gmail.com>",
    "author": "Xiang Lyu [aut, cre],\n  Jian Kang [aut],\n  Lexin Li [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hdiVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdiVAR Statistical Inference for Noisy Vector Autoregression The model is high-dimensional vector autoregression with measurement error, also known as linear gaussian state-space model. Provable sparse expectation-maximization algorithm is provided for the estimation of transition matrix and noise variances. Global and simultaneous testings are implemented for transition matrix with false discovery rate control. For more information, see the accompanying paper: Lyu, X., Kang, J., & Li, L. (2023). \"Statistical inference for high-dimensional vector autoregression with measurement error\", Statistica Sinica.  "
  },
  {
    "id": 13870,
    "package_name": "hdthreshold",
    "title": "Inference on Many Jumps in Nonparametric Panel Regression Models",
    "description": "Provides uniform testing procedures for existence and heterogeneity of threshold \n    effects in high-dimensional nonparametric panel regression models. The package accompanies\n    the paper Chen, Keilbar, Su and Wang (2023) \"Inference on many jumps in nonparametric panel\n    regression models\". arXiv preprint <doi:10.48550/arXiv.2312.01162>.",
    "version": "1.0.0",
    "maintainer": "Georg Keilbar <georg.keilbar@hu-berlin.de>",
    "author": "Georg Keilbar [aut, cre, cph],\n  Likai Chen [ctb],\n  Liangjun Su [ctb],\n  Weining Wang [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hdthreshold",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hdthreshold Inference on Many Jumps in Nonparametric Panel Regression Models Provides uniform testing procedures for existence and heterogeneity of threshold \n    effects in high-dimensional nonparametric panel regression models. The package accompanies\n    the paper Chen, Keilbar, Su and Wang (2023) \"Inference on many jumps in nonparametric panel\n    regression models\". arXiv preprint <doi:10.48550/arXiv.2312.01162>.  "
  },
  {
    "id": 13896,
    "package_name": "hedgehog",
    "title": "Property-Based Testing",
    "description": "Hedgehog will eat all your bugs.\n  'Hedgehog' is a property-based testing package in the spirit\n  of 'QuickCheck'. With 'Hedgehog', one can test properties\n  of their programs against randomly generated input, providing\n  far superior test coverage compared to unit testing. One of the\n  key benefits of 'Hedgehog' is integrated shrinking of\n  counterexamples, which allows one to quickly find the cause of\n  bugs, given salient examples when incorrect behaviour occurs.",
    "version": "0.2",
    "maintainer": "Huw Campbell <huw.campbell@gmail.com>",
    "author": "Huw Campbell [aut, cre]",
    "url": "https://github.com/hedgehogqa",
    "bug_reports": "https://github.com/hedgehogqa/r-hedgehog/issues",
    "repository": "https://cran.r-project.org/package=hedgehog",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hedgehog Property-Based Testing Hedgehog will eat all your bugs.\n  'Hedgehog' is a property-based testing package in the spirit\n  of 'QuickCheck'. With 'Hedgehog', one can test properties\n  of their programs against randomly generated input, providing\n  far superior test coverage compared to unit testing. One of the\n  key benefits of 'Hedgehog' is integrated shrinking of\n  counterexamples, which allows one to quickly find the cause of\n  bugs, given salient examples when incorrect behaviour occurs.  "
  },
  {
    "id": 13923,
    "package_name": "hetsurr",
    "title": "Assessing Heterogeneity in the Utility of a Surrogate Marker",
    "description": "Provides a function to assess and test for heterogeneity in the utility of a surrogate marker with respect to a baseline covariate. The main function can be used for either a continuous or discrete baseline covariate. More details will be available in the future in: Parast, L., Cai, T., Tian L (2021). \"Testing for Heterogeneity in the Utility of a Surrogate Marker.\" Biometrics, In press.",
    "version": "1.0",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "author": "Layla Parast",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hetsurr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hetsurr Assessing Heterogeneity in the Utility of a Surrogate Marker Provides a function to assess and test for heterogeneity in the utility of a surrogate marker with respect to a baseline covariate. The main function can be used for either a continuous or discrete baseline covariate. More details will be available in the future in: Parast, L., Cai, T., Tian L (2021). \"Testing for Heterogeneity in the Utility of a Surrogate Marker.\" Biometrics, In press.  "
  },
  {
    "id": 13926,
    "package_name": "hettest",
    "title": "Testing for a Treatment Effect Using a Heterogeneous Surrogate\nMarker",
    "description": "Tests for a treatment effect using surrogate marker information accounting for heterogeneity in the utility of the surrogate. Details are described in Parast et al (2022) <arXiv:2209.08315>.",
    "version": "1.0",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "author": "Layla Parast [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hettest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hettest Testing for a Treatment Effect Using a Heterogeneous Surrogate\nMarker Tests for a treatment effect using surrogate marker information accounting for heterogeneity in the utility of the surrogate. Details are described in Parast et al (2022) <arXiv:2209.08315>.  "
  },
  {
    "id": 13928,
    "package_name": "hettx",
    "title": "Fisherian and Neymanian Methods for Detecting and Measuring\nTreatment Effect Variation",
    "description": "Implements methods developed by Ding, Feller, and Miratrix (2016) <doi:10.1111/rssb.12124> <arXiv:1412.5000>,\n    and Ding, Feller, and Miratrix (2018) <doi:10.1080/01621459.2017.1407322> <arXiv:1605.06566>\n    for testing whether there is unexplained variation in treatment effects across observations, and for characterizing\n    the extent of the explained and unexplained variation in treatment effects. The package includes wrapper functions\n    implementing the proposed methods, as well as helper functions for analyzing and visualizing the results of the test.",
    "version": "0.1.3",
    "maintainer": "Ben Fifield <benfifield@gmail.com>",
    "author": "Peng Ding [aut],\n  Avi Feller [aut],\n  Ben Fifield [aut, cre],\n  Luke Miratrix [aut]",
    "url": "",
    "bug_reports": "https://github.com/bfifield/hettx/issues",
    "repository": "https://cran.r-project.org/package=hettx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hettx Fisherian and Neymanian Methods for Detecting and Measuring\nTreatment Effect Variation Implements methods developed by Ding, Feller, and Miratrix (2016) <doi:10.1111/rssb.12124> <arXiv:1412.5000>,\n    and Ding, Feller, and Miratrix (2018) <doi:10.1080/01621459.2017.1407322> <arXiv:1605.06566>\n    for testing whether there is unexplained variation in treatment effects across observations, and for characterizing\n    the extent of the explained and unexplained variation in treatment effects. The package includes wrapper functions\n    implementing the proposed methods, as well as helper functions for analyzing and visualizing the results of the test.  "
  },
  {
    "id": 13964,
    "package_name": "highDmean",
    "title": "Testing Two-Sample Mean in High Dimension",
    "description": "Implements the high-dimensional two-sample test \n    proposed by Zhang (2019) <http://hdl.handle.net/2097/40235>. \n    It also implements the test proposed by Srivastava, Katayama, \n    and Kano (2013) <doi:10.1016/j.jmva.2012.08.014>. These tests \n    are particularly suitable to high dimensional data from two populations \n    for which the classical multivariate Hotelling's T-square test fails due \n    to sample sizes smaller than dimensionality. In this case, the ZWL and ZWLm \n    tests proposed by Zhang (2019) <http://hdl.handle.net/2097/40235>, \n    referred to as zwl_test() in this package, provide a reliable and powerful test. ",
    "version": "0.1.0",
    "maintainer": "Huaiyu Zhang <huaiyuzhang1988@gmail.com>",
    "author": "Huaiyu Zhang, Haiyan Wang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=highDmean",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "highDmean Testing Two-Sample Mean in High Dimension Implements the high-dimensional two-sample test \n    proposed by Zhang (2019) <http://hdl.handle.net/2097/40235>. \n    It also implements the test proposed by Srivastava, Katayama, \n    and Kano (2013) <doi:10.1016/j.jmva.2012.08.014>. These tests \n    are particularly suitable to high dimensional data from two populations \n    for which the classical multivariate Hotelling's T-square test fails due \n    to sample sizes smaller than dimensionality. In this case, the ZWL and ZWLm \n    tests proposed by Zhang (2019) <http://hdl.handle.net/2097/40235>, \n    referred to as zwl_test() in this package, provide a reliable and powerful test.   "
  },
  {
    "id": 13990,
    "package_name": "hint",
    "title": "Tools for Hypothesis Testing Based on Hypergeometric\nIntersection Distributions",
    "description": "Hypergeometric Intersection distributions are a\n    broad group of distributions that describe the probability of picking\n    intersections when drawing independently from two (or more) urns\n    containing variable numbers of balls belonging to the same n\n    categories. <arXiv:1305.0717>.",
    "version": "0.1-3",
    "maintainer": "Alex T. Kalinka <alex.t.kalinka@gmail.com>",
    "author": "Alex T. Kalinka",
    "url": "https://github.com/alextkalinka/hint",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hint Tools for Hypothesis Testing Based on Hypergeometric\nIntersection Distributions Hypergeometric Intersection distributions are a\n    broad group of distributions that describe the probability of picking\n    intersections when drawing independently from two (or more) urns\n    containing variable numbers of balls belonging to the same n\n    categories. <arXiv:1305.0717>.  "
  },
  {
    "id": 14031,
    "package_name": "hommel",
    "title": "Methods for Closed Testing with Simes Inequality, in Particular\nHommel's Method",
    "description": "Provides methods for closed testing using Simes local tests. In particular, calculates adjusted p-values for Hommel's multiple testing method, and provides lower confidence bounds for true discovery proportions. A robust but more conservative variant of the closed testing procedure that does not require the assumption of Simes inequality is also implemented. The methods have been described in detail in Goeman et al (Biometrika 106, 841-856, 2019).",
    "version": "1.8",
    "maintainer": "Jelle Goeman <j.j.goeman@lumc.nl>",
    "author": "Jelle Goeman [aut, cre],\n  Rosa Meijer [aut],\n  Thijmen Krebs [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hommel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hommel Methods for Closed Testing with Simes Inequality, in Particular\nHommel's Method Provides methods for closed testing using Simes local tests. In particular, calculates adjusted p-values for Hommel's multiple testing method, and provides lower confidence bounds for true discovery proportions. A robust but more conservative variant of the closed testing procedure that does not require the assumption of Simes inequality is also implemented. The methods have been described in detail in Goeman et al (Biometrika 106, 841-856, 2019).  "
  },
  {
    "id": 14032,
    "package_name": "homnormal",
    "title": "Tests of Homogeneity of Variances",
    "description": "Most common exact, asymptotic and resample based tests are provided for testing the \n            homogeneity of variances of k normal distributions under normality. \n            These tests are Barlett, Bhandary & Dai, Brown & Forsythe, Chang et al., Gokpinar & Gokpinar, Levene, Liu and Xu, Gokpinar. \n            Also, a data generation function from multiple normal distribution is provided using any multiple normal parameters.  \n            Bartlett, M. S. (1937) <doi:10.1098/rspa.1937.0109> \n            Bhandary, M., & Dai, H. (2008) <doi:10.1080/03610910802431011>\n            Brown, M. B., & Forsythe, A. B. (1974).<doi:10.1080/01621459.1974.10482955>\n            Chang, C. H., Pal, N., & Lin, J. J. (2017) <doi:10.1080/03610918.2016.1202277>\n            Gokpinar E. & Gokpinar F. (2017) <doi:10.1080/03610918.2014.955110>\n            Liu, X., & Xu, X. (2010) <doi:10.1016/j.spl.2010.05.017>\n            Levene, H. (1960) <https://cir.nii.ac.jp/crid/1573950400526848896>\n            G\u00f6kp\u0131nar, E. (2020) <doi:10.1080/03610918.2020.1800037>.",
    "version": "0.1",
    "maintainer": "Fikri G\u00f6kp\u0131nar <fikri@gazi.edu.tr>",
    "author": "Fikri G\u00f6kp\u0131nar [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6310-8727>),\n  Esra G\u00f6kp\u0131nar [aut] (ORCID: <https://orcid.org/0000-0003-2148-4940>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=homnormal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "homnormal Tests of Homogeneity of Variances Most common exact, asymptotic and resample based tests are provided for testing the \n            homogeneity of variances of k normal distributions under normality. \n            These tests are Barlett, Bhandary & Dai, Brown & Forsythe, Chang et al., Gokpinar & Gokpinar, Levene, Liu and Xu, Gokpinar. \n            Also, a data generation function from multiple normal distribution is provided using any multiple normal parameters.  \n            Bartlett, M. S. (1937) <doi:10.1098/rspa.1937.0109> \n            Bhandary, M., & Dai, H. (2008) <doi:10.1080/03610910802431011>\n            Brown, M. B., & Forsythe, A. B. (1974).<doi:10.1080/01621459.1974.10482955>\n            Chang, C. H., Pal, N., & Lin, J. J. (2017) <doi:10.1080/03610918.2016.1202277>\n            Gokpinar E. & Gokpinar F. (2017) <doi:10.1080/03610918.2014.955110>\n            Liu, X., & Xu, X. (2010) <doi:10.1016/j.spl.2010.05.017>\n            Levene, H. (1960) <https://cir.nii.ac.jp/crid/1573950400526848896>\n            G\u00f6kp\u0131nar, E. (2020) <doi:10.1080/03610918.2020.1800037>.  "
  },
  {
    "id": 14063,
    "package_name": "hrt",
    "title": "Heteroskedasticity Robust Testing",
    "description": "Functions for testing affine hypotheses on the regression coefficient vector in regression models with heteroskedastic errors: (i) a function for computing various test statistics (in particular using HC0-HC4 covariance estimators based on unrestricted or restricted residuals); (ii) a function for numerically approximating the size of a test based on such test statistics and a user-supplied critical value; and, most importantly, (iii) a function for determining size-controlling critical values for such test statistics and a user-supplied significance level (also incorporating a check of conditions under which such a size-controlling critical value exists). The three functions are based on results in Poetscher and Preinerstorfer (2021) \"Valid Heteroskedasticity Robust Testing\" <doi:10.48550/arXiv.2104.12597>, which will appear as <doi:10.1017/S0266466623000269>.",
    "version": "1.0.2",
    "maintainer": "David Preinerstorfer <david.preinerstorfer@wu.ac.at>",
    "author": "David Preinerstorfer [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hrt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hrt Heteroskedasticity Robust Testing Functions for testing affine hypotheses on the regression coefficient vector in regression models with heteroskedastic errors: (i) a function for computing various test statistics (in particular using HC0-HC4 covariance estimators based on unrestricted or restricted residuals); (ii) a function for numerically approximating the size of a test based on such test statistics and a user-supplied critical value; and, most importantly, (iii) a function for determining size-controlling critical values for such test statistics and a user-supplied significance level (also incorporating a check of conditions under which such a size-controlling critical value exists). The three functions are based on results in Poetscher and Preinerstorfer (2021) \"Valid Heteroskedasticity Robust Testing\" <doi:10.48550/arXiv.2104.12597>, which will appear as <doi:10.1017/S0266466623000269>.  "
  },
  {
    "id": 14093,
    "package_name": "httptest",
    "title": "A Test Environment for HTTP Requests",
    "description": "Testing and documenting code that communicates with remote servers\n    can be painful. Dealing with authentication, server state,\n    and other complications can make testing seem too costly to\n    bother with. But it doesn't need to be that hard. This package enables one\n    to test all of the logic on the R sides of the API in your package without\n    requiring access to the remote service. Importantly, it provides three\n    contexts that mock the network connection in different ways, as well as\n    testing functions to assert that HTTP requests were---or were\n    not---made. It also allows one to safely record real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables one to write vignettes and other dynamic documents that can be\n    distributed without access to a live server.",
    "version": "4.2.3",
    "maintainer": "Neal Richardson <neal.p.richardson@gmail.com>",
    "author": "Neal Richardson [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-7992-3520>),\n  Jonathan Keane [ctb],\n  Ma\u00eblle Salmon [ctb] (ORCID: <https://orcid.org/0000-0002-2815-0399>)",
    "url": "https://enpiar.com/r/httptest/,\nhttps://github.com/nealrichardson/httptest",
    "bug_reports": "https://github.com/nealrichardson/httptest/issues",
    "repository": "https://cran.r-project.org/package=httptest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "httptest A Test Environment for HTTP Requests Testing and documenting code that communicates with remote servers\n    can be painful. Dealing with authentication, server state,\n    and other complications can make testing seem too costly to\n    bother with. But it doesn't need to be that hard. This package enables one\n    to test all of the logic on the R sides of the API in your package without\n    requiring access to the remote service. Importantly, it provides three\n    contexts that mock the network connection in different ways, as well as\n    testing functions to assert that HTTP requests were---or were\n    not---made. It also allows one to safely record real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables one to write vignettes and other dynamic documents that can be\n    distributed without access to a live server.  "
  },
  {
    "id": 14094,
    "package_name": "httptest2",
    "title": "Test Helpers for 'httr2'",
    "description": "Testing and documenting code that communicates with remote servers\n    can be painful. This package helps with writing tests for packages that\n    use 'httr2'. It enables testing all of the logic\n    on the R sides of the API without requiring access to the\n    remote service, and it also allows recording real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables writing vignettes and other dynamic documents that can be\n    distributed without access to a live server.",
    "version": "1.2.2",
    "maintainer": "Neal Richardson <neal.p.richardson@gmail.com>",
    "author": "Neal Richardson [aut, cre] (ORCID:\n    <https://orcid.org/0009-0002-7992-3520>),\n  Jonathan Keane [ctb],\n  Ma\u00eblle Salmon [ctb] (ORCID: <https://orcid.org/0000-0002-2815-0399>)",
    "url": "https://enpiar.com/httptest2/,\nhttps://github.com/nealrichardson/httptest2",
    "bug_reports": "https://github.com/nealrichardson/httptest2/issues",
    "repository": "https://cran.r-project.org/package=httptest2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "httptest2 Test Helpers for 'httr2' Testing and documenting code that communicates with remote servers\n    can be painful. This package helps with writing tests for packages that\n    use 'httr2'. It enables testing all of the logic\n    on the R sides of the API without requiring access to the\n    remote service, and it also allows recording real API responses to use as\n    test fixtures. The ability to save responses and load them offline also\n    enables writing vignettes and other dynamic documents that can be\n    distributed without access to a live server.  "
  },
  {
    "id": 14119,
    "package_name": "hySpc.testthat",
    "title": "'testthat' Unit Test Enhancements",
    "description": "Enhance package 'testthat' by allowing tests to be attached to the function/object they test. \n    This allows to keep functional and unit test code together.",
    "version": "0.2.1",
    "maintainer": "Claudia Beleites <Claudia.Beleites@chemometrix.gmbh>",
    "author": "Claudia Beleites [aut, cre],\n  Erick Oduniyi [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hySpc.testthat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hySpc.testthat 'testthat' Unit Test Enhancements Enhance package 'testthat' by allowing tests to be attached to the function/object they test. \n    This allows to keep functional and unit test code together.  "
  },
  {
    "id": 14160,
    "package_name": "hytest",
    "title": "Hypothesis Testing Based on Neyman-Pearson Lemma and Likelihood\nRatio Test",
    "description": "Error type I and Optimal critical values to test statistical hypothesis based on Neyman-Pearson Lemma and Likelihood ratio test \n    based on random samples from several distributions. The families of distributions are Bernoulli, Exponential, Geometric, Inverse Normal, \n    Normal, Gamma, Gumbel, Lognormal, Poisson, and Weibull.\n    This package is an ideal resource to help with the teaching of Statistics.\n    The main references for this package are Casella G. and Berger R. (2003,ISBN:0-534-24312-6 , \"Statistical Inference. Second Edition\", Duxbury Press) and \n    Hogg, R., McKean, J., and Craig, A. (2019,ISBN:013468699, \"Introduction to Mathematical Statistic. Eighth edition\", Pearson). ",
    "version": "0.1.1",
    "maintainer": "Carlos Alberto Cardozo Delgado <cardozorpackages@gmail.com>",
    "author": "Carlos Alberto Cardozo Delgado [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=hytest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "hytest Hypothesis Testing Based on Neyman-Pearson Lemma and Likelihood\nRatio Test Error type I and Optimal critical values to test statistical hypothesis based on Neyman-Pearson Lemma and Likelihood ratio test \n    based on random samples from several distributions. The families of distributions are Bernoulli, Exponential, Geometric, Inverse Normal, \n    Normal, Gamma, Gumbel, Lognormal, Poisson, and Weibull.\n    This package is an ideal resource to help with the teaching of Statistics.\n    The main references for this package are Casella G. and Berger R. (2003,ISBN:0-534-24312-6 , \"Statistical Inference. Second Edition\", Duxbury Press) and \n    Hogg, R., McKean, J., and Craig, A. (2019,ISBN:013468699, \"Introduction to Mathematical Statistic. Eighth edition\", Pearson).   "
  },
  {
    "id": 14179,
    "package_name": "iGSEA",
    "title": "Integrative Gene Set Enrichment Analysis Approaches",
    "description": "To integrate multiple GSEA studies, we propose a hybrid strategy,\n    iGSEA-AT, for choosing random effects (RE) versus fixed effect (FE) models,\n    with an attempt to achieve the potential maximum statistical efficiency as \n    well as stability in performance in various practical situations. In addition\n    to iGSEA-AT, this package also provides options to perform integrative GSEA\n    with testing based on a FE model (iGSEA-FE) and testing based on a RE model\n    (iGSEA-RE). The approaches account for different set sizes when testing a\n    database of gene sets. The function is easy to use, and the three approaches\n    can be applied to both binary and continuous phenotypes. ",
    "version": "1.2",
    "maintainer": "Wentao Lu <wlu1026@yahoo.com>",
    "author": "Wentao Lu, Xinlei Wang, Xiaowei Zhan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=iGSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "iGSEA Integrative Gene Set Enrichment Analysis Approaches To integrate multiple GSEA studies, we propose a hybrid strategy,\n    iGSEA-AT, for choosing random effects (RE) versus fixed effect (FE) models,\n    with an attempt to achieve the potential maximum statistical efficiency as \n    well as stability in performance in various practical situations. In addition\n    to iGSEA-AT, this package also provides options to perform integrative GSEA\n    with testing based on a FE model (iGSEA-FE) and testing based on a RE model\n    (iGSEA-RE). The approaches account for different set sizes when testing a\n    database of gene sets. The function is easy to use, and the three approaches\n    can be applied to both binary and continuous phenotypes.   "
  },
  {
    "id": 14279,
    "package_name": "idmact",
    "title": "Interpreting Differences Between Mean ACT Scores",
    "description": "Interpreting the differences between mean scale scores across various\n    forms of an assessment can be challenging. This difficulty arises from different\n    mappings between raw scores and scale scores, complex mathematical relationships,\n    adjustments based on judgmental procedures, and diverse equating functions applied\n    to different assessment forms. An alternative method involves running simulations\n    to explore the effect of incrementing raw scores on mean scale scores. The\n    'idmact' package provides an implementation of this approach based on the\n    algorithm detailed in Schiel (1998)\n    <https://www.act.org/content/dam/act/unsecured/documents/ACT_RR98-01.pdf> which\n    was developed to help interpret differences between mean scale scores on the\n    American College Testing (ACT) assessment. The function idmact_subj() within\n    the package offers a framework for running simulations on subject-level scores.\n    In contrast, the idmact_comp() function provides a framework for conducting\n    simulations on composite scores.",
    "version": "1.0.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre, cph]",
    "url": "https://github.com/mncube/idmact",
    "bug_reports": "https://github.com/mncube/idmact/issues",
    "repository": "https://cran.r-project.org/package=idmact",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "idmact Interpreting Differences Between Mean ACT Scores Interpreting the differences between mean scale scores across various\n    forms of an assessment can be challenging. This difficulty arises from different\n    mappings between raw scores and scale scores, complex mathematical relationships,\n    adjustments based on judgmental procedures, and diverse equating functions applied\n    to different assessment forms. An alternative method involves running simulations\n    to explore the effect of incrementing raw scores on mean scale scores. The\n    'idmact' package provides an implementation of this approach based on the\n    algorithm detailed in Schiel (1998)\n    <https://www.act.org/content/dam/act/unsecured/documents/ACT_RR98-01.pdf> which\n    was developed to help interpret differences between mean scale scores on the\n    American College Testing (ACT) assessment. The function idmact_subj() within\n    the package offers a framework for running simulations on subject-level scores.\n    In contrast, the idmact_comp() function provides a framework for conducting\n    simulations on composite scores.  "
  },
  {
    "id": 14287,
    "package_name": "ieTest",
    "title": "Indirect Effects Testing Methods in Mediation Analysis",
    "description": "Used in testing if the indirect effect from linear regression mediation analysis is equal to 0. Includes established methods such as the Sobel Test, Joint Significant test (maxP), and tests based off the distribution of the Product or Normal Random Variables. Additionally, this package adds more powerful tests based on Intersection-Union theory. These tests are the S-Test, the ps-test, and the ascending squares test. These new methods are uniformly more powerful than maxP, which is more powerful than Sobel and less anti-conservative than the Product of Normal Random Variables. These methods are explored by Kidd and Lin, (2024) <doi:10.1007/s12561-023-09386-6> and Kidd et al., (2025) <doi:10.1007/s10260-024-00777-7>. ",
    "version": "2.0",
    "maintainer": "John Kidd <jkidd@uvu.edu>",
    "author": "John Kidd [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ieTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ieTest Indirect Effects Testing Methods in Mediation Analysis Used in testing if the indirect effect from linear regression mediation analysis is equal to 0. Includes established methods such as the Sobel Test, Joint Significant test (maxP), and tests based off the distribution of the Product or Normal Random Variables. Additionally, this package adds more powerful tests based on Intersection-Union theory. These tests are the S-Test, the ps-test, and the ascending squares test. These new methods are uniformly more powerful than maxP, which is more powerful than Sobel and less anti-conservative than the Product of Normal Random Variables. These methods are explored by Kidd and Lin, (2024) <doi:10.1007/s12561-023-09386-6> and Kidd et al., (2025) <doi:10.1007/s10260-024-00777-7>.   "
  },
  {
    "id": 14302,
    "package_name": "igate",
    "title": "Guided Analytics for Testing Manufacturing Parameters",
    "description": "An implementation of the initial guided analytics for parameter testing and\n    controlband extraction framework. Functions are available for continuous and \n    categorical target variables as well as for generating standardized reports of the\n    conducted analysis. See <https://github.com/stefan-stein/igate> for more information\n    on the technology.",
    "version": "0.3.3",
    "maintainer": "Stefan Stein <s.stein@warwick.ac.uk>",
    "author": "Stefan Stein [aut, cre]",
    "url": "https://github.com/stefan-stein/igate",
    "bug_reports": "https://github.com/stefan-stein/igate/issues",
    "repository": "https://cran.r-project.org/package=igate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "igate Guided Analytics for Testing Manufacturing Parameters An implementation of the initial guided analytics for parameter testing and\n    controlband extraction framework. Functions are available for continuous and \n    categorical target variables as well as for generating standardized reports of the\n    conducted analysis. See <https://github.com/stefan-stein/igate> for more information\n    on the technology.  "
  },
  {
    "id": 14354,
    "package_name": "immunaut",
    "title": "Machine Learning Immunogenicity and Vaccine Response Analysis",
    "description": "Used for analyzing immune responses and predicting vaccine efficacy using machine learning and advanced data processing techniques. 'Immunaut' integrates both unsupervised and supervised learning methods, managing outliers and capturing immune response variability. It performs multiple rounds of predictive model testing to identify robust immunogenicity signatures that can predict vaccine responsiveness. The platform is designed to handle high-dimensional immune data, enabling researchers to uncover immune predictors and refine personalized vaccination strategies across diverse populations.",
    "version": "1.0.2",
    "maintainer": "Ivan Tomic <info@ivantomic.com>",
    "author": "Ivan Tomic [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3596-681X>),\n  Adriana Tomic [aut, ctb, cph, fnd] (ORCID:\n    <https://orcid.org/0000-0001-9885-3535>),\n  Stephanie Hao [aut] (ORCID: <https://orcid.org/0000-0002-3760-8234>)",
    "url": "https://github.com/atomiclaboratory/immunaut,\n<https://atomic-lab.org>",
    "bug_reports": "https://github.com/atomiclaboratory/immunaut/issues",
    "repository": "https://cran.r-project.org/package=immunaut",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "immunaut Machine Learning Immunogenicity and Vaccine Response Analysis Used for analyzing immune responses and predicting vaccine efficacy using machine learning and advanced data processing techniques. 'Immunaut' integrates both unsupervised and supervised learning methods, managing outliers and capturing immune response variability. It performs multiple rounds of predictive model testing to identify robust immunogenicity signatures that can predict vaccine responsiveness. The platform is designed to handle high-dimensional immune data, enabling researchers to uncover immune predictors and refine personalized vaccination strategies across diverse populations.  "
  },
  {
    "id": 14367,
    "package_name": "imprecise101",
    "title": "Introduction to Imprecise Probabilities",
    "description": "An imprecise inference presented in the study of Walley (1996) <doi:10.1111/j.2517-6161.1996.tb02065.x> is one of the statistical reasoning methods when prior information is unavailable. Functions and utils needed for illustrating this inferential paradigm are implemented for classroom teaching and further comprehensive research. Two imprecise models are demonstrated using multinomial data and 2x2 contingency table data. The concepts of prior ignorance and imprecision are discussed in lower and upper probabilities. Representation invariance principle, hypothesis testing, decision-making, and further generalization are also illustrated.",
    "version": "0.2.2.4",
    "maintainer": "Chel Hee Lee <chelhee.lee@ucalgary.ca>",
    "author": "Chel Hee Lee [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8209-8176>),\n  Mikelis Bickis [ctb],\n  Angela McCourt [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=imprecise101",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "imprecise101 Introduction to Imprecise Probabilities An imprecise inference presented in the study of Walley (1996) <doi:10.1111/j.2517-6161.1996.tb02065.x> is one of the statistical reasoning methods when prior information is unavailable. Functions and utils needed for illustrating this inferential paradigm are implemented for classroom teaching and further comprehensive research. Two imprecise models are demonstrated using multinomial data and 2x2 contingency table data. The concepts of prior ignorance and imprecision are discussed in lower and upper probabilities. Representation invariance principle, hypothesis testing, decision-making, and further generalization are also illustrated.  "
  },
  {
    "id": 14433,
    "package_name": "informedSen",
    "title": "Sensitivity Analysis Informed by a Test for Bias",
    "description": "After testing for biased treatment assignment in an observational study using an unaffected outcome, the sensitivity analysis is constrained to be compatible with that test.  The package uses the optimization software gurobi obtainable from <https://www.gurobi.com/>, together with its associated R package, also called gurobi; see: <https://www.gurobi.com/documentation/7.0/refman/installing_the_r_package.html>.  The method is a substantial computational and practical enhancement of a concept introduced in Rosenbaum (1992) Detecting bias with confidence in observational studies Biometrika, 79(2), 367-374  <doi:10.1093/biomet/79.2.367>.",
    "version": "1.0.7",
    "maintainer": "Paul R Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=informedSen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "informedSen Sensitivity Analysis Informed by a Test for Bias After testing for biased treatment assignment in an observational study using an unaffected outcome, the sensitivity analysis is constrained to be compatible with that test.  The package uses the optimization software gurobi obtainable from <https://www.gurobi.com/>, together with its associated R package, also called gurobi; see: <https://www.gurobi.com/documentation/7.0/refman/installing_the_r_package.html>.  The method is a substantial computational and practical enhancement of a concept introduced in Rosenbaum (1992) Detecting bias with confidence in observational studies Biometrika, 79(2), 367-374  <doi:10.1093/biomet/79.2.367>.  "
  },
  {
    "id": 14451,
    "package_name": "inphr",
    "title": "Statistical Inference for Persistence Homology Data",
    "description": "A set of functions for performing null hypothesis testing on\n    samples of persistence diagrams using the theory of permutations. Currently,\n    only two-sample testing is implemented. Inputs can be either samples of\n    persistence diagrams themselves or vectorizations. In the former case, they\n    are embedded in a metric space using either the Bottleneck or Wasserstein\n    distance. In the former case, persistence data becomes functional data and\n    inference is performed using tools available in the 'fdatest' package. Main\n    reference for the interval-wise testing method: Pini A., Vantini S. (2017)\n    \"Interval-wise testing for functional data\" <doi:10.1080/10485252.2017.1306627>.\n    Main reference for inference on populations of networks: Lovato, I., Pini, A.,\n    Stamm, A., & Vantini, S. (2020) \"Model-free two-sample test for network-valued\n    data\" <doi:10.1016/j.csda.2019.106896>.",
    "version": "0.0.1",
    "maintainer": "Aymeric Stamm <aymeric.stamm@cnrs.fr>",
    "author": "Aymeric Stamm [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8725-3654>)",
    "url": "https://github.com/tdaverse/inphr,\nhttps://tdaverse.github.io/inphr/",
    "bug_reports": "https://github.com/tdaverse/inphr/issues",
    "repository": "https://cran.r-project.org/package=inphr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "inphr Statistical Inference for Persistence Homology Data A set of functions for performing null hypothesis testing on\n    samples of persistence diagrams using the theory of permutations. Currently,\n    only two-sample testing is implemented. Inputs can be either samples of\n    persistence diagrams themselves or vectorizations. In the former case, they\n    are embedded in a metric space using either the Bottleneck or Wasserstein\n    distance. In the former case, persistence data becomes functional data and\n    inference is performed using tools available in the 'fdatest' package. Main\n    reference for the interval-wise testing method: Pini A., Vantini S. (2017)\n    \"Interval-wise testing for functional data\" <doi:10.1080/10485252.2017.1306627>.\n    Main reference for inference on populations of networks: Lovato, I., Pini, A.,\n    Stamm, A., & Vantini, S. (2020) \"Model-free two-sample test for network-valued\n    data\" <doi:10.1016/j.csda.2019.106896>.  "
  },
  {
    "id": 14465,
    "package_name": "insuranceData",
    "title": "A Collection of Insurance Datasets Useful in Risk Classification\nin Non-life Insurance",
    "description": "Insurance datasets, which are often used in claims severity and claims frequency modelling. It helps testing new regression models in those problems, such as GLM, GLMM, HGLM, non-linear mixed models etc. Most of the data sets are applied in the project \"Mixed models in ratemaking\" supported by grant NN 111461540 from Polish National Science Center.    ",
    "version": "1.0",
    "maintainer": "Alicja Wolny--Dominiak <alicja.wolny-dominiak@ue.katowice.pl>",
    "author": "Alicja Wolny--Dominiak and Michal Trzesiok",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=insuranceData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "insuranceData A Collection of Insurance Datasets Useful in Risk Classification\nin Non-life Insurance Insurance datasets, which are often used in claims severity and claims frequency modelling. It helps testing new regression models in those problems, such as GLM, GLMM, HGLM, non-linear mixed models etc. Most of the data sets are applied in the project \"Mixed models in ratemaking\" supported by grant NN 111461540 from Polish National Science Center.      "
  },
  {
    "id": 14477,
    "package_name": "intendo",
    "title": "A Group of Fun Datasets of Various Sizes and Differing Levels of\nQuality",
    "description": "Four datasets are provided here from the 'Intendo' game\n    'Super Jetroid'. It is data from the 2015 year of operation and it comprises\n    a revenue table ('all_revenue'), a daily users table ('users_daily'), a user\n    summary table ('user_summary'), and a table with data on all user sessions\n    ('all_sessions'). These core datasets come in different sizes, and, each of\n    them has a variant that was intentionally made faulty (totally riddled with\n    errors and inconsistencies). This suite of tables is useful for testing with\n    packages that focus on data validation and data documentation.",
    "version": "0.1.1",
    "maintainer": "Richard Iannone <riannone@me.com>",
    "author": "Richard Iannone [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3925-190X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=intendo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intendo A Group of Fun Datasets of Various Sizes and Differing Levels of\nQuality Four datasets are provided here from the 'Intendo' game\n    'Super Jetroid'. It is data from the 2015 year of operation and it comprises\n    a revenue table ('all_revenue'), a daily users table ('users_daily'), a user\n    summary table ('user_summary'), and a table with data on all user sessions\n    ('all_sessions'). These core datasets come in different sizes, and, each of\n    them has a variant that was intentionally made faulty (totally riddled with\n    errors and inconsistencies). This suite of tables is useful for testing with\n    packages that focus on data validation and data documentation.  "
  },
  {
    "id": 14506,
    "package_name": "intervcomp",
    "title": "Hypothesis Testing Using the Overlapping Interval Estimates",
    "description": "Performs hypothesis testing\n    using the interval estimates (e.g., confidence intervals). The\n    non-overlapping interval estimates indicates the statistical\n    significance. References to these procedures can be found at\n    Noguchi and Marmolejo-Ramos (2016) <doi:10.1080/00031305.2016.1200487>,\n    Bonett and Seier (2003) <doi:10.1198/0003130032323>, and\n    Lemm (2006) <doi:10.1300/J082v51n02_05>.",
    "version": "0.1.2",
    "maintainer": "Kimihiro Noguchi <kimihiro.noguchi@wwu.edu>",
    "author": "Kimihiro Noguchi [aut, cre],\n  Ryan Erps [ctb],\n  Chris Murphy [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=intervcomp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intervcomp Hypothesis Testing Using the Overlapping Interval Estimates Performs hypothesis testing\n    using the interval estimates (e.g., confidence intervals). The\n    non-overlapping interval estimates indicates the statistical\n    significance. References to these procedures can be found at\n    Noguchi and Marmolejo-Ramos (2016) <doi:10.1080/00031305.2016.1200487>,\n    Bonett and Seier (2003) <doi:10.1198/0003130032323>, and\n    Lemm (2006) <doi:10.1300/J082v51n02_05>.  "
  },
  {
    "id": 14513,
    "package_name": "intrinsicFRP",
    "title": "An R Package for Factor Model Asset Pricing",
    "description": "Functions for evaluating and testing asset pricing models, including\n    estimation and testing of factor risk premia, selection of \"strong\" risk \n    factors (factors having nonzero population correlation with test asset\n    returns), heteroskedasticity and autocorrelation robust covariance matrix\n    estimation and testing for model misspecification and identification. \n    The functions for estimating and testing factor risk \n    premia implement the Fama-MachBeth (1973) <doi:10.1086/260061> two-pass \n    approach, the misspecification-robust approaches of Kan-Robotti-Shanken (2013) \n    <doi:10.1111/jofi.12035>, and the approaches based on tradable factor risk\n    premia of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683>. The \n    functions for selecting the \"strong\" risk factors are based on the Oracle\n    estimator of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683> and the \n    factor screening procedure of Gospodinov-Kan-Robotti (2014) <doi:10.2139/ssrn.2579821>. \n    The functions for evaluating model misspecification implement the HJ\n    model misspecification distance of Kan-Robotti (2008) <doi:10.1016/j.jempfin.2008.03.003>,\n    which is a modification of the prominent Hansen-Jagannathan (1997)\n    <doi:10.1111/j.1540-6261.1997.tb04813.x> distance.\n    The functions for testing model identification \n    specialize the Kleibergen-Paap (2006) <doi:10.1016/j.jeconom.2005.02.011> \n    and the Chen-Fang (2019) <doi:10.1111/j.1540-6261.1997.tb04813.x> rank test \n    to the regression coefficient matrix of test asset returns on risk factors.\n    Finally, the function for heteroskedasticity and autocorrelation robust \n    covariance estimation implements the Newey-West (1994) <doi:10.2307/2297912>\n    covariance estimator.",
    "version": "2.1.0",
    "maintainer": "Alberto Quaini <alberto91quaini@gmail.com>",
    "author": "Alberto Quaini [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-1251-0599>)",
    "url": "https://github.com/a91quaini/intrinsicFRP",
    "bug_reports": "https://github.com/a91quaini/intrinsicFRP/issues",
    "repository": "https://cran.r-project.org/package=intrinsicFRP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "intrinsicFRP An R Package for Factor Model Asset Pricing Functions for evaluating and testing asset pricing models, including\n    estimation and testing of factor risk premia, selection of \"strong\" risk \n    factors (factors having nonzero population correlation with test asset\n    returns), heteroskedasticity and autocorrelation robust covariance matrix\n    estimation and testing for model misspecification and identification. \n    The functions for estimating and testing factor risk \n    premia implement the Fama-MachBeth (1973) <doi:10.1086/260061> two-pass \n    approach, the misspecification-robust approaches of Kan-Robotti-Shanken (2013) \n    <doi:10.1111/jofi.12035>, and the approaches based on tradable factor risk\n    premia of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683>. The \n    functions for selecting the \"strong\" risk factors are based on the Oracle\n    estimator of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683> and the \n    factor screening procedure of Gospodinov-Kan-Robotti (2014) <doi:10.2139/ssrn.2579821>. \n    The functions for evaluating model misspecification implement the HJ\n    model misspecification distance of Kan-Robotti (2008) <doi:10.1016/j.jempfin.2008.03.003>,\n    which is a modification of the prominent Hansen-Jagannathan (1997)\n    <doi:10.1111/j.1540-6261.1997.tb04813.x> distance.\n    The functions for testing model identification \n    specialize the Kleibergen-Paap (2006) <doi:10.1016/j.jeconom.2005.02.011> \n    and the Chen-Fang (2019) <doi:10.1111/j.1540-6261.1997.tb04813.x> rank test \n    to the regression coefficient matrix of test asset returns on risk factors.\n    Finally, the function for heteroskedasticity and autocorrelation robust \n    covariance estimation implements the Newey-West (1994) <doi:10.2307/2297912>\n    covariance estimator.  "
  },
  {
    "id": 14533,
    "package_name": "ioncopy",
    "title": "Calling Copy Number Alterations in Amplicon Sequencing Data",
    "description": "Method for the calculation of copy numbers and calling of copy number alterations. The algorithm uses coverage data from amplicon sequencing of a sample cohort as input. The method includes significance assessment, correction for multiple testing and does not depend on normal DNA controls. Budczies (2016 Mar 15) <doi:10.18632/oncotarget.7451>.",
    "version": "2.2.2",
    "maintainer": "Jan Budczies <jan.budczies@med.uni-heidelberg.de>",
    "author": "Jan Budczies, Eva Romanovsky",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ioncopy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ioncopy Calling Copy Number Alterations in Amplicon Sequencing Data Method for the calculation of copy numbers and calling of copy number alterations. The algorithm uses coverage data from amplicon sequencing of a sample cohort as input. The method includes significance assessment, correction for multiple testing and does not depend on normal DNA controls. Budczies (2016 Mar 15) <doi:10.18632/oncotarget.7451>.  "
  },
  {
    "id": 14588,
    "package_name": "irt",
    "title": "Item Response Theory and Computerized Adaptive Testing Functions",
    "description": "A collection of Item Response Theory (IRT) and Computerized \n     Adaptive Testing (CAT) functions that are used in psychometrics. ",
    "version": "0.2.9",
    "maintainer": "Emre Gonulates <egonulates@gmail.com>",
    "author": "Emre Gonulates [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3834-3266>)",
    "url": "https://github.com/egonulates/irt",
    "bug_reports": "https://github.com/egonulates/irt/issues",
    "repository": "https://cran.r-project.org/package=irt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "irt Item Response Theory and Computerized Adaptive Testing Functions A collection of Item Response Theory (IRT) and Computerized \n     Adaptive Testing (CAT) functions that are used in psychometrics.   "
  },
  {
    "id": 14604,
    "package_name": "islasso",
    "title": "The Induced Smoothed Lasso",
    "description": "An implementation of the induced smoothing (IS) idea to lasso regularization models to allow estimation and inference on the model coefficients (currently hypothesis testing only). Linear, logistic, Poisson and gamma regressions with several link functions are implemented. The algorithm is described in the original paper; see <doi:10.1177/0962280219842890> and discussed in a tutorial <doi:10.13140/RG.2.2.16360.11521>.",
    "version": "1.6.2",
    "maintainer": "Gianluca Sottile <gianluca.sottile@unipa.it>",
    "author": "Gianluca Sottile [aut, cre],\n  Giovanna Cilluffo [aut, ctb],\n  Vito MR Muggeo [aut, ctb]",
    "url": "https://gianluca-sottile.github.io/islasso/",
    "bug_reports": "https://github.com/gianluca-sottile/islasso/issues",
    "repository": "https://cran.r-project.org/package=islasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "islasso The Induced Smoothed Lasso An implementation of the induced smoothing (IS) idea to lasso regularization models to allow estimation and inference on the model coefficients (currently hypothesis testing only). Linear, logistic, Poisson and gamma regressions with several link functions are implemented. The algorithm is described in the original paper; see <doi:10.1177/0962280219842890> and discussed in a tutorial <doi:10.13140/RG.2.2.16360.11521>.  "
  },
  {
    "id": 14658,
    "package_name": "ivdesign",
    "title": "Hypothesis Testing in Cluster-Randomized Encouragement Designs",
    "description": "An implementation of randomization-based hypothesis \n    testing for three different estimands in a cluster-randomized \n    encouragement experiment. The three estimands include (1) testing\n    a cluster-level constant proportional treatment effect (Fisher's\n    sharp null hypothesis), (2) pooled effect ratio, and (3) average \n    cluster effect ratio. To test the third estimand, user needs to install\n    'Gurobi' (>= 9.0.1) optimizer via its R API. Please refer to \n    <https://www.gurobi.com/documentation/9.0/refman/ins_the_r_package.html>.",
    "version": "0.1.0",
    "maintainer": "Bo Zhang <bozhan@wharton.upenn.edu>",
    "author": "Bo Zhang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ivdesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ivdesign Hypothesis Testing in Cluster-Randomized Encouragement Designs An implementation of randomization-based hypothesis \n    testing for three different estimands in a cluster-randomized \n    encouragement experiment. The three estimands include (1) testing\n    a cluster-level constant proportional treatment effect (Fisher's\n    sharp null hypothesis), (2) pooled effect ratio, and (3) average \n    cluster effect ratio. To test the third estimand, user needs to install\n    'Gurobi' (>= 9.0.1) optimizer via its R API. Please refer to \n    <https://www.gurobi.com/documentation/9.0/refman/ins_the_r_package.html>.  "
  },
  {
    "id": 14683,
    "package_name": "jackstraw",
    "title": "Statistical Inference for Unsupervised Learning",
    "description": "Test for association between the observed data and their estimated latent variables. The jackstraw package provides a resampling strategy and testing scheme to estimate statistical significance of association between the observed data and their latent variables. Depending on the data type and the analysis aim, the latent variables may be estimated by principal component analysis (PCA), factor analysis (FA), K-means clustering, and related unsupervised learning algorithms. The jackstraw methods learn over-fitting characteristics inherent in this circular analysis, where the observed data are used to estimate the latent variables and used again to test against that estimated latent variables. When latent variables are estimated by PCA, the jackstraw enables statistical testing for association between observed variables and latent variables, as estimated by low-dimensional principal components (PCs). This essentially leads to identifying variables that are significantly associated with PCs. Similarly, unsupervised clustering, such as K-means clustering, partition around medoids (PAM), and others, finds coherent groups in high-dimensional data. The jackstraw estimates statistical significance of cluster membership, by testing association between data and cluster centers. Clustering membership can be improved by using the resulting jackstraw p-values and posterior inclusion probabilities (PIPs), with an application to unsupervised evaluation of cell identities in single cell RNA-seq (scRNA-seq).",
    "version": "1.3.17",
    "maintainer": "Neo Christopher Chung <nchchung@gmail.com>",
    "author": "Neo Christopher Chung [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6798-8867>),\n  John D. Storey [aut] (ORCID: <https://orcid.org/0000-0001-5992-402X>),\n  Wei Hao [aut],\n  Alejandro Ochoa [aut] (ORCID: <https://orcid.org/0000-0003-4928-3403>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jackstraw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jackstraw Statistical Inference for Unsupervised Learning Test for association between the observed data and their estimated latent variables. The jackstraw package provides a resampling strategy and testing scheme to estimate statistical significance of association between the observed data and their latent variables. Depending on the data type and the analysis aim, the latent variables may be estimated by principal component analysis (PCA), factor analysis (FA), K-means clustering, and related unsupervised learning algorithms. The jackstraw methods learn over-fitting characteristics inherent in this circular analysis, where the observed data are used to estimate the latent variables and used again to test against that estimated latent variables. When latent variables are estimated by PCA, the jackstraw enables statistical testing for association between observed variables and latent variables, as estimated by low-dimensional principal components (PCs). This essentially leads to identifying variables that are significantly associated with PCs. Similarly, unsupervised clustering, such as K-means clustering, partition around medoids (PAM), and others, finds coherent groups in high-dimensional data. The jackstraw estimates statistical significance of cluster membership, by testing association between data and cluster centers. Clustering membership can be improved by using the resulting jackstraw p-values and posterior inclusion probabilities (PIPs), with an application to unsupervised evaluation of cell identities in single cell RNA-seq (scRNA-seq).  "
  },
  {
    "id": 14710,
    "package_name": "jetty",
    "title": "Execute R in a 'Docker' Context",
    "description": "The goal of 'jetty' is to execute R functions and code\n    snippets in an isolated R subprocess within a 'Docker' container\n    and return the evaluated results to the local R session. 'jetty'\n    can install necessary packages at runtime and seamlessly propagates\n    errors and outputs from the 'Docker' subprocess back to the main\n    session. 'jetty' is primarily designed for sandboxed testing and\n    quick execution of example code.",
    "version": "0.2.2",
    "maintainer": "Daniel Molitor <molitdj97@gmail.com>",
    "author": "Daniel Molitor [aut, cph, cre]",
    "url": "https://github.com/dmolitor/jetty, http://www.dmolitor.com/jetty/",
    "bug_reports": "https://github.com/dmolitor/jetty/issues",
    "repository": "https://cran.r-project.org/package=jetty",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jetty Execute R in a 'Docker' Context The goal of 'jetty' is to execute R functions and code\n    snippets in an isolated R subprocess within a 'Docker' container\n    and return the evaluated results to the local R session. 'jetty'\n    can install necessary packages at runtime and seamlessly propagates\n    errors and outputs from the 'Docker' subprocess back to the main\n    session. 'jetty' is primarily designed for sandboxed testing and\n    quick execution of example code.  "
  },
  {
    "id": 14730,
    "package_name": "job",
    "title": "Run Code as an RStudio Job - Free Your Console",
    "description": "Call job::job({<code here>}) to run R code as an RStudio job and keep your console free in the meantime. This allows for a productive workflow while testing (multiple) long-running chunks of code. It can also be used to organize results using the RStudio Jobs GUI or to test code in a clean environment. Two RStudio Addins can be used to run selected code as a job.",
    "version": "0.3.1",
    "maintainer": "Jonas Kristoffer Lindel\u00f8v <jonas@lindeloev.dk>",
    "author": "Jonas Kristoffer Lindel\u00f8v [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4565-0595>)",
    "url": "https://lindeloev.github.io/job/",
    "bug_reports": "https://github.com/lindeloev/job/issues",
    "repository": "https://cran.r-project.org/package=job",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "job Run Code as an RStudio Job - Free Your Console Call job::job({<code here>}) to run R code as an RStudio job and keep your console free in the meantime. This allows for a productive workflow while testing (multiple) long-running chunks of code. It can also be used to organize results using the RStudio Jobs GUI or to test code in a clean environment. Two RStudio Addins can be used to run selected code as a job.  "
  },
  {
    "id": 14745,
    "package_name": "jointest",
    "title": "Multivariate Testing Through Joint Resampling-Based Tests",
    "description": "Runs resampling-based tests jointly, e.g., sign-flip score tests from Hemerik et al., (2020) <doi:10.1111/rssb.12369>, to allow for multivariate testing, i.e., weak and strong control of the Familywise Error Rate or True Discovery Proportion.",
    "version": "1.0",
    "maintainer": "Livio Finos <livio.finos@unipd.it>",
    "author": "Livio Finos [cre, aut] (ORCID: <https://orcid.org/0000-0003-3181-8078>),\n  Angela Andreella [aut],\n  Filippo Gambarota [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=jointest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jointest Multivariate Testing Through Joint Resampling-Based Tests Runs resampling-based tests jointly, e.g., sign-flip score tests from Hemerik et al., (2020) <doi:10.1111/rssb.12369>, to allow for multivariate testing, i.e., weak and strong control of the Familywise Error Rate or True Discovery Proportion.  "
  },
  {
    "id": 14777,
    "package_name": "jsonlite",
    "title": "A Simple and Robust JSON Parser and Generator for R",
    "description": "A reasonably fast JSON parser and generator, optimized for statistical \n    data and the web. Offers simple, flexible tools for working with JSON in R, and\n    is particularly powerful for building pipelines and interacting with a web API. \n    The implementation is based on the mapping described in the vignette (Ooms, 2014).\n    In addition to converting JSON data from/to R objects, 'jsonlite' contains \n    functions to stream, validate, and prettify JSON data. The unit tests included \n    with the package verify that all edge cases are encoded and decoded consistently \n    for use with dynamic data in systems and applications.",
    "version": "2.0.0",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre] (ORCID: <https://orcid.org/0000-0002-4035-0289>),\n  Duncan Temple Lang [ctb],\n  Lloyd Hilaiel [cph] (author of bundled libyajl)",
    "url": "https://jeroen.r-universe.dev/jsonlite\nhttps://arxiv.org/abs/1403.2805",
    "bug_reports": "https://github.com/jeroen/jsonlite/issues",
    "repository": "https://cran.r-project.org/package=jsonlite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "jsonlite A Simple and Robust JSON Parser and Generator for R A reasonably fast JSON parser and generator, optimized for statistical \n    data and the web. Offers simple, flexible tools for working with JSON in R, and\n    is particularly powerful for building pipelines and interacting with a web API. \n    The implementation is based on the mapping described in the vignette (Ooms, 2014).\n    In addition to converting JSON data from/to R objects, 'jsonlite' contains \n    functions to stream, validate, and prettify JSON data. The unit tests included \n    with the package verify that all edge cases are encoded and decoded consistently \n    for use with dynamic data in systems and applications.  "
  },
  {
    "id": 14812,
    "package_name": "kardl",
    "title": "Make Symmetric and Asymmetric ARDL Estimations",
    "description": "Implements estimation procedures for Autoregressive Distributed Lag (ARDL) \n    and Nonlinear ARDL (NARDL) models, which allow researchers to investigate both \n    short- and long-run relationships in time series data under mixed orders of integration. \n    The package supports simultaneous modeling of symmetric and asymmetric regressors, \n    flexible treatment of short-run and long-run asymmetries, and automated equation handling. \n    It includes several cointegration testing approaches such as the Pesaran-Shin-Smith F \n    and t bounds tests, the Banerjee error correction test, and the restricted ECM test, \n    together with diagnostic tools including Wald tests for asymmetry, ARCH tests, and \n    stability procedures (CUSUM and CUSUMQ). Methodological foundations are provided in \n    Pesaran, Shin, and Smith (2001) <doi:10.1016/S0304-4076(01)00049-5> and Shin, Yu, and \n    Greenwood-Nimmo (2014, ISBN:9780123855079).",
    "version": "0.1.1",
    "maintainer": "Huseyin Karamelikli <hakperest@gmail.com>",
    "author": "Huseyin Karamelikli [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7622-0972>),\n  Huseyin Utku Demir [aut] (ORCID:\n    <https://orcid.org/0000-0002-9140-0362>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kardl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kardl Make Symmetric and Asymmetric ARDL Estimations Implements estimation procedures for Autoregressive Distributed Lag (ARDL) \n    and Nonlinear ARDL (NARDL) models, which allow researchers to investigate both \n    short- and long-run relationships in time series data under mixed orders of integration. \n    The package supports simultaneous modeling of symmetric and asymmetric regressors, \n    flexible treatment of short-run and long-run asymmetries, and automated equation handling. \n    It includes several cointegration testing approaches such as the Pesaran-Shin-Smith F \n    and t bounds tests, the Banerjee error correction test, and the restricted ECM test, \n    together with diagnostic tools including Wald tests for asymmetry, ARCH tests, and \n    stability procedures (CUSUM and CUSUMQ). Methodological foundations are provided in \n    Pesaran, Shin, and Smith (2001) <doi:10.1016/S0304-4076(01)00049-5> and Shin, Yu, and \n    Greenwood-Nimmo (2014, ISBN:9780123855079).  "
  },
  {
    "id": 14837,
    "package_name": "kerTests",
    "title": "Generalized Kernel Two-Sample Tests",
    "description": "New kernel-based test and fast tests for testing whether two samples are from the same distribution. They work well particularly for high-dimensional data.\n      Song, H. and Chen, H. (2023) \n      <arXiv:2011.06127>.",
    "version": "0.1.4",
    "maintainer": "Hoseung Song <hosong@ucdavis.edu>",
    "author": "Hoseung Song [aut, cre],\n  Hao Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=kerTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kerTests Generalized Kernel Two-Sample Tests New kernel-based test and fast tests for testing whether two samples are from the same distribution. They work well particularly for high-dimensional data.\n      Song, H. and Chen, H. (2023) \n      <arXiv:2011.06127>.  "
  },
  {
    "id": 14950,
    "package_name": "krm",
    "title": "Kernel Based Regression Models",
    "description": "Implements several methods for testing the variance component parameter in regression models that contain kernel-based random effects, including a maximum of adjusted scores test. Several kernels are supported, including a profile hidden Markov model mutual information kernel for protein sequence. This package is described in Fong et al. (2015) <DOI:10.1093/biostatistics/kxu056>.",
    "version": "2022.10-17",
    "maintainer": "Youyi Fong <youyifong@gmail.com>",
    "author": "Youyi Fong [cre],\n  Saheli Datta [aut],\n  Krisztian Sebestyen [aut],\n  Steve Park [ctb],\n  Dave Geyer [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=krm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "krm Kernel Based Regression Models Implements several methods for testing the variance component parameter in regression models that contain kernel-based random effects, including a maximum of adjusted scores test. Several kernels are supported, including a profile hidden Markov model mutual information kernel for protein sequence. This package is described in Fong et al. (2015) <DOI:10.1093/biostatistics/kxu056>.  "
  },
  {
    "id": 14979,
    "package_name": "laGP",
    "title": "Local Approximate Gaussian Process Regression",
    "description": "Performs approximate GP regression for large computer experiments and spatial datasets.  The approximation is based on finding small local designs for prediction (independently) at particular inputs. OpenMP and SNOW parallelization are supported for prediction over a vast out-of-sample testing set; GPU acceleration is also supported for an important subroutine.  OpenMP and GPU features may require special compilation.  An interface to lower-level (full) GP inference and prediction is provided. Wrapper routines for blackbox optimization under mixed equality and inequality constraints via an augmented Lagrangian scheme, and for large scale computer model calibration, are also provided.  For details and tutorial, see Gramacy (2016 <doi:10.18637/jss.v072.i01>.",
    "version": "1.5-9",
    "maintainer": "Robert B. Gramacy  <rbg@vt.edu>",
    "author": "Robert B. Gramacy <rbg@vt.edu>, Furong Sun <furongs@vt.edu>",
    "url": "https://bobby.gramacy.com/r_packages/laGP/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=laGP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "laGP Local Approximate Gaussian Process Regression Performs approximate GP regression for large computer experiments and spatial datasets.  The approximation is based on finding small local designs for prediction (independently) at particular inputs. OpenMP and SNOW parallelization are supported for prediction over a vast out-of-sample testing set; GPU acceleration is also supported for an important subroutine.  OpenMP and GPU features may require special compilation.  An interface to lower-level (full) GP inference and prediction is provided. Wrapper routines for blackbox optimization under mixed equality and inequality constraints via an augmented Lagrangian scheme, and for large scale computer model calibration, are also provided.  For details and tutorial, see Gramacy (2016 <doi:10.18637/jss.v072.i01>.  "
  },
  {
    "id": 15011,
    "package_name": "lamme",
    "title": "Log-Analytic Methods for Multiplicative Effects",
    "description": "Log-analytic methods intended for testing multiplicative effects.",
    "version": "0.0.1",
    "maintainer": "Qimin Liu <qliu6@nd.edu>",
    "author": "Qimin Liu [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lamme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lamme Log-Analytic Methods for Multiplicative Effects Log-analytic methods intended for testing multiplicative effects.  "
  },
  {
    "id": 15218,
    "package_name": "lineartestr",
    "title": "Linear Specification Testing",
    "description": "Tests whether the linear hypothesis of a model\n    is correct specified using Dominguez-Lobato test. Also Ramsey's RESET (Regression Equation \n    Specification Error Test) test is implemented and Wald tests can be carried out. \n    Although RESET test is widely used to \n    test the linear hypothesis of a model, Dominguez and Lobato (2019) proposed a novel \n    approach that generalizes well known specification tests such as Ramsey's. This test \n    relies on wild-bootstrap; this package implements this approach to be \n    usable with any function that fits linear models and is compatible with \n    the update() function such as 'stats'::lm(), 'lfe'::felm() and 'forecast'::Arima(), \n    for ARMA (autoregressive\u2013moving-average) models. \n    Also the package can handle custom statistics such as Cramer von Mises and Kolmogorov \n    Smirnov, described by the authors, and custom distributions such as Mammen (discrete \n    and continuous) and Rademacher.\n    Manuel A. Dominguez & Ignacio N. Lobato (2019) <doi:10.1080/07474938.2019.1687116>.",
    "version": "1.0.0",
    "maintainer": "Federico Garza <fede.garza.ramirez@gmail.com>",
    "author": "Federico Garza [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7015-8186>)",
    "url": "https://github.com/FedericoGarza/lineartestr",
    "bug_reports": "https://github.com/FedericoGarza/lineartestr/issues",
    "repository": "https://cran.r-project.org/package=lineartestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lineartestr Linear Specification Testing Tests whether the linear hypothesis of a model\n    is correct specified using Dominguez-Lobato test. Also Ramsey's RESET (Regression Equation \n    Specification Error Test) test is implemented and Wald tests can be carried out. \n    Although RESET test is widely used to \n    test the linear hypothesis of a model, Dominguez and Lobato (2019) proposed a novel \n    approach that generalizes well known specification tests such as Ramsey's. This test \n    relies on wild-bootstrap; this package implements this approach to be \n    usable with any function that fits linear models and is compatible with \n    the update() function such as 'stats'::lm(), 'lfe'::felm() and 'forecast'::Arima(), \n    for ARMA (autoregressive\u2013moving-average) models. \n    Also the package can handle custom statistics such as Cramer von Mises and Kolmogorov \n    Smirnov, described by the authors, and custom distributions such as Mammen (discrete \n    and continuous) and Rademacher.\n    Manuel A. Dominguez & Ignacio N. Lobato (2019) <doi:10.1080/07474938.2019.1687116>.  "
  },
  {
    "id": 15225,
    "package_name": "lingdist",
    "title": "Fast Linguistic Distance and Alignment Computation",
    "description": "A fast generalized edit distance and string alignment computation mainly for linguistic aims. As a generalization to the classic edit distance algorithms, the package allows users to define custom cost for every symbol's insertion, deletion, and substitution. The package also allows character combinations in any length to be seen as a single symbol which is very useful for International Phonetic Alphabet (IPA) transcriptions with diacritics. In addition to edit distance result, users can get detailed alignment information such as all possible alignment scenarios between two strings which is useful for testing, illustration or any further usage. Either the distance matrix or its long table form can be obtained and tools to do such conversions are provided. All functions in the package are implemented in 'C++' and the distance matrix computation is parallelized leveraging the 'RcppThread' package.",
    "version": "1.0",
    "maintainer": "Chao Kong <kongchao1998@gmail.com>",
    "author": "Chao Kong [aut, cre] (ORCID: <https://orcid.org/0000-0002-6404-6142>)",
    "url": "https://github.com/fncokg/lingdist",
    "bug_reports": "https://github.com/fncokg/lingdist/issues",
    "repository": "https://cran.r-project.org/package=lingdist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lingdist Fast Linguistic Distance and Alignment Computation A fast generalized edit distance and string alignment computation mainly for linguistic aims. As a generalization to the classic edit distance algorithms, the package allows users to define custom cost for every symbol's insertion, deletion, and substitution. The package also allows character combinations in any length to be seen as a single symbol which is very useful for International Phonetic Alphabet (IPA) transcriptions with diacritics. In addition to edit distance result, users can get detailed alignment information such as all possible alignment scenarios between two strings which is useful for testing, illustration or any further usage. Either the distance matrix or its long table form can be obtained and tools to do such conversions are provided. All functions in the package are implemented in 'C++' and the distance matrix computation is parallelized leveraging the 'RcppThread' package.  "
  },
  {
    "id": 15254,
    "package_name": "lit",
    "title": "Latent Interaction Testing for Genome-Wide Studies",
    "description": "Identifying latent genetic interactions in genome-wide association studies\n    using the Latent Interaction Testing (LIT) framework.\n    LIT is a flexible kernel-based approach that leverages information across\n    multiple traits to detect latent genetic interactions without specifying or\n    observing the interacting variable (e.g., environment). LIT accepts\n    standard PLINK files as inputs to analyze large genome-wide association studies.",
    "version": "1.0.1",
    "maintainer": "Andrew Bass <ajbass@emory.edu>",
    "author": "Andrew Bass [aut, cre],\n  Michael Epstein [aut]",
    "url": "https://github.com/ajbass/lit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lit Latent Interaction Testing for Genome-Wide Studies Identifying latent genetic interactions in genome-wide association studies\n    using the Latent Interaction Testing (LIT) framework.\n    LIT is a flexible kernel-based approach that leverages information across\n    multiple traits to detect latent genetic interactions without specifying or\n    observing the interacting variable (e.g., environment). LIT accepts\n    standard PLINK files as inputs to analyze large genome-wide association studies.  "
  },
  {
    "id": 15262,
    "package_name": "liureg",
    "title": "Liu Regression with Liu Biasing Parameters and Statistics",
    "description": "Linear Liu regression coefficient's estimation and testing with different Liu related measures such as MSE, R-squared etc.\n  REFERENCES\n  i.   Akdeniz and Kaciranlar (1995) <doi:10.1080/03610929508831585>\n  ii.  Druilhet and Mom (2008) <doi:10.1016/j.jmva.2006.06.011>\n  iii. Imdadullah, Aslam, and Saima (2017)\n  iv.  Liu (1993) <doi:10.1080/03610929308831027>\n  v.   Liu (2001) <doi:10.1016/j.jspi.2010.05.030>.",
    "version": "1.1.2",
    "maintainer": "Imdad Ullah Muhammad <mimdadasad@gmail.com>",
    "author": "Imdad Ullah Muhammad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1315-491X>),\n  Aslam Muhammad [aut, ctb]",
    "url": "",
    "bug_reports": "http://rfaqs.com/contact",
    "repository": "https://cran.r-project.org/package=liureg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "liureg Liu Regression with Liu Biasing Parameters and Statistics Linear Liu regression coefficient's estimation and testing with different Liu related measures such as MSE, R-squared etc.\n  REFERENCES\n  i.   Akdeniz and Kaciranlar (1995) <doi:10.1080/03610929508831585>\n  ii.  Druilhet and Mom (2008) <doi:10.1016/j.jmva.2006.06.011>\n  iii. Imdadullah, Aslam, and Saima (2017)\n  iv.  Liu (1993) <doi:10.1080/03610929308831027>\n  v.   Liu (2001) <doi:10.1016/j.jspi.2010.05.030>.  "
  },
  {
    "id": 15300,
    "package_name": "lmridge",
    "title": "Linear Ridge Regression with Ridge Penalty and Ridge Statistics",
    "description": "Linear ridge regression coefficient's estimation and testing with different ridge related measures such as MSE, R-squared etc.\n  REFERENCES\n  i.   Hoerl and Kennard (1970) <doi:10.1080/00401706.1970.10488634>,\n  ii.  Halawa and El-Bassiouni (2000) <doi:10.1080/00949650008812006>,\n  iii. Imdadullah, Aslam, and Saima (2017),\n  iv.  Marquardt (1970) <doi:10.2307/1267205>.",
    "version": "1.2.2",
    "maintainer": "Imdad Ullah Muhammad <mimdadasad@gmail.com>",
    "author": "Imdad Ullah Muhammad [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1315-491X>),\n  Aslam Muhammad [aut, ctb]",
    "url": "",
    "bug_reports": "https://rfaqs.com/contact/",
    "repository": "https://cran.r-project.org/package=lmridge",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lmridge Linear Ridge Regression with Ridge Penalty and Ridge Statistics Linear ridge regression coefficient's estimation and testing with different ridge related measures such as MSE, R-squared etc.\n  REFERENCES\n  i.   Hoerl and Kennard (1970) <doi:10.1080/00401706.1970.10488634>,\n  ii.  Halawa and El-Bassiouni (2000) <doi:10.1080/00949650008812006>,\n  iii. Imdadullah, Aslam, and Saima (2017),\n  iv.  Marquardt (1970) <doi:10.2307/1267205>.  "
  },
  {
    "id": 15301,
    "package_name": "lmtest",
    "title": "Testing Linear Regression Models",
    "description": "A collection of tests, data sets, and examples\n for diagnostic checking in linear regression models. Furthermore,\n some generic tools for inference in parametric models are provided.",
    "version": "0.9-40",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Torsten Hothorn [aut] (ORCID: <https://orcid.org/0000-0001-8301-0471>),\n  Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Richard W. Farebrother [aut] (pan.f),\n  Clint Cummins [aut] (pan.f),\n  Giovanni Millo [ctb],\n  David Mitchell [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lmtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lmtest Testing Linear Regression Models A collection of tests, data sets, and examples\n for diagnostic checking in linear regression models. Furthermore,\n some generic tools for inference in parametric models are provided.  "
  },
  {
    "id": 15302,
    "package_name": "lmtestrob",
    "title": "Outlier Robust Specification Testing",
    "description": "Robust test(s) for model diagnostics in regression. The current version contains a robust test for functional specification (linearity). The test is based on the robust bounded-influence test by Heritier and Ronchetti (1994) <doi:10.1080/01621459.1994.10476822>.",
    "version": "0.1",
    "maintainer": "Mikhail Zhelonkin <Mikhail.Zhelonkin@gmail.com>",
    "author": "Mikhail Zhelonkin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6912-4074>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lmtestrob",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lmtestrob Outlier Robust Specification Testing Robust test(s) for model diagnostics in regression. The current version contains a robust test for functional specification (linearity). The test is based on the robust bounded-influence test by Heritier and Ronchetti (1994) <doi:10.1080/01621459.1994.10476822>.  "
  },
  {
    "id": 15310,
    "package_name": "loadings",
    "title": "Loadings for Principal Component Analysis and Partial Least\nSquares",
    "description": "Computing statistical hypothesis testing for loading in principal component analysis (PCA) (Yamamoto, H. et al. (2014) <doi:10.1186/1471-2105-15-51>), orthogonal smoothed PCA (OS-PCA) (Yamamoto, H. et al. (2021) <doi:10.3390/metabo11030149>), one-sided kernel PCA (Yamamoto, H. (2023) <doi:10.51094/jxiv.262>), partial least squares (PLS) and PLS discriminant analysis (PLS-DA) (Yamamoto, H. et al. (2009) <doi:10.1016/j.chemolab.2009.05.006>), PLS with rank order of groups (PLS-ROG) (Yamamoto, H. (2017) <doi:10.1002/cem.2883>), regularized canonical correlation analysis discriminant analysis (RCCA-DA) (Yamamoto, H. et al. (2008) <doi:10.1016/j.bej.2007.12.009>), multiset PLS and PLS-ROG (Yamamoto, H. (2022) <doi:10.1101/2022.08.30.505949>).",
    "version": "0.5.1",
    "maintainer": "Hiroyuki Yamamoto <h.yama2396@gmail.com>",
    "author": "Hiroyuki Yamamoto",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=loadings",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "loadings Loadings for Principal Component Analysis and Partial Least\nSquares Computing statistical hypothesis testing for loading in principal component analysis (PCA) (Yamamoto, H. et al. (2014) <doi:10.1186/1471-2105-15-51>), orthogonal smoothed PCA (OS-PCA) (Yamamoto, H. et al. (2021) <doi:10.3390/metabo11030149>), one-sided kernel PCA (Yamamoto, H. (2023) <doi:10.51094/jxiv.262>), partial least squares (PLS) and PLS discriminant analysis (PLS-DA) (Yamamoto, H. et al. (2009) <doi:10.1016/j.chemolab.2009.05.006>), PLS with rank order of groups (PLS-ROG) (Yamamoto, H. (2017) <doi:10.1002/cem.2883>), regularized canonical correlation analysis discriminant analysis (RCCA-DA) (Yamamoto, H. et al. (2008) <doi:10.1016/j.bej.2007.12.009>), multiset PLS and PLS-ROG (Yamamoto, H. (2022) <doi:10.1101/2022.08.30.505949>).  "
  },
  {
    "id": 15320,
    "package_name": "localgauss",
    "title": "Estimating Local Gaussian Parameters",
    "description": "Computational routines for estimating local Gaussian parameters. Local Gaussian parameters are useful for characterizing and testing for non-linear dependence within bivariate data. See e.g. Tjostheim and Hufthammer, Local Gaussian correlation: A new measure of dependence, Journal of Econometrics, 2013, Volume 172 (1), pages 33-48 <DOI:10.1016/j.jeconom.2012.08.001>.",
    "version": "0.41",
    "maintainer": "Tore Selland Kleppe <tore.kleppe@uis.no>",
    "author": "Tore Selland Kleppe <tore.kleppe@uis.no>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=localgauss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "localgauss Estimating Local Gaussian Parameters Computational routines for estimating local Gaussian parameters. Local Gaussian parameters are useful for characterizing and testing for non-linear dependence within bivariate data. See e.g. Tjostheim and Hufthammer, Local Gaussian correlation: A new measure of dependence, Journal of Econometrics, 2013, Volume 172 (1), pages 33-48 <DOI:10.1016/j.jeconom.2012.08.001>.  "
  },
  {
    "id": 15414,
    "package_name": "lpc",
    "title": "Lassoed Principal Components for Testing Significance of\nFeatures",
    "description": "Implements the LPC method of Witten&Tibshirani(Annals of Applied Statistics 2008) for identification of significant genes in a microarray experiment.",
    "version": "1.0.2.1",
    "maintainer": "Daniela M Witten <dwitten@uw.edu>",
    "author": "Daniela M Witten and Robert Tibshirani",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lpc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lpc Lassoed Principal Components for Testing Significance of\nFeatures Implements the LPC method of Witten&Tibshirani(Annals of Applied Statistics 2008) for identification of significant genes in a microarray experiment.  "
  },
  {
    "id": 15483,
    "package_name": "mExplorer",
    "title": "Identifying Master Gene Regulators from Gene Expression and\nDNA-Binding Data",
    "description": "The method 'm:Explorer' associates a given list of target genes (e.g. those involved in a biological process) to gene regulators such as transcription factors. Transcription factors that bind DNA near significantly many target genes or correlate with target genes in transcriptional (microarray or RNAseq data) are selected. Selection of candidate master regulators is carried out using multinomial regression models, likelihood ratio tests and multiple testing correction. Reference: m:Explorer: multinomial regression models reveal positive and negative regulators of longevity in yeast quiescence. Juri Reimand, Anu Aun, Jaak Vilo, Juan M Vaquerizas, Juhan Sedman and Nicholas M Luscombe. Genome Biology (2012) 13:R55 <doi:10.1186/gb-2012-13-6-r55>.",
    "version": "1.0.0",
    "maintainer": "Juri Reimand <juri.reimand@utoronto.ca>",
    "author": "Juri Reimand [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mExplorer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mExplorer Identifying Master Gene Regulators from Gene Expression and\nDNA-Binding Data The method 'm:Explorer' associates a given list of target genes (e.g. those involved in a biological process) to gene regulators such as transcription factors. Transcription factors that bind DNA near significantly many target genes or correlate with target genes in transcriptional (microarray or RNAseq data) are selected. Selection of candidate master regulators is carried out using multinomial regression models, likelihood ratio tests and multiple testing correction. Reference: m:Explorer: multinomial regression models reveal positive and negative regulators of longevity in yeast quiescence. Juri Reimand, Anu Aun, Jaak Vilo, Juan M Vaquerizas, Juhan Sedman and Nicholas M Luscombe. Genome Biology (2012) 13:R55 <doi:10.1186/gb-2012-13-6-r55>.  "
  },
  {
    "id": 15493,
    "package_name": "mMPA",
    "title": "Implementation of Marker-Assisted Mini-Pooling with Algorithm",
    "description": "To determine the number of quantitative assays needed for a sample \n    of data using pooled testing methods, which include mini-pooling (MP), MP \n    with algorithm (MPA), and marker-assisted MPA (mMPA). To estimate the number \n    of assays needed, the package also provides a tool to conduct Monte Carlo (MC) \n    to simulate different orders in which the sample would be collected to form pools. \n    Using MC avoids the dependence of the estimated number of assays on any specific \n    ordering of the samples to form pools.",
    "version": "1.2.0",
    "maintainer": "\"Tao Liu, PhD\" <tliu@stat.brown.edu>",
    "author": "\"Tao Liu, PhD <tliu@stat.brown.edu> [aut, cre]\" \"Yizhen Xu, ScM <yizhen_xu@brown.edu> [aut]\"  ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mMPA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mMPA Implementation of Marker-Assisted Mini-Pooling with Algorithm To determine the number of quantitative assays needed for a sample \n    of data using pooled testing methods, which include mini-pooling (MP), MP \n    with algorithm (MPA), and marker-assisted MPA (mMPA). To estimate the number \n    of assays needed, the package also provides a tool to conduct Monte Carlo (MC) \n    to simulate different orders in which the sample would be collected to form pools. \n    Using MC avoids the dependence of the estimated number of assays on any specific \n    ordering of the samples to form pools.  "
  },
  {
    "id": 15497,
    "package_name": "mSTEM",
    "title": "Multiple Testing of Local Extrema for Detection of Change Points",
    "description": "A new approach to detect change points based on smoothing and multiple testing, which is for long data sequence modeled as piecewise constant functions plus stationary Gaussian noise, see Dan Cheng and Armin Schwartzman (2015) <arXiv:1504.06384>.",
    "version": "1.0-1",
    "maintainer": "Zhibing He <zhibingh@asu.edu>",
    "author": "Zhibing He and Dan Cheng",
    "url": "https://arxiv.org/abs/1504.06384",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mSTEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mSTEM Multiple Testing of Local Extrema for Detection of Change Points A new approach to detect change points based on smoothing and multiple testing, which is for long data sequence modeled as piecewise constant functions plus stationary Gaussian noise, see Dan Cheng and Armin Schwartzman (2015) <arXiv:1504.06384>.  "
  },
  {
    "id": 15503,
    "package_name": "maat",
    "title": "Multiple Administrations Adaptive Testing",
    "description": "Provides an extension of the shadow-test approach to computerized adaptive\n    testing (CAT) implemented in the 'TestDesign' package for the assessment framework\n    involving multiple tests administered periodically throughout the year. This framework\n    is referred to as the Multiple Administrations Adaptive Testing (MAAT) and supports\n    multiple item pools vertically scaled and multiple phases (stages) of CAT within each test.\n    Between phases and tests, transitioning from one item pool (and associated constraints)\n    to another is allowed as deemed necessary to enhance the quality of measurement.",
    "version": "1.1.1",
    "maintainer": "Seung W. Choi <schoi@austin.utexas.edu>",
    "author": "Seung W. Choi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4777-5420>),\n  Sangdon Lim [aut] (ORCID: <https://orcid.org/0000-0002-2988-014X>),\n  Luping Niu [aut] (ORCID: <https://orcid.org/0000-0003-3696-1180>),\n  Sooyong Lee [aut] (ORCID: <https://orcid.org/0000-0002-7964-4508>),\n  M. Christina Schneider [ctb],\n  Jay Lee [ctb],\n  Garron Gianopulos [ctb]",
    "url": "https://choi-phd.github.io/maat/",
    "bug_reports": "https://github.com/choi-phd/maat/issues/",
    "repository": "https://cran.r-project.org/package=maat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maat Multiple Administrations Adaptive Testing Provides an extension of the shadow-test approach to computerized adaptive\n    testing (CAT) implemented in the 'TestDesign' package for the assessment framework\n    involving multiple tests administered periodically throughout the year. This framework\n    is referred to as the Multiple Administrations Adaptive Testing (MAAT) and supports\n    multiple item pools vertically scaled and multiple phases (stages) of CAT within each test.\n    Between phases and tests, transitioning from one item pool (and associated constraints)\n    to another is allowed as deemed necessary to enhance the quality of measurement.  "
  },
  {
    "id": 15545,
    "package_name": "maketools",
    "title": "Exploring and Testing the Toolchain and System Libraries",
    "description": "Helper functions that interface with the system utilities to learn \n    about the local build environment. Lets you explore 'make' rules to test the\n    local configuration, or query 'pkg-config' to find compiler flags and libs \n    needed for building packages with external dependencies. Also contains tools\n    to analyze which libraries that a installed R package linked to by inspecting\n    output from 'ldd' in combination with information from your distribution \n    package manager, e.g. 'rpm' or 'dpkg'.",
    "version": "1.3.2",
    "maintainer": "Jeroen Ooms <jeroenooms@gmail.com>",
    "author": "Jeroen Ooms [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4035-0289>)",
    "url": "https://jeroen.r-universe.dev/maketools",
    "bug_reports": "https://github.com/jeroen/maketools/issues",
    "repository": "https://cran.r-project.org/package=maketools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maketools Exploring and Testing the Toolchain and System Libraries Helper functions that interface with the system utilities to learn \n    about the local build environment. Lets you explore 'make' rules to test the\n    local configuration, or query 'pkg-config' to find compiler flags and libs \n    needed for building packages with external dependencies. Also contains tools\n    to analyze which libraries that a installed R package linked to by inspecting\n    output from 'ldd' in combination with information from your distribution \n    package manager, e.g. 'rpm' or 'dpkg'.  "
  },
  {
    "id": 15614,
    "package_name": "marcher",
    "title": "Migration and Range Change Estimation in R",
    "description": "A set of tools for likelihood-based estimation, model selection and testing of two- and three-range shift and migration models for animal movement data as described in Gurarie et al. (2017) <doi: 10.1111/1365-2656.12674>.  Provided movement data (X, Y and Time), including irregularly sampled data, functions estimate the time, duration and location of one or two range shifts, as well as the ranging area and auto-correlation structure of the movment.  Tests assess, for example, whether the shift was \"significant\", and whether a two-shift migration was a true return migration.",
    "version": "0.0-2",
    "maintainer": "Eliezer Gurarie <egurarie@umd.edu>",
    "author": "Eliezer Gurarie [aut, cre],\n  Farid Cheraghi [aut]",
    "url": "",
    "bug_reports": "https://github.com/EliGurarie/marcher/issues",
    "repository": "https://cran.r-project.org/package=marcher",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "marcher Migration and Range Change Estimation in R A set of tools for likelihood-based estimation, model selection and testing of two- and three-range shift and migration models for animal movement data as described in Gurarie et al. (2017) <doi: 10.1111/1365-2656.12674>.  Provided movement data (X, Y and Time), including irregularly sampled data, functions estimate the time, duration and location of one or two range shifts, as well as the ranging area and auto-correlation structure of the movment.  Tests assess, for example, whether the shift was \"significant\", and whether a two-shift migration was a true return migration.  "
  },
  {
    "id": 15628,
    "package_name": "markmyassignment",
    "title": "Automatic Marking of R Assignments",
    "description": "Automatic marking of R assignments for students and teachers based\n    on 'testthat' test suites.",
    "version": "0.8.9",
    "maintainer": "Mans Magnusson <mons.magnusson@gmail.com>",
    "author": "Mans Magnusson [aut, cre],\n  Oscar Pettersson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=markmyassignment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "markmyassignment Automatic Marking of R Assignments Automatic marking of R assignments for students and teachers based\n    on 'testthat' test suites.  "
  },
  {
    "id": 15642,
    "package_name": "mashr",
    "title": "Multivariate Adaptive Shrinkage",
    "description": "Implements the multivariate adaptive shrinkage (mash)\n    method of Urbut et al (2019) <DOI:10.1038/s41588-018-0268-8> for\n    estimating and testing large numbers of effects in many conditions\n    (or many outcomes). Mash takes an empirical Bayes approach to\n    testing and effect estimation; it estimates patterns of similarity\n    among conditions, then exploits these patterns to improve accuracy\n    of the effect estimates. The core linear algebra is implemented in\n    C++ for fast model fitting and posterior computation.",
    "version": "0.2.79",
    "maintainer": "Peter Carbonetto <peter.carbonetto@gmail.com>",
    "author": "Matthew Stephens [aut],\n  Sarah Urbut [aut],\n  Gao Wang [aut],\n  Yuxin Zou [aut],\n  Yunqi Yang [ctb],\n  Sam Roweis [cph],\n  David Hogg [cph],\n  Jo Bovy [cph],\n  Peter Carbonetto [aut, cre]",
    "url": "https://github.com/stephenslab/mashr",
    "bug_reports": "https://github.com/stephenslab/mashr/issues",
    "repository": "https://cran.r-project.org/package=mashr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mashr Multivariate Adaptive Shrinkage Implements the multivariate adaptive shrinkage (mash)\n    method of Urbut et al (2019) <DOI:10.1038/s41588-018-0268-8> for\n    estimating and testing large numbers of effects in many conditions\n    (or many outcomes). Mash takes an empirical Bayes approach to\n    testing and effect estimation; it estimates patterns of similarity\n    among conditions, then exploits these patterns to improve accuracy\n    of the effect estimates. The core linear algebra is implemented in\n    C++ for fast model fitting and posterior computation.  "
  },
  {
    "id": 15673,
    "package_name": "matrisk",
    "title": "Macroeconomic-at-Risk",
    "description": "The Macroeconomics-at-Risk (MaR) approach is based on a two-step semi-parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the MaR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.",
    "version": "0.1.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "author": "Quentin Lajaunie [aut, cre],\n  Guillaume Flament [aut],\n  Christophe Hurlin [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=matrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "matrisk Macroeconomic-at-Risk The Macroeconomics-at-Risk (MaR) approach is based on a two-step semi-parametric estimation procedure that allows to forecast the full conditional distribution of an economic variable at a given horizon, as a function of a set of factors. These density forecasts are then be used to produce coherent forecasts for any downside risk measure, e.g., value-at-risk, expected shortfall, downside entropy. Initially introduced by Adrian et al. (2019) <doi:10.1257/aer.20161923> to reveal the vulnerability of economic growth to financial conditions, the MaR approach is currently extensively used by international financial institutions to provide Value-at-Risk (VaR) type forecasts for GDP growth (Growth-at-Risk) or inflation (Inflation-at-Risk). This package provides methods for estimating these models. Datasets for the US and the Eurozone are available to allow testing of the Adrian et al (2019) model. This package constitutes a useful toolbox (data and functions) for private practitioners, scholars as well as policymakers.  "
  },
  {
    "id": 15694,
    "package_name": "maxLik",
    "title": "Maximum Likelihood Estimation and Related Tools",
    "description": "Functions for Maximum Likelihood (ML) estimation, non-linear\n   optimization, and related tools.  It includes a unified way to call\n   different optimizers, and classes and methods to handle the results from\n   the Maximum Likelihood viewpoint.  It also includes a number of convenience\n   tools for testing and developing your own models.",
    "version": "1.5-2.1",
    "maintainer": "Ott Toomet <otoomet@gmail.com>",
    "author": "Ott Toomet [aut, cre],\n  Arne Henningsen [aut],\n  Spencer Graves [ctb],\n  Yves Croissant [ctb],\n  David Hugh-Jones [ctb],\n  Luca Scrucca [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=maxLik",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "maxLik Maximum Likelihood Estimation and Related Tools Functions for Maximum Likelihood (ML) estimation, non-linear\n   optimization, and related tools.  It includes a unified way to call\n   different optimizers, and classes and methods to handle the results from\n   the Maximum Likelihood viewpoint.  It also includes a number of convenience\n   tools for testing and developing your own models.  "
  },
  {
    "id": 15708,
    "package_name": "mazeinda",
    "title": "Monotonic Association on Zero-Inflated Data",
    "description": "Methods for calculating and testing the significance of\n  pairwise monotonic association from and based on the work of\n  Pimentel (2009) <doi:10.4135/9781412985291.n2>. Computation of association of vectors from one\n  or multiple sets can be performed in parallel thanks to the\n  packages 'foreach' and 'doMC'.",
    "version": "0.0.2",
    "maintainer": "Alice Albasi <albasialice@gmail.com>",
    "author": "Alice Albasi [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mazeinda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mazeinda Monotonic Association on Zero-Inflated Data Methods for calculating and testing the significance of\n  pairwise monotonic association from and based on the work of\n  Pimentel (2009) <doi:10.4135/9781412985291.n2>. Computation of association of vectors from one\n  or multiple sets can be performed in parallel thanks to the\n  packages 'foreach' and 'doMC'.  "
  },
  {
    "id": 15723,
    "package_name": "mbreaks",
    "title": "Estimation and Inference for Structural Breaks in Linear\nRegression Models",
    "description": "Functions provide comprehensive treatments for estimating, inferring, testing and model selecting in linear regression models with structural breaks. The tests, estimation methods, inference and information criteria implemented are discussed in Bai and Perron (1998) \"Estimating and Testing Linear Models with Multiple Structural Changes\" <doi:10.2307/2998540>.",
    "version": "1.0.1",
    "maintainer": "Linh Nguyen <nguye535@purdue.edu>",
    "author": "Linh Nguyen [aut, cre],\n  Yohei Yamamoto [aut],\n  Pierre Perron [aut]",
    "url": "https://github.com/RoDivinity/mbreaks",
    "bug_reports": "https://github.com/RoDivinity/mbreaks/issues",
    "repository": "https://cran.r-project.org/package=mbreaks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mbreaks Estimation and Inference for Structural Breaks in Linear\nRegression Models Functions provide comprehensive treatments for estimating, inferring, testing and model selecting in linear regression models with structural breaks. The tests, estimation methods, inference and information criteria implemented are discussed in Bai and Perron (1998) \"Estimating and Testing Linear Models with Multiple Structural Changes\" <doi:10.2307/2998540>.  "
  },
  {
    "id": 15764,
    "package_name": "mcp",
    "title": "Regression with Multiple Change Points",
    "description": "Flexible and informed regression with Multiple Change Points. 'mcp' can infer change points in means, variances, autocorrelation structure, and any combination of these, as well as the parameters of the segments in between. All parameters are estimated with uncertainty and prediction intervals are supported - also near the change points. 'mcp' supports hypothesis testing via Savage-Dickey density ratio, posterior contrasts, and cross-validation. 'mcp' is described in Lindel\u00f8v (submitted) <doi:10.31219/osf.io/fzqxv> and generalizes the approach described in Carlin, Gelfand, & Smith (1992) <doi:10.2307/2347570> and Stephens (1994) <doi:10.2307/2986119>.",
    "version": "0.3.4",
    "maintainer": "Jonas Kristoffer Lindel\u00f8v <jonas@lindeloev.dk>",
    "author": "Jonas Kristoffer Lindel\u00f8v [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4565-0595>)",
    "url": "https://lindeloev.github.io/mcp/",
    "bug_reports": "https://github.com/lindeloev/mcp/issues",
    "repository": "https://cran.r-project.org/package=mcp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcp Regression with Multiple Change Points Flexible and informed regression with Multiple Change Points. 'mcp' can infer change points in means, variances, autocorrelation structure, and any combination of these, as well as the parameters of the segments in between. All parameters are estimated with uncertainty and prediction intervals are supported - also near the change points. 'mcp' supports hypothesis testing via Savage-Dickey density ratio, posterior contrasts, and cross-validation. 'mcp' is described in Lindel\u00f8v (submitted) <doi:10.31219/osf.io/fzqxv> and generalizes the approach described in Carlin, Gelfand, & Smith (1992) <doi:10.2307/2347570> and Stephens (1994) <doi:10.2307/2986119>.  "
  },
  {
    "id": 15766,
    "package_name": "mcprofile",
    "title": "Testing Generalized Linear Hypotheses for Generalized Linear\nModel Parameters by Profile Deviance",
    "description": "Calculation of signed root deviance profiles for linear combinations of parameters in a generalized linear model. Multiple tests and simultaneous confidence intervals are provided.",
    "version": "1.0-1",
    "maintainer": "Daniel Gerhard <00gerhard@gmail.com>",
    "author": "Daniel Gerhard [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcprofile",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcprofile Testing Generalized Linear Hypotheses for Generalized Linear\nModel Parameters by Profile Deviance Calculation of signed root deviance profiles for linear combinations of parameters in a generalized linear model. Multiple tests and simultaneous confidence intervals are provided.  "
  },
  {
    "id": 15775,
    "package_name": "mcunit",
    "title": "Unit Tests for MC Methods",
    "description": "Unit testing for Monte Carlo methods, particularly Markov Chain Monte Carlo (MCMC) methods, are implemented as extensions of the 'testthat' package. The MCMC methods check whether the MCMC chain has the correct invariant distribution. They do not check other properties of successful samplers such as whether the chain can reach all points, i.e. whether is recurrent. The tests require the ability to sample from the prior and to run steps of the MCMC chain. The methodology is described in Gandy and Scott (2020) <arXiv:2001.06465>.",
    "version": "0.3.2",
    "maintainer": "Axel Gandy <a.gandy@imperial.ac.uk>",
    "author": "Axel Gandy [aut, cre],\n  James Scott [aut]",
    "url": "https://bitbucket.org/agandy/mcunit/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mcunit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mcunit Unit Tests for MC Methods Unit testing for Monte Carlo methods, particularly Markov Chain Monte Carlo (MCMC) methods, are implemented as extensions of the 'testthat' package. The MCMC methods check whether the MCMC chain has the correct invariant distribution. They do not check other properties of successful samplers such as whether the chain can reach all points, i.e. whether is recurrent. The tests require the ability to sample from the prior and to run steps of the MCMC chain. The methodology is described in Gandy and Scott (2020) <arXiv:2001.06465>.  "
  },
  {
    "id": 15792,
    "package_name": "mdir.logrank",
    "title": "Multiple-Direction Logrank Test",
    "description": "Implemented are the one-sided and two-sided \n  multiple-direction logrank test for two-sample right \n  censored data. In addition to the statistics p-values are calculated: \n  1. For the one-sided testing problem one p-value based on a\n   wild bootstrap approach is determined. 2. In the two-sided case\n   one p-value based on a chi-squared approximation and \n   a second p-values based on a permutation approach are calculated.\n Ditzhaus, M. and Friedrich, S. (2018) <arXiv:1807.05504>.\n Ditzhaus, M. and Pauly, M. (2018) <arXiv:1808.05627>.",
    "version": "0.0.4",
    "maintainer": "Sarah Friedrich <sarah.friedrich@alumni.uni-ulm.de>",
    "author": "Marc Ditzhaus and Sarah Friedrich",
    "url": "",
    "bug_reports": "http://github.com/marcdii/mdir.logrank/issues",
    "repository": "https://cran.r-project.org/package=mdir.logrank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mdir.logrank Multiple-Direction Logrank Test Implemented are the one-sided and two-sided \n  multiple-direction logrank test for two-sample right \n  censored data. In addition to the statistics p-values are calculated: \n  1. For the one-sided testing problem one p-value based on a\n   wild bootstrap approach is determined. 2. In the two-sided case\n   one p-value based on a chi-squared approximation and \n   a second p-values based on a permutation approach are calculated.\n Ditzhaus, M. and Friedrich, S. (2018) <arXiv:1807.05504>.\n Ditzhaus, M. and Pauly, M. (2018) <arXiv:1808.05627>.  "
  },
  {
    "id": 15813,
    "package_name": "medScan",
    "title": "Large Scale Single Mediator Hypothesis Testing",
    "description": "A collection of methods for large scale single mediator hypothesis\n    testing. The six included methods for testing the mediation effect are Sobel's\n    test, Max P test, joint significance test under the composite null hypothesis,\n    high dimensional mediation testing, divide-aggregate composite null test,\n    and Sobel's test under the composite null hypothesis. Du et al (2023) \n    <doi:10.1002/gepi.22510>.",
    "version": "1.0.2",
    "maintainer": "Michael Kleinsasser <mkleinsa@umich.edu>",
    "author": "Jiacong Du [aut],\n  Michael Kleinsasser [cre]",
    "url": "https://github.com/umich-cphds/medScan",
    "bug_reports": "https://github.com/umich-cphds/medScan/issues",
    "repository": "https://cran.r-project.org/package=medScan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "medScan Large Scale Single Mediator Hypothesis Testing A collection of methods for large scale single mediator hypothesis\n    testing. The six included methods for testing the mediation effect are Sobel's\n    test, Max P test, joint significance test under the composite null hypothesis,\n    high dimensional mediation testing, divide-aggregate composite null test,\n    and Sobel's test under the composite null hypothesis. Du et al (2023) \n    <doi:10.1002/gepi.22510>.  "
  },
  {
    "id": 15866,
    "package_name": "meta4diag",
    "title": "Meta-Analysis for Diagnostic Test Studies",
    "description": "Bayesian inference analysis for bivariate meta-analysis of diagnostic test studies using integrated nested Laplace approximation with INLA. A purpose built graphic user interface is available. The installation of R package INLA is compulsory for successful usage. The INLA package can be obtained from <https://www.r-inla.org>. We recommend the testing version, which can be downloaded by running: install.packages(\"INLA\", repos=c(getOption(\"repos\"), INLA=\"https://inla.r-inla-download.org/R/testing\"), dep=TRUE).",
    "version": "2.1.1",
    "maintainer": "Jingyi Guo <jingyi.guo@ntnu.no>",
    "author": "Jingyi Guo <jingyi.guo@ntnu.no> and Andrea Riebler <andrea.riebler@ntnu.no>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=meta4diag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "meta4diag Meta-Analysis for Diagnostic Test Studies Bayesian inference analysis for bivariate meta-analysis of diagnostic test studies using integrated nested Laplace approximation with INLA. A purpose built graphic user interface is available. The installation of R package INLA is compulsory for successful usage. The INLA package can be obtained from <https://www.r-inla.org>. We recommend the testing version, which can be downloaded by running: install.packages(\"INLA\", repos=c(getOption(\"repos\"), INLA=\"https://inla.r-inla-download.org/R/testing\"), dep=TRUE).  "
  },
  {
    "id": 15893,
    "package_name": "metadat",
    "title": "Meta-Analysis Datasets",
    "description": "A collection of meta-analysis datasets for teaching purposes, illustrating/testing meta-analytic methods, and validating published analyses.",
    "version": "1.4-0",
    "maintainer": "Wolfgang Viechtbauer <wvb@metafor-project.org>",
    "author": "Wolfgang Viechtbauer [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3463-4063>),\n  Thomas White [aut] (ORCID: <https://orcid.org/0000-0002-3976-1734>),\n  Daniel Noble [aut] (ORCID: <https://orcid.org/0000-0001-9460-8743>),\n  Alistair Senior [aut] (ORCID: <https://orcid.org/0000-0001-9805-7280>),\n  W. Kyle Hamilton [aut] (ORCID: <https://orcid.org/0000-0002-8642-7990>),\n  Guido Schwarzer [dtc] (ORCID: <https://orcid.org/0000-0001-6214-9087>)",
    "url": "https://github.com/wviechtb/metadat,\nhttps://wviechtb.github.io/metadat/",
    "bug_reports": "https://github.com/wviechtb/metadat/issues",
    "repository": "https://cran.r-project.org/package=metadat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metadat Meta-Analysis Datasets A collection of meta-analysis datasets for teaching purposes, illustrating/testing meta-analytic methods, and validating published analyses.  "
  },
  {
    "id": 15945,
    "package_name": "metevalue",
    "title": "E-Value in the Omics Data Association Studies",
    "description": "In the omics data association studies, it is common to conduct the p-value corrections to control the false significance. Beyond the P-value corrections, E-value is recently studied to facilitate multiple testing correction based on V. Vovk and R. Wang (2021) <doi:10.1214/20-AOS2020>. This package provides E-value calculation for DNA methylation data and RNA-seq data. Currently, five data formats are supported: DNA methylation levels using DMR detection tools (BiSeq, DMRfinder, MethylKit, Metilene and other DNA methylation tools) and RNA-seq data. The relevant references are listed below: Katja Hebestreit and Hans-Ulrich Klein (2022) <doi:10.18129/B9.bioc.BiSeq>; Altuna Akalin et.al (2012) <doi:10.18129/B9.bioc.methylKit>.",
    "version": "0.2.4",
    "maintainer": "Yifan Yang <yfyang.86@hotmail.com>",
    "author": "Yifan Yang [aut, cre, cph],\n  Xiaoqing Pan [aut],\n  Haoyuan Liu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=metevalue",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metevalue E-Value in the Omics Data Association Studies In the omics data association studies, it is common to conduct the p-value corrections to control the false significance. Beyond the P-value corrections, E-value is recently studied to facilitate multiple testing correction based on V. Vovk and R. Wang (2021) <doi:10.1214/20-AOS2020>. This package provides E-value calculation for DNA methylation data and RNA-seq data. Currently, five data formats are supported: DNA methylation levels using DMR detection tools (BiSeq, DMRfinder, MethylKit, Metilene and other DNA methylation tools) and RNA-seq data. The relevant references are listed below: Katja Hebestreit and Hans-Ulrich Klein (2022) <doi:10.18129/B9.bioc.BiSeq>; Altuna Akalin et.al (2012) <doi:10.18129/B9.bioc.methylKit>.  "
  },
  {
    "id": 15984,
    "package_name": "mhtboot",
    "title": "Multiple Hypothesis Test Based on Distribution of p Values",
    "description": "A framework for multiple hypothesis testing based on distribution\n    of p values. It is well known that the p values come from different\n    distribution for null and alternatives, in this package we provide\n    functions to detect that change. We provide a method for using the change\n    in distribution of p values as a way to detect the true signals in the\n    data.",
    "version": "1.3.3",
    "maintainer": "Abhirup Mallik <malli066@umn.edu>",
    "author": "Abhirup Mallik [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mhtboot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mhtboot Multiple Hypothesis Test Based on Distribution of p Values A framework for multiple hypothesis testing based on distribution\n    of p values. It is well known that the p values come from different\n    distribution for null and alternatives, in this package we provide\n    functions to detect that change. We provide a method for using the change\n    in distribution of p values as a way to detect the true signals in the\n    data.  "
  },
  {
    "id": 15993,
    "package_name": "miRtest",
    "title": "Combined miRNA- And mRNA-Testing",
    "description": "Package for combined miRNA- and mRNA-testing.",
    "version": "2.2",
    "maintainer": "Stephan Artmann <stephanartmann@gmx.net>",
    "author": "Stephan Artmann [aut, cre],\n  Klaus Jung [aut],\n  Tim Beissbarth [aut]",
    "url": "https://pubmed.ncbi.nlm.nih.gov/22723856/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=miRtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "miRtest Combined miRNA- And mRNA-Testing Package for combined miRNA- and mRNA-testing.  "
  },
  {
    "id": 16023,
    "package_name": "micromapST",
    "title": "Linked Micromap Plots for U. S. and Other Geographic Areas",
    "description": "Provides the users with the ability to quickly create linked \n     micromap plots for a collection of geographic areas. Linked micromap \n     plots are visualizations of geo-referenced data that link statistical \n     graphics to an organized series of small maps or graphic images. \n     The Help description contains examples of how to use the 'micromapST' \n     function. Contained in this package are border group datasets to \n     support creating linked micromap plots for the 50 U.S. states and \n     District of Columbia (51 areas), the U. S. 20 Seer Registries, the \n     105 counties in the state of Kansas, the 62 counties of New York, \n     the 24 counties of Maryland, the 29 counties of Utah, the 32 \n     administrative areas in China, the 218 administrative areas in \n     the UK and Ireland (for testing only), the 25 districts in the \n     city of Seoul South Korea, and the 52 counties on the Africa continent.\n     A border group dataset contains the boundaries related to the data \n     level areas, a second layer boundaries, a top or third layer boundary, \n     a parameter list of run options, and a cross indexing table between \n     area names, abbreviations, numeric identification and alias matching \n     strings for the specific geographic area.  By specifying a border group, \n     the package create linked micromap plots for any geographic region.  \n     The user can create and provide their own border group dataset for \n     any area beyond the areas contained within the package with the \n     'BuildBorderGroup' function. In April of 2022, it was announced that \n     'maptools', 'rgdal', and 'rgeos' R packages would be retired in \n     middle to end of 2023 and removed from the CRAN libraries.  \n     The 'BuildBorderGroup' function was dependent on these packages.  \n     'micromapST' functions were not impacted by the retired R packages.  \n     Upgrading of 'BuildBorderGroup' function was completed and released with \n     version 3.0.0 on August 10, 2023 using the 'sf' R package.\n     References: Carr and Pickle, Chapman and Hall/CRC, Visualizing\n     Data Patterns with Micromaps, CRC Press, 2010. Pickle, Pearson,\n     and Carr (2015), micromapST: Exploring and Communicating\n     Geospatial Patterns in US State Data., Journal of Statistical\n     Software, 63(3), 1-25., <https://www.jstatsoft.org/v63/i03/>.\n     Copyrighted 2013, 2014, 2015, 2016, 2022, 2023, 2024, and \n     2025 by Carr, Pearson and Pickle.",
    "version": "3.1.1",
    "maintainer": "Jim Pearson <jbpearson353@gmail.com>",
    "author": "Jim Pearson [aut, cre, cph],\n  Dan Carr [aut, cph],\n  Linda Pickle [ctb, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=micromapST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "micromapST Linked Micromap Plots for U. S. and Other Geographic Areas Provides the users with the ability to quickly create linked \n     micromap plots for a collection of geographic areas. Linked micromap \n     plots are visualizations of geo-referenced data that link statistical \n     graphics to an organized series of small maps or graphic images. \n     The Help description contains examples of how to use the 'micromapST' \n     function. Contained in this package are border group datasets to \n     support creating linked micromap plots for the 50 U.S. states and \n     District of Columbia (51 areas), the U. S. 20 Seer Registries, the \n     105 counties in the state of Kansas, the 62 counties of New York, \n     the 24 counties of Maryland, the 29 counties of Utah, the 32 \n     administrative areas in China, the 218 administrative areas in \n     the UK and Ireland (for testing only), the 25 districts in the \n     city of Seoul South Korea, and the 52 counties on the Africa continent.\n     A border group dataset contains the boundaries related to the data \n     level areas, a second layer boundaries, a top or third layer boundary, \n     a parameter list of run options, and a cross indexing table between \n     area names, abbreviations, numeric identification and alias matching \n     strings for the specific geographic area.  By specifying a border group, \n     the package create linked micromap plots for any geographic region.  \n     The user can create and provide their own border group dataset for \n     any area beyond the areas contained within the package with the \n     'BuildBorderGroup' function. In April of 2022, it was announced that \n     'maptools', 'rgdal', and 'rgeos' R packages would be retired in \n     middle to end of 2023 and removed from the CRAN libraries.  \n     The 'BuildBorderGroup' function was dependent on these packages.  \n     'micromapST' functions were not impacted by the retired R packages.  \n     Upgrading of 'BuildBorderGroup' function was completed and released with \n     version 3.0.0 on August 10, 2023 using the 'sf' R package.\n     References: Carr and Pickle, Chapman and Hall/CRC, Visualizing\n     Data Patterns with Micromaps, CRC Press, 2010. Pickle, Pearson,\n     and Carr (2015), micromapST: Exploring and Communicating\n     Geospatial Patterns in US State Data., Journal of Statistical\n     Software, 63(3), 1-25., <https://www.jstatsoft.org/v63/i03/>.\n     Copyrighted 2013, 2014, 2015, 2016, 2022, 2023, 2024, and \n     2025 by Carr, Pearson and Pickle.  "
  },
  {
    "id": 16049,
    "package_name": "migraph",
    "title": "Inferential Methods for Multimodal and Other Networks",
    "description": "A set of tools for testing networks.\n   It includes functions for univariate and multivariate \n   conditional uniform graph and quadratic assignment procedure testing,\n   and network regression.\n   The package is a complement to \n   'Multimodal Political Networks' (2021, ISBN:9781108985000),\n   and includes various datasets used in the book.\n   Built on the 'manynet' package, all functions operate with matrices, \n   edge lists, and 'igraph', 'network', and 'tidygraph' objects,\n   and on one-mode and two-mode (bipartite) networks.",
    "version": "1.5.6",
    "maintainer": "James Hollway <james.hollway@graduateinstitute.ch>",
    "author": "James Hollway [cre, aut, ctb] (IHEID, ORCID:\n    <https://orcid.org/0000-0002-8361-9647>),\n  Henrique Sposito [ctb] (IHEID, ORCID:\n    <https://orcid.org/0000-0003-3420-6085>),\n  Jael Tan [ctb] (IHEID, ORCID: <https://orcid.org/0000-0002-6234-9764>),\n  Bernhard Bieri [ctb] (ORCID: <https://orcid.org/0000-0001-5943-9059>)",
    "url": "https://stocnet.github.io/migraph/",
    "bug_reports": "https://github.com/stocnet/migraph/issues",
    "repository": "https://cran.r-project.org/package=migraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "migraph Inferential Methods for Multimodal and Other Networks A set of tools for testing networks.\n   It includes functions for univariate and multivariate \n   conditional uniform graph and quadratic assignment procedure testing,\n   and network regression.\n   The package is a complement to \n   'Multimodal Political Networks' (2021, ISBN:9781108985000),\n   and includes various datasets used in the book.\n   Built on the 'manynet' package, all functions operate with matrices, \n   edge lists, and 'igraph', 'network', and 'tidygraph' objects,\n   and on one-mode and two-mode (bipartite) networks.  "
  },
  {
    "id": 16054,
    "package_name": "mikropml",
    "title": "User-Friendly R Package for Supervised Machine Learning\nPipelines",
    "description": "An interface to build machine learning models for\n    classification and regression problems. 'mikropml' implements the ML\n    pipeline described by Top\u00e7uo\u011flu et al. (2020)\n    <doi:10.1128/mBio.00434-20> with reasonable default options for data\n    preprocessing, hyperparameter tuning, cross-validation, testing, model\n    evaluation, and interpretation steps.  See the website\n    <https://www.schlosslab.org/mikropml/> for more information,\n    documentation, and examples.",
    "version": "1.7.0",
    "maintainer": "Kelly Sovacool <sovacool@umich.edu>",
    "author": "Beg\u00fcm Top\u00e7uo\u011flu [aut] (ORCID: <https://orcid.org/0000-0003-3140-537X>),\n  Zena Lapp [aut] (ORCID: <https://orcid.org/0000-0003-4674-2176>),\n  Kelly Sovacool [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3283-829X>),\n  Evan Snitkin [aut] (ORCID: <https://orcid.org/0000-0001-8409-278X>),\n  Jenna Wiens [aut] (ORCID: <https://orcid.org/0000-0002-1057-7722>),\n  Patrick Schloss [aut] (ORCID: <https://orcid.org/0000-0002-6935-4275>),\n  Nick Lesniak [ctb] (ORCID: <https://orcid.org/0000-0001-9359-5194>),\n  Courtney Armour [ctb] (ORCID: <https://orcid.org/0000-0002-5250-1224>),\n  Sarah Lucas [ctb] (ORCID: <https://orcid.org/0000-0003-1676-5801>),\n  Tuomas Borman [ctb] (ORCID: <https://orcid.org/0000-0002-8563-8884>)",
    "url": "https://www.schlosslab.org/mikropml/,\nhttps://github.com/SchlossLab/mikropml",
    "bug_reports": "https://github.com/SchlossLab/mikropml/issues",
    "repository": "https://cran.r-project.org/package=mikropml",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mikropml User-Friendly R Package for Supervised Machine Learning\nPipelines An interface to build machine learning models for\n    classification and regression problems. 'mikropml' implements the ML\n    pipeline described by Top\u00e7uo\u011flu et al. (2020)\n    <doi:10.1128/mBio.00434-20> with reasonable default options for data\n    preprocessing, hyperparameter tuning, cross-validation, testing, model\n    evaluation, and interpretation steps.  See the website\n    <https://www.schlosslab.org/mikropml/> for more information,\n    documentation, and examples.  "
  },
  {
    "id": 16086,
    "package_name": "minimaxALT",
    "title": "Generate Optimal Designs of Accelerated Life Test using\nPSO-Based Algorithm",
    "description": "A computationally efficient solution for generating optimal experimental designs in Accelerated Life Testing (ALT). Leveraging a Particle Swarm Optimization (PSO)-based hybrid algorithm, the package identifies optimal test plans that minimize estimation variance under specified failure models and stress profiles. For more detailed, see Lee et al. (2025), Optimal Robust Strategies for Accelerated Life Tests and Fatigue Testing of Polymer Composite Materials, submitted to Annals of Applied Statistics, <https://imstat.org/journals-and-publications/annals-of-applied-statistics/annals-of-applied-statistics-next-issues/>, and Hoang (2025), Model-Robust Minimax Design of Accelerated Life Tests via PSO-based Hybrid Algorithm, Master' Thesis, Unpublished. ",
    "version": "1.0.2",
    "maintainer": "Hoai-Linh Hoang <hoailinh.hoang17@gmail.com>",
    "author": "Hoai-Linh Hoang [aut, cre],\n  I-Chen Lee [aut],\n  Ping-Yang Chen [aut],\n  Ray-Bing Chen [aut],\n  Weng Kee Wong [aut]",
    "url": "https://github.com/hoanglinh171/minimaxALT",
    "bug_reports": "https://github.com/hoanglinh171/minimaxALT/issues",
    "repository": "https://cran.r-project.org/package=minimaxALT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "minimaxALT Generate Optimal Designs of Accelerated Life Test using\nPSO-Based Algorithm A computationally efficient solution for generating optimal experimental designs in Accelerated Life Testing (ALT). Leveraging a Particle Swarm Optimization (PSO)-based hybrid algorithm, the package identifies optimal test plans that minimize estimation variance under specified failure models and stress profiles. For more detailed, see Lee et al. (2025), Optimal Robust Strategies for Accelerated Life Tests and Fatigue Testing of Polymer Composite Materials, submitted to Annals of Applied Statistics, <https://imstat.org/journals-and-publications/annals-of-applied-statistics/annals-of-applied-statistics-next-issues/>, and Hoang (2025), Model-Robust Minimax Design of Accelerated Life Tests via PSO-based Hybrid Algorithm, Master' Thesis, Unpublished.   "
  },
  {
    "id": 16101,
    "package_name": "mirrorselect",
    "title": "Test CRAN/Bioconductor Mirror Speed",
    "description": "Testing CRAN and Bioconductor mirror speed by recording download time of 'src/base/COPYING' (for CRAN) and 'packages/release/bioc/html/ggtree.html' (for Bioconductor).",
    "version": "0.0.3",
    "maintainer": "Guangchuang Yu <guangchuangyu@gmail.com>",
    "author": "Guangchuang Yu [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6485-8781>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mirrorselect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mirrorselect Test CRAN/Bioconductor Mirror Speed Testing CRAN and Bioconductor mirror speed by recording download time of 'src/base/COPYING' (for CRAN) and 'packages/release/bioc/html/ggtree.html' (for Bioconductor).  "
  },
  {
    "id": 16103,
    "package_name": "mirtCAT",
    "title": "Computerized Adaptive Testing with Multidimensional Item\nResponse Theory",
    "description": "Provides tools to generate HTML interfaces for adaptive\n    and non-adaptive tests using the shiny\n    package (Chalmers (2016) <doi:10.18637/jss.v071.i05>). \n    Suitable for applying unidimensional and multidimensional\n    computerized adaptive tests (CAT) using item response theory methodology and for\n    creating simple questionnaires forms to collect response data directly in R.\n    Additionally, optimal test designs (e.g., \"shadow testing\") are supported\n    for tests that contain a large number of item selection constraints.\n    Finally, package contains tools useful for performing Monte Carlo simulations \n    for studying test item banks.",
    "version": "1.14",
    "maintainer": "Phil Chalmers <rphilip.chalmers@gmail.com>",
    "author": "Phil Chalmers [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5332-2810>),\n  Magnus Nordmo [ctb] (ORCID: <https://orcid.org/0000-0002-1977-1038>)",
    "url": "https://github.com/philchalmers/mirtCAT,\nhttps://github.com/philchalmers/mirtCAT/wiki,\nhttps://groups.google.com/forum/#!forum/mirt-package",
    "bug_reports": "https://github.com/philchalmers/mirtCAT/issues?state=open",
    "repository": "https://cran.r-project.org/package=mirtCAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mirtCAT Computerized Adaptive Testing with Multidimensional Item\nResponse Theory Provides tools to generate HTML interfaces for adaptive\n    and non-adaptive tests using the shiny\n    package (Chalmers (2016) <doi:10.18637/jss.v071.i05>). \n    Suitable for applying unidimensional and multidimensional\n    computerized adaptive tests (CAT) using item response theory methodology and for\n    creating simple questionnaires forms to collect response data directly in R.\n    Additionally, optimal test designs (e.g., \"shadow testing\") are supported\n    for tests that contain a large number of item selection constraints.\n    Finally, package contains tools useful for performing Monte Carlo simulations \n    for studying test item banks.  "
  },
  {
    "id": 16142,
    "package_name": "mixIndependR",
    "title": "Genetics and Independence Testing of Mixed Genetic Panels",
    "description": "Developed to deal with multi-locus genotype data, this package is especially designed for those panel which include different type of markers. Basic genetic parameters like allele frequency, genotype frequency, heterozygosity and Hardy-Weinberg test of mixed genetic data can be obtained.     In addition, a new test for mutual independence which is compatible for mixed genetic data is developed in this package.",
    "version": "1.0.0",
    "maintainer": "Bing Song <bing.song@utsouthwestern.edu>",
    "author": "Bing Song",
    "url": "https://github.com/ice4prince/mixIndependR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mixIndependR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixIndependR Genetics and Independence Testing of Mixed Genetic Panels Developed to deal with multi-locus genotype data, this package is especially designed for those panel which include different type of markers. Basic genetic parameters like allele frequency, genotype frequency, heterozygosity and Hardy-Weinberg test of mixed genetic data can be obtained.     In addition, a new test for mutual independence which is compatible for mixed genetic data is developed in this package.  "
  },
  {
    "id": 16175,
    "package_name": "mixtree",
    "title": "A Statistical Framework for Comparing Sets of Trees",
    "description": "Statistical framework for comparing sets of trees using hypothesis testing methods. Designed for transmission trees, phylogenetic trees, and directed acyclic graphs (DAGs), the package implements chi-squared tests to compare edge frequencies between sets and PERMANOVA to analyse topological dissimilarities with customisable distance metrics, following Anderson (2001) <doi:10.1111/j.1442-9993.2001.01070.pp.x>.",
    "version": "0.0.1",
    "maintainer": "Cyril Geismar <c.geismar21@imperial.ac.uk>",
    "author": "Cyril Geismar [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-8486-5890>)",
    "url": "https://cygei.github.io/mixtree/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mixtree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mixtree A Statistical Framework for Comparing Sets of Trees Statistical framework for comparing sets of trees using hypothesis testing methods. Designed for transmission trees, phylogenetic trees, and directed acyclic graphs (DAGs), the package implements chi-squared tests to compare edge frequencies between sets and PERMANOVA to analyse topological dissimilarities with customisable distance metrics, following Anderson (2001) <doi:10.1111/j.1442-9993.2001.01070.pp.x>.  "
  },
  {
    "id": 16194,
    "package_name": "mlearning",
    "title": "Machine Learning Algorithms with Unified Interface and Confusion\nMatrices",
    "description": "A unified interface is provided to various machine learning\n  algorithms like linear or quadratic discriminant analysis, k-nearest\n  neighbors, random forest, support vector machine, ... It allows to train,\n  test, and apply cross-validation using similar functions and function\n  arguments with a minimalist and clean, formula-based interface. Missing data\n  are processed the same way as base and stats R functions for all algorithms,\n  both in training and testing. Confusion matrices are also provided with a rich\n  set of metrics calculated and a few specific plots.",
    "version": "1.2.1",
    "maintainer": "Philippe Grosjean <phgrosjean@sciviews.org>",
    "author": "Philippe Grosjean [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2694-9471>),\n  Kevin Denis [aut]",
    "url": "https://www.sciviews.org/mlearning/",
    "bug_reports": "https://github.com/SciViews/mlearning/issues",
    "repository": "https://cran.r-project.org/package=mlearning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlearning Machine Learning Algorithms with Unified Interface and Confusion\nMatrices A unified interface is provided to various machine learning\n  algorithms like linear or quadratic discriminant analysis, k-nearest\n  neighbors, random forest, support vector machine, ... It allows to train,\n  test, and apply cross-validation using similar functions and function\n  arguments with a minimalist and clean, formula-based interface. Missing data\n  are processed the same way as base and stats R functions for all algorithms,\n  both in training and testing. Confusion matrices are also provided with a rich\n  set of metrics calculated and a few specific plots.  "
  },
  {
    "id": 16250,
    "package_name": "mlrv",
    "title": "Long-Run Variance Estimation in Time Series Regression",
    "description": "Plug-in and difference-based long-run covariance matrix estimation for time series regression. Two applications of hypothesis testing are also provided. The first one is for testing for structural stability in coefficient functions. The second one is aimed at detecting long memory in time series regression. Lujia Bai and Weichi Wu (2024)<doi:10.3150/23-BEJ1680> Zhou Zhou and Wei Biao Wu(2010)<doi:10.1111/j.1467-9868.2010.00743.x> Jianqing Fan and Wenyang Zhang<doi:10.1214/aos/1017939139> Lujia Bai and Weichi Wu(2024)<doi:10.1093/biomet/asae013> Dimitris N. Politis, Joseph P. Romano, Michael Wolf(1999)<doi:10.1007/978-1-4612-1554-7> Weichi Wu and Zhou Zhou(2018)<doi:10.1214/17-AOS1582>.",
    "version": "0.1.2",
    "maintainer": "Lujia Bai <bailujia98@gmail.com>",
    "author": "Lujia Bai [aut, cre],\n  Weichi Wu [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mlrv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mlrv Long-Run Variance Estimation in Time Series Regression Plug-in and difference-based long-run covariance matrix estimation for time series regression. Two applications of hypothesis testing are also provided. The first one is for testing for structural stability in coefficient functions. The second one is aimed at detecting long memory in time series regression. Lujia Bai and Weichi Wu (2024)<doi:10.3150/23-BEJ1680> Zhou Zhou and Wei Biao Wu(2010)<doi:10.1111/j.1467-9868.2010.00743.x> Jianqing Fan and Wenyang Zhang<doi:10.1214/aos/1017939139> Lujia Bai and Weichi Wu(2024)<doi:10.1093/biomet/asae013> Dimitris N. Politis, Joseph P. Romano, Michael Wolf(1999)<doi:10.1007/978-1-4612-1554-7> Weichi Wu and Zhou Zhou(2018)<doi:10.1214/17-AOS1582>.  "
  },
  {
    "id": 16277,
    "package_name": "mmibain",
    "title": "Bayesian Informative Hypotheses Evaluation Web Applications",
    "description": "Researchers often have expectations about the relations between means\n    of different groups or standardized regression coefficients; using informative\n    hypothesis testing to incorporate these expectations into the analysis through\n    order constraints increases statistical power\n    Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>. Another valuable\n    tool, the Bayes factor, can evaluate evidence for multiple hypotheses without\n    concerns about multiple testing, and can be used in Bayesian updating\n    Hoijtink, Mulder, van Lissa & Gu (2019) <doi:10.1037/met0000201>. The 'bain'\n    R package enables informative hypothesis testing using the Bayes factor. The\n    'mmibain' package provides 'shiny' web applications based on 'bain'. The\n    RepliCrisis() function launches a 'shiny' card game to simulate the evaluation\n    of replication studies while the mmibain() function launches a 'shiny'\n    application to fit Bayesian informative hypotheses evaluation models from\n    'bain'.",
    "version": "0.2.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/mmibain",
    "bug_reports": "https://github.com/mightymetrika/mmibain/issues",
    "repository": "https://cran.r-project.org/package=mmibain",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmibain Bayesian Informative Hypotheses Evaluation Web Applications Researchers often have expectations about the relations between means\n    of different groups or standardized regression coefficients; using informative\n    hypothesis testing to incorporate these expectations into the analysis through\n    order constraints increases statistical power\n    Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>. Another valuable\n    tool, the Bayes factor, can evaluate evidence for multiple hypotheses without\n    concerns about multiple testing, and can be used in Bayesian updating\n    Hoijtink, Mulder, van Lissa & Gu (2019) <doi:10.1037/met0000201>. The 'bain'\n    R package enables informative hypothesis testing using the Bayes factor. The\n    'mmibain' package provides 'shiny' web applications based on 'bain'. The\n    RepliCrisis() function launches a 'shiny' card game to simulate the evaluation\n    of replication studies while the mmibain() function launches a 'shiny'\n    application to fit Bayesian informative hypotheses evaluation models from\n    'bain'.  "
  },
  {
    "id": 16279,
    "package_name": "mmirestriktor",
    "title": "Informative Hypothesis Testing Web Applications",
    "description": "Offering enhanced statistical power compared to traditional\n    hypothesis testing methods, informative hypothesis testing allows researchers\n    to explicitly model their expectations regarding the relationships among\n    parameters. An important software tool for this framework is 'restriktor'.\n    The 'mmirestriktor' package provides 'shiny' web applications to implement\n    some of the basic functionality of 'restriktor'. The mmirestriktor() function\n    launches a 'shiny' application for fitting and analyzing models with\n    constraints. The FbarCards() function launches a card game application which\n    can help build intuition about informative hypothesis testing. The\n    iht_interpreter() helps interpret informative hypothesis testing results based\n    on guidelines in Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>.",
    "version": "0.3.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/mmirestriktor",
    "bug_reports": "https://github.com/mightymetrika/mmirestriktor/issues",
    "repository": "https://cran.r-project.org/package=mmirestriktor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmirestriktor Informative Hypothesis Testing Web Applications Offering enhanced statistical power compared to traditional\n    hypothesis testing methods, informative hypothesis testing allows researchers\n    to explicitly model their expectations regarding the relationships among\n    parameters. An important software tool for this framework is 'restriktor'.\n    The 'mmirestriktor' package provides 'shiny' web applications to implement\n    some of the basic functionality of 'restriktor'. The mmirestriktor() function\n    launches a 'shiny' application for fitting and analyzing models with\n    constraints. The FbarCards() function launches a card game application which\n    can help build intuition about informative hypothesis testing. The\n    iht_interpreter() helps interpret informative hypothesis testing results based\n    on guidelines in Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>.  "
  },
  {
    "id": 16280,
    "package_name": "mmmgee",
    "title": "Simultaneous Inference for Multiple Linear Contrasts in GEE\nModels",
    "description": "Provides global hypothesis tests, multiple testing procedures and simultaneous confidence intervals for multiple linear contrasts of regression\n\tcoefficients in a single generalized estimating equation (GEE) model or across multiple GEE models. GEE models are fit by a modified version of the 'geeM' package.",
    "version": "1.20",
    "maintainer": "Robin Ristl <robin.ristl@meduniwien.ac.at>",
    "author": "Robin Ristl [aut, cre],\n  Lee McDaniel [ctb] (Author of 'geeM' package),\n  Nick Henderson [ctb] (Author of 'geeM' package),\n  Melanie Prague [ctb] (Contributor to 'geeM' package)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mmmgee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmmgee Simultaneous Inference for Multiple Linear Contrasts in GEE\nModels Provides global hypothesis tests, multiple testing procedures and simultaneous confidence intervals for multiple linear contrasts of regression\n\tcoefficients in a single generalized estimating equation (GEE) model or across multiple GEE models. GEE models are fit by a modified version of the 'geeM' package.  "
  },
  {
    "id": 16286,
    "package_name": "mmrm",
    "title": "Mixed Models for Repeated Measures",
    "description": "Mixed models for repeated measures (MMRM) are a popular\n    choice for analyzing longitudinal continuous outcomes in randomized\n    clinical trials and beyond; see Cnaan, Laird and Slasor (1997)\n    <doi:10.1002/(SICI)1097-0258(19971030)16:20%3C2349::AID-SIM667%3E3.0.CO;2-E>\n    for a tutorial and Mallinckrodt, Lane, Schnell, Peng and Mancuso\n    (2008) <doi:10.1177/009286150804200402> for a review. This package\n    implements MMRM based on the marginal linear model without random\n    effects using Template Model Builder ('TMB') which enables fast and\n    robust model fitting. Users can specify a variety of covariance\n    matrices, weight observations, fit models with restricted or standard\n    maximum likelihood inference, perform hypothesis testing with\n    Satterthwaite or Kenward-Roger adjustment, and extract least square\n    means estimates by using 'emmeans'.",
    "version": "0.3.16",
    "maintainer": "Daniel Sabanes Bove <daniel.sabanes_bove@rconis.com>",
    "author": "Daniel Sabanes Bove [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0176-9239>),\n  Liming Li [aut] (ORCID: <https://orcid.org/0009-0008-6870-0878>),\n  Julia Dedic [aut],\n  Doug Kelkhoff [aut],\n  Kevin Kunzmann [aut],\n  Brian Matthew Lang [aut],\n  Christian Stock [aut],\n  Ya Wang [aut],\n  Craig Gower-Page [ctb],\n  Dan James [aut],\n  Jonathan Sidi [aut],\n  Daniel Leibovitz [aut],\n  Daniel D. Sjoberg [aut] (ORCID:\n    <https://orcid.org/0000-0003-0862-2018>),\n  Nikolas Ivan Krieger [aut] (ORCID:\n    <https://orcid.org/0000-0002-4581-3545>),\n  Lukas A. Widmer [ctb] (ORCID: <https://orcid.org/0000-0003-1471-3493>),\n  Boehringer Ingelheim Ltd. [cph, fnd],\n  Gilead Sciences, Inc. [cph, fnd],\n  F. Hoffmann-La Roche AG [cph, fnd],\n  Merck Sharp & Dohme, Inc. [cph, fnd],\n  AstraZeneca plc [cph, fnd],\n  inferential.biostatistics GmbH [cph, fnd]",
    "url": "https://openpharma.github.io/mmrm/",
    "bug_reports": "https://github.com/openpharma/mmrm/issues",
    "repository": "https://cran.r-project.org/package=mmrm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mmrm Mixed Models for Repeated Measures Mixed models for repeated measures (MMRM) are a popular\n    choice for analyzing longitudinal continuous outcomes in randomized\n    clinical trials and beyond; see Cnaan, Laird and Slasor (1997)\n    <doi:10.1002/(SICI)1097-0258(19971030)16:20%3C2349::AID-SIM667%3E3.0.CO;2-E>\n    for a tutorial and Mallinckrodt, Lane, Schnell, Peng and Mancuso\n    (2008) <doi:10.1177/009286150804200402> for a review. This package\n    implements MMRM based on the marginal linear model without random\n    effects using Template Model Builder ('TMB') which enables fast and\n    robust model fitting. Users can specify a variety of covariance\n    matrices, weight observations, fit models with restricted or standard\n    maximum likelihood inference, perform hypothesis testing with\n    Satterthwaite or Kenward-Roger adjustment, and extract least square\n    means estimates by using 'emmeans'.  "
  },
  {
    "id": 16298,
    "package_name": "moRphomenses",
    "title": "Geometric Morphometric Tools to Align, Scale, and Compare\n\"Shape\" of Menstrual Cycle Hormones",
    "description": "Mitteroecker & Gunz (2009) <doi:10.1007/s11692-009-9055-x> describe\n    how geometric morphometric methods allow researchers to quantify the size\n    and shape of physical biological structures. We provide tools to extend geometric\n    morphometric principles to the study of non-physical structures, hormone profiles, \n    as outlined in Ehrlich et al (2021) <doi:10.1002/ajpa.24514>. Easily transform \n    daily measures into multivariate landmark-based data. Includes custom functions \n    to apply multivariate methods for data exploration as well as hypothesis testing. \n    Also includes 'shiny' web app to streamline data exploration. Developed to study \n    menstrual cycle hormones but functions have been generalized and should be \n    applicable to any biomarker over any time period.",
    "version": "1.0.3",
    "maintainer": "Daniel Ehrlich <dan.ehrlich.e@gmail.com>",
    "author": "Daniel Ehrlich [aut, cre]",
    "url": "<https://github.com/ClancyLabUIUC/moRphomenses>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=moRphomenses",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "moRphomenses Geometric Morphometric Tools to Align, Scale, and Compare\n\"Shape\" of Menstrual Cycle Hormones Mitteroecker & Gunz (2009) <doi:10.1007/s11692-009-9055-x> describe\n    how geometric morphometric methods allow researchers to quantify the size\n    and shape of physical biological structures. We provide tools to extend geometric\n    morphometric principles to the study of non-physical structures, hormone profiles, \n    as outlined in Ehrlich et al (2021) <doi:10.1002/ajpa.24514>. Easily transform \n    daily measures into multivariate landmark-based data. Includes custom functions \n    to apply multivariate methods for data exploration as well as hypothesis testing. \n    Also includes 'shiny' web app to streamline data exploration. Developed to study \n    menstrual cycle hormones but functions have been generalized and should be \n    applicable to any biomarker over any time period.  "
  },
  {
    "id": 16304,
    "package_name": "mockthat",
    "title": "Function Mocking for Unit Testing",
    "description": "With the deprecation of mocking capabilities shipped with\n    'testthat' as of 'edition 3' it is left to third-party packages to replace\n    this functionality, which in some test-scenarios is essential in order to\n    run unit tests in limited environments (such as no Internet connection).\n    Mocking in this setting means temporarily substituting a function with a\n    stub that acts in some sense like the original function (for example by\n    serving a HTTP response that has been cached as a file). The only exported\n    function 'with_mock()' is modeled after the eponymous 'testthat' function\n    with the intention of providing a drop-in replacement.",
    "version": "0.2.8",
    "maintainer": "Nicolas Bennett <nicolas.bennett@stat.math.ethz.ch>",
    "author": "Nicolas Bennett [aut, cre]",
    "url": "https://nbenn.github.io/mockthat/",
    "bug_reports": "https://github.com/nbenn/mockthat/issues",
    "repository": "https://cran.r-project.org/package=mockthat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mockthat Function Mocking for Unit Testing With the deprecation of mocking capabilities shipped with\n    'testthat' as of 'edition 3' it is left to third-party packages to replace\n    this functionality, which in some test-scenarios is essential in order to\n    run unit tests in limited environments (such as no Internet connection).\n    Mocking in this setting means temporarily substituting a function with a\n    stub that acts in some sense like the original function (for example by\n    serving a HTTP response that has been cached as a file). The only exported\n    function 'with_mock()' is modeled after the eponymous 'testthat' function\n    with the intention of providing a drop-in replacement.  "
  },
  {
    "id": 16324,
    "package_name": "modeltests",
    "title": "Testing Infrastructure for Broom Model Generics",
    "description": "Provides a number of testthat tests that can be\n    used to verify that tidy(), glance() and augment() methods meet\n    consistent specifications. This allows methods for the same generic to\n    be spread across multiple packages, since all of those packages can\n    make the same guarantees to users about returned objects.",
    "version": "0.1.7",
    "maintainer": "Alex Hayes <alexpghayes@gmail.com>",
    "author": "Alex Hayes [aut, cre] (ORCID: <https://orcid.org/0000-0002-4985-5160>),\n  Simon Couch [aut]",
    "url": "https://github.com/alexpghayes/modeltests",
    "bug_reports": "https://github.com/alexpghayes/modeltests/issues",
    "repository": "https://cran.r-project.org/package=modeltests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "modeltests Testing Infrastructure for Broom Model Generics Provides a number of testthat tests that can be\n    used to verify that tidy(), glance() and augment() methods meet\n    consistent specifications. This allows methods for the same generic to\n    be spread across multiple packages, since all of those packages can\n    make the same guarantees to users about returned objects.  "
  },
  {
    "id": 16444,
    "package_name": "mpt",
    "title": "Multinomial Processing Tree Models",
    "description": "Fitting and testing multinomial processing tree (MPT) models, a\n  class of nonlinear models for categorical data.  The parameters are the\n  link probabilities of a tree-like graph and represent the latent cognitive\n  processing steps executed to arrive at observable response categories\n  (Batchelder & Riefer, 1999 <doi:10.3758/bf03210812>; Erdfelder et al., 2009\n  <doi:10.1027/0044-3409.217.3.108>; Riefer & Batchelder, 1988\n  <doi:10.1037/0033-295x.95.3.318>).",
    "version": "1.0-0",
    "maintainer": "Florian Wickelmaier <wickelmaier@web.de>",
    "author": "Florian Wickelmaier [aut, cre],\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>)",
    "url": "https://www.mathpsy.uni-tuebingen.de/wickelmaier/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mpt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mpt Multinomial Processing Tree Models Fitting and testing multinomial processing tree (MPT) models, a\n  class of nonlinear models for categorical data.  The parameters are the\n  link probabilities of a tree-like graph and represent the latent cognitive\n  processing steps executed to arrive at observable response categories\n  (Batchelder & Riefer, 1999 <doi:10.3758/bf03210812>; Erdfelder et al., 2009\n  <doi:10.1027/0044-3409.217.3.108>; Riefer & Batchelder, 1988\n  <doi:10.1037/0033-295x.95.3.318>).  "
  },
  {
    "id": 16497,
    "package_name": "mstR",
    "title": "Procedures to Generate Patterns under Multistage Testing",
    "description": "Generation of response patterns under dichotomous and polytomous computerized multistage testing (MST) framework. It holds various item response theory (IRT) and score-based methods to select the next module and estimate ability levels (Magis, Yan and von Davier (2017, ISBN:978-3-319-69218-0)). ",
    "version": "1.2",
    "maintainer": "David Magis <david.magis@uliege.be>",
    "author": "David Magis (U Liege, Belgium), Duanli Yan (ETS, USA), Alina von Davier (ACTNext, USA)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mstR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mstR Procedures to Generate Patterns under Multistage Testing Generation of response patterns under dichotomous and polytomous computerized multistage testing (MST) framework. It holds various item response theory (IRT) and score-based methods to select the next module and estimate ability levels (Magis, Yan and von Davier (2017, ISBN:978-3-319-69218-0)).   "
  },
  {
    "id": 16530,
    "package_name": "multIntTestFunc",
    "title": "Provides Test Functions for Multivariate Integration",
    "description": "Provides implementations of functions that can be used to\n    test multivariate integration routines. The package covers six\n    different integration domains (unit hypercube, unit ball, unit sphere,\n    standard simplex, non-negative real numbers and R^n). For each domain\n    several functions with different properties (smooth,\n    non-differentiable, ...) are available. The functions are available in\n    all dimensions n >= 1. For each function the exact value of the\n    integral is known and implemented to allow testing the accuracy of\n    multivariate integration routines. Details on the available test\n    functions can be found at on the development website.",
    "version": "0.3.0",
    "maintainer": "Klaus Herrmann <klaus.herrmann@usherbrooke.ca>",
    "author": "Klaus Herrmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8044-5717>)",
    "url": "https://github.com/KlausHerrmann/multIntTestFunc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multIntTestFunc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multIntTestFunc Provides Test Functions for Multivariate Integration Provides implementations of functions that can be used to\n    test multivariate integration routines. The package covers six\n    different integration domains (unit hypercube, unit ball, unit sphere,\n    standard simplex, non-negative real numbers and R^n). For each domain\n    several functions with different properties (smooth,\n    non-differentiable, ...) are available. The functions are available in\n    all dimensions n >= 1. For each function the exact value of the\n    integral is known and implemented to allow testing the accuracy of\n    multivariate integration routines. Details on the available test\n    functions can be found at on the development website.  "
  },
  {
    "id": 16534,
    "package_name": "multfisher",
    "title": "Optimal Exact Tests for Multiple Binary Endpoints",
    "description": "Calculates exact hypothesis tests to compare a treatment and a reference group with respect to multiple binary endpoints.\n    The tested null hypothesis is an identical multidimensional distribution of successes and failures in both groups. The alternative\n    hypothesis is a larger success proportion in the treatment group in at least one endpoint. The tests are based on the multivariate\n    permutation distribution of subjects between the two groups. For this permutation distribution, rejection regions are calculated \n    that satisfy one of different possible optimization criteria. In particular, regions with maximal exhaustion of the nominal\n    significance level, maximal power under a specified alternative or maximal number of elements can be found. Optimization is achieved\n    by a branch-and-bound algorithm. By application of the closed testing principle, the global hypothesis tests are extended to multiple\n    testing procedures.",
    "version": "1.1",
    "maintainer": "Robin Ristl <robin.ristl@meduniwien.ac.at>",
    "author": "Robin Ristl [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multfisher",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multfisher Optimal Exact Tests for Multiple Binary Endpoints Calculates exact hypothesis tests to compare a treatment and a reference group with respect to multiple binary endpoints.\n    The tested null hypothesis is an identical multidimensional distribution of successes and failures in both groups. The alternative\n    hypothesis is a larger success proportion in the treatment group in at least one endpoint. The tests are based on the multivariate\n    permutation distribution of subjects between the two groups. For this permutation distribution, rejection regions are calculated \n    that satisfy one of different possible optimization criteria. In particular, regions with maximal exhaustion of the nominal\n    significance level, maximal power under a specified alternative or maximal number of elements can be found. Optimization is achieved\n    by a branch-and-bound algorithm. By application of the closed testing principle, the global hypothesis tests are extended to multiple\n    testing procedures.  "
  },
  {
    "id": 16539,
    "package_name": "multiCA",
    "title": "Multinomial Cochran-Armitage Trend Test",
    "description": "Implements a generalization of the Cochran-Armitage trend test to\n    multinomial data. In addition to an overall test, multiple testing adjusted\n    p-values for trend in individual outcomes and power calculation is\n    available.",
    "version": "1.2.0",
    "maintainer": "Aniko Szabo <aszabo@mcw.edu>",
    "author": "Aniko Szabo [aut, cre]",
    "url": "https://github.com/anikoszabo/multiCA",
    "bug_reports": "https://github.com/anikoszabo/multiCA/issues",
    "repository": "https://cran.r-project.org/package=multiCA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multiCA Multinomial Cochran-Armitage Trend Test Implements a generalization of the Cochran-Armitage trend test to\n    multinomial data. In addition to an overall test, multiple testing adjusted\n    p-values for trend in individual outcomes and power calculation is\n    available.  "
  },
  {
    "id": 16580,
    "package_name": "multimedia",
    "title": "Multimodal Mediation Analysis",
    "description": "Multimodal mediation analysis is an emerging problem in microbiome data analysis. Multimedia make advanced mediation analysis techniques easy to use, ensuring that all statistical components are transparent and adaptable to specific problem contexts. The package provides a uniform interface to direct and indirect effect estimation, synthetic null hypothesis testing, bootstrap confidence interval construction, and sensitivity analysis. More details are available in Jiang et al. (2024) \"multimedia: Multimodal Mediation Analysis of Microbiome Data\" <doi:10.1101/2024.03.27.587024>.",
    "version": "0.2.0",
    "maintainer": "Kris Sankaran <ksankaran@wisc.edu>",
    "author": "Kris Sankaran [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-1971>),\n  Hanying Jiang [aut]",
    "url": "https://krisrs1128.github.io/multimedia/,\nhttps://github.com/krisrs1128/multimedia/",
    "bug_reports": "https://github.com/krisrs1128/multimedia/issues/",
    "repository": "https://cran.r-project.org/package=multimedia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multimedia Multimodal Mediation Analysis Multimodal mediation analysis is an emerging problem in microbiome data analysis. Multimedia make advanced mediation analysis techniques easy to use, ensuring that all statistical components are transparent and adaptable to specific problem contexts. The package provides a uniform interface to direct and indirect effect estimation, synthetic null hypothesis testing, bootstrap confidence interval construction, and sensitivity analysis. More details are available in Jiang et al. (2024) \"multimedia: Multimodal Mediation Analysis of Microbiome Data\" <doi:10.1101/2024.03.27.587024>.  "
  },
  {
    "id": 16583,
    "package_name": "multimode",
    "title": "Mode Testing and Exploring",
    "description": "Different examples and methods for testing (including different proposals described in Ameijeiras-Alonso et al., 2019 <DOI:10.1007/s11749-018-0611-5>) and exploring (including the mode tree, mode forest and SiZer) the number of modes using nonparametric techniques <DOI:10.18637/jss.v097.i09>.",
    "version": "1.5",
    "maintainer": "Jose Ameijeiras-Alonso <jose.ameijeiras@usc.es>",
    "author": "Jose Ameijeiras-Alonso [aut,cre],\n  Rosa M. Crujeiras [aut], \n  Alberto Rodr\u00edguez-Casal [aut],\n  The R Core Team 1996-2012 [ctb,cph] (C function 'BinDist2' obtained from package 'stats'),\n  The R Foundation 2005 [ctb,cph] (C function 'BinDist2' obtained from package 'stats')",
    "url": "https://doi.org/10.18637/jss.v097.i09",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multimode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multimode Mode Testing and Exploring Different examples and methods for testing (including different proposals described in Ameijeiras-Alonso et al., 2019 <DOI:10.1007/s11749-018-0611-5>) and exploring (including the mode tree, mode forest and SiZer) the number of modes using nonparametric techniques <DOI:10.18637/jss.v097.i09>.  "
  },
  {
    "id": 16620,
    "package_name": "multxpert",
    "title": "Common Multiple Testing Procedures and Gatekeeping Procedures",
    "description": "Implementation of commonly used p-value-based and\n        parametric multiple testing procedures (computation of adjusted\n        p-values and simultaneous confidence intervals) and parallel\n        gatekeeping procedures based on the methodology presented in\n        the book \"Multiple Testing Problems in Pharmaceutical\n        Statistics\" (edited by Alex Dmitrienko, Ajit C. Tamhane and\n        Frank Bretz) published by Chapman and Hall/CRC Press 2009.",
    "version": "0.1.1",
    "maintainer": "Eric Nantz <eric.nantz@gmail.com>",
    "author": "Alex Dmitrienko, Eric Nantz, and Gautier Paux, with\n        contributions by Thomas Brechenmacher",
    "url": "http://multxpert.com/wiki/MultXpert_package",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multxpert",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multxpert Common Multiple Testing Procedures and Gatekeeping Procedures Implementation of commonly used p-value-based and\n        parametric multiple testing procedures (computation of adjusted\n        p-values and simultaneous confidence intervals) and parallel\n        gatekeeping procedures based on the methodology presented in\n        the book \"Multiple Testing Problems in Pharmaceutical\n        Statistics\" (edited by Alex Dmitrienko, Ajit C. Tamhane and\n        Frank Bretz) published by Chapman and Hall/CRC Press 2009.  "
  },
  {
    "id": 16633,
    "package_name": "mutoss",
    "title": "Unified Multiple Testing Procedures",
    "description": "Designed to ease the application and comparison of multiple\n    hypothesis testing procedures for FWER, gFWER, FDR and FDX. Methods are \n    standardized and usable by the accompanying 'mutossGUI'.",
    "version": "0.1-13",
    "maintainer": "Kornelius Rohmeyer <rohmeyer@small-projects.de>",
    "author": "MuToss Coding Team (Berlin 2010), Gilles Blanchard, Thorsten Dickhaus,\n    Niklas Hack, Frank Konietschke, Kornelius Rohmeyer, \n    Jonathan Rosenblatt, Marsel Scheer, Wiebke Werft",
    "url": "https://github.com/kornl/mutoss/",
    "bug_reports": "https://github.com/kornl/mutoss/issues/",
    "repository": "https://cran.r-project.org/package=mutoss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mutoss Unified Multiple Testing Procedures Designed to ease the application and comparison of multiple\n    hypothesis testing procedures for FWER, gFWER, FDR and FDX. Methods are \n    standardized and usable by the accompanying 'mutossGUI'.  "
  },
  {
    "id": 16634,
    "package_name": "muttest",
    "title": "Mutation Testing",
    "description": "Measure quality of your tests.\n    'muttest' introduces small changes (mutations) to your code\n    and runs your tests to check if they catch the changes.\n    If they do, your tests are good.\n    If not, your assertions are not specific enough.\n    'muttest' gives you percent score of how often your tests catch the changes.",
    "version": "0.1.0",
    "maintainer": "Jakub Sobolewski <jakupsob@gmail.com>",
    "author": "Jakub Sobolewski [aut, cre]",
    "url": "https://jakubsob.github.io/muttest/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=muttest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "muttest Mutation Testing Measure quality of your tests.\n    'muttest' introduces small changes (mutations) to your code\n    and runs your tests to check if they catch the changes.\n    If they do, your tests are good.\n    If not, your assertions are not specific enough.\n    'muttest' gives you percent score of how often your tests catch the changes.  "
  },
  {
    "id": 16790,
    "package_name": "negligible",
    "title": "A Collection of Functions for Negligible Effect/Equivalence\nTesting",
    "description": "Researchers often want to evaluate whether there is a negligible\n    relationship among variables. The 'negligible' package provides functions that \n    are useful for conducting negligible effect testing (also called\n    equivalence testing). For example, there are functions for evaluating the \n    equivalence of means or the presence of a negligible association \n    (correlation or  regression). Beribisky, N., Mara, C., & Cribbie, R. A. (2020) <doi:10.20982/tqmp.16.4.p424>.\n    Beribisky, N., Davidson, H., Cribbie, R. A. (2019) <doi:10.7717/peerj.6853>.\n    Shiskina, T., Farmus, L., & Cribbie, R. A. (2018) <doi:10.20982/tqmp.14.3.p167>.\n    Mara, C. & Cribbie, R. A. (2017) <doi:10.1080/00220973.2017.1301356>.\n    Counsell, A. & Cribbie, R. A. (2015) <doi:10.1111/bmsp.12045>.\n    van Wieringen, K. & Cribbie, R. A. (2014) <doi:10.1111/bmsp.12015>.\n    Goertzen, J. R. & Cribbie, R. A. (2010) <doi:10.1348/000711009x475853>.\n    Cribbie, R. A., Gruman, J. & Arpin-Cribbie, C. (2004) <doi:10.1002/jclp.10217>.",
    "version": "0.1.9",
    "maintainer": "Robert Cribbie <cribbie@yorku.ca>",
    "author": "Robert Cribbie [aut, cre],\n  Udi Alter [aut],\n  Nataly Beribisky [aut],\n  Phil Chalmers [aut],\n  Alyssa Counsell [aut],\n  Linda Farmus [aut],\n  Naomi Martinez Gutierrez [aut],\n  Victoria Ng [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=negligible",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "negligible A Collection of Functions for Negligible Effect/Equivalence\nTesting Researchers often want to evaluate whether there is a negligible\n    relationship among variables. The 'negligible' package provides functions that \n    are useful for conducting negligible effect testing (also called\n    equivalence testing). For example, there are functions for evaluating the \n    equivalence of means or the presence of a negligible association \n    (correlation or  regression). Beribisky, N., Mara, C., & Cribbie, R. A. (2020) <doi:10.20982/tqmp.16.4.p424>.\n    Beribisky, N., Davidson, H., Cribbie, R. A. (2019) <doi:10.7717/peerj.6853>.\n    Shiskina, T., Farmus, L., & Cribbie, R. A. (2018) <doi:10.20982/tqmp.14.3.p167>.\n    Mara, C. & Cribbie, R. A. (2017) <doi:10.1080/00220973.2017.1301356>.\n    Counsell, A. & Cribbie, R. A. (2015) <doi:10.1111/bmsp.12045>.\n    van Wieringen, K. & Cribbie, R. A. (2014) <doi:10.1111/bmsp.12015>.\n    Goertzen, J. R. & Cribbie, R. A. (2010) <doi:10.1348/000711009x475853>.\n    Cribbie, R. A., Gruman, J. & Arpin-Cribbie, C. (2004) <doi:10.1002/jclp.10217>.  "
  },
  {
    "id": 16840,
    "package_name": "nett",
    "title": "Network Analysis and Community Detection",
    "description": "Features tools for the network data analysis and community detection. \n    Provides multiple methods for fitting, model selection and goodness-of-fit testing in degree-corrected stochastic blocks models. \n    Most of the computations are fast and scalable for sparse networks, esp. for Poisson versions of the models.\n    Implements the following: \n    Amini, Chen, Bickel and Levina (2013) <doi:10.1214/13-AOS1138>\n    Bickel and Sarkar (2015) <doi:10.1111/rssb.12117>\n    Lei (2016) <doi:10.1214/15-AOS1370>\n    Wang and Bickel (2017) <doi:10.1214/16-AOS1457>\n    Zhang and Amini (2020) <arXiv:2012.15047>\n    Le and Levina (2022) <doi:10.1214/21-EJS1971>.",
    "version": "1.0.0",
    "maintainer": "Arash A. Amini <aaamini@ucla.edu>",
    "author": "Arash A. Amini [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2808-8310>),\n  Linfan Zhang [aut]",
    "url": "https://github.com/aaamini/nett",
    "bug_reports": "https://github.com/aaamini/nett/issues",
    "repository": "https://cran.r-project.org/package=nett",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nett Network Analysis and Community Detection Features tools for the network data analysis and community detection. \n    Provides multiple methods for fitting, model selection and goodness-of-fit testing in degree-corrected stochastic blocks models. \n    Most of the computations are fast and scalable for sparse networks, esp. for Poisson versions of the models.\n    Implements the following: \n    Amini, Chen, Bickel and Levina (2013) <doi:10.1214/13-AOS1138>\n    Bickel and Sarkar (2015) <doi:10.1111/rssb.12117>\n    Lei (2016) <doi:10.1214/15-AOS1370>\n    Wang and Bickel (2017) <doi:10.1214/16-AOS1457>\n    Zhang and Amini (2020) <arXiv:2012.15047>\n    Le and Levina (2022) <doi:10.1214/21-EJS1971>.  "
  },
  {
    "id": 16870,
    "package_name": "nevada",
    "title": "Network-Valued Data Analysis",
    "description": "A flexible statistical framework for network-valued data analysis. \n    It leverages the complexity of the space of distributions on graphs by using \n    the permutation framework for inference as implemented in the 'flipr' package. \n    Currently, only the two-sample testing problem is covered and generalization \n    to k samples and regression will be added in the future as well. It is a \n    4-step procedure where the user chooses a suitable representation of the \n    networks, a suitable metric to embed the representation into a metric space, \n    one or more test statistics to target specific aspects of the distributions \n    to be compared and a formula to compute the permutation p-value. Two types \n    of inference are provided: a global test answering whether there is a \n    difference between the distributions that generated the two samples and a \n    local test for localizing differences on the network structure. The latter \n    is assumed to be shared by all networks of both samples. References: Lovato, \n    I., Pini, A., Stamm, A., Vantini, S. (2020) \"Model-free two-sample test for \n    network-valued data\" <doi:10.1016/j.csda.2019.106896>; Lovato, I., Pini, A., \n    Stamm, A., Taquet, M., Vantini, S. (2021) \"Multiscale null hypothesis \n    testing for network-valued data: Analysis of brain networks of patients with \n    autism\" <doi:10.1111/rssc.12463>.",
    "version": "0.2.0",
    "maintainer": "Aymeric Stamm <aymeric.stamm@cnrs.fr>",
    "author": "Ilenia Lovato [aut],\n  Alessia Pini [aut],\n  Aymeric Stamm [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8725-3654>),\n  Simone Vantini [aut]",
    "url": "https://astamm.github.io/nevada/,\nhttps://github.com/astamm/nevada/",
    "bug_reports": "https://github.com/astamm/nevada/issues/",
    "repository": "https://cran.r-project.org/package=nevada",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nevada Network-Valued Data Analysis A flexible statistical framework for network-valued data analysis. \n    It leverages the complexity of the space of distributions on graphs by using \n    the permutation framework for inference as implemented in the 'flipr' package. \n    Currently, only the two-sample testing problem is covered and generalization \n    to k samples and regression will be added in the future as well. It is a \n    4-step procedure where the user chooses a suitable representation of the \n    networks, a suitable metric to embed the representation into a metric space, \n    one or more test statistics to target specific aspects of the distributions \n    to be compared and a formula to compute the permutation p-value. Two types \n    of inference are provided: a global test answering whether there is a \n    difference between the distributions that generated the two samples and a \n    local test for localizing differences on the network structure. The latter \n    is assumed to be shared by all networks of both samples. References: Lovato, \n    I., Pini, A., Stamm, A., Vantini, S. (2020) \"Model-free two-sample test for \n    network-valued data\" <doi:10.1016/j.csda.2019.106896>; Lovato, I., Pini, A., \n    Stamm, A., Taquet, M., Vantini, S. (2021) \"Multiscale null hypothesis \n    testing for network-valued data: Analysis of brain networks of patients with \n    autism\" <doi:10.1111/rssc.12463>.  "
  },
  {
    "id": 16873,
    "package_name": "newFocus",
    "title": "True Discovery Guarantee by Combining Partial Closed Testings",
    "description": "Closed testing has been proved powerful for true discovery guarantee. The computation of closed testing is, however, quite burdensome. A general way to reduce computational complexity is to combine partial closed testings for some prespecified feature sets of interest. Partial closed testings are performed at Bonferroni-corrected alpha level to guarantee the lower bounds for the number of true discoveries in prespecified sets are simultaneously valid. For any post hoc chosen sets of interest, coherence property is used to get the lower bound. In this package, we implement closed testing with globaltest to calculate the lower bound for number of true discoveries, see Ningning Xu et.al (2021) <arXiv:2001.01541> for detailed description. ",
    "version": "1.1",
    "maintainer": "Ningning Xu <n.xu@lumc.nl>",
    "author": "Ningning Xu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=newFocus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "newFocus True Discovery Guarantee by Combining Partial Closed Testings Closed testing has been proved powerful for true discovery guarantee. The computation of closed testing is, however, quite burdensome. A general way to reduce computational complexity is to combine partial closed testings for some prespecified feature sets of interest. Partial closed testings are performed at Bonferroni-corrected alpha level to guarantee the lower bounds for the number of true discoveries in prespecified sets are simultaneously valid. For any post hoc chosen sets of interest, coherence property is used to get the lower bound. In this package, we implement closed testing with globaltest to calculate the lower bound for number of true discoveries, see Ningning Xu et.al (2021) <arXiv:2001.01541> for detailed description.   "
  },
  {
    "id": 16903,
    "package_name": "nhstplot",
    "title": "Plot Null Hypothesis Significance Tests",
    "description": "Illustrate graphically the most common Null Hypothesis Significance Testing procedures. More specifically, this package provides functions to plot Chi-Squared, F, t (one- and two-tailed) and z (one- and two-tailed) tests, by plotting the probability density under the null hypothesis as a function of the different test statistic values. Although highly flexible (color theme, fonts, etc.), only the minimal number of arguments (observed test statistic, degrees of freedom) are necessary for a clear and useful graph to be plotted, with the observed test statistic and the p value, as well as their corresponding value labels. The axes are automatically scaled to present the relevant part and the overall shape of the probability density function. This package is especially intended for education purposes, as it provides a helpful support to help explain the Null Hypothesis Significance Testing process, its use and/or shortcomings.",
    "version": "1.4.2",
    "maintainer": "Nils Myszkowski <nilsmyszkowskiscience@gmail.com>",
    "author": "Nils Myszkowski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1322-0777>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nhstplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nhstplot Plot Null Hypothesis Significance Tests Illustrate graphically the most common Null Hypothesis Significance Testing procedures. More specifically, this package provides functions to plot Chi-Squared, F, t (one- and two-tailed) and z (one- and two-tailed) tests, by plotting the probability density under the null hypothesis as a function of the different test statistic values. Although highly flexible (color theme, fonts, etc.), only the minimal number of arguments (observed test statistic, degrees of freedom) are necessary for a clear and useful graph to be plotted, with the observed test statistic and the p value, as well as their corresponding value labels. The axes are automatically scaled to present the relevant part and the overall shape of the probability density function. This package is especially intended for education purposes, as it provides a helpful support to help explain the Null Hypothesis Significance Testing process, its use and/or shortcomings.  "
  },
  {
    "id": 16994,
    "package_name": "nnspat",
    "title": "Nearest Neighbor Methods for Spatial Patterns",
    "description": "Contains the functions for testing the spatial patterns (of segregation, spatial symmetry, \n      association, disease clustering, species correspondence, and reflexivity) based on nearest neighbor relations, \n      especially using contingency tables such as \n      nearest neighbor contingency tables (Ceyhan (2010) <doi:10.1007/s10651-008-0104-x> and \n      Ceyhan (2017) <doi:10.1016/j.jkss.2016.10.002> and references therein),\n      nearest neighbor symmetry contingency tables (Ceyhan (2014) <doi:10.1155/2014/698296>),\n      species correspondence contingency tables and reflexivity contingency tables (Ceyhan (2018) \n      <doi:10.2436/20.8080.02.72> for two (or higher) dimensional data. \n      The package also contains functions for generating patterns of segregation, association,\n      uniformity in a multi-class setting (Ceyhan (2014) <doi:10.1007/s00477-013-0824-9>), \n      and various non-random labeling patterns for disease clustering \n      in two dimensional cases (Ceyhan (2014)\n      <doi:10.1002/sim.6053>), and for visualization of all these patterns \n      for the two dimensional data.\n      The tests are usually (asymptotic) normal z-tests or chi-square tests.",
    "version": "0.1.2",
    "maintainer": "Elvan Ceyhan <elvanceyhan@gmail.com>",
    "author": "Elvan Ceyhan",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nnspat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nnspat Nearest Neighbor Methods for Spatial Patterns Contains the functions for testing the spatial patterns (of segregation, spatial symmetry, \n      association, disease clustering, species correspondence, and reflexivity) based on nearest neighbor relations, \n      especially using contingency tables such as \n      nearest neighbor contingency tables (Ceyhan (2010) <doi:10.1007/s10651-008-0104-x> and \n      Ceyhan (2017) <doi:10.1016/j.jkss.2016.10.002> and references therein),\n      nearest neighbor symmetry contingency tables (Ceyhan (2014) <doi:10.1155/2014/698296>),\n      species correspondence contingency tables and reflexivity contingency tables (Ceyhan (2018) \n      <doi:10.2436/20.8080.02.72> for two (or higher) dimensional data. \n      The package also contains functions for generating patterns of segregation, association,\n      uniformity in a multi-class setting (Ceyhan (2014) <doi:10.1007/s00477-013-0824-9>), \n      and various non-random labeling patterns for disease clustering \n      in two dimensional cases (Ceyhan (2014)\n      <doi:10.1002/sim.6053>), and for visualization of all these patterns \n      for the two dimensional data.\n      The tests are usually (asymptotic) normal z-tests or chi-square tests.  "
  },
  {
    "id": 17008,
    "package_name": "noisySBM",
    "title": "Noisy Stochastic Block Mode: Graph Inference by Multiple Testing",
    "description": "Variational Expectation-Maximization algorithm to fit the noisy stochastic block model to an observed dense graph \n    and to perform a node clustering. Moreover, a graph inference procedure to recover the underlying \n    binary graph. This procedure comes with a control of the false discovery rate. The method is described\n    in the article \"Powerful graph inference with false discovery rate control\" by T. Rebafka, \n    E. Roquain, F. Villers (2020) <arXiv:1907.10176>.",
    "version": "0.1.4",
    "maintainer": "Tabea Rebafka <tabea.rebafka@sorbonne-universite.fr>",
    "author": "Tabea Rebafka [aut, cre],\n  Etienne Roquain [ctb],\n  Fanny Villers [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=noisySBM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "noisySBM Noisy Stochastic Block Mode: Graph Inference by Multiple Testing Variational Expectation-Maximization algorithm to fit the noisy stochastic block model to an observed dense graph \n    and to perform a node clustering. Moreover, a graph inference procedure to recover the underlying \n    binary graph. This procedure comes with a control of the false discovery rate. The method is described\n    in the article \"Powerful graph inference with false discovery rate control\" by T. Rebafka, \n    E. Roquain, F. Villers (2020) <arXiv:1907.10176>.  "
  },
  {
    "id": 17025,
    "package_name": "nonlinearTseries",
    "title": "Nonlinear Time Series Analysis",
    "description": "Functions for nonlinear time series analysis. This package permits\n    the computation of the  most-used nonlinear statistics/algorithms\n    including generalized correlation dimension, information dimension,\n    largest Lyapunov exponent, sample entropy and Recurrence\n    Quantification Analysis (RQA), among others. Basic routines\n    for surrogate data testing are also included. Part of this work\n    was based on the  book \"Nonlinear time series analysis\" by\n    Holger Kantz and Thomas Schreiber (ISBN: 9780521529020).",
    "version": "0.3.1",
    "maintainer": "Constantino A. Garcia <constantino.garciama@ceu.es>",
    "author": "Constantino A. Garcia [aut, cre],\n  Gunther Sawitzki [ctb]",
    "url": "https://github.com/constantino-garcia/nonlinearTseries",
    "bug_reports": "https://github.com/constantino-garcia/nonlinearTseries/issues",
    "repository": "https://cran.r-project.org/package=nonlinearTseries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nonlinearTseries Nonlinear Time Series Analysis Functions for nonlinear time series analysis. This package permits\n    the computation of the  most-used nonlinear statistics/algorithms\n    including generalized correlation dimension, information dimension,\n    largest Lyapunov exponent, sample entropy and Recurrence\n    Quantification Analysis (RQA), among others. Basic routines\n    for surrogate data testing are also included. Part of this work\n    was based on the  book \"Nonlinear time series analysis\" by\n    Holger Kantz and Thomas Schreiber (ISBN: 9780521529020).  "
  },
  {
    "id": 17030,
    "package_name": "nonnest2",
    "title": "Tests of Non-Nested Models",
    "description": "Testing non-nested models via theory supplied by Vuong (1989) <DOI:10.2307/1912557>.\n    Includes tests of model distinguishability and of model fit that can be applied\n    to both nested and non-nested models. Also includes functionality to obtain\n    confidence intervals associated with AIC and BIC. This material is partially based on\n    work supported by the National Science Foundation under Grant Number SES-1061334.",
    "version": "0.5-8",
    "maintainer": "Edgar Merkle <merklee@missouri.edu>",
    "author": "Edgar Merkle [aut, cre],\n  Dongjun You [aut],\n  Lennart Schneider [ctb],\n  Mauricio Garnier-Villarreal [ctb],\n  Seongho Bae [ctb],\n  Phil Chalmers [ctb]",
    "url": "https://github.com/qpsy/nonnest2",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nonnest2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nonnest2 Tests of Non-Nested Models Testing non-nested models via theory supplied by Vuong (1989) <DOI:10.2307/1912557>.\n    Includes tests of model distinguishability and of model fit that can be applied\n    to both nested and non-nested models. Also includes functionality to obtain\n    confidence intervals associated with AIC and BIC. This material is partially based on\n    work supported by the National Science Foundation under Grant Number SES-1061334.  "
  },
  {
    "id": 17051,
    "package_name": "nortest",
    "title": "Tests for Normality",
    "description": "Five omnibus tests for testing the composite hypothesis of\n        normality.",
    "version": "1.0-4",
    "maintainer": "Uwe Ligges <ligges@statistik.tu-dortmund.de>",
    "author": "Juergen Gross [aut],\n  Uwe Ligges [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nortest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nortest Tests for Normality Five omnibus tests for testing the composite hypothesis of\n        normality.  "
  },
  {
    "id": 17076,
    "package_name": "npboottprmFBar",
    "title": "Informative Nonparametric Bootstrap Test with Pooled Resampling",
    "description": "Sample sizes are often small due to hard to reach target populations,\n    rare target events, time constraints, limited budgets, or ethical considerations.\n    Two statistical methods with promising performance in small samples are the\n    nonparametric bootstrap test with pooled resampling method, which is the\n    focus of Dwivedi, Mallawaarachchi, and Alvarado (2017) <doi:10.1002/sim.7263>,\n    and informative hypothesis testing, which is implemented in the 'restriktor'\n    package. The 'npboottprmFBar' package uses the nonparametric bootstrap test\n    with pooled resampling method to implement informative hypothesis testing.\n    The bootFbar() function can be used to analyze data with this method and the\n    persimon() function can be used to conduct performance simulations on type-one\n    error and statistical power.",
    "version": "0.2.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "author": "Mackson Ncube [aut, cre],\n  mightymetrika, LLC [cph, fnd]",
    "url": "https://github.com/mightymetrika/npboottprmFBar",
    "bug_reports": "https://github.com/mightymetrika/npboottprmFBar/issues",
    "repository": "https://cran.r-project.org/package=npboottprmFBar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "npboottprmFBar Informative Nonparametric Bootstrap Test with Pooled Resampling Sample sizes are often small due to hard to reach target populations,\n    rare target events, time constraints, limited budgets, or ethical considerations.\n    Two statistical methods with promising performance in small samples are the\n    nonparametric bootstrap test with pooled resampling method, which is the\n    focus of Dwivedi, Mallawaarachchi, and Alvarado (2017) <doi:10.1002/sim.7263>,\n    and informative hypothesis testing, which is implemented in the 'restriktor'\n    package. The 'npboottprmFBar' package uses the nonparametric bootstrap test\n    with pooled resampling method to implement informative hypothesis testing.\n    The bootFbar() function can be used to analyze data with this method and the\n    persimon() function can be used to conduct performance simulations on type-one\n    error and statistical power.  "
  },
  {
    "id": 17085,
    "package_name": "nph",
    "title": "Planning and Analysing Survival Studies under Non-Proportional\nHazards",
    "description": "Piecewise constant hazard functions are used to flexibly model survival distributions with non-proportional hazards and \n\tto simulate data from the specified distributions. A function to calculate weighted log-rank tests for the comparison of two\n\thazard functions is included. Also, a function to calculate a test using the maximum of a set of test statistics from weighted\n\tlog-rank tests (MaxCombo test) is provided. This test utilizes the asymptotic multivariate normal joint distribution of the\n\tseparate test statistics. The correlation is estimated from the data. These methods are described in Ristl et al. (2021) \n\t<doi:10.1002/pst.2062>.\n\tFinally, a function is provided for the estimation and inferential statistics of various parameters that quantify the difference\n\tbetween two survival curves. Eligible parameters are differences in survival probabilities, log survival probabilities,\n\tcomplementary log log (cloglog) transformed survival probabilities, quantiles of the survival functions, log transformed quantiles,\n\trestricted mean survival times, as well as an average hazard ratio, the Cox model score statistic (logrank statistic), and the\n\tCox-model hazard ratio. Adjustments for multiple testing and simultaneous confidence intervals are calculated using a multivariate\n\tnormal approximation to the set of selected parameters.",
    "version": "2.1",
    "maintainer": "Robin Ristl <robin.ristl@meduniwien.ac.at>",
    "author": "Robin Ristl [aut, cre],\n  Nicolas Ballarini [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nph Planning and Analysing Survival Studies under Non-Proportional\nHazards Piecewise constant hazard functions are used to flexibly model survival distributions with non-proportional hazards and \n\tto simulate data from the specified distributions. A function to calculate weighted log-rank tests for the comparison of two\n\thazard functions is included. Also, a function to calculate a test using the maximum of a set of test statistics from weighted\n\tlog-rank tests (MaxCombo test) is provided. This test utilizes the asymptotic multivariate normal joint distribution of the\n\tseparate test statistics. The correlation is estimated from the data. These methods are described in Ristl et al. (2021) \n\t<doi:10.1002/pst.2062>.\n\tFinally, a function is provided for the estimation and inferential statistics of various parameters that quantify the difference\n\tbetween two survival curves. Eligible parameters are differences in survival probabilities, log survival probabilities,\n\tcomplementary log log (cloglog) transformed survival probabilities, quantiles of the survival functions, log transformed quantiles,\n\trestricted mean survival times, as well as an average hazard ratio, the Cox model score statistic (logrank statistic), and the\n\tCox-model hazard ratio. Adjustments for multiple testing and simultaneous confidence intervals are calculated using a multivariate\n\tnormal approximation to the set of selected parameters.  "
  },
  {
    "id": 17103,
    "package_name": "nproc",
    "title": "Neyman-Pearson (NP) Classification Algorithms and NP Receiver\nOperating Characteristic (NP-ROC) Curves",
    "description": "In many binary classification applications, such as disease\n    diagnosis and spam detection, practitioners commonly face the need to limit\n    type I error (i.e., the conditional probability of misclassifying a class 0\n    observation as class 1) so that it remains below a desired threshold. To address\n    this need, the Neyman-Pearson (NP) classification paradigm is a natural choice;\n    it minimizes type II error (i.e., the conditional probability of misclassifying\n    a class 1 observation as class 0) while enforcing an upper bound, alpha, on the\n    type I error. Although the NP paradigm has a century-long history in hypothesis\n    testing, it has not been well recognized and implemented in classification\n    schemes. Common practices that directly limit the empirical type I error to\n    no more than alpha do not satisfy the type I error control objective because\n    the resulting classifiers are still likely to have type I errors much larger\n    than alpha. As a result, the NP paradigm has not been properly implemented\n    for many classification scenarios in practice. In this work, we develop the\n    first umbrella algorithm that implements the NP paradigm for all scoring-type\n    classification methods, including popular methods such as logistic regression,\n    support vector machines and random forests. Powered by this umbrella algorithm,\n    we propose a novel graphical tool for NP classification methods: NP receiver\n    operating characteristic (NP-ROC) bands, motivated by the popular receiver\n    operating characteristic (ROC) curves. NP-ROC bands will help choose in a data\n    adaptive way and compare different NP classifiers. ",
    "version": "2.1.5",
    "maintainer": "Yang Feng <yangfengstat@gmail.com>",
    "author": "Yang Feng [aut, cre],\n  Jessica Li [aut],\n  Xin Tong [aut],\n  Ye Tian [ctb]",
    "url": "http://advances.sciencemag.org/content/4/2/eaao1659",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=nproc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nproc Neyman-Pearson (NP) Classification Algorithms and NP Receiver\nOperating Characteristic (NP-ROC) Curves In many binary classification applications, such as disease\n    diagnosis and spam detection, practitioners commonly face the need to limit\n    type I error (i.e., the conditional probability of misclassifying a class 0\n    observation as class 1) so that it remains below a desired threshold. To address\n    this need, the Neyman-Pearson (NP) classification paradigm is a natural choice;\n    it minimizes type II error (i.e., the conditional probability of misclassifying\n    a class 1 observation as class 0) while enforcing an upper bound, alpha, on the\n    type I error. Although the NP paradigm has a century-long history in hypothesis\n    testing, it has not been well recognized and implemented in classification\n    schemes. Common practices that directly limit the empirical type I error to\n    no more than alpha do not satisfy the type I error control objective because\n    the resulting classifiers are still likely to have type I errors much larger\n    than alpha. As a result, the NP paradigm has not been properly implemented\n    for many classification scenarios in practice. In this work, we develop the\n    first umbrella algorithm that implements the NP paradigm for all scoring-type\n    classification methods, including popular methods such as logistic regression,\n    support vector machines and random forests. Powered by this umbrella algorithm,\n    we propose a novel graphical tool for NP classification methods: NP receiver\n    operating characteristic (NP-ROC) bands, motivated by the popular receiver\n    operating characteristic (ROC) curves. NP-ROC bands will help choose in a data\n    adaptive way and compare different NP classifiers.   "
  },
  {
    "id": 17106,
    "package_name": "npsm",
    "title": "Nonparametric Statistical Methods",
    "description": "Accompanies the book \"Nonparametric Statistical Methods Using R, 2nd Edition\" by Kloke and McKean (2024, ISBN:9780367651350).  Includes methods, datasets, and random number generation useful for the study of robust and/or nonparametric statistics.  Emphasizes classical nonparametric methods for a variety of designs --- especially one-sample and two-sample problems.  Includes methods for general scores, including estimation and testing for the two-sample location problem as well as Hogg's adaptive method.",
    "version": "2.0.1",
    "maintainer": "John Kloke <johndkloke@gmail.com>",
    "author": "John Kloke [aut, cre],\n  Joseph McKean [aut]",
    "url": "https://github.com/kloke/npsm, https://github.com/kloke/book",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=npsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "npsm Nonparametric Statistical Methods Accompanies the book \"Nonparametric Statistical Methods Using R, 2nd Edition\" by Kloke and McKean (2024, ISBN:9780367651350).  Includes methods, datasets, and random number generation useful for the study of robust and/or nonparametric statistics.  Emphasizes classical nonparametric methods for a variety of designs --- especially one-sample and two-sample problems.  Includes methods for general scores, including estimation and testing for the two-sample location problem as well as Hogg's adaptive method.  "
  },
  {
    "id": 17205,
    "package_name": "odiffr",
    "title": "Fast Pixel-by-Pixel Image Comparison Using 'odiff'",
    "description": "R bindings to 'odiff', a blazing-fast pixel-by-pixel image\n    comparison tool <https://github.com/dmtrKovalenko/odiff>. Supports PNG,\n    JPEG, WEBP, and TIFF with configurable thresholds, antialiasing detection,\n    and region ignoring. Requires system installation of 'odiff'. Ideal for\n    visual regression testing in automated workflows.",
    "version": "0.5.1",
    "maintainer": "Ben Wolstenholme <odiffr@benwolst.dev>",
    "author": "Ben Wolstenholme [aut, cre]",
    "url": "https://github.com/BenWolst/odiffr",
    "bug_reports": "https://github.com/BenWolst/odiffr/issues",
    "repository": "https://cran.r-project.org/package=odiffr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "odiffr Fast Pixel-by-Pixel Image Comparison Using 'odiff' R bindings to 'odiff', a blazing-fast pixel-by-pixel image\n    comparison tool <https://github.com/dmtrKovalenko/odiff>. Supports PNG,\n    JPEG, WEBP, and TIFF with configurable thresholds, antialiasing detection,\n    and region ignoring. Requires system installation of 'odiff'. Ideal for\n    visual regression testing in automated workflows.  "
  },
  {
    "id": 17236,
    "package_name": "omicwas",
    "title": "Cell-Type-Specific Association Testing in Bulk Omics Experiments",
    "description": "In bulk epigenome/transcriptome experiments, molecular expression\n    is measured in a tissue, which is a mixture of multiple types of cells.\n    This package tests association of a disease/phenotype with a molecular marker\n    for each cell type.\n    The proportion of cell types in each sample needs to be given as input.\n    The package is applicable to epigenome-wide association study (EWAS) and\n    differential gene expression analysis.\n    Takeuchi and Kato (submitted)\n    \"omicwas: cell-type-specific epigenome-wide and transcriptome association study\".",
    "version": "0.8.0",
    "maintainer": "Fumihiko Takeuchi <fumihiko@takeuchi.name>",
    "author": "Fumihiko Takeuchi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3185-5661>)",
    "url": "https://github.com/fumi-github/omicwas",
    "bug_reports": "https://github.com/fumi-github/omicwas/issues",
    "repository": "https://cran.r-project.org/package=omicwas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "omicwas Cell-Type-Specific Association Testing in Bulk Omics Experiments In bulk epigenome/transcriptome experiments, molecular expression\n    is measured in a tissue, which is a mixture of multiple types of cells.\n    This package tests association of a disease/phenotype with a molecular marker\n    for each cell type.\n    The proportion of cell types in each sample needs to be given as input.\n    The package is applicable to epigenome-wide association study (EWAS) and\n    differential gene expression analysis.\n    Takeuchi and Kato (submitted)\n    \"omicwas: cell-type-specific epigenome-wide and transcriptome association study\".  "
  },
  {
    "id": 17239,
    "package_name": "omock",
    "title": "Creation of Mock Observational Medical Outcomes Partnership\nCommon Data Model",
    "description": "Creates mock data for testing and package development for the\n    Observational Medical Outcomes Partnership common data model. The\n    package offers functions crafted with pipeline-friendly\n    implementation, enabling users to effortlessly include only the\n    necessary tables for their testing needs.",
    "version": "0.6.0",
    "maintainer": "Mike Du <mike.du@ndorms.ox.ac.uk>",
    "author": "Mike Du [aut, cre] (ORCID: <https://orcid.org/0000-0002-9517-8834>),\n  Mart\u00ed Catal\u00e0 [aut] (ORCID: <https://orcid.org/0000-0003-3308-9905>),\n  Edward Burn [aut] (ORCID: <https://orcid.org/0000-0002-9286-1128>),\n  Nuria Mercade-Besora [aut] (ORCID:\n    <https://orcid.org/0009-0006-7948-3747>),\n  Xihang Chen [aut] (ORCID: <https://orcid.org/0009-0001-8112-8959>)",
    "url": "https://ohdsi.github.io/omock/, https://github.com/ohdsi/omock",
    "bug_reports": "https://github.com/ohdsi/omock/issues",
    "repository": "https://cran.r-project.org/package=omock",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "omock Creation of Mock Observational Medical Outcomes Partnership\nCommon Data Model Creates mock data for testing and package development for the\n    Observational Medical Outcomes Partnership common data model. The\n    package offers functions crafted with pipeline-friendly\n    implementation, enabling users to effortlessly include only the\n    necessary tables for their testing needs.  "
  },
  {
    "id": 17280,
    "package_name": "ooplah",
    "title": "Helper Functions for Class Object-Oriented Programming",
    "description": "Helper functions for coding object-oriented programming with\n    a focus on R6. Includes functions for assertions and testing, looping,\n    and re-usable design patterns including Abstract and Decorator\n    classes.",
    "version": "0.2.0",
    "maintainer": "Raphael Sonabend <raphaelsonabend@gmail.com>",
    "author": "Raphael Sonabend [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9225-4654>)",
    "url": "https://xoopR.github.io/ooplah/, https://github.com/xoopR/ooplah",
    "bug_reports": "https://github.com/xoopR/ooplah/issues",
    "repository": "https://cran.r-project.org/package=ooplah",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ooplah Helper Functions for Class Object-Oriented Programming Helper functions for coding object-oriented programming with\n    a focus on R6. Includes functions for assertions and testing, looping,\n    and re-usable design patterns including Abstract and Decorator\n    classes.  "
  },
  {
    "id": 17282,
    "package_name": "oottest",
    "title": "Out-of-Treatment Testing",
    "description": "Implements the out-of-treatment testing from Kuelpmann and Kuzmics (2020) <doi:10.2139/ssrn.3441675> based on the Vuong Test introduced in Vuong (1989) <doi:10.2307/1912557>. \n    Out-of treatment testing allows for a direct, pairwise likelihood comparison of theories, calibrated with pre-existing data.",
    "version": "0.9.1",
    "maintainer": "Philipp K\u00fclpmann <philipp@kuelpmann.org>",
    "author": "Philipp K\u00fclpmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3757-7172>),\n  Panagiota Charalampidou [aut],\n  University of Vienna [cph]",
    "url": "https://github.com/PhilippKuelpmann/oottest",
    "bug_reports": "https://github.com/PhilippKuelpmann/oottest/issues",
    "repository": "https://cran.r-project.org/package=oottest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oottest Out-of-Treatment Testing Implements the out-of-treatment testing from Kuelpmann and Kuzmics (2020) <doi:10.2139/ssrn.3441675> based on the Vuong Test introduced in Vuong (1989) <doi:10.2307/1912557>. \n    Out-of treatment testing allows for a direct, pairwise likelihood comparison of theories, calibrated with pre-existing data.  "
  },
  {
    "id": 17343,
    "package_name": "optimCheck",
    "title": "Graphical and Numerical Checks for Mode-Finding Routines",
    "description": "Tools for checking that the output of an optimization algorithm is indeed at a local mode of the objective function.  This is accomplished graphically by calculating all one-dimensional \"projection plots\" of the objective function, i.e., varying each input variable one at a time with all other elements of the potential solution being fixed.  The numerical values in these plots can be readily extracted for the purpose of automated and systematic unit-testing of optimization routines.  ",
    "version": "1.0.1",
    "maintainer": "Martin Lysy <mlysy@uwaterloo.ca>",
    "author": "Martin Lysy [aut, cre]",
    "url": "https://github.com/mlysy/optimCheck",
    "bug_reports": "https://github.com/mlysy/optimCheck/issues",
    "repository": "https://cran.r-project.org/package=optimCheck",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "optimCheck Graphical and Numerical Checks for Mode-Finding Routines Tools for checking that the output of an optimization algorithm is indeed at a local mode of the objective function.  This is accomplished graphically by calculating all one-dimensional \"projection plots\" of the objective function, i.e., varying each input variable one at a time with all other elements of the potential solution being fixed.  The numerical values in these plots can be readily extracted for the purpose of automated and systematic unit-testing of optimization routines.    "
  },
  {
    "id": 17388,
    "package_name": "ordinalTables",
    "title": "Fit Models to Two-Way Tables with Correlated Ordered Response\nCategories",
    "description": "Fit a variety of models to two-way tables with ordered categories.\n  Most of the models are appropriate to apply to tables of that have correlated ordered\n  response categories.  There is a particular interest in rater data and models for rescore\n  tables. Some utility functions (e.g., Cohen's kappa and weighted kappa) support\n  more general work on rater agreement.\n  Because the names of the models are very similar, the functions that implement them are\n  organized by last name of the primary author of the article or book that suggested the model,\n  with the name of the function beginning with that author's name and an underscore.  This\n  may make some models more difficult to locate if one doesn't have the original sources.  The\n  vignettes and tests can help to locate models of interest.  For more dertaiils see the following references:\n  Agresti, A. (1983) <doi:10.1016/0167-7152(83)90051-2> \"A Simple Diagonals-Parameter Symmetry And Quasi-Symmetry Model\",\n  Agrestim A. (1983) <doi:10.2307/2531022> \"Testing Marginal Homogeneity for Ordinal Categorical Variables\",\n  Agresti, A. (1988) <doi:10.2307/2531866> \"A Model For Agreement Between Ratings On An Ordinal Scale\",\n  Agresti, A. (1989) <doi:10.1016/0167-7152(89)90104-1> \"An Agreement Model With Kappa As Parameter\",\n  Agresti, A. (2010 ISBN:978-0470082898) \"Analysis Of Ordinal Categorical Data\",\n  Bhapkar, V. P. (1966) <doi:10.1080/01621459.1966.10502021> \"A Note On The Equivalence Of Two Test Criteria For Hypotheses In Categorical Data\",\n  Bhapkar, V. P. (1979) <doi:10.2307/2530344> \"On Tests Of Marginal Symmetry And Quasi-Symmetry In Two And Three-Dimensional Contingency Tables\",\n  Bowker, A. H. (1948) <doi:10.2307/2280710> \"A Test For Symmetry In Contingency Tables\",\n  Clayton, D. G. (1974) <doi:10.2307/2335638> \"Some Odds Ratio Statistics For The Analysis Of Ordered Categorical Data\",\n  Cliff, N. (1993) <doi:10.1037/0033-2909.114.3.494> \"Dominance Statistics: Ordinal Analyses To Answer Ordinal Questions\",\n  Cliff, N. (1996 ISBN:978-0805813333) \"Ordinal Methods For Behavioral Data Analysis\",\n  Goodman, L. A. (1979) <doi:10.1080/01621459.1979.10481650> \"Simple Models For The Analysis Of Association In Cross-Classifications Having Ordered Categories\",\n  Goodman, L. A. (1979) <doi:10.2307/2335159> \"Multiplicative Models For Square Contingency Tables With Ordered Categories\",\n  Ireland, C. T., Ku, H. H., & Kullback, S. (1969) <doi:10.2307/2286071> \"Symmetry And Marginal Homogeneity Of An r \u00d7 r Contingency Table\",\n  Ishi-kuntz, M. (1994 ISBN:978-0803943766) \"Ordinal Log-linear Models\",\n  McCullah, P. (1977) <doi:10.2307/2345320> \"A Logistic Model For Paired Comparisons With Ordered Categorical Data\",\n  McCullagh, P. (1978) <doi:10.2307/2335224> A Class Of Parametric Models For The Analysis Of Square Contingency Tables With Ordered Categories\",\n  McCullagh, P. (1980) <doi:10.1111/j.2517-6161.1980.tb01109.x> \"Regression Models For Ordinal Data\",\n  Penn State: Eberly College of Science (undated) <https://online.stat.psu.edu/stat504/lesson/11> \"Stat 504: Analysis of Discrete Data, 11. Advanced Topics I\",\n  Schuster, C. (2001) <doi:10.3102/10769986026003331> \"Kappa As A Parameter Of A Symmetry Model For Rater Agreement\",\n  Shoukri, M. M. (2004 ISBN:978-1584883210). \"Measures Of Interobserver Agreement\",\n  Stuart, A. (1953) <doi:10.2307/2333101> \"The Estimation Of And Comparison Of Strengths Of Association In Contingency Tables\",\n  Stuart, A. (1955) <doi:10.2307/2333387> \"A Test For Homogeneity Of The Marginal Distributions In A Two-Way Classification\",\n  von Eye, A., & Mun, E. Y. (2005 ISBN:978-0805849677) \"Analyzing Rater Agreement: Manifest Variable Methods\".",
    "version": "1.0.0.3",
    "maintainer": "John R. Donoghue <jdonoghue0823@gmail.com>",
    "author": "John R. Donoghue [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ordinalTables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ordinalTables Fit Models to Two-Way Tables with Correlated Ordered Response\nCategories Fit a variety of models to two-way tables with ordered categories.\n  Most of the models are appropriate to apply to tables of that have correlated ordered\n  response categories.  There is a particular interest in rater data and models for rescore\n  tables. Some utility functions (e.g., Cohen's kappa and weighted kappa) support\n  more general work on rater agreement.\n  Because the names of the models are very similar, the functions that implement them are\n  organized by last name of the primary author of the article or book that suggested the model,\n  with the name of the function beginning with that author's name and an underscore.  This\n  may make some models more difficult to locate if one doesn't have the original sources.  The\n  vignettes and tests can help to locate models of interest.  For more dertaiils see the following references:\n  Agresti, A. (1983) <doi:10.1016/0167-7152(83)90051-2> \"A Simple Diagonals-Parameter Symmetry And Quasi-Symmetry Model\",\n  Agrestim A. (1983) <doi:10.2307/2531022> \"Testing Marginal Homogeneity for Ordinal Categorical Variables\",\n  Agresti, A. (1988) <doi:10.2307/2531866> \"A Model For Agreement Between Ratings On An Ordinal Scale\",\n  Agresti, A. (1989) <doi:10.1016/0167-7152(89)90104-1> \"An Agreement Model With Kappa As Parameter\",\n  Agresti, A. (2010 ISBN:978-0470082898) \"Analysis Of Ordinal Categorical Data\",\n  Bhapkar, V. P. (1966) <doi:10.1080/01621459.1966.10502021> \"A Note On The Equivalence Of Two Test Criteria For Hypotheses In Categorical Data\",\n  Bhapkar, V. P. (1979) <doi:10.2307/2530344> \"On Tests Of Marginal Symmetry And Quasi-Symmetry In Two And Three-Dimensional Contingency Tables\",\n  Bowker, A. H. (1948) <doi:10.2307/2280710> \"A Test For Symmetry In Contingency Tables\",\n  Clayton, D. G. (1974) <doi:10.2307/2335638> \"Some Odds Ratio Statistics For The Analysis Of Ordered Categorical Data\",\n  Cliff, N. (1993) <doi:10.1037/0033-2909.114.3.494> \"Dominance Statistics: Ordinal Analyses To Answer Ordinal Questions\",\n  Cliff, N. (1996 ISBN:978-0805813333) \"Ordinal Methods For Behavioral Data Analysis\",\n  Goodman, L. A. (1979) <doi:10.1080/01621459.1979.10481650> \"Simple Models For The Analysis Of Association In Cross-Classifications Having Ordered Categories\",\n  Goodman, L. A. (1979) <doi:10.2307/2335159> \"Multiplicative Models For Square Contingency Tables With Ordered Categories\",\n  Ireland, C. T., Ku, H. H., & Kullback, S. (1969) <doi:10.2307/2286071> \"Symmetry And Marginal Homogeneity Of An r \u00d7 r Contingency Table\",\n  Ishi-kuntz, M. (1994 ISBN:978-0803943766) \"Ordinal Log-linear Models\",\n  McCullah, P. (1977) <doi:10.2307/2345320> \"A Logistic Model For Paired Comparisons With Ordered Categorical Data\",\n  McCullagh, P. (1978) <doi:10.2307/2335224> A Class Of Parametric Models For The Analysis Of Square Contingency Tables With Ordered Categories\",\n  McCullagh, P. (1980) <doi:10.1111/j.2517-6161.1980.tb01109.x> \"Regression Models For Ordinal Data\",\n  Penn State: Eberly College of Science (undated) <https://online.stat.psu.edu/stat504/lesson/11> \"Stat 504: Analysis of Discrete Data, 11. Advanced Topics I\",\n  Schuster, C. (2001) <doi:10.3102/10769986026003331> \"Kappa As A Parameter Of A Symmetry Model For Rater Agreement\",\n  Shoukri, M. M. (2004 ISBN:978-1584883210). \"Measures Of Interobserver Agreement\",\n  Stuart, A. (1953) <doi:10.2307/2333101> \"The Estimation Of And Comparison Of Strengths Of Association In Contingency Tables\",\n  Stuart, A. (1955) <doi:10.2307/2333387> \"A Test For Homogeneity Of The Marginal Distributions In A Two-Way Classification\",\n  von Eye, A., & Mun, E. Y. (2005 ISBN:978-0805849677) \"Analyzing Rater Agreement: Manifest Variable Methods\".  "
  },
  {
    "id": 17403,
    "package_name": "origin",
    "title": "Explicitly Qualifying Namespaces by Automatically Adding 'pkg::'\nto Functions",
    "description": "Automatically adding 'pkg::' to a function, i.e. mutate()\n    becomes dplyr::mutate(). It is up to the user to determine which\n    packages should be used explicitly, whether to include base R packages\n    or use the functionality on selected text, a file, or a complete\n    directory. User friendly logging is provided in the 'RStudio' Markers\n    pane. Lives in the spirit of 'lintr' and 'styler'. Can also be used\n    for checking which packages are actually used in a project.",
    "version": "1.2.0",
    "maintainer": "Matthias Nistler <m_nistler@web.de>",
    "author": "Matthias Nistler [aut, cre]",
    "url": "https://github.com/mnist91/origin",
    "bug_reports": "https://github.com/mnist91/origin/issues",
    "repository": "https://cran.r-project.org/package=origin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "origin Explicitly Qualifying Namespaces by Automatically Adding 'pkg::'\nto Functions Automatically adding 'pkg::' to a function, i.e. mutate()\n    becomes dplyr::mutate(). It is up to the user to determine which\n    packages should be used explicitly, whether to include base R packages\n    or use the functionality on selected text, a file, or a complete\n    directory. User friendly logging is provided in the 'RStudio' Markers\n    pane. Lives in the spirit of 'lintr' and 'styler'. Can also be used\n    for checking which packages are actually used in a project.  "
  },
  {
    "id": 17472,
    "package_name": "pCalibrate",
    "title": "Bayesian Calibrations of p-Values",
    "description": "Implements transformations of p-values to the smallest possible Bayes factor within the specified class of alternative hypotheses, as described in Held & Ott (2018, <doi:10.1146/annurev-statistics-031017-100307>). Covers several common testing scenarios such as z-tests, t-tests, likelihood ratio tests and the F-test. ",
    "version": "0.2-1",
    "maintainer": "Manuela Ott <manuela.c.ott@gmail.com>",
    "author": "Manuela Ott [aut, cre] (ORCID: <https://orcid.org/0000-0002-8050-7279>),\n  Leonhard Held [aut] (ORCID: <https://orcid.org/0000-0002-8686-5325>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pCalibrate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pCalibrate Bayesian Calibrations of p-Values Implements transformations of p-values to the smallest possible Bayes factor within the specified class of alternative hypotheses, as described in Held & Ott (2018, <doi:10.1146/annurev-statistics-031017-100307>). Covers several common testing scenarios such as z-tests, t-tests, likelihood ratio tests and the F-test.   "
  },
  {
    "id": 17477,
    "package_name": "pKSEA",
    "title": "Prediction-Based Kinase-Substrate Enrichment Analysis",
    "description": "A tool for inferring kinase activity changes from \n    phosphoproteomics data. 'pKSEA' uses kinase-substrate \n    prediction scores to weight observed changes in \n    phosphopeptide abundance to calculate a phosphopeptide-level \n    contribution score, then sums up these contribution scores by \n    kinase to obtain a phosphoproteome-level kinase activity \n    change score (KAC score). 'pKSEA' then assesses the \n    significance of changes in predicted substrate abundances for \n    each kinase using permutation testing. This results in a \n    permutation score (pKSEA significance score) reflecting the \n    likelihood of a similarly high or low KAC from random chance, \n    which can then be interpreted in an analogous manner to an \n    empirically calculated p-value. 'pKSEA' contains default \n    databases of kinase-substrate predictions from 'NetworKIN' \n    (NetworKINPred_db) <http://networkin.info> \n    Horn, et. al (2014) <doi:10.1038/nmeth.2968>\n    and of known kinase-substrate links from 'PhosphoSitePlus' \n    (KSEAdb) <https://www.phosphosite.org/>\n    Hornbeck PV, et. al (2015) <doi:10.1093/nar/gku1267>.",
    "version": "0.0.1",
    "maintainer": "Peter Liao <pll21@case.edu>",
    "author": "Peter Liao [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pKSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pKSEA Prediction-Based Kinase-Substrate Enrichment Analysis A tool for inferring kinase activity changes from \n    phosphoproteomics data. 'pKSEA' uses kinase-substrate \n    prediction scores to weight observed changes in \n    phosphopeptide abundance to calculate a phosphopeptide-level \n    contribution score, then sums up these contribution scores by \n    kinase to obtain a phosphoproteome-level kinase activity \n    change score (KAC score). 'pKSEA' then assesses the \n    significance of changes in predicted substrate abundances for \n    each kinase using permutation testing. This results in a \n    permutation score (pKSEA significance score) reflecting the \n    likelihood of a similarly high or low KAC from random chance, \n    which can then be interpreted in an analogous manner to an \n    empirically calculated p-value. 'pKSEA' contains default \n    databases of kinase-substrate predictions from 'NetworKIN' \n    (NetworKINPred_db) <http://networkin.info> \n    Horn, et. al (2014) <doi:10.1038/nmeth.2968>\n    and of known kinase-substrate links from 'PhosphoSitePlus' \n    (KSEAdb) <https://www.phosphosite.org/>\n    Hornbeck PV, et. al (2015) <doi:10.1093/nar/gku1267>.  "
  },
  {
    "id": 17496,
    "package_name": "packager",
    "title": "Create, Build and Maintain Packages",
    "description": "Helper functions for package creation, building and\n    maintenance. Designed to work with a build system such as 'GNU make' or\n    package 'fakemake' to help you to conditionally work through the stages of\n    package development (such as spell checking, linting, testing, before\n    building and checking a package).",
    "version": "1.15.3",
    "maintainer": "Andreas Dominik Cullmann <fvafrcu@mailbox.org>",
    "author": "Andreas Dominik Cullmann [aut, cre]",
    "url": "https://gitlab.com/fvafrcu/packager",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=packager",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "packager Create, Build and Maintain Packages Helper functions for package creation, building and\n    maintenance. Designed to work with a build system such as 'GNU make' or\n    package 'fakemake' to help you to conditionally work through the stages of\n    package development (such as spell checking, linting, testing, before\n    building and checking a package).  "
  },
  {
    "id": 17501,
    "package_name": "pacotest",
    "title": "Testing for Partial Copulas and the Simplifying Assumption in\nVine Copulas",
    "description": "Routines for two different test types, the Constant Conditional Correlation (CCC) test and the Vectorial Independence (VI) test are provided (Kurz and Spanhel (2022) <doi:10.1214/22-EJS2051>). The tests can be applied to check whether a conditional copula coincides with its partial copula. Functions to test whether a regular vine copula satisfies the so-called simplifying assumption or to test a single copula within a regular vine copula to be a (j-1)-th order partial copula are available. The CCC test comes with a decision tree approach to allow testing in high-dimensional settings.",
    "version": "0.4.3",
    "maintainer": "Malte S. Kurz <mkurz-software@gmx.de>",
    "author": "Malte S. Kurz [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/MalteKurz/pacotest/issues",
    "repository": "https://cran.r-project.org/package=pacotest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pacotest Testing for Partial Copulas and the Simplifying Assumption in\nVine Copulas Routines for two different test types, the Constant Conditional Correlation (CCC) test and the Vectorial Independence (VI) test are provided (Kurz and Spanhel (2022) <doi:10.1214/22-EJS2051>). The tests can be applied to check whether a conditional copula coincides with its partial copula. Functions to test whether a regular vine copula satisfies the so-called simplifying assumption or to test a single copula within a regular vine copula to be a (j-1)-th order partial copula are available. The CCC test comes with a decision tree approach to allow testing in high-dimensional settings.  "
  },
  {
    "id": 17619,
    "package_name": "pass.lme",
    "title": "Power and Sample Size for Linear Mixed Effect Models",
    "description": "Power and sample size calculation for testing fixed effect coefficients in multilevel linear mixed effect models with one or more than one independent populations. Laird, Nan M. and Ware, James H. (1982) <doi:10.2307/2529876>.",
    "version": "0.9.0",
    "maintainer": "Marco Chak Yan YU <marcocyyu@gmail.com>",
    "author": "Marco Chak Yan YU",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pass.lme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pass.lme Power and Sample Size for Linear Mixed Effect Models Power and sample size calculation for testing fixed effect coefficients in multilevel linear mixed effect models with one or more than one independent populations. Laird, Nan M. and Ware, James H. (1982) <doi:10.2307/2529876>.  "
  },
  {
    "id": 17641,
    "package_name": "patrick",
    "title": "Parameterized Unit Testing",
    "description": "This is an extension of the 'testthat' package that\n    lets you add parameters to your unit tests. Parameterized unit tests\n    are often easier to read and more reliable, since they follow the DNRY\n    (do not repeat yourself) rule.",
    "version": "0.3.1",
    "maintainer": "Michael Chirico <chiricom@google.com>",
    "author": "Michael Quinn [aut],\n  Michael Chirico [aut, cre]",
    "url": "https://github.com/google/patrick",
    "bug_reports": "https://github.com/google/patrick/issues",
    "repository": "https://cran.r-project.org/package=patrick",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "patrick Parameterized Unit Testing This is an extension of the 'testthat' package that\n    lets you add parameters to your unit tests. Parameterized unit tests\n    are often easier to read and more reliable, since they follow the DNRY\n    (do not repeat yourself) rule.  "
  },
  {
    "id": 17688,
    "package_name": "pcal",
    "title": "Calibration of P-Values for Point Null Hypothesis Testing",
    "description": "Calibrate p-values under a robust perspective using the methods developed\n    by Sellke, Bayarri, and Berger (2001) <doi:10.1198/000313001300339950> and obtain \n    measures of the evidence provided by the data in favor of point null hypotheses \n    which are safer and more straightforward to interpret.",
    "version": "1.0.0",
    "maintainer": "Pedro Fonseca <pedro.teles.fonseca@outlook.com>",
    "author": "Pedro Fonseca [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5708-081X>),\n  Rui Paulo [ctb, ths] (ORCID: <https://orcid.org/0000-0002-4802-527X>),\n  FCT [fnd]",
    "url": "https://pedro-teles-fonseca.github.io/pcal/,\nhttps://github.com/pedro-teles-fonseca/pcal",
    "bug_reports": "https://github.com/pedro-teles-fonseca/pcal/issues",
    "repository": "https://cran.r-project.org/package=pcal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcal Calibration of P-Values for Point Null Hypothesis Testing Calibrate p-values under a robust perspective using the methods developed\n    by Sellke, Bayarri, and Berger (2001) <doi:10.1198/000313001300339950> and obtain \n    measures of the evidence provided by the data in favor of point null hypotheses \n    which are safer and more straightforward to interpret.  "
  },
  {
    "id": 17694,
    "package_name": "pcds",
    "title": "Proximity Catch Digraphs and Their Applications",
    "description": "Contains the functions for construction and visualization of various families \n    of the proximity catch digraphs (PCDs), see (Ceyhan (2005) ISBN:978-3-639-19063-2),\n    for computing the graph invariants for testing the patterns of segregation and association against complete spatial randomness (CSR)\n    or uniformity in one, two and three dimensional cases.\n    The package also has tools for generating points from these spatial patterns.\n    The graph invariants used in testing spatial point data are the domination number (Ceyhan (2011)\n    <doi:10.1080/03610921003597211>) and arc density (Ceyhan et al. (2006) <doi:10.1016/j.csda.2005.03.002>;\n    Ceyhan et al. (2007) <doi:10.1002/cjs.5550350106>). The PCD families considered are Arc-Slice PCDs,\n    Proportional-Edge PCDs, and Central Similarity PCDs. ",
    "version": "0.1.8",
    "maintainer": "Elvan Ceyhan <elvanceyhan@gmail.com>",
    "author": "Elvan Ceyhan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pcds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcds Proximity Catch Digraphs and Their Applications Contains the functions for construction and visualization of various families \n    of the proximity catch digraphs (PCDs), see (Ceyhan (2005) ISBN:978-3-639-19063-2),\n    for computing the graph invariants for testing the patterns of segregation and association against complete spatial randomness (CSR)\n    or uniformity in one, two and three dimensional cases.\n    The package also has tools for generating points from these spatial patterns.\n    The graph invariants used in testing spatial point data are the domination number (Ceyhan (2011)\n    <doi:10.1080/03610921003597211>) and arc density (Ceyhan et al. (2006) <doi:10.1016/j.csda.2005.03.002>;\n    Ceyhan et al. (2007) <doi:10.1002/cjs.5550350106>). The PCD families considered are Arc-Slice PCDs,\n    Proportional-Edge PCDs, and Central Similarity PCDs.   "
  },
  {
    "id": 17695,
    "package_name": "pcds.ugraph",
    "title": "Underlying Graphs of Proximity Catch Digraphs and Their\nApplications",
    "description": "Contains the functions for construction and visualization of underlying and reflexivity graphs of \n            the three families of the proximity catch digraphs (PCDs), see (Ceyhan (2005) ISBN:978-3-639-19063-2),\n            and for computing the edge density of these PCD-based graphs which are then\n            used for testing the patterns of segregation and association against complete spatial randomness (CSR))\n            or uniformity in one and two dimensional cases. \n            The PCD families considered are Arc-Slice PCDs, Proportional-Edge (PE) PCDs (Ceyhan et al. (2006) <doi:10.1016/j.csda.2005.03.002>) \n            and Central Similarity PCDs (Ceyhan et al. (2007) <doi:10.1002/cjs.5550350106>). \n            See also (Ceyhan (2016) <doi:10.1016/j.stamet.2016.07.003>) for edge density of the underlying and \n            reflexivity graphs of PE-PCDs.\n            The package also has tools for visualization of PCD-based graphs for one, two, and three dimensional data. ",
    "version": "0.1.1",
    "maintainer": "Elvan Ceyhan <elvanceyhan@gmail.com>",
    "author": "Elvan Ceyhan [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pcds.ugraph",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcds.ugraph Underlying Graphs of Proximity Catch Digraphs and Their\nApplications Contains the functions for construction and visualization of underlying and reflexivity graphs of \n            the three families of the proximity catch digraphs (PCDs), see (Ceyhan (2005) ISBN:978-3-639-19063-2),\n            and for computing the edge density of these PCD-based graphs which are then\n            used for testing the patterns of segregation and association against complete spatial randomness (CSR))\n            or uniformity in one and two dimensional cases. \n            The PCD families considered are Arc-Slice PCDs, Proportional-Edge (PE) PCDs (Ceyhan et al. (2006) <doi:10.1016/j.csda.2005.03.002>) \n            and Central Similarity PCDs (Ceyhan et al. (2007) <doi:10.1002/cjs.5550350106>). \n            See also (Ceyhan (2016) <doi:10.1016/j.stamet.2016.07.003>) for edge density of the underlying and \n            reflexivity graphs of PE-PCDs.\n            The package also has tools for visualization of PCD-based graphs for one, two, and three dimensional data.   "
  },
  {
    "id": 17711,
    "package_name": "pcutils",
    "title": "Some Useful Functions for Statistics and Visualization",
    "description": "Offers a range of utilities and functions for everyday programming tasks. 1.Data\n           Manipulation. Such as grouping and merging, column splitting, and character expansion. 2.File\n           Handling. Read and convert files in popular formats. 3.Plotting Assistance. Helpful utilities\n           for generating color palettes, validating color formats, and adding transparency.\n           4.Statistical Analysis. Includes functions for pairwise comparisons and multiple testing\n           corrections, enabling perform statistical analyses with ease. 5.Graph Plotting, Provides\n           efficient tools for creating doughnut plot and multi-layered doughnut plot; Venn diagrams,\n           including traditional Venn diagrams, upset plots, and flower plots; Simplified functions for\n           creating stacked bar plots, or a box plot with alphabets group for multiple comparison group.",
    "version": "0.2.8",
    "maintainer": "Chen Peng <pengchen2001@zju.edu.cn>",
    "author": "Chen Peng [aut, cre] (ORCID: <https://orcid.org/0000-0002-9449-7606>)",
    "url": "https://github.com/Asa12138/pcutils",
    "bug_reports": "https://github.com/Asa12138/pcutils/issues",
    "repository": "https://cran.r-project.org/package=pcutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pcutils Some Useful Functions for Statistics and Visualization Offers a range of utilities and functions for everyday programming tasks. 1.Data\n           Manipulation. Such as grouping and merging, column splitting, and character expansion. 2.File\n           Handling. Read and convert files in popular formats. 3.Plotting Assistance. Helpful utilities\n           for generating color palettes, validating color formats, and adding transparency.\n           4.Statistical Analysis. Includes functions for pairwise comparisons and multiple testing\n           corrections, enabling perform statistical analyses with ease. 5.Graph Plotting, Provides\n           efficient tools for creating doughnut plot and multi-layered doughnut plot; Venn diagrams,\n           including traditional Venn diagrams, upset plots, and flower plots; Simplified functions for\n           creating stacked bar plots, or a box plot with alphabets group for multiple comparison group.  "
  },
  {
    "id": 17718,
    "package_name": "pdcor",
    "title": "Fast and Light-Weight Partial Distance Correlation",
    "description": "Fast and memory-less computation of the partial distance correlation for vectors and matrices. Permutation-based and asymptotic hypothesis testing for zero partial distance correlation are also performed. References include: Szekely G. J. and Rizzo M. L. (2014). \"Partial distance correlation with methods for dissimilarities\". The Annals Statistics, 42(6): 2382--2412. <doi:10.1214/14-AOS1255>. Shen C., Panda S. and Vogelstein J. T. (2022). \"The Chi-Square Test of Distance Correlation\". Journal of Computational and Graphical Statistics, 31(1): 254--262. <doi:10.1080/10618600.2021.1938585>. Szekely G. J. and Rizzo M. L. (2023). \"The Energy of Data and Distance Correlation\". Chapman and Hall/CRC. <ISBN:9781482242744>. Kontemeniotis N., Vargiakakis R. and Tsagris M. (2025). On independence testing using the (partial) distance correlation. <doi:10.48550/arXiv.2506.15659>.  ",
    "version": "1.2",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre],\n  Nikolaos Kontemeniotis [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pdcor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdcor Fast and Light-Weight Partial Distance Correlation Fast and memory-less computation of the partial distance correlation for vectors and matrices. Permutation-based and asymptotic hypothesis testing for zero partial distance correlation are also performed. References include: Szekely G. J. and Rizzo M. L. (2014). \"Partial distance correlation with methods for dissimilarities\". The Annals Statistics, 42(6): 2382--2412. <doi:10.1214/14-AOS1255>. Shen C., Panda S. and Vogelstein J. T. (2022). \"The Chi-Square Test of Distance Correlation\". Journal of Computational and Graphical Statistics, 31(1): 254--262. <doi:10.1080/10618600.2021.1938585>. Szekely G. J. and Rizzo M. L. (2023). \"The Energy of Data and Distance Correlation\". Chapman and Hall/CRC. <ISBN:9781482242744>. Kontemeniotis N., Vargiakakis R. and Tsagris M. (2025). On independence testing using the (partial) distance correlation. <doi:10.48550/arXiv.2506.15659>.    "
  },
  {
    "id": 17730,
    "package_name": "pdt",
    "title": "Permutation Distancing Test",
    "description": "Permutation (randomisation) test for single-case phase design data with \n    two phases (e.g., pre- and post-treatment). Correction for dependency of observations \n    is done through stepwise resampling the time series while varying \n    the distance between observations. The required distance 0,1,2,3.. is determined \n    based on repeated dependency testing while stepwise increasing the distance.\n    In preparation: Vroegindeweij et al. \"A Permutation distancing test \n    for single-case observational AB phase design data: A Monte Carlo simulation study\".",
    "version": "0.0.2",
    "maintainer": "Jan Houtveen <janhoutveen@gmail.com>",
    "author": "Jan Houtveen [aut, cre],\n  Anouk Vroegindeweij [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pdt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdt Permutation Distancing Test Permutation (randomisation) test for single-case phase design data with \n    two phases (e.g., pre- and post-treatment). Correction for dependency of observations \n    is done through stepwise resampling the time series while varying \n    the distance between observations. The required distance 0,1,2,3.. is determined \n    based on repeated dependency testing while stepwise increasing the distance.\n    In preparation: Vroegindeweij et al. \"A Permutation distancing test \n    for single-case observational AB phase design data: A Monte Carlo simulation study\".  "
  },
  {
    "id": 17731,
    "package_name": "pdynmc",
    "title": "Moment Condition Based Estimation of Linear Dynamic Panel Data\nModels",
    "description": "Linear dynamic panel data modeling based on linear and\n    nonlinear moment conditions as proposed by\n    Holtz-Eakin, Newey, and Rosen (1988) <doi:10.2307/1913103>,\n    Ahn and Schmidt (1995) <doi:10.1016/0304-4076(94)01641-C>,\n    and Arellano and Bover (1995) <doi:10.1016/0304-4076(94)01642-D>.\n    Estimation of the model parameters relies on the Generalized\n    Method of Moments (GMM) and instrumental variables (IV) estimation,\n    numerical optimization (when nonlinear moment conditions are\n    employed) and the computation of closed form solutions (when\n    estimation is based on linear moment conditions). One-step,\n    two-step and iterated estimation is available. For inference\n    and specification\n    testing, Windmeijer (2005) <doi:10.1016/j.jeconom.2004.02.005>\n    and doubly corrected standard errors\n    (Hwang, Kang, Lee, 2021 <doi:10.1016/j.jeconom.2020.09.010>)\n    are available. Additionally, serial correlation tests, tests for\n    overidentification, and Wald tests are provided. Functions for\n    visualizing panel data structures and modeling results obtained\n    from GMM estimation are also available. The plot methods include\n    functions to plot unbalanced panel structure, coefficient ranges\n    and coefficient paths across GMM iterations (the latter is\n    implemented according to the plot shown in\n    Hansen and Lee, 2021 <doi:10.3982/ECTA16274>).\n    For a more detailed description of the GMM-based functionality,\n    please see Fritsch, Pua, Schnurbus (2021) <doi:10.32614/RJ-2021-035>.\n    For more details on the IV-based estimation routines,\n    see Fritsch, Pua, and Schnurbus (WP, 2024) and\n    Han and Phillips (2010) <doi:10.1017/S026646660909063X>.",
    "version": "0.9.12",
    "maintainer": "Markus Fritsch <Markus.Fritsch@uni-Passau.de>",
    "author": "Markus Fritsch [aut, cre],\n  Joachim Schnurbus [aut],\n  Andrew Adrian Yu Pua [aut]",
    "url": "https://github.com/markusfritsch/pdynmc",
    "bug_reports": "https://github.com/markusfritsch/pdynmc/issues",
    "repository": "https://cran.r-project.org/package=pdynmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pdynmc Moment Condition Based Estimation of Linear Dynamic Panel Data\nModels Linear dynamic panel data modeling based on linear and\n    nonlinear moment conditions as proposed by\n    Holtz-Eakin, Newey, and Rosen (1988) <doi:10.2307/1913103>,\n    Ahn and Schmidt (1995) <doi:10.1016/0304-4076(94)01641-C>,\n    and Arellano and Bover (1995) <doi:10.1016/0304-4076(94)01642-D>.\n    Estimation of the model parameters relies on the Generalized\n    Method of Moments (GMM) and instrumental variables (IV) estimation,\n    numerical optimization (when nonlinear moment conditions are\n    employed) and the computation of closed form solutions (when\n    estimation is based on linear moment conditions). One-step,\n    two-step and iterated estimation is available. For inference\n    and specification\n    testing, Windmeijer (2005) <doi:10.1016/j.jeconom.2004.02.005>\n    and doubly corrected standard errors\n    (Hwang, Kang, Lee, 2021 <doi:10.1016/j.jeconom.2020.09.010>)\n    are available. Additionally, serial correlation tests, tests for\n    overidentification, and Wald tests are provided. Functions for\n    visualizing panel data structures and modeling results obtained\n    from GMM estimation are also available. The plot methods include\n    functions to plot unbalanced panel structure, coefficient ranges\n    and coefficient paths across GMM iterations (the latter is\n    implemented according to the plot shown in\n    Hansen and Lee, 2021 <doi:10.3982/ECTA16274>).\n    For a more detailed description of the GMM-based functionality,\n    please see Fritsch, Pua, Schnurbus (2021) <doi:10.32614/RJ-2021-035>.\n    For more details on the IV-based estimation routines,\n    see Fritsch, Pua, and Schnurbus (WP, 2024) and\n    Han and Phillips (2010) <doi:10.1017/S026646660909063X>.  "
  },
  {
    "id": 17788,
    "package_name": "peramo",
    "title": "Permutation Tests for Randomization Model",
    "description": "Perform permutation-based hypothesis testing for randomized\n    experiments as suggested in Ludbrook & Dudley (1998) <doi:10.2307/2685470>\n    and Ernst (2004) <doi:10.1214/088342304000000396>, introduced in Pham et al.\n    (2022) <doi:10.1016/j.chemosphere.2022.136736>.",
    "version": "0.1.5",
    "maintainer": "Duy Nghia Pham <nghiapham@yandex.com>",
    "author": "Duy Nghia Pham [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1349-1710>),\n  Inna M. Sokolova [ths] (ORCID: <https://orcid.org/0000-0002-2068-4302>)",
    "url": "",
    "bug_reports": "https://github.com/phamdn/peramo/issues",
    "repository": "https://cran.r-project.org/package=peramo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "peramo Permutation Tests for Randomization Model Perform permutation-based hypothesis testing for randomized\n    experiments as suggested in Ludbrook & Dudley (1998) <doi:10.2307/2685470>\n    and Ernst (2004) <doi:10.1214/088342304000000396>, introduced in Pham et al.\n    (2022) <doi:10.1016/j.chemosphere.2022.136736>.  "
  },
  {
    "id": 17798,
    "package_name": "permubiome",
    "title": "A Permutation Based Test for Biomarker Discovery in Microbiome\nData",
    "description": "The permubiome R package was created to perform a permutation-based non-parametric analysis on microbiome data for biomarker discovery aims. This test executes thousands of comparisons in a pairwise manner, after a random shuffling of data into the different groups of study with a prior selection of the microbiome features with the largest variation among groups. Previous to the permutation test itself, data can be normalized according to different methods proposed to handle microbiome data ('proportions' or 'Anders'). The median-based differences between groups resulting from the multiple simulations are fitted to a normal distribution with the aim to calculate their significance. A multiple testing correction based on Benjamini-Hochberg method (fdr) is finally applied to extract the differentially presented features between groups of your dataset. LATEST UPDATES: v1.1 and olders incorporates function to parse COLUMN format; v1.2 and olders incorporates -optimize- function to maximize evaluation of features with largest inter-class variation; v1.3 and olders includes the -size.effect- function to perform estimation statistics using the bootstrap-coupled approach implemented in the 'dabestr' (>=0.3.0) R package. Current v1.3.2 fixed bug with \"Class\" recognition and updated 'dabestr' functions.",
    "version": "1.3.2",
    "maintainer": "Alfonso Benitez-Paez <alfbenpa@gmail.com>",
    "author": "Alfonso Benitez-Paez",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=permubiome",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "permubiome A Permutation Based Test for Biomarker Discovery in Microbiome\nData The permubiome R package was created to perform a permutation-based non-parametric analysis on microbiome data for biomarker discovery aims. This test executes thousands of comparisons in a pairwise manner, after a random shuffling of data into the different groups of study with a prior selection of the microbiome features with the largest variation among groups. Previous to the permutation test itself, data can be normalized according to different methods proposed to handle microbiome data ('proportions' or 'Anders'). The median-based differences between groups resulting from the multiple simulations are fitted to a normal distribution with the aim to calculate their significance. A multiple testing correction based on Benjamini-Hochberg method (fdr) is finally applied to extract the differentially presented features between groups of your dataset. LATEST UPDATES: v1.1 and olders incorporates function to parse COLUMN format; v1.2 and olders incorporates -optimize- function to maximize evaluation of features with largest inter-class variation; v1.3 and olders includes the -size.effect- function to perform estimation statistics using the bootstrap-coupled approach implemented in the 'dabestr' (>=0.3.0) R package. Current v1.3.2 fixed bug with \"Class\" recognition and updated 'dabestr' functions.  "
  },
  {
    "id": 17799,
    "package_name": "permuco",
    "title": "Permutation Tests for Regression, (Repeated Measures)\nANOVA/ANCOVA and Comparison of Signals",
    "description": "Functions to compute p-values based on permutation tests. Regression, ANOVA and ANCOVA, omnibus F-tests, marginal unilateral and bilateral t-tests are available. Several methods to handle nuisance variables are implemented (Kherad-Pajouh, S., & Renaud, O. (2010) <doi:10.1016/j.csda.2010.02.015> ; Kherad-Pajouh, S., & Renaud, O. (2014) <doi:10.1007/s00362-014-0617-3> ; Winkler, A. M., Ridgway, G. R., Webster, M. A., Smith, S. M., & Nichols, T. E. (2014) <doi:10.1016/j.neuroimage.2014.01.060>). An extension for the comparison of signals issued from experimental conditions (e.g. EEG/ERP signals) is provided. Several corrections for multiple testing are possible, including the cluster-mass statistic (Maris, E., & Oostenveld, R. (2007) <doi:10.1016/j.jneumeth.2007.03.024>) and the threshold-free cluster enhancement (Smith, S. M., & Nichols, T. E. (2009) <doi:10.1016/j.neuroimage.2008.03.061>). ",
    "version": "1.1.3",
    "maintainer": "Jaromil Frossard <jaromil.frossard@gmail.com>",
    "author": "Jaromil Frossard [aut, cre],\n  Olivier Renaud [aut]",
    "url": "https://github.com/jaromilfrossard/permuco",
    "bug_reports": "https://github.com/jaromilfrossard/permuco/issues",
    "repository": "https://cran.r-project.org/package=permuco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "permuco Permutation Tests for Regression, (Repeated Measures)\nANOVA/ANCOVA and Comparison of Signals Functions to compute p-values based on permutation tests. Regression, ANOVA and ANCOVA, omnibus F-tests, marginal unilateral and bilateral t-tests are available. Several methods to handle nuisance variables are implemented (Kherad-Pajouh, S., & Renaud, O. (2010) <doi:10.1016/j.csda.2010.02.015> ; Kherad-Pajouh, S., & Renaud, O. (2014) <doi:10.1007/s00362-014-0617-3> ; Winkler, A. M., Ridgway, G. R., Webster, M. A., Smith, S. M., & Nichols, T. E. (2014) <doi:10.1016/j.neuroimage.2014.01.060>). An extension for the comparison of signals issued from experimental conditions (e.g. EEG/ERP signals) is provided. Several corrections for multiple testing are possible, including the cluster-mass statistic (Maris, E., & Oostenveld, R. (2007) <doi:10.1016/j.jneumeth.2007.03.024>) and the threshold-free cluster enhancement (Smith, S. M., & Nichols, T. E. (2009) <doi:10.1016/j.neuroimage.2008.03.061>).   "
  },
  {
    "id": 17802,
    "package_name": "permutes",
    "title": "Permutation Tests for Time Series Data",
    "description": "Helps you determine the analysis window to use when analyzing densely-sampled\n    time-series data, such as EEG data, using permutation testing (Maris & Oostenveld, 2007)\n    <doi:10.1016/j.jneumeth.2007.03.024>. These permutation tests can help identify the timepoints\n    where significance of an effect begins and ends, and the results can be plotted in various\n    types of heatmap for reporting. Mixed-effects models are supported using an implementation of\n    the approach by Lee & Braun (2012) <doi:10.1111/j.1541-0420.2011.01675.x>.",
    "version": "2.8",
    "maintainer": "Cesko C. Voeten <cvoeten@gmail.com>",
    "author": "Cesko C. Voeten [aut, cre]",
    "url": "",
    "bug_reports": "https://gitlab.com/cvoeten/permutes/-/issues",
    "repository": "https://cran.r-project.org/package=permutes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "permutes Permutation Tests for Time Series Data Helps you determine the analysis window to use when analyzing densely-sampled\n    time-series data, such as EEG data, using permutation testing (Maris & Oostenveld, 2007)\n    <doi:10.1016/j.jneumeth.2007.03.024>. These permutation tests can help identify the timepoints\n    where significance of an effect begins and ends, and the results can be plotted in various\n    types of heatmap for reporting. Mixed-effects models are supported using an implementation of\n    the approach by Lee & Braun (2012) <doi:10.1111/j.1541-0420.2011.01675.x>.  "
  },
  {
    "id": 17841,
    "package_name": "pgsc",
    "title": "Computes Powell's Generalized Synthetic Control Estimator",
    "description": "Computes the generalized synthetic control estimator described in\n    Powell (2017) <doi:10.7249/WR1142>.  Provides both point estimates, and hypothesis testing.",
    "version": "1.0.0",
    "maintainer": "Philip Barrett <pobarrett@gmail.com>",
    "author": "Philip Barrett",
    "url": "https://github.com/philipbarrett/pgsc",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pgsc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pgsc Computes Powell's Generalized Synthetic Control Estimator Computes the generalized synthetic control estimator described in\n    Powell (2017) <doi:10.7249/WR1142>.  Provides both point estimates, and hypothesis testing.  "
  },
  {
    "id": 17853,
    "package_name": "pharmaversesdtm",
    "title": "SDTM Test Data for the 'Pharmaverse' Family of Packages",
    "description": "A set of Study Data Tabulation Model (SDTM) datasets from the\n    Clinical Data Interchange Standards Consortium (CDISC) pilot project\n    used for testing and developing Analysis Data Model (ADaM) datasets\n    inside the pharmaverse family of packages. SDTM dataset specifications\n    are described in the CDISC SDTM implementation guide, accessible by\n    creating a free account on <https://www.cdisc.org/>.",
    "version": "1.3.1",
    "maintainer": "Lina Patil <lina.patil@cytel.com>",
    "author": "Lina Patil [aut, cre],\n  Stefan Bundfuss [aut] (ORCID: <https://orcid.org/0009-0005-0027-1198>),\n  Fanny Gautier [aut] (ORCID: <https://orcid.org/0009-0004-3581-0131>),\n  Edoardo Mancini [aut] (ORCID: <https://orcid.org/0009-0006-4899-8641>),\n  Tomoyuki Namai [aut],\n  Vinh Nguyen [aut],\n  Vladyslav Shuliar [aut] (ORCID:\n    <https://orcid.org/0009-0008-2354-8999>),\n  Cytel Inc. [cph, fnd],\n  F. Hoffmann-La Roche AG [cph, fnd],\n  GlaxoSmithKline LLC [cph, fnd]",
    "url": "https://pharmaverse.github.io/pharmaversesdtm/,\nhttps://github.com/pharmaverse/pharmaversesdtm/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pharmaversesdtm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pharmaversesdtm SDTM Test Data for the 'Pharmaverse' Family of Packages A set of Study Data Tabulation Model (SDTM) datasets from the\n    Clinical Data Interchange Standards Consortium (CDISC) pilot project\n    used for testing and developing Analysis Data Model (ADaM) datasets\n    inside the pharmaverse family of packages. SDTM dataset specifications\n    are described in the CDISC SDTM implementation guide, accessible by\n    creating a free account on <https://www.cdisc.org/>.  "
  },
  {
    "id": 17861,
    "package_name": "phd",
    "title": "Permutation Testing in High-Dimensional Linear Models",
    "description": "Provides permutation methods for testing in high-dimensional linear models.\n    The tests are often robust against heteroscedasticity and non-normality \n    and usually perform well under anti-sparsity.\n    See Hemerik, Thoresen and Finos (2021) <doi:10.1080/00949655.2020.1836183>.",
    "version": "0.2",
    "maintainer": "Jesse Hemerik <jesse.hemerik@wur.nl>",
    "author": "Jesse Hemerik, Livio Finos",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phd Permutation Testing in High-Dimensional Linear Models Provides permutation methods for testing in high-dimensional linear models.\n    The tests are often robust against heteroscedasticity and non-normality \n    and usually perform well under anti-sparsity.\n    See Hemerik, Thoresen and Finos (2021) <doi:10.1080/00949655.2020.1836183>.  "
  },
  {
    "id": 17866,
    "package_name": "phenesse",
    "title": "Estimate Phenological Metrics using Presence-Only Data",
    "description": "Generates Weibull-parameterized estimates of phenology for any percentile of \n    a distribution using the framework established in Cooke (1979)\n    <doi:10.1093/biomet/66.2.367>. Extensive testing against other \n    estimators suggest the weib_percentile() function is especially useful in \n    generating more accurate and less biased estimates of onset and offset \n    (Belitz et al. 2020) <doi:10.1111/2041-210X.13448>. Non-parametric \n    bootstrapping can be used to generate confidence intervals around those \n    estimates, although this is computationally expensive. Additionally, this \n    package offers an easy way to perform non-parametric bootstrapping to \n    generate confidence intervals for quantile estimates, mean estimates, \n    or any statistical function of interest.",
    "version": "0.1.3",
    "maintainer": "Michael Belitz <michaelbelitz06@gmail.com>",
    "author": "Michael Belitz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8162-5998>),\n  Caitlin Campbell [ctb] (ORCID: <https://orcid.org/0000-0002-8199-7775>),\n  Daijiang Li [ctb] (ORCID: <https://orcid.org/0000-0002-0925-3421>)",
    "url": "https://github.com/mbelitz/phenesse",
    "bug_reports": "https://github.com/mbelitz/phenesse/issues",
    "repository": "https://cran.r-project.org/package=phenesse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phenesse Estimate Phenological Metrics using Presence-Only Data Generates Weibull-parameterized estimates of phenology for any percentile of \n    a distribution using the framework established in Cooke (1979)\n    <doi:10.1093/biomet/66.2.367>. Extensive testing against other \n    estimators suggest the weib_percentile() function is especially useful in \n    generating more accurate and less biased estimates of onset and offset \n    (Belitz et al. 2020) <doi:10.1111/2041-210X.13448>. Non-parametric \n    bootstrapping can be used to generate confidence intervals around those \n    estimates, although this is computationally expensive. Additionally, this \n    package offers an easy way to perform non-parametric bootstrapping to \n    generate confidence intervals for quantile estimates, mean estimates, \n    or any statistical function of interest.  "
  },
  {
    "id": 17916,
    "package_name": "phylopairs",
    "title": "Comparative Analyses of Lineage-Pair Traits",
    "description": "Facilitates the testing of causal relationships among lineage-pair traits in a phylogenetically informed context. Lineage-pair traits are characters that are defined for pairs of lineages instead of individual taxa. Examples include the strength of reproductive isolation, range overlap, competition coefficient, diet niche similarity, and relative hybrid fitness. Users supply a lineage-pair dataset and a phylogeny. 'phylopairs' calculates a covariance matrix for the pairwise-defined data and provides built-in models to test for relationships among variables while taking this covariance into account. Bayesian sampling is run through built-in 'Stan' programs via the 'rstan' package. The various models and methods that this package makes available are described in Anderson et al. (In Review), Coyne and Orr (1989) <doi:10.1111/j.1558-5646.1989.tb04233.x>, Fitzpatrick (2002) <doi:10.1111/j.0014-3820.2002.tb00860.x>, and Castillo (2007) <doi:10.1002/ece3.3093>.",
    "version": "0.1.1",
    "maintainer": "Sean A. S. Anderson <sean.as.anderson@gmail.com>",
    "author": "Sean A. S. Anderson [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phylopairs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylopairs Comparative Analyses of Lineage-Pair Traits Facilitates the testing of causal relationships among lineage-pair traits in a phylogenetically informed context. Lineage-pair traits are characters that are defined for pairs of lineages instead of individual taxa. Examples include the strength of reproductive isolation, range overlap, competition coefficient, diet niche similarity, and relative hybrid fitness. Users supply a lineage-pair dataset and a phylogeny. 'phylopairs' calculates a covariance matrix for the pairwise-defined data and provides built-in models to test for relationships among variables while taking this covariance into account. Bayesian sampling is run through built-in 'Stan' programs via the 'rstan' package. The various models and methods that this package makes available are described in Anderson et al. (In Review), Coyne and Orr (1989) <doi:10.1111/j.1558-5646.1989.tb04233.x>, Fitzpatrick (2002) <doi:10.1111/j.0014-3820.2002.tb00860.x>, and Castillo (2007) <doi:10.1002/ece3.3093>.  "
  },
  {
    "id": 17921,
    "package_name": "phyloseqGraphTest",
    "title": "Graph-Based Permutation Tests for Microbiome Data",
    "description": "Provides functions for graph-based multiple-sample\n    testing and visualization of microbiome data, in particular data\n    stored in 'phyloseq' objects. The tests are based on those\n    described in Friedman and Rafsky (1979)\n    <http://www.jstor.org/stable/2958919>, and the tests are described\n    in more detail in Callahan et al. (2016)\n    <doi:10.12688/f1000research.8986.1>.",
    "version": "0.1.1",
    "maintainer": "Julia Fukuyama <julia.fukuyama@gmail.com>",
    "author": "Julia Fukuyama [aut, cre]",
    "url": "https://github.com/jfukuyama/phyloseqGraphTest",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=phyloseqGraphTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phyloseqGraphTest Graph-Based Permutation Tests for Microbiome Data Provides functions for graph-based multiple-sample\n    testing and visualization of microbiome data, in particular data\n    stored in 'phyloseq' objects. The tests are based on those\n    described in Friedman and Rafsky (1979)\n    <http://www.jstor.org/stable/2958919>, and the tests are described\n    in more detail in Callahan et al. (2016)\n    <doi:10.12688/f1000research.8986.1>.  "
  },
  {
    "id": 17923,
    "package_name": "phylosignalDB",
    "title": "Explore Phylogenetic Signals Using Distance-Based Methods",
    "description": "A unified method, called M statistic, is provided for detecting phylogenetic signals in continuous traits, discrete traits, and multi-trait combinations. Blomberg and Garland (2002) <doi:10.1046/j.1420-9101.2002.00472.x> provided a widely accepted statistical definition of the phylogenetic signal, which is the \"tendency for related species to resemble each other more than they resemble species drawn at random from the tree\". The M statistic strictly adheres to the definition of phylogenetic signal, formulating an index and developing a method of testing in strict accordance with the definition, instead of relying on correlation analysis or evolutionary models. The novel method equivalently expressed the textual definition of the phylogenetic signal as an inequality equation of the phylogenetic and trait distances and constructed the M statistic. Also, there are more distance-based methods under development.",
    "version": "0.2.2",
    "maintainer": "Liang Yao <dylanyao@126.com>",
    "author": "Liang Yao [aut, cre],\n  Ye Yuan [aut]",
    "url": "https://github.com/anonymous-eco/phylosignalDB",
    "bug_reports": "https://github.com/anonymous-eco/phylosignalDB/issues",
    "repository": "https://cran.r-project.org/package=phylosignalDB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phylosignalDB Explore Phylogenetic Signals Using Distance-Based Methods A unified method, called M statistic, is provided for detecting phylogenetic signals in continuous traits, discrete traits, and multi-trait combinations. Blomberg and Garland (2002) <doi:10.1046/j.1420-9101.2002.00472.x> provided a widely accepted statistical definition of the phylogenetic signal, which is the \"tendency for related species to resemble each other more than they resemble species drawn at random from the tree\". The M statistic strictly adheres to the definition of phylogenetic signal, formulating an index and developing a method of testing in strict accordance with the definition, instead of relying on correlation analysis or evolutionary models. The novel method equivalently expressed the textual definition of the phylogenetic signal as an inequality equation of the phylogenetic and trait distances and constructed the M statistic. Also, there are more distance-based methods under development.  "
  },
  {
    "id": 17983,
    "package_name": "pkgKitten",
    "title": "Create Simple Packages Which Do not Upset R Package Checks",
    "description": "Provides a function kitten() which creates cute little \n packages which pass R package checks. This sets it apart from \n package.skeleton() which it calls, and which leaves imperfect files \n behind. As this is not exactly helpful for beginners, kitten() offers \n an alternative. Unit test support can be added via the 'tinytest'\n package (if present), and documentation-creation support can be\n added via 'roxygen2' (if present).",
    "version": "0.2.4",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>)",
    "url": "https://github.com/eddelbuettel/pkgkitten,\nhttps://eddelbuettel.github.io/pkgkitten/",
    "bug_reports": "https://github.com/eddelbuettel/pkgkitten/issues",
    "repository": "https://cran.r-project.org/package=pkgKitten",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pkgKitten Create Simple Packages Which Do not Upset R Package Checks Provides a function kitten() which creates cute little \n packages which pass R package checks. This sets it apart from \n package.skeleton() which it calls, and which leaves imperfect files \n behind. As this is not exactly helpful for beginners, kitten() offers \n an alternative. Unit test support can be added via the 'tinytest'\n package (if present), and documentation-creation support can be\n added via 'roxygen2' (if present).  "
  },
  {
    "id": 17996,
    "package_name": "pks",
    "title": "Probabilistic Knowledge Structures",
    "description": "Fitting and testing probabilistic knowledge structures,\n  especially the basic local independence model (BLIM, Doignon & Flamagne,\n  1999) and the simple learning model (SLM), using the minimum discrepancy\n  maximum likelihood (MDML) method (Heller & Wickelmaier, 2013\n  <doi:10.1016/j.endm.2013.05.145>).",
    "version": "0.6-1",
    "maintainer": "Florian Wickelmaier <wickelmaier@web.de>",
    "author": "Florian Wickelmaier [aut, cre],\n  Juergen Heller [aut],\n  Julian Mollenhauer [aut],\n  Pasquale Anselmi [ctb],\n  Debora de Chiusole [ctb],\n  Andrea Brancaccio [ctb],\n  Luca Stefanutti [ctb]",
    "url": "https://www.mathpsy.uni-tuebingen.de/wickelmaier/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pks Probabilistic Knowledge Structures Fitting and testing probabilistic knowledge structures,\n  especially the basic local independence model (BLIM, Doignon & Flamagne,\n  1999) and the simple learning model (SLM), using the minimum discrepancy\n  maximum likelihood (MDML) method (Heller & Wickelmaier, 2013\n  <doi:10.1016/j.endm.2013.05.145>).  "
  },
  {
    "id": 18205,
    "package_name": "portfolioBacktest",
    "title": "Automated Backtesting of Portfolios over Multiple Datasets",
    "description": "Automated backtesting of multiple portfolios over multiple \n    datasets of stock prices in a rolling-window fashion. Intended for \n    researchers and practitioners to backtest a set of different portfolios, \n    as well as by a course instructor to assess the students in their portfolio \n    design in a fully automated and convenient manner, with results conveniently \n    formatted in tables and plots. Each portfolio design is easily defined as a\n    function that takes as input a window of the stock prices and outputs the \n    portfolio weights. Multiple portfolios can be easily specified as a list \n    of functions or as files in a folder. Multiple datasets can be conveniently \n    extracted randomly from different markets, different time periods, and \n    different subsets of the stock universe. The results can be later assessed \n    and ranked with tables based on a number of performance criteria (e.g., \n    expected return, volatility, Sharpe ratio, drawdown, turnover rate, return \n    on investment, computational time, etc.), as well as plotted in a number of \n    ways with nice barplots and boxplots.",
    "version": "0.4.1",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "author": "Daniel P. Palomar [cre, aut],\n  Rui Zhou [aut]",
    "url": "https://CRAN.R-project.org/package=portfolioBacktest,\nhttps://github.com/dppalomar/portfolioBacktest",
    "bug_reports": "https://github.com/dppalomar/portfolioBacktest/issues",
    "repository": "https://cran.r-project.org/package=portfolioBacktest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "portfolioBacktest Automated Backtesting of Portfolios over Multiple Datasets Automated backtesting of multiple portfolios over multiple \n    datasets of stock prices in a rolling-window fashion. Intended for \n    researchers and practitioners to backtest a set of different portfolios, \n    as well as by a course instructor to assess the students in their portfolio \n    design in a fully automated and convenient manner, with results conveniently \n    formatted in tables and plots. Each portfolio design is easily defined as a\n    function that takes as input a window of the stock prices and outputs the \n    portfolio weights. Multiple portfolios can be easily specified as a list \n    of functions or as files in a folder. Multiple datasets can be conveniently \n    extracted randomly from different markets, different time periods, and \n    different subsets of the stock universe. The results can be later assessed \n    and ranked with tables based on a number of performance criteria (e.g., \n    expected return, volatility, Sharpe ratio, drawdown, turnover rate, return \n    on investment, computational time, etc.), as well as plotted in a number of \n    ways with nice barplots and boxplots.  "
  },
  {
    "id": 18207,
    "package_name": "portn",
    "title": "Portfolio Analysis for Nature",
    "description": "The functions are designed to find the efficient mean-variance frontier or \n portfolio weights for static portfolio (called  Markowitz portfolio) analysis in resource \n economics or nature conservation. Using the nonlinear programming solver ('Rsolnp'), \n this package deals with the quadratic minimization of the variance-covariances without \n shorting (i.e., non-negative portfolio weights) studied in Ando and Mallory (2012) \n <doi:10.1073/pnas.1114653109>. See the examples, testing versions, and more details from: \n <https://github.com/ysd2004/portn>.",
    "version": "1.0.0",
    "maintainer": "Seong Yun <seong.yun@msstate.edu>",
    "author": "Seong Yun [aut, cre]",
    "url": "https://github.com/ysd2004/portn",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=portn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "portn Portfolio Analysis for Nature The functions are designed to find the efficient mean-variance frontier or \n portfolio weights for static portfolio (called  Markowitz portfolio) analysis in resource \n economics or nature conservation. Using the nonlinear programming solver ('Rsolnp'), \n this package deals with the quadratic minimization of the variance-covariances without \n shorting (i.e., non-negative portfolio weights) studied in Ando and Mallory (2012) \n <doi:10.1073/pnas.1114653109>. See the examples, testing versions, and more details from: \n <https://github.com/ysd2004/portn>.  "
  },
  {
    "id": 18234,
    "package_name": "powerCompRisk",
    "title": "Power Analysis Tool for Joint Testing Hazards with Competing\nRisks Data",
    "description": "A power analysis tool for jointly testing the cause-1 cause-specific hazard and the any-cause hazard with competing risks data.",
    "version": "1.0.1",
    "maintainer": "Eric Kawaguchi <erickawaguchi@ucla.edu>",
    "author": "Qing Yang[aut], Wing K. Fung[aut], Eric Kawaguchi[ctb], Gang Li[aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerCompRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerCompRisk Power Analysis Tool for Joint Testing Hazards with Competing\nRisks Data A power analysis tool for jointly testing the cause-1 cause-specific hazard and the any-cause hazard with competing risks data.  "
  },
  {
    "id": 18235,
    "package_name": "powerEQTL",
    "title": "Power and Sample Size Calculation for Bulk Tissue and\nSingle-Cell eQTL Analysis",
    "description": "Power and sample size calculation for bulk tissue and single-cell eQTL analysis\n             based on ANOVA, simple linear regression, or linear mixed effects model. It can also calculate power/sample size \n             for testing the association of a SNP to a continuous type phenotype.\n             Please see the reference: Dong X, Li X, Chang T-W, Scherzer CR, Weiss ST, Qiu W. (2021) <doi:10.1093/bioinformatics/btab385>.",
    "version": "0.3.6",
    "maintainer": "Weiliang Qiu <weiliang.qiu@gmail.com>",
    "author": "Xianjun Dong [aut, ctb],\n  Xiaoqi Li [aut, ctb],\n  Tzuu-Wang Chang [aut, ctb],\n  Scott T. Weiss [aut, ctb],\n  Weiliang Qiu [aut, cre]",
    "url": "https://github.com/sterding/powerEQTL,\nhttps://bwhbioinfo.shinyapps.io/powerEQTL/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerEQTL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerEQTL Power and Sample Size Calculation for Bulk Tissue and\nSingle-Cell eQTL Analysis Power and sample size calculation for bulk tissue and single-cell eQTL analysis\n             based on ANOVA, simple linear regression, or linear mixed effects model. It can also calculate power/sample size \n             for testing the association of a SNP to a continuous type phenotype.\n             Please see the reference: Dong X, Li X, Chang T-W, Scherzer CR, Weiss ST, Qiu W. (2021) <doi:10.1093/bioinformatics/btab385>.  "
  },
  {
    "id": 18236,
    "package_name": "powerGWASinteraction",
    "title": "Power Calculations for GxE and GxG Interactions for GWAS",
    "description": "Analytical power calculations for GxE and GxG interactions for case-control studies of candidate genes and genome-wide association studies (GWAS). This includes power calculation for four two-step screening and testing procedures.  It can also calculate power for GxE and GxG without any screening.    ",
    "version": "1.1.3",
    "maintainer": "Charles Kooperberg <clk@fredhutch.org>",
    "author": "Charles Kooperberg <clk@fredhutch.org> and Li Hsu <lih@fredhutch.org>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerGWASinteraction",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerGWASinteraction Power Calculations for GxE and GxG Interactions for GWAS Analytical power calculations for GxE and GxG interactions for case-control studies of candidate genes and genome-wide association studies (GWAS). This includes power calculation for four two-step screening and testing procedures.  It can also calculate power for GxE and GxG without any screening.      "
  },
  {
    "id": 18237,
    "package_name": "powerHE",
    "title": "Power and Sample Size Calculations with Hierarchical Endpoints",
    "description": "Calculate sample size or power for hierarchical endpoints. \n  The package can handle any type of outcomes (binary, continuous, count, ordinal, \n  time-to-event) and any number of such endpoints. It allows users to calculate \n  sample size with a given power or to calculate power with a given sample size \n  for hypothesis testing based on win ratios, win odds, net benefit, or DOOR \n  (desirability of outcome ranking) as treatment effect between two groups for \n  hierarchical endpoints. The methods of this package are described further in \n  the paper by Barnhart, H. X. et al. (2024, <doi:10.1080/19466315.2024.2365629>).",
    "version": "1.0.1",
    "maintainer": "Sarah Wu <O2E@duke.edu>",
    "author": "Sarah Wu [aut, cre],\n  Dylan Thibault [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerHE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerHE Power and Sample Size Calculations with Hierarchical Endpoints Calculate sample size or power for hierarchical endpoints. \n  The package can handle any type of outcomes (binary, continuous, count, ordinal, \n  time-to-event) and any number of such endpoints. It allows users to calculate \n  sample size with a given power or to calculate power with a given sample size \n  for hypothesis testing based on win ratios, win odds, net benefit, or DOOR \n  (desirability of outcome ranking) as treatment effect between two groups for \n  hierarchical endpoints. The methods of this package are described further in \n  the paper by Barnhart, H. X. et al. (2024, <doi:10.1080/19466315.2024.2365629>).  "
  },
  {
    "id": 18239,
    "package_name": "powerMediation",
    "title": "Power/Sample Size Calculation for Mediation Analysis",
    "description": "Functions to \n        calculate power and sample size for testing\n        (1) mediation effects; \n        (2) the slope in a simple linear regression; \n        (3) odds ratio in a simple logistic regression;\n        (4) mean change for longitudinal study with 2 time points;\n        (5) interaction effect in 2-way ANOVA; and\n        (6) the slope in a simple Poisson regression.",
    "version": "0.3.4",
    "maintainer": "Weiliang Qiu <weiliang.qiu@gmail.com>",
    "author": "Weiliang Qiu <weiliang.qiu@gmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerMediation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerMediation Power/Sample Size Calculation for Mediation Analysis Functions to \n        calculate power and sample size for testing\n        (1) mediation effects; \n        (2) the slope in a simple linear regression; \n        (3) odds ratio in a simple logistic regression;\n        (4) mean change for longitudinal study with 2 time points;\n        (5) interaction effect in 2-way ANOVA; and\n        (6) the slope in a simple Poisson regression.  "
  },
  {
    "id": 18242,
    "package_name": "powerSurvEpi",
    "title": "Power and Sample Size Calculation for Survival Analysis of\nEpidemiological Studies",
    "description": "Functions to calculate power and\n                sample size for testing main effect or interaction effect in\n                the survival analysis of epidemiological studies\n                (non-randomized studies), taking into account the \n                correlation between the covariate of the\n                interest and other covariates. Some calculations also take \n                into account the competing risks and stratified analysis. \n                This package also includes\n                a set of functions to calculate power and sample size\n                for testing main effect in the survival analysis of \n                randomized clinical trials and conditional logistic regression for nested case-control study.",
    "version": "0.1.5",
    "maintainer": "Weiliang Qiu <weiliang.qiu@gmail.com>",
    "author": "Weiliang Qiu [aut, cre],\n  Jorge Chavarro [aut],\n  Ross Lazarus [aut],\n  Bernard Rosner [aut],\n  Jing Ma [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerSurvEpi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerSurvEpi Power and Sample Size Calculation for Survival Analysis of\nEpidemiological Studies Functions to calculate power and\n                sample size for testing main effect or interaction effect in\n                the survival analysis of epidemiological studies\n                (non-randomized studies), taking into account the \n                correlation between the covariate of the\n                interest and other covariates. Some calculations also take \n                into account the competing risks and stratified analysis. \n                This package also includes\n                a set of functions to calculate power and sample size\n                for testing main effect in the survival analysis of \n                randomized clinical trials and conditional logistic regression for nested case-control study.  "
  },
  {
    "id": 18250,
    "package_name": "powerpkg",
    "title": "Power Analyses for the Affected Sib Pair and the TDT Design",
    "description": "There are two main functions: (1) To estimate the power of testing for linkage using an\n        affected sib pair design, as a function of the recurrence risk\n        ratios. We will use analytical power formulae as implemented in\n        R. These are based on a Mathematica notebook created by Martin\n        Farrall. (2) To examine how the power of the transmission\n        disequilibrium test (TDT) depends on the disease allele\n        frequency, the marker allele frequency, the strength of the\n        linkage disequilibrium, and the magnitude of the genetic\n        effect. We will use an R program that implements the power\n        formulae of Abel and Muller-Myhsok (1998). These formulae allow\n        one to quickly compute power of the TDT approach under a\n        variety of different conditions. This R program was modeled on\n        Martin Farrall's Mathematica notebook.",
    "version": "1.6",
    "maintainer": "Daniel E. Weeks <weeks@pitt.edu>",
    "author": "Daniel E. Weeks",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=powerpkg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "powerpkg Power Analyses for the Affected Sib Pair and the TDT Design There are two main functions: (1) To estimate the power of testing for linkage using an\n        affected sib pair design, as a function of the recurrence risk\n        ratios. We will use analytical power formulae as implemented in\n        R. These are based on a Mathematica notebook created by Martin\n        Farrall. (2) To examine how the power of the transmission\n        disequilibrium test (TDT) depends on the disease allele\n        frequency, the marker allele frequency, the strength of the\n        linkage disequilibrium, and the magnitude of the genetic\n        effect. We will use an R program that implements the power\n        formulae of Abel and Muller-Myhsok (1998). These formulae allow\n        one to quickly compute power of the TDT approach under a\n        variety of different conditions. This R program was modeled on\n        Martin Farrall's Mathematica notebook.  "
  },
  {
    "id": 18272,
    "package_name": "pprof",
    "title": "Modeling, Standardization and Testing for Provider Profiling",
    "description": "Implements linear and generalized linear models for provider profiling,\n    incorporating both fixed and random effects. For large-scale providers, the linear \n    profiled-based method and the SerBIN method for binary data \n    reduce the computational burden. Provides post-modeling features, such as \n    indirect and direct standardization measures, hypothesis testing, \n    confidence intervals, and post-estimation visualization. \n    For more information, see Wu et al. (2022) <doi:10.1002/sim.9387>.",
    "version": "1.0.2",
    "maintainer": "Xiaohan Liu <xhliuu@umich.edu>",
    "author": "Xiaohan Liu [aut, cre],\n  Lingfeng Luo [aut],\n  Yubo Shao [aut],\n  Xiangeng Fang [aut],\n  Wenbo Wu [aut],\n  Kevin He [aut]",
    "url": "https://github.com/UM-KevinHe/pprof",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pprof",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pprof Modeling, Standardization and Testing for Provider Profiling Implements linear and generalized linear models for provider profiling,\n    incorporating both fixed and random effects. For large-scale providers, the linear \n    profiled-based method and the SerBIN method for binary data \n    reduce the computational burden. Provides post-modeling features, such as \n    indirect and direct standardization measures, hypothesis testing, \n    confidence intervals, and post-estimation visualization. \n    For more information, see Wu et al. (2022) <doi:10.1002/sim.9387>.  "
  },
  {
    "id": 18283,
    "package_name": "prabclus",
    "title": "Functions for Clustering and Testing of Presence-Absence,\nAbundance and Multilocus Genetic Data",
    "description": "Distance-based parametric bootstrap tests for clustering with \n  spatial neighborhood information. Some distance measures, \n  Clustering of presence-absence, abundance and multilocus genetic data \n  for species delimitation, nearest neighbor \n  based noise detection. Genetic distances between communities.\n  Tests whether various distance-based regressions\n  are equal. Try package?prabclus for on overview. ",
    "version": "2.3-4",
    "maintainer": "Christian Hennig <christian.hennig@unibo.it>",
    "author": "Christian Hennig [aut, cre],\n  Bernhard Hausdorf [aut]",
    "url": "https://www.unibo.it/sitoweb/christian.hennig/en/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=prabclus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prabclus Functions for Clustering and Testing of Presence-Absence,\nAbundance and Multilocus Genetic Data Distance-based parametric bootstrap tests for clustering with \n  spatial neighborhood information. Some distance measures, \n  Clustering of presence-absence, abundance and multilocus genetic data \n  for species delimitation, nearest neighbor \n  based noise detection. Genetic distances between communities.\n  Tests whether various distance-based regressions\n  are equal. Try package?prabclus for on overview.   "
  },
  {
    "id": 18290,
    "package_name": "prcbench",
    "title": "Testing Workbench for Precision-Recall Curves",
    "description": "A testing workbench to evaluate tools that calculate precision-recall curves.\n    Saito and Rehmsmeier (2015) <doi:10.1371/journal.pone.0118432>.",
    "version": "1.1.10",
    "maintainer": "Takaya Saito <takaya.saito@outlook.com>",
    "author": "Takaya Saito [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0154-8452>),\n  Marc Rehmsmeier [aut] (ORCID: <https://orcid.org/0000-0002-5021-7721>)",
    "url": "https://evalclass.github.io/prcbench/,\nhttps://github.com/evalclass/prcbench",
    "bug_reports": "https://github.com/evalclass/prcbench/issues",
    "repository": "https://cran.r-project.org/package=prcbench",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prcbench Testing Workbench for Precision-Recall Curves A testing workbench to evaluate tools that calculate precision-recall curves.\n    Saito and Rehmsmeier (2015) <doi:10.1371/journal.pone.0118432>.  "
  },
  {
    "id": 18338,
    "package_name": "pretest",
    "title": "A Novel Approach to Predictive Accuracy Testing in Nested\nEnvironments",
    "description": "This repository contains the codes for using the predictive accuracy comparison tests developed in Pitarakis, J. (2023) <doi:10.1017/S0266466623000154>. ",
    "version": "0.2",
    "maintainer": "Rong Peng <r.peng@soton.ac.uk>",
    "author": "Rong Peng [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pretest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pretest A Novel Approach to Predictive Accuracy Testing in Nested\nEnvironments This repository contains the codes for using the predictive accuracy comparison tests developed in Pitarakis, J. (2023) <doi:10.1017/S0266466623000154>.   "
  },
  {
    "id": 18339,
    "package_name": "pretestcad",
    "title": "Pretest Probability for Coronary Artery Disease",
    "description": "An application to calculate a patient's pretest probability\n    (PTP) for obstructive Coronary Artery Disease (CAD) from a collection\n    of guidelines or studies. Guidelines usually comes from the American\n    Heart Association (AHA), American College of Cardiology (ACC) or\n    European Society of Cardiology (ESC). Examples of PTP scores that\n    comes from studies are the 2020 Winther et al. basic, Risk\n    Factor-weighted Clinical Likelihood (RF-CL) and Coronary Artery\n    Calcium Score-weighted Clinical Likelihood (CACS-CL) models\n    <doi:10.1016/j.jacc.2020.09.585>, 2019 Reeh et al. basic and clinical\n    models <doi:10.1093/eurheartj/ehy806> and 2017 Fordyce et al. PROMISE\n    Minimal-Risk Tool <doi:10.1001/jamacardio.2016.5501>.  As diagnosis of\n    CAD involves a costly and invasive coronary angiography procedure for\n    patients, having a reliable PTP for CAD helps doctors to make better\n    decisions during patient management.  This ensures high risk patients\n    can be diagnosed and treated early for CAD while avoiding unnecessary\n    testing for low risk patients.",
    "version": "1.1.0",
    "maintainer": "Jeremy Selva <jeremy1189.jjs@gmail.com>",
    "author": "Jeremy Selva [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4498-2662>)",
    "url": "https://github.com/JauntyJJS/pretestcad,\nhttps://jauntyjjs.github.io/pretestcad/",
    "bug_reports": "https://github.com/JauntyJJS/pretestcad/issues",
    "repository": "https://cran.r-project.org/package=pretestcad",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pretestcad Pretest Probability for Coronary Artery Disease An application to calculate a patient's pretest probability\n    (PTP) for obstructive Coronary Artery Disease (CAD) from a collection\n    of guidelines or studies. Guidelines usually comes from the American\n    Heart Association (AHA), American College of Cardiology (ACC) or\n    European Society of Cardiology (ESC). Examples of PTP scores that\n    comes from studies are the 2020 Winther et al. basic, Risk\n    Factor-weighted Clinical Likelihood (RF-CL) and Coronary Artery\n    Calcium Score-weighted Clinical Likelihood (CACS-CL) models\n    <doi:10.1016/j.jacc.2020.09.585>, 2019 Reeh et al. basic and clinical\n    models <doi:10.1093/eurheartj/ehy806> and 2017 Fordyce et al. PROMISE\n    Minimal-Risk Tool <doi:10.1001/jamacardio.2016.5501>.  As diagnosis of\n    CAD involves a costly and invasive coronary angiography procedure for\n    patients, having a reliable PTP for CAD helps doctors to make better\n    decisions during patient management.  This ensures high risk patients\n    can be diagnosed and treated early for CAD while avoiding unnecessary\n    testing for low risk patients.  "
  },
  {
    "id": 18362,
    "package_name": "primes",
    "title": "Fast Functions for Prime Numbers",
    "description": "Fast functions for dealing with prime numbers, such as testing\n    whether a number is prime and generating a sequence prime numbers.\n    Additional functions include finding prime factors and Ruth-Aaron pairs,\n    finding next and previous prime numbers in the series, finding or estimating\n    the nth prime, estimating the number of primes less than or equal to an\n    arbitrary number, computing primorials, prime k-tuples (e.g., twin primes),\n    finding the greatest common divisor and smallest (least) common multiple,\n    testing whether two numbers are coprime, and computing Euler's totient\n    function. Most functions are vectorized for speed and convenience.",
    "version": "1.6.1",
    "maintainer": "Paul Egeler <paulegeler@gmail.com>",
    "author": "Os Keyes [aut],\n  Paul Egeler [aut, cre] (ORCID: <https://orcid.org/0000-0001-6948-9498>)",
    "url": "https://github.com/ironholds/primes",
    "bug_reports": "https://github.com/ironholds/primes/issues",
    "repository": "https://cran.r-project.org/package=primes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "primes Fast Functions for Prime Numbers Fast functions for dealing with prime numbers, such as testing\n    whether a number is prime and generating a sequence prime numbers.\n    Additional functions include finding prime factors and Ruth-Aaron pairs,\n    finding next and previous prime numbers in the series, finding or estimating\n    the nth prime, estimating the number of primes less than or equal to an\n    arbitrary number, computing primorials, prime k-tuples (e.g., twin primes),\n    finding the greatest common divisor and smallest (least) common multiple,\n    testing whether two numbers are coprime, and computing Euler's totient\n    function. Most functions are vectorized for speed and convenience.  "
  },
  {
    "id": 18385,
    "package_name": "probe",
    "title": "Sparse High-Dimensional Linear Regression with PROBE",
    "description": "Implements an efficient and powerful Bayesian approach for sparse high-dimensional linear regression. It uses minimal prior assumptions on the parameters through plug-in empirical Bayes estimates of hyperparameters. An efficient Parameter-Expanded Expectation-Conditional-Maximization (PX-ECM) algorithm estimates maximum a posteriori (MAP) values of regression parameters and variable selection probabilities. The PX-ECM results in a robust computationally efficient coordinate-wise optimization, which adjusts for the impact of other predictor variables. The E-step is motivated by the popular two-group approach to multiple testing. The result is a PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse high-dimensional linear regression, implemented using one-at-a-time or all-at-once type optimization. More information can be found in McLain, Zgodic, and Bondell (2022) <arXiv:2209.08139>.",
    "version": "1.1",
    "maintainer": "Alexander McLain <mclaina@mailbox.sc.edu>",
    "author": "Alexander McLain [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5475-0670>),\n  Anja Zodiac [aut, ctb]",
    "url": "",
    "bug_reports": "https://github.com/alexmclain/PROBE/issues",
    "repository": "https://cran.r-project.org/package=probe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "probe Sparse High-Dimensional Linear Regression with PROBE Implements an efficient and powerful Bayesian approach for sparse high-dimensional linear regression. It uses minimal prior assumptions on the parameters through plug-in empirical Bayes estimates of hyperparameters. An efficient Parameter-Expanded Expectation-Conditional-Maximization (PX-ECM) algorithm estimates maximum a posteriori (MAP) values of regression parameters and variable selection probabilities. The PX-ECM results in a robust computationally efficient coordinate-wise optimization, which adjusts for the impact of other predictor variables. The E-step is motivated by the popular two-group approach to multiple testing. The result is a PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse high-dimensional linear regression, implemented using one-at-a-time or all-at-once type optimization. More information can be found in McLain, Zgodic, and Bondell (2022) <arXiv:2209.08139>.  "
  },
  {
    "id": 18388,
    "package_name": "probstats4econ",
    "title": "Companion Package to Probability and Statistics for Economics\nand Business",
    "description": "Utilities for multiple hypothesis testing, companion datasets from \"Probability and Statistics for Economics and Business: An Introduction Using R\" by Jason Abrevaya (MIT Press, under contract).",
    "version": "0.3.1",
    "maintainer": "Nathan Gardner Hattersley <nhattersley@utexas.edu>",
    "author": "Jason Abrevaya [aut, cph],\n  Nathan Gardner Hattersley [aut, cre]",
    "url": "https://probstats4econ.com/package.html,\nhttps://probstats4econ.com/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=probstats4econ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "probstats4econ Companion Package to Probability and Statistics for Economics\nand Business Utilities for multiple hypothesis testing, companion datasets from \"Probability and Statistics for Economics and Business: An Introduction Using R\" by Jason Abrevaya (MIT Press, under contract).  "
  },
  {
    "id": 18451,
    "package_name": "prototest",
    "title": "Inference on Prototypes from Clusters of Features",
    "description": "Procedures for testing for group-wide signal in clusters of variables. Tests can be performed for single groups in isolation (univariate) or multiple groups together (multivariate). Specific tests include the exact and approximate (un)selective likelihood ratio tests described in Reid et al (2015), the selective F test and marginal screening prototype test of Reid and Tibshirani (2015). User may pre-specify columns to be included in prototype formation, or allow the function to select them itself. A mixture of these two is also possible. Any variable selection is accounted for using the selective inference framework. Options for non-sampling and hit-and-run null reference distributions.",
    "version": "1.2",
    "maintainer": "Stephen Reid <sreid1652@gmail.com>",
    "author": "Stephen Reid",
    "url": "http://arxiv.org/abs/1511.07839",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=prototest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "prototest Inference on Prototypes from Clusters of Features Procedures for testing for group-wide signal in clusters of variables. Tests can be performed for single groups in isolation (univariate) or multiple groups together (multivariate). Specific tests include the exact and approximate (un)selective likelihood ratio tests described in Reid et al (2015), the selective F test and marginal screening prototype test of Reid and Tibshirani (2015). User may pre-specify columns to be included in prototype formation, or allow the function to select them itself. A mixture of these two is also possible. Any variable selection is accounted for using the selective inference framework. Options for non-sampling and hit-and-run null reference distributions.  "
  },
  {
    "id": 18488,
    "package_name": "pseval",
    "title": "Methods for Evaluating Principal Surrogates of Treatment\nResponse",
    "description": "Contains the core methods for the evaluation of principal\n    surrogates in a single clinical trial. Provides a flexible interface for\n    defining models for the risk given treatment and the surrogate, the models\n    for integration over the missing counterfactual surrogate responses, and the\n    estimation methods. Estimated maximum likelihood and pseudo-score can be used\n    for estimation, and the bootstrap for inference. A variety of post-estimation\n    summary methods are provided, including print, summary, plot, and testing.",
    "version": "1.3.3",
    "maintainer": "Michael C Sachs <sachsmc@gmail.com>",
    "author": "Michael C Sachs [aut, cre],\n  Erin E Gabriel [aut]",
    "url": "https://sachsmc.github.io/pseval/",
    "bug_reports": "https://github.com/sachsmc/pseval/issues/",
    "repository": "https://cran.r-project.org/package=pseval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pseval Methods for Evaluating Principal Surrogates of Treatment\nResponse Contains the core methods for the evaluation of principal\n    surrogates in a single clinical trial. Provides a flexible interface for\n    defining models for the risk given treatment and the surrogate, the models\n    for integration over the missing counterfactual surrogate responses, and the\n    estimation methods. Estimated maximum likelihood and pseudo-score can be used\n    for estimation, and the bootstrap for inference. A variety of post-estimation\n    summary methods are provided, including print, summary, plot, and testing.  "
  },
  {
    "id": 18497,
    "package_name": "pso",
    "title": "Particle Swarm Optimization",
    "description": "Provides an implementation of particle swarm optimisation consistent with the standard PSO 2007/2011 by Maurice Clerc. Additionally a number of ancillary routines are provided for easy testing and graphics.",
    "version": "1.0.4",
    "maintainer": "Claus Bendtsen <papyrus.bendtsen@gmail.com>",
    "author": "Claus Bendtsen <papyrus.bendtsen@gmail.com>.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pso Particle Swarm Optimization Provides an implementation of particle swarm optimisation consistent with the standard PSO 2007/2011 by Maurice Clerc. Additionally a number of ancillary routines are provided for easy testing and graphics.  "
  },
  {
    "id": 18505,
    "package_name": "pssmooth",
    "title": "Flexible and Efficient Evaluation of Principal\nSurrogates/Treatment Effect Modifiers",
    "description": "Implements estimation and testing procedures for evaluating an intermediate biomarker response as a principal surrogate of a clinical response to treatment (i.e., principal stratification effect modification analysis), as described in Juraska M, Huang Y, and Gilbert PB (2020), Inference on treatment effect modification by biomarker response in a three-phase sampling design, Biostatistics, 21(3): 545-560 <doi:10.1093/biostatistics/kxy074>. The methods avoid the restrictive 'placebo structural risk' modeling assumption common to past methods and further improve robustness by the use of nonparametric kernel smoothing for biomarker density estimation. A randomized controlled two-group clinical efficacy trial is assumed with an ordered categorical or continuous univariate biomarker response measured at a fixed timepoint post-randomization and with a univariate baseline surrogate measure allowed to be observed in only a subset of trial participants with an observed biomarker response (see the flexible three-phase sampling design in the paper for details). Bootstrap-based procedures are available for pointwise and simultaneous confidence intervals and testing of four relevant hypotheses. Summary and plotting functions are provided for estimation results.",
    "version": "1.0.3",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "author": "Michal Juraska [aut, cre]",
    "url": "https://github.com/mjuraska/pssmooth",
    "bug_reports": "https://github.com/mjuraska/pssmooth/issues",
    "repository": "https://cran.r-project.org/package=pssmooth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pssmooth Flexible and Efficient Evaluation of Principal\nSurrogates/Treatment Effect Modifiers Implements estimation and testing procedures for evaluating an intermediate biomarker response as a principal surrogate of a clinical response to treatment (i.e., principal stratification effect modification analysis), as described in Juraska M, Huang Y, and Gilbert PB (2020), Inference on treatment effect modification by biomarker response in a three-phase sampling design, Biostatistics, 21(3): 545-560 <doi:10.1093/biostatistics/kxy074>. The methods avoid the restrictive 'placebo structural risk' modeling assumption common to past methods and further improve robustness by the use of nonparametric kernel smoothing for biomarker density estimation. A randomized controlled two-group clinical efficacy trial is assumed with an ordered categorical or continuous univariate biomarker response measured at a fixed timepoint post-randomization and with a univariate baseline surrogate measure allowed to be observed in only a subset of trial participants with an observed biomarker response (see the flexible three-phase sampling design in the paper for details). Bootstrap-based procedures are available for pointwise and simultaneous confidence intervals and testing of four relevant hypotheses. Summary and plotting functions are provided for estimation results.  "
  },
  {
    "id": 18511,
    "package_name": "psych",
    "title": "Procedures for Psychological, Psychometric, and Personality\nResearch",
    "description": "A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using  factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. Several functions  serve as a useful front end for structural equation modeling.  Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the <https://personality-project.org/r/> web page.",
    "version": "2.5.6",
    "maintainer": "William Revelle <revelle@northwestern.edu>",
    "author": "William Revelle [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4880-9610>)",
    "url": "https://personality-project.org/r/psych/\nhttps://personality-project.org/r/psych-manual.pdf",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psych Procedures for Psychological, Psychometric, and Personality\nResearch A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology.  Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using  factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis.  Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. Several functions  serve as a useful front end for structural equation modeling.  Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the <https://personality-project.org/r/> web page.  "
  },
  {
    "id": 18518,
    "package_name": "psychonetrics",
    "title": "Structural Equation Modeling and Confirmatory Network Analysis",
    "description": "Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search.",
    "version": "0.13.2",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "author": "Sacha Epskamp [aut, cre]",
    "url": "http://psychonetrics.org/",
    "bug_reports": "https://github.com/SachaEpskamp/psychonetrics/issues",
    "repository": "https://cran.r-project.org/package=psychonetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psychonetrics Structural Equation Modeling and Confirmatory Network Analysis Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search.  "
  },
  {
    "id": 18553,
    "package_name": "puniform",
    "title": "Meta-Analysis Methods Correcting for Publication Bias",
    "description": "Provides meta-analysis methods that correct for\n    publication bias and outcome reporting bias. Four methods and a visual tool \n    are currently included in the package. The p-uniform method as described in \n    van Assen, van Aert, and Wicherts (2015) <doi:10.1037/met0000025> \n    can be used for estimating the average effect size, testing the null hypothesis \n    of no effect, and testing for publication bias using only the statistically \n    significant effect sizes of primary studies. The second method in the package \n    is the p-uniform* method as described in van Aert and van Assen (2023) \n    <doi:10.31222/osf.io/zqjr9>. This method is an extension of the p-uniform \n    method that allows for estimation of the average effect size and the \n    between-study variance in a meta-analysis, and uses both the statistically \n    significant and nonsignificant effect sizes. The third method in the package \n    is the hybrid method as described in van Aert and van Assen (2018) \n    <doi:10.3758/s13428-017-0967-6>. The hybrid method is a meta-analysis method \n    for combining a conventional study and replication/preregistered study while \n    taking into account statistical significance of the conventional study. This\n    method was extended in van Aert (2025) <doi:10.1037/met0000719> \n    such that it allows for the inclusion of multiple conventional and \n    replication/preregistered studies. The p-uniform and hybrid method are based \n    on the statistical theory that the distribution of p-values is uniform \n    conditional on the population effect size. The fourth method in the package \n    is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert \n    and van Assen (2018) <doi:10.1371/journal.pone.0175302>. This method computes \n    posterior probabilities for four true effect sizes (no, small, medium, and \n    large) based on an original study and replication while taking into account \n    publication bias in the original study. The method can also be used for \n    computing the required sample size of the replication akin to power analysis \n    in null-hypothesis significance testing. The meta-plot is a visual tool for \n    meta-analysis that provides information on the primary studies in the \n    meta-analysis, the results of the meta-analysis, and characteristics of the \n    research on the effect under study (van Assen et al., 2023). Helper functions \n    to apply the Correcting for Outcome Reporting Bias (CORB) method to correct \n    for outcome reporting bias in a meta-analysis (van Aert & Wicherts, 2023).",
    "version": "0.2.8",
    "maintainer": "Robbie C.M. van Aert <R.C.M.vanAert@tilburguniversity.edu>",
    "author": "Robbie C.M. van Aert [aut, cre]",
    "url": "https://github.com/RobbievanAert/puniform",
    "bug_reports": "https://github.com/RobbievanAert/puniform/issues",
    "repository": "https://cran.r-project.org/package=puniform",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "puniform Meta-Analysis Methods Correcting for Publication Bias Provides meta-analysis methods that correct for\n    publication bias and outcome reporting bias. Four methods and a visual tool \n    are currently included in the package. The p-uniform method as described in \n    van Assen, van Aert, and Wicherts (2015) <doi:10.1037/met0000025> \n    can be used for estimating the average effect size, testing the null hypothesis \n    of no effect, and testing for publication bias using only the statistically \n    significant effect sizes of primary studies. The second method in the package \n    is the p-uniform* method as described in van Aert and van Assen (2023) \n    <doi:10.31222/osf.io/zqjr9>. This method is an extension of the p-uniform \n    method that allows for estimation of the average effect size and the \n    between-study variance in a meta-analysis, and uses both the statistically \n    significant and nonsignificant effect sizes. The third method in the package \n    is the hybrid method as described in van Aert and van Assen (2018) \n    <doi:10.3758/s13428-017-0967-6>. The hybrid method is a meta-analysis method \n    for combining a conventional study and replication/preregistered study while \n    taking into account statistical significance of the conventional study. This\n    method was extended in van Aert (2025) <doi:10.1037/met0000719> \n    such that it allows for the inclusion of multiple conventional and \n    replication/preregistered studies. The p-uniform and hybrid method are based \n    on the statistical theory that the distribution of p-values is uniform \n    conditional on the population effect size. The fourth method in the package \n    is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in van Aert \n    and van Assen (2018) <doi:10.1371/journal.pone.0175302>. This method computes \n    posterior probabilities for four true effect sizes (no, small, medium, and \n    large) based on an original study and replication while taking into account \n    publication bias in the original study. The method can also be used for \n    computing the required sample size of the replication akin to power analysis \n    in null-hypothesis significance testing. The meta-plot is a visual tool for \n    meta-analysis that provides information on the primary studies in the \n    meta-analysis, the results of the meta-analysis, and characteristics of the \n    research on the effect under study (van Assen et al., 2023). Helper functions \n    to apply the Correcting for Outcome Reporting Bias (CORB) method to correct \n    for outcome reporting bias in a meta-analysis (van Aert & Wicherts, 2023).  "
  },
  {
    "id": 18564,
    "package_name": "pvLRT",
    "title": "Likelihood Ratio Test-Based Approaches to Pharmacovigilance",
    "description": "A suite of likelihood ratio test based methods to use in pharmacovigilance. Contains various testing and post-processing functions.",
    "version": "0.5.1",
    "maintainer": "Saptarshi Chakraborty <chakra.saptarshi@gmail.com>",
    "author": "Saptarshi Chakraborty [aut, cre],\n  Marianthi Markatou [aut],\n  Anran Liu [ctb]",
    "url": "",
    "bug_reports": "https://github.com/c7rishi/pvLRT/issues",
    "repository": "https://cran.r-project.org/package=pvLRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pvLRT Likelihood Ratio Test-Based Approaches to Pharmacovigilance A suite of likelihood ratio test based methods to use in pharmacovigilance. Contains various testing and post-processing functions.  "
  },
  {
    "id": 18580,
    "package_name": "pwrAB",
    "title": "Power Analysis for AB Testing",
    "description": "Power analysis for AB testing. The calculations are based on the Welch's unequal variances t-test,\n  which is generally preferred over the Student's t-test when sample sizes and variances of the two groups are\n  unequal, which is frequently the case in AB testing. In such situations, the Student's t-test will give \n  biased results due to using the pooled standard deviation, unlike the Welch's t-test.",
    "version": "0.1.0",
    "maintainer": "William Cha <william.minseuk.cha@gmail.com>",
    "author": "William Cha [aut, cre]",
    "url": "http://github.com/williamcha/pwrAB",
    "bug_reports": "http://github.com/williamcha/pwrAB/issues",
    "repository": "https://cran.r-project.org/package=pwrAB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pwrAB Power Analysis for AB Testing Power analysis for AB testing. The calculations are based on the Welch's unequal variances t-test,\n  which is generally preferred over the Student's t-test when sample sizes and variances of the two groups are\n  unequal, which is frequently the case in AB testing. In such situations, the Student's t-test will give \n  biased results due to using the pooled standard deviation, unlike the Welch's t-test.  "
  },
  {
    "id": 18581,
    "package_name": "pwrFDR",
    "title": "FDR Power",
    "description": "Computing Average and TPX Power under various BHFDR type sequential\n    procedures. All of these procedures involve control of some summary of the\n    distribution of the FDP, e.g. the proportion of discoveries which are false\n    in a given experiment. The most widely known of these, the BH-FDR procedure,\n    controls the FDR which is the mean of the FDP. A lesser known procedure, due\n    to Lehmann and Romano, controls the FDX, or probability that the FDP exceeds\n    a user provided threshold. This is less conservative than FWE control\n    procedures but much more conservative than the BH-FDR proceudre. This\n    package and the references supporting it introduce a new procedure for\n    controlling the FDX which we call the BH-FDX procedure. This procedure\n    iteratively identifies, given alpha and lower threshold delta, an alpha*\n    less than alpha at which BH-FDR guarantees FDX control.  This uses\n    asymptotic approximation and is only slightly more conservative than the\n    BH-FDR procedure. Likewise, we can think of the power in multiple testing\n    experiments in terms of a summary of the distribution of the True Positive\n    Proportion (TPP), the portion of tests truly non-null distributed that are\n    called significant. The package will compute power, sample size or any other\n    missing parameter required for power defined as (i) the mean of the TPP\n    (average power) or (ii) the probability that the TPP exceeds a given value,\n    lambda, (TPX power) via asymptotic approximation. All supplied theoretical\n    results are also obtainable via simulation. The suggested approach is to\n    narrow in on a design via the theoretical approaches and then make final\n    adjustments/verify the results by simulation. The theoretical results are\n    described in Izmirlian, G (2020) Statistics and Probability letters,\n    \"<doi:10.1016/j.spl.2020.108713>\", and an applied paper describing the\n    methodology with a simulation study is in preparation.\n    See citation(\"pwrFDR\").",
    "version": "3.2.4",
    "maintainer": "Grant Izmirlian <izmirlig@mail.nih.gov>",
    "author": "Grant Izmirlian [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pwrFDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pwrFDR FDR Power Computing Average and TPX Power under various BHFDR type sequential\n    procedures. All of these procedures involve control of some summary of the\n    distribution of the FDP, e.g. the proportion of discoveries which are false\n    in a given experiment. The most widely known of these, the BH-FDR procedure,\n    controls the FDR which is the mean of the FDP. A lesser known procedure, due\n    to Lehmann and Romano, controls the FDX, or probability that the FDP exceeds\n    a user provided threshold. This is less conservative than FWE control\n    procedures but much more conservative than the BH-FDR proceudre. This\n    package and the references supporting it introduce a new procedure for\n    controlling the FDX which we call the BH-FDX procedure. This procedure\n    iteratively identifies, given alpha and lower threshold delta, an alpha*\n    less than alpha at which BH-FDR guarantees FDX control.  This uses\n    asymptotic approximation and is only slightly more conservative than the\n    BH-FDR procedure. Likewise, we can think of the power in multiple testing\n    experiments in terms of a summary of the distribution of the True Positive\n    Proportion (TPP), the portion of tests truly non-null distributed that are\n    called significant. The package will compute power, sample size or any other\n    missing parameter required for power defined as (i) the mean of the TPP\n    (average power) or (ii) the probability that the TPP exceeds a given value,\n    lambda, (TPX power) via asymptotic approximation. All supplied theoretical\n    results are also obtainable via simulation. The suggested approach is to\n    narrow in on a design via the theoretical approaches and then make final\n    adjustments/verify the results by simulation. The theoretical results are\n    described in Izmirlian, G (2020) Statistics and Probability letters,\n    \"<doi:10.1016/j.spl.2020.108713>\", and an applied paper describing the\n    methodology with a simulation study is in preparation.\n    See citation(\"pwrFDR\").  "
  },
  {
    "id": 18582,
    "package_name": "pwrRasch",
    "title": "Statistical Power Simulation for Testing the Rasch Model",
    "description": "Statistical power simulation for testing the Rasch Model based on a three-way analysis of variance design with mixed classification.",
    "version": "0.1-2",
    "maintainer": "Takuya Yanagida <takuya.yanagida@univie.ac.at>",
    "author": "Takuya Yanagida [cre, aut],\n  Jan Steinfeld [aut],\n  Thomas Kiefer [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pwrRasch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pwrRasch Statistical Power Simulation for Testing the Rasch Model Statistical power simulation for testing the Rasch Model based on a three-way analysis of variance design with mixed classification.  "
  },
  {
    "id": 18661,
    "package_name": "qqconf",
    "title": "Creates Simultaneous Testing Bands for QQ-Plots",
    "description": "Provides functionality for creating Quantile-Quantile (QQ) and Probability-Probability (PP) plots with simultaneous \n    testing bands to asses significance of sample deviation from a reference distribution <doi:10.18637/jss.v106.i10>.",
    "version": "1.3.2",
    "maintainer": "Eric Weine <ericweine15@gmail.com>",
    "author": "Eric Weine [aut, cre],\n  Mary Sara McPeek [aut],\n  Abney Mark [aut]",
    "url": "https://github.com/eweine/qqconf",
    "bug_reports": "https://github.com/eweine/qqconf/issues",
    "repository": "https://cran.r-project.org/package=qqconf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qqconf Creates Simultaneous Testing Bands for QQ-Plots Provides functionality for creating Quantile-Quantile (QQ) and Probability-Probability (PP) plots with simultaneous \n    testing bands to asses significance of sample deviation from a reference distribution <doi:10.18637/jss.v106.i10>.  "
  },
  {
    "id": 18664,
    "package_name": "qqtest",
    "title": "Self Calibrating Quantile-Quantile Plots for Visual Testing",
    "description": "Provides the function qqtest which incorporates uncertainty in its\n    qqplot display(s) so that the user might have a better sense of the\n    evidence against the specified distributional hypothesis.  qqtest draws a\n    quantile quantile plot for visually assessing whether the data come from a\n    test distribution that has been defined in one of many ways.  The vertical\n    axis plots the data quantiles, the horizontal those of a test distribution.\n    The default behaviour generates 1000 samples from the test distribution and\n    overlays the plot with shaded pointwise interval estimates for the ordered\n    quantiles from the test distribution.  A small number of independently\n    generated exemplar quantile plots can also be overlaid.  Both the interval\n    estimates and the exemplars provide different comparative information to\n    assess the evidence provided by the qqplot for or against the hypothesis\n    that the data come from the test distribution (default is normal or\n    gaussian).  Finally, a visual test of significance (a lineup plot) can also\n    be displayed to test the null hypothesis that the data come from the test\n    distribution.",
    "version": "1.2.0",
    "maintainer": "Wayne Oldford <rwoldford@uwaterloo.ca>",
    "author": "Wayne Oldford [aut, cre]",
    "url": "https://github.com/rwoldford/qqtest,\nhttps://rwoldford.github.io/qqtest/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=qqtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qqtest Self Calibrating Quantile-Quantile Plots for Visual Testing Provides the function qqtest which incorporates uncertainty in its\n    qqplot display(s) so that the user might have a better sense of the\n    evidence against the specified distributional hypothesis.  qqtest draws a\n    quantile quantile plot for visually assessing whether the data come from a\n    test distribution that has been defined in one of many ways.  The vertical\n    axis plots the data quantiles, the horizontal those of a test distribution.\n    The default behaviour generates 1000 samples from the test distribution and\n    overlays the plot with shaded pointwise interval estimates for the ordered\n    quantiles from the test distribution.  A small number of independently\n    generated exemplar quantile plots can also be overlaid.  Both the interval\n    estimates and the exemplars provide different comparative information to\n    assess the evidence provided by the qqplot for or against the hypothesis\n    that the data come from the test distribution (default is normal or\n    gaussian).  Finally, a visual test of significance (a lineup plot) can also\n    be displayed to test the null hypothesis that the data come from the test\n    distribution.  "
  },
  {
    "id": 18699,
    "package_name": "qtl2pleio",
    "title": "Testing Pleiotropy in Multiparental Populations",
    "description": "We implement an\n    adaptation of Jiang & Zeng's (1995) <https://www.genetics.org/content/140/3/1111> likelihood ratio test for testing\n    the null hypothesis of pleiotropy against the alternative hypothesis,\n    two separate quantitative trait loci. The test differs from that in Jiang & Zeng (1995) <https://www.genetics.org/content/140/3/1111> \n    and that in Tian et al. (2016) <doi:10.1534/genetics.115.183624> in\n    that our test accommodates multiparental populations.",
    "version": "1.4.3",
    "maintainer": "Frederick J Boehm <frederick.boehm@gmail.com>",
    "author": "Frederick J Boehm [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1644-5931>)",
    "url": "https://github.com/fboehm/qtl2pleio",
    "bug_reports": "https://github.com/fboehm/qtl2pleio/issues",
    "repository": "https://cran.r-project.org/package=qtl2pleio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "qtl2pleio Testing Pleiotropy in Multiparental Populations We implement an\n    adaptation of Jiang & Zeng's (1995) <https://www.genetics.org/content/140/3/1111> likelihood ratio test for testing\n    the null hypothesis of pleiotropy against the alternative hypothesis,\n    two separate quantitative trait loci. The test differs from that in Jiang & Zeng (1995) <https://www.genetics.org/content/140/3/1111> \n    and that in Tian et al. (2016) <doi:10.1534/genetics.115.183624> in\n    that our test accommodates multiparental populations.  "
  },
  {
    "id": 18731,
    "package_name": "quantilogram",
    "title": "Cross-Quantilogram",
    "description": "Estimation and inference methods for the cross-quantilogram.\n    The cross-quantilogram is a measure of nonlinear dependence between\n    two variables, based on either unconditional or conditional quantile\n    functions.  It can be considered an extension of the correlogram,\n    which is a correlation function over multiple lag periods that mainly\n    focuses on linear dependency.  One can use the cross-quantilogram to\n    detect the presence of directional predictability from one time series\n    to another.  This package provides a statistical inference method\n    based on the stationary bootstrap.  For detailed theoretical and\n    empirical explanations, see Linton and Whang (2007) for univariate\n    time series analysis and Han, Linton, Oka and Whang (2016) for\n    multivariate time series analysis.  The full references for these key\n    publications are as follows: (1) Linton, O., and Whang, Y. J. (2007).\n    The quantilogram: with an application to evaluating directional\n    predictability.  Journal of Econometrics, 141(1), 250-282\n    <doi:10.1016/j.jeconom.2007.01.004>; (2) Han, H., Linton, O., Oka, T.,\n    and Whang, Y. J. (2016).  The cross-quantilogram: measuring quantile\n    dependence and testing directional predictability between time series.\n    Journal of Econometrics, 193(1), 251-270\n    <doi:10.1016/j.jeconom.2016.03.001>.",
    "version": "3.1.1",
    "maintainer": "Tatsushi Oka <oka.econ@gmail.com>",
    "author": "Tatsushi Oka [aut, cre],\n  Heejon Han [ctb],\n  Oliver Linton [ctb],\n  Yoon-Jae Whang [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quantilogram",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quantilogram Cross-Quantilogram Estimation and inference methods for the cross-quantilogram.\n    The cross-quantilogram is a measure of nonlinear dependence between\n    two variables, based on either unconditional or conditional quantile\n    functions.  It can be considered an extension of the correlogram,\n    which is a correlation function over multiple lag periods that mainly\n    focuses on linear dependency.  One can use the cross-quantilogram to\n    detect the presence of directional predictability from one time series\n    to another.  This package provides a statistical inference method\n    based on the stationary bootstrap.  For detailed theoretical and\n    empirical explanations, see Linton and Whang (2007) for univariate\n    time series analysis and Han, Linton, Oka and Whang (2016) for\n    multivariate time series analysis.  The full references for these key\n    publications are as follows: (1) Linton, O., and Whang, Y. J. (2007).\n    The quantilogram: with an application to evaluating directional\n    predictability.  Journal of Econometrics, 141(1), 250-282\n    <doi:10.1016/j.jeconom.2007.01.004>; (2) Han, H., Linton, O., Oka, T.,\n    and Whang, Y. J. (2016).  The cross-quantilogram: measuring quantile\n    dependence and testing directional predictability between time series.\n    Journal of Econometrics, 193(1), 251-270\n    <doi:10.1016/j.jeconom.2016.03.001>.  "
  },
  {
    "id": 18743,
    "package_name": "quarks",
    "title": "Simple Methods for Calculating and Backtesting Value at Risk and\nExpected Shortfall",
    "description": "Enables the user to calculate Value at Risk (VaR)\n    and Expected Shortfall (ES) by means of various types of historical\n    simulation. Currently plain-, age-, volatility-weighted- and filtered\n    historical simulation are implemented in this package. Volatility weighting\n    can be carried out via an exponentially weighted moving average model\n    (EWMA) or other GARCH-type models. The performance can be assessed via\n    Traffic Light Test, Coverage Tests and Loss Functions. The methods of the\n    package are described in Gurrola-Perez, P. and Murphy, D. (2015)\n    <https://EconPapers.repec.org/RePEc:boe:boeewp:0525> as well as McNeil, J.,\n    Frey, R., and Embrechts, P. (2015) <https://ideas.repec.org/b/pup/pbooks/10496.html>.",
    "version": "1.1.5",
    "maintainer": "Sebastian Letmathe <sebastian.let@yahoo.com>",
    "author": "Sebastian Letmathe [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quarks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quarks Simple Methods for Calculating and Backtesting Value at Risk and\nExpected Shortfall Enables the user to calculate Value at Risk (VaR)\n    and Expected Shortfall (ES) by means of various types of historical\n    simulation. Currently plain-, age-, volatility-weighted- and filtered\n    historical simulation are implemented in this package. Volatility weighting\n    can be carried out via an exponentially weighted moving average model\n    (EWMA) or other GARCH-type models. The performance can be assessed via\n    Traffic Light Test, Coverage Tests and Loss Functions. The methods of the\n    package are described in Gurrola-Perez, P. and Murphy, D. (2015)\n    <https://EconPapers.repec.org/RePEc:boe:boeewp:0525> as well as McNeil, J.,\n    Frey, R., and Embrechts, P. (2015) <https://ideas.repec.org/b/pup/pbooks/10496.html>.  "
  },
  {
    "id": 18749,
    "package_name": "quasar",
    "title": "Valid Inference on Multiple Quantile Regressions",
    "description": "The approach is based on the closed testing procedure to control familywise error rate in a strong sense.\n  The local tests implemented are Wald-type and rank-score. \n  The method is described in De Santis, et al., (2025), <doi:10.48550/arXiv.2511.07999>.",
    "version": "0.1.0",
    "maintainer": "Angela Andreella <angela.andreella@unive.it>",
    "author": "Angela Andreella [aut, cre],\n  Anna Vesely [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quasar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quasar Valid Inference on Multiple Quantile Regressions The approach is based on the closed testing procedure to control familywise error rate in a strong sense.\n  The local tests implemented are Wald-type and rank-score. \n  The method is described in De Santis, et al., (2025), <doi:10.48550/arXiv.2511.07999>.  "
  },
  {
    "id": 18760,
    "package_name": "quickPlot",
    "title": "A System of Plotting Optimized for Speed and Modularity",
    "description": "A high-level plotting system, compatible with `ggplot2` objects, \n    maps from `sf`, `terra`, `raster`, `sp`. It is built primarily on the \n    'grid' package. The objective of the package is to provide a plotting system \n    that is built for speed and modularity. This is useful for quick visualizations \n    when testing code and for plotting multiple figures to the same device from \n    independent sources that may be independent of one another (i.e., different \n    function or modules the create the visualizations).",
    "version": "1.0.4",
    "maintainer": "Eliot J B McIntire <eliot.mcintire@canada.ca>",
    "author": "Eliot J B McIntire [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6914-8316>),\n  Alex M Chubaty [aut] (ORCID: <https://orcid.org/0000-0001-7146-8135>),\n  His Majesty the King in Right of Canada, as represented by the Minister\n    of Natural Resources Canada [cph]",
    "url": "https://quickplot.predictiveecology.org,\nhttps://github.com/PredictiveEcology/quickPlot",
    "bug_reports": "https://github.com/PredictiveEcology/quickPlot/issues",
    "repository": "https://cran.r-project.org/package=quickPlot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quickPlot A System of Plotting Optimized for Speed and Modularity A high-level plotting system, compatible with `ggplot2` objects, \n    maps from `sf`, `terra`, `raster`, `sp`. It is built primarily on the \n    'grid' package. The objective of the package is to provide a plotting system \n    that is built for speed and modularity. This is useful for quick visualizations \n    when testing code and for plotting multiple figures to the same device from \n    independent sources that may be independent of one another (i.e., different \n    function or modules the create the visualizations).  "
  },
  {
    "id": 18762,
    "package_name": "quickcheck",
    "title": "Property Based Testing",
    "description": "Property based testing, inspired by \n    the original 'QuickCheck'. This package builds on \n    the property based testing framework provided by \n    'hedgehog' and is designed to seamlessly integrate with \n    'testthat'.",
    "version": "0.1.3",
    "maintainer": "Andrew McNeil <andrew.richard.mcneil@gmail.com>",
    "author": "Andrew McNeil [aut, cre]",
    "url": "https://github.com/armcn/quickcheck,\nhttps://armcn.github.io/quickcheck/",
    "bug_reports": "https://github.com/armcn/quickcheck/issues",
    "repository": "https://cran.r-project.org/package=quickcheck",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quickcheck Property Based Testing Property based testing, inspired by \n    the original 'QuickCheck'. This package builds on \n    the property based testing framework provided by \n    'hedgehog' and is designed to seamlessly integrate with \n    'testthat'.  "
  },
  {
    "id": 18872,
    "package_name": "rNeighborGWAS",
    "title": "Testing Neighbor Effects in Marker-Based Regressions",
    "description": "To incorporate neighbor genotypic identity into genome-wide association studies, the package provides a set of functions for variation partitioning and association mapping. The theoretical background of the method is described in Sato et al. (2021) <doi:10.1038/s41437-020-00401-w>.",
    "version": "1.2.5",
    "maintainer": "Yasuhiro Sato <sato.yasuhiro.36c@kyoto-u.jp>",
    "author": "Yasuhiro Sato [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6466-723X>),\n  Eiji Yamamoto [aut],\n  Kentaro K. Shimizu [aut] (ORCID:\n    <https://orcid.org/0000-0002-6483-1781>),\n  Atsushi J. Nagano [aut] (ORCID:\n    <https://orcid.org/0000-0001-7891-5049>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rNeighborGWAS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rNeighborGWAS Testing Neighbor Effects in Marker-Based Regressions To incorporate neighbor genotypic identity into genome-wide association studies, the package provides a set of functions for variation partitioning and association mapping. The theoretical background of the method is described in Sato et al. (2021) <doi:10.1038/s41437-020-00401-w>.  "
  },
  {
    "id": 18874,
    "package_name": "rOCEAN",
    "title": "Two-Way Feature Set Testing for Multi-Omics",
    "description": "For any two way feature-set from a pair of pre-processed omics data, 3 different true discovery proportions (TDP), namely pairwise-TDP, column-TDP and row-TDP are calculated. Due to embedded closed testing procedure, the choice of feature-sets can be changed infinite times and even after seeing the data without any change in type I error rate. For more details refer to Ebrahimpoor et al., (2024) <doi:10.48550/arXiv.2410.19523>.",
    "version": "1.0",
    "maintainer": "Mitra Ebrahimpoor <mitra.ebrahimpoor@gmail.com>",
    "author": "Mitra Ebrahimpoor [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2299-876X>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rOCEAN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rOCEAN Two-Way Feature Set Testing for Multi-Omics For any two way feature-set from a pair of pre-processed omics data, 3 different true discovery proportions (TDP), namely pairwise-TDP, column-TDP and row-TDP are calculated. Due to embedded closed testing procedure, the choice of feature-sets can be changed infinite times and even after seeing the data without any change in type I error rate. For more details refer to Ebrahimpoor et al., (2024) <doi:10.48550/arXiv.2410.19523>.  "
  },
  {
    "id": 18892,
    "package_name": "rSEA",
    "title": "Simultaneous Enrichment Analysis",
    "description": "SEA performs simultaneous feature-set testing for (gen)omics data. It tests the unified null hypothesis and controls the family-wise error rate for all possible pathways. The unified null hypothesis is defined as: \"The proportion of true features in the set is less than or equal to a threshold.\" Family-wise error rate control is provided through use of closed testing with Simes test. There are some practical functions to play around with the pathways of interest.",
    "version": "2.1.2",
    "maintainer": "Mitra Ebrahimpoor<mitra.ebrahimpoor@gmail.com>",
    "author": "Mitra Ebrahimpoor",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rSEA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rSEA Simultaneous Enrichment Analysis SEA performs simultaneous feature-set testing for (gen)omics data. It tests the unified null hypothesis and controls the family-wise error rate for all possible pathways. The unified null hypothesis is defined as: \"The proportion of true features in the set is less than or equal to a threshold.\" Family-wise error rate control is provided through use of closed testing with Simes test. There are some practical functions to play around with the pathways of interest.  "
  },
  {
    "id": 18904,
    "package_name": "rTRNG",
    "title": "Advanced and Parallel Random Number Generation via 'TRNG'",
    "description": "Embeds sources and headers from Tina's Random\n    Number Generator ('TRNG') C++ library. Exposes some functionality for\n    easier access, testing and benchmarking into R. Provides examples of\n    how to use parallel RNG with 'RcppParallel'. The methods and\n    techniques behind 'TRNG' are illustrated in the package vignettes and\n    examples. Full documentation is available in Bauke (2021)\n    <https://github.com/rabauke/trng4/blob/v4.23.1/doc/trng.pdf>.",
    "version": "4.23.1-4",
    "maintainer": "Riccardo Porreca <riccardo.porreca@mirai-solutions.com>",
    "author": "Riccardo Porreca [aut, cre],\n  Roland Schmid [aut],\n  Mirai Solutions GmbH [cph],\n  Heiko Bauke [ctb, cph] (TRNG sources and headers)",
    "url": "https://github.com/miraisolutions/rTRNG#readme,\nhttps://mirai-solutions.ch",
    "bug_reports": "https://github.com/miraisolutions/rTRNG/issues",
    "repository": "https://cran.r-project.org/package=rTRNG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rTRNG Advanced and Parallel Random Number Generation via 'TRNG' Embeds sources and headers from Tina's Random\n    Number Generator ('TRNG') C++ library. Exposes some functionality for\n    easier access, testing and benchmarking into R. Provides examples of\n    how to use parallel RNG with 'RcppParallel'. The methods and\n    techniques behind 'TRNG' are illustrated in the package vignettes and\n    examples. Full documentation is available in Bauke (2021)\n    <https://github.com/rabauke/trng4/blob/v4.23.1/doc/trng.pdf>.  "
  },
  {
    "id": 18922,
    "package_name": "radiant.basics",
    "title": "Basics Menu for Radiant: Business Analytics using R and Shiny",
    "description": "The Radiant Basics menu includes interfaces for probability \n    calculation, central limit theorem simulation, comparing means and proportions, \n    goodness-of-fit testing, cross-tabs, and correlation. The application extends \n    the functionality in 'radiant.data'.",
    "version": "1.6.6",
    "maintainer": "Vincent Nijs <radiant@rady.ucsd.edu>",
    "author": "Vincent Nijs [aut, cre]",
    "url": "https://github.com/radiant-rstats/radiant.basics/,\nhttps://radiant-rstats.github.io/radiant.basics/,\nhttps://radiant-rstats.github.io/docs/",
    "bug_reports": "https://github.com/radiant-rstats/radiant.basics/issues/",
    "repository": "https://cran.r-project.org/package=radiant.basics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "radiant.basics Basics Menu for Radiant: Business Analytics using R and Shiny The Radiant Basics menu includes interfaces for probability \n    calculation, central limit theorem simulation, comparing means and proportions, \n    goodness-of-fit testing, cross-tabs, and correlation. The application extends \n    the functionality in 'radiant.data'.  "
  },
  {
    "id": 18965,
    "package_name": "randomLCA",
    "title": "Random Effects Latent Class Analysis",
    "description": "Fits standard and random effects latent class models. The single level random effects model is described in Qu et al <doi:10.2307/2533043> and the two level random effects model in Beath and Heller <doi:10.1177/1471082X0800900302>. Examples are given for their use in diagnostic testing.",
    "version": "1.1-4",
    "maintainer": "Ken Beath <ken@kjbeath.id.au>",
    "author": "Ken Beath [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=randomLCA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "randomLCA Random Effects Latent Class Analysis Fits standard and random effects latent class models. The single level random effects model is described in Qu et al <doi:10.2307/2533043> and the two level random effects model in Beath and Heller <doi:10.1177/1471082X0800900302>. Examples are given for their use in diagnostic testing.  "
  },
  {
    "id": 18975,
    "package_name": "randtests",
    "title": "Testing Randomness in R",
    "description": "Provides several non parametric randomness tests for numeric sequences.",
    "version": "1.0.2",
    "maintainer": "Frederico Caeiro <fac@fct.unl.pt>",
    "author": "Frederico Caeiro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8628-7281>),\n  Ayana Mateus [aut] (ORCID: <https://orcid.org/0000-0002-5630-3321>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=randtests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "randtests Testing Randomness in R Provides several non parametric randomness tests for numeric sequences.  "
  },
  {
    "id": 19015,
    "package_name": "rashnu",
    "title": "Balanced Sample Size and Power Calculation Tools",
    "description": "Implements sample size and power calculation methods with a focus on balance and fairness in study design, inspired by the Zoroastrian deity Rashnu, the judge who weighs truth. Supports survival analysis and various hypothesis testing frameworks.",
    "version": "0.1.2",
    "maintainer": "Gyeom Hwangbo <hbgyeom@gmail.com>",
    "author": "Sungho Choi [aut],\n  Gyeom Hwangbo [ctb, cre],\n  Zarathu [cph, fnd]",
    "url": "https://zarathucorp.github.io/rashnu/,\nhttps://github.com/zarathucorp/rashnu",
    "bug_reports": "https://github.com/zarathucorp/rashnu/issues",
    "repository": "https://cran.r-project.org/package=rashnu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rashnu Balanced Sample Size and Power Calculation Tools Implements sample size and power calculation methods with a focus on balance and fairness in study design, inspired by the Zoroastrian deity Rashnu, the judge who weighs truth. Supports survival analysis and various hypothesis testing frameworks.  "
  },
  {
    "id": 19032,
    "package_name": "raters",
    "title": "A Modification of Fleiss' Kappa in Case of Nominal and Ordinal\nVariables",
    "description": "The kappa statistic implemented by Fleiss is a very popular index for assessing the reliability of agreement among multiple observers. It is used both in the psychological and in the psychiatric field. Other fields of application are typically medicine, biology and engineering. Unfortunately,the kappa statistic may behave inconsistently in case of strong agreement between raters, since this index assumes lower values than it would have been expected. We propose a modification kappa implemented by Fleiss in case of nominal and ordinal variables. Monte Carlo simulations are used both to testing statistical hypotheses and to calculating percentile bootstrap confidence intervals based on proposed statistic in case of nominal and ordinal data.",
    "version": "2.1.1",
    "maintainer": "Daniele Giardiello <daniele.giardiello1@gmail.com>",
    "author": "Daniele Giardiello [cre],\n  Piero Quatto [aut],\n  Enrico Ripamonti [aut],\n  Stefano Vigliani [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=raters",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "raters A Modification of Fleiss' Kappa in Case of Nominal and Ordinal\nVariables The kappa statistic implemented by Fleiss is a very popular index for assessing the reliability of agreement among multiple observers. It is used both in the psychological and in the psychiatric field. Other fields of application are typically medicine, biology and engineering. Unfortunately,the kappa statistic may behave inconsistently in case of strong agreement between raters, since this index assumes lower values than it would have been expected. We propose a modification kappa implemented by Fleiss in case of nominal and ordinal variables. Monte Carlo simulations are used both to testing statistical hypotheses and to calculating percentile bootstrap confidence intervals based on proposed statistic in case of nominal and ordinal data.  "
  },
  {
    "id": 19096,
    "package_name": "rcens",
    "title": "Generate Sample Censoring",
    "description": "Provides functions to generate censored samples of type I, II and III, from any random sample generator. It also supplies the option to create left and right censorship. Along with this, the generation of samples with interval censoring is in the  testing phase, with two options of fixed length intervals and random lengths.",
    "version": "0.1.1",
    "maintainer": "Daniel Saavedra <dlsaavedra@uc.cl>",
    "author": "Daniel Saavedra [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5084-6843>),\n  Pedro L Ramos [aut]",
    "url": "https://github.com/dlsaavedra/rcens",
    "bug_reports": "https://github.com/dlsaavedra/rcens/issues",
    "repository": "https://cran.r-project.org/package=rcens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcens Generate Sample Censoring Provides functions to generate censored samples of type I, II and III, from any random sample generator. It also supplies the option to create left and right censorship. Along with this, the generation of samples with interval censoring is in the  testing phase, with two options of fixed length intervals and random lengths.  "
  },
  {
    "id": 19111,
    "package_name": "rcompendium",
    "title": "Create a Package or Research Compendium Structure",
    "description": "Makes easier the creation of R package or research compendium \n    (i.e. a predefined files/folders structure) so that users can focus on the \n    code/analysis instead of wasting time organizing files. A full \n    ready-to-work structure is set up with some additional features: version \n    control, remote repository creation, CI/CD configuration (check package \n    integrity under several OS, test code with 'testthat', and build and deploy \n    website using 'pkgdown'). This package heavily relies on the R packages \n    'devtools' and 'usethis' and follows recommendations made by Wickham H. \n    (2015) <ISBN:9781491910597> and Marwick B. et al. (2018) \n    <doi:10.7287/peerj.preprints.3192v2>.",
    "version": "1.4",
    "maintainer": "Nicolas Casajus <nicolas.casajus@fondationbiodiversite.fr>",
    "author": "Nicolas Casajus [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5537-5294>)",
    "url": "https://github.com/FRBCesab/rcompendium,\nhttps://frbcesab.github.io/rcompendium/",
    "bug_reports": "https://github.com/FRBCesab/rcompendium/issues",
    "repository": "https://cran.r-project.org/package=rcompendium",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rcompendium Create a Package or Research Compendium Structure Makes easier the creation of R package or research compendium \n    (i.e. a predefined files/folders structure) so that users can focus on the \n    code/analysis instead of wasting time organizing files. A full \n    ready-to-work structure is set up with some additional features: version \n    control, remote repository creation, CI/CD configuration (check package \n    integrity under several OS, test code with 'testthat', and build and deploy \n    website using 'pkgdown'). This package heavily relies on the R packages \n    'devtools' and 'usethis' and follows recommendations made by Wickham H. \n    (2015) <ISBN:9781491910597> and Marwick B. et al. (2018) \n    <doi:10.7287/peerj.preprints.3192v2>.  "
  },
  {
    "id": 19132,
    "package_name": "rdcor",
    "title": "Rank Distance Correlation Coefficient",
    "description": "The rank distance correlation <doi:10.1080/01621459.2020.1782223> is computed. Included also is a function to perform permutation based testing.  ",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "author": "Michail Tsagris [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rdcor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdcor Rank Distance Correlation Coefficient The rank distance correlation <doi:10.1080/01621459.2020.1782223> is computed. Included also is a function to perform permutation based testing.    "
  },
  {
    "id": 19134,
    "package_name": "rddensity",
    "title": "Manipulation Testing Based on Density Discontinuity",
    "description": "Density discontinuity testing (a.k.a. manipulation testing) is commonly employed in regression discontinuity designs and other program evaluation settings to detect perfect self-selection (manipulation) around a cutoff where treatment/policy assignment changes. This package implements manipulation testing procedures using the local polynomial density estimators: rddensity() to construct test statistics and p-values given a prespecified cutoff, rdbwdensity() to perform data-driven bandwidth selection, and rdplotdensity() to construct density plots.",
    "version": "2.6",
    "maintainer": "Xinwei Ma <x1ma@ucsd.edu>",
    "author": "Matias D. Cattaneo [aut],\n  Michael Jansson [aut],\n  Xinwei Ma [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rddensity",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rddensity Manipulation Testing Based on Density Discontinuity Density discontinuity testing (a.k.a. manipulation testing) is commonly employed in regression discontinuity designs and other program evaluation settings to detect perfect self-selection (manipulation) around a cutoff where treatment/policy assignment changes. This package implements manipulation testing procedures using the local polynomial density estimators: rddensity() to construct test statistics and p-values given a prespecified cutoff, rdbwdensity() to perform data-driven bandwidth selection, and rdplotdensity() to construct density plots.  "
  },
  {
    "id": 19136,
    "package_name": "rddtools",
    "title": "Toolbox for Regression Discontinuity Design ('RDD')",
    "description": "Set of functions for Regression Discontinuity Design ('RDD'), for\n    data visualisation, estimation and testing.",
    "version": "2.0.2",
    "maintainer": "Matthieu Stigler <Matthieu.Stigler@gmail.com>",
    "author": "Matthieu Stigler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6802-4290>),\n  Bastiaan Quast [aut] (ORCID: <https://orcid.org/0000-0002-2951-3577>)",
    "url": "https://github.com/bquast/rddtools",
    "bug_reports": "https://github.com/bquast/rddtools/issues",
    "repository": "https://cran.r-project.org/package=rddtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rddtools Toolbox for Regression Discontinuity Design ('RDD') Set of functions for Regression Discontinuity Design ('RDD'), for\n    data visualisation, estimation and testing.  "
  },
  {
    "id": 19145,
    "package_name": "rdlocrand",
    "title": "Local Randomization Methods for RD Designs",
    "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. Under the local randomization approach, RD designs can be interpreted as randomized experiments inside a window around the cutoff. This package provides tools to perform randomization inference for RD designs under local randomization: rdrandinf() to perform hypothesis testing using randomization inference, rdwinselect() to select a window around the cutoff in which randomization is likely to hold, rdsensitivity() to assess the sensitivity of the results to different window lengths and null hypotheses and rdrbounds() to construct Rosenbaum bounds for sensitivity to unobserved confounders. See Cattaneo, Titiunik and Vazquez-Bare (2016) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2016_Stata.pdf> for further methodological details.",
    "version": "1.1",
    "maintainer": "Gonzalo Vazquez-Bare <gvazquez@econ.ucsb.edu>",
    "author": "Matias D. Cattaneo [aut],\n  Rocio Titiunik [aut],\n  Gonzalo Vazquez-Bare [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rdlocrand",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rdlocrand Local Randomization Methods for RD Designs The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. Under the local randomization approach, RD designs can be interpreted as randomized experiments inside a window around the cutoff. This package provides tools to perform randomization inference for RD designs under local randomization: rdrandinf() to perform hypothesis testing using randomization inference, rdwinselect() to select a window around the cutoff in which randomization is likely to hold, rdsensitivity() to assess the sensitivity of the results to different window lengths and null hypotheses and rdrbounds() to construct Rosenbaum bounds for sensitivity to unobserved confounders. See Cattaneo, Titiunik and Vazquez-Bare (2016) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2016_Stata.pdf> for further methodological details.  "
  },
  {
    "id": 19174,
    "package_name": "readMLData",
    "title": "Reading Machine Learning Benchmark Data Sets in Different\nFormats",
    "description": "Functions for reading data sets in different formats\n  for testing machine learning tools are provided. This allows to run\n  a loop over several data sets in their original form, for example\n  if they are downloaded from UCI Machine Learning Repository.\n  The data are not part of the package and have to be downloaded\n  separately.",
    "version": "0.9-7",
    "maintainer": "Petr Savicky <savicky@cs.cas.cz>",
    "author": "Petr Savicky",
    "url": "http://www.cs.cas.cz/~savicky/readMLData",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=readMLData",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "readMLData Reading Machine Learning Benchmark Data Sets in Different\nFormats Functions for reading data sets in different formats\n  for testing machine learning tools are provided. This allows to run\n  a loop over several data sets in their original form, for example\n  if they are downloaded from UCI Machine Learning Repository.\n  The data are not part of the package and have to be downloaded\n  separately.  "
  },
  {
    "id": 19200,
    "package_name": "realtest",
    "title": "Where Expectations Meet Reality: Realistic Unit Testing",
    "description": "\n    A framework for unit testing for realistic minimalists,\n    where we distinguish between expected, acceptable, current, fallback,\n    ideal, or regressive behaviour. It can also be used for monitoring\n    third-party software projects for changes.",
    "version": "0.2.3",
    "maintainer": "Marek Gagolewski <marek@gagolewski.com>",
    "author": "Marek Gagolewski [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-0637-6028>)",
    "url": "https://realtest.gagolewski.com,\nhttps://github.com/gagolews/realtest",
    "bug_reports": "https://github.com/gagolews/realtest/issues",
    "repository": "https://cran.r-project.org/package=realtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "realtest Where Expectations Meet Reality: Realistic Unit Testing \n    A framework for unit testing for realistic minimalists,\n    where we distinguish between expected, acceptable, current, fallback,\n    ideal, or regressive behaviour. It can also be used for monitoring\n    third-party software projects for changes.  "
  },
  {
    "id": 19213,
    "package_name": "recapr",
    "title": "Two Event Mark-Recapture Experiment",
    "description": "Tools are provided for estimating, testing, and simulating\n    abundance in a two-event (Petersen) mark-recapture experiment. Functions are\n    given to calculate the Petersen, Chapman, and Bailey estimators and associated\n    variances. However, the principal utility is a set of functions to simulate\n    random draws from these estimators, and use these to conduct hypothesis\n    tests and power calculations. Additionally, a set of functions are provided\n    for generating confidence intervals via bootstrapping. Functions are also\n    provided to test abundance estimator consistency under complete or partial\n    stratification, and to calculate stratified or partially stratified estimators.\n    Functions are also provided to calculate recommended sample sizes.\n    Referenced methods can be found in Arnason et al. (1996) <ISSN:0706-6457>, \n    Bailey (1951) <DOI:10.2307/2332575>, \n    Bailey (1952) <DOI:10.2307/1913>, \n    Chapman (1951) NAID:20001644490, \n    Cohen (1988) ISBN:0-12-179060-6, \n    Darroch (1961) <DOI:10.2307/2332748>, and\n    Robson and Regier (1964) <ISSN:1548-8659>.",
    "version": "0.4.4",
    "maintainer": "Matt Tyers <matttyersstat@gmail.com>",
    "author": "Matt Tyers [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=recapr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "recapr Two Event Mark-Recapture Experiment Tools are provided for estimating, testing, and simulating\n    abundance in a two-event (Petersen) mark-recapture experiment. Functions are\n    given to calculate the Petersen, Chapman, and Bailey estimators and associated\n    variances. However, the principal utility is a set of functions to simulate\n    random draws from these estimators, and use these to conduct hypothesis\n    tests and power calculations. Additionally, a set of functions are provided\n    for generating confidence intervals via bootstrapping. Functions are also\n    provided to test abundance estimator consistency under complete or partial\n    stratification, and to calculate stratified or partially stratified estimators.\n    Functions are also provided to calculate recommended sample sizes.\n    Referenced methods can be found in Arnason et al. (1996) <ISSN:0706-6457>, \n    Bailey (1951) <DOI:10.2307/2332575>, \n    Bailey (1952) <DOI:10.2307/1913>, \n    Chapman (1951) NAID:20001644490, \n    Cohen (1988) ISBN:0-12-179060-6, \n    Darroch (1961) <DOI:10.2307/2332748>, and\n    Robson and Regier (1964) <ISSN:1548-8659>.  "
  },
  {
    "id": 19230,
    "package_name": "recommenderlab",
    "title": "Lab for Developing and Testing Recommender Algorithms",
    "description": "Provides a research infrastructure to develop and evaluate\n    collaborative filtering recommender algorithms. This includes a sparse \n    representation for user-item matrices, many popular algorithms, top-N recommendations,\n    and cross-validation. Hahsler (2022) <doi:10.48550/arXiv.2205.12371>.",
    "version": "1.0.7",
    "maintainer": "Michael Hahsler <mhahsler@lyle.smu.edu>",
    "author": "Michael Hahsler [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2716-1405>),\n  Bregt Vereet [ctb]",
    "url": "https://github.com/mhahsler/recommenderlab",
    "bug_reports": "https://github.com/mhahsler/recommenderlab/issues",
    "repository": "https://cran.r-project.org/package=recommenderlab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "recommenderlab Lab for Developing and Testing Recommender Algorithms Provides a research infrastructure to develop and evaluate\n    collaborative filtering recommender algorithms. This includes a sparse \n    representation for user-item matrices, many popular algorithms, top-N recommendations,\n    and cross-validation. Hahsler (2022) <doi:10.48550/arXiv.2205.12371>.  "
  },
  {
    "id": 19298,
    "package_name": "regrrr",
    "title": "Toolkit for Compiling, (Post-Hoc) Testing, and Plotting\nRegression Results",
    "description": "Compiling regression results into a publishable format, conducting post-hoc hypothesis testing, and plotting moderating effects (the effect of X on Y becomes stronger/weaker as Z increases).",
    "version": "0.1.3",
    "maintainer": "Rui K. Yang <rkzyang@gmail.com>",
    "author": "Rui K. Yang [aut, cre],\n  Luyao Peng [aut]",
    "url": "",
    "bug_reports": "https://github.com/RkzYang/regrrr/issues",
    "repository": "https://cran.r-project.org/package=regrrr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "regrrr Toolkit for Compiling, (Post-Hoc) Testing, and Plotting\nRegression Results Compiling regression results into a publishable format, conducting post-hoc hypothesis testing, and plotting moderating effects (the effect of X on Y becomes stronger/weaker as Z increases).  "
  },
  {
    "id": 19308,
    "package_name": "relMix",
    "title": "Relationship Inference for DNA Mixtures",
    "description": "\n  Analysis of DNA mixtures involving relatives by computation of likelihood ratios that account for dropout and drop-in, mutations, silent alleles and population \n  substructure. This is useful in kinship cases, like non-invasive prenatal paternity testing, where deductions about individuals' relationships rely on DNA mixtures, \n  and in criminal cases where the contributors to a mixed DNA stain may be related. Relationships are represented \n  by pedigrees and can include kinship between more than two individuals. The main function is relMix() and its graphical user interface relMixGUI(). \n  The implementation and method is described in Dorum et al. (2017) <doi:10.1007/s00414-016-1526-x>, Hernandis et al. (2019)\n  <doi:10.1016/j.fsigss.2019.09.085> and Kaur et al. (2016) <doi:10.1007/s00414-015-1276-1>.",
    "version": "1.4.1",
    "maintainer": "Guro Dorum <guro.dorum@gmail.com>",
    "author": "Guro Dorum [aut, cre],\n  Elias Hernandis [aut],\n  Navreet Kaur [ctb],\n  Thore Egeland [ctb],\n  Magnus Dehli Vigeland [ctb]",
    "url": "https://gdorum.github.io/relMix/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=relMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "relMix Relationship Inference for DNA Mixtures \n  Analysis of DNA mixtures involving relatives by computation of likelihood ratios that account for dropout and drop-in, mutations, silent alleles and population \n  substructure. This is useful in kinship cases, like non-invasive prenatal paternity testing, where deductions about individuals' relationships rely on DNA mixtures, \n  and in criminal cases where the contributors to a mixed DNA stain may be related. Relationships are represented \n  by pedigrees and can include kinship between more than two individuals. The main function is relMix() and its graphical user interface relMixGUI(). \n  The implementation and method is described in Dorum et al. (2017) <doi:10.1007/s00414-016-1526-x>, Hernandis et al. (2019)\n  <doi:10.1016/j.fsigss.2019.09.085> and Kaur et al. (2016) <doi:10.1007/s00414-015-1276-1>.  "
  },
  {
    "id": 19409,
    "package_name": "restriktor",
    "title": "Restricted Statistical Estimation and Inference for Linear\nModels",
    "description": "Allow for easy-to-use testing or evaluating of linear equality and inequality restrictions about parameters and effects in (generalized) linear statistical models.",
    "version": "0.6-10",
    "maintainer": "Leonard Vanbrabant <info@restriktor.org>",
    "author": "Leonard Vanbrabant [aut, cre],\n  Rebecca Kuiper [aut],\n  Yves Rosseel [ctb],\n  Aleksandra Dacko [ctb]",
    "url": "https://restriktor.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=restriktor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "restriktor Restricted Statistical Estimation and Inference for Linear\nModels Allow for easy-to-use testing or evaluating of linear equality and inequality restrictions about parameters and effects in (generalized) linear statistical models.  "
  },
  {
    "id": 19424,
    "package_name": "reval",
    "title": "Argument Table Generation for Sensitivity Analysis",
    "description": "Simplified scenario testing and sensitivity analysis,\n    redesigned to use packages 'future' and 'furrr'. Provides\n    functions for generating function argument sets using\n    one-factor-at-a-time (OFAT) and (sampled) permutations.",
    "version": "3.1-0",
    "maintainer": "Michael C Koohafkan <michael.koohafkan@gmail.com>",
    "author": "Michael C Koohafkan [aut, cre]",
    "url": "https://github.com/mkoohafkan/reval",
    "bug_reports": "https://github.com/mkoohafkan/reval/issues",
    "repository": "https://cran.r-project.org/package=reval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reval Argument Table Generation for Sensitivity Analysis Simplified scenario testing and sensitivity analysis,\n    redesigned to use packages 'future' and 'furrr'. Provides\n    functions for generating function argument sets using\n    one-factor-at-a-time (OFAT) and (sampled) permutations.  "
  },
  {
    "id": 19454,
    "package_name": "rfriend",
    "title": "Provides Batch Functions and Visualisation for Basic Statistical\nProcedures",
    "description": "Designed to streamline data analysis and statistical testing, reducing the length of R \n    scripts while generating well-formatted outputs in 'pdf', 'Microsoft Word', and 'Microsoft Excel' \n    formats. In essence, the package contains functions which are sophisticated wrappers around \n    existing R functions that are called by using 'f_' (user f_riendly) prefix followed by the normal \n    function name. This first version of the 'rfriend' package focuses  primarily on data exploration, \n    including tools for creating summary tables, f_summary(), performing data transformations, \n    f_boxcox() in part based on 'MASS/boxcox' and 'rcompanion', and f_bestNormalize() \n    which wraps and extends functionality from the 'bestNormalize' package. Furthermore, 'rfriend' \n    can automatically (or on request) generate visualizations such as boxplots, f_boxplot(), \n    QQ-plots, f_qqnorm(), histograms f_hist(), and density plots. Additionally, the package includes \n    four statistical test functions: f_aov(), f_kruskal_test(), f_glm(), f_chisq_test for sequential \n    testing and visualisation of the 'stats' functions: aov(), kruskal.test(), glm() and chisq.test. \n    These functions support testing multiple response variables and predictors, while also handling \n    assumption checks, data transformations, and post hoc tests. Post hoc results are automatically \n    summarized in a table using the compact letter display (cld) format for easy interpretation. The \n    package also provides a function to do model comparison, f_model_comparison(), and several \n    utility functions to simplify common R tasks. For example, f_clear() clears the workspace and \n    restarts R with a single command; f_setwd() sets the working directory to match the directory \n    of the current script; f_theme() quickly changes 'RStudio' themes; and f_factors() converts \n    multiple columns of a data frame to factors, and much more. If you encounter any issues or have \n    feature requests, please feel free to contact me via email.",
    "version": "2.0.0",
    "maintainer": "Sander H. van Delden <plantmind@proton.me>",
    "author": "Sander H. van Delden [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rfriend",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rfriend Provides Batch Functions and Visualisation for Basic Statistical\nProcedures Designed to streamline data analysis and statistical testing, reducing the length of R \n    scripts while generating well-formatted outputs in 'pdf', 'Microsoft Word', and 'Microsoft Excel' \n    formats. In essence, the package contains functions which are sophisticated wrappers around \n    existing R functions that are called by using 'f_' (user f_riendly) prefix followed by the normal \n    function name. This first version of the 'rfriend' package focuses  primarily on data exploration, \n    including tools for creating summary tables, f_summary(), performing data transformations, \n    f_boxcox() in part based on 'MASS/boxcox' and 'rcompanion', and f_bestNormalize() \n    which wraps and extends functionality from the 'bestNormalize' package. Furthermore, 'rfriend' \n    can automatically (or on request) generate visualizations such as boxplots, f_boxplot(), \n    QQ-plots, f_qqnorm(), histograms f_hist(), and density plots. Additionally, the package includes \n    four statistical test functions: f_aov(), f_kruskal_test(), f_glm(), f_chisq_test for sequential \n    testing and visualisation of the 'stats' functions: aov(), kruskal.test(), glm() and chisq.test. \n    These functions support testing multiple response variables and predictors, while also handling \n    assumption checks, data transformations, and post hoc tests. Post hoc results are automatically \n    summarized in a table using the compact letter display (cld) format for easy interpretation. The \n    package also provides a function to do model comparison, f_model_comparison(), and several \n    utility functions to simplify common R tasks. For example, f_clear() clears the workspace and \n    restarts R with a single command; f_setwd() sets the working directory to match the directory \n    of the current script; f_theme() quickly changes 'RStudio' themes; and f_factors() converts \n    multiple columns of a data frame to factors, and much more. If you encounter any issues or have \n    feature requests, please feel free to contact me via email.  "
  },
  {
    "id": 19455,
    "package_name": "rfvimptest",
    "title": "Sequential Permutation Testing of Random Forest Variable\nImportance Measures",
    "description": "Sequential permutation testing for statistical\n  significance of predictors in random forests and other prediction methods. \n  The main function of the package is rfvimptest(), which allows to test for \n  the   statistical significance of predictors in random forests using \n  different (sequential) permutation test strategies [1]. The advantage \n  of sequential over conventional permutation tests is that they\n  are computationally considerably less intensive, as the sequential\n  procedure is stopped as soon as there is sufficient evidence\n  for either the null or the alternative hypothesis.\n  Reference:\n  [1] Hapfelmeier, A., Hornung, R. & Haller, B. (2023) Efficient permutation\n\t  testing of variable importance measures by the example of random forests.\n\t  Computational Statistics & Data Analysis 181:107689, <doi:10.1016/j.csda.2022.107689>.",
    "version": "0.1.4",
    "maintainer": "Roman Hornung <hornung@ibe.med.uni-muenchen.de>",
    "author": "Alexander Hapfelmeier [aut],\n  Roman Hornung [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rfvimptest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rfvimptest Sequential Permutation Testing of Random Forest Variable\nImportance Measures Sequential permutation testing for statistical\n  significance of predictors in random forests and other prediction methods. \n  The main function of the package is rfvimptest(), which allows to test for \n  the   statistical significance of predictors in random forests using \n  different (sequential) permutation test strategies [1]. The advantage \n  of sequential over conventional permutation tests is that they\n  are computationally considerably less intensive, as the sequential\n  procedure is stopped as soon as there is sufficient evidence\n  for either the null or the alternative hypothesis.\n  Reference:\n  [1] Hapfelmeier, A., Hornung, R. & Haller, B. (2023) Efficient permutation\n\t  testing of variable importance measures by the example of random forests.\n\t  Computational Statistics & Data Analysis 181:107689, <doi:10.1016/j.csda.2022.107689>.  "
  },
  {
    "id": 19529,
    "package_name": "risk.assessr",
    "title": "Assessing Package Risk Metrics",
    "description": "A reliable and validated tool that captures detailed risk metrics \n    such as R 'CMD' check, test coverage, traceability matrix, documentation, dependencies, \n    reverse dependencies, suggested dependency analysis, repository data, \n    and enhanced reporting for R packages that are local or stored \n    on remote repositories such as GitHub, CRAN, and Bioconductor.",
    "version": "3.0.1",
    "maintainer": "Edward Gillian <edward.gillian-ext@sanofi.com>",
    "author": "Edward Gillian [cre, aut] (ORCID:\n    <https://orcid.org/0000-0003-2732-5107>),\n  Hugo Bottois [aut] (ORCID: <https://orcid.org/0000-0003-4674-0875>),\n  Paulin Charliquart [aut],\n  Andre Couturier [aut],\n  Sanofi [cph, fnd]",
    "url": "https://sanofi-public.github.io/risk.assessr/",
    "bug_reports": "https://github.com/Sanofi-Public/risk.assessr/issues",
    "repository": "https://cran.r-project.org/package=risk.assessr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "risk.assessr Assessing Package Risk Metrics A reliable and validated tool that captures detailed risk metrics \n    such as R 'CMD' check, test coverage, traceability matrix, documentation, dependencies, \n    reverse dependencies, suggested dependency analysis, repository data, \n    and enhanced reporting for R packages that are local or stored \n    on remote repositories such as GitHub, CRAN, and Bioconductor.  "
  },
  {
    "id": 19620,
    "package_name": "rmpw",
    "title": "Causal Mediation Analysis Using Weighting Approach",
    "description": "We implement causal mediation analysis using the methods proposed by Hong (2010) and Hong, Deutsch & Hill (2015) <doi:10.3102/1076998615583902>. It allows the estimation and hypothesis testing of causal mediation effects through ratio of mediator probability weights (RMPW). This strategy conveniently relaxes the assumption of no treatment-by-mediator interaction while greatly simplifying the outcome model specification without invoking strong distributional assumptions. We also implement a sensitivity analysis by extending the RMPW method to assess potential bias in the presence of omitted pretreatment or posttreatment covariates. The sensitivity analysis strategy was proposed by Hong, Qin, and Yang (2018) <doi:10.3102/1076998617749561>.",
    "version": "0.0.6",
    "maintainer": "Xu Qin <xuqin@uchicago.edu>",
    "author": "Xu Qin [aut, cre],\n  Guanglei Hong [aut],\n  Fan Yang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rmpw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rmpw Causal Mediation Analysis Using Weighting Approach We implement causal mediation analysis using the methods proposed by Hong (2010) and Hong, Deutsch & Hill (2015) <doi:10.3102/1076998615583902>. It allows the estimation and hypothesis testing of causal mediation effects through ratio of mediator probability weights (RMPW). This strategy conveniently relaxes the assumption of no treatment-by-mediator interaction while greatly simplifying the outcome model specification without invoking strong distributional assumptions. We also implement a sensitivity analysis by extending the RMPW method to assess potential bias in the presence of omitted pretreatment or posttreatment covariates. The sensitivity analysis strategy was proposed by Hong, Qin, and Yang (2018) <doi:10.3102/1076998617749561>.  "
  },
  {
    "id": 19621,
    "package_name": "rms",
    "title": "Regression Modeling Strategies",
    "description": "Regression modeling, testing, estimation, validation,\n\tgraphics, prediction, and typesetting by storing enhanced model design\n\tattributes in the fit.  'rms' is a collection of functions that\n\tassist with and streamline modeling.  It also contains functions for\n\tbinary and ordinal logistic regression models, ordinal models for\n  continuous Y with a variety of distribution families, and the Buckley-James\n\tmultiple regression model for right-censored responses, and implements\n\tpenalized maximum likelihood estimation for logistic and ordinary\n\tlinear models.  'rms' works with almost any regression model, but it\n\twas especially written to work with binary or ordinal regression\n\tmodels, Cox regression, accelerated failure time models,\n\tordinary linear models,\tthe Buckley-James model, generalized least\n\tsquares for serially or spatially correlated observations, generalized\n\tlinear models, and quantile regression.",
    "version": "8.1-0",
    "maintainer": "Frank E Harrell Jr <fh@fharrell.com>",
    "author": "Frank E Harrell Jr [aut, cre]",
    "url": "https://hbiostat.org/R/rms/, https://github.com/harrelfe/rms",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rms Regression Modeling Strategies Regression modeling, testing, estimation, validation,\n\tgraphics, prediction, and typesetting by storing enhanced model design\n\tattributes in the fit.  'rms' is a collection of functions that\n\tassist with and streamline modeling.  It also contains functions for\n\tbinary and ordinal logistic regression models, ordinal models for\n  continuous Y with a variety of distribution families, and the Buckley-James\n\tmultiple regression model for right-censored responses, and implements\n\tpenalized maximum likelihood estimation for logistic and ordinary\n\tlinear models.  'rms' works with almost any regression model, but it\n\twas especially written to work with binary or ordinal regression\n\tmodels, Cox regression, accelerated failure time models,\n\tordinary linear models,\tthe Buckley-James model, generalized least\n\tsquares for serially or spatially correlated observations, generalized\n\tlinear models, and quantile regression.  "
  },
  {
    "id": 19680,
    "package_name": "robust2sls",
    "title": "Outlier Robust Two-Stage Least Squares Inference and Testing",
    "description": "An implementation of easy tools for outlier robust inference in\n    two-stage least squares (2SLS) models. The user specifies a reference \n    distribution against which observations are classified as outliers or not. \n    After removing the outliers, adjusted standard errors are automatically \n    provided. Furthermore, several statistical tests for the false outlier \n    detection rate can be calculated. The outlier removing algorithm can be \n    iterated a fixed number of times or until the procedure converges. The \n    algorithms and robust inference are described in more detail in Jiao (2019) \n    <https://drive.google.com/file/d/1qPxDJnLlzLqdk94X9wwVASptf1MPpI2w/view>.",
    "version": "0.2.3",
    "maintainer": "Jonas Kurle <mail@jonaskurle.com>",
    "author": "Jonas Kurle [aut, cre] (ORCID: <https://orcid.org/0000-0003-2197-2012>)",
    "url": "https://github.com/jkurle/robust2sls",
    "bug_reports": "https://github.com/jkurle/robust2sls/issues",
    "repository": "https://cran.r-project.org/package=robust2sls",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "robust2sls Outlier Robust Two-Stage Least Squares Inference and Testing An implementation of easy tools for outlier robust inference in\n    two-stage least squares (2SLS) models. The user specifies a reference \n    distribution against which observations are classified as outliers or not. \n    After removing the outliers, adjusted standard errors are automatically \n    provided. Furthermore, several statistical tests for the false outlier \n    detection rate can be calculated. The outlier removing algorithm can be \n    iterated a fixed number of times or until the procedure converges. The \n    algorithms and robust inference are described in more detail in Jiao (2019) \n    <https://drive.google.com/file/d/1qPxDJnLlzLqdk94X9wwVASptf1MPpI2w/view>.  "
  },
  {
    "id": 19683,
    "package_name": "robustETM",
    "title": "Robust Methods using Exponential Tilt Model",
    "description": "Testing homogeneity for generalized exponential tilt model. This package includes a collection of functions for (1) implementing methods for testing homogeneity for generalized exponential tilt model; and (2) implementing existing methods under comparison.",
    "version": "1.0",
    "maintainer": "Chuan Hong <hong.chuan.hannah@gmail.com>",
    "author": "Chuan Hong, Yong Chen, Yang Ning, Hao Wu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=robustETM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "robustETM Robust Methods using Exponential Tilt Model Testing homogeneity for generalized exponential tilt model. This package includes a collection of functions for (1) implementing methods for testing homogeneity for generalized exponential tilt model; and (2) implementing existing methods under comparison.  "
  },
  {
    "id": 19703,
    "package_name": "rocbc",
    "title": "Statistical Inference for Box-Cox Based Receiver Operating\nCharacteristic Curves",
    "description": "Generation of Box-Cox based ROC curves and several aspects of inferences and hypothesis testing. Can be used when inferences for one biomarker (Bantis LE, Nakas CT, Reiser B. (2018)<doi:10.1002/bimj.201700107>) are of interest or when comparisons of two correlated biomarkers (Bantis LE, Nakas CT, Reiser B. (2021)<doi:10.1002/bimj.202000128>) are of interest. Provides inferences and comparisons around the AUC, the Youden index, the sensitivity at a given specificity level (and vice versa), the optimal operating point of the ROC curve (in the Youden sense), and the Youden based cutoff.",
    "version": "3.1.0",
    "maintainer": "Benjamin Brewer <tennisbenj@gmail.com>",
    "author": "Leonidas Bantis [aut],\n  Benjamin Brewer [cre, ctb],\n  Christos Nakas [ctb],\n  Benjamin Reiser [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rocbc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rocbc Statistical Inference for Box-Cox Based Receiver Operating\nCharacteristic Curves Generation of Box-Cox based ROC curves and several aspects of inferences and hypothesis testing. Can be used when inferences for one biomarker (Bantis LE, Nakas CT, Reiser B. (2018)<doi:10.1002/bimj.201700107>) are of interest or when comparisons of two correlated biomarkers (Bantis LE, Nakas CT, Reiser B. (2021)<doi:10.1002/bimj.202000128>) are of interest. Provides inferences and comparisons around the AUC, the Youden index, the sensitivity at a given specificity level (and vice versa), the optimal operating point of the ROC curve (in the Youden sense), and the Youden based cutoff.  "
  },
  {
    "id": 19710,
    "package_name": "rockx",
    "title": "Easily Import Data from Your 'ODK-X Sync Endpoint'",
    "description": "Provides helper functions for authenticating and retrieving data from your 'ODK-X Sync Endpoint'. This is an early release intended for testing and feedback.",
    "version": "0.1.0",
    "maintainer": "Emil Rossing <hello@sapiens-solutions.com>",
    "author": "Emil Rossing [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rockx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rockx Easily Import Data from Your 'ODK-X Sync Endpoint' Provides helper functions for authenticating and retrieving data from your 'ODK-X Sync Endpoint'. This is an early release intended for testing and feedback.  "
  },
  {
    "id": 19719,
    "package_name": "rofanova",
    "title": "Robust Functional Analysis of Variance",
    "description": "Implements the robust functional analysis of variance (RoFANOVA), described in Centofanti et al. (2021) <arXiv:2112.10643>. \n    It allows testing mean differences among groups of  functional data by being robust against the presence of outliers.",
    "version": "1.0.0",
    "maintainer": "Fabio Centofanti <fabio.centofanti@unina.it>",
    "author": "Fabio Centofanti [cre, aut],\n  Bianca Maria Colosimo [aut],\n  Marco Luigi Grasso [aut],\n  Antonio Lepore [aut],\n  Alessandra Menafoglio [aut],\n  Biagio Palumbo [aut],\n  Simone Vantini [aut]",
    "url": "https://github.com/unina-sfere/rofanova",
    "bug_reports": "https://github.com/unina-sfere/rofanova/issues",
    "repository": "https://cran.r-project.org/package=rofanova",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rofanova Robust Functional Analysis of Variance Implements the robust functional analysis of variance (RoFANOVA), described in Centofanti et al. (2021) <arXiv:2112.10643>. \n    It allows testing mean differences among groups of  functional data by being robust against the presence of outliers.  "
  },
  {
    "id": 19720,
    "package_name": "roger",
    "title": "Automated Grading of R Scripts",
    "description": "Tools for grading the coding style and documentation of R\n  scripts. This is the R component of Roger the Omni Grader, an\n  automated grading system for computer programming projects based on\n  Unix shell scripts; see <https://gitlab.com/roger-project>. The\n  package also provides an R interface to the shell scripts. Inspired by\n  the lintr package.",
    "version": "1.5-1",
    "maintainer": "Vincent Goulet <vincent.goulet@act.ulaval.ca>",
    "author": "Vincent Goulet [aut, cre],\n  Samuel Fr\u00e9chette [aut],\n  Jean-Christophe Langlois [aut],\n  Jim Hester [ctb]",
    "url": "https://roger-project.gitlab.io",
    "bug_reports": "https://gitlab.com/roger-project/roger-rpkg/-/issues",
    "repository": "https://cran.r-project.org/package=roger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roger Automated Grading of R Scripts Tools for grading the coding style and documentation of R\n  scripts. This is the R component of Roger the Omni Grader, an\n  automated grading system for computer programming projects based on\n  Unix shell scripts; see <https://gitlab.com/roger-project>. The\n  package also provides an R interface to the shell scripts. Inspired by\n  the lintr package.  "
  },
  {
    "id": 19765,
    "package_name": "roxut",
    "title": "Document Unit Tests Roxygen-Style",
    "description": "Much as 'roxygen2' allows one to document functions in the same file as the function itself, 'roxut'  allows one to write the unit tests in the same file as the function.  Once processed, the unit tests are moved to the appropriate directory.  Currently supports 'testthat' and 'tinytest' frameworks. The 'roxygen2' package provides much of the infrastructure.",
    "version": "0.4.0",
    "maintainer": "Bryan A. Hanson <hanson@depauw.edu>",
    "author": "Bryan A. Hanson [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3536-8246>),\n  Claudia Beleites [ctb],\n  Hadley Wickham [aut, cph] (roxygen2 code),\n  Peter Danenberg [aut, cph] (roxygen2 code),\n  G\u00e1bor Cs\u00e1rdi [aut] (roxygen2 code),\n  Manuel Eugster [aut, cph] (roxygen2 code),\n  RStudio [cph] (roxygen2 code)",
    "url": "https://github.com/bryanhanson/roxut",
    "bug_reports": "https://github.com/bryanhanson/roxut/issues",
    "repository": "https://cran.r-project.org/package=roxut",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roxut Document Unit Tests Roxygen-Style Much as 'roxygen2' allows one to document functions in the same file as the function itself, 'roxut'  allows one to write the unit tests in the same file as the function.  Once processed, the unit tests are moved to the appropriate directory.  Currently supports 'testthat' and 'tinytest' frameworks. The 'roxygen2' package provides much of the infrastructure.  "
  },
  {
    "id": 19769,
    "package_name": "roxytest",
    "title": "Various Tests with 'roxygen2'",
    "description": "Various tests as 'roxygen2' roclets: e.g. 'testthat' and 'tinytest' tests. \n  Also other static analysis tools as checking parameter documentation consistency and others.",
    "version": "0.0.2",
    "maintainer": "Mikkel Meyer Andersen <mikl@math.aau.dk>",
    "author": "Mikkel Meyer Andersen [aut, cre],\n  Ege Rubak [ctb]",
    "url": "",
    "bug_reports": "https://github.com/mikldk/roxytest/issues",
    "repository": "https://cran.r-project.org/package=roxytest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "roxytest Various Tests with 'roxygen2' Various tests as 'roxygen2' roclets: e.g. 'testthat' and 'tinytest' tests. \n  Also other static analysis tools as checking parameter documentation consistency and others.  "
  },
  {
    "id": 19837,
    "package_name": "rsatscan",
    "title": "Tools, Classes, and Methods for Interfacing with 'SaTScan'\nStand-Alone Software",
    "description": "'SaTScan'(TM) <https://www.satscan.org> is software for finding regions in \n    Time, Space, or Time-Space that have excess risk, based on scan statistics, and \n\tuses Monte Carlo hypothesis testing to generate P-values for these regions.  The \n\t'rsatscan' package provides functions for writing R data frames in \n\t'SaTScan'-readable formats, for setting 'SaTScan' parameters, for running 'SaTScan' in \n\tthe OS, and for reading the files that 'SaTScan' creates.  ",
    "version": "1.0.9",
    "maintainer": "Scott Hostovich <HostovichS@imsweb.com>",
    "author": "Ken Kleinman [aut],\n  Scott Hostovich [cre],\n  Amer Moosa [ctb]",
    "url": "https://www.satscan.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rsatscan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsatscan Tools, Classes, and Methods for Interfacing with 'SaTScan'\nStand-Alone Software 'SaTScan'(TM) <https://www.satscan.org> is software for finding regions in \n    Time, Space, or Time-Space that have excess risk, based on scan statistics, and \n\tuses Monte Carlo hypothesis testing to generate P-values for these regions.  The \n\t'rsatscan' package provides functions for writing R data frames in \n\t'SaTScan'-readable formats, for setting 'SaTScan' parameters, for running 'SaTScan' in \n\tthe OS, and for reading the files that 'SaTScan' creates.    "
  },
  {
    "id": 19844,
    "package_name": "rseedcalc",
    "title": "Estimating the Proportion of Genetically Modified Seeds in\nSeedlots via Multinomial Group Testing",
    "description": "Estimate the percentage of seeds in a seedlot that contain stacks\n    of genetically modified traits.  Estimates are calculated using a\n    multinomial group testing model with maximum likelihood estimation of the\n    parameters.",
    "version": "1.3",
    "maintainer": "Kevin Wright <kw.stat@gmail.com>",
    "author": "Kevin Wright [aut, cre],\n  Jean-Louis Laffont [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rseedcalc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rseedcalc Estimating the Proportion of Genetically Modified Seeds in\nSeedlots via Multinomial Group Testing Estimate the percentage of seeds in a seedlot that contain stacks\n    of genetically modified traits.  Estimates are calculated using a\n    multinomial group testing model with maximum likelihood estimation of the\n    parameters.  "
  },
  {
    "id": 19940,
    "package_name": "runDRT",
    "title": "Run Doubly Ranked Tests",
    "description": "Doubly ranked tests are nonparametric tests for grouped functional and multivariate data.\n    The testing procedure first ranks a matrix (or three dimensional array) of data by column (if a matrix) or by cell (across the third dimension if an array).\n    By default, it calculates a sufficient statistic for the subject's order within the sample using the observed ranks, taken over the columns or cells.\n    Depending on the number of groups, G, the summarized ranks are then analyzed using either a Wilcoxon Rank Sum test (G = 2) or a Kruskal-Wallis (G greater than 2).",
    "version": "0.1.0",
    "maintainer": "Mark J. Meyer <mjm556@georgetown.edu>",
    "author": "Mark J. Meyer [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3942-9675>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=runDRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "runDRT Run Doubly Ranked Tests Doubly ranked tests are nonparametric tests for grouped functional and multivariate data.\n    The testing procedure first ranks a matrix (or three dimensional array) of data by column (if a matrix) or by cell (across the third dimension if an array).\n    By default, it calculates a sufficient statistic for the subject's order within the sample using the observed ranks, taken over the columns or cells.\n    Depending on the number of groups, G, the summarized ranks are then analyzed using either a Wilcoxon Rank Sum test (G = 2) or a Kruskal-Wallis (G greater than 2).  "
  },
  {
    "id": 19941,
    "package_name": "runExamplesWrapper",
    "title": "Wrapper for 'run_examples()'",
    "description": "Captures errors encountered when running 'run_examples()', and processes and archives them. The function 'run_examples()' within the 'devtools' package allows batch execution of all of the examples within a given package. This is much more convenient than testing each example manually. However, a major inconvenience is that if an error is encountered, the program stops and does not complete testing the remaining examples. Also, there is not a systematic record of the results, namely which package functions had no examples, which had examples that failed, and which had examples that succeeded. The current package provides the missing functionality.",
    "version": "1.0",
    "maintainer": "Barry Zeeberg <barryz2013@gmail.com>",
    "author": "Barry Zeeberg [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=runExamplesWrapper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "runExamplesWrapper Wrapper for 'run_examples()' Captures errors encountered when running 'run_examples()', and processes and archives them. The function 'run_examples()' within the 'devtools' package allows batch execution of all of the examples within a given package. This is much more convenient than testing each example manually. However, a major inconvenience is that if an error is encountered, the program stops and does not complete testing the remaining examples. Also, there is not a systematic record of the results, namely which package functions had no examples, which had examples that failed, and which had examples that succeeded. The current package provides the missing functionality.  "
  },
  {
    "id": 20001,
    "package_name": "sEparaTe",
    "title": "Maximum Likelihood Estimation and Likelihood Ratio Test\nFunctions for Separable Variance-Covariance Structures",
    "description": "Maximum likelihood estimation of the parameters\n    of matrix and 3rd-order tensor normal distributions with unstructured\n    factor variance covariance matrices, two procedures, and for unbiased\n    modified likelihood ratio testing of simple and double separability\n    for variance-covariance structures, two procedures. References:\n    Dutilleul P. (1999) <doi:10.1080/00949659908811970>, \n    Manceur AM, Dutilleul P. (2013)\n    <doi:10.1016/j.cam.2012.09.017>, and Manceur AM, Dutilleul \n    P. (2013) <doi:10.1016/j.spl.2012.10.020>.",
    "version": "0.3.2",
    "maintainer": "Timothy Schwinghamer <timothy.schwinghamer@AGR.GC.CA>",
    "author": "Ameur Manceur [aut],\n  Timothy Schwinghamer [aut, cre],\n  Pierre Dutilleul [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sEparaTe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sEparaTe Maximum Likelihood Estimation and Likelihood Ratio Test\nFunctions for Separable Variance-Covariance Structures Maximum likelihood estimation of the parameters\n    of matrix and 3rd-order tensor normal distributions with unstructured\n    factor variance covariance matrices, two procedures, and for unbiased\n    modified likelihood ratio testing of simple and double separability\n    for variance-covariance structures, two procedures. References:\n    Dutilleul P. (1999) <doi:10.1080/00949659908811970>, \n    Manceur AM, Dutilleul P. (2013)\n    <doi:10.1016/j.cam.2012.09.017>, and Manceur AM, Dutilleul \n    P. (2013) <doi:10.1016/j.spl.2012.10.020>.  "
  },
  {
    "id": 20003,
    "package_name": "sGBJ",
    "title": "Survival Extension of the Generalized Berk-Jones Test",
    "description": "Implements an extension of the Generalized Berk-Jones (GBJ) statistic for\n    survival data, sGBJ. It computes the sGBJ statistic and its p-value for testing \n    the association between a gene set and a time-to-event outcome with possible \n    adjustment on additional covariates. Detailed method is available at Villain L, Ferte T, \n    Thiebaut R and Hejblum BP (2021) <doi:10.1101/2021.09.07.459329>.",
    "version": "0.1.1",
    "maintainer": "Laura Villain <sistm.soft.maintain@gmail.com>",
    "author": "Laura Villain [aut, cre],\n  Thomas Ferte [aut],\n  Rodolphe Thiebault [aut],\n  Boris P. Hejblum [aut]",
    "url": "https://github.com/lauravillain/sGBJ",
    "bug_reports": "https://github.com/lauravillain/sGBJ/issues",
    "repository": "https://cran.r-project.org/package=sGBJ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sGBJ Survival Extension of the Generalized Berk-Jones Test Implements an extension of the Generalized Berk-Jones (GBJ) statistic for\n    survival data, sGBJ. It computes the sGBJ statistic and its p-value for testing \n    the association between a gene set and a time-to-event outcome with possible \n    adjustment on additional covariates. Detailed method is available at Villain L, Ferte T, \n    Thiebaut R and Hejblum BP (2021) <doi:10.1101/2021.09.07.459329>.  "
  },
  {
    "id": 20053,
    "package_name": "safestats",
    "title": "Safe Anytime-Valid Inference",
    "description": "Functions to design and apply tests that are anytime valid. The\n  functions can be used to design hypothesis tests in the prospective/randomised\n  control trial setting or in the observational/retrospective setting. The\n  resulting tests remain valid under both optional stopping and optional\n  continuation. The current version includes safe t-tests and safe tests of\n  two proportions. For details on the theory of safe tests, see\n  Grunwald, de Heide and Koolen (2019) \"Safe Testing\" <arXiv:1906.07801>, \n  for details on safe logrank tests see ter Schure, Perez-Ortiz, Ly and Grunwald\n  (2020) \"The Safe Logrank Test: Error Control under Continuous Monitoring with \n  Unlimited Horizon\" <arXiv:2011.06931v3> and Turner, Ly and Grunwald (2021)\n  \"Safe Tests and Always-Valid Confidence Intervals for contingency tables and \n  beyond\" <arXiv:2106.02693> for details on safe contingency table tests.",
    "version": "0.8.7",
    "maintainer": "Alexander Ly <a.ly@jasp-stats.org>",
    "author": "Rosanne Turner [aut],\n  Alexander Ly [cre, aut],\n  Muriel Felipe Perez-Ortiz [ctb],\n  Judith ter Schure [ctb],\n  Peter Grunwald [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=safestats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "safestats Safe Anytime-Valid Inference Functions to design and apply tests that are anytime valid. The\n  functions can be used to design hypothesis tests in the prospective/randomised\n  control trial setting or in the observational/retrospective setting. The\n  resulting tests remain valid under both optional stopping and optional\n  continuation. The current version includes safe t-tests and safe tests of\n  two proportions. For details on the theory of safe tests, see\n  Grunwald, de Heide and Koolen (2019) \"Safe Testing\" <arXiv:1906.07801>, \n  for details on safe logrank tests see ter Schure, Perez-Ortiz, Ly and Grunwald\n  (2020) \"The Safe Logrank Test: Error Control under Continuous Monitoring with \n  Unlimited Horizon\" <arXiv:2011.06931v3> and Turner, Ly and Grunwald (2021)\n  \"Safe Tests and Always-Valid Confidence Intervals for contingency tables and \n  beyond\" <arXiv:2106.02693> for details on safe contingency table tests.  "
  },
  {
    "id": 20079,
    "package_name": "samplesizeestimator",
    "title": "Calculate Sample Size for Various Scenarios",
    "description": "Calculates sample size for various scenarios, such as sample size\n    to estimate population proportion with stated absolute or relative\n    precision, testing a single proportion with a reference value, to estimate\n    the population mean with stated absolute or relative precision, testing\n    single mean with a reference value and sample size for comparing two\n    unpaired or independent means, comparing two paired means, the sample size\n    For case control studies, estimating the odds ratio with stated precision,\n    testing the odds ratio with a reference value, estimating relative risk with\n    stated precision, testing relative risk with a reference value, testing\n    a correlation coefficient with a specified value, etc. \n    <https://www.academia.edu/39511442/Adequacy_of_Sample_Size_in_Health_Studies#:~:text=Determining%20the%20sample%20size%20for,may%20yield%20statistically%20inconclusive%20results.>. ",
    "version": "1.0.0",
    "maintainer": "R Amala <amalar.statistics@gmail.com>",
    "author": "R Amala [aut, cre, cph],\n  G Kumarapandiyan [aut],\n  A Srividya [ctb],\n  M Rajeswari [ctb],\n  Ashwani Kumar [ctb]",
    "url": "",
    "bug_reports": "https://forms.gle/NcznwYU9yx5HdJu29",
    "repository": "https://cran.r-project.org/package=samplesizeestimator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "samplesizeestimator Calculate Sample Size for Various Scenarios Calculates sample size for various scenarios, such as sample size\n    to estimate population proportion with stated absolute or relative\n    precision, testing a single proportion with a reference value, to estimate\n    the population mean with stated absolute or relative precision, testing\n    single mean with a reference value and sample size for comparing two\n    unpaired or independent means, comparing two paired means, the sample size\n    For case control studies, estimating the odds ratio with stated precision,\n    testing the odds ratio with a reference value, estimating relative risk with\n    stated precision, testing relative risk with a reference value, testing\n    a correlation coefficient with a specified value, etc. \n    <https://www.academia.edu/39511442/Adequacy_of_Sample_Size_in_Health_Studies#:~:text=Determining%20the%20sample%20size%20for,may%20yield%20statistically%20inconclusive%20results.>.   "
  },
  {
    "id": 20099,
    "package_name": "sanitizers",
    "title": "C/C++ Source Code to Trigger Address and Undefined Behaviour\nSanitizers",
    "description": "Recent gcc and clang compiler versions provide functionality to\n test for memory violations and other undefined behaviour; this is often \n referred to as \"Address Sanitizer\" (or 'ASAN') and \"Undefined Behaviour \n Sanitizer\" ('UBSAN'). The Writing R Extension manual describes this in some\n detail in Section 4.3 title \"Checking Memory Access\".\n\n    This feature has to be enabled in the corresponding binary, eg in R, which\n is somewhat involved as it also required a current compiler toolchain which \n is not yet widely available, or in the case of Windows, not available at all\n (via the common Rtools mechanism).\n\n    As an alternative, pre-built Docker containers such as the Rocker container\n 'r-devel-san' or the multi-purpose container 'r-debug' can be used.\n\n    This package then provides a means of testing the compiler setup as the\n known code failures provides in the sample code here should be detected\n correctly, whereas a default build of R will let the package pass.\n\n    The code samples are based on the examples from the Address Sanitizer\n Wiki at <https://github.com/google/sanitizers/wiki>.",
    "version": "0.1.1",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel",
    "url": "https://github.com/eddelbuettel/sanitizers,\nhttps://dirk.eddelbuettel.com/code/sanitizers.html",
    "bug_reports": "https://github.com/eddelbuettel/sanitizers/issues",
    "repository": "https://cran.r-project.org/package=sanitizers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sanitizers C/C++ Source Code to Trigger Address and Undefined Behaviour\nSanitizers Recent gcc and clang compiler versions provide functionality to\n test for memory violations and other undefined behaviour; this is often \n referred to as \"Address Sanitizer\" (or 'ASAN') and \"Undefined Behaviour \n Sanitizer\" ('UBSAN'). The Writing R Extension manual describes this in some\n detail in Section 4.3 title \"Checking Memory Access\".\n\n    This feature has to be enabled in the corresponding binary, eg in R, which\n is somewhat involved as it also required a current compiler toolchain which \n is not yet widely available, or in the case of Windows, not available at all\n (via the common Rtools mechanism).\n\n    As an alternative, pre-built Docker containers such as the Rocker container\n 'r-devel-san' or the multi-purpose container 'r-debug' can be used.\n\n    This package then provides a means of testing the compiler setup as the\n known code failures provides in the sample code here should be detected\n correctly, whereas a default build of R will let the package pass.\n\n    The code samples are based on the examples from the Address Sanitizer\n Wiki at <https://github.com/google/sanitizers/wiki>.  "
  },
  {
    "id": 20138,
    "package_name": "sbim",
    "title": "Simulation-Based Inference using a Metamodel for Log-Likelihood\nEstimator",
    "description": "Parameter inference methods for models defined implicitly using a random simulator. Inference is carried out using simulation-based estimates of the log-likelihood of the data. The inference methods implemented in this package are explained in Park, J. (2025) <doi:10.48550/arxiv.2311.09446>. These methods are built on a simulation metamodel which assumes that the estimates of the log-likelihood are approximately normally distributed with the mean function that is locally quadratic around its maximum. Parameter estimation and uncertainty quantification can be carried out using the ht() function (for hypothesis testing) and the ci() function (for constructing a confidence interval for one-dimensional parameters).",
    "version": "1.0.0",
    "maintainer": "Joonha Park <j.park@ku.edu>",
    "author": "Joonha Park [aut, cre] (ORCID: <https://orcid.org/0000-0002-4493-7730>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sbim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sbim Simulation-Based Inference using a Metamodel for Log-Likelihood\nEstimator Parameter inference methods for models defined implicitly using a random simulator. Inference is carried out using simulation-based estimates of the log-likelihood of the data. The inference methods implemented in this package are explained in Park, J. (2025) <doi:10.48550/arxiv.2311.09446>. These methods are built on a simulation metamodel which assumes that the estimates of the log-likelihood are approximately normally distributed with the mean function that is locally quadratic around its maximum. Parameter estimation and uncertainty quantification can be carried out using the ht() function (for hypothesis testing) and the ci() function (for constructing a confidence interval for one-dimensional parameters).  "
  },
  {
    "id": 20158,
    "package_name": "scISR",
    "title": "Single-Cell Imputation using Subspace Regression",
    "description": "Provides an imputation pipeline for single-cell RNA sequencing data. \n  The 'scISR' method uses a hypothesis-testing technique to identify zero-valued entries that are most likely affected by dropout events and estimates the dropout values using a subspace regression model (Tran et.al. (2022) <DOI:10.1038/s41598-022-06500-4>).",
    "version": "0.1.1",
    "maintainer": "Duc Tran <duct@nevada.unr.edu>",
    "author": "Duc Tran [aut, cre],\n  Bang Tran [aut],\n  Hung Nguyen [aut],\n  Tin Nguyen [fnd]",
    "url": "https://github.com/duct317/scISR",
    "bug_reports": "https://github.com/duct317/scISR/issues",
    "repository": "https://cran.r-project.org/package=scISR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scISR Single-Cell Imputation using Subspace Regression Provides an imputation pipeline for single-cell RNA sequencing data. \n  The 'scISR' method uses a hypothesis-testing technique to identify zero-valued entries that are most likely affected by dropout events and estimates the dropout values using a subspace regression model (Tran et.al. (2022) <DOI:10.1038/s41598-022-06500-4>).  "
  },
  {
    "id": 20184,
    "package_name": "scanstatistics",
    "title": "Space-Time Anomaly Detection using Scan Statistics",
    "description": "Detection of anomalous space-time clusters using the scan \n  statistics methodology. Focuses on prospective surveillance of data streams, \n  scanning for clusters with ongoing anomalies. Hypothesis testing is made \n  possible by Monte Carlo simulation. All\u00e9vius (2018) <doi:10.21105/joss.00515>.",
    "version": "1.1.2",
    "maintainer": "Paul Romer Present <paul.romerpresent@fastmail.fm>",
    "author": "Benjamin All\u00e9vius [aut],\n  Paul Romer Present [ctb, cre]",
    "url": "https://github.com/promerpr/scanstatistics",
    "bug_reports": "https://github.com/promerpr/scanstatistics/issues",
    "repository": "https://cran.r-project.org/package=scanstatistics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "scanstatistics Space-Time Anomaly Detection using Scan Statistics Detection of anomalous space-time clusters using the scan \n  statistics methodology. Focuses on prospective surveillance of data streams, \n  scanning for clusters with ongoing anomalies. Hypothesis testing is made \n  possible by Monte Carlo simulation. All\u00e9vius (2018) <doi:10.21105/joss.00515>.  "
  },
  {
    "id": 20276,
    "package_name": "sdafilter",
    "title": "Symmetrized Data Aggregation",
    "description": "We develop a new class of distribution free multiple testing rules for false discovery rate (FDR) control under general dependence. A key element in our proposal is a symmetrized data aggregation (SDA) approach to incorporating the dependence structure via sample splitting, data screening and information pooling. The proposed SDA filter first constructs a sequence of ranking statistics that fulfill global symmetry properties, and then chooses a data driven threshold along the ranking to control the FDR. For more information, see the website below and the accompanying paper: Du et al. (2020), \"False Discovery Rate Control Under General Dependence By Symmetrized Data Aggregation\", <arXiv:2002.11992>.",
    "version": "1.0.0",
    "maintainer": "Lilun Du <dulilun@ust.hk>",
    "author": "Lilun Du [aut, cre],\n  Xu Guo [ctb],\n  Wenguang Sun [ctb],\n  Changliang Zou [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sdafilter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sdafilter Symmetrized Data Aggregation We develop a new class of distribution free multiple testing rules for false discovery rate (FDR) control under general dependence. A key element in our proposal is a symmetrized data aggregation (SDA) approach to incorporating the dependence structure via sample splitting, data screening and information pooling. The proposed SDA filter first constructs a sequence of ranking statistics that fulfill global symmetry properties, and then chooses a data driven threshold along the ranking to control the FDR. For more information, see the website below and the accompanying paper: Du et al. (2020), \"False Discovery Rate Control Under General Dependence By Symmetrized Data Aggregation\", <arXiv:2002.11992>.  "
  },
  {
    "id": 20304,
    "package_name": "searchAnalyzeR",
    "title": "Advanced Analytics and Testing Framework for Systematic Review\nSearch Strategies",
    "description": "Provides comprehensive analytics, reporting, and testing capabilities \n    for systematic review search strategies. The package focuses on validating search \n    performance, generating standardized 'PRISMA'-compliant reports, and ensuring \n    reproducibility in evidence synthesis. Features include precision-recall analysis, \n    cross-database performance comparison, benchmark validation against gold standards, \n    sensitivity analysis, temporal coverage assessment, automated report generation,\n    and statistical comparison of search strategies. Supports multiple export formats \n    including 'CSV', 'Excel', 'RIS', 'BibTeX', and 'EndNote'. Includes tools for duplicate \n    detection, search strategy optimization, cross-validation frameworks, meta-analysis \n    of benchmark results, power analysis for study design, and reproducibility package \n    creation. Optionally connects to 'PubMed' for direct database searching and real-time \n    strategy comparison using the 'E-utilities' 'API'. Enhanced with bootstrap comparison \n    methods, 'McNemar' test for strategy evaluation, and comprehensive visualization tools \n    for performance assessment. Methods based on Manning et al. (2008) \n    for information retrieval metrics, Moher et al. (2009)\n    for 'PRISMA' guidelines, and Sampson et al. (2006) for \n    systematic review search methodology.",
    "version": "0.1.0",
    "maintainer": "Chao Liu <chaoliu@cedarville.edu>",
    "author": "Chao Liu [aut, cre] (ORCID: <https://orcid.org/0000-0002-9979-8272>)",
    "url": "https://github.com/chaoliu-cl/searchAnalyzeR",
    "bug_reports": "https://github.com/chaoliu-cl/searchAnalyzeR/issues",
    "repository": "https://cran.r-project.org/package=searchAnalyzeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "searchAnalyzeR Advanced Analytics and Testing Framework for Systematic Review\nSearch Strategies Provides comprehensive analytics, reporting, and testing capabilities \n    for systematic review search strategies. The package focuses on validating search \n    performance, generating standardized 'PRISMA'-compliant reports, and ensuring \n    reproducibility in evidence synthesis. Features include precision-recall analysis, \n    cross-database performance comparison, benchmark validation against gold standards, \n    sensitivity analysis, temporal coverage assessment, automated report generation,\n    and statistical comparison of search strategies. Supports multiple export formats \n    including 'CSV', 'Excel', 'RIS', 'BibTeX', and 'EndNote'. Includes tools for duplicate \n    detection, search strategy optimization, cross-validation frameworks, meta-analysis \n    of benchmark results, power analysis for study design, and reproducibility package \n    creation. Optionally connects to 'PubMed' for direct database searching and real-time \n    strategy comparison using the 'E-utilities' 'API'. Enhanced with bootstrap comparison \n    methods, 'McNemar' test for strategy evaluation, and comprehensive visualization tools \n    for performance assessment. Methods based on Manning et al. (2008) \n    for information retrieval metrics, Moher et al. (2009)\n    for 'PRISMA' guidelines, and Sampson et al. (2006) for \n    systematic review search methodology.  "
  },
  {
    "id": 20337,
    "package_name": "segmented",
    "title": "Regression Models with Break-Points / Change-Points Estimation\n(with Possibly Random Effects)",
    "description": "Fitting regression models where, in addition to possible linear terms, one or more covariates have segmented (i.e., broken-line or piece-wise linear) or stepmented (i.e. piece-wise constant) effects. Multiple breakpoints for the same variable are allowed. \n  The estimation method is discussed in Muggeo (2003, <doi:10.1002/sim.1545>) and \n  illustrated in Muggeo (2008, <https://www.r-project.org/doc/Rnews/Rnews_2008-1.pdf>). An approach for hypothesis testing is presented \n  in Muggeo (2016, <doi:10.1080/00949655.2016.1149855>), and interval estimation for the breakpoint is discussed in Muggeo (2017, <doi:10.1111/anzs.12200>). \n  Segmented mixed models, i.e. random effects in the change point, are discussed in Muggeo (2014, <doi:10.1177/1471082X13504721>).\n  Estimation of piecewise-constant relationships and changepoints (mean-shift models) is \n  discussed in Fasola et al. (2018, <doi:10.1007/s00180-017-0740-4>).",
    "version": "2.1-4",
    "maintainer": "Vito M. R. Muggeo <vito.muggeo@unipa.it>",
    "author": "Vito M. R. Muggeo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3386-4054>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=segmented",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "segmented Regression Models with Break-Points / Change-Points Estimation\n(with Possibly Random Effects) Fitting regression models where, in addition to possible linear terms, one or more covariates have segmented (i.e., broken-line or piece-wise linear) or stepmented (i.e. piece-wise constant) effects. Multiple breakpoints for the same variable are allowed. \n  The estimation method is discussed in Muggeo (2003, <doi:10.1002/sim.1545>) and \n  illustrated in Muggeo (2008, <https://www.r-project.org/doc/Rnews/Rnews_2008-1.pdf>). An approach for hypothesis testing is presented \n  in Muggeo (2016, <doi:10.1080/00949655.2016.1149855>), and interval estimation for the breakpoint is discussed in Muggeo (2017, <doi:10.1111/anzs.12200>). \n  Segmented mixed models, i.e. random effects in the change point, are discussed in Muggeo (2014, <doi:10.1177/1471082X13504721>).\n  Estimation of piecewise-constant relationships and changepoints (mean-shift models) is \n  discussed in Fasola et al. (2018, <doi:10.1007/s00180-017-0740-4>).  "
  },
  {
    "id": 20356,
    "package_name": "selenider",
    "title": "Concise, Lazy and Reliable Wrapper for 'chromote' and 'selenium'",
    "description": "A user-friendly wrapper for web automation, using either\n    'chromote' or 'selenium'. Provides a simple and consistent API to make\n    web scraping and testing scripts easy to write and understand.\n    Elements are lazy, and automatically wait for the website to be valid,\n    resulting in reliable and reproducible code, with no visible impact on\n    the experience of the programmer.",
    "version": "0.4.1",
    "maintainer": "Ashby Thorpe <ashbythorpe@gmail.com>",
    "author": "Ashby Thorpe [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-3106-099X>)",
    "url": "https://github.com/ashbythorpe/selenider,\nhttps://ashbythorpe.github.io/selenider/",
    "bug_reports": "https://github.com/ashbythorpe/selenider/issues",
    "repository": "https://cran.r-project.org/package=selenider",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "selenider Concise, Lazy and Reliable Wrapper for 'chromote' and 'selenium' A user-friendly wrapper for web automation, using either\n    'chromote' or 'selenium'. Provides a simple and consistent API to make\n    web scraping and testing scripts easy to write and understand.\n    Elements are lazy, and automatically wait for the website to be valid,\n    resulting in reliable and reproducible code, with no visible impact on\n    the experience of the programmer.  "
  },
  {
    "id": 20365,
    "package_name": "semTests",
    "title": "Goodness-of-Fit Testing for Structural Equation Models",
    "description": "Supports eigenvalue block-averaging p-values (Foldnes, Gr\u00f8nneberg, 2018) <doi:10.1080/10705511.2017.1373021>,\n    penalized eigenvalue block-averaging p-values (Foldnes, Moss, Gr\u00f8nneberg, 2024) <doi:10.1080/10705511.2024.2372028>, penalized\n    regression p-values (Foldnes, Moss, Gr\u00f8nneberg, 2024) <doi:10.1080/10705511.2024.2372028>, as well as traditional p-values such as Satorra-Bentler. All p-values can\n    be calculated using unbiased or biased gamma estimates (Du, Bentler, 2022) <doi:10.1080/10705511.2022.2063870> \n    and two choices of chi square statistics.",
    "version": "0.7.1",
    "maintainer": "Jonas Moss <jonas.moss.statistics@gmail.com>",
    "author": "Jonas Moss [aut, cre] (ORCID: <https://orcid.org/0000-0002-6876-6964>),\n  Nj\u00e5l Foldnes [ctb] (ORCID: <https://orcid.org/0000-0001-6889-6067>),\n  Steffen Gr\u00f8nneberg [ctb] (ORCID:\n    <https://orcid.org/0000-0003-2785-6530>)",
    "url": "https://github.com/JonasMoss/semTests",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=semTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "semTests Goodness-of-Fit Testing for Structural Equation Models Supports eigenvalue block-averaging p-values (Foldnes, Gr\u00f8nneberg, 2018) <doi:10.1080/10705511.2017.1373021>,\n    penalized eigenvalue block-averaging p-values (Foldnes, Moss, Gr\u00f8nneberg, 2024) <doi:10.1080/10705511.2024.2372028>, penalized\n    regression p-values (Foldnes, Moss, Gr\u00f8nneberg, 2024) <doi:10.1080/10705511.2024.2372028>, as well as traditional p-values such as Satorra-Bentler. All p-values can\n    be calculated using unbiased or biased gamma estimates (Du, Bentler, 2022) <doi:10.1080/10705511.2022.2063870> \n    and two choices of chi square statistics.  "
  },
  {
    "id": 20381,
    "package_name": "seminrExtras",
    "title": "Conduct Additional Modeling and Analysis for 'seminr'",
    "description": "Supplemental functions for estimating and analysing structural equation models including Cross Validated Prediction and Testing (CVPAT, Liengaard et al., 2021 <doi:10.1111/deci.12445>).",
    "version": "0.2.0",
    "maintainer": "Nicholas Patrick Danks <nicholasdanks@hotmail.com>",
    "author": "Soumya Ray [aut, ths],\n  Nicholas Patrick Danks [aut, cre]",
    "url": "https://github.com/sem-in-r/seminr",
    "bug_reports": "https://github.com/sem-in-r/seminr/issues",
    "repository": "https://cran.r-project.org/package=seminrExtras",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "seminrExtras Conduct Additional Modeling and Analysis for 'seminr' Supplemental functions for estimating and analysing structural equation models including Cross Validated Prediction and Testing (CVPAT, Liengaard et al., 2021 <doi:10.1111/deci.12445>).  "
  },
  {
    "id": 20480,
    "package_name": "sft",
    "title": "Functions for Systems Factorial Technology Analysis of Data",
    "description": "A series of tools for analyzing Systems Factorial Technology data.  This includes functions for plotting and statistically testing capacity coefficient functions and survivor interaction contrast functions.  Houpt, Blaha, McIntire, Havig, and Townsend (2013) <doi:10.3758/s13428-013-0377-3> provide a basic introduction to Systems Factorial Technology along with examples using the sft R package.",
    "version": "2.4",
    "maintainer": "Joe Houpt <joseph.houpt@utsa.edu>",
    "author": "Joe Houpt [aut, cre] (ORCID: <https://orcid.org/0000-0002-2784-5535>),\n  Leslie Blaha [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sft",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sft Functions for Systems Factorial Technology Analysis of Data A series of tools for analyzing Systems Factorial Technology data.  This includes functions for plotting and statistically testing capacity coefficient functions and survivor interaction contrast functions.  Houpt, Blaha, McIntire, Havig, and Townsend (2013) <doi:10.3758/s13428-013-0377-3> provide a basic introduction to Systems Factorial Technology along with examples using the sft R package.  "
  },
  {
    "id": 20495,
    "package_name": "sgof",
    "title": "Multiple Hypothesis Testing",
    "description": "Seven different methods for multiple testing problems. The SGoF-type methods (see for example, Carvajal Rodr\u00edguez et al., 2009 <doi:10.1186/1471-2105-10-209>;  de U\u00f1a \u00c1lvarez, 2012 <doi:10.1515/1544-6115.1812>; Castro Conde et al., 2015 <doi:10.1177/0962280215597580>) and the BH and BY false discovery rate controlling procedures.",
    "version": "2.3.5",
    "maintainer": "Irene Castro Conde <irene.castro@uvigo.es>",
    "author": "Irene Castro Conde and Jacobo de Una Alvarez",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sgof",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sgof Multiple Hypothesis Testing Seven different methods for multiple testing problems. The SGoF-type methods (see for example, Carvajal Rodr\u00edguez et al., 2009 <doi:10.1186/1471-2105-10-209>;  de U\u00f1a \u00c1lvarez, 2012 <doi:10.1515/1544-6115.1812>; Castro Conde et al., 2015 <doi:10.1177/0962280215597580>) and the BH and BY false discovery rate controlling procedures.  "
  },
  {
    "id": 20506,
    "package_name": "shadowVIMP",
    "title": "Covariate Selection Based on VIMP Permutation-Like Testing",
    "description": "A statistical method for reducing the number of covariates in\n    an analysis by evaluating Variable Importance Measures (VIMPs) derived\n    from the Random Forest algorithm. It performs statistical tests on the\n    VIMPs and outputs whether the covariate is significant along with the\n    p-values.",
    "version": "1.0.2",
    "maintainer": "Oktawia Miluch <oktawia.miluch@staburo.de>",
    "author": "Tim Mueller [aut],\n  Oktawia Miluch [aut, cre],\n  Staburo GmbH [cph, fnd]",
    "url": "https://github.com/OktawiaStaburo/shadowVIMP",
    "bug_reports": "https://github.com/OktawiaStaburo/shadowVIMP/issues",
    "repository": "https://cran.r-project.org/package=shadowVIMP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shadowVIMP Covariate Selection Based on VIMP Permutation-Like Testing A statistical method for reducing the number of covariates in\n    an analysis by evaluating Variable Importance Measures (VIMPs) derived\n    from the Random Forest algorithm. It performs statistical tests on the\n    VIMPs and outputs whether the covariate is significant along with the\n    p-values.  "
  },
  {
    "id": 20515,
    "package_name": "shapley",
    "title": "Weighted Mean SHAP and CI for Robust Feature Assessment in ML\nGrid",
    "description": "This R package introduces Weighted Mean SHapley Additive exPlanations (WMSHAP), an innovative method for calculating SHAP values for a grid of fine-tuned base-learner machine learning models as well as stacked ensembles, a method not previously available due to the common reliance on single best-performing models. By integrating the weighted mean SHAP values from individual base-learners comprising the ensemble or individual base-learners in a tuning grid search, the package weights SHAP contributions according to each model's performance, assessed by multiple either R squared (for both regression and classification models). alternatively, this software also offers weighting SHAP values based on the area under the precision-recall curve (AUCPR), the area under the curve (AUC), and F2 measures for binary classifiers. It further extends this framework to implement weighted confidence intervals for weighted mean SHAP values, offering a more comprehensive and robust feature importance evaluation over a grid of machine learning models, instead of solely computing SHAP values for the best model. This methodology is particularly beneficial for addressing the severe class imbalance (class rarity) problem by providing a transparent, generalized measure of feature importance that mitigates the risk of reporting SHAP values for an overfitted or biased model and maintains robustness under severe class imbalance, where there is no universal criteria of identifying the absolute best model. Furthermore, the package implements hypothesis testing to ascertain the statistical significance of SHAP values for individual features, as well as comparative significance testing of SHAP contributions between features. Additionally, it tackles a critical gap in feature selection literature by presenting criteria for the automatic feature selection of the most important features across a grid of models or stacked ensembles, eliminating the need for arbitrary determination of the number of top features to be extracted. This utility is invaluable for researchers analyzing feature significance, particularly within severely imbalanced outcomes where conventional methods fall short. Moreover, it is also expected to report democratic feature importance across a grid of models, resulting in a more comprehensive and generalizable feature selection. The package further implements a novel method for visualizing SHAP values both at subject level and feature level as well as a plot for feature selection based on the weighted mean SHAP ratios.",
    "version": "0.5.1",
    "maintainer": "E. F. Haghish <haghish@hotmail.com>",
    "author": "E. F. Haghish [aut, cre, cph]",
    "url": "https://github.com/haghish/shapley",
    "bug_reports": "https://github.com/haghish/shapley/issues",
    "repository": "https://cran.r-project.org/package=shapley",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shapley Weighted Mean SHAP and CI for Robust Feature Assessment in ML\nGrid This R package introduces Weighted Mean SHapley Additive exPlanations (WMSHAP), an innovative method for calculating SHAP values for a grid of fine-tuned base-learner machine learning models as well as stacked ensembles, a method not previously available due to the common reliance on single best-performing models. By integrating the weighted mean SHAP values from individual base-learners comprising the ensemble or individual base-learners in a tuning grid search, the package weights SHAP contributions according to each model's performance, assessed by multiple either R squared (for both regression and classification models). alternatively, this software also offers weighting SHAP values based on the area under the precision-recall curve (AUCPR), the area under the curve (AUC), and F2 measures for binary classifiers. It further extends this framework to implement weighted confidence intervals for weighted mean SHAP values, offering a more comprehensive and robust feature importance evaluation over a grid of machine learning models, instead of solely computing SHAP values for the best model. This methodology is particularly beneficial for addressing the severe class imbalance (class rarity) problem by providing a transparent, generalized measure of feature importance that mitigates the risk of reporting SHAP values for an overfitted or biased model and maintains robustness under severe class imbalance, where there is no universal criteria of identifying the absolute best model. Furthermore, the package implements hypothesis testing to ascertain the statistical significance of SHAP values for individual features, as well as comparative significance testing of SHAP contributions between features. Additionally, it tackles a critical gap in feature selection literature by presenting criteria for the automatic feature selection of the most important features across a grid of models or stacked ensembles, eliminating the need for arbitrary determination of the number of top features to be extracted. This utility is invaluable for researchers analyzing feature significance, particularly within severely imbalanced outcomes where conventional methods fall short. Moreover, it is also expected to report democratic feature importance across a grid of models, resulting in a more comprehensive and generalizable feature selection. The package further implements a novel method for visualizing SHAP values both at subject level and feature level as well as a plot for feature selection based on the weighted mean SHAP ratios.  "
  },
  {
    "id": 20533,
    "package_name": "shiftR",
    "title": "Fast Enrichment Analysis via Circular Permutations",
    "description": "Fast enrichment analysis for locally correlated statistics\n        via circular permutations.\n        The analysis can be performed at multiple significance thresholds\n        for both primary and auxiliary data sets with\n        efficient correction for multiple testing.",
    "version": "1.5",
    "maintainer": "Andrey A Shabalin <andrey.shabalin@gmail.com>",
    "author": "Andrey A Shabalin [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0309-6821>),\n  Edwin J C G van den Oord [aut]",
    "url": "https://github.com/andreyshabalin/shiftR",
    "bug_reports": "https://github.com/andreyshabalin/shiftR/issues",
    "repository": "https://cran.r-project.org/package=shiftR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shiftR Fast Enrichment Analysis via Circular Permutations Fast enrichment analysis for locally correlated statistics\n        via circular permutations.\n        The analysis can be performed at multiple significance thresholds\n        for both primary and auxiliary data sets with\n        efficient correction for multiple testing.  "
  },
  {
    "id": 20673,
    "package_name": "sievePH",
    "title": "Sieve Analysis Methods for Proportional Hazards Models",
    "description": "Implements a suite of semiparametric and nonparametric kernel-smoothed estimation and testing procedures for continuous mark-specific stratified hazard ratio (treatment/placebo) models in a randomized treatment efficacy trial with a time-to-event endpoint. Semiparametric methods, allowing multivariate marks, are described in Juraska M and Gilbert PB (2013), Mark-specific hazard ratio model with multivariate continuous marks: an application to vaccine efficacy. Biometrics 69(2):328-337 <doi:10.1111/biom.12016>, and in Juraska M and Gilbert PB (2016), Mark-specific hazard ratio model with missing multivariate marks. Lifetime Data Analysis 22(4):606-25 <doi:10.1007/s10985-015-9353-9>. Nonparametric kernel-smoothed methods, allowing univariate marks only, are described in Sun Y and Gilbert PB (2012), Estimation of stratified mark\u2010specific proportional hazards models with missing marks. Scandinavian Journal of Statistics}, 39(1):34-52 <doi:10.1111/j.1467-9469.2011.00746.x>, and in Gilbert PB and Sun Y (2015), Inferences on relative failure rates in stratified mark-specific proportional hazards models with missing marks, with application to human immunodeficiency virus vaccine efficacy trials. Journal of the Royal Statistical Society Series C: Applied Statistics, 64(1):49-73 <doi:10.1111/rssc.12067>. Both semiparametric and nonparametric approaches consider two scenarios: (1) the mark is fully observed in all subjects who experience the event of interest, and (2) the mark is subject to missingness-at-random in subjects who experience the event of interest. For models with missing marks, estimators are implemented based on (i) inverse probability weighting (IPW) of complete cases (for the semiparametric framework), and (ii) augmentation of the IPW estimating functions by leveraging correlations between the mark and auxiliary data to 'impute' the augmentation term for subjects with missing marks (for both the semiparametric and nonparametric framework). The augmented IPW estimators are doubly robust and recommended for use with incomplete mark data. The semiparametric methods make two key assumptions: (i) the time-to-event is assumed to be conditionally independent of the mark given treatment, and (ii) the weight function in the semiparametric density ratio/biased sampling model is assumed to be exponential. Diagnostic testing procedures for evaluating validity of both assumptions are implemented. Summary and plotting functions are provided for estimation and inferential results.",
    "version": "1.1",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "author": "Michal Juraska [aut, cre],\n  Li Li [ctb],\n  Stephanie Wu [ctb]",
    "url": "https://github.com/mjuraska/sievePH",
    "bug_reports": "https://github.com/mjuraska/sievePH/issues",
    "repository": "https://cran.r-project.org/package=sievePH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sievePH Sieve Analysis Methods for Proportional Hazards Models Implements a suite of semiparametric and nonparametric kernel-smoothed estimation and testing procedures for continuous mark-specific stratified hazard ratio (treatment/placebo) models in a randomized treatment efficacy trial with a time-to-event endpoint. Semiparametric methods, allowing multivariate marks, are described in Juraska M and Gilbert PB (2013), Mark-specific hazard ratio model with multivariate continuous marks: an application to vaccine efficacy. Biometrics 69(2):328-337 <doi:10.1111/biom.12016>, and in Juraska M and Gilbert PB (2016), Mark-specific hazard ratio model with missing multivariate marks. Lifetime Data Analysis 22(4):606-25 <doi:10.1007/s10985-015-9353-9>. Nonparametric kernel-smoothed methods, allowing univariate marks only, are described in Sun Y and Gilbert PB (2012), Estimation of stratified mark\u2010specific proportional hazards models with missing marks. Scandinavian Journal of Statistics}, 39(1):34-52 <doi:10.1111/j.1467-9469.2011.00746.x>, and in Gilbert PB and Sun Y (2015), Inferences on relative failure rates in stratified mark-specific proportional hazards models with missing marks, with application to human immunodeficiency virus vaccine efficacy trials. Journal of the Royal Statistical Society Series C: Applied Statistics, 64(1):49-73 <doi:10.1111/rssc.12067>. Both semiparametric and nonparametric approaches consider two scenarios: (1) the mark is fully observed in all subjects who experience the event of interest, and (2) the mark is subject to missingness-at-random in subjects who experience the event of interest. For models with missing marks, estimators are implemented based on (i) inverse probability weighting (IPW) of complete cases (for the semiparametric framework), and (ii) augmentation of the IPW estimating functions by leveraging correlations between the mark and auxiliary data to 'impute' the augmentation term for subjects with missing marks (for both the semiparametric and nonparametric framework). The augmented IPW estimators are doubly robust and recommended for use with incomplete mark data. The semiparametric methods make two key assumptions: (i) the time-to-event is assumed to be conditionally independent of the mark given treatment, and (ii) the weight function in the semiparametric density ratio/biased sampling model is assumed to be exponential. Diagnostic testing procedures for evaluating validity of both assumptions are implemented. Summary and plotting functions are provided for estimation and inferential results.  "
  },
  {
    "id": 20679,
    "package_name": "sigclust",
    "title": "Statistical Significance of Clustering",
    "description": "SigClust is a statistical method for testing the\n        significance of clustering results. SigClust can be applied to\n        assess the statistical significance of splitting a data set\n        into two clusters. For more than two clusters, SigClust can be\n        used iteratively.",
    "version": "1.1.0.1",
    "maintainer": "Hanwen Huang <hanwenh.unc@gmail.com>",
    "author": "Hanwen Huang, Yufeng Liu & J. S. Marron",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sigclust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sigclust Statistical Significance of Clustering SigClust is a statistical method for testing the\n        significance of clustering results. SigClust can be applied to\n        assess the statistical significance of splitting a data set\n        into two clusters. For more than two clusters, SigClust can be\n        used iteratively.  "
  },
  {
    "id": 20699,
    "package_name": "simCAT",
    "title": "Implements Computerized Adaptive Testing Simulations",
    "description": "Computerized Adaptive Testing simulations with dichotomous and polytomous items. Selects items with Maximum Fisher Information method or randomly, with or without constraints (content balancing and item exposure control). Evaluates the simulation results in terms of precision, item exposure, and test length. Inspired on Magis & Barrada (2017) <doi:10.18637/jss.v076.c01>.",
    "version": "1.0.1",
    "maintainer": "Alexandre Jaloto <alexandrejaloto@gmail.com>",
    "author": "Alexandre Jaloto [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5291-1768>),\n  Ricardo Primi [ths] (ORCID: <https://orcid.org/0000-0003-4227-6745>)",
    "url": "https://github.com/alexandrejaloto/simCAT",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simCAT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simCAT Implements Computerized Adaptive Testing Simulations Computerized Adaptive Testing simulations with dichotomous and polytomous items. Selects items with Maximum Fisher Information method or randomly, with or without constraints (content balancing and item exposure control). Evaluates the simulation results in terms of precision, item exposure, and test length. Inspired on Magis & Barrada (2017) <doi:10.18637/jss.v076.c01>.  "
  },
  {
    "id": 20719,
    "package_name": "simboot",
    "title": "Simultaneous Inference for Diversity Indices",
    "description": "Provides estimation of simultaneous bootstrap and asymptotic confidence intervals for diversity indices, namely the Shannon and the Simpson index. Several pre--specified multiple comparison types are available to choose. Further user--defined contrast matrices are applicable. In addition, simboot estimates adjusted as well as unadjusted p--values for two of the three proposed bootstrap methods. Further simboot allows for comparing biological diversities of two or more groups while simultaneously testing a user-defined selection of Hill numbers of orders q, which are considered as appropriate and useful indices for measuring diversity.",
    "version": "0.2-8",
    "maintainer": "Ralph Scherer <shearer.ra76@gmail.com>",
    "author": "Ralph Scherer [cre, aut],\n  Philip Pallmann [aut]",
    "url": "https://github.com/shearer/simboot,\nhttp://shearer.github.io/simboot/",
    "bug_reports": "https://github.com/shearer/simboot/issues",
    "repository": "https://cran.r-project.org/package=simboot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simboot Simultaneous Inference for Diversity Indices Provides estimation of simultaneous bootstrap and asymptotic confidence intervals for diversity indices, namely the Shannon and the Simpson index. Several pre--specified multiple comparison types are available to choose. Further user--defined contrast matrices are applicable. In addition, simboot estimates adjusted as well as unadjusted p--values for two of the three proposed bootstrap methods. Further simboot allows for comparing biological diversities of two or more groups while simultaneously testing a user-defined selection of Hill numbers of orders q, which are considered as appropriate and useful indices for measuring diversity.  "
  },
  {
    "id": 20723,
    "package_name": "simctest",
    "title": "Safe Implementation of Monte Carlo Tests",
    "description": "Algorithms for the implementation and evaluation of Monte Carlo tests, as well as for their use in multiple testing procedures.",
    "version": "2.6.1",
    "maintainer": "Axel Gandy <a.gandy@imperial.ac.uk>",
    "author": "Axel Gandy [aut, cre],\n  Patrick Rubin-Delanchy [ctb],\n  Georg Hahn [ctb],\n  Dong Ding [ctb]",
    "url": "https://www.ma.imperial.ac.uk/~agandy/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simctest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simctest Safe Implementation of Monte Carlo Tests Algorithms for the implementation and evaluation of Monte Carlo tests, as well as for their use in multiple testing procedures.  "
  },
  {
    "id": 20749,
    "package_name": "simpleFDR",
    "title": "Simple False Discovery Rate Calculation",
    "description": "Using the adjustment method from Benjamini & Hochberg (1995) <doi:10.1111/j.2517-6161.1995.tb02031.x>, this package determines which variables are significant under repeated testing with a given dataframe of p values and an user defined \"q\" threshold.  It then returns the original dataframe along with a significance column where an asterisk denotes a significant p value after FDR calculation, and NA denotes all other p values. This package uses the Benjamini & Hochberg method specifically as described in Lee, S., & Lee, D. K. (2018) <doi:10.4097/kja.d.18.00242>. ",
    "version": "1.1",
    "maintainer": "Stephen Wisser <swisser98@gmail.com>",
    "author": "Stephen C Wisser",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simpleFDR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simpleFDR Simple False Discovery Rate Calculation Using the adjustment method from Benjamini & Hochberg (1995) <doi:10.1111/j.2517-6161.1995.tb02031.x>, this package determines which variables are significant under repeated testing with a given dataframe of p values and an user defined \"q\" threshold.  It then returns the original dataframe along with a significance column where an asterisk denotes a significant p value after FDR calculation, and NA denotes all other p values. This package uses the Benjamini & Hochberg method specifically as described in Lee, S., & Lee, D. K. (2018) <doi:10.4097/kja.d.18.00242>.   "
  },
  {
    "id": 20750,
    "package_name": "simpleMH",
    "title": "Simple Metropolis-Hastings MCMC Algorithm",
    "description": "A very bare-bones interface to use the Metropolis-Hastings Monte \n    Carlo Markov Chain algorithm. It is suitable for teaching and testing \n    purposes. ",
    "version": "0.1.1",
    "maintainer": "Hugo Gruson <hugo.gruson+R@normalesup.org>",
    "author": "Hugo Gruson [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4094-1476>)",
    "url": "https://github.com/Bisaloo/simpleMH",
    "bug_reports": "https://github.com/Bisaloo/simpleMH/issues",
    "repository": "https://cran.r-project.org/package=simpleMH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simpleMH Simple Metropolis-Hastings MCMC Algorithm A very bare-bones interface to use the Metropolis-Hastings Monte \n    Carlo Markov Chain algorithm. It is suitable for teaching and testing \n    purposes.   "
  },
  {
    "id": 20752,
    "package_name": "simplePHENOTYPES",
    "title": "Simulation of Pleiotropic, Linked and Epistatic Phenotypes",
    "description": "The number of studies involving correlated traits and the availability of tools to handle this type of data has increased considerably in the last decade. With such a demand, we need tools for testing hypotheses related to single and multi-trait (correlated) phenotypes based on many genetic settings. Thus, we implemented various options for simulation of pleiotropy and Linkage Disequilibrium under additive, dominance and epistatic models. The simulation currently takes a marker data set as an input and then uses it for simulating multiple traits as described in Fernandes and Lipka (2020) <doi:10.1186/s12859-020-03804-y>.",
    "version": "1.3.0",
    "maintainer": "Samuel Fernandes <samuelf@illinois.edu>",
    "author": "Samuel Fernandes [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8269-535X>),\n  Alexander Lipka [aut] (ORCID: <https://orcid.org/0000-0003-1571-8528>)",
    "url": "https://github.com/samuelbfernandes/simplePHENOTYPES",
    "bug_reports": "https://github.com/samuelbfernandes/simplePHENOTYPES/issues",
    "repository": "https://cran.r-project.org/package=simplePHENOTYPES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simplePHENOTYPES Simulation of Pleiotropic, Linked and Epistatic Phenotypes The number of studies involving correlated traits and the availability of tools to handle this type of data has increased considerably in the last decade. With such a demand, we need tools for testing hypotheses related to single and multi-trait (correlated) phenotypes based on many genetic settings. Thus, we implemented various options for simulation of pleiotropy and Linkage Disequilibrium under additive, dominance and epistatic models. The simulation currently takes a marker data set as an input and then uses it for simulating multiple traits as described in Fernandes and Lipka (2020) <doi:10.1186/s12859-020-03804-y>.  "
  },
  {
    "id": 20782,
    "package_name": "simulMGF",
    "title": "Simulate SNP Matrix, Phenotype and Genotypic Effects",
    "description": "Simulate genotypes in SNP (single nucleotide polymorphisms) Matrix as random numbers from an uniform distribution, for diploid organisms (coded by 0, 1, 2), Sikorska et al., (2013) <doi:10.1186/1471-2105-14-166>, or half-sib/full-sib SNP matrix from real or simulated parents SNP data, assuming mendelian segregation. Simulate phenotypic traits for real or simulated SNP data, controlled by a specific number of quantitative trait loci  and their effects, sampled from a Normal or an Uniform distributions, assuming a pure additive model. This is useful for testing association and genomic prediction models or for educational purposes.",
    "version": "0.1.1",
    "maintainer": "Martin Nahuel Garcia <garcia.martin@inta.gob.ar>",
    "author": "Martin Nahuel Garcia [aut, cre]",
    "url": "https://github.com/mngar/simulMGF",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simulMGF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simulMGF Simulate SNP Matrix, Phenotype and Genotypic Effects Simulate genotypes in SNP (single nucleotide polymorphisms) Matrix as random numbers from an uniform distribution, for diploid organisms (coded by 0, 1, 2), Sikorska et al., (2013) <doi:10.1186/1471-2105-14-166>, or half-sib/full-sib SNP matrix from real or simulated parents SNP data, assuming mendelian segregation. Simulate phenotypic traits for real or simulated SNP data, controlled by a specific number of quantitative trait loci  and their effects, sampled from a Normal or an Uniform distributions, assuming a pure additive model. This is useful for testing association and genomic prediction models or for educational purposes.  "
  },
  {
    "id": 20792,
    "package_name": "singcar",
    "title": "Comparing Single Cases to Small Samples",
    "description": "When comparing single cases to control populations and no parameters are known researchers and clinicians must estimate these with a control sample. This is often done when testing a case's abnormality on some variable or testing abnormality of the discrepancy between two variables. Appropriate frequentist and Bayesian methods for doing this are here implemented, including tests allowing for the inclusion of covariates. These have been developed first and foremost by John Crawford and Paul Garthwaite, e.g. in Crawford and Howell (1998) <doi:10.1076/clin.12.4.482.7241>, Crawford and Garthwaite (2005) <doi:10.1037/0894-4105.19.3.318>, Crawford and Garthwaite (2007) <doi:10.1080/02643290701290146> and Crawford, Garthwaite and Ryan (2011) <doi:10.1016/j.cortex.2011.02.017>. The package is also equipped with power calculators for each method. ",
    "version": "0.1.5",
    "maintainer": "Jonathan Rittmo <j.rittmo@gmail.com>",
    "author": "Jonathan Rittmo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5075-0166>),\n  Robert McIntosh [aut] (ORCID: <https://orcid.org/0000-0002-7615-6699>)",
    "url": "https://github.com/jorittmo/singcar",
    "bug_reports": "https://github.com/jorittmo/singcar/issues",
    "repository": "https://cran.r-project.org/package=singcar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "singcar Comparing Single Cases to Small Samples When comparing single cases to control populations and no parameters are known researchers and clinicians must estimate these with a control sample. This is often done when testing a case's abnormality on some variable or testing abnormality of the discrepancy between two variables. Appropriate frequentist and Bayesian methods for doing this are here implemented, including tests allowing for the inclusion of covariates. These have been developed first and foremost by John Crawford and Paul Garthwaite, e.g. in Crawford and Howell (1998) <doi:10.1076/clin.12.4.482.7241>, Crawford and Garthwaite (2005) <doi:10.1037/0894-4105.19.3.318>, Crawford and Garthwaite (2007) <doi:10.1080/02643290701290146> and Crawford, Garthwaite and Ryan (2011) <doi:10.1016/j.cortex.2011.02.017>. The package is also equipped with power calculators for each method.   "
  },
  {
    "id": 20824,
    "package_name": "skater",
    "title": "Utilities for SNP-Based Kinship Analysis",
    "description": "Utilities for single nucleotide polymorphism (SNP) based kinship analysis\n    testing and evaluation. The 'skater' package contains functions for importing, parsing, \n    and analyzing pedigree data, performing relationship degree inference, benchmarking \n    relationship degree classification, and summarizing identity by descent (IBD) segment data.\n    Package functions and methods are described in Turner et al. (2021) \"skater: An R package \n    for SNP-based Kinship Analysis, Testing, and Evaluation\" <doi:10.1101/2021.07.21.453083>.",
    "version": "0.1.2",
    "maintainer": "Stephen Turner <vustephen@gmail.com>",
    "author": "Stephen Turner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9140-9028>),\n  Matthew Scholz [aut] (ORCID: <https://orcid.org/0000-0003-3686-1227>),\n  VP Nagraj [aut] (ORCID: <https://orcid.org/0000-0003-0060-566X>),\n  Signature Science, LLC. [cph]",
    "url": "https://github.com/signaturescience/skater",
    "bug_reports": "https://github.com/signaturescience/skater/issues",
    "repository": "https://cran.r-project.org/package=skater",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "skater Utilities for SNP-Based Kinship Analysis Utilities for single nucleotide polymorphism (SNP) based kinship analysis\n    testing and evaluation. The 'skater' package contains functions for importing, parsing, \n    and analyzing pedigree data, performing relationship degree inference, benchmarking \n    relationship degree classification, and summarizing identity by descent (IBD) segment data.\n    Package functions and methods are described in Turner et al. (2021) \"skater: An R package \n    for SNP-based Kinship Analysis, Testing, and Evaluation\" <doi:10.1101/2021.07.21.453083>.  "
  },
  {
    "id": 20825,
    "package_name": "skedastic",
    "title": "Handling Heteroskedasticity in the Linear Regression Model",
    "description": "Implements numerous methods for testing for, modelling, and \n    correcting for heteroskedasticity in the classical linear regression \n    model. The most novel contribution of the \n    package is found in the functions that implement the as-yet-unpublished \n    auxiliary linear variance models and auxiliary nonlinear variance \n    models that are designed to estimate error variances in a heteroskedastic \n    linear regression model. These models follow principles of statistical \n    learning described in Hastie (2009) <doi:10.1007/978-0-387-21606-5>. \n    The nonlinear version of the model is estimated using quasi-likelihood \n    methods as described in Seber and Wild (2003, ISBN: 0-471-47135-6).\n    Bootstrap methods for approximate confidence intervals for error variances \n    are implemented as described in Efron and Tibshirani \n    (1993, ISBN: 978-1-4899-4541-9), including also the expansion technique \n    described in Hesterberg (2014) <doi:10.1080/00031305.2015.1089789>. The \n    wild bootstrap employed here follows the description in Davidson and \n    Flachaire (2008) <doi:10.1016/j.jeconom.2008.08.003>. Tuning of \n    hyper-parameters makes use of a golden section search function that is \n    modelled after the MATLAB function of Zarnowiec (2022) \n    <https://www.mathworks.com/matlabcentral/fileexchange/25919-golden-section-method-algorithm>.\n    A methodological description of the algorithm can be found in Fox (2021, \n    ISBN: 978-1-003-00957-3).\n    There are 25 different functions that implement hypothesis tests for \n    heteroskedasticity. These include a test based on Anscombe (1961) \n    <https://projecteuclid.org/euclid.bsmsp/1200512155>, Ramsey's (1969) \n    BAMSET Test <doi:10.1111/j.2517-6161.1969.tb00796.x>, the tests of Bickel \n    (1978) <doi:10.1214/aos/1176344124>, Breusch and Pagan (1979)  \n    <doi:10.2307/1911963> with and without the modification \n    proposed by Koenker (1981) <doi:10.1016/0304-4076(81)90062-2>, Carapeto and \n    Holt (2003) <doi:10.1080/0266476022000018475>, Cook and Weisberg (1983) \n    <doi:10.1093/biomet/70.1.1> (including their graphical methods), Diblasi \n    and Bowman (1997) <doi:10.1016/S0167-7152(96)00115-0>, Dufour, Khalaf, \n    Bernard, and Genest (2004) <doi:10.1016/j.jeconom.2003.10.024>, Evans and \n    King (1985) <doi:10.1016/0304-4076(85)90085-5> and Evans and King (1988) \n    <doi:10.1016/0304-4076(88)90006-1>, Glejser (1969) \n    <doi:10.1080/01621459.1969.10500976> as formulated by \n    Mittelhammer, Judge and Miller (2000, ISBN: 0-521-62394-4), Godfrey and \n    Orme (1999) <doi:10.1080/07474939908800438>, Goldfeld and Quandt \n    (1965) <doi:10.1080/01621459.1965.10480811>, Harrison and McCabe (1979) \n    <doi:10.1080/01621459.1979.10482544>, Harvey (1976) <doi:10.2307/1913974>, \n    Honda (1989) <doi:10.1111/j.2517-6161.1989.tb01749.x>, Horn (1981) \n    <doi:10.1080/03610928108828074>, Li and Yao (2019) \n    <doi:10.1016/j.ecosta.2018.01.001> with and without the modification of \n    Bai, Pan, and Yin (2016) <doi:10.1007/s11749-017-0575-x>, Rackauskas and \n    Zuokas (2007) <doi:10.1007/s10986-007-0018-6>, Simonoff and Tsai (1994) \n    <doi:10.2307/2986026> with and without the modification of Ferrari, \n    Cysneiros, and Cribari-Neto (2004) <doi:10.1016/S0378-3758(03)00210-6>, \n    Szroeter (1978) <doi:10.2307/1913831>, Verbyla (1993) \n    <doi:10.1111/j.2517-6161.1993.tb01918.x>, White (1980) \n    <doi:10.2307/1912934>, Wilcox and Keselman (2006) \n    <doi:10.1080/10629360500107923>, Yuce (2008) \n    <https://dergipark.org.tr/en/pub/iuekois/issue/8989/112070>, and Zhou, \n    Song, and Thompson (2015) <doi:10.1002/cjs.11252>. Besides these \n    heteroskedasticity tests, there are supporting functions that compute the \n    BLUS residuals of Theil (1965) <doi:10.1080/01621459.1965.10480851>, the \n    conditional two-sided p-values of Kulinskaya (2008) <doi:10.48550/arXiv.0810.2124>, \n    and probabilities for the nonparametric trend statistic of Lehmann (1975, \n    ISBN: 0-816-24996-1). For handling heteroskedasticity, in addition to the \n    new auxiliary variance model methods, there is a function \n    to implement various existing Heteroskedasticity-Consistent Covariance \n    Matrix Estimators from the literature, such as those of White (1980) \n    <doi:10.2307/1912934>, MacKinnon and White (1985) \n    <doi:10.1016/0304-4076(85)90158-7>, Cribari-Neto (2004) \n    <doi:10.1016/S0167-9473(02)00366-3>, Cribari-Neto et al. (2007) \n    <doi:10.1080/03610920601126589>, Cribari-Neto and da Silva (2011) \n    <doi:10.1007/s10182-010-0141-2>, Aftab and Chang (2016) \n    <doi:10.18187/pjsor.v12i2.983>, and Li et al. (2017) \n    <doi:10.1080/00949655.2016.1198906>. ",
    "version": "2.0.3",
    "maintainer": "Thomas Farrar <tjfarrar@alumni.uwaterloo.ca>",
    "author": "Thomas Farrar [aut, cre] (0000-0003-0744-6972),\n  University of the Western Cape [cph]",
    "url": "https://github.com/tjfarrar/skedastic",
    "bug_reports": "https://github.com/tjfarrar/skedastic/issues",
    "repository": "https://cran.r-project.org/package=skedastic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "skedastic Handling Heteroskedasticity in the Linear Regression Model Implements numerous methods for testing for, modelling, and \n    correcting for heteroskedasticity in the classical linear regression \n    model. The most novel contribution of the \n    package is found in the functions that implement the as-yet-unpublished \n    auxiliary linear variance models and auxiliary nonlinear variance \n    models that are designed to estimate error variances in a heteroskedastic \n    linear regression model. These models follow principles of statistical \n    learning described in Hastie (2009) <doi:10.1007/978-0-387-21606-5>. \n    The nonlinear version of the model is estimated using quasi-likelihood \n    methods as described in Seber and Wild (2003, ISBN: 0-471-47135-6).\n    Bootstrap methods for approximate confidence intervals for error variances \n    are implemented as described in Efron and Tibshirani \n    (1993, ISBN: 978-1-4899-4541-9), including also the expansion technique \n    described in Hesterberg (2014) <doi:10.1080/00031305.2015.1089789>. The \n    wild bootstrap employed here follows the description in Davidson and \n    Flachaire (2008) <doi:10.1016/j.jeconom.2008.08.003>. Tuning of \n    hyper-parameters makes use of a golden section search function that is \n    modelled after the MATLAB function of Zarnowiec (2022) \n    <https://www.mathworks.com/matlabcentral/fileexchange/25919-golden-section-method-algorithm>.\n    A methodological description of the algorithm can be found in Fox (2021, \n    ISBN: 978-1-003-00957-3).\n    There are 25 different functions that implement hypothesis tests for \n    heteroskedasticity. These include a test based on Anscombe (1961) \n    <https://projecteuclid.org/euclid.bsmsp/1200512155>, Ramsey's (1969) \n    BAMSET Test <doi:10.1111/j.2517-6161.1969.tb00796.x>, the tests of Bickel \n    (1978) <doi:10.1214/aos/1176344124>, Breusch and Pagan (1979)  \n    <doi:10.2307/1911963> with and without the modification \n    proposed by Koenker (1981) <doi:10.1016/0304-4076(81)90062-2>, Carapeto and \n    Holt (2003) <doi:10.1080/0266476022000018475>, Cook and Weisberg (1983) \n    <doi:10.1093/biomet/70.1.1> (including their graphical methods), Diblasi \n    and Bowman (1997) <doi:10.1016/S0167-7152(96)00115-0>, Dufour, Khalaf, \n    Bernard, and Genest (2004) <doi:10.1016/j.jeconom.2003.10.024>, Evans and \n    King (1985) <doi:10.1016/0304-4076(85)90085-5> and Evans and King (1988) \n    <doi:10.1016/0304-4076(88)90006-1>, Glejser (1969) \n    <doi:10.1080/01621459.1969.10500976> as formulated by \n    Mittelhammer, Judge and Miller (2000, ISBN: 0-521-62394-4), Godfrey and \n    Orme (1999) <doi:10.1080/07474939908800438>, Goldfeld and Quandt \n    (1965) <doi:10.1080/01621459.1965.10480811>, Harrison and McCabe (1979) \n    <doi:10.1080/01621459.1979.10482544>, Harvey (1976) <doi:10.2307/1913974>, \n    Honda (1989) <doi:10.1111/j.2517-6161.1989.tb01749.x>, Horn (1981) \n    <doi:10.1080/03610928108828074>, Li and Yao (2019) \n    <doi:10.1016/j.ecosta.2018.01.001> with and without the modification of \n    Bai, Pan, and Yin (2016) <doi:10.1007/s11749-017-0575-x>, Rackauskas and \n    Zuokas (2007) <doi:10.1007/s10986-007-0018-6>, Simonoff and Tsai (1994) \n    <doi:10.2307/2986026> with and without the modification of Ferrari, \n    Cysneiros, and Cribari-Neto (2004) <doi:10.1016/S0378-3758(03)00210-6>, \n    Szroeter (1978) <doi:10.2307/1913831>, Verbyla (1993) \n    <doi:10.1111/j.2517-6161.1993.tb01918.x>, White (1980) \n    <doi:10.2307/1912934>, Wilcox and Keselman (2006) \n    <doi:10.1080/10629360500107923>, Yuce (2008) \n    <https://dergipark.org.tr/en/pub/iuekois/issue/8989/112070>, and Zhou, \n    Song, and Thompson (2015) <doi:10.1002/cjs.11252>. Besides these \n    heteroskedasticity tests, there are supporting functions that compute the \n    BLUS residuals of Theil (1965) <doi:10.1080/01621459.1965.10480851>, the \n    conditional two-sided p-values of Kulinskaya (2008) <doi:10.48550/arXiv.0810.2124>, \n    and probabilities for the nonparametric trend statistic of Lehmann (1975, \n    ISBN: 0-816-24996-1). For handling heteroskedasticity, in addition to the \n    new auxiliary variance model methods, there is a function \n    to implement various existing Heteroskedasticity-Consistent Covariance \n    Matrix Estimators from the literature, such as those of White (1980) \n    <doi:10.2307/1912934>, MacKinnon and White (1985) \n    <doi:10.1016/0304-4076(85)90158-7>, Cribari-Neto (2004) \n    <doi:10.1016/S0167-9473(02)00366-3>, Cribari-Neto et al. (2007) \n    <doi:10.1080/03610920601126589>, Cribari-Neto and da Silva (2011) \n    <doi:10.1007/s10182-010-0141-2>, Aftab and Chang (2016) \n    <doi:10.18187/pjsor.v12i2.983>, and Li et al. (2017) \n    <doi:10.1080/00949655.2016.1198906>.   "
  },
  {
    "id": 20826,
    "package_name": "skeletor",
    "title": "An R Package Skeleton Generator",
    "description": "A tool for bootstrapping new packages with useful defaults,\n    including a test suite outline that passes checks and helpers for running\n    tests, checking test coverage, building vignettes, and more. Package\n    skeletons it creates are set up for pushing your package to\n    'GitHub' and using other hosted services for building and test automation.",
    "version": "1.0.4",
    "maintainer": "Neal Richardson <neal.p.richardson@gmail.com>",
    "author": "Neal Richardson [aut, cre]",
    "url": "https://github.com/nealrichardson/skeletor",
    "bug_reports": "https://github.com/nealrichardson/skeletor/issues",
    "repository": "https://cran.r-project.org/package=skeletor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "skeletor An R Package Skeleton Generator A tool for bootstrapping new packages with useful defaults,\n    including a test suite outline that passes checks and helpers for running\n    tests, checking test coverage, building vignettes, and more. Package\n    skeletons it creates are set up for pushing your package to\n    'GitHub' and using other hosted services for building and test automation.  "
  },
  {
    "id": 20884,
    "package_name": "smatr",
    "title": "(Standardised) Major Axis Estimation and Testing Routines",
    "description": "Methods for fitting bivariate lines in\n    allometry using the major axis (MA) or standardised major axis (SMA), and\n    for making inferences about such lines. The available methods of inference\n    include confidence intervals and one-sample tests for slope and elevation,\n    testing for a common slope or elevation amongst several allometric lines,\n    constructing a confidence interval for a common slope or elevation, and\n    testing for no shift along a common axis, amongst several samples. \n    See Warton et al. 2012 <doi:10.1111/j.2041-210X.2011.00153.x> for methods description.",
    "version": "3.4-8",
    "maintainer": "Remko Duursma <remkoduursma@gmail.com>",
    "author": "David Warton <David.Warton@unsw.edu.au>, Remko Duursma, Daniel Falster\n    and Sara Taskinen.",
    "url": "http://web.maths.unsw.edu.au/~dwarton,\nhttp://www.bitbucket.org/remkoduursma/smatr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=smatr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smatr (Standardised) Major Axis Estimation and Testing Routines Methods for fitting bivariate lines in\n    allometry using the major axis (MA) or standardised major axis (SMA), and\n    for making inferences about such lines. The available methods of inference\n    include confidence intervals and one-sample tests for slope and elevation,\n    testing for a common slope or elevation amongst several allometric lines,\n    constructing a confidence interval for a common slope or elevation, and\n    testing for no shift along a common axis, amongst several samples. \n    See Warton et al. 2012 <doi:10.1111/j.2041-210X.2011.00153.x> for methods description.  "
  },
  {
    "id": 20920,
    "package_name": "smoots",
    "title": "Nonparametric Estimation of the Trend and Its Derivatives in TS",
    "description": "The nonparametric trend and its derivatives in equidistant time \n    series (TS) with short-memory stationary errors can be estimated. The \n    estimation is conducted via local polynomial regression using an \n    automatically selected bandwidth obtained by a built-in iterative plug-in \n    algorithm or a bandwidth fixed by the user. A Nadaraya-Watson kernel \n    smoother is also built-in as a comparison. With version 1.1.0, a linearity \n    test for the trend function, forecasting methods and backtesting \n    approaches are implemented as well.\n    The smoothing methods of the package are described in Feng, Y., Gries, T., \n    and Fritz, M. (2020) <doi:10.1080/10485252.2020.1759598>.",
    "version": "1.1.4",
    "maintainer": "Dominik Schulz <schulzd@mail.uni-paderborn.de>",
    "author": "Yuanhua Feng [aut] (Paderborn University, Germany),\n  Sebastian Letmathe [aut] (Paderborn University, Germany),\n  Dominik Schulz [aut, cre] (Paderborn University, Germany),\n  Thomas Gries [ctb] (Paderborn University, Germany),\n  Marlon Fritz [ctb] (Paderborn University, Germany)",
    "url": "https://wiwi.uni-paderborn.de/en/dep4/feng/\nhttps://wiwi.uni-paderborn.de/dep4/gries/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=smoots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smoots Nonparametric Estimation of the Trend and Its Derivatives in TS The nonparametric trend and its derivatives in equidistant time \n    series (TS) with short-memory stationary errors can be estimated. The \n    estimation is conducted via local polynomial regression using an \n    automatically selected bandwidth obtained by a built-in iterative plug-in \n    algorithm or a bandwidth fixed by the user. A Nadaraya-Watson kernel \n    smoother is also built-in as a comparison. With version 1.1.0, a linearity \n    test for the trend function, forecasting methods and backtesting \n    approaches are implemented as well.\n    The smoothing methods of the package are described in Feng, Y., Gries, T., \n    and Fritz, M. (2020) <doi:10.1080/10485252.2020.1759598>.  "
  },
  {
    "id": 20923,
    "package_name": "smovie",
    "title": "Some Movies to Illustrate Concepts in Statistics",
    "description": "Provides movies to help students to understand statistical \n  concepts.  The 'rpanel' package  <https://cran.r-project.org/package=rpanel> \n  is used to create interactive plots that move to illustrate key statistical \n  ideas and methods.  There are movies to: visualise probability distributions\n  (including user-supplied ones); illustrate sampling distributions of the\n  sample mean (central limit theorem), the median, the sample maximum \n  (extremal types theorem) and (the Fisher transformation of the) \n  product moment correlation coefficient; examine the influence of an \n  individual observation in simple linear regression; illustrate key concepts \n  in statistical hypothesis testing. Also provided are dpqr functions for the \n  distribution of the Fisher transformation of the correlation coefficient \n  under sampling from a bivariate normal distribution.",
    "version": "1.1.6",
    "maintainer": "Paul J. Northrop <p.northrop@ucl.ac.uk>",
    "author": "Paul J. Northrop [aut, cre, cph]",
    "url": "https://paulnorthrop.github.io/smovie/,\nhttps://github.com/paulnorthrop/smovie/",
    "bug_reports": "https://github.com/paulnorthrop/smovie/issues",
    "repository": "https://cran.r-project.org/package=smovie",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "smovie Some Movies to Illustrate Concepts in Statistics Provides movies to help students to understand statistical \n  concepts.  The 'rpanel' package  <https://cran.r-project.org/package=rpanel> \n  is used to create interactive plots that move to illustrate key statistical \n  ideas and methods.  There are movies to: visualise probability distributions\n  (including user-supplied ones); illustrate sampling distributions of the\n  sample mean (central limit theorem), the median, the sample maximum \n  (extremal types theorem) and (the Fisher transformation of the) \n  product moment correlation coefficient; examine the influence of an \n  individual observation in simple linear regression; illustrate key concepts \n  in statistical hypothesis testing. Also provided are dpqr functions for the \n  distribution of the Fisher transformation of the correlation coefficient \n  under sampling from a bivariate normal distribution.  "
  },
  {
    "id": 20996,
    "package_name": "someMTP",
    "title": "Some Multiple Testing Procedures",
    "description": "It's a collection of functions for Multiplicity Correction and Multiple Testing.",
    "version": "1.4.1.1",
    "maintainer": "livio finos <livio@stat.unipd.it>",
    "author": "livio finos",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=someMTP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "someMTP Some Multiple Testing Procedures It's a collection of functions for Multiplicity Correction and Multiple Testing.  "
  },
  {
    "id": 21054,
    "package_name": "spaero",
    "title": "Software for Project AERO",
    "description": "Implements methods for anticipating the emergence and eradication\n    of infectious diseases from surveillance time series. Also provides support\n    for computational experiments testing the performance of such methods.",
    "version": "0.6.0",
    "maintainer": "Eamon O'Dea <odea35@gmail.com>",
    "author": "Eamon O'Dea [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/e3bo/spaero/issues/",
    "repository": "https://cran.r-project.org/package=spaero",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spaero Software for Project AERO Implements methods for anticipating the emergence and eradication\n    of infectious diseases from surveillance time series. Also provides support\n    for computational experiments testing the performance of such methods.  "
  },
  {
    "id": 21186,
    "package_name": "speff2trial",
    "title": "Semiparametric Efficient Estimation for a Two-Sample Treatment\nEffect",
    "description": "Performs estimation and testing of the treatment effect in a 2-group randomized clinical trial with a quantitative, dichotomous, or right-censored time-to-event endpoint. The method improves efficiency by leveraging baseline predictors of the endpoint. The inverse probability weighting technique of Robins, Rotnitzky, and Zhao (JASA, 1994) is used to provide unbiased estimation when the endpoint is missing at random.",
    "version": "1.0.5",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "author": "Michal Juraska <mjuraska@fredhutch.org>, with contributions from Peter B. Gilbert <pgilbert@scharp.org>, Xiaomin \n  Lu <xlu2@phhp.ufl.edu>, Min Zhang <mzhangst@umich.edu>, Marie Davidian <davidian@stat.ncsu.edu>, and Anastasios A. Tsiatis <tsiatis@stat.ncsu.edu>",
    "url": "https://github.com/mjuraska/speff2trial",
    "bug_reports": "https://github.com/mjuraska/speff2trial/issues",
    "repository": "https://cran.r-project.org/package=speff2trial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "speff2trial Semiparametric Efficient Estimation for a Two-Sample Treatment\nEffect Performs estimation and testing of the treatment effect in a 2-group randomized clinical trial with a quantitative, dichotomous, or right-censored time-to-event endpoint. The method improves efficiency by leveraging baseline predictors of the endpoint. The inverse probability weighting technique of Robins, Rotnitzky, and Zhao (JASA, 1994) is used to provide unbiased estimation when the endpoint is missing at random.  "
  },
  {
    "id": 21196,
    "package_name": "spgs",
    "title": "Statistical Patterns in Genomic Sequences",
    "description": "A collection of statistical hypothesis tests and other \n\ttechniques for identifying certain spatial relationships/phenomena in \n\tDNA sequences. In particular, it provides tests and graphical methods for determining \n\twhether or not DNA sequences comply with Chargaff's second parity rule \n\tor exhibit purine-pyrimidine parity. In addition, there are functions for \n\tefficiently simulating discrete state space Markov chains and testing \n\tarbitrary symbolic sequences of symbols for the presence of first-order \n\tMarkovianness.\n\tAlso, it has functions for counting words/k-mers (and cylinder patterns) in \n\tarbitrary symbolic sequences. Functions which take a DNA sequence as input \n\tcan handle sequences stored as SeqFastadna objects from the 'seqinr' package.",
    "version": "1.0-4",
    "maintainer": "Andrew Hart <ahart@dim.uchile.cl>",
    "author": "Andrew Hart [aut, cre],\n  Servet Mart\u00ednez [aut],\n  Universidad de Chile [cph],\n  INRIA-Chile [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=spgs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spgs Statistical Patterns in Genomic Sequences A collection of statistical hypothesis tests and other \n\ttechniques for identifying certain spatial relationships/phenomena in \n\tDNA sequences. In particular, it provides tests and graphical methods for determining \n\twhether or not DNA sequences comply with Chargaff's second parity rule \n\tor exhibit purine-pyrimidine parity. In addition, there are functions for \n\tefficiently simulating discrete state space Markov chains and testing \n\tarbitrary symbolic sequences of symbols for the presence of first-order \n\tMarkovianness.\n\tAlso, it has functions for counting words/k-mers (and cylinder patterns) in \n\tarbitrary symbolic sequences. Functions which take a DNA sequence as input \n\tcan handle sequences stored as SeqFastadna objects from the 'seqinr' package.  "
  },
  {
    "id": 21207,
    "package_name": "spider",
    "title": "Species Identity and Evolution in R",
    "description": "Analysis of species limits and DNA barcoding data. Included are functions for generating important summary statistics from DNA barcode data, assessing specimen identification efficacy, testing and optimizing divergence threshold limits, assessment of diagnostic nucleotides, and calculation of the probability of reciprocal monophyly. Additionally, a sliding window function offers opportunities to analyse information across a gene, often used for marker design in degraded DNA studies. Further information on the package has been published in Brown et al (2012) <doi:10.1111/j.1755-0998.2011.03108.x>.",
    "version": "1.5.1",
    "maintainer": "Rupert A. Collins <rupertcollins@gmail.com>",
    "author": "Samuel Brown [aut],\n  Stephane Boyer [aut],\n  Marie-Caroline Lefort [aut],\n  Jagoba Malumbres-Olarte [aut],\n  Cor Vink [aut],\n  Rob Cruickshank [aut],\n  Rupert A. Collins [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-9135-1169>)",
    "url": "https://github.com/boopsboops/spider",
    "bug_reports": "https://github.com/boopsboops/spider/issues",
    "repository": "https://cran.r-project.org/package=spider",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spider Species Identity and Evolution in R Analysis of species limits and DNA barcoding data. Included are functions for generating important summary statistics from DNA barcode data, assessing specimen identification efficacy, testing and optimizing divergence threshold limits, assessment of diagnostic nucleotides, and calculation of the probability of reciprocal monophyly. Additionally, a sliding window function offers opportunities to analyse information across a gene, often used for marker design in degraded DNA studies. Further information on the package has been published in Brown et al (2012) <doi:10.1111/j.1755-0998.2011.03108.x>.  "
  },
  {
    "id": 21233,
    "package_name": "splm",
    "title": "Econometric Models for Spatial Panel Data",
    "description": "ML and GM estimation and diagnostic testing of econometric models for spatial panel data.",
    "version": "1.6-5",
    "maintainer": "Giovanni Millo <giovanni.millo@deams.units.it>",
    "author": "Giovanni Millo [aut, cre],\n  Gianfranco Piras [aut],\n  Roger Bivand [ctb] (ORCID: <https://orcid.org/0000-0003-2392-6140>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=splm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "splm Econometric Models for Spatial Panel Data ML and GM estimation and diagnostic testing of econometric models for spatial panel data.  "
  },
  {
    "id": 21259,
    "package_name": "spqdep",
    "title": "Testing for Spatial Independence of Cross-Sectional Qualitative\nData",
    "description": "Testing for Spatial Dependence of Qualitative Data in Cross Section. The list of functions includes join-count tests, Q test, spatial scan test, similarity test and spatial runs test. The methodology of these models can be found in <doi:10.1007/s10109-009-0100-1> and <doi:10.1080/13658816.2011.586327>.",
    "version": "0.1.3.6",
    "maintainer": "Fernando Lopez <fernando.lopez@upct.es>",
    "author": "Fernando Lopez [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5397-9748>),\n  Roman Minguez [aut] (ORCID: <https://orcid.org/0000-0002-0490-3181>),\n  Antonio Paez [aut] (ORCID: <https://orcid.org/0000-0001-6912-9919>),\n  Manuel Ruiz [aut] (ORCID: <https://orcid.org/0000-0001-9228-6410>)",
    "url": "https://f8l5h9.github.io/spqdep/",
    "bug_reports": "https://github.com/f8l5h9/spqdep/issues",
    "repository": "https://cran.r-project.org/package=spqdep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spqdep Testing for Spatial Independence of Cross-Sectional Qualitative\nData Testing for Spatial Dependence of Qualitative Data in Cross Section. The list of functions includes join-count tests, Q test, spatial scan test, similarity test and spatial runs test. The methodology of these models can be found in <doi:10.1007/s10109-009-0100-1> and <doi:10.1080/13658816.2011.586327>.  "
  },
  {
    "id": 21266,
    "package_name": "sprtt",
    "title": "Sequential Probability Ratio Tests Toolbox",
    "description": "It is a toolbox for Sequential Probability Ratio Tests (SPRT),  Wald (1945) <doi:10.2134/agronj1947.00021962003900070011x>.\n    SPRTs are applied to the data during the sampling process, ideally after each observation.\n    At any stage, the test will return a decision to either continue sampling or terminate and accept one of the specified hypotheses.\n    The seq_ttest() function performs one-sample, two-sample, and paired t-tests for testing one- and two-sided hypotheses (Schnuerch & Erdfelder (2019) <doi:10.1037/met0000234>).\n    The seq_anova() function allows to perform a sequential one-way fixed effects ANOVA (Steinhilber et al. (2023) <doi:10.31234/osf.io/m64ne>).\n    Learn more about the package by using vignettes \"browseVignettes(package = \"sprtt\")\" or go to the website <https://meikesteinhilber.github.io/sprtt/>.",
    "version": "0.2.0",
    "maintainer": "Meike Steinhilber <Meike.Steinhilber@aol.com>",
    "author": "Meike Steinhilber [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7144-2100>),\n  Martin Schnuerch [aut, ths] (ORCID:\n    <https://orcid.org/0000-0001-6531-2265>),\n  Anna-Lena Schubert [aut, ths] (ORCID:\n    <https://orcid.org/0000-0001-7248-0662>)",
    "url": "https://meikesteinhilber.github.io/sprtt/",
    "bug_reports": "https://github.com/MeikeSteinhilber/sprtt/issues",
    "repository": "https://cran.r-project.org/package=sprtt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sprtt Sequential Probability Ratio Tests Toolbox It is a toolbox for Sequential Probability Ratio Tests (SPRT),  Wald (1945) <doi:10.2134/agronj1947.00021962003900070011x>.\n    SPRTs are applied to the data during the sampling process, ideally after each observation.\n    At any stage, the test will return a decision to either continue sampling or terminate and accept one of the specified hypotheses.\n    The seq_ttest() function performs one-sample, two-sample, and paired t-tests for testing one- and two-sided hypotheses (Schnuerch & Erdfelder (2019) <doi:10.1037/met0000234>).\n    The seq_anova() function allows to perform a sequential one-way fixed effects ANOVA (Steinhilber et al. (2023) <doi:10.31234/osf.io/m64ne>).\n    Learn more about the package by using vignettes \"browseVignettes(package = \"sprtt\")\" or go to the website <https://meikesteinhilber.github.io/sprtt/>.  "
  },
  {
    "id": 21305,
    "package_name": "ss3sim",
    "title": "Fisheries Stock Assessment Simulation Testing with Stock\nSynthesis",
    "description": "Develops a framework for fisheries stock assessment simulation\n    testing with Stock Synthesis (SS) as described in Anderson et al.\n    (2014) <doi:10.1371/journal.pone.0092725>.",
    "version": "1.0.3",
    "maintainer": "Kelli F. Johnson <kelli.johnson@noaa.gov>",
    "author": "Kelli F. Johnson [aut, cre],\n  Sean C. Anderson [aut] (ORCID: <https://orcid.org/0000-0001-9563-1937>),\n  Kathryn Doering [aut],\n  Cole Monnahan [aut],\n  Christine Stawitz [aut],\n  Ian Taylor [aut],\n  Curry Cunningham [ctb],\n  Allan Hicks [ctb],\n  Felipe Hurtado-Ferro [ctb],\n  Peter Kuriyama [ctb],\n  Roberto Licandeo [ctb],\n  Carey McGilliard [ctb],\n  Melissa Murdian [ctb],\n  Kotaro Ono [ctb],\n  Merrill Rudd [ctb],\n  Cody Szuwalski [ctb],\n  Juan Valero [ctb],\n  Athol Whitten [ctb]",
    "url": "https://github.com/ss3sim/ss3sim",
    "bug_reports": "https://github.com/ss3sim/ss3sim/issues",
    "repository": "https://cran.r-project.org/package=ss3sim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ss3sim Fisheries Stock Assessment Simulation Testing with Stock\nSynthesis Develops a framework for fisheries stock assessment simulation\n    testing with Stock Synthesis (SS) as described in Anderson et al.\n    (2014) <doi:10.1371/journal.pone.0092725>.  "
  },
  {
    "id": 21312,
    "package_name": "sscor",
    "title": "Robust Correlation Estimation and Testing Based on Spatial Signs",
    "description": "Provides the spatial sign correlation and the two-stage spatial sign correlation as well as a one-sample test for the correlation coefficient.",
    "version": "0.2.1",
    "maintainer": "Alexander Duerre <alexander.duerre@tu-dortmund.de>",
    "author": "Alexander Duerre [aut, cre],\n  Daniel Vogel [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sscor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sscor Robust Correlation Estimation and Testing Based on Spatial Signs Provides the spatial sign correlation and the two-stage spatial sign correlation as well as a one-sample test for the correlation coefficient.  "
  },
  {
    "id": 21395,
    "package_name": "statcomp",
    "title": "Statistical Complexity and Information Measures for Time Series\nAnalysis",
    "description": "An implementation of local and global statistical complexity measures (aka Information Theory Quantifiers, ITQ) for time series analysis based on ordinal statistics (Bandt and Pompe (2002) <DOI:10.1103/PhysRevLett.88.174102>). Several distance measures that operate on ordinal pattern distributions, auxiliary functions for ordinal pattern analysis, and generating functions for stochastic and deterministic-chaotic processes for ITQ testing are provided. ",
    "version": "0.1.0",
    "maintainer": "Sebastian Sippel <sebastian.sippel@env.ethz.ch>",
    "author": "Sebastian Sippel [aut, cre] (Original package development was supported\n    by MPI Biogeochemistry, BGI Department),\n  Holger Lange [aut],\n  Fabian Gans [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=statcomp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "statcomp Statistical Complexity and Information Measures for Time Series\nAnalysis An implementation of local and global statistical complexity measures (aka Information Theory Quantifiers, ITQ) for time series analysis based on ordinal statistics (Bandt and Pompe (2002) <DOI:10.1103/PhysRevLett.88.174102>). Several distance measures that operate on ordinal pattern distributions, auxiliary functions for ordinal pattern analysis, and generating functions for stochastic and deterministic-chaotic processes for ITQ testing are provided.   "
  },
  {
    "id": 21437,
    "package_name": "steepness",
    "title": "Testing Steepness of Dominance Hierarchies",
    "description": "The steepness package computes steepness as a\n        property of dominance hierarchies. Steepness is defined as the\n        absolute slope of the straight line fitted to the normalized\n        David's scores. The normalized David's scores can be obtained\n        on the basis of dyadic dominance indices corrected for chance\n        or by means of proportions of wins. Given an observed\n        sociomatrix, it computes hierarchy's steepness and estimates\n        statistical significance by means of a randomization test.",
    "version": "0.3-0",
    "maintainer": "David Leiva <dleivaur@ub.edu>",
    "author": "David Leiva <dleivaur@ub.edu> & Han de Vries\n        <J.deVries1@uu.nl>.",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=steepness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "steepness Testing Steepness of Dominance Hierarchies The steepness package computes steepness as a\n        property of dominance hierarchies. Steepness is defined as the\n        absolute slope of the straight line fitted to the normalized\n        David's scores. The normalized David's scores can be obtained\n        on the basis of dyadic dominance indices corrected for chance\n        or by means of proportions of wins. Given an observed\n        sociomatrix, it computes hierarchy's steepness and estimates\n        statistical significance by means of a randomization test.  "
  },
  {
    "id": 21451,
    "package_name": "stepjglm",
    "title": "Variable Selection for Joint Modeling of Mean and Dispersion",
    "description": "A Package for selecting variables for the joint modeling of mean and dispersion (including models for mixture experiments) based on hypothesis testing and the quality of model's fit. In each iteration of the selection process, a criterion for checking the goodness of fit is used as a filter for choosing the terms that will be evaluated by a hypothesis test. Pinto & Pereira (2021) <arXiv:2109.07978>.",
    "version": "0.0.1",
    "maintainer": "Leandro A. Pereira <leandro.ap@ufu.br>",
    "author": "Leandro A. Pereira [aut, cre],\n  Edmilson R. Pinto [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stepjglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stepjglm Variable Selection for Joint Modeling of Mean and Dispersion A Package for selecting variables for the joint modeling of mean and dispersion (including models for mixture experiments) based on hypothesis testing and the quality of model's fit. In each iteration of the selection process, a criterion for checking the goodness of fit is used as a filter for choosing the terms that will be evaluated by a hypothesis test. Pinto & Pereira (2021) <arXiv:2109.07978>.  "
  },
  {
    "id": 21512,
    "package_name": "strata.MaxCombo",
    "title": "Stratified Max-Combo Test",
    "description": "Non-proportional hazard (NPH) is commonly observed in immuno-oncology studies, where the survival curves of the treatment and control groups show delayed separation. To properly account for NPH, several statistical methods have been developed. One such method is Max-Combo test, which is a straightforward and flexible hypothesis testing method that can simultaneously test for constant, early, middle, and late treatment effects. However, the majority of the Max-Combo test performed in clinical studies are unstratified, ignoring the important prognostic stratification factors. To fill this gap, we have developed an R package for stratified Max-Combo testing that accounts for stratified baseline factors. Our package explores various methods for calculating combined test statistics, estimating joint distributions, and determining the p-values.",
    "version": "0.0.1",
    "maintainer": "Yuwen Liu <yuwenliu9@gmail.com>",
    "author": "Yuwen Liu [aut, cre],\n  Yumeng Wang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=strata.MaxCombo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strata.MaxCombo Stratified Max-Combo Test Non-proportional hazard (NPH) is commonly observed in immuno-oncology studies, where the survival curves of the treatment and control groups show delayed separation. To properly account for NPH, several statistical methods have been developed. One such method is Max-Combo test, which is a straightforward and flexible hypothesis testing method that can simultaneously test for constant, early, middle, and late treatment effects. However, the majority of the Max-Combo test performed in clinical studies are unstratified, ignoring the important prognostic stratification factors. To fill this gap, we have developed an R package for stratified Max-Combo testing that accounts for stratified baseline factors. Our package explores various methods for calculating combined test statistics, estimating joint distributions, and determining the p-values.  "
  },
  {
    "id": 21534,
    "package_name": "stressor",
    "title": "Algorithms for Testing Models under Stress",
    "description": "Traditional model evaluation metrics fail to capture model \n    performance under less than ideal conditions. This package employs \n    techniques to evaluate models \"under-stress\". This includes testing \n    models' extrapolation ability, or testing accuracy on specific \n    sub-samples of the overall model space. Details describing stress-testing \n    methods in this package are provided in \n    Haycock (2023) <doi:10.26076/2am5-9f67>. The other primary contribution of\n    this package is provided to R users access to the 'Python' library 'PyCaret'\n    <https://pycaret.org/> for quick and easy access to auto-tuned \n    machine learning models. ",
    "version": "0.2.0",
    "maintainer": "Sam Haycock <haycock.sam@outlook.com>",
    "author": "Sam Haycock [aut, cre],\n  Brennan Bean [aut],\n  Utah State University [cph, fnd],\n  Thermo Fisher Scientific Inc. [fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=stressor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "stressor Algorithms for Testing Models under Stress Traditional model evaluation metrics fail to capture model \n    performance under less than ideal conditions. This package employs \n    techniques to evaluate models \"under-stress\". This includes testing \n    models' extrapolation ability, or testing accuracy on specific \n    sub-samples of the overall model space. Details describing stress-testing \n    methods in this package are provided in \n    Haycock (2023) <doi:10.26076/2am5-9f67>. The other primary contribution of\n    this package is provided to R users access to the 'Python' library 'PyCaret'\n    <https://pycaret.org/> for quick and easy access to auto-tuned \n    machine learning models.   "
  },
  {
    "id": 21550,
    "package_name": "strucchange",
    "title": "Testing, Monitoring, and Dating Structural Changes",
    "description": "Testing, monitoring and dating structural changes in (linear)\n             regression models. strucchange features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n\t     fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM,\n\t     recursive/moving estimates) and F statistics, respectively.\n             It is possible to monitor incoming data online using\n             fluctuation processes.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals.\n             Emphasis is always given to methods for visualizing the data.",
    "version": "1.5-4",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "author": "Achim Zeileis [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0918-3766>),\n  Friedrich Leisch [aut],\n  Kurt Hornik [aut],\n  Christian Kleiber [aut],\n  Bruce Hansen [ctb],\n  Edgar C. Merkle [ctb],\n  Nikolaus Umlauf [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=strucchange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strucchange Testing, Monitoring, and Dating Structural Changes Testing, monitoring and dating structural changes in (linear)\n             regression models. strucchange features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n\t     fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM,\n\t     recursive/moving estimates) and F statistics, respectively.\n             It is possible to monitor incoming data online using\n             fluctuation processes.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals.\n             Emphasis is always given to methods for visualizing the data.  "
  },
  {
    "id": 21551,
    "package_name": "strucchangeRcpp",
    "title": "Testing, Monitoring, and Dating Structural Changes: C++ Version",
    "description": "A fast implementation with additional experimental features for\n             testing, monitoring and dating structural changes in (linear)\n             regression models. 'strucchangeRcpp' features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n             fit, plot and test fluctuation processes (e.g. cumulative/moving\n             sum, recursive/moving estimates) and F statistics, respectively.\n             These methods are described in Zeileis et al. (2002)\n             <doi:10.18637/jss.v007.i02>.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals,\n             and their magnitude as well as the model fit can be evaluated\n             using a variety of statistical measures.",
    "version": "1.5-4-1.0.1",
    "maintainer": "Dainius Masiliunas <pastas4@gmail.com>",
    "author": "Dainius Masiliunas [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5654-1277>),\n  Achim Zeileis [aut] (ORCID: <https://orcid.org/0000-0003-0918-3766>),\n  Marius Appel [aut],\n  Friedrich Leisch [aut],\n  Kurt Hornik [aut],\n  Christian Kleiber [aut],\n  Andrei Mirt [ctb] (ORCID: <https://orcid.org/0000-0003-3654-2090>),\n  Bruce Hansen [ctb],\n  Edgar C. Merkle [ctb],\n  Nikolaus Umlauf [ctb]",
    "url": "https://github.com/bfast2/strucchangeRcpp/",
    "bug_reports": "https://github.com/bfast2/strucchangeRcpp/issues",
    "repository": "https://cran.r-project.org/package=strucchangeRcpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "strucchangeRcpp Testing, Monitoring, and Dating Structural Changes: C++ Version A fast implementation with additional experimental features for\n             testing, monitoring and dating structural changes in (linear)\n             regression models. 'strucchangeRcpp' features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n             fit, plot and test fluctuation processes (e.g. cumulative/moving\n             sum, recursive/moving estimates) and F statistics, respectively.\n             These methods are described in Zeileis et al. (2002)\n             <doi:10.18637/jss.v007.i02>.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals,\n             and their magnitude as well as the model fit can be evaluated\n             using a variety of statistical measures.  "
  },
  {
    "id": 21552,
    "package_name": "structSSI",
    "title": "Multiple Testing for Hypotheses with Hierarchical or Group\nStructure",
    "description": "Performs multiple testing corrections that take specific structure\n    of hypotheses into account, as described in Sankaran & Holmes (2014)\n    <doi:10.18637/jss.v059.i13>.",
    "version": "1.2.1",
    "maintainer": "Kris Sankaran <ksankaran@wisc.edu>",
    "author": "Kris Sankaran [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9415-1971>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=structSSI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "structSSI Multiple Testing for Hypotheses with Hierarchical or Group\nStructure Performs multiple testing corrections that take specific structure\n    of hypotheses into account, as described in Sankaran & Holmes (2014)\n    <doi:10.18637/jss.v059.i13>.  "
  },
  {
    "id": 21567,
    "package_name": "subgxe",
    "title": "Combine Multiple GWAS by Using Gene-Environment Interactions",
    "description": "Classical methods for combining summary data from genome-wide\n    association studies (GWAS) only use marginal genetic effects and power can\n    be compromised in the presence of heterogeneity. 'subgxe' is a R package\n    that implements p-value assisted subset testing for association (pASTA),\n    a method developed by Yu et al. (2019) <doi:10.1159/000496867>. pASTA\n    generalizes association analysis based on subsets by incorporating\n    gene-environment interactions into the testing procedure.",
    "version": "0.9.0",
    "maintainer": "Alexander Rix <alexrix@umich.edu>",
    "author": "Youfei Yu [aut],\n  Alexander Rix [cre]",
    "url": "https://github.com/umich-cphds/subgxe",
    "bug_reports": "https://github.com/umich-cphds/subgxe/issues",
    "repository": "https://cran.r-project.org/package=subgxe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "subgxe Combine Multiple GWAS by Using Gene-Environment Interactions Classical methods for combining summary data from genome-wide\n    association studies (GWAS) only use marginal genetic effects and power can\n    be compromised in the presence of heterogeneity. 'subgxe' is a R package\n    that implements p-value assisted subset testing for association (pASTA),\n    a method developed by Yu et al. (2019) <doi:10.1159/000496867>. pASTA\n    generalizes association analysis based on subsets by incorporating\n    gene-environment interactions into the testing procedure.  "
  },
  {
    "id": 21569,
    "package_name": "submax",
    "title": "Effect Modification in Observational Studies Using the Submax\nMethod",
    "description": "Effect modification occurs if a treatment effect is larger or more stable in certain subgroups defined by observed covariates.  The submax or subgroup-maximum method of Lee et al. (2018) <doi:10.1111/biom.12884> does an overall test and separate tests in subgroups, correcting for multiple testing using the joint distribution.",
    "version": "1.1.5",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=submax",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "submax Effect Modification in Observational Studies Using the Submax\nMethod Effect modification occurs if a treatment effect is larger or more stable in certain subgroups defined by observed covariates.  The submax or subgroup-maximum method of Lee et al. (2018) <doi:10.1111/biom.12884> does an overall test and separate tests in subgroups, correcting for multiple testing using the joint distribution.  "
  },
  {
    "id": 21591,
    "package_name": "sumSome",
    "title": "Permutation True Discovery Guarantee by Sum-Based Tests",
    "description": "It allows to quickly perform permutation-based closed testing by sum-based global tests, and construct lower confidence bounds for the TDP, simultaneously over all subsets of hypotheses. As a main feature, it produces simultaneous lower confidence bounds for the proportion of active voxels in different clusters for fMRI cluster analysis. Details may be found in Vesely, Finos, and Goeman (2020) <arXiv:2102.11759>.",
    "version": "1.1.0",
    "maintainer": "Anna Vesely <anna.vesely@phd.unipd.it>",
    "author": "Anna Vesely",
    "url": "https://github.com/annavesely/sumSome",
    "bug_reports": "https://github.com/annavesely/sumSome/issues",
    "repository": "https://cran.r-project.org/package=sumSome",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sumSome Permutation True Discovery Guarantee by Sum-Based Tests It allows to quickly perform permutation-based closed testing by sum-based global tests, and construct lower confidence bounds for the TDP, simultaneously over all subsets of hypotheses. As a main feature, it produces simultaneous lower confidence bounds for the proportion of active voxels in different clusters for fMRI cluster analysis. Details may be found in Vesely, Finos, and Goeman (2020) <arXiv:2102.11759>.  "
  },
  {
    "id": 21607,
    "package_name": "superdiag",
    "title": "A Comprehensive Test Suite for Testing Markov Chain\nNonconvergence",
    "description": "The 'superdiag' package provides a comprehensive test suite for testing\n  Markov Chain nonconvergence. It integrates five standard empirical MCMC convergence\n  diagnostics (Gelman-Rubin, Geweke, Heidelberger-Welch, Raftery-Lewis, and Hellinger\n  distance) and plotting functions for trace plots and density histograms. The functions\n  of the package can be used to present all diagnostic statistics and graphs at once\n  for conveniently checking MCMC nonconvergence.",
    "version": "2.0",
    "maintainer": "Le Bao <lbaole17@gmail.com>",
    "author": "Le Bao <lbaole17@gmail.com>, Jeff Gill <jgill@mac.com>, Tsung-han Tsai <thtsai@nccu.edu.tw> and Jonathan Rapkin",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=superdiag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "superdiag A Comprehensive Test Suite for Testing Markov Chain\nNonconvergence The 'superdiag' package provides a comprehensive test suite for testing\n  Markov Chain nonconvergence. It integrates five standard empirical MCMC convergence\n  diagnostics (Gelman-Rubin, Geweke, Heidelberger-Welch, Raftery-Lewis, and Hellinger\n  distance) and plotting functions for trace plots and density histograms. The functions\n  of the package can be used to present all diagnostic statistics and graphs at once\n  for conveniently checking MCMC nonconvergence.  "
  },
  {
    "id": 21626,
    "package_name": "surtvep",
    "title": "Cox Non-Proportional Hazards Model with Time-Varying\nCoefficients",
    "description": "Fit Cox non-proportional hazards models with time-varying coefficients. \n    Both unpenalized procedures (Newton and proximal Newton) and penalized procedures \n    (P-splines and smoothing splines) are included using B-spline basis functions for \n    estimating time-varying coefficients. For penalized procedures, cross validations, \n    mAIC, TIC or GIC are implemented to select tuning parameters. Utilities for \n    carrying out post-estimation visualization, summarization, point-wise confidence \n    interval and hypothesis testing are also provided.\n    For more information, see Wu et al. (2022) <doi: 10.1007/s10985-021-09544-2> and \n    Luo et al. (2023) <doi:10.1177/09622802231181471>.",
    "version": "1.0.0",
    "maintainer": "Lingfeng Luo <lfluo@umich.edu>",
    "author": "Lingfeng Luo [aut, cre],\n  Wenbo Wu [aut],\n  Kevin He [aut]",
    "url": "https://github.com/UM-KevinHe/surtvep,\nhttps://um-kevinhe.github.io/surtvep/",
    "bug_reports": "https://github.com/UM-KevinHe/surtvep/issues",
    "repository": "https://cran.r-project.org/package=surtvep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "surtvep Cox Non-Proportional Hazards Model with Time-Varying\nCoefficients Fit Cox non-proportional hazards models with time-varying coefficients. \n    Both unpenalized procedures (Newton and proximal Newton) and penalized procedures \n    (P-splines and smoothing splines) are included using B-spline basis functions for \n    estimating time-varying coefficients. For penalized procedures, cross validations, \n    mAIC, TIC or GIC are implemented to select tuning parameters. Utilities for \n    carrying out post-estimation visualization, summarization, point-wise confidence \n    interval and hypothesis testing are also provided.\n    For more information, see Wu et al. (2022) <doi: 10.1007/s10985-021-09544-2> and \n    Luo et al. (2023) <doi:10.1177/09622802231181471>.  "
  },
  {
    "id": 21635,
    "package_name": "survELtest",
    "title": "Comparing Multiple Survival Functions with Crossing Hazards",
    "description": "Computing the one-sided/two-sided integrated/maximally selected EL statistics for simultaneous testing, the one-sided/two-sided EL tests for pointwise testing, and an initial test that precedes one-sided testing to exclude the possibility of crossings or alternative orderings among the survival functions. ",
    "version": "2.0.1",
    "maintainer": "Guo-You Lan <jj6020770416jj@gmail.com>",
    "author": "Hsin-wen Chang [aut, cre] <hwchang@stat.sinica.edu.tw>",
    "url": "https://github.com/news11/survELtest",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survELtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survELtest Comparing Multiple Survival Functions with Crossing Hazards Computing the one-sided/two-sided integrated/maximally selected EL statistics for simultaneous testing, the one-sided/two-sided EL tests for pointwise testing, and an initial test that precedes one-sided testing to exclude the possibility of crossings or alternative orderings among the survival functions.   "
  },
  {
    "id": 21651,
    "package_name": "survcompare",
    "title": "Nested Cross-Validation to Compare Cox-PH, Cox-Lasso, Survival\nRandom Forests",
    "description": "Performs repeated nested cross-validation for Cox Proportionate Hazards, Cox Lasso, Survival Random Forest, and their ensemble. Returns internally validated concordance index, time-dependent area under the curve, Brier score, calibration slope, and statistical testing of non-linear ensemble outperforming the baseline Cox model. In this, it helps researchers to quantify the gain of using a more complex survival model, or justify its redundancy. Equally, it shows the performance value of the non-linear and interaction terms, and may highlight the need of further feature transformation. Further details can be found in Shamsutdinova, Stamate, Roberts, & Stahl (2022) \"Combining Cox Model and Tree-Based Algorithms to Boost Performance and Preserve Interpretability for Health Outcomes\" <doi:10.1007/978-3-031-08337-2_15>, where the method is described as Ensemble 1.",
    "version": "0.3.0",
    "maintainer": "Diana Shamsutdinova <diana.shamsutdinova.github@gmail.com>",
    "author": "Diana Shamsutdinova [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2434-3641>),\n  Daniel Stahl [aut] (ORCID: <https://orcid.org/0000-0001-7987-6619>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=survcompare",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "survcompare Nested Cross-Validation to Compare Cox-PH, Cox-Lasso, Survival\nRandom Forests Performs repeated nested cross-validation for Cox Proportionate Hazards, Cox Lasso, Survival Random Forest, and their ensemble. Returns internally validated concordance index, time-dependent area under the curve, Brier score, calibration slope, and statistical testing of non-linear ensemble outperforming the baseline Cox model. In this, it helps researchers to quantify the gain of using a more complex survival model, or justify its redundancy. Equally, it shows the performance value of the non-linear and interaction terms, and may highlight the need of further feature transformation. Further details can be found in Shamsutdinova, Stamate, Roberts, & Stahl (2022) \"Combining Cox Model and Tree-Based Algorithms to Boost Performance and Preserve Interpretability for Health Outcomes\" <doi:10.1007/978-3-031-08337-2_15>, where the method is described as Ensemble 1.  "
  },
  {
    "id": 21702,
    "package_name": "svUnit",
    "title": "'SciViews::R' - Unit, Integration and System Testing",
    "description": "A complete unit test system.",
    "version": "1.0.8",
    "maintainer": "Philippe Grosjean <phgrosjean@sciviews.org>",
    "author": "Philippe Grosjean [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2694-9471>)",
    "url": "https://github.com/SciViews/svUnit,\nhttps://www.sciviews.org/svUnit/,\nhttps://sciviews.r-universe.dev/svUnit",
    "bug_reports": "https://github.com/SciViews/svUnit/issues",
    "repository": "https://cran.r-project.org/package=svUnit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svUnit 'SciViews::R' - Unit, Integration and System Testing A complete unit test system.  "
  },
  {
    "id": 21712,
    "package_name": "svplots",
    "title": "Sample Variance Plots (Sv-Plots)",
    "description": "Two versions of sample variance plots, Sv-plot1 and Sv-plot2, will be provided illustrating \n             the squared deviations from sample variance. Besides indicating the contribution of squared \n             deviations for the sample variability, these plots are capable of detecting characteristics of the\n             distribution such as symmetry, skewness and outliers. A remarkable graphical method based on \n             Sv-plot2 can determine the decision on testing hypotheses over one or two population means. \n             In sum, Sv-plots will be appealing visualization tools. Complete description of this methodology  \n             can be found in the article, Wijesuriya (2020) <doi:10.1080/03610918.2020.1851716>. ",
    "version": "0.1.0",
    "maintainer": "Uditha Amarananda Wijesuriya <u.wijesuriya@usi.edu>",
    "author": "Uditha Amarananda Wijesuriya <u.wijesuriya@usi.edu> ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=svplots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "svplots Sample Variance Plots (Sv-Plots) Two versions of sample variance plots, Sv-plot1 and Sv-plot2, will be provided illustrating \n             the squared deviations from sample variance. Besides indicating the contribution of squared \n             deviations for the sample variability, these plots are capable of detecting characteristics of the\n             distribution such as symmetry, skewness and outliers. A remarkable graphical method based on \n             Sv-plot2 can determine the decision on testing hypotheses over one or two population means. \n             In sum, Sv-plots will be appealing visualization tools. Complete description of this methodology  \n             can be found in the article, Wijesuriya (2020) <doi:10.1080/03610918.2020.1851716>.   "
  },
  {
    "id": 21730,
    "package_name": "swaglm",
    "title": "Fast Sparse Wrapper Algorithm for Generalized Linear Models and\nTesting Procedures for Network of Highly Predictive Variables",
    "description": "Provides a fast implementation of the SWAG algorithm for Generalized Linear Models which allows to perform a meta-learning procedure that combines \n    screening and wrapper methods to find a set of extremely low-dimensional attribute combinations. The package then performs test on the network of selected models to identify the variables that are highly predictive by using entropy-based network measures.",
    "version": "0.0.1",
    "maintainer": "Lionel Voirol <lionelvoirol@hotmail.com>",
    "author": "Lionel Voirol [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1696-1407>),\n  Yagmur Ozdemir [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=swaglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "swaglm Fast Sparse Wrapper Algorithm for Generalized Linear Models and\nTesting Procedures for Network of Highly Predictive Variables Provides a fast implementation of the SWAG algorithm for Generalized Linear Models which allows to perform a meta-learning procedure that combines \n    screening and wrapper methods to find a set of extremely low-dimensional attribute combinations. The package then performs test on the network of selected models to identify the variables that are highly predictive by using entropy-based network measures.  "
  },
  {
    "id": 21763,
    "package_name": "symmetry",
    "title": "Testing for Symmetry of Data and Model Residuals",
    "description": "Implementations of a large number of tests for symmetry and their \n    bootstrap variants, which can be used for testing the symmetry of random\n    samples around a known or unknown mean. Functions are also there for testing\n    the symmetry of model residuals around zero. Currently, the supported models\n    are linear models and generalized autoregressive conditional\n    heteroskedasticity (GARCH) models (fitted with the 'fGarch' package). All\n    tests are implemented using the 'Rcpp' package which ensures great\n    performance of the code.",
    "version": "0.2.3",
    "maintainer": "Blagoje Ivanovi\u0107 <blagoje.ivanovic@matf.bg.ac.rs>",
    "author": "Blagoje Ivanovi\u0107 [aut, cre]\n        Bojana Milo\u0161evi\u0107 [aut]\n        Marko Obradovi\u0107 [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=symmetry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "symmetry Testing for Symmetry of Data and Model Residuals Implementations of a large number of tests for symmetry and their \n    bootstrap variants, which can be used for testing the symmetry of random\n    samples around a known or unknown mean. Functions are also there for testing\n    the symmetry of model residuals around zero. Currently, the supported models\n    are linear models and generalized autoregressive conditional\n    heteroskedasticity (GARCH) models (fitted with the 'fGarch' package). All\n    tests are implemented using the 'Rcpp' package which ensures great\n    performance of the code.  "
  },
  {
    "id": 21774,
    "package_name": "synthesis",
    "title": "Generate Synthetic Data from Statistical Models",
    "description": "Generate synthetic time series from commonly used statistical models, including linear, nonlinear and chaotic systems. Applications to testing methods can be found in Jiang, Z., Sharma, A., & Johnson, F. (2019) <doi:10.1016/j.advwatres.2019.103430> and Jiang, Z., Sharma, A., & Johnson, F. (2020) <doi:10.1029/2019WR026962> associated with an open-source tool by Jiang, Z., Rashid, M. M., Johnson, F., & Sharma, A. (2020) <doi:10.1016/j.envsoft.2020.104907>.",
    "version": "1.2.5",
    "maintainer": "Ze Jiang <ze.jiang@unsw.edu.au>",
    "author": "Ze Jiang [aut, cre] (ORCID: <https://orcid.org/0000-0002-3472-0829>)",
    "url": "https://github.com/zejiang-unsw/synthesis#readme",
    "bug_reports": "https://github.com/zejiang-unsw/synthesis/issues",
    "repository": "https://cran.r-project.org/package=synthesis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "synthesis Generate Synthetic Data from Statistical Models Generate synthetic time series from commonly used statistical models, including linear, nonlinear and chaotic systems. Applications to testing methods can be found in Jiang, Z., Sharma, A., & Johnson, F. (2019) <doi:10.1016/j.advwatres.2019.103430> and Jiang, Z., Sharma, A., & Johnson, F. (2020) <doi:10.1029/2019WR026962> associated with an open-source tool by Jiang, Z., Rashid, M. M., Johnson, F., & Sharma, A. (2020) <doi:10.1016/j.envsoft.2020.104907>.  "
  },
  {
    "id": 21780,
    "package_name": "sysAgNPs",
    "title": "Systematic Quantification of AgNPs to Unleash their Potential\nfor Applicability",
    "description": "There is variation across AgNPs due to differences in characterization techniques and testing metrics employed in studies. To address this problem, we have developed a systematic evaluation framework called 'sysAgNPs'. Within this framework, Distribution Entropy (DE) is utilized to measure the uncertainty of  feature categories of AgNPs, Proclivity Entropy (PE) assesses the preference of these categories, and Combination Entropy (CE) quantifies the uncertainty of feature combinations of AgNPs. Additionally, a Markov chain model is employed to examine the relationships among the sub-features of AgNPs and to determine a Transition Score (TS) scoring standard that is based on steady-state probabilities. The 'sysAgNPs' framework provides metrics for evaluating AgNPs, which helps to unravel their complexity and facilitates effective comparisons among different AgNPs, thereby advancing the scientific research and application of these AgNPs.",
    "version": "1.0.0",
    "maintainer": "Xiting Wang <XitingWang2023@outlook.com>",
    "author": "Xiting Wang [aut, cre] (ORCID: <https://orcid.org/0009-0009-5235-0006>),\n  Longfei Mao [aut, cph] (ORCID: <https://orcid.org/0000-0003-0759-0501>),\n  Jiamin Hu [ctb] (ORCID: <https://orcid.org/0000-0003-3030-2117>)",
    "url": "https://github.com/xitingwang-ida/sysAgNPs",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=sysAgNPs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sysAgNPs Systematic Quantification of AgNPs to Unleash their Potential\nfor Applicability There is variation across AgNPs due to differences in characterization techniques and testing metrics employed in studies. To address this problem, we have developed a systematic evaluation framework called 'sysAgNPs'. Within this framework, Distribution Entropy (DE) is utilized to measure the uncertainty of  feature categories of AgNPs, Proclivity Entropy (PE) assesses the preference of these categories, and Combination Entropy (CE) quantifies the uncertainty of feature combinations of AgNPs. Additionally, a Markov chain model is employed to examine the relationships among the sub-features of AgNPs and to determine a Transition Score (TS) scoring standard that is based on steady-state probabilities. The 'sysAgNPs' framework provides metrics for evaluating AgNPs, which helps to unravel their complexity and facilitates effective comparisons among different AgNPs, thereby advancing the scientific research and application of these AgNPs.  "
  },
  {
    "id": 21871,
    "package_name": "tcftt",
    "title": "Two-Sample Tests for Skewed Data",
    "description": "The classical two-sample t-test works well for the normally distributed data or \n    data with large sample size. The tcfu() and tt() tests implemented in this package provide \n    better type-I-error control with more accurate power when testing the equality of two-sample \n    means for skewed populations having unequal variances. These tests are especially useful \n    when the sample sizes are moderate. The tcfu() uses the Cornish-Fisher expansion to achieve \n    a better approximation to the true percentiles. The tt() provides transformations of the Welch's \n    t-statistic so that the sampling distribution become more symmetric. For more technical details, \n    please refer to Zhang (2019) <http://hdl.handle.net/2097/40235>.",
    "version": "0.1.0",
    "maintainer": "Huaiyu Zhang <huaiyuzhang1988@gmail.com>",
    "author": "Huaiyu Zhang, Haiyan Wang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tcftt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tcftt Two-Sample Tests for Skewed Data The classical two-sample t-test works well for the normally distributed data or \n    data with large sample size. The tcfu() and tt() tests implemented in this package provide \n    better type-I-error control with more accurate power when testing the equality of two-sample \n    means for skewed populations having unequal variances. These tests are especially useful \n    when the sample sizes are moderate. The tcfu() uses the Cornish-Fisher expansion to achieve \n    a better approximation to the true percentiles. The tt() provides transformations of the Welch's \n    t-statistic so that the sampling distribution become more symmetric. For more technical details, \n    please refer to Zhang (2019) <http://hdl.handle.net/2097/40235>.  "
  },
  {
    "id": 21874,
    "package_name": "tcl",
    "title": "Testing in Conditional Likelihood Context",
    "description": "An implementation of hypothesis testing in an extended Rasch modeling framework,\n    including sample size planning procedures and power computations. Provides 4 statistical tests, \n    i.e., gradient test (GR), likelihood ratio test (LR), Rao score or Lagrange multiplier test (RS), \n    and Wald test, for testing a number of hypotheses referring to the Rasch model (RM), linear \n    logistic test model (LLTM), rating scale model (RSM), and partial credit model (PCM). Three \n    types of functions for power and sample size computations are provided. Firstly, functions to \n    compute the sample size given a user-specified (predetermined) deviation from the hypothesis \n    to be tested, the level alpha, and the power of the test. Secondly, functions to evaluate the \n    power of the tests given a user-specified (predetermined) deviation from the hypothesis to be \n    tested, the level alpha of the test, and the sample size. Thirdly, functions to evaluate the \n    so-called post hoc power of the tests. This is the power of the tests given the observed \n    deviation of the data from the hypothesis to be tested and a user-specified level alpha of the\n    test. Power and sample size computations are based on a Monte Carlo simulation approach. It is\n    computationally very efficient. The variance of the random error in computing power and sample\n    size arising from the simulation approach is analytically derived by using the delta method. \n    Additionally, functions to compute the power of the tests as a function of an effect measure \n    interpreted as explained variance are provided.\n    Draxler, C., & Alexandrowicz, R. W. (2015), <doi:10.1007/s11336-015-9472-y>.",
    "version": "1.0.1",
    "maintainer": "Clemens Draxler <clemens.draxler@umit-tirol.at>",
    "author": "Clemens Draxler [aut, cre],\n  Andreas Kurz [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tcl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tcl Testing in Conditional Likelihood Context An implementation of hypothesis testing in an extended Rasch modeling framework,\n    including sample size planning procedures and power computations. Provides 4 statistical tests, \n    i.e., gradient test (GR), likelihood ratio test (LR), Rao score or Lagrange multiplier test (RS), \n    and Wald test, for testing a number of hypotheses referring to the Rasch model (RM), linear \n    logistic test model (LLTM), rating scale model (RSM), and partial credit model (PCM). Three \n    types of functions for power and sample size computations are provided. Firstly, functions to \n    compute the sample size given a user-specified (predetermined) deviation from the hypothesis \n    to be tested, the level alpha, and the power of the test. Secondly, functions to evaluate the \n    power of the tests given a user-specified (predetermined) deviation from the hypothesis to be \n    tested, the level alpha of the test, and the sample size. Thirdly, functions to evaluate the \n    so-called post hoc power of the tests. This is the power of the tests given the observed \n    deviation of the data from the hypothesis to be tested and a user-specified level alpha of the\n    test. Power and sample size computations are based on a Monte Carlo simulation approach. It is\n    computationally very efficient. The variance of the random error in computing power and sample\n    size arising from the simulation approach is analytically derived by using the delta method. \n    Additionally, functions to compute the power of the tests as a function of an effect measure \n    interpreted as explained variance are provided.\n    Draxler, C., & Alexandrowicz, R. W. (2015), <doi:10.1007/s11336-015-9472-y>.  "
  },
  {
    "id": 21893,
    "package_name": "tea",
    "title": "Threshold Estimation Approaches",
    "description": "Different approaches for selecting the threshold in generalized Pareto distributions. Most of them are based on minimizing the AMSE-criterion or at least by reducing the bias of the assumed GPD-model. Others are heuristically motivated by searching for stable sample paths, i.e. a nearly constant region of the tail index estimator with respect to k, which is the number of data in the tail. The third class is motivated by graphical inspection. In addition, a sequential testing procedure for GPD-GoF-tests is also implemented here.",
    "version": "1.1",
    "maintainer": "Johannes Ossberger <johannes.ossberger@gmail.com>",
    "author": "Johannes Ossberger",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tea",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tea Threshold Estimation Approaches Different approaches for selecting the threshold in generalized Pareto distributions. Most of them are based on minimizing the AMSE-criterion or at least by reducing the bias of the assumed GPD-model. Others are heuristically motivated by searching for stable sample paths, i.e. a nearly constant region of the tail index estimator with respect to k, which is the number of data in the tail. The third class is motivated by graphical inspection. In addition, a sequential testing procedure for GPD-GoF-tests is also implemented here.  "
  },
  {
    "id": 21928,
    "package_name": "tensorEVD",
    "title": "A Fast Algorithm to Factorize High-Dimensional Tensor Product\nMatrices",
    "description": "Here we provide tools for the computation and factorization of high-dimensional\n\t     tensor products that are formed by smaller matrices. The methods are based on\n\t     properties of Kronecker products (Searle 1982, p. 265, ISBN-10: 0470009616).\n\t     We evaluated this methodology by benchmark testing and illustrated its use in\n\t     Gaussian Linear Models ('Lopez-Cruz et al., 2024') <doi:10.1093/g3journal/jkae001>.",
    "version": "0.1.4",
    "maintainer": "Marco Lopez-Cruz <maraloc@gmail.com>",
    "author": "Marco Lopez-Cruz [aut, cre],\n  Gustavo de los Campos [aut],\n  Paulino Perez-Rodriguez [aut]",
    "url": "https://github.com/MarcooLopez/tensorEVD",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tensorEVD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tensorEVD A Fast Algorithm to Factorize High-Dimensional Tensor Product\nMatrices Here we provide tools for the computation and factorization of high-dimensional\n\t     tensor products that are formed by smaller matrices. The methods are based on\n\t     properties of Kronecker products (Searle 1982, p. 265, ISBN-10: 0470009616).\n\t     We evaluated this methodology by benchmark testing and illustrated its use in\n\t     Gaussian Linear Models ('Lopez-Cruz et al., 2024') <doi:10.1093/g3journal/jkae001>.  "
  },
  {
    "id": 21933,
    "package_name": "tensr",
    "title": "Covariance Inference and Decompositions for Tensor Datasets",
    "description": "A collection of functions for Kronecker structured covariance\n    estimation and testing under the array normal model. For estimation,\n    maximum likelihood and Bayesian equivariant estimation procedures are\n    implemented. For testing, a likelihood ratio testing procedure is\n    available. This package also contains additional functions for manipulating\n    and decomposing tensor data sets. This work was partially supported by NSF\n    grant DMS-1505136. Details of the methods are described in\n    Gerard and Hoff (2015) <doi:10.1016/j.jmva.2015.01.020> and\n    Gerard and Hoff (2016) <doi:10.1016/j.laa.2016.04.033>.",
    "version": "1.0.2",
    "maintainer": "David Gerard <gerard.1787@gmail.com>",
    "author": "David Gerard [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9450-5023>),\n  Peter Hoff [aut]",
    "url": "https://github.com/dcgerard/tensr",
    "bug_reports": "https://github.com/dcgerard/tensr/issues",
    "repository": "https://cran.r-project.org/package=tensr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tensr Covariance Inference and Decompositions for Tensor Datasets A collection of functions for Kronecker structured covariance\n    estimation and testing under the array normal model. For estimation,\n    maximum likelihood and Bayesian equivariant estimation procedures are\n    implemented. For testing, a likelihood ratio testing procedure is\n    available. This package also contains additional functions for manipulating\n    and decomposing tensor data sets. This work was partially supported by NSF\n    grant DMS-1505136. Details of the methods are described in\n    Gerard and Hoff (2015) <doi:10.1016/j.jmva.2015.01.020> and\n    Gerard and Hoff (2016) <doi:10.1016/j.laa.2016.04.033>.  "
  },
  {
    "id": 21948,
    "package_name": "testarguments",
    "title": "Test (Multiple) Arguments of a User-Defined Prediction Algorithm",
    "description": "Finding the best values for user-specified arguments of a prediction algorithm can be difficult, particularly if there is an interaction between argument levels. This package automates the testing of any user-defined prediction algorithm over an arbitrary number of arguments. It includes functions for testing the algorithm over the given arguments with respect to an arbitrary number of user-defined diagnostics, visualising the results of these tests, and finding the optimal argument combinations with respect to each diagnostic.",
    "version": "0.0.1",
    "maintainer": "Matthew Sainsbury-Dale <msainsburydale@gmail.com>",
    "author": "Matthew Sainsbury-Dale [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=testarguments",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testarguments Test (Multiple) Arguments of a User-Defined Prediction Algorithm Finding the best values for user-specified arguments of a prediction algorithm can be difficult, particularly if there is an interaction between argument levels. This package automates the testing of any user-defined prediction algorithm over an arbitrary number of arguments. It includes functions for testing the algorithm over the given arguments with respect to an arbitrary number of user-defined diagnostics, visualising the results of these tests, and finding the optimal argument combinations with respect to each diagnostic.  "
  },
  {
    "id": 21949,
    "package_name": "testassay",
    "title": "A Hypothesis Testing Framework for Validating an Assay for\nPrecision",
    "description": "A common way of validating a biological assay for is through a\n    procedure, where m levels of an analyte are measured with n replicates at each\n    level, and if all m estimates of the coefficient of variation (CV) are less\n    than some prespecified level, then the assay is declared validated for precision\n    within the range of the m analyte levels. Two limitations of this procedure are:\n    there is no clear statistical statement of precision upon passing, and it is\n    unclear how to modify the procedure for assays with constant standard deviation.\n    We provide tools to convert such a procedure into a set of m hypothesis tests.\n    This reframing motivates the m:n:q procedure, which upon completion delivers\n    a 100q% upper confidence limit on the CV. Additionally, for a post-validation\n    assay output of y, the method gives an ``effective standard deviation interval''\n    of log(y) plus or minus r, which is a 68% confidence interval on log(mu), where\n    mu is the expected value of the assay output for that sample. Further, the m:n:q\n    procedure can be straightforwardly applied to constant standard deviation assays.\n    We illustrate these tools by applying them to a growth inhibition assay. This is\n    an implementation of the methods described in Fay, Sachs, and Miura (2018) \n    <doi:10.1002/sim.7528>.",
    "version": "0.1.1",
    "maintainer": "Michael C Sachs <sachsmc@gmail.com>",
    "author": "Michael C Sachs and Michael P Fay",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=testassay",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testassay A Hypothesis Testing Framework for Validating an Assay for\nPrecision A common way of validating a biological assay for is through a\n    procedure, where m levels of an analyte are measured with n replicates at each\n    level, and if all m estimates of the coefficient of variation (CV) are less\n    than some prespecified level, then the assay is declared validated for precision\n    within the range of the m analyte levels. Two limitations of this procedure are:\n    there is no clear statistical statement of precision upon passing, and it is\n    unclear how to modify the procedure for assays with constant standard deviation.\n    We provide tools to convert such a procedure into a set of m hypothesis tests.\n    This reframing motivates the m:n:q procedure, which upon completion delivers\n    a 100q% upper confidence limit on the CV. Additionally, for a post-validation\n    assay output of y, the method gives an ``effective standard deviation interval''\n    of log(y) plus or minus r, which is a 68% confidence interval on log(mu), where\n    mu is the expected value of the assay output for that sample. Further, the m:n:q\n    procedure can be straightforwardly applied to constant standard deviation assays.\n    We illustrate these tools by applying them to a growth inhibition assay. This is\n    an implementation of the methods described in Fay, Sachs, and Miura (2018) \n    <doi:10.1002/sim.7528>.  "
  },
  {
    "id": 21950,
    "package_name": "testcorr",
    "title": "Testing Zero Correlation",
    "description": "Computes the test statistics for examining the significance of autocorrelation in univariate time series, cross-correlation in bivariate time series, Pearson correlations in multivariate series and test statistics for i.i.d. property of univariate series given in Dalla, Giraitis and Phillips (2022), <https://www.cambridge.org/core/journals/econometric-theory/article/abs/robust-tests-for-white-noise-and-crosscorrelation/4D77C12C52433F4C6735E584C779403A>, <https://elischolar.library.yale.edu/cowles-discussion-paper-series/57/>.",
    "version": "0.3.0",
    "maintainer": "Violetta Dalla <vidalla@econ.uoa.gr>",
    "author": "Violetta Dalla [aut, cre],\n  Liudas Giraitis [aut],\n  Peter C. B. Phillips [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=testcorr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testcorr Testing Zero Correlation Computes the test statistics for examining the significance of autocorrelation in univariate time series, cross-correlation in bivariate time series, Pearson correlations in multivariate series and test statistics for i.i.d. property of univariate series given in Dalla, Giraitis and Phillips (2022), <https://www.cambridge.org/core/journals/econometric-theory/article/abs/robust-tests-for-white-noise-and-crosscorrelation/4D77C12C52433F4C6735E584C779403A>, <https://elischolar.library.yale.edu/cowles-discussion-paper-series/57/>.  "
  },
  {
    "id": 21951,
    "package_name": "testdat",
    "title": "Data Unit Testing for R",
    "description": "Test your data! An extension of the 'testthat' unit testing\n    framework with a family of functions and reporting tools for checking\n    and validating data frames.",
    "version": "0.4.4",
    "maintainer": "Danny Smith <danny@gorcha.org>",
    "author": "Danny Smith [aut, cre],\n  Kinto Behr [aut],\n  The Social Research Centre [cph]",
    "url": "https://socialresearchcentre.github.io/testdat/,\nhttps://github.com/socialresearchcentre/testdat",
    "bug_reports": "https://github.com/socialresearchcentre/testdat/issues",
    "repository": "https://cran.r-project.org/package=testdat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testdat Data Unit Testing for R Test your data! An extension of the 'testthat' unit testing\n    framework with a family of functions and reporting tools for checking\n    and validating data frames.  "
  },
  {
    "id": 21954,
    "package_name": "testex",
    "title": "Add Tests to Examples",
    "description": "\n    Add tests in-line in examples. Provides standalone functions for\n    facilitating easier test writing in Rd files. However, a more familiar\n    interface is provided using 'roxygen2' tags. Tools are also provided for\n    facilitating package configuration and use with 'testthat'.",
    "version": "0.2.1",
    "maintainer": "Doug Kelkhoff <doug.kelkhoff@gmail.com>",
    "author": "Doug Kelkhoff [aut, cre]",
    "url": "https://github.com/dgkf/testex, https://dgkf.github.io/testex/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=testex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testex Add Tests to Examples \n    Add tests in-line in examples. Provides standalone functions for\n    facilitating easier test writing in Rd files. However, a more familiar\n    interface is provided using 'roxygen2' tags. Tools are also provided for\n    facilitating package configuration and use with 'testthat'.  "
  },
  {
    "id": 21956,
    "package_name": "testit",
    "title": "A Simple Package for Testing R Packages",
    "description": "Provides two convenience functions assert() and test_pkg() to\n    facilitate testing R packages.",
    "version": "0.14",
    "maintainer": "Yihui Xie <xie@yihui.name>",
    "author": "Yihui Xie [aut, cre] (ORCID: <https://orcid.org/0000-0003-0645-5666>,\n    URL: https://yihui.org),\n  Tomas Kalibera [ctb],\n  Steven Mortimer [ctb]",
    "url": "https://github.com/yihui/testit",
    "bug_reports": "https://github.com/yihui/testit/issues",
    "repository": "https://cran.r-project.org/package=testit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testit A Simple Package for Testing R Packages Provides two convenience functions assert() and test_pkg() to\n    facilitate testing R packages.  "
  },
  {
    "id": 21957,
    "package_name": "testthatdocs",
    "title": "Automated and Idempotent Unit Tests Documentation for\nReproducible Quality Assurance",
    "description": "\n    Automates documentation of test_that() calls within R test files. \n    The package scans test sources, extracts human-readable test titles (even when \n    composed with functions like paste() or glue::glue(), ... etc.), and generates \n    reproducible roxygen2-style listings that can be inserted both globally and \n    per-section. It ensures idempotent updates and supports customizable numbering \n    templates with hierarchical indices. Designed for developers, QA teams, and package\n    maintainers seeking consistent, self-documenting test inventories. ",
    "version": "1.0.23",
    "maintainer": "Rafal Urniaz <rafal.urniaz@cantab.net>",
    "author": "Rafal Urniaz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0192-2165>)",
    "url": "https://github.com/urniaz/testthatdocs",
    "bug_reports": "https://github.com/urniaz/testthatdocs/issues",
    "repository": "https://cran.r-project.org/package=testthatdocs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testthatdocs Automated and Idempotent Unit Tests Documentation for\nReproducible Quality Assurance \n    Automates documentation of test_that() calls within R test files. \n    The package scans test sources, extracts human-readable test titles (even when \n    composed with functions like paste() or glue::glue(), ... etc.), and generates \n    reproducible roxygen2-style listings that can be inserted both globally and \n    per-section. It ensures idempotent updates and supports customizable numbering \n    templates with hierarchical indices. Designed for developers, QA teams, and package\n    maintainers seeking consistent, self-documenting test inventories.   "
  },
  {
    "id": 21958,
    "package_name": "testthatmulti",
    "title": "Testing for R Packages with Multiple Attempts for Noisy Tests",
    "description": "Runs tests using the 'testthat' package but allows for multiple\n    attempts for a single test. This is useful for noisy or flaky tests\n    that generally pass but can fail due to occasional random errors,\n    such as numeric instability or using random data.",
    "version": "0.2.0",
    "maintainer": "Collin Erickson <collinberickson@gmail.com>",
    "author": "Collin Erickson [aut, cre]",
    "url": "https://github.com/CollinErickson/testthatmulti",
    "bug_reports": "https://github.com/CollinErickson/testthatmulti/issues",
    "repository": "https://cran.r-project.org/package=testthatmulti",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testthatmulti Testing for R Packages with Multiple Attempts for Noisy Tests Runs tests using the 'testthat' package but allows for multiple\n    attempts for a single test. This is useful for noisy or flaky tests\n    that generally pass but can fail due to occasional random errors,\n    such as numeric instability or using random data.  "
  },
  {
    "id": 21959,
    "package_name": "testthis",
    "title": "Utils and 'RStudio' Addins to Make Testing Even More Fun",
    "description": "Utility functions and 'RStudio' addins for writing,\n    running and organizing automated tests. Integrates tightly with the\n    packages 'testthat', 'devtools' and 'usethis'.  Hotkeys can be\n    assigned to the 'RStudio' addins for running tests in a single file or\n    to switch between a source file and the associated test file. In\n    addition, testthis provides function to manage and run tests in\n    subdirectories of the test/testthat directory.",
    "version": "1.1.1",
    "maintainer": "Stefan Fleck <stefan.b.fleck@gmail.com>",
    "author": "Stefan Fleck [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3344-9851>)",
    "url": "https://s-fleck.github.io/testthis",
    "bug_reports": "https://github.com/s-fleck/testthis/issues",
    "repository": "https://cran.r-project.org/package=testthis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testthis Utils and 'RStudio' Addins to Make Testing Even More Fun Utility functions and 'RStudio' addins for writing,\n    running and organizing automated tests. Integrates tightly with the\n    packages 'testthat', 'devtools' and 'usethis'.  Hotkeys can be\n    assigned to the 'RStudio' addins for running tests in a single file or\n    to switch between a source file and the associated test file. In\n    addition, testthis provides function to manage and run tests in\n    subdirectories of the test/testthat directory.  "
  },
  {
    "id": 21960,
    "package_name": "testtwice",
    "title": "Testing One Hypothesis Twice in Observational Studies",
    "description": "Tests one hypothesis with several test statistics, correcting for multiple testing.  The central function in the package is testtwice().  In a sensitivity analysis, the method has the largest design sensitivity of its component tests.  The package implements the method and examples in Rosenbaum, P. R. (2012) <doi:10.1093/biomet/ass032> Testing one hypothesis twice in observational studies. Biometrika, 99(4), 763-774.  ",
    "version": "1.0.3",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "author": "Paul R. Rosenbaum",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=testtwice",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "testtwice Testing One Hypothesis Twice in Observational Studies Tests one hypothesis with several test statistics, correcting for multiple testing.  The central function in the package is testtwice().  In a sensitivity analysis, the method has the largest design sensitivity of its component tests.  The package implements the method and examples in Rosenbaum, P. R. (2012) <doi:10.1093/biomet/ass032> Testing one hypothesis twice in observational studies. Biometrika, 99(4), 763-774.    "
  },
  {
    "id": 22004,
    "package_name": "tfdeploy",
    "title": "Deploy 'TensorFlow' Models",
    "description": "Tools to deploy 'TensorFlow' <https://www.tensorflow.org/> models across \n  multiple services. Currently, it provides a local server for testing 'cloudml' \n  compatible services.",
    "version": "0.6.1",
    "maintainer": "Daniel Falbel <daniel@rstudio.com>",
    "author": "Javier Luraschi [aut, ctb],\n  Daniel Falbel [cre, ctb],\n  RStudio [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tfdeploy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tfdeploy Deploy 'TensorFlow' Models Tools to deploy 'TensorFlow' <https://www.tensorflow.org/> models across \n  multiple services. Currently, it provides a local server for testing 'cloudml' \n  compatible services.  "
  },
  {
    "id": 22029,
    "package_name": "this.path",
    "title": "Get Executing Script's Path",
    "description": "Determine the path of the executing script. Compatible\n        with several popular GUIs: 'Rgui', 'RStudio', 'Positron',\n        'VSCode', 'Jupyter', 'Emacs', and 'Rscript' (shell). Compatible\n        with several functions and packages: 'source()',\n        'sys.source()', 'debugSource()' in 'RStudio',\n        'compiler::loadcmp()', 'utils::Sweave()', 'box::use()',\n        'knitr::knit()', 'plumber::plumb()', 'shiny::runApp()',\n        'package:targets', and 'testthat::source_file()'.",
    "version": "2.7.1",
    "maintainer": "Iris Simmons <ikwsimmo@gmail.com>",
    "author": "Iris Simmons [aut, cre]",
    "url": "https://github.com/ArcadeAntics/this.path",
    "bug_reports": "https://github.com/ArcadeAntics/this.path/issues",
    "repository": "https://cran.r-project.org/package=this.path",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "this.path Get Executing Script's Path Determine the path of the executing script. Compatible\n        with several popular GUIs: 'Rgui', 'RStudio', 'Positron',\n        'VSCode', 'Jupyter', 'Emacs', and 'Rscript' (shell). Compatible\n        with several functions and packages: 'source()',\n        'sys.source()', 'debugSource()' in 'RStudio',\n        'compiler::loadcmp()', 'utils::Sweave()', 'box::use()',\n        'knitr::knit()', 'plumber::plumb()', 'shiny::runApp()',\n        'package:targets', and 'testthat::source_file()'.  "
  },
  {
    "id": 22062,
    "package_name": "tidySummaries",
    "title": "Tidy Statistical Summaries for Exploratory Data Analysis",
    "description": "Provides a tidy set of functions for summarising data, \n    including descriptive statistics, frequency tables with normality testing, \n    and group-wise significance testing. Designed for fast, readable, \n    and easy exploration of both numeric and categorical data. ",
    "version": "0.1.0",
    "maintainer": "Kleanthis Koupidis <kleanthis.koupidis@gmail.com>",
    "author": "Kleanthis Koupidis [aut, cre],\n  Nikolaos Koupidis [aut]",
    "url": "https://github.com/kleanthisk10/tidySummaries",
    "bug_reports": "https://github.com/kleanthisk10/tidySummaries/issues",
    "repository": "https://cran.r-project.org/package=tidySummaries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidySummaries Tidy Statistical Summaries for Exploratory Data Analysis Provides a tidy set of functions for summarising data, \n    including descriptive statistics, frequency tables with normality testing, \n    and group-wise significance testing. Designed for fast, readable, \n    and easy exploration of both numeric and categorical data.   "
  },
  {
    "id": 22187,
    "package_name": "tinycodet",
    "title": "Functions to Help in your Coding Etiquette",
    "description": "Adds some functions to help in your coding etiquette.\n    'tinycodet' primarily focuses on 4 aspects.\n    1) Safer decimal (in)equality testing,\n    standard-evaluated alternatives to with() and aes(),\n    and other functions for safer coding.\n    2) A new package import system,\n    that attempts to combine the benefits of using a package without attaching it,\n    with the benefits of attaching a package.\n    3) Extending the string manipulation capabilities of the 'stringi' R package.\n    4) Reducing repetitive code.\n    Besides linking to 'Rcpp', 'tinycodet' has only one other dependency, namely 'stringi'.",
    "version": "0.5.8",
    "maintainer": "Tony Wilkes <tony_a_wilkes@outlook.com>",
    "author": "Tony Wilkes [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-9498-8379>)",
    "url": "https://github.com/tony-aw/tinycodet/,\nhttps://tony-aw.github.io/tinycodet/",
    "bug_reports": "https://github.com/tony-aw/tinycodet/issues/",
    "repository": "https://cran.r-project.org/package=tinycodet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tinycodet Functions to Help in your Coding Etiquette Adds some functions to help in your coding etiquette.\n    'tinycodet' primarily focuses on 4 aspects.\n    1) Safer decimal (in)equality testing,\n    standard-evaluated alternatives to with() and aes(),\n    and other functions for safer coding.\n    2) A new package import system,\n    that attempts to combine the benefits of using a package without attaching it,\n    with the benefits of attaching a package.\n    3) Extending the string manipulation capabilities of the 'stringi' R package.\n    4) Reducing repetitive code.\n    Besides linking to 'Rcpp', 'tinycodet' has only one other dependency, namely 'stringi'.  "
  },
  {
    "id": 22193,
    "package_name": "tinytest",
    "title": "Lightweight and Feature Complete Unit Testing Framework",
    "description": "Provides a lightweight (zero-dependency) and easy to use \n    unit testing framework. Main features: install tests with \n    the package. Test results are treated as data that can be stored and \n    manipulated. Test files are R scripts interspersed with test commands, that\n    can be programmed over. Fully automated build-install-test sequence for \n    packages. Skip tests when not run locally (e.g. on CRAN). Flexible and \n    configurable output printing. Compare computed output with output stored \n    with the package. Run tests in parallel. Extensible by other packages.\n    Report side effects.",
    "version": "1.4.1",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9807-4686>)",
    "url": "https://github.com/markvanderloo/tinytest",
    "bug_reports": "https://github.com/markvanderloo/tinytest/issues",
    "repository": "https://cran.r-project.org/package=tinytest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tinytest Lightweight and Feature Complete Unit Testing Framework Provides a lightweight (zero-dependency) and easy to use \n    unit testing framework. Main features: install tests with \n    the package. Test results are treated as data that can be stored and \n    manipulated. Test files are R scripts interspersed with test commands, that\n    can be programmed over. Fully automated build-install-test sequence for \n    packages. Skip tests when not run locally (e.g. on CRAN). Flexible and \n    configurable output printing. Compare computed output with output stored \n    with the package. Run tests in parallel. Extensible by other packages.\n    Report side effects.  "
  },
  {
    "id": 22194,
    "package_name": "tinytest2JUnit",
    "title": "Convert 'tinytest' Output to JUnit XML",
    "description": "Unit testing is a solid component of automated CI/CD pipelines. \n\t'tinytest' - a lightweight, zero-dependency  alternative to 'testthat' was developed.\n\tTo be able to integrate 'tinytests' results into common CI/CD systems the test results from \n\ttinytest need to be caputred and converted to JUnit XML format. \n\t'tinytest2JUnit' enables this conversion while staying also lightweight \n\tand only have 'tinytest' as its dependency.",
    "version": "1.1.2",
    "maintainer": "Lennart Tuijnder <lennart.tuijnder@openanalytics.eu>",
    "author": "Anne-Katrin Hess [aut],\n  Lennart Tuijnder [aut, cre]",
    "url": "https://github.com/openanalytics/tinytest2JUnit",
    "bug_reports": "https://github.com/openanalytics/tinytest2JUnit/issues",
    "repository": "https://cran.r-project.org/package=tinytest2JUnit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tinytest2JUnit Convert 'tinytest' Output to JUnit XML Unit testing is a solid component of automated CI/CD pipelines. \n\t'tinytest' - a lightweight, zero-dependency  alternative to 'testthat' was developed.\n\tTo be able to integrate 'tinytests' results into common CI/CD systems the test results from \n\ttinytest need to be caputred and converted to JUnit XML format. \n\t'tinytest2JUnit' enables this conversion while staying also lightweight \n\tand only have 'tinytest' as its dependency.  "
  },
  {
    "id": 22205,
    "package_name": "titanic",
    "title": "Titanic Passenger Survival Data Set",
    "description": "This data set provides information on the fate of passengers on\n    the fatal maiden voyage of the ocean liner \"Titanic\", summarized according\n    to economic status (class), sex, age and survival. Whereas the base R\n    Titanic data found by calling data(\"Titanic\") is an array resulting from\n    cross-tabulating 2201 observations, these data sets are the individual\n    non-aggregated observations and formatted in a machine learning context\n    with a training sample, a testing sample, and two additional data sets\n    that can be used for deeper machine learning analysis. These data sets\n    are also the data sets downloaded from the Kaggle competition and thus\n    lowers the barrier to entry for users new to R or machine learing.",
    "version": "0.1.0",
    "maintainer": "Paul Hendricks <paul.hendricks.2013@owu.edu>",
    "author": "Paul Hendricks [aut, cre]",
    "url": "https://github.com/paulhendricks/titanic",
    "bug_reports": "https://github.com/paulhendricks/titanic/issues",
    "repository": "https://cran.r-project.org/package=titanic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "titanic Titanic Passenger Survival Data Set This data set provides information on the fate of passengers on\n    the fatal maiden voyage of the ocean liner \"Titanic\", summarized according\n    to economic status (class), sex, age and survival. Whereas the base R\n    Titanic data found by calling data(\"Titanic\") is an array resulting from\n    cross-tabulating 2201 observations, these data sets are the individual\n    non-aggregated observations and formatted in a machine learning context\n    with a training sample, a testing sample, and two additional data sets\n    that can be used for deeper machine learning analysis. These data sets\n    are also the data sets downloaded from the Kaggle competition and thus\n    lowers the barrier to entry for users new to R or machine learing.  "
  },
  {
    "id": 22270,
    "package_name": "topics",
    "title": "Creating and Significance Testing Language Features for\nVisualisation",
    "description": "Implements differential language analysis with statistical tests and offers various language visualization techniques for n-grams and topics. It also supports the 'text' package. For more information, visit <https://r-topics.org/> and <https://www.r-text.org/>.",
    "version": "0.62",
    "maintainer": "Oscar Kjell <oscar.kjell@psy.lu.se>",
    "author": "Leon Ackermann [aut] (ORCID: <https://orcid.org/0009-0008-8583-8748>),\n  Zhuojun Gu [aut] (ORCID: <https://orcid.org/0009-0000-1610-4830>),\n  Oscar Kjell [aut, cre] (ORCID: <https://orcid.org/0000-0002-2728-6278>)",
    "url": "https://r-topics.org/",
    "bug_reports": "https://github.com/theharmonylab/topics/issues",
    "repository": "https://cran.r-project.org/package=topics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "topics Creating and Significance Testing Language Features for\nVisualisation Implements differential language analysis with statistical tests and offers various language visualization techniques for n-grams and topics. It also supports the 'text' package. For more information, visit <https://r-topics.org/> and <https://www.r-text.org/>.  "
  },
  {
    "id": 22378,
    "package_name": "treediff",
    "title": "Testing Differences Between Families of Trees",
    "description": "Perform test to detect differences in structure between families of\n             trees. The method is based on cophenetic distances and aggregated\n             Student's tests.",
    "version": "0.2.1",
    "maintainer": "Nathalie Vialaneix <nathalie.vialaneix@inrae.fr>",
    "author": "Nathalie Vialaneix [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-1156-0639>),\n  Gwendaelle Cardenas [aut],\n  Marie Chavent [aut],\n  Sylvain Foissac [aut],\n  Pierre Neuvial [aut] (ORCID: <https://orcid.org/0000-0003-3584-9998>),\n  Nathanael Randriamihamison [aut]",
    "url": "https://forgemia.inra.fr/scales/treediff",
    "bug_reports": "https://forgemia.inra.fr/scales/treediff/-/issues",
    "repository": "https://cran.r-project.org/package=treediff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "treediff Testing Differences Between Families of Trees Perform test to detect differences in structure between families of\n             trees. The method is based on cophenetic distances and aggregated\n             Student's tests.  "
  },
  {
    "id": 22396,
    "package_name": "trendtestR",
    "title": "Exploratory Trend Analysis and Visualization for Time-Series and\nGrouped Data",
    "description": "Provides a set of exploratory data analysis (EDA) tools for \n    visualizing trends, diagnosing data types for beginner-friendly workflows, \n    and automatically routing to suitable statistical tests or trend exploration \n    models. Includes unified plotting functions for trend lines, grouped boxplots, \n    and comparative scatterplots; automated statistical testing (e.g., t-test, \n    Wilcoxon, ANOVA, Kruskal-Wallis, Tukey, Dunn) with optional effect size \n    calculation; and model-based trend analysis using generalized additive \n    models (GAM) for count data, generalized linear models (GLM) for continuous \n    data, and zero-inflated models (ZIP/ZINB) for count data with potential \n    zero-inflation. \n    Also supports time-window continuity checks, cross-year \n    handling in compare_monthly_cases(), and ARIMA-ready preparation with \n    stationarity diagnostics, ensuring consistent parameter styles for \n    reproducible research and user-friendly workflows.Methods are \n    based on R Core Team (2024) <https://www.R-project.org/>, \n    Wood, S.N.(2017, ISBN:978-1498728331),\n    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>, \n    Simon Jackman (2024) <https://github.com/atahk/pscl/>,    \n    Achim Zeileis, Christian Kleiber, Simon Jackman (2008) <doi:10.18637/jss.v027.i08>.",
    "version": "1.0.1",
    "maintainer": "Gelan Huang <huanggelan97@icloud.com>",
    "author": "Gelan Huang [aut, cre]",
    "url": "https://github.com/GrahnH/trendtestR",
    "bug_reports": "https://github.com/GrahnH/trendtestR/issues",
    "repository": "https://cran.r-project.org/package=trendtestR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trendtestR Exploratory Trend Analysis and Visualization for Time-Series and\nGrouped Data Provides a set of exploratory data analysis (EDA) tools for \n    visualizing trends, diagnosing data types for beginner-friendly workflows, \n    and automatically routing to suitable statistical tests or trend exploration \n    models. Includes unified plotting functions for trend lines, grouped boxplots, \n    and comparative scatterplots; automated statistical testing (e.g., t-test, \n    Wilcoxon, ANOVA, Kruskal-Wallis, Tukey, Dunn) with optional effect size \n    calculation; and model-based trend analysis using generalized additive \n    models (GAM) for count data, generalized linear models (GLM) for continuous \n    data, and zero-inflated models (ZIP/ZINB) for count data with potential \n    zero-inflation. \n    Also supports time-window continuity checks, cross-year \n    handling in compare_monthly_cases(), and ARIMA-ready preparation with \n    stationarity diagnostics, ensuring consistent parameter styles for \n    reproducible research and user-friendly workflows.Methods are \n    based on R Core Team (2024) <https://www.R-project.org/>, \n    Wood, S.N.(2017, ISBN:978-1498728331),\n    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>, \n    Simon Jackman (2024) <https://github.com/atahk/pscl/>,    \n    Achim Zeileis, Christian Kleiber, Simon Jackman (2008) <doi:10.18637/jss.v027.i08>.  "
  },
  {
    "id": 22405,
    "package_name": "triggerstrategy",
    "title": "Trigger Strategy in Clinical Trials",
    "description": "The trigger strategy is a general framework for a multistage statistical design with multiple hypotheses, allowing an adaptive selection of interim analyses. The selection of interim stages can be associated with some prespecified endpoints which serve as the trigger. This selection allows us to refine the critical boundaries in hypotheses testing procedures, and potentially increase the statistical power. This package includes several trial designs using the trigger strategy. \n    See Gou, J. (2023), \"Trigger strategy in repeated tests on multiple hypotheses\", Statistics in Biopharmaceutical Research, 15(1), 133-140, and Gou, J. (2022), \"Sample size optimization and initial allocation of the significance levels in group sequential trials with multiple endpoints\", Biometrical Journal, 64(2), 301-311.",
    "version": "1.2.0",
    "maintainer": "Jiangtao Gou <gouRpackage@gmail.com>",
    "author": "Jiangtao Gou [aut, cre],\n  Fengqing (Zoe) Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=triggerstrategy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "triggerstrategy Trigger Strategy in Clinical Trials The trigger strategy is a general framework for a multistage statistical design with multiple hypotheses, allowing an adaptive selection of interim analyses. The selection of interim stages can be associated with some prespecified endpoints which serve as the trigger. This selection allows us to refine the critical boundaries in hypotheses testing procedures, and potentially increase the statistical power. This package includes several trial designs using the trigger strategy. \n    See Gou, J. (2023), \"Trigger strategy in repeated tests on multiple hypotheses\", Statistics in Biopharmaceutical Research, 15(1), 133-140, and Gou, J. (2022), \"Sample size optimization and initial allocation of the significance levels in group sequential trials with multiple endpoints\", Biometrical Journal, 64(2), 301-311.  "
  },
  {
    "id": 22410,
    "package_name": "trimr",
    "title": "An Implementation of Common Response Time Trimming Methods",
    "description": "Provides various commonly-used response time trimming\n    methods, including the recursive / moving-criterion methods reported by\n    Van Selst and Jolicoeur (1994). By passing trimming functions raw data files,\n    the package will return trimmed data ready for inferential testing.",
    "version": "1.1.1",
    "maintainer": "James Grange <grange.jim@gmail.com>",
    "author": "James Grange [cre, aut],\n  Ed Berry [ctb]",
    "url": "https://github.com/JimGrange/trimr",
    "bug_reports": "https://github.com/JimGrange/trimr/issues",
    "repository": "https://cran.r-project.org/package=trimr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "trimr An Implementation of Common Response Time Trimming Methods Provides various commonly-used response time trimming\n    methods, including the recursive / moving-criterion methods reported by\n    Van Selst and Jolicoeur (1994). By passing trimming functions raw data files,\n    the package will return trimmed data ready for inferential testing.  "
  },
  {
    "id": 22427,
    "package_name": "truh",
    "title": "Two-Sample Nonparametric Testing Under Heterogeneity",
    "description": "Implements the TRUH test statistic for two sample testing under heterogeneity. TRUH incorporates the underlying heterogeneity and imbalance in the samples, and provides a conservative test for the composite null hypothesis that the two samples arise from the \n   same mixture distribution but may differ with respect to the mixing weights. See Trambak Banerjee, Bhaswar B. Bhattacharya, Gourab Mukherjee Ann. Appl. Stat. 14(4): 1777-1805 (December 2020). <DOI:10.1214/20-AOAS1362> for more details.",
    "version": "1.0.0",
    "maintainer": "Nathan Smith <nathan_smith_99@ku.edu>",
    "author": "Nathan Smith [aut, cre],\n  Trambak Banerjee [aut],\n  Bhaswar Bhattacharya [aut],\n  Gourab Mukherjee [aut]",
    "url": "https://github.com/natesmith07/truh",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=truh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "truh Two-Sample Nonparametric Testing Under Heterogeneity Implements the TRUH test statistic for two sample testing under heterogeneity. TRUH incorporates the underlying heterogeneity and imbalance in the samples, and provides a conservative test for the composite null hypothesis that the two samples arise from the \n   same mixture distribution but may differ with respect to the mixing weights. See Trambak Banerjee, Bhaswar B. Bhattacharya, Gourab Mukherjee Ann. Appl. Stat. 14(4): 1777-1805 (December 2020). <DOI:10.1214/20-AOAS1362> for more details.  "
  },
  {
    "id": 22441,
    "package_name": "tsDyn",
    "title": "Nonlinear Time Series Models with Regime Switching",
    "description": "Implements nonlinear autoregressive (AR) time series models. For univariate series, a non-parametric approach is available through additive nonlinear AR. Parametric modeling and testing for regime switching dynamics is available when the transition is either direct (TAR: threshold AR) or smooth (STAR: smooth transition AR, LSTAR). For multivariate series, one can estimate a range of TVAR or threshold cointegration TVECM models with two or three regimes. Tests can be conducted for TVAR as well as for TVECM (Hansen and Seo 2002 and Seo 2006). ",
    "version": "11.0.5.2",
    "maintainer": "Matthieu Stigler <Matthieu.Stigler@gmail.com>",
    "author": "Antonio Fabio Di Narzo [aut] (ORCID:\n    <https://orcid.org/0000-0002-4033-5038>),\n  Jose Luis Aznarte [ctb] (ORCID:\n    <https://orcid.org/0000-0002-1636-244X>),\n  Matthieu Stigler [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6802-4290>)",
    "url": "https://github.com/MatthieuStigler/tsDyn/wiki",
    "bug_reports": "https://github.com/MatthieuStigler/tsDyn/issues",
    "repository": "https://cran.r-project.org/package=tsDyn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsDyn Nonlinear Time Series Models with Regime Switching Implements nonlinear autoregressive (AR) time series models. For univariate series, a non-parametric approach is available through additive nonlinear AR. Parametric modeling and testing for regime switching dynamics is available when the transition is either direct (TAR: threshold AR) or smooth (STAR: smooth transition AR, LSTAR). For multivariate series, one can estimate a range of TVAR or threshold cointegration TVECM models with two or three regimes. Tests can be conducted for TVAR as well as for TVECM (Hansen and Seo 2002 and Seo 2006).   "
  },
  {
    "id": 22446,
    "package_name": "tsallisqexp",
    "title": "Tsallis q-Exp Distribution",
    "description": "Tsallis distribution also known as the q-exponential family distribution. Provide distribution d, p, q, r functions, fitting and testing functions. Project initiated by Paul Higbie and based on Cosma Shalizi's code.",
    "version": "0.9-5",
    "maintainer": "Christophe Dutang <dutangc@gmail.com>",
    "author": "Christophe Dutang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6732-1501>),\n  Cosma Shalizi [aut] (ORCID: <https://orcid.org/0000-0002-9195-1308>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tsallisqexp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsallisqexp Tsallis q-Exp Distribution Tsallis distribution also known as the q-exponential family distribution. Provide distribution d, p, q, r functions, fitting and testing functions. Project initiated by Paul Higbie and based on Cosma Shalizi's code.  "
  },
  {
    "id": 22468,
    "package_name": "tsgarch",
    "title": "Univariate GARCH Models",
    "description": "Multiple flavors of the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model with a large choice of conditional distributions. Methods for specification, estimation, prediction, filtering, simulation, statistical testing and more. Represents a partial re-write and re-think of 'rugarch', making use of automatic differentiation for estimation.",
    "version": "1.0.3",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "author": "Alexios Galanos [aut, cre, cph] (ORCID:\n    <https://orcid.org/0009-0000-9308-0457>)",
    "url": "https://github.com/tsmodels/tsgarch",
    "bug_reports": "https://github.com/tsmodels/tsgarch/issues",
    "repository": "https://cran.r-project.org/package=tsgarch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsgarch Univariate GARCH Models Multiple flavors of the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model with a large choice of conditional distributions. Methods for specification, estimation, prediction, filtering, simulation, statistical testing and more. Represents a partial re-write and re-think of 'rugarch', making use of automatic differentiation for estimation.  "
  },
  {
    "id": 22475,
    "package_name": "tsissm",
    "title": "Linear Innovations State Space Unobserved Components Model",
    "description": "Unobserved components time series model using the linear innovations state space representation (single source of error) with choice of error distributions and option for dynamic variance. Methods for estimation using automatic differentiation, automatic model selection and ensembling, prediction, filtering, simulation and backtesting. Based on the model described in Hyndman et al (2012) <doi:10.1198/jasa.2011.tm09771>.",
    "version": "1.0.2",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "author": "Alexios Galanos [aut, cre] (ORCID:\n    <https://orcid.org/0009-0000-9308-0457>)",
    "url": "https://github.com/tsmodels/tsissm,\nhttps://www.nopredict.com/packages/tsissm",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tsissm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tsissm Linear Innovations State Space Unobserved Components Model Unobserved components time series model using the linear innovations state space representation (single source of error) with choice of error distributions and option for dynamic variance. Methods for estimation using automatic differentiation, automatic model selection and ensembling, prediction, filtering, simulation and backtesting. Based on the model described in Hyndman et al (2012) <doi:10.1198/jasa.2011.tm09771>.  "
  },
  {
    "id": 22497,
    "package_name": "ttScreening",
    "title": "Genome-Wide DNA Methylation Sites Screening by Use of Training\nand Testing Samples",
    "description": "A screening process utilizing training and testing samples to filter out uninformative DNA methylation sites. Surrogate variables (SVs) of DNA methylation are included in the filtering process to explain unknown factor effects.",
    "version": "1.7",
    "maintainer": "Meredith Ray <maray@memphis.edu>",
    "author": "Meredith Ray [aut, cre],\n  Xin Tong [aut],\n  Hongmei Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ttScreening",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ttScreening Genome-Wide DNA Methylation Sites Screening by Use of Training\nand Testing Samples A screening process utilizing training and testing samples to filter out uninformative DNA methylation sites. Surrogate variables (SVs) of DNA methylation are included in the filtering process to explain unknown factor effects.  "
  },
  {
    "id": 22502,
    "package_name": "ttdo",
    "title": "Extend 'tinytest' with 'diffobj' and 'tinysnapshot'",
    "description": "The 'tinytest' package offers a light-weight zero-dependency unit-testing\n framework to which this package adds support via the 'diffobj' package for 'diff'-style\n textual comparison of R objects, as well as via 'tinysnapshot' package for visual\n differences in plots.",
    "version": "0.0.10",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "author": "Dirk Eddelbuettel [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6419-907X>),\n  Alton Barbehenn [aut] (ORCID: <https://orcid.org/0009-0000-3364-7204>)",
    "url": "https://github.com/eddelbuettel/ttdo/,\nhttps://dirk.eddelbuettel.com/code/ttdo.html",
    "bug_reports": "https://github.com/eddelbuettel/ttdo/issues",
    "repository": "https://cran.r-project.org/package=ttdo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ttdo Extend 'tinytest' with 'diffobj' and 'tinysnapshot' The 'tinytest' package offers a light-weight zero-dependency unit-testing\n framework to which this package adds support via the 'diffobj' package for 'diff'-style\n textual comparison of R objects, as well as via 'tinysnapshot' package for visual\n differences in plots.  "
  },
  {
    "id": 22521,
    "package_name": "tutorial.helpers",
    "title": "Helper Functions for Creating Tutorials",
    "description": "Helper functions for creating, editing, and testing tutorials\n    created with the 'learnr' package. Provides a simple method for allowing\n    students to download their answers to tutorial questions. For examples\n    of its use, see the 'r4ds.tutorials' package.",
    "version": "0.6.0",
    "maintainer": "David Kane <dave.kane@gmail.com>",
    "author": "David Kane [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6660-3934>)",
    "url": "https://ppbds.github.io/tutorial.helpers/,\nhttps://github.com/PPBDS/tutorial.helpers",
    "bug_reports": "https://github.com/PPBDS/tutorial.helpers/issues",
    "repository": "https://cran.r-project.org/package=tutorial.helpers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tutorial.helpers Helper Functions for Creating Tutorials Helper functions for creating, editing, and testing tutorials\n    created with the 'learnr' package. Provides a simple method for allowing\n    students to download their answers to tutorial questions. For examples\n    of its use, see the 'r4ds.tutorials' package.  "
  },
  {
    "id": 22528,
    "package_name": "tvem",
    "title": "Time-Varying Effect Models",
    "description": "Fits time-varying effect models (TVEM). These are a kind of application of varying-coefficient models in the context of longitudinal data, allowing the strength of linear, logistic, or Poisson regression relationships to change over time.  These models are described further in Tan, Shiyko, Li, Li & Dierker (2012) <doi:10.1037/a0025814>.  We thank Kaylee Litson, Patricia Berglund, Yajnaseni Chakraborti, and Hanjoo Kim for their valuable help with testing the package and the documentation. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.",
    "version": "1.4.1",
    "maintainer": "John J. Dziak <dziakj1@gmail.com>",
    "author": "John J. Dziak [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-0762-5495>),\n  Donna L. Coffman [aut] (ORCID: <https://orcid.org/0000-0001-6305-6579>),\n  Runze Li [aut] (ORCID: <https://orcid.org/0000-0002-0154-2202>),\n  Kaylee Litson [aut] (ORCID: <https://orcid.org/0000-0003-1296-4811>),\n  Yajnaseni Chakraborti [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tvem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tvem Time-Varying Effect Models Fits time-varying effect models (TVEM). These are a kind of application of varying-coefficient models in the context of longitudinal data, allowing the strength of linear, logistic, or Poisson regression relationships to change over time.  These models are described further in Tan, Shiyko, Li, Li & Dierker (2012) <doi:10.1037/a0025814>.  We thank Kaylee Litson, Patricia Berglund, Yajnaseni Chakraborti, and Hanjoo Kim for their valuable help with testing the package and the documentation. The development of this package was part of a research project supported by National Institutes of Health grants P50 DA039838 from the National Institute of Drug Abuse and 1R01 CA229542-01 from the National Cancer Institute and the NIH Office of Behavioral and Social Science Research. Content is solely the responsibility of the authors and does not necessarily represent the official views of the funding institutions mentioned above. This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.  "
  },
  {
    "id": 22559,
    "package_name": "twosamples",
    "title": "Fast Permutation Based Two Sample Tests",
    "description": "Fast randomization based two sample tests. \n  Testing the hypothesis that two samples come from the same distribution using randomization to create p-values. Included tests are: Kolmogorov-Smirnov, Kuiper, Cramer-von Mises, Anderson-Darling, Wasserstein, and DTS. The default test (two_sample) is based on the DTS test statistic, as it is the most powerful, and thus most useful to most users. \n  The DTS test statistic builds on the Wasserstein distance by using a weighting scheme like that of Anderson-Darling. See the companion paper at <arXiv:2007.01360> or <https://codowd.com/public/DTS.pdf> for details of that test statistic, and non-standard uses of the package (parallel for big N, weighted observations, one sample tests, etc). We also include the permutation scheme to make test building simple for others.",
    "version": "2.0.1",
    "maintainer": "Connor Dowd <cd@codowd.com>",
    "author": "Connor Dowd [aut, cre] (ORCID: <https://orcid.org/0000-0002-9782-0931>)",
    "url": "https://twosampletest.com, https://github.com/cdowd/twosamples",
    "bug_reports": "https://github.com/cdowd/twosamples/issues",
    "repository": "https://cran.r-project.org/package=twosamples",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twosamples Fast Permutation Based Two Sample Tests Fast randomization based two sample tests. \n  Testing the hypothesis that two samples come from the same distribution using randomization to create p-values. Included tests are: Kolmogorov-Smirnov, Kuiper, Cramer-von Mises, Anderson-Darling, Wasserstein, and DTS. The default test (two_sample) is based on the DTS test statistic, as it is the most powerful, and thus most useful to most users. \n  The DTS test statistic builds on the Wasserstein distance by using a weighting scheme like that of Anderson-Darling. See the companion paper at <arXiv:2007.01360> or <https://codowd.com/public/DTS.pdf> for details of that test statistic, and non-standard uses of the package (parallel for big N, weighted observations, one sample tests, etc). We also include the permutation scheme to make test building simple for others.  "
  },
  {
    "id": 22560,
    "package_name": "twosigma",
    "title": "DE Analysis for Single-Cell RNA-Sequencing Data",
    "description": "Implements the TWO-Component Single Cell Model-Based Association Method (TWO-SIGMA) for gene-level differential expression (DE) analysis and DE-based gene set testing of single-cell RNA-sequencing datasets. See Van Buren et al. (2020) <doi:10.1002/gepi.22361> and Van Buren et al. (2021) <doi:10.1101/2021.01.24.427979>.   ",
    "version": "1.0.2",
    "maintainer": "Eric Van Buren <edvanburen@gmail.com>",
    "author": "Eric Van Buren [aut, cre],\n  Yun Li [aut],\n  Di Wu [aut],\n  Ming Hu [aut]",
    "url": "https://github.com/edvanburen/twosigma",
    "bug_reports": "https://github.com/edvanburen/twosigma/issues",
    "repository": "https://cran.r-project.org/package=twosigma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twosigma DE Analysis for Single-Cell RNA-Sequencing Data Implements the TWO-Component Single Cell Model-Based Association Method (TWO-SIGMA) for gene-level differential expression (DE) analysis and DE-based gene set testing of single-cell RNA-sequencing datasets. See Van Buren et al. (2020) <doi:10.1002/gepi.22361> and Van Buren et al. (2021) <doi:10.1101/2021.01.24.427979>.     "
  },
  {
    "id": 22564,
    "package_name": "twoxtwo",
    "title": "Work with Two-by-Two Tables",
    "description": "A collection of functions for data analysis with two-by-two contingency tables. The package provides tools to compute measures of effect (odds ratio, risk ratio, and risk difference), calculate impact numbers and attributable fractions, and perform hypothesis testing. Statistical analysis methods are oriented towards epidemiological investigation of relationships between exposures and outcomes.",
    "version": "0.1.0",
    "maintainer": "VP Nagraj <nagraj@nagraj.net>",
    "author": "VP Nagraj [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=twoxtwo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twoxtwo Work with Two-by-Two Tables A collection of functions for data analysis with two-by-two contingency tables. The package provides tools to compute measures of effect (odds ratio, risk ratio, and risk difference), calculate impact numbers and attributable fractions, and perform hypothesis testing. Statistical analysis methods are oriented towards epidemiological investigation of relationships between exposures and outcomes.  "
  },
  {
    "id": 22589,
    "package_name": "ufRisk",
    "title": "Risk Measure Calculation in Financial TS",
    "description": "Enables the user to calculate Value at Risk (VaR) and Expected \n    Shortfall (ES) by means of various parametric and semiparametric \n    GARCH-type models. For the latter the estimation of the nonparametric scale\n    function is carried out by means of a data-driven smoothing approach. Model\n    quality, in terms of forecasting VaR and ES, can be assessed by means of \n    various backtesting methods such as the traffic light test for VaR and a \n    newly developed traffic light test for ES. The approaches implemented in \n    this package are described in e.g. Feng Y., Beran J., Letmathe S. and \n    Ghosh S. (2020) <https://ideas.repec.org/p/pdn/ciepap/137.html> as well as \n    Letmathe S., Feng Y. and Uhde A. (2021) \n    <https://ideas.repec.org/p/pdn/ciepap/141.html>. ",
    "version": "1.0.7",
    "maintainer": "Sebastian Letmathe <sebastian.letmathe@uni-paderborn.de>",
    "author": "Yuanhua Feng [aut] (Paderborn University, Germany),\n  Xuehai Zhang [aut] (Former research associate at Paderborn University,\n    Germany),\n  Christian Peitz [aut] (Paderborn University, Germany),\n  Dominik Schulz [aut] (Paderborn University, Germany),\n  Shujie Li [aut] (Paderborn Universtiy, Germany),\n  Sebastian Letmathe [aut, cre] (Paderborn University, Germany)",
    "url": "https://wiwi.uni-paderborn.de/en/dep4/feng/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=ufRisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ufRisk Risk Measure Calculation in Financial TS Enables the user to calculate Value at Risk (VaR) and Expected \n    Shortfall (ES) by means of various parametric and semiparametric \n    GARCH-type models. For the latter the estimation of the nonparametric scale\n    function is carried out by means of a data-driven smoothing approach. Model\n    quality, in terms of forecasting VaR and ES, can be assessed by means of \n    various backtesting methods such as the traffic light test for VaR and a \n    newly developed traffic light test for ES. The approaches implemented in \n    this package are described in e.g. Feng Y., Beran J., Letmathe S. and \n    Ghosh S. (2020) <https://ideas.repec.org/p/pdn/ciepap/137.html> as well as \n    Letmathe S., Feng Y. and Uhde A. (2021) \n    <https://ideas.repec.org/p/pdn/ciepap/141.html>.   "
  },
  {
    "id": 22641,
    "package_name": "unitizer",
    "title": "Interactive R Unit Tests",
    "description": "Simplifies regression tests by comparing objects produced by test\n    code with earlier versions of those same objects.  If objects are unchanged\n    the tests pass, otherwise execution stops with error details.  If in\n    interactive mode, tests can be reviewed through the provided interactive\n    environment.",
    "version": "1.4.23",
    "maintainer": "Brodie Gaslam <brodie.gaslam@yahoo.com>",
    "author": "Brodie Gaslam [aut, cre],\n  Michael https://github.com/MichaelChirico [ctb],\n  R Core Team [cph] (Traceback function sources.)",
    "url": "https://github.com/brodieG/unitizer",
    "bug_reports": "https://github.com/brodieG/unitizer/issues",
    "repository": "https://cran.r-project.org/package=unitizer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unitizer Interactive R Unit Tests Simplifies regression tests by comparing objects produced by test\n    code with earlier versions of those same objects.  If objects are unchanged\n    the tests pass, otherwise execution stops with error details.  If in\n    interactive mode, tests can be reviewed through the provided interactive\n    environment.  "
  },
  {
    "id": 22644,
    "package_name": "unittest",
    "title": "TAP-Compliant Unit Testing",
    "description": "\n    Concise TAP <http://testanything.org/> compliant unit testing package. Authored tests can be run using CMD check with minimal implementation overhead.",
    "version": "1.7-0",
    "maintainer": "Jamie Lentin <lentinj@shuttlethread.com>",
    "author": "Jamie Lentin [aut, cre],\n  Anthony Hennessey [aut]",
    "url": "",
    "bug_reports": "https://github.com/ravingmantis/unittest/issues",
    "repository": "https://cran.r-project.org/package=unittest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unittest TAP-Compliant Unit Testing \n    Concise TAP <http://testanything.org/> compliant unit testing package. Authored tests can be run using CMD check with minimal implementation overhead.  "
  },
  {
    "id": 22720,
    "package_name": "uxr",
    "title": "User Experience Research",
    "description": "Provides convenience functions for user experience research\n    with an emphasis on quantitative user experience testing and reporting.\n    The functions are designed to translate statistical approaches to \n    applied user experience research.",
    "version": "0.2.0",
    "maintainer": "Joe Chelladurai <joe.chelladurai@yahoo.com>",
    "author": "Joe Chelladurai [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8477-3753>)",
    "url": "https://joe-chelladurai.github.io/uxr/",
    "bug_reports": "https://github.com/joe-chelladurai/uxr/issues",
    "repository": "https://cran.r-project.org/package=uxr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uxr User Experience Research Provides convenience functions for user experience research\n    with an emphasis on quantitative user experience testing and reporting.\n    The functions are designed to translate statistical approaches to \n    applied user experience research.  "
  },
  {
    "id": 22762,
    "package_name": "varTestnlme",
    "title": "Variance Components Testing for Linear and Nonlinear Mixed\nEffects Models",
    "description": "An implementation of the Likelihood ratio Test (LRT) for testing that,\n    in a (non)linear mixed effects model, the variances of a subset of the random\n    effects are equal to zero. There is no restriction on the subset of variances\n    that can be tested: for example, it is possible to test that all the variances\n    are equal to zero. Note that the implemented test is asymptotic.\n    This package should be used on model fits from packages 'nlme', 'lmer', and 'saemix'.\n    Charlotte Baey and Estelle Kuhn (2019) <doi:10.18637/jss.v107.i06>.",
    "version": "1.3.5",
    "maintainer": "Charlotte Baey <charlotte.baey@univ-lille.fr>",
    "author": "Charlotte Baey [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1413-1058>),\n  Estelle Kuhn [aut]",
    "url": "https://github.com/baeyc/varTestnlme/",
    "bug_reports": "https://github.com/baeyc/varTestnlme/issues",
    "repository": "https://cran.r-project.org/package=varTestnlme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "varTestnlme Variance Components Testing for Linear and Nonlinear Mixed\nEffects Models An implementation of the Likelihood ratio Test (LRT) for testing that,\n    in a (non)linear mixed effects model, the variances of a subset of the random\n    effects are equal to zero. There is no restriction on the subset of variances\n    that can be tested: for example, it is possible to test that all the variances\n    are equal to zero. Note that the implemented test is asymptotic.\n    This package should be used on model fits from packages 'nlme', 'lmer', and 'saemix'.\n    Charlotte Baey and Estelle Kuhn (2019) <doi:10.18637/jss.v107.i06>.  "
  },
  {
    "id": 22773,
    "package_name": "variosig",
    "title": "Testing Spatial Dependence Using Empirical Variogram",
    "description": "Applying Monte Carlo permutation to generate pointwise variogram envelope and checking for spatial dependence at different scales using permutation test. Empirical Brown's method and Fisher's method are used to compute overall p-value for hypothesis test.",
    "version": "0.3-1",
    "maintainer": "Craig Wang <craig.wang@math.uzh.ch>",
    "author": "Craig Wang [aut, cre] (ORCID: <https://orcid.org/0000-0003-1804-2463>),\n  Reinhard Furrer [ctb] (ORCID: <https://orcid.org/0000-0002-6319-2332>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=variosig",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "variosig Testing Spatial Dependence Using Empirical Variogram Applying Monte Carlo permutation to generate pointwise variogram envelope and checking for spatial dependence at different scales using permutation test. Empirical Brown's method and Fisher's method are used to compute overall p-value for hypothesis test.  "
  },
  {
    "id": 22777,
    "package_name": "vars",
    "title": "VAR Modelling",
    "description": "Estimation, lag selection, diagnostic testing, forecasting, causality analysis, forecast error variance decomposition and impulse response functions of VAR models and estimation of SVAR and SVEC models.",
    "version": "1.6-1",
    "maintainer": "Bernhard Pfaff <bernhard@pfaffikus.de>",
    "author": "Bernhard Pfaff [aut, cre],\n  Matthieu Stigler [ctb]",
    "url": "https://www.pfaffikus.de",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vars VAR Modelling Estimation, lag selection, diagnostic testing, forecasting, causality analysis, forecast error variance decomposition and impulse response functions of VAR models and estimation of SVAR and SVEC models.  "
  },
  {
    "id": 22778,
    "package_name": "vartest",
    "title": "Tests for Variance Homogeneity",
    "description": "Performs 20 omnibus tests for testing the composite hypothesis of variance homogeneity.",
    "version": "1.3",
    "maintainer": "Osman Dag <osman.dag@outlook.com>",
    "author": "Gozde Cosar [aut],\n  Merve Kasikci [aut],\n  Osman Dag [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vartest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vartest Tests for Variance Homogeneity Performs 20 omnibus tests for testing the composite hypothesis of variance homogeneity.  "
  },
  {
    "id": 22808,
    "package_name": "veesa",
    "title": "Pipeline for Explainable Machine Learning with Functional Data",
    "description": "Implements the Variable importance Explainable Elastic Shape Analysis pipeline for explainable machine learning with functional data inputs. Converts training and testing data functional inputs to elastic shape analysis principal components that account for vertical and/or horizontal variability. Computes feature importance to identify important principal components and visualizes variability captured by functional principal components. See Goode et al. (2025) <doi:10.48550/arXiv.2501.07602> for technical details about the methodology.",
    "version": "0.1.7",
    "maintainer": "Katherine Goode <kjgoode@sandia.gov>",
    "author": "Katherine Goode [cre, aut],\n  J. Derek Tucker [aut],\n  Sandia National Laboratories [cph, fnd]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=veesa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "veesa Pipeline for Explainable Machine Learning with Functional Data Implements the Variable importance Explainable Elastic Shape Analysis pipeline for explainable machine learning with functional data inputs. Converts training and testing data functional inputs to elastic shape analysis principal components that account for vertical and/or horizontal variability. Computes feature importance to identify important principal components and visualizes variability captured by functional principal components. See Goode et al. (2025) <doi:10.48550/arXiv.2501.07602> for technical details about the methodology.  "
  },
  {
    "id": 22819,
    "package_name": "vek",
    "title": "Predicate Helper Functions for Testing Simple Atomic Vectors",
    "description": "\n  Predicate helper functions for testing atomic vectors in R. All functions take\n  a single argument 'x' and check whether it's of the target type of base-R\n  atomic vector (i.e. no class extensions nor attributes other than 'names'),\n  returning TRUE or FALSE. Some additionally check for value (e.g. absence of\n  missing values, infinities, blank characters, or 'names' attribute; or having\n  length 1).",
    "version": "1.0.0",
    "maintainer": "Sam Semegne <sam.ahoi@hotmail.com>",
    "author": "Sam Semegne [aut, cre]",
    "url": "https://github.com/samsemegne/vek",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vek",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vek Predicate Helper Functions for Testing Simple Atomic Vectors \n  Predicate helper functions for testing atomic vectors in R. All functions take\n  a single argument 'x' and check whether it's of the target type of base-R\n  atomic vector (i.e. no class extensions nor attributes other than 'names'),\n  returning TRUE or FALSE. Some additionally check for value (e.g. absence of\n  missing values, infinities, blank characters, or 'names' attribute; or having\n  length 1).  "
  },
  {
    "id": 22893,
    "package_name": "vita",
    "title": "Variable Importance Testing Approaches",
    "description": "Implements the novel testing approach by Janitza et al.(2015)\n    <http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-25587-4>\n    for the permutation variable importance measure in a random forest and the\n    PIMP-algorithm by Altmann et al.(2010) <doi:10.1093/bioinformatics/btq134>.\n    Janitza et al.(2015) <http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-25587-4>\n    do not use the \"standard\" permutation variable\n    importance but the cross-validated permutation variable\n    importance for the novel test approach. The cross-validated\n    permutation variable importance is not based on the out-of-bag\n    observations but uses a similar strategy which is inspired by\n    the cross-validation procedure. The novel test approach can be\n    applied for classification trees as well as for regression\n    trees. However, the use of the novel testing approach has not\n    been tested for regression trees so far, so this routine is\n    meant for the expert user only and its current state is rather\n    experimental.",
    "version": "1.0.0",
    "maintainer": "Ender Celik <celik.p.ender@gmail.com>",
    "author": "Ender Celik [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vita",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vita Variable Importance Testing Approaches Implements the novel testing approach by Janitza et al.(2015)\n    <http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-25587-4>\n    for the permutation variable importance measure in a random forest and the\n    PIMP-algorithm by Altmann et al.(2010) <doi:10.1093/bioinformatics/btq134>.\n    Janitza et al.(2015) <http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-25587-4>\n    do not use the \"standard\" permutation variable\n    importance but the cross-validated permutation variable\n    importance for the novel test approach. The cross-validated\n    permutation variable importance is not based on the out-of-bag\n    observations but uses a similar strategy which is inspired by\n    the cross-validation procedure. The novel test approach can be\n    applied for classification trees as well as for regression\n    trees. However, the use of the novel testing approach has not\n    been tested for regression trees so far, so this routine is\n    meant for the expert user only and its current state is rather\n    experimental.  "
  },
  {
    "id": 22982,
    "package_name": "walrus",
    "title": "Robust Statistical Methods",
    "description": "A toolbox of common robust statistical tests, including robust\n  descriptives, robust t-tests, and robust ANOVA. It is also available as a\n  module for 'jamovi' (see <https://www.jamovi.org> for more information).\n  Walrus is based on the WRS2 package by Patrick Mair, which is in turn based on\n  the scripts and work of Rand Wilcox. These analyses are described in depth in\n  the book 'Introduction to Robust Estimation & Hypothesis Testing'.",
    "version": "1.0.5",
    "maintainer": "Jonathon Love <jon@thon.cc>",
    "author": "Jonathon Love, Patrick Mair",
    "url": "https://github.com/jamovi/walrus",
    "bug_reports": "https://github.com/jamovi/walrus/issues",
    "repository": "https://cran.r-project.org/package=walrus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "walrus Robust Statistical Methods A toolbox of common robust statistical tests, including robust\n  descriptives, robust t-tests, and robust ANOVA. It is also available as a\n  module for 'jamovi' (see <https://www.jamovi.org> for more information).\n  Walrus is based on the WRS2 package by Patrick Mair, which is in turn based on\n  the scripts and work of Rand Wilcox. These analyses are described in depth in\n  the book 'Introduction to Robust Estimation & Hypothesis Testing'.  "
  },
  {
    "id": 23014,
    "package_name": "wbsd",
    "title": "Wild Bootstrap Size Diagnostics",
    "description": "Implements the diagnostic \"theta\" developed in Poetscher and Preinerstorfer (2020) \"How Reliable are Bootstrap-based Heteroskedasticity Robust Tests?\" <arXiv:2005.04089>. This diagnostic can be used to detect and weed out bootstrap-based procedures that provably have size equal to one for a given testing problem. The implementation covers a large variety of bootstrap-based procedures, cf. the above mentioned article for details. A function for computing bootstrap p-values is provided.",
    "version": "1.0.0",
    "maintainer": "David Preinerstorfer <david.preinerstorfer@ulb.be>",
    "author": "David Preinerstorfer ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wbsd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wbsd Wild Bootstrap Size Diagnostics Implements the diagnostic \"theta\" developed in Poetscher and Preinerstorfer (2020) \"How Reliable are Bootstrap-based Heteroskedasticity Robust Tests?\" <arXiv:2005.04089>. This diagnostic can be used to detect and weed out bootstrap-based procedures that provably have size equal to one for a given testing problem. The implementation covers a large variety of bootstrap-based procedures, cf. the above mentioned article for details. A function for computing bootstrap p-values is provided.  "
  },
  {
    "id": 23060,
    "package_name": "welchADF",
    "title": "Welch-James Statistic for Robust Hypothesis Testing under\nHeterocedasticity and Non-Normality",
    "description": "Implementation of Johansen's general formulation of Welch-James's statistic with Approximate Degrees of Freedom, which makes it suitable for testing \n    any linear hypothesis concerning cell means in univariate and multivariate mixed model designs when the data pose non-normality and non-homogeneous variance. Some \n    improvements, namely trimmed means and Winsorized variances, and bootstrapping for calculating an empirical critical value, have been added to the classical formulation. \n    The code departs from a previous SAS implementation by L.M. Lix and H.J. Keselman, available at <http://supp.apa.org/psycarticles/supplemental/met_13_2_110/SAS_Program.pdf> and\n    published in Keselman, H.J., Wilcox, R.R., and Lix, L.M. (2003) <DOI:10.1111/1469-8986.00060>.",
    "version": "0.3.2",
    "maintainer": "Pablo J. Villacorta <pjvi@decsai.ugr.es>",
    "author": "Pablo J. Villacorta <pjvi@decsai.ugr.es>",
    "url": "<http://decsai.ugr.es/~pjvi/r-packages.html>",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=welchADF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "welchADF Welch-James Statistic for Robust Hypothesis Testing under\nHeterocedasticity and Non-Normality Implementation of Johansen's general formulation of Welch-James's statistic with Approximate Degrees of Freedom, which makes it suitable for testing \n    any linear hypothesis concerning cell means in univariate and multivariate mixed model designs when the data pose non-normality and non-homogeneous variance. Some \n    improvements, namely trimmed means and Winsorized variances, and bootstrapping for calculating an empirical critical value, have been added to the classical formulation. \n    The code departs from a previous SAS implementation by L.M. Lix and H.J. Keselman, available at <http://supp.apa.org/psycarticles/supplemental/met_13_2_110/SAS_Program.pdf> and\n    published in Keselman, H.J., Wilcox, R.R., and Lix, L.M. (2003) <DOI:10.1111/1469-8986.00060>.  "
  },
  {
    "id": 23082,
    "package_name": "whippr",
    "title": "Tools for Manipulating Gas Exchange Data",
    "description": "Set of tools for manipulating gas exchange data from cardiopulmonary exercise testing.",
    "version": "0.1.4",
    "maintainer": "Felipe Mattioni Maturana <felipe.mattioni@med.uni-tuebingen.de>",
    "author": "Felipe Mattioni Maturana [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4221-6104>)",
    "url": "https://fmmattioni.github.io/whippr/,\nhttps://github.com/fmmattioni/whippr",
    "bug_reports": "https://github.com/fmmattioni/whippr/issues",
    "repository": "https://cran.r-project.org/package=whippr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "whippr Tools for Manipulating Gas Exchange Data Set of tools for manipulating gas exchange data from cardiopulmonary exercise testing.  "
  },
  {
    "id": 23173,
    "package_name": "wrMisc",
    "title": "Analyze Experimental High-Throughput (Omics) Data",
    "description": "The efficient treatment and convenient analysis of experimental high-throughput (omics) data gets facilitated through this collection of diverse functions. \n  Several functions address advanced object-conversions, like manipulating lists of lists or lists of arrays, reorganizing lists to arrays or into separate vectors, merging of multiple entries, etc.  \n  Another set of functions provides speed-optimized calculation of standard deviation (sd), coefficient of variance (CV) or standard error of the mean (SEM)  \n  for data in matrixes or means per line with respect to additional grouping (eg n groups of replicates). \n  A group of functions facilitate dealing with non-redundant information, by indexing unique, adding counters to redundant or eliminating lines with respect redundancy in a given reference-column, etc. \n  Help is provided to identify very closely matching numeric values to generate (partial) distance matrixes for very big data in a memory efficient manner or to reduce the complexity of large data-sets by combining very close values. \n  Other functions help aligning a matrix or data.frame to a reference using partial matching or to mine an experimental setup to extract patterns of replicate samples.\n  Many times large experimental datasets need some additional filtering, adequate functions are provided. \n  Convenient data normalization is supported in various different modes, parameter estimation via permutations or boot-strap as well as flexible testing of multiple pair-wise combinations using the framework of 'limma' is provided, too.\n  Batch reading (or writing) of sets of files and combining data to arrays is supported, too. ",
    "version": "1.15.4",
    "maintainer": "Wolfgang Raffelsberger <w.raffelsberger@gmail.com>",
    "author": "Wolfgang Raffelsberger [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wrMisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wrMisc Analyze Experimental High-Throughput (Omics) Data The efficient treatment and convenient analysis of experimental high-throughput (omics) data gets facilitated through this collection of diverse functions. \n  Several functions address advanced object-conversions, like manipulating lists of lists or lists of arrays, reorganizing lists to arrays or into separate vectors, merging of multiple entries, etc.  \n  Another set of functions provides speed-optimized calculation of standard deviation (sd), coefficient of variance (CV) or standard error of the mean (SEM)  \n  for data in matrixes or means per line with respect to additional grouping (eg n groups of replicates). \n  A group of functions facilitate dealing with non-redundant information, by indexing unique, adding counters to redundant or eliminating lines with respect redundancy in a given reference-column, etc. \n  Help is provided to identify very closely matching numeric values to generate (partial) distance matrixes for very big data in a memory efficient manner or to reduce the complexity of large data-sets by combining very close values. \n  Other functions help aligning a matrix or data.frame to a reference using partial matching or to mine an experimental setup to extract patterns of replicate samples.\n  Many times large experimental datasets need some additional filtering, adequate functions are provided. \n  Convenient data normalization is supported in various different modes, parameter estimation via permutations or boot-strap as well as flexible testing of multiple pair-wise combinations using the framework of 'limma' is provided, too.\n  Batch reading (or writing) of sets of files and combining data to arrays is supported, too.   "
  },
  {
    "id": 23174,
    "package_name": "wrProteo",
    "title": "Proteomics Data Analysis Functions",
    "description": "Data analysis of proteomics experiments by mass spectrometry is supported by this collection of functions mostly dedicated to the analysis of (bottom-up) quantitative (XIC) data. \n    Fasta-formatted proteomes (eg from UniProt Consortium <doi:10.1093/nar/gky1049>) can be read with automatic parsing and multiple annotation types (like species origin, abbreviated gene names, etc) extracted. \n\tInitial results from multiple software for protein (and peptide) quantitation can be imported (to a common format): \n\tMaxQuant (Tyanova et al 2016 <doi:10.1038/nprot.2016.136>), Dia-NN (Demichev et al 2020 <doi:10.1038/s41592-019-0638-x>), \n\tFragpipe (da Veiga et al 2020 <doi:10.1038/s41592-020-0912-y>), ionbot (Degroeve et al 2021 <doi:10.1101/2021.07.02.450686>),\n\tMassChroq (Valot et al 2011 <doi:10.1002/pmic.201100120>), \n    OpenMS (Strauss et al 2021 <doi:10.1038/nmeth.3959>), ProteomeDiscoverer (Orsburn 2021 <doi:10.3390/proteomes9010015>), \n\tProline (Bouyssie et al 2020 <doi:10.1093/bioinformatics/btaa118>), AlphaPept (preprint Strauss et al <doi:10.1101/2021.07.23.453379>) \n\tand Wombat-P (Bouyssie et al 2023 <doi:10.1021/acs.jproteome.3c00636>. \n\tMeta-data provided by initial analysis software and/or in sdrf format can be integrated to the analysis. \n    Quantitative proteomics measurements frequently contain multiple NA values, due to physical absence of given peptides in some samples, limitations in sensitivity or other reasons. \n    Help is provided to inspect the data graphically to investigate the nature of NA-values via their respective replicate measurements \n\tand to help/confirm the choice of NA-replacement algorithms. \n\tMeta-data in sdrf-format (Perez-Riverol et al 2020 <doi:10.1021/acs.jproteome.0c00376>) or similar tabular formats can be imported and included.\n\tMissing values can be inspected and imputed based on the concept of NA-neighbours or other methods.\n    Dedicated filtering and statistical testing using the framework of package 'limma' <doi:10.18129/B9.bioc.limma> can be run, enhanced by multiple rounds of NA-replacements to provide robustness towards rare stochastic events. \n    Multi-species samples, as frequently used in benchmark-tests (eg Navarro et al 2016 <doi:10.1038/nbt.3685>, Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>), can be run with special options considering \n    such sub-groups during normalization and testing. Subsequently, ROC curves (Hand and Till 2001 <doi:10.1023/A:1010920819831>) can be constructed to compare multiple analysis approaches.\n    As detailed example the data-set from Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>) quantified by MaxQuant, ProteomeDiscoverer,\n    and Proline is provided with a detailed analysis of heterologous spike-in proteins.     ",
    "version": "1.13.3",
    "maintainer": "Wolfgang Raffelsberger <w.raffelsberger@gmail.com>",
    "author": "Wolfgang Raffelsberger [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wrProteo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wrProteo Proteomics Data Analysis Functions Data analysis of proteomics experiments by mass spectrometry is supported by this collection of functions mostly dedicated to the analysis of (bottom-up) quantitative (XIC) data. \n    Fasta-formatted proteomes (eg from UniProt Consortium <doi:10.1093/nar/gky1049>) can be read with automatic parsing and multiple annotation types (like species origin, abbreviated gene names, etc) extracted. \n\tInitial results from multiple software for protein (and peptide) quantitation can be imported (to a common format): \n\tMaxQuant (Tyanova et al 2016 <doi:10.1038/nprot.2016.136>), Dia-NN (Demichev et al 2020 <doi:10.1038/s41592-019-0638-x>), \n\tFragpipe (da Veiga et al 2020 <doi:10.1038/s41592-020-0912-y>), ionbot (Degroeve et al 2021 <doi:10.1101/2021.07.02.450686>),\n\tMassChroq (Valot et al 2011 <doi:10.1002/pmic.201100120>), \n    OpenMS (Strauss et al 2021 <doi:10.1038/nmeth.3959>), ProteomeDiscoverer (Orsburn 2021 <doi:10.3390/proteomes9010015>), \n\tProline (Bouyssie et al 2020 <doi:10.1093/bioinformatics/btaa118>), AlphaPept (preprint Strauss et al <doi:10.1101/2021.07.23.453379>) \n\tand Wombat-P (Bouyssie et al 2023 <doi:10.1021/acs.jproteome.3c00636>. \n\tMeta-data provided by initial analysis software and/or in sdrf format can be integrated to the analysis. \n    Quantitative proteomics measurements frequently contain multiple NA values, due to physical absence of given peptides in some samples, limitations in sensitivity or other reasons. \n    Help is provided to inspect the data graphically to investigate the nature of NA-values via their respective replicate measurements \n\tand to help/confirm the choice of NA-replacement algorithms. \n\tMeta-data in sdrf-format (Perez-Riverol et al 2020 <doi:10.1021/acs.jproteome.0c00376>) or similar tabular formats can be imported and included.\n\tMissing values can be inspected and imputed based on the concept of NA-neighbours or other methods.\n    Dedicated filtering and statistical testing using the framework of package 'limma' <doi:10.18129/B9.bioc.limma> can be run, enhanced by multiple rounds of NA-replacements to provide robustness towards rare stochastic events. \n    Multi-species samples, as frequently used in benchmark-tests (eg Navarro et al 2016 <doi:10.1038/nbt.3685>, Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>), can be run with special options considering \n    such sub-groups during normalization and testing. Subsequently, ROC curves (Hand and Till 2001 <doi:10.1023/A:1010920819831>) can be constructed to compare multiple analysis approaches.\n    As detailed example the data-set from Ramus et al 2016 <doi:10.1016/j.jprot.2015.11.011>) quantified by MaxQuant, ProteomeDiscoverer,\n    and Proline is provided with a detailed analysis of heterologous spike-in proteins.       "
  },
  {
    "id": 23190,
    "package_name": "wtest",
    "title": "The W-Test for Genetic Interactions Testing",
    "description": "Perform the calculation of W-test, diagnostic checking, calculate\n    minor allele frequency (MAF) and odds ratio.",
    "version": "3.2",
    "maintainer": "Rui Sun <rsunzju@gmail.com>",
    "author": "Rui Sun, Maggie Haitian Wang",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wtest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wtest The W-Test for Genetic Interactions Testing Perform the calculation of W-test, diagnostic checking, calculate\n    minor allele frequency (MAF) and odds ratio.  "
  },
  {
    "id": 23194,
    "package_name": "wyz.code.metaTesting",
    "title": "Wizardry Code Meta Testing",
    "description": "Meta testing is the ability to test a function without having to \n    provide its parameter values.\n    Those values will be generated, based on semantic naming of parameters, as \n    introduced by package 'wyz.code.offensiveProgramming'.\n    Value generation logic can be completed with your own data types \n    and generation schemes. This to meet your most specific requirements and to \n    answer to a wide variety of usages, from general use case to very specific\n    ones.\n    While using meta testing, it becomes easier to generate stress test \n    campaigns, non-regression test campaigns and robustness test campaigns, as \n    generated tests can be saved and reused from session to session. \n    Main benefits of using 'wyz.code.metaTesting' is ability to discover valid \n    and invalid function parameter combinations, ability to infer valid \n    parameter values, and to provide smart summaries that allows you to focus\n    on dysfunctional cases. ",
    "version": "1.1.22",
    "maintainer": "Fabien Gelineau <neonira@gmail.com>",
    "author": "Fabien Gelineau <neonira@gmail.com>",
    "url": "https://neonira.github.io/offensiveProgrammingBook_v1.2.2/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wyz.code.metaTesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wyz.code.metaTesting Wizardry Code Meta Testing Meta testing is the ability to test a function without having to \n    provide its parameter values.\n    Those values will be generated, based on semantic naming of parameters, as \n    introduced by package 'wyz.code.offensiveProgramming'.\n    Value generation logic can be completed with your own data types \n    and generation schemes. This to meet your most specific requirements and to \n    answer to a wide variety of usages, from general use case to very specific\n    ones.\n    While using meta testing, it becomes easier to generate stress test \n    campaigns, non-regression test campaigns and robustness test campaigns, as \n    generated tests can be saved and reused from session to session. \n    Main benefits of using 'wyz.code.metaTesting' is ability to discover valid \n    and invalid function parameter combinations, ability to infer valid \n    parameter values, and to provide smart summaries that allows you to focus\n    on dysfunctional cases.   "
  },
  {
    "id": 23197,
    "package_name": "wyz.code.testthat",
    "title": "Wizardry Code Offensive Programming Test Generation",
    "description": "Allows to generate automatically 'testthat' code files from offensive \n    programming test cases. Generated test files are complete and ready to run.\n    Using 'wyz.code.testthat' you will earn a lot of time, reduce the number of\n    errors in test case production, be able to test immediately generated files \n    without any need to view or modify them, and enter a zero time latency between \n    code implementation and industrial testing. As with 'testthat', you may\n    complete provided test cases according to your needs to push testing further, \n    but this need is nearly void when using 'wyz.code.offensiveProgramming'. ",
    "version": "1.1.20",
    "maintainer": "Fabien Gelineau <neonira@gmail.com>",
    "author": "Fabien Gelineau <neonira@gmail.com>",
    "url": "https://neonira.github.io/offensiveProgrammingBook_v1.2.2/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=wyz.code.testthat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wyz.code.testthat Wizardry Code Offensive Programming Test Generation Allows to generate automatically 'testthat' code files from offensive \n    programming test cases. Generated test files are complete and ready to run.\n    Using 'wyz.code.testthat' you will earn a lot of time, reduce the number of\n    errors in test case production, be able to test immediately generated files \n    without any need to view or modify them, and enter a zero time latency between \n    code implementation and industrial testing. As with 'testthat', you may\n    complete provided test cases according to your needs to push testing further, \n    but this need is nearly void when using 'wyz.code.offensiveProgramming'.   "
  },
  {
    "id": 23250,
    "package_name": "xpectr",
    "title": "Generates Expectations for 'testthat' Unit Testing",
    "description": "Helps systematize and ease the process of \n    building unit tests with the 'testthat' package by providing \n    tools for generating expectations.",
    "version": "0.4.4",
    "maintainer": "Ludvig Renbo Olsen <r-pkgs@ludvigolsen.dk>",
    "author": "Ludvig Renbo Olsen [aut, cre] (ORCID:\n    <https://orcid.org/0009-0006-6798-7454>),\n  R. Mark Sharp [ctb]",
    "url": "https://github.com/ludvigolsen/xpectr",
    "bug_reports": "https://github.com/ludvigolsen/xpectr/issues",
    "repository": "https://cran.r-project.org/package=xpectr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xpectr Generates Expectations for 'testthat' Unit Testing Helps systematize and ease the process of \n    building unit tests with the 'testthat' package by providing \n    tools for generating expectations.  "
  },
  {
    "id": 23268,
    "package_name": "xxIRT",
    "title": "Item Response Theory and Computer-Based Testing in R",
    "description": "A suite of psychometric analysis tools for research and operation, including:\n    (1) computation of probability, information, and likelihood for the 3PL, GPCM, and GRM;\n    (2) parameter estimation using joint or marginal likelihood estimation method;\n    (3) simulation of computerized adaptive testing using built-in or customized algorithms;\n    (4) assembly and simulation of multistage testing. \n    The full documentation and tutorials are at <https://github.com/xluo11/xxIRT>.",
    "version": "2.1.2",
    "maintainer": "Xiao Luo <xluo1986@gmail.com>",
    "author": "Xiao Luo [aut, cre]",
    "url": "https://github.com/xluo11/xxIRT",
    "bug_reports": "https://github.com/xluo11/xxIRT/issues",
    "repository": "https://cran.r-project.org/package=xxIRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "xxIRT Item Response Theory and Computer-Based Testing in R A suite of psychometric analysis tools for research and operation, including:\n    (1) computation of probability, information, and likelihood for the 3PL, GPCM, and GRM;\n    (2) parameter estimation using joint or marginal likelihood estimation method;\n    (3) simulation of computerized adaptive testing using built-in or customized algorithms;\n    (4) assembly and simulation of multistage testing. \n    The full documentation and tutorials are at <https://github.com/xluo11/xxIRT>.  "
  },
  {
    "id": 23316,
    "package_name": "zfa",
    "title": "Zoom-Focus Algorithm",
    "description": "Performs Zoom-Focus Algorithm (ZFA) to optimize testing regions for rare variant association tests in exome sequencing data.",
    "version": "1.1.0",
    "maintainer": "Yexian Zhang <yxzhang@bethbio.com>",
    "author": "Yexian Zhang [cre, aut],\n  Haoyi Weng [aut],\n  Maggie Wang [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=zfa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "zfa Zoom-Focus Algorithm Performs Zoom-Focus Algorithm (ZFA) to optimize testing regions for rare variant association tests in exome sequencing data.  "
  }
]